robust incentive techniques for peer-to-peer networks 
michal feldman 
mfeldman sims berkeley edu 
kevin lai 
klai hp com 
ion stoica 
istoica cs berkeley edu 
john chuang 
chuang sims berkeley edu 
 
school of information 
management and systems 
u c berkeley 
 
hp labs 
computer science division 
u c berkeley 
abstract 
lack of cooperation free riding is one of the key problems that 
confronts today s p p systems what makes this problem 
particularly difficult is the unique set of challenges that p p systems 
pose large populations high turnover asymmetry of interest 
collusion zero-cost identities and traitors to tackle these challenges we 
model the p p system using the generalized prisoner s dilemma 
 gpd and propose the reciprocative decision function as the 
basis of a family of incentives techniques these techniques are fully 
distributed and include discriminating server selection 
maxflowbased subjective reputation and adaptive stranger policies through 
simulation we show that these techniques can drive a system of 
strategic users to nearly optimal levels of cooperation 
categories and subject descriptors 
c computer-communication networks distributed 
systems j social and behavioral sciences economics 
general terms 
design economics 
 introduction 
many peer-to-peer p p systems rely on cooperation among 
selfinterested users for example in a file-sharing system overall 
download latency and failure rate increase when users do not share 
their resources in a wireless ad-hoc network overall packet 
latency and loss rate increase when nodes refuse to forward packets 
on behalf of others further examples are file preservation 
discussion boards online auctions and overlay routing 
 in many of these systems users have natural disincentives to 
cooperate because cooperation consumes their own resources and 
may degrade their own performance as a result each user s 
attempt to maximize her own utility effectively lowers the overall 
a 
bc 
figure example of asymmetry of interest a wants service from b 
b wants service form c and c wants service from a 
utility of the system avoiding this tragedy of the commons 
requires incentives for cooperation 
we adopt a game-theoretic approach in addressing this problem in 
particular we use a prisoners dilemma model to capture the 
essential tension between individual and social utility asymmetric 
payoff matrices to allow asymmetric transactions between peers 
and a learning-based population dynamic model to specify the 
behavior of individual peers which can be changed continuously 
while social dilemmas have been studied extensively p p 
applications impose a unique set of challenges including 
 large populations and high turnover a file sharing 
system such as gnutella and kazaa can exceed 
simultaneous users and nodes can have an average life-time of the 
order of minutes 
 asymmetry of interest asymmetric transactions of p p 
systems create the possibility for asymmetry of interest in 
the example in figure a wants service from b b wants 
service from c and c wants service from a 
 zero-cost identity many p p systems allow peers to 
continuously switch identities i e whitewash 
strategies that work well in traditional prisoners dilemma games 
such as tit-for-tat will not fare well in the p p context 
therefore we propose a family of scalable and robust incentive 
techniques based upon a novel reciprocative decision function to 
address these challenges and provide different tradeoffs 
 discriminating server selection cooperation requires 
familiarity between entities either directly or indirectly 
however the large populations and high turnover of p p systems 
makes it less likely that repeat interactions will occur with 
a familiar entity we show that by having each peer keep a 
 
private history of the actions of other peers toward her and 
using discriminating server selection the reciprocative 
decision function can scale to large populations and moderate 
levels of turnover 
 shared history scaling to higher turnover and mitigating 
asymmetry of interest requires shared history consider the 
example in figure if everyone provides service then the 
system operates optimally however if everyone keeps only 
private history no one will provide service because b does 
not know that a has served c etc we show that with shared 
history b knows that a served c and consequently will serve 
a this results in a higher level of cooperation than with 
private history the cost of shared history is a distributed 
infrastructure e g distributed hash table-based storage to store 
the history 
 maxflow-based subjective reputation shared history 
creates the possibility for collusion in the example in figure 
c can falsely claim that a served him thus deceiving b into 
providing service we show that a maxflow-based algorithm 
that computes reputation subjectively promotes cooperation 
despite collusion among of the population the basic idea 
is that b would only believe c if c had already provided 
service to b the cost of the maxflow algorithm is its o v 
 
running time where v is the number of nodes in the system 
to eliminate this cost we have developed a constant mean 
running time variation which trades effectiveness for 
complexity of computation we show that the maxflow-based 
algorithm scales better than private history in the presence of 
colluders without the centralized trust required in previous 
work 
 adaptive stranger policy zero-cost identities allows 
noncooperating peers to escape the consequences of not 
cooperating and eventually destroy cooperation in the system if not 
stopped we show that if reciprocative peers treat strangers 
 peers with no history using a policy that adapts to the 
behavior of previous strangers peers have little incentive to 
whitewash and whitewashing can be nearly eliminated from 
the system the adaptive stranger policy does this without 
requiring centralized allocation of identities an entry fee for 
newcomers or rate-limiting 
 short-term history history also creates the possibility that 
a previously well-behaved peer with a good reputation will 
turn traitor and use his good reputation to exploit other peers 
the peer could be making a strategic decision or someone 
may have hijacked her identity e g by compromising her 
host long-term history exacerbates this problem by 
allowing peers with many previous transactions to exploit that 
history for many new transactions we show that short-term 
history prevents traitors from disrupting cooperation 
the rest of the paper is organized as follows we describe the model 
in section and the reciprocative decision function in section we 
then proceed to the incentive techniques in section in section 
we describe the challenges of large populations and high turnover 
and show the effectiveness of discriminating server selection and 
shared history in section we describe collusion and 
demonstrate how subjective reputation mitigates it in section we 
present the problem of zero-cost identities and show how an 
adaptive stranger policy promotes persistent identities in section 
we show how traitors disrupt cooperation and how short-term 
history deals with them we discuss related work in section and 
conclude in section 
 model and assumptions 
in this section we present our assumptions about p p systems and 
their users and introduce a model that aims to capture the behavior 
of users in a p p system 
 assumptions 
we assume a p p system in which users are strategic i e they 
act rationally to maximize their benefit however to capture some 
of the real-life unpredictability in the behavior of users we allow 
users to randomly change their behavior with a low probability see 
section 
for simplicity we assume a homogeneous system in which all peers 
issue and satisfy requests at the same rate a peer can satisfy any 
request and unless otherwise specified peers request service 
uniformly at random from the population 
 finally we assume that all 
transactions incur the same cost to all servers and provide the same 
benefit to all clients 
we assume that users can pollute shared history with false 
recommendations section switch identities at zero-cost 
 section and spoof other users section we do not assume 
any centralized trust or centralized infrastructure 
 model 
to aid the development and study of the incentive schemes in this 
section we present a model of the users behaviors in particular 
we model the benefits and costs of p p interactions the game and 
population dynamics caused by mutation learning and turnover 
our model is designed to have the following properties that 
characterize a large set of p p systems 
 social dilemma universal cooperation should result in 
optimal overall utility but individuals who exploit the 
cooperation of others while not cooperating themselves i e 
defecting should benefit more than users who do cooperate 
 asymmetric transactions a peer may want service from 
another peer while not currently being able to provide the 
service that the second peer wants transactions should be 
able to have asymmetric payoffs 
 untraceable defections a peer should not be able to 
determine the identity of peers who have defected on her this 
models the difficulty or expense of determining that a peer 
could have provided a service but didn t for example in the 
gnutella file sharing system a peer may simply ignore 
queries despite possessing the desired file thus preventing 
the querying peer from identifying the defecting peer 
 dynamic population peers should be able to change their 
behavior and enter or leave the system independently and 
continuously 
 the exception is discussed in section 
 
cooperate 
defect 
cooperate defectclient 
server 
sc rr 
sc st sc pp 
sc ts 
figure payoff matrix for the generalized prisoner s dilemma t r 
p and s stand for temptation reward punishment and sucker 
respectively 
 generalized prisoner s dilemma 
the prisoner s dilemma developed by flood dresher and tucker 
in is a non-cooperative repeated game satisfying the 
social dilemma requirement each game consists of two players who 
can defect or cooperate depending how each acts the players 
receive a payoff the players use a strategy to decide how to act 
unfortunately existing work either uses a specific asymmetric payoff 
matrix or only gives the general form for a symmetric one 
instead we use the generalized prisoner s dilemma gpd which 
specifies the general form for an asymmetric payoff matrix that 
preserves the social dilemma in the gpd one player is the client and 
one player is the server in each game and it is only the decision 
of the server that is meaningful for determining the outome of the 
transaction a player can be a client in one game and a server in 
another the client and server receive the payoff from a generalized 
payoff matrix figure rc sc tc and pc are the client s payoff 
and rs ss ts and ps are the server s payoff a gpd payoff 
matrix must have the following properties to create a social dilemma 
 mutual cooperation leads to higher payoffs than mutual 
defection rs rc ps pc 
 mutual cooperation leads to higher payoffs than one player 
suckering the other rs rc sc ts and rs rc 
ss tc 
 defection dominates cooperation at least weakly at the 
individual level for the entity who decides whether to 
cooperate or defect ts ≥ rs and ps ≥ ss and ts rs or 
ps ss 
the last set of inequalities assume that clients do not incur a cost 
regardless of whether they cooperate or defect and therefore clients 
always cooperate these properties correspond to similar properties 
of the classic prisoner s dilemma and allow any form of 
asymmetric transaction while still creating a social dilemma 
furthermore one or more of the four possible actions client 
cooperate and defect and server cooperate and defect can be 
untraceable if one player makes an untraceable action the other player 
does not know the identity of the first player 
for example to model a p p application like file sharing or 
overlay routing we use the specific payoff matrix values shown in 
figure this satisfies the inequalities specified above where only the 
server can choose between cooperating and defecting in addition 
for this particular payoff matrix clients are unable to trace server 
defections this is the payoff matrix that we use in our simulation 
results 
request 
service 
don t request 
 - 
 
 
 
provide 
service 
ignore 
request 
client 
server 
figure the payoff matrix for an application like p p file sharing or 
overlay routing 
 population dynamics 
a characteristic of p p systems is that peers change their 
behavior and enter or leave the system independently and continuously 
several studies of repeated prisoner s dilemma games use 
an evolutionary model of population dynamics an 
evolutionary model is not suitable for p p systems because it only 
specifies the global behavior and all changes occur at discrete times 
for example it may specify that a population of 
cooperate players and defect players evolves into a population 
with and players respectively it does not specify which specific 
players switched furthermore all the switching occurs at the end 
of a generation instead of continuously like in a real p p system as 
a result evolutionary population dynamics do not accurately model 
turnover traitors and strangers 
in our model entities take independent and continuous actions that 
change the composition of the population time consists of rounds 
in each round every player plays one game as a client and one game 
as a server at the end of a round a player may mutate learn 
 turnover or stay the same if a player mutates she switches to 
a randomly picked strategy if she learns she switches to a strategy 
that she believes will produce a higher score described in more 
detail below if she maintains her identity after switching strategies 
then she is referred to as a traitor if a player suffers turnover she 
leaves the system and is replaced with a newcomer who uses the 
same strategy as the exiting player 
to learn a player collects local information about the performance 
of different strategies this information consists of both her 
personal observations of strategy performance and the observations of 
those players she interacts with this models users communicating 
out-of-band about how strategies perform let s be the running 
average of the performance of a player s current strategy per round 
and age be the number of rounds she has been using the strategy a 
strategy s rating is 
runningaverage s ∗ age 
runningaverage age 
 
we use the age and compute the running average before the ratio to 
prevent young samples which are more likely to be outliers from 
skewing the rating at the end of a round a player switches to 
highest rated strategy with a probability proportional to the difference 
in score between her current strategy and the highest rated strategy 
 
 reciprocative decision 
function 
in this section we present the new decision function reciprocative 
that is the basis for our incentive techniques a decision function 
maps from a history of a player s actions to a decision whether to 
cooperate with or defect on that player a strategy consists of a 
decision function private or shared history a server selection 
mechanism and a stranger policy our approach to incentives is to 
design strategies which maximize both individual and social benefit 
strategic users will choose to use such strategies and thereby drive 
the system to high levels of cooperation two examples of 
simple decision functions are cooperate and defect 
 cooperate models a naive user who does not yet realize 
that she is being exploited defect models a greedy user 
who is intent on exploiting the system in the absence of incentive 
techniques defect users will quickly dominate the 
cooperate users and destroy cooperation in the system 
our requirements for a decision function are that it can use 
shared and subjective history it can deal with untraceable 
defections and it is robust against different patterns of defection 
previous decision functions such as tit-for-tat and image 
 see section do not satisfy these criteria for example tit-for-tat 
and image base their decisions on both cooperations and defections 
therefore cannot deal with untraceable defections in this section 
and the remaining sections we demonstrate how the 
reciprocativebased strategies satisfy all of the requirements stated above 
the probability that a reciprocative player cooperates with a peer 
is a function of its normalized generosity generosity measures the 
benefit an entity has provided relative to the benefit it has 
consumed this is important because entities which consume more 
services than they provide even if they provide many services will 
cause cooperation to collapse for some entity i let pi and ci be the 
services i has provided and consumed respectively entity i s 
generosity is simply the ratio of the service it provides to the service it 
consumes 
g i pi ci 
one possibility is to cooperate with a probability equal to the 
generosity although this is effective in some cases in other cases a 
reciprocative player may consume more than she provides e g 
when initially using the stranger defect policy in this will 
cause reciprocative players to defect on each other to prevent this 
situation a reciprocative player uses its own generosity as a 
measuring stick to judge its peer s generosity normalized generosity 
measures entity i s generosity relative to entity j s generosity more 
concretely entity i s normalized generosity as perceived by entity 
j is 
gj i g i g j 
in the remainder of this section we describe our simulation 
framework and use it to demonstrate the benefits of the baseline 
reciprocative decision function 
parameter nominal value section 
population size 
run time rounds 
payoff matrix file sharing 
ratio using cooperate 
ratio using defect 
ratio using reciprocative 
mutation probability 
learning probability 
turnover probability 
hit rate 
table default simulation parameters 
 simulation framework 
our simulator implements the model described in section we use 
the asymmetric file sharing payoff matrix figure with 
untraceable defections because it models transactions in many p p 
systems like file-sharing and packet forwarding in ad-hoc and overlay 
networks our simulation study is composed of different scenarios 
reflecting the challenges of various non-cooperative behaviors 
table presents the nominal parameter values used in our simulation 
the ratio using rows refer to the initial ratio of the total 
population using a particular strategy in each scenario we vary the value 
range of a specific parameter to reflect a particular situation or 
attack we then vary the exact properties of the reciprocative strategy 
to defend against that situation or attack 
 baseline results 
 
 
 
 
 
 
 
 
 
 
 
 
 
population 
time 
 a total population 
 
 
 
 
 
 
 
 
 
 
 
 
 
time 
 b total population 
defector 
cooperator 
recip private 
figure the evolution of strategy populations over time time the 
number of elapsed rounds population is the number of players using 
a strategy 
in this section we present the dynamics of the game for the 
basic scenario presented in table to familiarize the reader and set 
a baseline for more complicated scenarios figures a 
players and b players show players switching to higher 
scoring strategies over time in two separate runs of the simulator each 
point in the graph represents the number of players using a 
particular strategy at one point in time figures a and b show the 
corresponding mean overall score per round this measures the degree 
of cooperation in the system is the maximum possible achieved 
when everybody cooperates and is the minimum achieved when 
everybody defects from the file sharing payoff matrix a net of 
means everyone is able to download a file and a means that no one 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
meanoverallscore round 
time 
 a total population 
 
 
 
 
 
 
 
 
 
 
 
 
 
time 
 b total population 
figure the mean overall per round score over time 
is able to do so we use this metric in all later results to evaluate our 
incentive techniques 
figure a shows that the reciprocative strategy using private 
history causes a system of players to converge to a cooperation 
level of but drops to for players one would expect the 
 player system to reach the optimal level of cooperation 
because all the defectors are eliminated from the system it does not 
because of asymmetry of interest for example suppose player b 
is using reciprocative with private history player a may happen to 
ask for service from player b twice in succession without 
providing service to player b in the interim player b does not know of the 
service player a has provided to others so player b will reject 
service to player a even though player a is cooperative we discuss 
solutions to asymmetry of interest and the failure of reciprocative 
in the player system in section 
 reciprocative-based incentive 
techniques 
in this section we present our incentives techniques and evaluate 
their behavior by simulation to make the exposition clear we group 
our techniques by the challenges they address large populations 
and high turnover section collusions section zero-cost 
identities section and traitors section 
 large populations and high turnover 
the large populations and high turnover of p p systems makes it 
less likely that repeat interactions will occur with a familiar entity 
under these conditions basing decisions only on private history 
 records about interactions the peer has been directly involved in 
is not effective in addition private history does not deal well with 
asymmetry of interest for example if player b has cooperated with 
others but not with player a himself in the past player a has no 
indication of player b s generosity thus may unduly defect on him 
we propose two mechanisms to alleviate the problem of few repeat 
transactions server-selection and shared history 
 server selection 
a natural way to increase the probability of interacting with familiar 
peers is by discriminating server selection however the 
asymmetry of transactions challenges selection mechanisms unlike in the 
prisoner s dilemma payoff matrix where players can benefit one 
another within a single transaction transactions in gpd are 
asymmetric as a result a player who selects her donor for the second 
time without contributing to her in the interim may face a defection 
in addition due to untraceability of defections it is impossible to 
maintain blacklists to avoid interactions with known defectors 
in order to deal with asymmetric transactions every player holds 
 fixed size lists of both past donors and past recipients and selects 
a server from one of these lists at random with equal 
probabilities this way users approach their past recipients and give them a 
chance to reciprocate 
in scenarios with selective users we omit the complete availability 
assumption to prevent players from being clustered into a lot of 
very small groups thus we assume that every player can perform 
the requested service with probability p for the results presented in 
this section p in addition in order to avoid bias in favor of 
the selective players all players including the non-discriminative 
ones select servers for games 
figure demonstrates the effectiveness of the proposed selection 
mechanism in scenarios with large population sizes we fix the 
initial ratio of reciprocative in the population while varying 
the population size between to notice that while in 
figures a and b the data points demonstrates the evolution of the 
system over time each data point in this figure is the result of an 
entire simulation for a specific scenario the figure shows that the 
reciprocative decision function using private history in conjunction 
with selective behavior can scale to large populations 
in figure we fix the population size and vary the turnover rate 
it demonstrates that while selective behavior is effective for low 
turnover rates as turnover gets higher selective behavior does not 
scale this occurs because selection is only effective as long as 
players from the past stay alive for long enough such that they can 
be selected for future games 
 shared history 
in order to mitigate asymmetry of interest and scale to higher 
turnover rate there is a need in shared history shared history means 
that every peer keeps records about all of the interactions that 
occur in the system regardless of whether he was directly involved 
in them or not it allows players to leverage off of the experiences 
of others in cases of few repeat transactions it only requires that 
someone has interacted with a particular player for the entire 
population to observe it thus scales better to large populations and high 
turnovers and also tolerates asymmetry of interest some examples 
of shared history schemes are 
figure shows the effectiveness of shared history under high 
turnover rates in this figure we fix the population size and vary the 
turnover rate while selective players with private history can only 
tolerate a moderate turnover shared history scales to turnovers of 
up to approximately this means that of the players leave 
the system at the end of each round in figure we fix the turnover 
and vary the population size it shows that shared history causes 
the system to converge to optimal cooperation and performance 
regardless of the size of the population 
these results show that shared history addresses all three 
challenges of large populations high turnover and asymmetry of 
transactions nevertheless shared history has two disadvantages first 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
meanoverallscore round 
numplayers 
shared non-sel 
private non-sel 
private selective 
figure private vs shared history as a function of population size 
 
 
 
 
 
 
 
 
meanoverallscore round 
turnover 
shared non-sel 
private non-sel 
private selective 
figure performance of selection mechanism under turnover the 
x-axis is the turnover rate the y-axis is the mean overall per round 
score 
while a decentralized implementation of private history is 
straightforward implementation of shared-history requires communication 
overhead or centralization a decentralized shared history can be 
implemented for example on top of a dht using a peer-to-peer 
storage system or by disseminating information to other 
entities in a similar way to routing protocols second and more 
fundamental shared history is vulnerable to collusion in the next section 
we propose a mechanism that addresses this problem 
 collusion and other shared history 
attacks 
 collusion 
while shared history is scalable it is vulnerable to collusion 
collusion can be either positive e g defecting entities claim that other 
defecting entities cooperated with them or negative e g entities 
claim that other cooperative entities defected on them collusion 
subverts any strategy in which everyone in the system agrees on the 
reputation of a player objective reputation an example of 
objective reputation is to use the reciprocative decision function with 
shared history to count the total number of cooperations a player 
has given to and received from all entities in the system another 
example is the image strategy the effect of collusion is 
magnified in systems with zero-cost identities where users can create 
fake identities that report false statements 
instead to deal with collusion entities can compute reputation 
subjectively where player a weighs player b s opinions based on how 
much player a trusts player b our subjective algorithm is based 
on maxflow maxflow is a graph theoretic problem which 
given a directed graph with weighted edges asks what is the greatest 
rate at which material can be shipped from the source to the target 
without violating any capacity constraints for example in figure 
each edge is labeled with the amount of traffic that can travel on it 
the maxflow algorithm computes the maximum amount of traffic 
that can go from the source s to the target t without violating the 
constraints in this example even though there is a loop of high 
capacity edges the maxflow between the source and the target is only 
 the numbers in brackets represent the actual flow on each edge 
in the solution 
 
 
 
s t 
 
 
 
 
 
figure each edge in the graph is labeled with its capacity and the 
actual flow it carries in brackets the maxflow between the source and 
the target in the graph is 
c 
c 
cccc 
 
 
 
 
 
 
 
 
 
a 
b 
figure this graph illustrates the robustness of maxflow in the 
presence of colluders who report bogus high reputation values 
we apply the maxflow algorithm by constructing a graph whose 
vertices are entities and the edges are the services that entities have 
received from each other this information can be stored using the 
same methods as the shared history a maxflow is the greatest level 
of reputation the source can give to the sink without violating 
reputation capacity constraints as a result nodes who dishonestly 
report high reputation values will not be able to subvert the 
reputation system 
figure illustrates a scenario in which all the colluders labeled 
with c report high reputation values for each other when node a 
computes the subjective reputation of b using the maxflow 
algorithm it will not be affected by the local false reputation values 
rather the maxflow in this case will be this is because no service 
has been received from any of the colluders 
 
in our algorithm the benefit that entity i has received indirectly 
from entity j is the maxflow from j to i conversely the benefit that 
entity i has provided indirectly to j is the maxflow from i to j the 
subjective reputation of entity j as perceived by i is 
min 
maxflow j to i 
maxflow i to j 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
meanoverallscore round 
population 
shared 
private 
subjective 
figure subjective shared history compared to objective shared 
history and private history in the presence of colluders 
algorithm constanttimemaxflow bound the mean running time 
of maxflow to a constant 
method ctmaxflow self src dst 
 self surplus ← self surplus self increment 
{use the running mean as a prediction } 
 if random ∗self surplus self mean iterations then 
 return none {not enough surplus to run } 
 end if 
{get the flow and number of iterations used from the maxflow alg } 
 flow iterations ← maxflow self g src dst 
 self surplus ← self surplus − iterations 
{keep a running mean of the number of iterations used } 
 self mean iterations ← self α ∗ self mean iterations − 
self α ∗ iterations 
 return flow 
the cost of maxflow is its long running time the standard 
preflowpush maxflow algorithm has a worst case running time of o v 
 
instead we use algorithm which has a constant mean running 
time but sometimes returns no flow even though one exists the 
essential idea is to bound the mean number of nodes examined 
during the maxflow computation this bounds the overhead but also 
bounds the effectiveness despite this the results below show that 
a maxflow-based reciprocative decision function scales to higher 
populations than one using private history 
figure compares the effectiveness of subjective reputation to 
objective reputation in the presence of colluders in these 
scenarios defectors collude by claiming that other colluders that they 
encounter gave them cooperations for that encounter also the 
parameters for algorithm are set as follows increment 
α 
as in previous sections reciprocative with private history results 
in cooperation up to a point beyond which it fails the difference 
here is that objective shared history fails for all population sizes 
this is because the reciprocative players cooperate with the 
colluders because of their high reputations however subjective 
history can reach high levels of cooperation regardless of colluders 
this is because there are no high weight paths in the cooperation 
graph from colluders to any non-colluders so the maxflow from 
a colluder to any non-colluder is therefore a subjective 
reciprocative player will conclude that that colluder has not provided 
any service to her and will reject service to the colluder thus the 
maxflow algorithm enables reciprocative to maintain the 
scalability of shared history without being vulnerable to collusion or 
requiring centralized trust e g trusted peers since we bound the 
running time of the maxflow algorithm cooperation decreases as 
the population size increases but the key point is that the subjective 
reciprocative decision function scales to higher populations than 
one using private history this advantage only increases over time 
as cpu power increases and more cycles can be devoted to running 
the maxflow algorithm by increasing the increment parameter 
despite the robustness of the maxflow algorithm to the simple form 
of collusion described previously it still has vulnerabilities to more 
sophisticated attacks one is for an entity the mole to provide 
service and then lie positively about other colluders the other 
colluders can then exploit their reputation to receive service however 
the effectiveness of this attack relies on the amount of service that 
the mole provides since the mole is paying all of the cost of 
providing service and receiving none of the benefit she has a strong 
incentive to stop colluding and try another strategy this forces the 
colluders to use mechanisms to maintain cooperation within their 
group which may drive the cost of collusion to exceed the benefit 
 false reports 
another attack is for a defector to lie about receiving or providing 
service to another entity there are four possibile actions that can be 
lied about providing service not providing service receiving 
service and not receiving service falsely claiming to receive service 
is the simple collusion attack described above falsely claiming not 
to have provided service provides no benefit to the attacker 
falsely claiming to have provided service or not to have received 
it allows an attacker to boost her own reputation and or lower the 
reputation of another entity an entity may want to lower another 
entity s reputation in order to discourage others from selecting it 
and exclusively use its service these false claims are clearly 
identifiable in the shared history as inconsistencies where one entity 
claims a transaction occurred and another claims it did not to limit 
this attack we modify the maxflow algorithm so that an entity 
always believes the entity that is closer to him in the flow graph if 
both entities are equally distant then the disputed edge in the flow is 
not critical to the evaluation and is ignored this modification 
prevents those cases where the attacker is making false claims about an 
entity that is closer than her to the evaluating entity which prevents 
her from boosting her own reputation the remaining possibilities 
are for the attacker to falsely claim to have provided service to or 
not to have received it from a victim entity that is farther from the 
evalulator than her in these cases an attacker can only lower the 
reputation of the victim the effectiveness of doing this is limited 
by the number of services provided and received by the attacker 
which makes executing this attack expensive 
 
 zero-cost identities 
history assumes that entities maintain persistent identities 
however in most p p systems identities are zero-cost this is desirable 
for network growth as it encourages newcomers to join the system 
however this also allows misbehaving users to escape the 
consequences of their actions by switching to new identities i e 
whitewashing whitewashers can cause the system to collapse if they 
are not punished appropriately unfortunately a player cannot tell 
if a stranger is a whitewasher or a legitimate newcomer always 
cooperating with strangers encourages newcomers to join but at the 
same time encourages whitewashing behavior always defecting on 
strangers prevents whitewashing but discourages newcomers from 
joining and may also initiate unfavorable cycles of defection 
this tension suggests that any stranger policy that has a fixed 
probability of cooperating with strangers will fail by either being too 
stingy when most strangers are newcomers or too generous when 
most strangers are whitewashers our solution is the stranger 
adaptive stranger policy the idea is to be generous to strangers 
when they are being generous and stingy when they are stingy 
let ps and cs be the number of services that strangers have 
provided and consumed respectively the probability that a player 
using stranger adaptive helps a stranger is ps cs however we do 
not wish to keep these counts permanently for reasons described in 
section also players may not know when strangers defect 
because defections are untraceable as described in section 
consequently instead of keeping ps and cs we assume that k ps cs 
where k is a constant and we keep the running ratio r ps cs 
when we need to increment ps or cs we generate the current 
values of ps and cs from k and r 
cs k r 
ps cs ∗ r 
we then compute the new r as follows 
r ps cs if the stranger provided service 
r ps cs if the stranger consumed service 
this method allows us to keep a running ratio that reflects the 
recent generosity of strangers without knowing when strangers have 
defected 
 
 
 
 
 
 
 
 
meanoverallscore round 
turnover 
stranger cooperate 
stranger defect 
stranger adaptive 
figure different stranger policies for reciprocative with shared 
history the x-axis is the turnover rate on a log scale the y-axis is the 
mean overall per round score 
figures and compare the effectiveness of the 
reciprocative strategy using different policies toward strangers figure 
 
 
 
 
 
 
 
 
meanoverallscore round 
turnover 
stranger cooperate 
stranger defect 
stranger adaptive 
figure different stranger policies for reciprocative with private 
history the x-axis is the turnover rate on a log scale the y-axis is the 
mean overall per round score 
compares different stranger policies for reciprocative with shared 
history while figure is with private history in both figures 
the players using the defect strategy change their 
identity whitewash after every transaction and are indistinguishable 
from legitimate newcomers the reciprocative players using the 
stranger cooperate policy completely fail to achieve cooperation 
this stranger policy allows whitewashers to maximize their payoff 
and consequently provides a high incentive for users to switch to 
whitewashing 
in contrast figure shows that the stranger defect policy is 
effective with shared history this is because whitewashers always 
appear to be strangers and therefore the reciprocative players will 
always defect on them this is consistent with previous work 
showing that punishing strangers deals with whitewashers 
however figure shows that stranger defect is not effective with 
private history this is because reciprocative requires some initial 
cooperation to bootstrap in the shared history case a reciprocative 
player can observe that another player has already cooperated with 
others with private history the reciprocative player only knows 
about the other players actions toward her therefore the initial 
defection dictated by the stranger defect policy will lead to later 
defections which will prevent reciprocative players from ever 
cooperating with each other in other simulations not shown here 
the stranger defect stranger policy fails even with shared history 
when there are no initial cooperate players 
figure shows that with shared history the stranger 
adaptive policy performs as well as stranger defect policy until the 
turnover rate is very high of the population turning over after 
every transaction in these scenarios stranger adaptive is using 
k and each player keeps a private r more importantly it is 
significantly better than stranger defect policy with private 
history because it can bootstrap cooperation although the stranger 
defect policy is marginally more effective than stranger 
adaptive at very high rates of turnover p p systems are unlikely to 
operate there because other services e g routing also cannot tolerate 
very high turnover 
we conclude that of the stranger policies that we have explored 
stranger adaptive is the most effective by using stranger 
adaptive p p systems with zero-cost identities and a sufficiently 
low turnover can sustain cooperation without a centralized 
allocation of identities 
 
 traitors 
traitors are players who acquire high reputation scores by 
cooperating for a while and then traitorously turn into defectors before 
leaving the system they model both users who turn deliberately 
to gain a higher score and cooperators whose identities have been 
stolen and exploited by defectors a strategy that maintains 
longterm history without discriminating between old and recent actions 
becomes highly vulnerable to exploitation by these traitors 
the top two graphs in figure demonstrate the effect of traitors 
on cooperation in a system where players keep long-term history 
 never clear history in these simulations we run for rounds 
and allow cooperative players to keep their identities when 
switching to the defector strategy we use the default values for the 
other parameters without traitors the cooperative strategies thrive 
with traitors the cooperative strategies thrive until a cooperator 
turns traitor after rounds as this cooperator exploits her 
reputation to achieve a high score other cooperative players notice this 
and follow suit via learning cooperation eventually collapses on 
the other hand if we maintain short-term history and or 
discounting ancient history vis-a-vis recent history traitors can be quickly 
detected and the overall cooperation level stays high as shown in 
the bottom two graphs in figure 
 
 
 
 
 
 
 k k 
long-termhistory 
no traitors 
population 
 
 
 
 
 
 
 k k 
traitors 
defector 
cooperator 
recip shared 
 
 
 
 
 
 
 k k 
short-termhistory 
time 
population 
 
 
 
 
 
 
 k k 
time 
figure keeping long-term vs short-term history both with and 
without traitors 
 related work 
previous work has examined the incentive problem as applied to 
societies in general and more recently to internet applications and 
peer-to-peer systems in particular a well-known phenomenon in 
this context is the tragedy of the commons where resources 
are under-provisioned due to selfish users who free-ride on the 
system s resources and is especially common in large networks 
 
the problem has been extensively studied adopting a game 
theoretic approach the prisoners dilemma model provides a 
natural framework to study the effectiveness of different strategies in 
establishing cooperation among players in a simulation 
environment with many repeated games persistent identities and no 
collusion axelrod shows that the tit-for-tat strategy dominates 
our model assumes growth follows local learning rather than 
evolutionary dynamics and also allows for more kinds of attacks 
nowak and sigmund introduce the image strategy and 
demonstrate its ability to establish cooperation among players despite few 
repeat transactions by the employment of shared history players 
using image cooperate with players whose global count of 
cooperations minus defections exceeds some threshold as a result an 
image player is either vulnerable to partial defectors if the 
threshold is set too low or does not cooperate with other image players 
 if the threshold is set too high 
in recent years researchers have used economic mechanism 
design theory to tackle the cooperation problem in internet 
applications mechanism design is the inverse of game theory it asks how 
to design a game in which the behavior of strategic players results 
in the socially desired outcome distributed algorithmic 
mechanism design seeks solutions within this framework that are both 
fully distributed and computationally tractable and 
are examples of applying damd to bgp routing and multicast cost 
sharing more recently damd has been also studied in dynamic 
environments in this context demonstrating the superiority of 
a cooperative strategy as in the case of our work is consistent with 
the objective of incentivizing the desired behavior among selfish 
players 
the unique challenges imposed by peer-to-peer systems inspired 
additional body of work mainly in the context of packet 
forwarding in wireless ad-hoc routing and file 
sharing friedman and resnick consider the 
problem of zero-cost identities in online environments and find that in 
such systems punishing all newcomers is inevitable using a 
theoretical model they demonstrate that such a system can converge to 
cooperation only for sufficiently low turnover rates which our 
results confirm and show that whitewashing and collusion can 
have dire consequences for peer-to-peer systems and are difficult to 
prevent in a fully decentralized system 
some commercial file sharing clients provide incentive 
mechanisms which are enforced by making it difficult for the user 
to modify the source code these mechanisms can be circumvented 
by a skilled user or by a competing company releasing a compatible 
client without the incentive restrictions also these mechanisms are 
still vulnerable to zero-cost identities and collusion bittorrent 
uses tit-for-tat as a method for resource allocation where a user s 
upload rate dictates his download rate 
 conclusions 
in this paper we take a game theoretic approach to the 
problem of cooperation in peer-to-peer networks addressing the 
challenges imposed by p p systems including large populations high 
turnover asymmetry of interest and zero-cost identities we propose 
a family of scalable and robust incentive techniques based upon 
the reciprocative decision function to support cooperative 
behavior and improve overall system performance 
we find that the adoption of shared history and discriminating 
server selection techniques can mitigate the challenge of few repeat 
transactions that arises due to large population size high turnover 
and asymmetry of interest furthermore cooperation can be 
established even in the presence of zero-cost identities through the use of 
an adaptive policy towards strangers finally colluders and traitors 
can be kept in check via subjective reputations and short-term 
history respectively 
 
 acknowledgments 
we thank mary baker t j giuli petros maniatis the 
anonymous reviewer and our shepherd margo seltzer for their useful 
comments that helped improve the paper this work is supported 
in part by the national science foundation under itr awards 
ani- and ani- and career award ani- 
views and conclusions contained in this document are those of the 
authors and should not be interpreted as representing the official 
policies either expressed or implied of nsf or the u s 
government 
 references 
 kazaa http www kazaa com 
 limewire http www limewire com 
 adar e and huberman b a free riding on gnutella first 
monday october 
 axelrod r the evolution of cooperation basic books 
 buragohain c agrawal d and suri s a 
game-theoretic framework for incentives in p p systems in 
international conference on peer-to-peer computing sep 
 castro m druschel p ganesh a rowstron a and 
wallach d s security for structured peer-to-peer overlay 
networks in proceedings of multimedia computing and networking 
 mmcn 
 cohen b incentives build robustness in bittorrent in st workshop 
on economics of peer-to-peer systems 
 crowcroft j gibbens r kelly f and ˘ ostring s 
modeling incentives for collaboration in mobile ad-hoc networks 
in modeling and optimization in mobile ad-hoc and wireless 
networks 
 douceur j r the sybil attack in electronic proceedings of the 
international workshop on peer-to-peer systems 
 feigenbaum j papadimitriou c sami r and shenker 
s a bgp-based mechanism for lowest-cost routing in 
proceedings of the acm symposium on principles of distributed 
computing 
 feigenbaum j papadimitriou c and shenker s sharing 
the cost of multicast transmissions in journal of computer and 
system sciences vol pp - 
 feigenbaum j and shenker s distributed algorithmic 
mechanism design recent results and future directions in 
proceedings of the international workshop on discrete algorithms 
and methods for mobile computing and communications 
 friedman e and resnick p the social cost of cheap 
pseudonyms journal of economics and management strategy 
 - 
 fudenberg d and levine d k the theory of learning in 
games the mit press 
 golle p leyton-brown k mironov i and 
lillibridge m incentives for sharing in peer-to-peer networks 
in proceedings of the rd acm conference on electronic commerce 
october 
 gross b and acquisti a balances of power on ebay peers 
or unquals in workshop on economics of peer-to-peer networks 
 
 gu b and jarvenpaa s are contributions to p p technical 
forums private or public goods - an empirical investigation in st 
workshop on economics of peer-to-peer systems 
 hardin g the tragedy of the commons science 
 - 
 josef hofbauer and karl sigmund evolutionary games and 
population dynamics cambridge university press 
 kamvar s d schlosser m t and garcia-molina h 
the eigentrust algorithm for reputation management in p p 
networks in proceedings of the twelfth international world wide 
web conference may 
 kan g peer-to-peer harnessing the power of disruptive 
technologies st ed o reilly associates inc march 
ch gnutella pp - 
 kuhn s prisoner s dilemma in the stanford encyclopedia of 
philosophy edward n zalta ed summer ed 
 lee s sherwood r and bhattacharjee b cooperative 
peer groups in nice in proceedings of the ieee infocom 
 levien r and aiken a attack-resistant trust metrics for 
public key certification in proceedings of the usenix security 
symposium pp - 
 maniatis p roussopoulos m giuli t j rosenthal 
d s h baker m and muliadi y preserving peer replicas 
by rate-limited sampled voting in acm symposium on operating 
systems principles 
 marti s giuli t j lai k and baker m mitigating 
routing misbehavior in mobile ad-hoc networks in proceedings of 
mobicom pp - 
 michiardi p and molva r a game theoretical approach to 
evaluate cooperation enforcement mechanisms in mobile ad-hoc 
networks in modeling and optimization in mobile ad-hoc and 
wireless networks 
 nowak m a and sigmund k evolution of indirect 
reciprocity by image scoring nature - 
 olson m the logic of collective action public goods and the 
theory of groups harvard university press 
 raghavan b and snoeren a priority forwarding in ad-hoc 
networks with self-ineterested parties in workshop on economics of 
peer-to-peer systems june 
 ranganathan k ripeanu m sarin a and foster i 
to share or not to share an analysis of incentives to contribute in 
collaborative file sharing environments in workshop on economics 
of peer-to-peer systems june 
 reiter m k and stubblebine s g authentication metric 
analysis and design acm transactions on information and system 
security - 
 saroiu s gummadi p k and gribble s d a 
measurement study of peer-to-peer file sharing systems in 
proceedings of multimedia computing and networking 
 mmcn 
 smith j m evolution and the theory of games cambridge 
university press 
 urpi a bonuccelli m and giordano s modeling 
cooperation in mobile ad-hoc networks a formal description of 
selfishness in modeling and optimization in mobile ad-hoc and 
wireless networks 
 vishnumurthy v chandrakumar s and sirer e g 
karma a secure economic framework for p p resource 
sharing in workshop on economics of peer-to-peer networks 
 wang w and li b to play or to control a game-based 
control-theoretic approach to peer-to-peer incentive engineering 
in international workshop on quality of service june 
 woodard c j and parkes d c strategyproof mechanisms 
for ad-hoc network formation in workshop on economics of 
peer-to-peer systems june 
 
self-interested automated mechanism design and 
implications for optimal combinatorial auctions∗ 
vincent conitzer 
carnegie mellon university 
 forbes avenue 
pittsburgh pa usa 
conitzer cs cmu edu 
tuomas sandholm 
carnegie mellon university 
 forbes avenue 
pittsburgh pa usa 
sandholm cs cmu edu 
abstract 
often an outcome must be chosen on the basis of the 
preferences reported by a group of agents the key difficulty 
is that the agents may report their preferences insincerely 
to make the chosen outcome more favorable to themselves 
mechanism design is the art of designing the rules of the 
game so that the agents are motivated to report their 
preferences truthfully and a desirable outcome is chosen in a 
recently proposed approach-called automated mechanism 
design-a mechanism is computed for the preference 
aggregation setting at hand this has several advantages but the 
downside is that the mechanism design optimization 
problem needs to be solved anew each time unlike the earlier 
work on automated mechanism design that studied a 
benevolent designer in this paper we study automated mechanism 
design problems where the designer is self-interested in this 
case the center cares only about which outcome is chosen 
and what payments are made to it the reason that the 
agents preferences are relevant is that the center is 
constrained to making each agent at least as well off as the agent 
would have been had it not participated in the mechanism 
in this setting we show that designing optimal deterministic 
mechanisms is np-complete in two important special cases 
when the center is interested only in the payments made 
to it and when payments are not possible and the center 
is interested only in the outcome chosen we then show 
how allowing for randomization in the mechanism makes 
problems in this setting computationally easy finally we 
show that the payment-maximizing amd problem is closely 
related to an interesting variant of the optimal 
 revenuemaximizing combinatorial auction design problem where 
the bidders have best-only preferences we show that 
here too designing an optimal deterministic auction is 
npcomplete but designing an optimal randomized auction is 
easy 
categories and subject descriptors 
f theory of computation analysis of algorithms 
and problem complexity j computer applications 
social and behavioral sciences-economics 
general terms 
algorithms economics theory 
 introduction 
in multiagent settings often an outcome must be 
chosen on the basis of the preferences reported by a group of 
agents such outcomes could be potential presidents joint 
plans allocations of goods or resources etc the preference 
aggregator generally does not know the agents preferences 
a priori rather the agents report their preferences to the 
coordinator unfortunately an agent may have an incentive 
to misreport its preferences in order to mislead the 
mechanism into selecting an outcome that is more desirable to 
the agent than the outcome that would be selected if the 
agent revealed its preferences truthfully such manipulation 
is undesirable because preference aggregation mechanisms 
are tailored to aggregate preferences in a socially desirable 
way and if the agents reveal their preferences insincerely a 
socially undesirable outcome may be chosen 
manipulability is a pervasive problem across preference 
aggregation mechanisms a seminal negative result the 
gibbard-satterthwaite theorem shows that under any 
nondictatorial preference aggregation scheme if there are at 
least possible outcomes there are preferences under which 
an agent is better off reporting untruthfully a 
preference aggregation scheme is called dictatorial if one of 
the agents dictates the outcome no matter what preferences 
the other agents report 
what the aggregator would like to do is design a 
preference aggregation mechanism so that the self-interested 
agents are motivated to report their preferences truthfully 
and the mechanism chooses an outcome that is desirable 
from the perspective of some objective this is the classic 
setting of mechanism design in game theory in this paper 
we study the case where the designer is self-interested that 
is the designer does not directly care about how the 
out 
come relates to the agents preferences but is rather 
concerned with its own agenda for which outcome should be 
chosen and with maximizing payments to itself this is the 
mechanism design setting most relevant to electronic 
commerce 
in the case where the mechanism designer is interested in 
maximizing some notion of social welfare the importance 
of collecting the agents preferences is clear it is perhaps 
less obvious why they should be collected when the designer 
is self-interested and hence its objective is not directly 
related to the agents preferences the reason for this is that 
often the agents preferences impose limits on how the 
designer chooses the outcome and payments the most 
common such constraint is that of individual rationality ir 
which means that the mechanism cannot make any agent 
worse off than the agent would have been had it not 
participated in the mechanism for instance in the setting 
of optimal auction design the designer auctioneer is only 
concerned with how much revenue is collected and not per 
se with how well the allocation of the good or goods 
corresponds to the agents preferences nevertheless the 
designer cannot force an agent to pay more than its valuation 
for the bundle of goods allocated to it therefore even a 
self-interested designer will choose an outcome that makes 
the agents reasonably well off on the other hand the 
designer will not necessarily choose a social welfare 
maximizing outcome for example if the designer always chooses 
an outcome that maximizes social welfare with respect to 
the reported preferences and forces each agent to pay the 
difference between the utility it has now and the utility it 
would have had if it had not participated in the mechanism 
it is easy to see that agents may have an incentive to 
misreport their preferences-and this may actually lead to less 
revenue being collected indeed one of the counterintuitive 
results of optimal auction design theory is that sometimes 
the good is allocated to nobody even when the auctioneer 
has a reservation price of 
classical mechanism design provides some general 
mechanisms which under certain assumptions satisfy some 
notion of nonmanipulability and maximize some objective the 
upside of these mechanisms is that they do not rely on 
 even probabilistic information about the agents 
preferences e g the vickrey-clarke-groves vcg mechanism 
 or they can be easily applied to any probability 
distribution over the preferences e g the dagva 
mechanism the myerson auction and the maskin-riley 
multi-unit auction however the general mechanisms 
also have significant downsides 
 the most famous and most broadly applicable general 
mechanisms vcg and dagva only maximize social 
welfare if the designer is self-interested as is the case 
in many electronic commerce settings these 
mechanisms do not maximize the designer s objective 
 the general mechanisms that do focus on a 
selfinterested designer are only applicable in very restricted 
settings-such as myerson s expected revenue 
maximizing auction for selling a single item and maskin 
and riley s expected revenue maximizing auction for 
selling multiple identical units of an item 
 even in the restricted settings in which these 
mechanisms apply the mechanisms only allow for payment 
maximization in practice the designer may also be 
interested in the outcome per se for example an 
auctioneer may care which bidder receives the item 
 it is often assumed that side payments can be used 
to tailor the agents incentives but this is not always 
practical for example in barter-based electronic 
marketplaces-such as recipco firstbarter com 
barterone and intagio-side payments are not 
allowed furthermore among software agents it might 
be more desirable to construct mechanisms that do not 
rely on the ability to make payments because many 
software agents do not have the infrastructure to make 
payments 
in contrast we follow a recent approach where the 
mechanism is designed automatically for the specific problem at 
hand this approach addresses all of the downsides listed 
above we formulate the mechanism design problem as an 
optimization problem the input is characterized by the 
number of agents the agents possible types preferences 
and the aggregator s prior distributions over the agents 
types the output is a nonmanipulable mechanism that is 
optimal with respect to some objective this approach is 
called automated mechanism design 
the automated mechanism design approach has four 
advantages over the classical approach of designing general 
mechanisms first it can be used even in settings that 
do not satisfy the assumptions of the classical mechanisms 
 such as availability of side payments or that the objective 
is social welfare second it may allow one to circumvent 
impossibility results such as the gibbard-satterthwaite 
theorem which state that there is no mechanism that is 
desirable across all preferences when the mechanism is designed 
for the setting at hand it does not matter that it would 
not work more generally third it may yield better 
mechanisms in terms of stronger nonmanipulability guarantees 
and or better outcomes than classical mechanisms because 
the mechanism capitalizes on the particulars of the setting 
 the probabilistic information that the designer has about 
the agents types given the vast amount of information 
that parties have about each other today this approach is 
likely to lead to tremendous savings over classical 
mechanisms which largely ignore that information for example 
imagine a company automatically creating its procurement 
mechanism based on statistical knowledge about its 
suppliers rather than using a classical descending procurement 
auction fourth the burden of design is shifted from 
humans to a machine 
however automated mechanism design requires the 
mechanism design optimization problem to be solved anew for 
each setting hence its computational complexity becomes 
a key issue previous research has studied this question for 
benevolent designers-that wish to maximize for example 
social welfare in this paper we study the 
computational complexity of automated mechanism design in the 
case of a self-interested designer this is an important 
setting for automated mechanism design due to the shortage 
of general mechanisms in this area and the fact that in 
most e-commerce settings the designer is self-interested we 
also show that this problem is closely related to a particular 
optimal revenue-maximizing combinatorial auction design 
problem 
 
the rest of this paper is organized as follows in 
section we justify the focus on nonmanipulable mechanisms 
in section we define the problem we study in section 
we show that designing an optimal deterministic mechanism 
is np-complete even when the designer only cares about the 
payments made to it in section we show that designing 
an optimal deterministic mechanism is also np-complete 
when payments are not possible and the designer is only 
interested in the outcome chosen in section we show that 
an optimal randomized mechanism can be designed in 
polynomial time even in the general case finally in section 
we show that for designing optimal combinatorial auctions 
under best-only preferences our results on amd imply that 
this problem is np-complete for deterministic auctions but 
easy for randomized auctions 
 justifying the focus on 
nonmanipulable mechanisms 
before we define the computational problem of automated 
mechanism design we should justify our focus on 
nonmanipulable mechanisms after all it is not immediately 
obvious that there are no manipulable mechanisms that even 
when agents report their types strategically and hence 
sometimes untruthfully still reach better outcomes according to 
whatever objective we use than any nonmanipulable 
mechanism this does however turn out to be the case given 
any mechanism we can construct a nonmanipulable 
mechanism whose performance is identical as follows we build 
an interface layer between the agents and the original 
mechanism the agents report their preferences or types to 
the interface layer subsequently the interface layer inputs 
into the original mechanism the types that the agents would 
have strategically reported to the original mechanism if their 
types were as declared to the interface layer the resulting 
outcome is the outcome of the new mechanism since the 
interface layer acts strategically on each agent s behalf 
there is never an incentive to report falsely to the interface 
layer and hence the types reported by the interface layer 
are the strategic types that would have been reported 
without the interface layer so the results are exactly as they 
would have been with the original mechanism this 
argument is known in the mechanism design literature as the 
revelation principle there are computational difficulties 
with applying the revelation principle in large combinatorial 
outcome and type spaces however because here we 
focus on flatly represented outcome and type spaces this is 
not a concern here given this we can focus on truthful 
mechanisms in the rest of the paper 
 definitions 
we now formalize the automated mechanism design 
setting 
definition in an automated mechanism design 
setting we are given 
 a finite set of outcomes o 
 a finite set of n agents 
 for each agent i 
 a finite set of types θi 
 a probability distribution γi over θi in the case of 
correlated types there is a single joint distribution 
γ over θ × × θn and 
 a utility function ui θi × o → r 
 an objective function whose expectation the designer 
wishes to maximize 
there are many possible objective functions the designer 
might have for example social welfare where the designer 
seeks to maximize the sum of the agents utilities or the 
minimum utility of any agent where the designer seeks to 
maximize the worst utility had by any agent in both 
of these cases the designer is benevolent because the 
designer in some sense is pursuing the agents collective 
happiness however in this paper we focus on the case of 
a self-interested designer a self-interested designer cares 
only about the outcome chosen that is the designer does 
not care how the outcome relates to the agents preferences 
but rather has a fixed preference over the outcomes and 
about the net payments made by the agents which flow to 
the designer 
definition a self-interested designer has an objective 
function given by g o 
n 
i 
πi where g o → r indicates 
the designer s own preference over the outcomes and πi is 
the payment made by agent i in the case where g 
everywhere the designer is said to be payment maximizing 
in the case where payments are not possible g constitutes 
the objective function by itself 
we now define the kinds of mechanisms under study by 
the revelation principle we can restrict attention to 
truthful direct revelation mechanisms where agents report their 
types directly and never have an incentive to misreport them 
definition we consider the following kinds of 
mechanism 
 a deterministic mechanism without payments consists 
of an outcome selection function o θ × θ × × 
θn → o 
 a randomized mechanism without payments consists 
of a distribution selection function p θ × θ × × 
θn → p o where p o is the set of probability 
distributions over o 
 a deterministic mechanism with payments consists of 
an outcome selection function o θ ×θ × ×θn → 
o and for each agent i a payment selection function 
πi θ × θ × × θn → r where πi θ θn 
gives the payment made by agent i when the reported 
types are θ θn 
 
though this follows standard game theory notation 
the fact that the agent has both a utility function and a type 
is perhaps confusing the types encode the various possible 
preferences that the agent may turn out to have and the 
agent s type is not known to the aggregator the utility 
function is common knowledge but because the agent s type 
is a parameter in the agent s utility function the aggregator 
cannot know what the agent s utility is without knowing the 
agent s type 
 
 a randomized mechanism with payments consists of 
a distribution selection function p θ × θ × × 
θn → p o and for each agent i a payment selection 
function πi θ × θ × × θn → r 
there are two types of constraint on the designer in 
building the mechanism 
 individual rationality ir constraints 
the first type of constraint is the following the utility of 
each agent has to be at least as great as the agent s fallback 
utility that is the utility that the agent would receive if it 
did not participate in the mechanism otherwise that agent 
would not participate in the mechanism-and no agent s 
participation can ever hurt the mechanism designer s 
objective because at worst the mechanism can ignore an agent 
by pretending the agent is not there furthermore if no 
such constraint applied the designer could simply make the 
agents pay an infinite amount this type of constraint is 
called an ir individual rationality constraint there are 
three different possible ir constraints ex ante ex interim 
and ex post depending on what the agent knows about its 
own type and the others types when deciding whether to 
participate in the mechanism ex ante ir means that the 
agent would participate if it knew nothing at all not even 
its own type we will not study this concept in this paper 
ex interim ir means that the agent would always 
participate if it knew only its own type but not those of the others 
ex post ir means that the agent would always participate 
even if it knew everybody s type we will define the 
latter two notions of ir formally first we need to formalize 
the concept of the fallback outcome we assume that each 
agent s fallback utility is zero for each one of its types this 
is without loss of generality because we can add a constant 
term to an agent s utility function for a given type 
without affecting the decision-making behavior of that expected 
utility maximizing agent 
definition in any automated mechanism design 
setting with an ir constraint there is a fallback outcome o ∈ 
o where for any agent i and any type θi ∈ θi we have 
ui θi o additionally in the case of a self-interested 
designer g o 
we can now to define the notions of individual rationality 
definition individual rationality ir is defined by 
 a deterministic mechanism is ex interim ir if for any 
agent i and any type θi ∈ θi we have 
e θ θi− θi θn θi 
 ui θi o θ θn −πi θ θn 
≥ 
a randomized mechanism is ex interim ir if for any 
agent i and any type θi ∈ θi we have 
e θ θi− θi θn θi 
eo θ θn ui θi o −πi θ θn 
≥ 
 a deterministic mechanism is ex post ir if for any 
agent i and any type vector θ θn ∈ θ × × 
θn we have ui θi o θ θn − πi θ θn ≥ 
 
 
we do not randomize over payments because as long as 
the agents and the designer are risk neutral with respect to 
payments that is their utility is linear in payments there 
is no reason to randomize over payments 
a randomized mechanism is ex post ir if for any agent 
i and any type vector θ θn ∈ θ × × θn 
we have eo θ θn ui θi o − πi θ θn ≥ 
the terms involving payments can be left out in the case 
where payments are not possible 
 incentive compatibility ic constraints 
the second type of constraint says that the agents should 
never have an incentive to misreport their type as justified 
above by the revelation principle for this type of 
constraint the two most common variants or solution concepts 
are implementation in dominant strategies and 
implementation in bayes-nash equilibrium 
definition given an automated mechanism design 
setting a mechanism is said to implement its outcome and 
payment functions in dominant strategies if truthtelling is 
always optimal even when the types reported by the other 
agents are already known formally for any agent i any 
type vector θ θi θn ∈ θ × × θi × × θn 
and any alternative type report ˆθi ∈ θi in the case of 
deterministic mechanisms we have 
ui θi o θ θi θn − πi θ θi θn ≥ 
ui θi o θ ˆθi θn − πi θ ˆθi θn 
in the case of randomized mechanisms we have 
eo θ θi θn ui θi o − πi θ θi θn ≥ 
eo θ ˆθi θn 
 ui θi o − πi θ ˆθi θn 
the terms involving payments can be left out in the case 
where payments are not possible 
thus in dominant strategies implementation truthtelling 
is optimal regardless of what the other agents report if it 
is optimal only given that the other agents are truthful and 
given that one does not know the other agents types we 
have implementation in bayes-nash equilibrium 
definition given an automated mechanism design 
setting a mechanism is said to implement its outcome and 
payment functions in bayes-nash equilibrium if truthtelling 
is always optimal to an agent when that agent does not yet 
know anything about the other agents types and the other 
agents are telling the truth formally for any agent i any 
type θi ∈ θi and any alternative type report ˆθi ∈ θi in the 
case of deterministic mechanisms we have 
e θ θi− θi θn θi 
 ui θi o θ θi θn − 
πi θ θi θn ≥ 
e θ θi− θi θn θi 
 ui θi o θ ˆθi θn − 
πi θ ˆθi θn 
in the case of randomized mechanisms we have 
e θ θi− θi θn θi 
eo θ θi θn ui θi o − 
πi θ θi θn ≥ 
e θ θi− θi θn θi 
eo θ ˆθi θn 
 ui θi o − 
πi θ ˆθi θn 
the terms involving payments can be left out in the case 
where payments are not possible 
 
 automated mechanism design 
we can now define the computational problem we study 
definition automated-mechanism-design 
 amd we are given 
 an automated mechanism design setting 
 an ir notion ex interim ex post or none 
 a solution concept dominant strategies or bayes-nash 
 whether payments are possible 
 whether randomization is possible 
 in the decision variant of the problem a target value 
g 
we are asked whether there exists a mechanism of the 
specified kind in terms of payments and randomization that 
satisfies both the ir notion and the solution concept and 
gives an expected value of at least g for the objective 
an interesting special case is the setting where there is 
only one agent in this case the reporting agent always 
knows everything there is to know about the other agents 
types-because there are no other agents since ex post and 
ex interim ir only differ on what an agent is assumed to 
know about other agents types the two ir concepts 
coincide here also because implementation in dominant 
strategies and implementation in bayes-nash equilibrium only 
differ on what an agent is assumed to know about other agents 
types the two solution concepts coincide here this 
observation will prove to be a useful tool in proving hardness 
results if we prove computational hardness in the 
singleagent setting this immediately implies hardness for both 
ir concepts for both solution concepts for any number of 
agents 
 
payment-maximizingdeterministic amd is hard 
in this section we demonstrate that it is np-complete 
to design a deterministic mechanism that maximizes the 
expected sum of the payments collected from the agents 
we show that this problem is hard even in the single-agent 
setting thereby immediately showing it hard for both ir 
concepts for both solution concepts to demonstrate 
nphardness we reduce from the minsat problem 
definition minsat we are given a formula φ 
in conjunctive normal form represented by a set of boolean 
variables v and a set of clauses c and an integer k k 
 c we are asked whether there exists an assignment to the 
variables in v such that at most k clauses in φ are satisfied 
minsat was recently shown to be np-complete we 
can now present our result 
theorem payment-maximizing deterministic amd is 
np-complete even for a single agent even with a uniform 
distribution over types 
proof it is easy to show that the problem is in np 
to show np-hardness we reduce an arbitrary minsat 
instance to the following single-agent payment-maximizing 
deterministic amd instance let the agent s type set be 
θ {θc c ∈ c} ∪ {θv v ∈ v } where c is the set of 
clauses in the minsat instance and v is the set of 
variables let the probability distribution over these types be 
uniform let the outcome set be o {o } ∪ {oc c ∈ 
c} ∪ {ol l ∈ l} where l is the set of literals that is 
l { v v ∈ v } ∪ {−v v ∈ v } let the notation v l v 
denote that v is the variable corresponding to the literal l 
that is l ∈ { v −v} let l ∈ c denote that the literal l 
occurs in clause c then let the agent s utility function 
be given by u θc ol θ for all l ∈ l with l ∈ c 
u θc ol for all l ∈ l with l ∈ c u θc oc θ 
u θc oc for all c ∈ c with c c u θv ol θ for all 
l ∈ l with v l v u θv ol for all l ∈ l with v l v 
u θv oc for all c ∈ c the goal of the amd instance 
is g θ c −k 
 θ 
 where k is the goal of the minsat 
instance we show the instances are equivalent first suppose 
there is a solution to the minsat instance let the 
assignment of truth values to the variables in this solution be given 
by the function f v → l where v f v v for all v ∈ v 
then for every v ∈ v let o θv of v and π θv θ 
for every c ∈ c let o θc oc let π θc θ if c 
is not satisfied in the minsat solution and π θc θ 
if c is satisfied it is straightforward to check that the ir 
constraint is satisfied we now check that the agent has no 
incentive to misreport if the agent s type is some θv then 
any other report will give it an outcome that is no better 
for a payment that is no less so it has no incentive to 
misreport if the agent s type is some θc where c is a satisfied 
clause again any other report will give it an outcome that 
is no better for a payment that is no less so it has no 
incentive to misreport the final case to check is where the 
agent s type is some θc where c is an unsatisfied clause in 
this case we observe that for none of the types reporting it 
leads to an outcome ol for a literal l ∈ c precisely because 
the clause is not satisfied in the minsat instance because 
also no type besides θc leads to the outcome oc reporting 
any other type will give an outcome with utility while still 
forcing a payment of at least θ from the agent clearly the 
agent is better off reporting truthfully for a total utility of 
 this establishes that the agent never has an incentive to 
misreport finally we show that the goal is reached if s is 
the number of satisfied clauses in the minsat solution so 
that s ≤ k the expected payment from this mechanism 
is v θ s θ c −s θ 
 θ 
≥ v θ k θ c −k θ 
 θ 
 
 θ c −k 
 θ 
 g so there is a solution to the amd 
instance 
now suppose there is a solution to the amd instance 
given by an outcome function o and a payment function 
π first suppose there is some v ∈ v such that o θv ∈ 
{o v o−v} then the utility that the agent derives from 
the given outcome for this type is and hence by ir no 
payment can be extracted from the agent for this type 
because again by ir the maximum payment that can be 
extracted for any other type is θ it follows that the 
maximum expected payment that could be obtained is at 
most θ − θ 
 θ 
 θ g contradicting that this is a 
solution to the amd instance it follows that in the solution 
to the amd instance for every v ∈ v o θv ∈ {o v o−v} 
 
we can interpret this as an assignment of truth values to 
the variables v is set to true if o θv o v and to false 
if o θv o−v we claim this assignment is a solution to 
the minsat instance by the ir constraint the maximum 
payment we can extract from any type θv is θ because 
there can be no incentives for the agent to report falsely for 
any clause c satisfied by the given assignment the maximum 
payment we can extract for the corresponding type θc is θ 
 for if we extracted more from this type the agent s utility 
in this case would be less than and if v is the variable 
satisfying c in the assignment so that o θv ol where l occurs 
in c then the agent would be better off reporting θv instead 
of the truthful report θc to get an outcome worth θ to 
it while having to pay at most θ finally for any 
unsatisfied clause c by the ir constraint the maximum payment 
we can extract for the corresponding type θc is θ it 
follows that the expected payment from our mechanism is 
at most v θ s θ c −s θ 
θ 
 where s is the number of 
satisfied clauses because our mechanism achieves the goal 
it follows that v θ s θ c −s θ 
θ 
≥ g which by simple 
algebraic manipulations is equivalent to s ≤ k so there is 
a solution to the minsat instance 
because payment-maximizing amd is just the special case 
of amd for a self-interested designer where the designer has 
no preferences over the outcome chosen this immediately 
implies hardness for the general case of amd for a 
selfinterested designer where payments are possible however 
it does not yet imply hardness for the special case where 
payments are not possible we will prove hardness in this 
case in the next section 
 self-interested deterministic 
amd without payments is hard 
in this section we demonstrate that it is np-complete to 
design a deterministic mechanism that maximizes the 
expectation of the designer s objective when payments are not 
possible we show that this problem is hard even in the 
single-agent setting thereby immediately showing it hard 
for both ir concepts for both solution concepts 
theorem without payments deterministic amd for 
a self-interested designer is np-complete even for a single 
agent even with a uniform distribution over types 
proof it is easy to show that the problem is in np 
to show np-hardness we reduce an arbitrary minsat 
instance to the following single-agent self-interested 
deterministic amd without payments instance let the agent s type 
set be θ {θc c ∈ c} ∪ {θv v ∈ v } where c is the 
set of clauses in the minsat instance and v is the set of 
variables let the probability distribution over these types 
be uniform let the outcome set be o {o } ∪ {oc c ∈ 
c}∪{ol l ∈ l}∪{o∗ 
} where l is the set of literals that is 
l { v v ∈ v } ∪ {−v v ∈ v } let the notation v l v 
denote that v is the variable corresponding to the literal l 
that is l ∈ { v −v} let l ∈ c denote that the literal l 
occurs in clause c then let the agent s utility function be 
given by u θc ol for all l ∈ l with l ∈ c u θc ol − 
for all l ∈ l with l ∈ c u θc oc u θc oc − for 
all c ∈ c with c c u θc o∗ 
 u θv ol for all 
l ∈ l with v l v u θv ol − for all l ∈ l with 
v l v u θv oc − for all c ∈ c u θv o∗ 
 − let 
the designer s objective function be given by g o∗ 
 θ 
g ol θ for all l ∈ l g oc θ for all c ∈ c the goal 
of the amd instance is g θ c −k 
 θ 
 where k is the 
goal of the minsat instance we show the instances are 
equivalent first suppose there is a solution to the minsat 
instance let the assignment of truth values to the variables 
in this solution be given by the function f v → l where 
v f v v for all v ∈ v then for every v ∈ v let 
o θv of v for every c ∈ c that is satisfied in the 
minsat solution let o θc oc for every unsatisfied c ∈ c 
let o θc o∗ 
 it is straightforward to check that the ir 
constraint is satisfied we now check that the agent has no 
incentive to misreport if the agent s type is some θv it is 
getting the maximum utility for that type so it has no 
incentive to misreport if the agent s type is some θc where c 
is a satisfied clause again it is getting the maximum utility 
for that type so it has no incentive to misreport the final 
case to check is where the agent s type is some θc where c is 
an unsatisfied clause in this case we observe that for none 
of the types reporting it leads to an outcome ol for a literal 
l ∈ c precisely because the clause is not satisfied in the 
minsat instance because also no type leads to the outcome 
oc there is no outcome that the mechanism ever selects that 
would give the agent utility greater than for type θc and 
hence the agent has no incentive to report falsely this 
establishes that the agent never has an incentive to misreport 
finally we show that the goal is reached if s is the number 
of satisfied clauses in the minsat solution so that s ≤ k 
then the expected value of the designer s objective function 
is v θ s θ c −s θ 
 θ 
≥ v θ k θ c −k θ 
 θ 
 
 θ c −k 
 θ 
 g so there is a solution to the amd 
instance 
now suppose there is a solution to the amd instance 
given by an outcome function o first suppose there is 
some v ∈ v such that o θv ∈ {o v o−v} the only other 
outcome that the mechanism is allowed to choose under the 
ir constraint is o this has an objective value of and 
because the highest value the objective function ever takes 
is θ it follows that the maximum expected value of 
the objective function that could be obtained is at most 
 θ − θ 
 θ 
 θ g contradicting that this is a 
solution to the amd instance it follows that in the solution 
to the amd instance for every v ∈ v o θv ∈ {o v o−v} 
we can interpret this as an assignment of truth values to 
the variables v is set to true if o θv o v and to false 
if o θv o−v we claim this assignment is a solution to 
the minsat instance by the above for any type θv the 
value of the objective function in this mechanism will be 
 θ for any clause c satisfied by the given assignment 
the value of the objective function in the case where the 
agent reports type θc will be at most θ this is because 
we cannot choose the outcome o∗ 
for such a type as in 
this case the agent would have an incentive to report θv 
instead where v is the variable satisfying c in the 
assignment so that o θv ol where l occurs in c finally 
for any unsatisfied clause c the maximum value the 
objective function can take in the case where the agent 
reports type θc is θ simply because this is the largest 
value the function ever takes it follows that the expected 
value of the objective function for our mechanism is at most 
v θ s θ c −s θ 
θ 
 where s is the number of satisfied 
 
clauses because our mechanism achieves the goal it follows 
that v θ s θ c −s θ 
θ 
≥ g which by simple algebraic 
manipulations is equivalent to s ≤ k so there is a solution 
to the minsat instance 
both of our hardness results relied on the constraint that 
the mechanism should be deterministic in the next section 
we show that the hardness of design disappears when we 
allow for randomization in the mechanism 
 randomized amd for a 
selfinterested designer is easy 
we now show how allowing for randomization over the 
outcomes makes the problem of self-interested amd tractable 
through linear programming for any constant number of 
agents 
theorem self-interested randomized amd with a 
constant number of agents is solvable in polynomial time by 
linear programming both with and without payments both for 
ex post and ex interim ir and both for implementation in 
dominant strategies and for implementation in bayes-nash 
equilibrium-even if the types are correlated 
proof because linear programs can be solved in 
polynomial time all we need to show is that the number of 
variables and equations in our program is polynomial for any 
constant number of agents-that is exponential only in n 
throughout for purposes of determining the size of the 
linear program let t maxi{ θi } the variables of our linear 
program will be the probabilities p θ θ θn o at 
most tn 
 o variables and the payments πi θ θ θn 
 at most ntn 
variables we show the linear program for 
the case where payments are possible the case without 
payments is easily obtained from this by simply omitting all the 
payment variables in the program or by adding additional 
constraints forcing the payments to be 
first we show the ir constraints for ex post ir we add 
the following at most ntn 
 constraints to the lp 
 for every i ∈ { n} and for every θ θ θn 
∈ θ × θ × × θn we add 
 
o∈o 
 p θ θ θn o u θi o − πi θ θ θn ≥ 
for ex interim ir we add the following at most nt 
constraints to the lp 
 for every i ∈ { n} for every θi ∈ θi we add 
θ θn 
γ θ θn θi 
o∈o 
 p θ θ θn o u θi o − 
πi θ θ θn ≥ 
now we show the solution concept constraints for 
implementation in dominant strategies we add the following 
 at most ntn 
 constraints to the lp 
 for every i ∈ { n} for every 
 θ θ θi θn ∈ θ × θ × × θn and for every 
alternative type report ˆθi ∈ θi we add the constraint 
 
o∈o 
 p θ θ θi θn o u θi o − 
πi θ θ θi θn ≥ 
 
o∈o 
 p θ θ ˆθi θn o u θi o − 
πi θ θ ˆθi θn 
finally for implementation in bayes-nash equilibrium we 
add the following at most nt 
 constraints to the lp 
 for every i ∈ { n} for every θi ∈ θi and for 
every alternative type report ˆθi ∈ θi we add the constraint 
θ θn 
γ θ θn θi 
o∈o 
 p θ θ θi θn o u θi o 
− πi θ θ θi θn ≥ 
θ θn 
γ θ θn θi 
o∈o 
 p θ θ ˆθi θn o u θi o 
− πi θ θ ˆθi θn 
all that is left to do is to give the expression the designer 
is seeking to maximize which is 
 
θ θn 
γ θ θn 
o∈o 
 p θ θ θi θn o g o 
 
n 
i 
πi θ θ θn 
as we indicated the number of variables and constraints 
is exponential only in n and hence the linear program is of 
polynomial size for constant numbers of agents thus the 
problem is solvable in polynomial time 
 implications for an optimal 
combinatorial auction design 
problem 
in this section we will demonstrate some interesting 
consequences of the problem of automated mechanism design 
for a self-interested designer on designing optimal 
combinatorial auctions 
consider a combinatorial auction with a set s of items 
for sale for any bundle b ⊆ s let ui θi b be bidder 
i s utility for receiving bundle b when the bidder s type is 
θi the optimal auction design problem is to specify the 
rules of the auction so as to maximize expected revenue to 
the auctioneer by the revelation principle without loss 
of generality we can assume the auction is truthful the 
optimal auction design problem is solved for the case of a 
single item by the famous myerson auction however 
designing optimal auctions in combinatorial auctions is a 
recognized open research problem the problem is 
open even if there are only two items for sale the 
twoitem case with a very special form of complementarity and 
no substitutability has been solved recently 
suppose we have free disposal-items can be thrown away 
at no cost also suppose that the bidders preferences have 
the following structure whenever a bidder receives a bundle 
of items the bidder s utility for that bundle is determined 
by the best item in the bundle only we emphasize that 
 
which item is the best is allowed to depend on the bidder s 
type 
definition bidder i is said to have best-only 
preferences over bundles of items if there exists a function vi 
θi × s → r such that for any θi ∈ θi for any b ⊆ s 
ui θi b maxs∈b vi θi s 
we make the following useful observation in this setting 
there is no sense in awarding a bidder more than one item 
the reason is that if the bidder is reporting truthfully taking 
all but the highest valued item away from the bidder will 
not hurt the bidder and by free disposal doing so can only 
reduce the incentive for this bidder to falsely report this 
type when the bidder actually has another type 
we now show that the problem of designing a 
deterministic optimal auction here is np-complete by a reduction 
from the payment maximizing amd problem 
theorem given an optimal combinatorial auction 
design problem under best-only preferences given by a set of 
items s and for each bidder i a finite type space θi and a 
function vi θi × s → r such that for any θi ∈ θi for any 
b ⊆ s ui θi b maxs∈b vi θi s designing the 
optimal deterministic auction is np-complete even for a single 
bidder with a uniform distribution over types 
proof the problem is in np because we can 
nondeterministically generate an allocation rule and then set the 
payments using linear programming 
to show np-hardness we reduce an arbitrary 
paymentmaximizing deterministic amd instance with a single agent 
and a uniform distribution over types to the following 
optimal combinatorial auction design problem instance with 
a single bidder with best-only preferences for every 
outcome o ∈ o in the amd instance besides the outcome o 
let there be one item so ∈ s let the type space be the 
same and let v θi so ui θi o where u is as specified in 
the amd instance let the expected revenue target value 
be the same in both instances we show the instances are 
equivalent 
first suppose there exists a solution to the amd instance 
given by an outcome function and a payment function then 
if the amd solution chooses outcome o for a type in the 
optimal auction solution allocate {so} to the bidder for this 
type unless o o in which case we allocate {} to the 
bidder let the payment functions be the same in both 
instances then the utility that an agent receives for 
reporting a type given the true type in either solution is the 
same so we have incentive compatibility in the optimal 
auction solution moreover because the type distribution and 
the payment function are the same the expected revenue to 
the auctioneer designer is the same it follows that there 
exists a solution to the optimal auction design instance 
now suppose there exists a solution to the optimal auction 
design instance by the at-most-one-item observation we 
can assume without loss of generality that the solution never 
allocates more than one item then if the optimal auction 
solution allocates item so to the bidder for a type in the 
amd solution let the mechanism choose outcome o for that 
type if the optimal auction solution allocates nothing to the 
bidder for a type in the amd solution let the mechanism 
choose outcome o for that type let the payment functions 
be the same then the utility that an agent receives for 
reporting a type given the true type in either solution is 
the same so we have incentive compatibility in the amd 
solution moreover because the type distribution and the 
payment function are the same the expected revenue to the 
designer auctioneer is the same it follows that there exists 
a solution to the amd instance 
fortunately we can also carry through the easiness result 
for randomized mechanisms to this combinatorial auction 
setting-giving us one of the few known polynomial-time 
algorithms for an optimal combinatorial auction design 
problem 
theorem given an optimal combinatorial auction 
design problem under best-only preferences given by a set of 
items s and for each bidder i a finite type space θi and a 
function vi θi × s → r such that for any θi ∈ θi for 
any b ⊆ s ui θi b maxs∈b vi θi s if the number of 
bidders is a constant k then the optimal randomized 
auction can be designed in polynomial time for any ic and 
ir constraints 
proof by the at-most-one-item observation we can 
without loss of generality restrict ourselves to allocations where 
each bidder receives at most one item there are fewer than 
 s k 
such allocations-that is a polynomial number 
of allocations because we can list the outcomes explicitly 
we can simply solve this as a payment-maximizing amd 
instance with linear programming 
 related research on 
complexity in mechanism design 
there has been considerable recent interest in mechanism 
design in computer science some of it has focused on 
issues of computational complexity but most of that work has 
strived toward designing mechanisms that are easy to 
execute e g rather than studying the 
complexity of designing the mechanism the closest piece of 
earlier work studied the complexity of automated mechanism 
design by a benevolent designer roughgarden has 
studied the complexity of designing a good network 
topology for agents that selfishly choose the links they use 
this is related to mechanism design but differs significantly 
in that the designer only has restricted control over the rules 
of the game because there is no party that can impose the 
outcome or side payments also there is no explicit 
reporting of preferences 
 conclusions and future 
research 
often an outcome must be chosen on the basis of the 
preferences reported by a group of agents the key difficulty 
is that the agents may report their preferences insincerely 
to make the chosen outcome more favorable to themselves 
mechanism design is the art of designing the rules of the 
game so that the agents are motivated to report their 
preferences truthfully and a desirable outcome is chosen in a 
recently emerging approach-called automated mechanism 
design-a mechanism is computed for the specific preference 
aggregation setting at hand this has several advantages 
 
but the downside is that the mechanism design optimization 
problem needs to be solved anew each time unlike earlier 
work on automated mechanism design that studied a 
benevolent designer in this paper we studied automated 
mechanism design problems where the designer is 
self-interesteda setting much more relevant for electronic commerce in 
this setting the center cares only about which outcome is 
chosen and what payments are made to it the reason that 
the agents preferences are relevant is that the center is 
constrained to making each agent at least as well off as the agent 
would have been had it not participated in the mechanism 
in this setting we showed that designing an optimal 
deterministic mechanism is np-complete in two important 
special cases when the center is interested only in the payments 
made to it and when payments are not possible and the 
center is interested only in the outcome chosen these 
hardness results imply hardness in all more general automated 
mechanism design settings with a self-interested designer 
the hardness results apply whether the individual 
rationality participation constraints are applied ex interim or ex 
post and whether the solution concept is dominant 
strategies implementation or bayes-nash equilibrium 
implementation we then showed that allowing randomization in the 
mechanism makes the design problem in all these settings 
computationally easy finally we showed that the 
paymentmaximizing amd problem is closely related to an interesting 
variant of the optimal revenue-maximizing combinatorial 
auction design problem where the bidders have best-only 
preferences we showed that here too designing an 
optimal deterministic mechanism is np-complete even with one 
agent but designing an optimal randomized mechanism is 
easy 
future research includes studying automated mechanism 
design with a self-interested designer in more restricted 
settings such as auctions where the designer s objective may 
include preferences about which bidder should receive the 
good-as well as payments we also want to study the 
complexity of automated mechanism design in settings where the 
outcome and type spaces have special structure so they can 
be represented more concisely finally we plan to assemble 
a data set of real-world mechanism design problems-both 
historical and current-and apply automated mechanism 
design to those problems 
 references 
 m armstrong optimal multi-object auctions review 
of economic studies - 
 k arrow the property rights doctrine and demand 
revelation under incomplete information in 
m boskin editor economics and human welfare 
new york academic press 
 c avery and t hendershott bundling and optimal 
auctions of multiple products review of economic 
studies - 
 e h clarke multipart pricing of public goods public 
choice - 
 v conitzer and t sandholm complexity of 
mechanism design in proceedings of the th annual 
conference on uncertainty in artificial intelligence 
 uai- pages - edmonton canada 
 v conitzer and t sandholm automated mechanism 
design complexity results stemming from the 
single-agent setting in proceedings of the th 
international conference on electronic commerce 
 icec- pages - pittsburgh pa usa 
 v conitzer and t sandholm computational 
criticisms of the revelation principle in proceedings of 
the acm conference on electronic commerce 
 acm-ec new york ny short paper 
full-length version appeared in the aamas- 
workshop on agent-mediated electronic commerce 
 amec 
 c d aspremont and l a g´erard-varet incentives 
and incomplete information journal of public 
economics - 
 j feigenbaum c papadimitriou and s shenker 
sharing the cost of muliticast transmissions journal 
of computer and system sciences - 
early version in proceedings of the annual acm 
symposium on theory of computing stoc 
 a gibbard manipulation of voting schemes 
econometrica - 
 t groves incentives in teams econometrica 
 - 
 j hershberger and s suri vickrey prices and 
shortest paths what is an edge worth in 
proceedings of the annual symposium on foundations 
of computer science focs 
 l khachiyan a polynomial algorithm in linear 
programming soviet math doklady - 
 
 r kohli r krishnamurthi and p mirchandani the 
minimum satisfiability problem siam journal of 
discrete mathematics - 
 d lehmann l i o callaghan and y shoham 
truth revelation in rapid approximately efficient 
combinatorial auctions journal of the acm 
 - early version appeared in 
proceedings of the acm conference on electronic 
commerce acm-ec 
 a mas-colell m whinston and j r green 
microeconomic theory oxford university press 
 e s maskin and j riley optimal multi-unit 
auctions in f hahn editor the economics of 
missing markets information and games 
chapter pages - clarendon press oxford 
 
 r myerson optimal auction design mathematics of 
operation research - 
 n nisan and a ronen computationally feasible 
vcg mechanisms in proceedings of the acm 
conference on electronic commerce acm-ec 
pages - minneapolis mn 
 n nisan and a ronen algorithmic mechanism 
design games and economic behavior - 
 early version in proceedings of the annual acm 
symposium on theory of computing stoc 
 t roughgarden designing networks for selfish users 
is hard in proceedings of the annual symposium on 
foundations of computer science focs 
 t sandholm issues in computational vickrey 
auctions international journal of electronic 
commerce - special issue on 
 
applying intelligent agents for electronic commerce 
a short early version appeared at the second 
international conference on multi-agent systems 
 icmas pages - 
 m a satterthwaite strategy-proofness and arrow s 
conditions existence and correspondence theorems for 
voting procedures and social welfare functions 
journal of economic theory - 
 w vickrey counterspeculation auctions and 
competitive sealed tenders journal of finance 
 - 
 r v vohra research problems in combinatorial 
auctions mimeo version oct 
 
complexity of iterated dominance∗ 
vincent conitzer 
carnegie mellon university 
 forbes avenue 
pittsburgh pa usa 
conitzer cs cmu edu 
tuomas sandholm 
carnegie mellon university 
 forbes avenue 
pittsburgh pa usa 
sandholm cs cmu edu 
abstract 
we study various computational aspects of solving games 
using dominance and iterated dominance we first study 
both strict and weak dominance not iterated and show 
that checking whether a given strategy is dominated by 
some mixed strategy can be done in polynomial time using 
a single linear program solve we then move on to 
iterated dominance we show that determining whether there 
is some path that eliminates a given strategy is np-complete 
with iterated weak dominance this allows us to also show 
that determining whether there is a path that leads to a 
unique solution is np-complete both of these results hold 
both with and without dominance by mixed strategies a 
weaker version of the second result only without dominance 
by mixed strategies was already known iterated strict 
dominance on the other hand is path-independent both 
with and without dominance by mixed strategies and can 
therefore be done in polynomial time 
we then study what happens when the dominating 
strategy is allowed to place positive probability on only a few 
pure strategies first we show that finding the dominating 
strategy with minimum support size is np-complete both 
for strict and weak dominance then we show that 
iterated strict dominance becomes path-dependent when there 
is a limit on the support size of the dominating strategies 
and that deciding whether a given strategy can be 
eliminated by iterated strict dominance under this restriction is 
np-complete even when the limit on the support size is 
finally we study bayesian games we show that unlike in 
normal form games deciding whether a given pure strategy 
is dominated by another pure strategy in a bayesian game is 
np-complete both with strict and weak dominance 
however deciding whether a strategy is dominated by some 
mixed strategy can still be done in polynomial time with 
a single linear program solve both with strict and weak 
∗ 
this material is based upon work supported by the 
national science foundation under itr grants iis- 
and iis- and a sloan fellowship 
dominance finally we show that iterated dominance 
using pure strategies can require an exponential number of 
iterations in a bayesian game both with strict and weak 
dominance 
categories and subject descriptors 
f theory of computation analysis of algorithms 
and problem complexity j computer applications 
social and behavioral sciences-economics 
general terms 
algorithms economics theory 
 introduction 
in multiagent systems with self-interested agents the 
optimal action for one agent may depend on the actions taken 
by other agents in such settings the agents require tools 
from game theory to rationally decide on an action game 
theory offers various formal models of strategic settings-the 
best-known of which is a game in normal or matrix form 
specifying a utility payoff for each agent for each 
combination of strategies that the agents choose-as well as 
solution concepts which given a game specify which outcomes 
are reasonable under various assumptions of rationality and 
common knowledge 
probably the best-known and certainly the most-studied 
solution concept is that of nash equilibrium a nash 
equilibrium specifies a strategy for each player in such a way that 
no player has an incentive to unilaterally deviate from the 
prescribed strategy recently numerous papers have 
studied computing nash equilibria in various settings 
 and the complexity of constructing a nash 
equilibrium in normal form games has been labeled one of the 
two most important open problems on the boundary of p 
today 
the problem of computing solutions according to the 
perhaps more elementary solution concepts of dominance and 
iterated dominance has received much less attention after 
an early short paper on an easy case the main 
computational study of these concepts has actually taken place 
in a paper in the game theory community 
 a strategy 
strictly dominates another strategy if it performs strictly 
 
this is not to say that computer scientists have ignored 
 
better against all vectors of opponent strategies and weakly 
dominates it if it performs at least as well against all vectors 
of opponent strategies and strictly better against at least 
one the idea is that dominated strategies can be eliminated 
from consideration in iterated dominance the elimination 
proceeds in rounds and becomes easier as more strategies 
are eliminated in any given round the dominating 
strategy no longer needs to perform better than or as well as the 
dominated strategy against opponent strategies that were 
eliminated in earlier rounds computing solutions 
according to iterated dominance is important for at least the 
following reasons it can be computationally easier than 
computing for instance a nash equilibrium and therefore 
it can be useful as a preprocessing step in computing a nash 
equilibrium and iterated dominance requires a weaker 
rationality assumption on the players than for instance 
nash equilibrium and therefore solutions derived according 
to it are more likely to occur 
in this paper we study some fundamental computational 
questions concerning dominance and iterated dominance 
including how hard it is to check whether a given strategy can 
be eliminated by each of the variants of these notions the 
rest of the paper is organized as follows in section we 
briefly review definitions and basic properties of normal form 
games strict and weak dominance and iterated strict and 
weak dominance in the remaining sections we study 
computational aspects of dominance and iterated dominance in 
section we study one-shot not iterated dominance in 
section we study iterated dominance in section we 
study dominance and iterated dominance when the 
dominating strategy can only place probability on a few pure 
strategies finally in section we study dominance and 
iterated dominance in bayesian games 
 definitions and basic properties 
in this section we briefly review normal form games as 
well as dominance and iterated dominance both strict and 
weak an n-player normal form game is defined as follows 
definition a normal form game is given by a set of 
players { n} and for each player i a finite set of 
pure strategies σi and a utility function ui σ × σ × × 
σn → r where ui σ σ σn denotes player i s utility 
when each player j plays action σj 
the two main notions of dominance are defined as follows 
definition player i s strategy σi is said to be strictly 
dominated by player i s strategy σi if for any vector of 
strategies σ−i for the other players ui σi σ−i ui σi σ−i 
player i s strategy σi is said to be weakly dominated by 
player i s strategy σi if for any vector of strategies σ−i for the 
other players ui σi σ−i ≥ ui σi σ−i and for at least one 
vector of strategies σ−i for the other players ui σi σ−i 
ui σi σ−i 
in this definition it is sometimes allowed for the 
dominating strategy σi to be a mixed strategy that is a probability 
distribution over pure strategies in this case the utilities in 
dominance altogether for example simple dominance 
checks are sometimes used as a subroutine in searching for 
nash equilibria 
the definition are the expected utilities 
there are other 
notions of dominance such as very weak dominance in which 
no strict inequality is required so two strategies can 
dominate each other but we will not study them here when we 
are looking at the dominance relations for player i the other 
players −i can be thought of as a single player 
 
therefore in the rest of the paper when we study one-shot not 
iterated dominance we will focus without loss of generality 
on two-player games 
in two-player games we will 
generally refer to the players as r row and c column rather 
than and 
in iterated dominance dominated strategies are removed 
from the game and no longer have any effect on future 
dominance relations iterated dominance can eliminate more 
strategies than dominance as follows σr may originally 
not dominate σr because the latter performs better against 
σc but then once σc is removed because it is dominated by 
σc σr dominates σr and the latter can be removed for 
example in the following game r can be removed first after 
which d is dominated 
l r 
u 
d 
either strict or weak dominance can be used in the 
definition of iterated dominance we note that the process of 
iterated dominance is never helped by removing a dominated 
mixed strategy for the following reason if σi gives player i 
a higher utility than σi against mixed strategy σj for player 
j i and strategies σ−{i j} for the other players then for 
at least one pure strategy σj that σj places positive 
probability on σi must perform better than σi against σj and 
strategies σ−{i j} for the other players thus removing the 
mixed strategy σj does not introduce any new dominances 
more detailed discussions and examples can be found in 
standard texts on microeconomics or game theory 
we are now ready to move on to the core of this paper 
 dominance not iterated 
in this section we study the notion of one-shot not 
iterated dominance as a first observation checking whether a 
given strategy is strictly weakly dominated by some pure 
strategy is straightforward by checking for every pure 
strategy for that player whether the latter strategy performs 
strictly better against all the opponent s strategies at least 
as well against all the opponent s strategies and strictly 
 
the dominated strategy σi is of course also allowed to be 
mixed but this has no technical implications for the paper 
when we study one-shot dominance we ask whether a given 
strategy is dominated and it does not matter whether the 
given strategy is pure or mixed when we study iterated 
dominance there is no use in eliminating mixed strategies 
as we will see shortly 
 
this player may have a very large strategy space one pure 
strategy for every vector of pure strategies for the players 
that are being replaced nevertheless this will not result in 
an increase in our representation size because the original 
representation already had to specify utilities for each of 
these vectors 
 
we note that a restriction to two-player games would not 
be without loss of generality for iterated dominance this 
is because for iterated dominance we need to look at the 
dominated strategies of each individual player so we cannot 
merge any players 
 
better against at least one 
next we show that 
checking whether a given strategy is dominated by some mixed 
strategy can be done in polynomial time by solving a single 
linear program similar linear programs have been given 
before we present the result here for completeness and 
because we will build on the linear programs given below in 
theorem 
proposition given the row player s utilities a 
subset dr of the row player s pure strategies σr and a 
distinguished strategy σ∗ 
r for the row player we can check in time 
polynomial in the size of the game by solving a single linear 
program of polynomial size whether there exists some mixed 
strategy σr that places positive probability only on strategies 
in dr and dominates σ∗ 
r both for strict and for weak 
dominance 
proof let pdr be the probability that σr places on dr ∈ 
dr we will solve a single linear program in each of our 
algorithms linear programs can be solved in polynomial 
time for strict dominance the question is whether the 
pdr can be set so that for every pure strategy for the column 
player σc ∈ σc 
dr∈dr 
pdr ur dr σc ur σ∗ 
r σc because 
the inequality must be strict we cannot solve this directly 
by linear programming we proceed as follows because the 
game is finite we may assume without loss of generality that 
all utilities are positive if not simply add a constant to all 
utilities solve the following linear program 
minimize 
dr∈dr 
pdr 
such that 
for any σc ∈ σc 
dr∈dr 
pdr ur dr σc ≥ ur σ∗ 
r σc 
if σ∗ 
r is strictly dominated by some mixed strategy this 
linear program has a solution with objective value 
 the dominating strategy is a feasible solution with 
objective value exactly because no constraint is binding for this 
solution we can reduce one of the probabilities slightly 
without affecting feasibility thereby obtaining a solution with 
objective value moreover if this linear program has a 
solution with objective value there is a mixed strategy 
strictly dominating σ∗ 
r which can be obtained by taking the 
lp solution and adding the remaining probability to any 
strategy because all the utilities are positive this will add 
to the left side of any inequality so all inequalities will 
become strict 
for weak dominance we can solve the following linear 
program 
maximize 
σc∈σc 
 
dr∈dr 
pdr ur dr σc − ur σ∗ 
r σc 
such that 
for any σc ∈ σc 
dr∈dr 
pdr ur dr σc ≥ ur σ∗ 
r σc 
dr∈dr 
pdr 
if σ∗ 
r is weakly dominated by some mixed strategy then 
that mixed strategy is a feasible solution to this program 
with objective value because for at least one strategy 
σc ∈ σc we have 
dr∈dr 
pdr ur dr σc − ur σ∗ 
r σc on 
the other hand if this program has a solution with 
objective value then for at least one strategy σc ∈ σc we 
 
recall that the assumption of a single opponent that is 
the assumption of two players is without loss of generality 
for one-shot dominance 
must have 
dr∈dr 
pdr ur dr σc − ur σ∗ 
r σc and thus 
the linear program s solution is a weakly dominating mixed 
strategy 
 iterated dominance 
we now move on to iterated dominance it is well-known 
that iterated strict dominance is path-independent 
 that is if we remove dominated strategies until no more 
dominated strategies remain in the end the remaining 
strategies for each player will be the same regardless of the 
order in which strategies are removed because of this to 
see whether a given strategy can be eliminated by iterated 
strict dominance all that needs to be done is to 
repeatedly remove strategies that are strictly dominated until no 
more dominated strategies remain because we can check in 
polynomial time whether any given strategy is dominated 
 whether or not dominance by mixed strategies is allowed 
as described in section this whole procedure takes only 
polynomial time in the case of iterated dominance by pure 
strategies with two players knuth et al slightly 
improve on speed up the straightforward implementation of 
this procedure by keeping track of for each ordered pair of 
strategies for a player the number of opponent strategies 
that prevent the first strategy from dominating the second 
hereby the runtime for an m × n game is reduced from 
o m n 
 to o m n 
 actually they only study 
very weak dominance for which no strict inequalities are 
required but the approach is easily extended 
in contrast iterated weak dominance is known to be 
pathdependent 
for example in the following game using 
iterated weak dominance we can eliminate m first and then 
d or r first and then u 
l m r 
u 
d 
therefore while the procedure of removing weakly 
dominated strategies until no more weakly dominated strategies 
remain can certainly be executed in polynomial time which 
strategies survive in the end depends on the order in which 
we remove the dominated strategies we will investigate 
two questions for iterated weak dominance whether a given 
strategy is eliminated in some path and whether there is a 
path to a unique solution one pure strategy left per player 
we will show that both of these problems are 
computationally hard 
definition given a game in normal form and a 
distinguished strategy σ∗ 
 iwd-strategy-elimination 
asks whether there is some path of iterated weak dominance 
that eliminates σ∗ 
 given a game in normal form 
iwdunique-solution asks whether there is some path of 
iterated weak dominance that leads to a unique solution one 
strategy left per player 
the following lemma shows a special case of normal form 
games in which allowing for weak dominance by mixed 
strategies in addition to weak dominance by pure strategies does 
 
there is however a restriction of weak dominance called 
nice weak dominance which is path-independent for 
an overview of path-independence results see apt 
 
not help we will prove the hardness results in this setting 
so that they will hold whether or not dominance by mixed 
strategies is allowed 
lemma suppose that all the utilities in a game are in 
{ } then every pure strategy that is weakly dominated by 
a mixed strategy is also weakly dominated by a pure strategy 
proof suppose pure strategy σ is weakly dominated by 
mixed strategy σ∗ 
 if σ gets a utility of against some 
opponent strategy or vector of opponent strategies if there 
are more than players then all the pure strategies that 
σ∗ 
places positive probability on must also get a utility of 
against that opponent strategy or else the expected utility 
would be smaller than moreover at least one of the 
pure strategies that σ∗ 
places positive probability on must 
get a utility of against an opponent strategy that σ gets 
 against or else the inequality would never be strict it 
follows that this pure strategy weakly dominates σ 
we are now ready to prove the main results of this section 
theorem iwd-strategy-elimination is 
npcomplete even with players and with and being the 
only utilities occurring in the matrix-whether or not 
dominance by mixed strategies is allowed 
proof the problem is in np because given a sequence 
of strategies to be eliminated we can easily check whether 
this is a valid sequence of eliminations even when 
dominance by mixed strategies is allowed using proposition 
to show that the problem is np-hard we reduce an 
arbitrary satisfiability instance given by a nonempty set of 
clauses c over a nonempty set of variables v with 
corresponding literals l { v v ∈ v } ∪ {−v v ∈ v } to the 
following iwd-strategy-elimination instance in 
this instance we will specify that certain strategies are 
uneliminable a strategy σr can be made uneliminable even 
when and are the only allowed utilities by adding 
another strategy σr and another opponent strategy σc so that 
 σr and σr are the only strategies that give the row player 
a utility of against σc σr and σr always give the row 
player the same utility σc is the only strategy that gives 
the column player a utility of against σr but otherwise 
σc always gives the column player utility this makes it 
impossible to eliminate any of these three strategies we 
will not explicitly specify the additional strategies to make 
the proof more legible 
in this proof we will denote row player strategies by s and 
column player strategies by t to improve legibility let the 
row player s pure strategy set be given as follows for every 
variable v ∈ v the row player has corresponding strategies 
s 
 v s 
 v s 
−v s 
−v additionally the row player has the 
following strategies s 
 and s 
 where s 
 σ∗ 
r that is it is 
the strategy we seek to eliminate finally for every clause 
c ∈ c the row player has corresponding strategies s 
c 
 uneliminable and s 
c let the column player s pure strategy set 
be given as follows for every variable v ∈ v the column 
player has a corresponding strategy tv for every clause 
c ∈ c the column player has a corresponding strategy tc 
and additionally for every literal l ∈ l that occurs in c a 
strategy tc l for every variable v ∈ v the column player 
has corresponding strategies t v t−v both uneliminable 
finally the column player has three additional strategies 
t 
 uneliminable t 
 and t 
the utility function for the row player is given as follows 
 ur s 
 v tv for all v ∈ v 
 ur s 
 v tv for all v ∈ v 
 ur s 
−v tv for all v ∈ v 
 ur s 
−v tv for all v ∈ v 
 ur s 
 v t for all v ∈ v 
 ur s 
 v t for all v ∈ v 
 ur s 
−v t for all v ∈ v 
 ur s 
−v t for all v ∈ v 
 ur sb 
 v t v for all v ∈ v and b ∈ { } 
 ur sb 
−v t−v for all v ∈ v and b ∈ { } 
 ur sl t otherwise for all l ∈ l and t ∈ s 
 ur s 
 tc for all c ∈ c 
 ur s 
 tc for all c ∈ c 
 ur sb 
 t 
 for all b ∈ { } 
 ur s 
 t 
 
 ur s 
 t 
 
 ur sb 
 t otherwise for all b ∈ { } and t ∈ s 
 ur sb 
c t otherwise for all c ∈ c and b ∈ { } 
and the row player s utility is in every other case the 
utility function for the column player is given as follows 
 uc s tv for all v ∈ v and s ∈ s 
 uc s t for all s ∈ s 
 uc s 
l tc for all c ∈ c and l ∈ l where l ∈ c 
 literal l occurs in clause c 
 uc s 
l 
 tc l for all c ∈ c and l l ∈ l l l 
where l ∈ c 
 uc s 
c tc for all c ∈ c 
 uc s 
c tc for all c ∈ c 
 uc sb 
c tc l for all c ∈ c l ∈ l and b ∈ { } 
 uc s tc uc s tc l otherwise for all c ∈ c and 
l ∈ l 
and the column player s utility is in every other case we 
now show that the two instances are equivalent 
first suppose there is a solution to the satisfiability 
instance that is a truth-value assignment to the variables in 
v such that all clauses are satisfied then consider the 
following sequence of eliminations in our game for every 
variable v that is set to true in the assignment eliminate 
tv which gives the column player utility everywhere 
then for every variable v that is set to true in the 
assignment eliminate s 
 v using s 
 v which is possible because tv 
has been eliminated and because t has not been eliminated 
 yet now eliminate t which gives the column player 
utility everywhere next for every variable v that is set 
to false in the assignment eliminate s 
−v using s 
−v which 
is possible because t has been eliminated and because tv 
has not been eliminated yet for every clause c which 
has the variable corresponding to one of its positive literals 
l v set to true in the assignment eliminate tc using tc l 
 which is possible because s 
l has been eliminated and s 
c 
has not been eliminated yet for every clause c which 
has the variable corresponding to one of its negative literals 
l −v set to false in the assignment eliminate tc using tc l 
 
 which is possible because s 
l has been eliminated and s 
c has 
not been eliminated yet because the assignment 
satisfied the formula all the tc have now been eliminated thus 
we can eliminate s 
 σ∗ 
r using s 
 it follows that there is a 
solution to the iwd-strategy-elimination instance 
now suppose there is a solution to the 
iwd-strategyelimination instance by lemma we can assume that 
all the dominances are by pure strategies we first observe 
that only s 
 can eliminate s 
 σ∗ 
r because it is the only 
other strategy that gets the row player a utility of against 
t 
 and t 
 is uneliminable however because s 
 performs 
better than s 
 against the tc strategies it follows that all of 
the tc strategies must be eliminated for each c ∈ c the 
strategy tc can only be eliminated by one of the strategies tc l 
 with the same c because these are the only other strategies 
that get the column player a utility of against s 
c and s 
c is 
uneliminable but in order for some tc l to eliminate tc s 
l 
must be eliminated first only s 
l can eliminate s 
l because 
it is the only other strategy that gets the row player a utility 
of against tl and tl is uneliminable we next show that for 
every v ∈ v only one of s 
 v s 
−v can be eliminated this 
is because in order for s 
 v to eliminate s 
 v tv needs to 
have been eliminated and t not so tv must be eliminated 
before t but in order for s 
−v to eliminate s 
−v t needs to 
have been eliminated and tv not so t must be eliminated 
before tv so set v to true if s 
 v is eliminated and to false 
otherwise because by the above for every clause c one of 
the s 
l with l ∈ c must be eliminated it follows that this is 
a satisfying assignment to the satisfiability instance 
using theorem it is now relatively easy to show that 
iwd-unique-solution is also np-complete under the 
same restrictions 
theorem iwd-unique-solution is np-complete 
even with players and with and being the only 
utilities occurring in the matrix-whether or not dominance by 
mixed strategies is allowed 
proof again the problem is in np because we can 
nondeterministically choose the sequence of eliminations and 
verify whether it is correct to show np-hardness we reduce 
an arbitrary iwd-strategy-elimination instance to 
the following iwd-unique-solution instance let all 
the strategies for each player from the original instance 
remain part of the new instance and let the utilities resulting 
from the players playing a pair of these strategies be the 
same we add three additional strategies σ 
r σ 
r σ 
r for the 
row player and three additional strategies σ 
c σ 
c σ 
c for the 
column player let the additional utilities be as follows 
 ur σr σj 
c for all σr ∈ {σ 
r σ 
r σ 
r } and j ∈ { } 
 ur σi 
r σc for all i ∈ { } and σc ∈ {σ 
c σ 
c } 
 ur σi 
r σ 
c for all i ∈ { } 
 ur σ 
r σ 
c 
 and the row player s utility is in all other cases 
involving a new strategy 
 uc σ 
r σc for all σc ∈ {σ 
c σ 
c σ 
c } 
 uc σ∗ 
r σj 
c for all j ∈ { } σ∗ 
r is the strategy to 
be eliminated in the original instance 
 uc σi 
r σ 
c for all i ∈ { } 
 ur σ 
r σ 
c 
 ur σ 
r σ 
c 
 and the column player s utility is in all other cases 
involving a new strategy 
we proceed to show that the two instances are equivalent 
first suppose there exists a solution to the original 
iwdstrategy-elimination instance then perform the 
same sequence of eliminations to eliminate σ∗ 
r in the new 
iwd-unique-solution instance this is possible 
because at any stage any weak dominance for the row player 
in the original instance is still a weak dominance in the new 
instance because the two strategies utilities for the row 
player are the same when the column player plays one of the 
new strategies and the same is true for the column player 
once σ∗ 
r is eliminated let σ 
c eliminate σ 
c it performs 
better against σ 
r then let σ 
r eliminate all the other 
remaining strategies for the row player it always performs 
better against either σ 
c or σ 
c finally σ 
c is the unique best 
response against σ 
r among the column player s remaining 
strategies so let it eliminate all the other remaining 
strategies for the column player thus there exists a solution to 
the iwd-unique-solution instance 
now suppose there exists a solution to the 
iwd-uniquesolution instance by lemma we can assume that all 
the dominances are by pure strategies we will show that 
none of the new strategies σ 
r σ 
r σ 
r σ 
c σ 
c σ 
c can either 
eliminate another strategy or be eliminated before σ∗ 
r is 
eliminated thus there must be a sequence of eliminations 
ending in the elimination of σ∗ 
r which does not involve any of 
the new strategies and is therefore a valid sequence of 
eliminations in the original game because all original strategies 
perform the same against each new strategy we now show 
that this is true by exhausting all possibilities for the first 
elimination before σ∗ 
r is eliminated that involves a new 
strategy none of the σi 
r can be eliminated by a σr ∈ {σ 
r σ 
r σ 
r } 
because the σi 
r perform better against σ 
c σ 
r cannot 
eliminate any other strategy because it always performs poorer 
against σ 
c σ 
r and σ 
r are equivalent from the row player s 
perspective and thus cannot eliminate each other and 
cannot eliminate any other strategy because they always 
perform poorer against σ 
c none of the σj 
c can be eliminated 
by a σc ∈ {σ 
c σ 
c σ 
c } because the σj 
c always perform 
better against either σ 
r or σ 
r σ 
c cannot eliminate any other 
strategy because it always performs poorer against either 
σ∗ 
r or σ 
r σ 
c cannot eliminate any other strategy because 
it always performs poorer against σ 
r or σ 
r σ 
c cannot 
eliminate any other strategy because it always performs poorer 
against σ 
r or σ 
r thus there exists a solution to the 
iwdstrategy-elimination instance 
a slightly weaker version of the part of theorem 
concerning dominance by pure strategies only is the main result 
of gilboa et al besides not proving the result for 
dominance by mixed strategies the original result was weaker 
because it required utilities { } rather than 
just { } and because of this our lemma cannot be 
applied to it to get the result for mixed strategies 
 iterated dominance using 
mixed strategies with small 
supports 
when showing that a strategy is dominated by a mixed 
strategy there are several reasons to prefer exhibiting a 
 
dominating strategy that places positive probability on as 
few pure strategies as possible first this will reduce the 
number of bits required to specify the dominating 
strategy and thus the proof of dominance can be communicated 
quicker if the dominating mixed strategy places positive 
probability on only k strategies then it can be specified 
using k real numbers for the probabilities plus k log m where 
m is the number of strategies for the player under 
consideration bits to indicate which strategies are used second 
the proof of dominance will be cleaner for a dominating 
mixed strategy it is typically always in the case of strict 
dominance possible to spread some of the probability onto 
any unused pure strategy and still have a dominating 
strategy but this obscures which pure strategies are the ones that 
are key in making the mixed strategy dominating third 
because by the previous the argument for eliminating the 
dominated strategy is simpler and easier to understand it is 
more likely to be accepted fourth the level of risk 
neutrality required for the argument to work is reduced at least in 
the extreme case where dominance by a single pure strategy 
can be exhibited no risk neutrality is required here 
this motivates the following problem 
definition minimum-dominating-set we 
are given the row player s utilities of a game in normal form 
a distinguished strategy σ∗ 
for the row player a specification 
of whether the dominance should be strict or weak and a 
number k we are asked whether there exists a mixed 
strategy σ for the row player that places positive probability on 
at most k pure strategies and dominates σ∗ 
in the required 
sense 
unfortunately this problem is np-complete 
theorem minimum-dominating-set is 
npcomplete both for strict and for weak dominance 
proof the problem is in np because we can 
nondeterministically choose a set of at most k strategies to give 
positive probability and decide whether we can dominate 
σ∗ 
with these k strategies as described in proposition to 
show np-hardness we reduce an arbitrary set-cover 
instance given a set s subsets s s sr and a number 
t can all of s be covered by at most t of the subsets 
to the following minimum-dominating-set instance 
for every element s ∈ s there is a pure strategy σs for 
the column player for every subset si there is a pure 
strategy σsi for the row player finally there is the 
distinguished pure strategy σ∗ 
for the row player the row 
player s utilities are as follows ur σsi σs t if s ∈ si 
ur σsi σs if s ∈ si ur σ∗ 
 σs for all s ∈ s 
finally we let k t we now proceed to show that the two 
instances are equivalent 
first suppose there exists a solution to the set-cover 
instance without loss of generality we can assume that 
there are exactly k subsets in the cover then for every 
si that is in the cover let the dominating strategy σ place 
exactly 
k 
probability on the corresponding pure strategy 
σsi now if we let n s be the number of subsets in the cover 
containing s we observe that that n s ≥ then for every 
strategy σs for the column player the row player s expected 
utility for playing σ when the column player is playing σs is 
u σ σs n s 
k 
 k ≥ k 
k 
 u σ∗ 
 σs so σ strictly 
 and thus also weakly dominates σ∗ 
 and there exists a 
solution to the minimum-dominating-set instance 
now suppose there exists a solution to the 
minimumdominating-set instance consider the at most k 
pure strategies of the form σsi on which the dominating 
mixed strategy σ places positive probability and let t be 
the collection of the corresponding subsets si we claim that 
t is a cover for suppose there is some s ∈ s that is not in 
any of the subsets in t then if the column player plays σs 
the row player when playing σ will always receive utility 
 -as opposed to the utility of the row player would receive 
for playing σ∗ 
 contradicting the fact that σ dominates σ∗ 
 whether this dominance is weak or strict it follows that 
there exists a solution to the set-cover instance 
on the other hand if we require that the dominating 
strategy only places positive probability on a very small 
number of pure strategies then it once again becomes easy 
to check whether a strategy is dominated specifically to 
find out whether player i s strategy σ∗ 
is dominated by 
a strategy that places positive probability on only k pure 
strategies we can simply check for every subset of k of 
player i s pure strategies whether there is a strategy that 
places positive probability only on these k strategies and 
dominates σ∗ 
 using proposition this requires only 
o σi k 
 such checks thus if k is a constant this 
constitutes a polynomial-time algorithm 
a natural question to ask next is whether iterated strict 
dominance remains computationally easy when dominating 
strategies are required to place positive probability on at 
most k pure strategies where k is a small constant we 
have already shown in section that iterated weak 
dominance is hard even when k that is only dominance by 
pure strategies is allowed of course if iterated strict 
dominance were path-independent under this restriction 
computational easiness would follow as it did in section 
however it turns out that this is not the case 
observation if we restrict the dominating strategies 
to place positive probability on at most two pure strategies 
iterated strict dominance becomes path-dependent 
proof consider the following game 
 
 
 
 
 
let i j denote the outcome in which the row player plays 
the ith row and the column player plays the jth column 
because and are all nash equilibria none 
of the column player s pure strategies will ever be eliminated 
and neither will rows and we now observe that 
randomizing uniformly over rows and dominates row 
and randomizing uniformly over rows and dominates row 
 however if we eliminate row first it becomes impossible 
to dominate row without randomizing over at least pure 
strategies 
indeed iterated strict dominance turns out to be hard 
even when k 
theorem if we restrict the dominating strategies to 
place positive probability on at most three pure strategies it 
becomes np-complete to decide whether a given strategy can 
be eliminated using iterated strict dominance 
 
proof the problem is in np because given a sequence 
of strategies to be eliminated we can check in polynomial 
time whether this is a valid sequence of eliminations for any 
strategy to be eliminated we can check for every subset of 
three other strategies whether there is a strategy placing 
positive probability on only these three strategies that 
dominates the strategy to be eliminated using proposition 
to show that the problem is np-hard we reduce an 
arbitrary satisfiability instance given by a nonempty set of 
clauses c over a nonempty set of variables v with 
corresponding literals l { v v ∈ v } ∪ {−v v ∈ v } to the 
following two-player game 
for every variable v ∈ v the row player has strategies 
s v s−v s 
v s 
v s 
v s 
v and the column player has strategies 
t 
v t 
v t 
v t 
v for every clause c ∈ c the row player has a 
strategy sc and the column player has a strategy tc as well 
as for every literal l occurring in c an additional strategy 
tl 
c the row player has two additional strategies s and s 
 s is the strategy that we are seeking to eliminate finally 
the column player has one additional strategy t 
the utility function for the row player is given as follows 
 where is some sufficiently small number 
 ur s v tj 
v if j ∈ { } for all v ∈ v 
 ur s v tj 
v if j ∈ { } for all v ∈ v 
 ur s−v tj 
v if j ∈ { } for all v ∈ v 
 ur s−v tj 
v if j ∈ { } for all v ∈ v 
 ur s v t ur s−v t for all v ∈ v and t ∈ 
{t 
v t 
v t 
v t 
v} 
 ur si 
v ti 
v for all v ∈ v and i ∈ { } 
 ur si 
v t for all v ∈ v i ∈ { } and t ti 
v 
 ur sc tc for all c ∈ c 
 ur sc t for all c ∈ c and t tc 
 ur s t 
 ur s t for all t t 
 ur s t 
 ur s tc for all c ∈ c 
 ur s t for all t ∈ {t } ∪ {tc c ∈ c} 
the utility function for the column player is given as 
follows 
 uc si 
v ti 
v for all v ∈ v and i ∈ { } 
 uc s ti 
v for all v ∈ v i ∈ { } and s si 
v 
 uc sc tc for all c ∈ c 
 uc sl tc for all c ∈ c and l ∈ l occurring in c 
 uc s tc for all c ∈ c and s ∈ {sc} ∪ {sl l ∈ c} 
 uc sc tl 
c for all c ∈ c 
 uc sl tl 
c for all c ∈ c and l l occurring in 
c 
 uc s tl 
c for all c ∈ c and s ∈ {sc} ∪ {sl l ∈ 
c l l } 
 uc s t 
 uc s t for all s s 
we now show that the two instances are equivalent first 
suppose that there is a solution to the satisfiability instance 
then consider the following sequence of eliminations in our 
game for every variable v that is set to true in the 
satisfying assignment eliminate s v with the mixed strategy σr 
that places probability on s−v probability on s 
v 
and probability on s 
v the expected utility of 
playing σr against t 
v or t 
v is against t 
v or t 
v it is 
 and against anything else it is hence the 
dominance is valid similarly for every variable v that 
is set to false in the satisfying assignment eliminate s−v 
with the mixed strategy σr that places probability on 
s v probability on s 
v and probability on s 
v the 
expected utility of playing σr against t 
v or t 
v is 
against t 
v or t 
v it is and against anything else it 
is hence the dominance is valid for every 
c ∈ c eliminate tc with any tl 
c for which l was set to true 
in the satisfying assignment this is a valid dominance 
because tl 
c performs better than tc against any strategy other 
than sl and we eliminated sl in step or in step 
finally eliminate s with s this is a valid dominance 
because s performs better than s against any strategy other 
than those in {tc c ∈ c} which we eliminated in step 
hence there is an elimination path that eliminates s 
now suppose that there is an elimination path that 
eliminates s the strategy that eventually dominates s must 
place most of its probability on s because s is the only 
other strategy that performs well against t which cannot 
be eliminated before s but s performs significantly worse 
than s against any strategy tc with c ∈ c so it follows that 
all these strategies must be eliminated first each strategy 
tc can only be eliminated by a strategy that places most 
of its weight on the corresponding strategies tl 
c with l ∈ c 
because they are the only other strategies that perform well 
against sc which cannot be eliminated before tc but each 
strategy tl 
c performs significantly worse than tc against sl 
so it follows that for every clause c for one of the literals 
l occurring in it sl must be eliminated first now 
strategies of the form tj 
v will never be eliminated because they are 
the unique best responses to the corresponding strategies sj 
v 
 which are in turn the best responses to the corresponding 
tj 
v as a result if strategy s v respectively s−v is 
eliminated then its opposite strategy s−v respectively s v can 
no longer be eliminated for the following reason there is no 
other pure strategy remaining that gets a significant utility 
against more than one of the strategies t 
v t 
v t 
v t 
v but s−v 
 respectively s v gets significant utility against all and 
therefore cannot be dominated by a mixed strategy placing 
positive probability on at most strategies it follows that 
for each v ∈ v at most one of the strategies s v s−v is 
eliminated in such a way that for every clause c for one 
of the literals l occurring in it sl must be eliminated but 
then setting all the literals l such that sl is eliminated to 
true constitutes a solution to the satisfiability instance 
in the next section we return to the setting where there 
is no restriction on the number of pure strategies on which 
a dominating mixed strategy can place positive probability 
 iterated dominance in 
bayesian games 
so far we have focused on normal form games that are 
flatly represented that is every matrix entry is given 
ex 
plicitly however for many games the flat representation 
is too large to write down explicitly and instead some 
representation that exploits the structure of the game needs to 
be used bayesian games besides being of interest in their 
own right can be thought of as a useful structured 
representation of normal form games and we will study them in 
this section 
in a bayesian game each player first receives privately 
held preference information the player s type from a 
distribution which determines the utility that that player receives 
for every outcome of that is vector of actions played in the 
game after receiving this type the player plays an action 
based on it 
definition a bayesian game is given by a set of 
players { n} and for each player i a finite set of 
actions ai a finite type space θi with a probability 
distribution πi over it and a utility function ui θi × a × a × 
 × an → r where ui θi a a an denotes player 
i s utility when i s type is θi and each player j plays action 
aj a pure strategy in a bayesian game is a mapping from 
types to actions σi θi → ai where σi θi denotes the 
action that player i plays for type θi 
any vector of pure strategies in a bayesian game defines 
an expected utility for each player and therefore we can 
translate a bayesian game into a normal form game in this 
normal form game the notions of dominance and iterated 
dominance are defined as before however the normal form 
representation of the game is exponentially larger than the 
bayesian representation because each player i has ai θi 
distinct pure strategies thus any algorithm for bayesian 
games that relies on expanding the game to its normal form 
will require exponential time specifically our easiness 
results for normal form games do not directly transfer to this 
setting in fact it turns out that checking whether a 
strategy is dominated by a pure strategy is hard in bayesian 
games 
theorem in a bayesian game it is np-complete to 
decide whether a given pure strategy σr θr → ar is 
dominated by some other pure strategy both for strict and weak 
dominance even when the row player s distribution over 
types is uniform 
proof the problem is in np because it is easy to verify 
whether a candidate dominating strategy is indeed a 
dominating strategy to show that the problem is np-hard we 
reduce an arbitrary satisfiability instance given by a set of 
clauses c using variables from v to the following bayesian 
game let the row player s action set be ar {t f } and 
let the column player s action set be ac {ac c ∈ c} 
let the row player s type set be θr {θv v ∈ v } with a 
distribution πr that is uniform let the row player s utility 
function be as follows 
 ur θv ac for all v ∈ v and c ∈ c 
 ur θv b ac v for all v ∈ v c ∈ c and b ∈ {t f} 
such that setting v to b satisfies c 
 ur θv b ac − for all v ∈ v c ∈ c and b ∈ {t f} 
such that setting v to b does not satisfy c 
 
in general a player can also receive a signal about the other 
players preferences but we will not concern ourselves with 
that here 
let the pure strategy to be dominated be the one that 
plays for every type we show that the strategy is 
dominated by a pure strategy if and only if there is a solution to 
the satisfiability instance 
first suppose there is a solution to the satisfiability 
instance then let σd 
r be given by σd 
r θv t if v is set 
to true in the solution to the satisfiability instance and 
σd 
r θv f otherwise then against any action ac by the 
column player there is at least one type θv such that either 
 v ∈ c and σd 
r θv t or −v ∈ c and σd 
r θv f thus 
the row player s expected utility against action ac is at least 
 v 
 v 
− v − 
 v 
 
 v 
 so σd 
r is a dominating strategy 
now suppose there is a dominating pure strategy σd 
r 
this dominating strategy must play t or f for at least one 
type thus against any ac by the column player there must 
at least be some type θv for which ur θv σd 
r θv ac 
that is there must be at least one variable v such that 
setting v to σd 
r θv satifies c but then setting each v to σd 
r θv 
must satisfy all the clauses so a satisfying assignment 
exists 
however it turns out that we can modify the linear 
programs from proposition to obtain a polynomial time 
algorithm for checking whether a strategy is dominated by a 
mixed strategy in bayesian games 
theorem in a bayesian game it can be decided in 
polynomial time whether a given possibly mixed strategy 
σr is dominated by some other mixed strategy using linear 
programming both for strict and weak dominance 
proof we can modify the linear programs presented in 
proposition as follows for strict dominance again 
assuming without loss of generality that all the utilities in 
the game are positive use the following linear program in 
which pσr 
r θr ar is the probability that σr the strategy to 
be dominated places on ar for type θr 
minimize 
θr∈θr ar∈ar 
pr ar 
such that 
for any ac ∈ ac 
θr∈θr ar∈ar 
π θr ur θr ar ac pr θr ar ≥ 
θr∈θr ar∈ar 
π θr ur θr ar ac pσr 
r θr ar 
for any θr ∈ θr 
ar∈ar 
pr θr ar ≤ 
assuming that π θr for all θr ∈ θr this program 
will return an objective value smaller than θr if and only 
if σr is strictly dominated by reasoning similar to that done 
in proposition 
for weak dominance use the following linear program 
maximize 
ac∈ac 
 
θr∈θr ar∈ar 
π θr ur θr ar ac pr θr ar − 
θr∈θr ar∈ar 
π θr ur θr ar ac pσr 
r θr ar 
such that 
for any ac ∈ ac 
θr∈θr ar∈ar 
π θr ur θr ar ac pr θr ar ≥ 
θr∈θr ar∈ar 
π θr ur θr ar ac pσr 
r θr ar 
for any θr ∈ θr 
ar∈ar 
pr θr ar 
this program will return an objective value greater than 
 if and only if σr is weakly dominated by reasoning similar 
to that done in proposition 
we now turn to iterated dominance in bayesian games 
na¨ıvely one might argue that iterated dominance in bayesian 
 
games always requires an exponential number of steps when 
a significant fraction of the game s pure strategies can be 
eliminated because there are exponentially many pure 
strategies however this is not a very strong argument because 
oftentimes we can eliminate exponentially many pure 
strategies in one step for example if for some type θr ∈ θr we 
have for all ac ∈ ac that u θr a 
r ac u θr a 
r ac then 
any pure strategy for the row player which plays action a 
r 
for type θr is dominated by the strategy that plays 
action a 
r for type θr instead -and there are exponentially 
many ar θr − 
 such strategies it is therefore 
conceivable that we need only polynomially many eliminations of 
collections of a player s strategies however the following 
theorem shows that this is not the case by giving an 
example where an exponential number of iterations that is 
alternations between the players in eliminating strategies 
is required we emphasize that this is not a result about 
computational complexity 
theorem even in symmetric -player bayesian games 
iterated dominance by pure strategies can require an 
exponential number of iterations both for strict and weak 
dominance even with only three actions per player 
proof let each player i ∈ { } have n types 
θ 
i θ 
i θn 
i let each player i have actions ai bi ci 
and let the utility function of each player be defined as 
follows in the below i and i are shorthand for 
i mod and i mod when used as player indices 
also −∞ can be replaced by a sufficiently negative 
number finally δ and should be chosen to be very small even 
compared to − n 
 and should be more than twice as 
large as δ 
 ui θ 
i ai ci ci − 
 ui θ 
i ai si si for si ci or si ci 
 ui θ 
i bi si si − for si ai and si 
ai 
 ui θ 
i bi si si −∞ for si ai or si 
ai 
 ui θ 
i ci si si −∞ for all si si 
 ui θj 
i ai si si −∞ for all si si when j 
 
 ui θj 
i bi si si − for all si si when j 
 ui θj 
i ci si ci δ − − for all si when 
j 
 ui θj 
i ci si si δ− for all si and si ci 
when j 
let the distribution over each player s types be given by 
p θj 
i −j 
 with the exception that p θ 
i − 
 − n 
 
we will be interested in eliminating strategies of the 
following form play bi for type θ 
i and play one of bi or ci 
otherwise because the utility function is the same for any 
type θj 
i with j these strategies are effectively defined 
by the total probability that they place on ci 
which is 
t 
i − 
 − n 
 n 
j tj 
i −j 
where tj 
i if player i 
 
note that the strategies are still pure strategies the 
probability placed on an action by a strategy here is simply the 
sum of the probabilities of the types for which the strategy 
chooses that action 
plays ci for type θj 
i and otherwise this probability is 
different for any two different strategies of the given form 
and we have exponentially many different strategies of the 
given form for any probability q which can be expressed 
as t − 
 − n 
 n 
j tj −j 
 with all tj ∈ { } 
let σi q denote the unique strategy of the given form for 
player i which places a total probability of q on ci any 
strategy that plays ci for type θ 
i or ai for some type θj 
i 
with j can immediately be eliminated we will show 
that after that we must eliminate the strategies σi q with 
high q first slowly working down to those with lower q 
claim if σi q and σi q have not yet been 
eliminated and q q then σi q cannot yet be eliminated 
proof first we show that no strategy σi q can 
eliminate σi q against σi q σi q the utility of 
playing σi p is − p · δ − p · q thus when q it is 
best to set p as high as possible and we note that σi 
and σi have not been eliminated but when q it 
is best to set p as low as possible because δ q thus 
whether q q or q q σi q will always do strictly 
better than σi q against some remaining opponent 
strategies hence no strategy σi q can eliminate σi q the 
only other pure strategies that could dominate σi q are 
strategies that play ai for type θ 
i and bi or ci for all other 
types let us take such a strategy and suppose that it plays 
c with probability p against σi q σi q which have 
not yet been eliminated the utility of playing this 
strategy is − q 
 − p · δ − p · q on the other hand 
playing σi q gives − q · δ − q · q because q q we 
have − q 
 −q · q and because δ and are small 
it follows that σi q receives a higher utility therefore no 
strategy dominates σi q proving the claim 
claim if for all q q σi q and σi q have been 
eliminated then σi q can be eliminated proof consider 
the strategy for player i that plays ai for type θ 
i and bi for 
all other types call this strategy σi we claim σi dominates 
σi q first if either of the other players k plays ak for θ 
k 
then σi performs better than σi q which receives −∞ in 
some cases because the strategies for player k that play 
ck for type θ 
k or ak for some type θj 
k with j have 
already been eliminated all that remains to check is that σi 
performs better than σi q whenever both of the other two 
players play strategies of the following form play bk for type 
θ 
k and play one of bk or ck otherwise we note that among 
these strategies there are none left that place probability 
greater than q on ck letting qk denote the probability with 
which player k plays ck the expected utility of playing σi 
is −qi · qi − on the other hand the utility of 
playing σi q is − q · δ − q · qi because qi ≤ q the 
difference between these two expressions is at least − δ 
which is positive it follows that σi dominates σi q 
from claim it follows that all strategies of the form 
σi q will eventually be eliminated however claim shows 
that we cannot go ahead and eliminate multiple such 
strategies for one player unless at least one other player 
simultaneously keeps up in the eliminated strategies every time 
a σi q is eliminated such that σi q and σi q have 
not yet been eliminated we need to eliminate one of the 
latter two strategies before any σi q with q q can be 
eliminated-that is we need to alternate between players 
because there are exponentially many strategies of the form 
σi q it follows that iterated elimination will require 
exponentially many iterations to complete 
 
it follows that an efficient algorithm for iterated 
dominance strict or weak by pure strategies in bayesian games 
if it exists must somehow be able to perform at least part 
of many iterations in a single step of the algorithm because 
if each step only performed a single iteration we would need 
exponentially many steps interestingly knuth et al 
argue that iterated dominance appears to be an inherently 
sequential problem in light of their result that iterated very 
weak dominance is p-complete that is apparently not 
efficiently parallelizable suggesting that aggregating many 
iterations may be difficult 
 conclusions 
while the nash equilibrium solution concept is studied 
more and more intensely in our community the perhaps 
more elementary concept of iterated dominance has 
received much less attention in this paper we studied various 
computational aspects of this concept 
we first studied both strict and weak dominance not 
iterated and showed that checking whether a given strategy 
is dominated by some mixed strategy can be done in 
polynomial time using a single linear program solve we then 
moved on to iterated dominance we showed that 
determining whether there is some path that eliminates a given 
strategy is np-complete with iterated weak dominance this 
allowed us to also show that determining whether there is a 
path that leads to a unique solution is np-complete both 
of these results hold both with and without dominance by 
mixed strategies a weaker version of the second result 
 only without dominance by mixed strategies was already 
known iterated strict dominance on the other hand 
is path-independent both with and without dominance by 
mixed strategies and can therefore be done in polynomial 
time 
we then studied what happens when the dominating 
strategy is allowed to place positive probability on only a few pure 
strategies first we showed that finding the dominating 
strategy with minimum support size is np-complete both 
for strict and weak dominance then we showed that 
iterated strict dominance becomes path-dependent when there 
is a limit on the support size of the dominating strategies 
and that deciding whether a given strategy can be 
eliminated by iterated strict dominance under this restriction is 
np-complete even when the limit on the support size is 
finally we studied dominance and iterated dominance in 
bayesian games as an example of a concise representation 
language for normal form games that is interesting in its own 
right we showed that unlike in normal form games 
deciding whether a given pure strategy is dominated by another 
pure strategy in a bayesian game is np-complete both with 
strict and weak dominance however deciding whether a 
strategy is dominated by some mixed strategy can still be 
done in polynomial time with a single linear program solve 
 both with strict and weak dominance finally we showed 
that iterated dominance using pure strategies can require an 
exponential number of iterations in a bayesian game both 
with strict and weak dominance 
there are various avenues for future research first there 
is the open question of whether it is possible to complete 
iterated dominance in bayesian games in polynomial time 
 even though we showed that an exponential number of 
alternations between the players in eliminating strategies is 
sometimes required second we can study computational 
aspects of iterated dominance in concise representations 
of normal form games other than bayesian games-for 
example in graphical games or local-effect action graph 
games how to efficiently perform iterated very weak 
dominance has already been studied for partially observable 
stochastic games finally we can ask whether some of 
the algorithms we described such as the one for iterated 
strict dominance with mixed strategies can be made faster 
 references 
 krzysztof r apt uniform proofs of order independence for 
various strategy elimination procedures contributions to 
theoretical economics 
 nivan a r bhat and kevin leyton-brown computing 
nash equilibria of action-graph games in uai 
 ben blum christian r shelton and daphne koller a 
continuation method for nash equilibria in structured 
games in ijcai 
 vincent conitzer and tuomas sandholm complexity 
results about nash equilibria in ijcai pages - 
 
 drew fudenberg and jean tirole game theory mit 
press 
 itzhak gilboa ehud kalai and eitan zemel on the order 
of eliminating dominated strategies operations research 
letters - 
 itzhak gilboa ehud kalai and eitan zemel the 
complexity of eliminating dominated strategies 
mathematics of operation research - 
 eric a hansen daniel s bernstein and shlomo 
zilberstein dynamic programming for partially observable 
stochastic games in aaai pages - 
 michael kearns michael littman and satinder singh 
graphical models for game theory in uai 
 leonid khachiyan a polynomial algorithm in linear 
programming soviet math doklady - 
 donald e knuth christos h papadimitriou and john n 
tsitsiklis a note on strategy elimination in bimatrix 
games operations research letters - 
 kevin leyton-brown and moshe tennenholtz local-effect 
games in ijcai 
 richard lipton evangelos markakis and aranyak mehta 
playing large games using simple strategies in acm-ec 
pages - 
 michael littman and peter stone a polynomial-time nash 
equilibrium algorithm for repeated games in acm-ec 
pages - 
 leslie m marx and jeroen m swinkels order 
independence for iterated weak dominance games and 
economic behavior - 
 leslie m marx and jeroen m swinkels corrigendum 
order independence for iterated weak dominance games 
and economic behavior - 
 andreu mas-colell michael whinston and jerry r green 
microeconomic theory oxford university press 
 roger myerson game theory analysis of conflict 
harvard university press cambridge 
 martin j osborne and ariel rubinstein a course in 
game theory mit press 
 christos papadimitriou algorithms games and the 
internet in stoc pages - 
 ryan porter eugene nudelman and yoav shoham simple 
search methods for finding a nash equilibrium in aaai 
pages - 
 
empirical mechanism design methods with application 
to a supply-chain scenario 
yevgeniy vorobeychik christopher kiekintveld and michael p wellman 
university of michigan 
computer science engineering 
ann arbor mi - usa 
{ yvorobey ckiekint wellman } umich edu 
abstract 
our proposed methods employ learning and search techniques to 
estimate outcome features of interest as a function of mechanism 
parameter settings we illustrate our approach with a design task 
from a supply-chain trading competition designers adopted 
several rule changes in order to deter particular procurement behavior 
but the measures proved insufficient our empirical mechanism 
analysis models the relation between a key design parameter and 
outcomes confirming the observed behavior and indicating that no 
reasonable parameter settings would have been likely to achieve the 
desired effect more generally we show that under certain 
conditions the estimator of optimal mechanism parameter setting based 
on empirical data is consistent 
categories and subject descriptors 
i computing methodologies simulation and modeling j 
 computer applications social and behavioral 
sciences-economics 
general terms 
algorithms economics design 
 motivation 
we illustrate our problem with an anecdote from a supply chain 
research exercise the and trading agent competition 
 tac supply chain management scm game tac scm 
defines a scenario where agents compete to maximize their profits 
as manufacturers in a supply chain the agents procure components 
from the various suppliers and assemble finished goods for sale to 
customers repeatedly over a simulated year 
as it happened the specified negotiation behavior of suppliers 
provided a great incentive for agents to procure large quantities of 
components on day the very beginning of the simulation during 
the early rounds of the scm competition several agent 
developers discovered this and the apparent success led to most agents 
performing the majority of their purchasing on day although 
jockeying for day- procurement turned out to be an interesting 
strategic issue in itself the phenomenon detracted from other 
interesting problems such as adapting production levels to varying 
demand since component costs were already sunk and dynamic 
management of production sales and inventory several 
participants noted that the predominance of day- procurement 
overshadowed other key research issues such as factory scheduling and 
optimizing bids for customer orders after the 
tournament there was a general consensus in the tac community that 
the rules should be changed to deter large day- procurement 
the task facing game organizers can be viewed as a problem in 
mechanism design the designers have certain game features 
under their control and a set of objectives regarding game outcomes 
unlike most academic treatments of mechanism design the 
objective is a behavioral feature moderate day- procurement rather 
than an allocation feature like economic efficiency and the allowed 
mechanisms are restricted to those judged to require only an 
incremental modification of the current game replacing the 
supplychain negotiation procedures with a one-shot direct mechanism for 
example was not an option we believe that such operational 
restrictions and idiosyncratic objectives are actually quite typical of 
practical mechanism design settings where they are perhaps more 
commonly characterized as incentive engineering problems 
in response to the problem the tac scm designers adopted 
several rule changes intended to penalize large day- orders these 
included modifications to supplier pricing policies and introduction 
of storage costs assessed on inventories of components and finished 
goods despite the changes day- procurement was very high in 
the early rounds of the competition in a drastic measure the 
gamemaster imposed a fivefold increase of storage costs midway 
through the tournament even this did not stem the tide and day- 
procurement in the final rounds actually increased by some 
measures from 
the apparent difficulty in identifying rule modifications that 
effect moderation in day- procurement is quite striking although 
the designs were widely discussed predictions for the effects of 
various proposals were supported primarily by intuitive arguments 
or at best by back-of-the-envelope calculations much of the 
difficulty of course is anticipating the agents and their 
developers responses without essentially running a gaming exercise for 
this purpose the episode caused us to consider whether new 
ap 
proaches or tools could enable more systematic analysis of design 
options standard game-theoretic and mechanism design methods 
are clearly relevant although the lack of an analytic description of 
the game seems to be an impediment under the assumption that 
the simulator itself is the only reliable source of outcome 
computation we refer to our task as empirical mechanism design 
in the sequel we develop some general methods for empirical 
mechanism design and apply them to the tac scm redesign 
problem our analysis focuses on the setting of storage costs taking 
other game modifications as fixed since this is the most direct 
deterrent to early procurement adopted our results confirm the basic 
intuition that incentives for day- purchasing decrease as storage 
costs rise we also confirm that the high day- procurement 
observed in the tournament is a rational response to the setting 
of storage costs used finally we conclude from our data that it is 
very unlikely that any reasonable setting of storage costs would 
result in acceptable levels of day- procurement so a different design 
approach would have been required to eliminate this problem 
overall we contribute a formal framework and a set of methods 
for tackling indirect mechanism design problems in settings where 
only a black-box description of players utilities is available our 
methods incorporate estimation of sets of nash equilibria and 
sample nash equilibria used in conjuction to support general claims 
about the structure of the mechanism designer s utility as well as a 
restricted probabilistic analysis to assess the likelihood of 
conclusions we believe that most realistic problems are too complex to 
be amenable to exact analysis consequently we advocate the 
approach of gathering evidence to provide indirect support of specific 
hypotheses 
 preliminaries 
a normal form game 
is denoted by i {ri} {ui r } where i 
refers to the set of players and m i is the number of players 
ri is the set of strategies available to player i ∈ i with r 
r × ×rm representing the set of joint strategies of all players 
we designate the set of pure strategies available to player i by ai 
and denote the joint set of pure strategies of all players by a 
a × ×am it is often convenient to refer to a strategy of player 
i separately from that of the remaining players to accommodate 
this we use a−i to denote the joint strategy of all players other than 
player i 
let si be the set of all probability distributions mixtures over 
ai and similarly s be the set of all distributions over a an s ∈ s 
is called a mixed strategy profile when the game is finite i e a 
and i are both finite the probability that a ∈ a is played under 
s is written s a s ai a−i when the distribution s is not 
correlated we can simply say si ai when referring to the probability 
player i plays ai under s 
next we define the payoff utility function of each player i by 
ui a ×· · ·×am → r where ui ai a−i indicates the payoff to 
player i to playing pure strategy ai when the remaining players play 
a−i we can extend this definition to mixed strategies by assuming 
that ui are von neumann-morgenstern vnm utilities as follows 
ui s es ui where es is the expectation taken with respect to 
the probability distribution of play induced by the players mixed 
strategy s 
 
by employing the normal form we model agents as playing a 
single action with decisions taken simultaneously this is appropriate 
for our current study which treats strategies agent programs as 
atomic actions we could capture finer-grained decisions about 
action over time in the extensive form although any extensive game 
can be recast in normal form doing so may sacrifice compactness 
and blur relevant distinctions e g subgame perfection 
occasionally we write ui x y to mean that x ∈ ai or si and 
y ∈ a−i or s−i depending on context we also express the set of 
utility functions of all players as u · {u · um · } 
we define a function r → r interpreted as the maximum 
benefit any player can obtain by deviating from its strategy in the 
specified profile 
 r max 
i∈i 
max 
ai∈ai 
 ui ai r−i − ui r 
where r belongs to some strategy set r of either pure or mixed 
strategies 
faced with a game an agent would ideally play its best strategy 
given those played by the other agents a configuration where all 
agents play strategies that are best responses to the others 
constitutes a nash equilibrium 
definition a strategy profile r r rm constitutes 
a nash equilibrium of game i {ri} {ui r } if for every i ∈ i 
ri ∈ ri ui ri r−i ≥ ui ri r−i 
when r ∈ a the above defines a pure strategy nash equilibrium 
otherwise the definition describes a mixed strategy nash 
equilibrium we often appeal to the concept of an approximate or -nash 
equilibrium where is the maximum benefit to any agent for 
deviating from the prescribed strategy thus r as defined above 
is such that profile r is an -nash equilibrium iff r ≤ 
in this study we devote particular attention to games that exhibit 
symmetry with respect to payoffs rendering agents strategically 
identical 
definition a game i {ri} {ui r } is symmetric if for 
all i j ∈ i a ri rj and b ui ri r−i uj rj r−j 
whenever ri rj and r−i r−j 
 the model 
we model the strategic interactions between the designer of the 
mechanism and its participants as a two-stage game the designer 
moves first by selecting a value θ from a set of allowable 
mechanism settings θ all the participant agents observe the mechanism 
parameter θ and move simultaneously thereafter for example the 
designer could be deciding between a first-price and second-price 
sealed-bid auction mechanisms with the presumption that after the 
choice has been made the bidders will participate with full 
awareness of the auction rules 
since the participants play with full knowledge of the 
mechanism parameter we define a game between them in the second stage 
as γθ i {ri} {ui r θ } we refer to γθ as a game induced 
by θ let n θ be the set of strategy profiles considered solutions 
of the game γθ 
suppose that the goal of the designer is to optimize the value 
of some welfare function w r θ dependent on the mechanism 
parameter and resulting play r we define a pessimistic measure 
w ˆr θ inf{w r θ r ∈ ˆr} representing the worst-case 
welfare of the game induced by θ assuming that agents play some 
joint strategy in ˆr typically we care about w n θ θ the 
worst-case outcome of playing some solution 
on some problems we can gain considerable advantage by 
using an aggregation function to map the welfare outcome of a game 
 
we generally adopt nash equilibrium as the solution concept and 
thus take n θ to be the set of equilibria however much of the 
methodology developed here could be employed with alternative 
criteria for deriving agent behavior from a game definition 
 
again alternatives are available for example if one has a 
probability distribution over the solution set n θ it would be natural 
to take the expectation of w r θ instead 
 
specified in terms of agent strategies to an equivalent welfare 
outcome specified in terms of a lower-dimensional summary 
definition a function φ r × · · · × rm → rq 
is an 
aggregation function if m ≥ q and w r θ v φ r θ for 
some function v 
we overload the function symbol to apply to sets of strategy 
profiles φ ˆr {φ r r ∈ ˆr} for convenience of exposition we 
write φ∗ 
 θ to mean φ n θ 
using an aggregation function yields a more compact 
representation of strategy profiles for example suppose-as in our 
application below-that an agent s strategy is defined by a numeric 
parameter if all we care about is the total value played we may 
take φ a 
pm 
i ai if we have chosen our aggregator carefully 
we may also capture structure not obvious otherwise for example 
φ∗ 
 θ could be decreasing in θ whereas n θ might have a more 
complex structure 
given a description of the solution correspondence n θ 
 equivalently φ∗ 
 θ the designer faces a standard optimization 
problem alternatively given a simulator that could produce an 
unbiased sample from the distribution of w n θ θ for any θ the 
designer would be faced with another much appreciated problem 
in the literature simulation optimization 
however even for a game γθ with known payoffs it may be 
computationally intractable to solve for nash equilibria particularly if 
the game has large or infinite strategy sets additionally we wish 
to study games where the payoffs are not explicitly given but must 
be determined from simulation or other experience with the game 
accordingly we assume that we are given a possibly noisy data 
set of payoff realizations do { θ 
 a 
 u 
 θk 
 ak 
 uk 
 } 
where for every data point θi 
is the observed mechanism parameter 
setting ai 
is the observed pure strategy profile of the participants 
and ui 
is the corresponding realization of agent payoffs we may 
also have additional data generated by a possibly noisy simulator 
ds { θk 
 ak 
 uk 
 θk l 
 ak l 
 uk l 
 } let d 
{do ds} be the combined data set either do or ds may be null 
for a particular problem 
in the remainder of this paper we apply our modeling approach 
together with several empirical game-theoretic methods in order to 
answer questions regarding the design of the tac scm scenario 
 empirical design analysis 
since our data comes in the form of payoff experience and not as 
the value of an objective function for given settings of the control 
variable we can no longer rely on the methods for optimizing 
functions using simulations indeed a fundamental aspect of our design 
problem involves estimating the nash equilibrium correspondence 
furthermore we cannot rely directly on the convergence results 
that abound in the simulation optimization literature and must 
establish probabilistic analysis methods tailored for our problem 
setting 
 tac scm design problem 
we describe our empirical design analysis methods by presenting 
a detailed application to the tac scm scenario introduced above 
recall that during the tournament the designers of the 
supplychain game chose to dramatically increase storage costs as a 
measure aimed at curbing day- procurement to little avail here we 
systematically explore the relationship between storage costs and 
 
this is often the case for real games of interest where natural 
language or algorithmic descriptions may substitute for a formal 
specification of strategy and payoff functions 
the aggregate quantity of components procured on day in 
equilibrium in doing so we consider several questions raised during and 
after the tournament first does increasing storage costs actually 
reduce day- procurement second was the excessive day- 
procurement that was observed during the tournament rational 
and third could increasing storage costs sufficiently have reduced 
day- procurement to an acceptable level and if so what should 
the setting of storage costs have been it is this third question that 
defines the mechanism design aspect of our analysis 
to apply our methods we must specify the agent strategy sets 
the designer s welfare function the mechanism parameter space 
and the source of data we restrict the agent strategies to be a 
multiplier on the quantity of the day- requests by one of the 
finalists deep maize in the tac scm tournament we further 
restrict it to the set since any strategy below is illegal 
and strategies above are extremely aggressive thus unlikely to 
provide refuting deviations beyond those available from included 
strategies and certainly not part of any desirable equilibrium all 
other behavior is based on the behavior of deep maize and is 
identical for all agents this choice can provide only an estimate of the 
actual tournament behavior of a typical agent however we 
believe that the general form of the results should be robust to changes 
in the full agent behavior 
we model the designer s welfare function as a threshold on the 
sum of day- purchases let φ a 
p 
i ai be the 
aggregation function representing the sum of day- procurement of the six 
agents participating in a particular supply-chain game for mixed 
strategy profiles s we take expectation of φ with respect to the 
mixture the designer s welfare function w n θ θ is then given by 
i{sup{φ∗ 
 θ } ≤ α} where α is the maximum acceptable level of 
day- procurement and i is the indicator function the designer 
selects a value θ of storage costs expressed as an annual 
percentage of the baseline value of components in the inventory charged 
daily from the set θ r 
 since the designer s decision 
depends only on φ∗ 
 θ we present all of our results in terms of the 
value of the aggregation function 
 estimating nash equilibria 
the objective of tac scm agents is to maximize profits 
realized over a game instance thus if we fix a strategy for each agent 
at the beginning of the simulation and record the corresponding 
profits at the end we will have obtained a data point in the form 
 a u a if we also have fixed the parameter θ of the simulator 
the resulting data point becomes part of our data set d this data 
set then contains data only in the form of pure strategies of 
players and their corresponding payoffs and consequently in order to 
formulate the designer s problem as optimization we must first 
determine or approximate the set of nash equilibria of each game γθ 
thus we need methods for approximating nash equilibria for 
infinite games below we describe the two methods we used in our 
study the first has been explored empirically before whereas the 
second is introduced here as the method specifically designed to 
approximate a set of nash equilibria 
 payoff function approximation 
the first method for estimating nash equilibria based on data 
uses supervised learning to approximate payoff functions of 
mech 
we do not address whether and how other measures e g 
constraining procurement directly could have achieved design 
objectives our approach takes as given some set of design options in 
this case defined by the storage cost parameter in principle our 
methods could be applied to a different or larger design space 
though with corresponding complexity growth 
 
anism participants from a data set of game experience once 
approximate payoff functions are available for all players the nash 
equilibria may be either found analytically or approximated using 
numerical techniques depending on the learning model in what 
follows we estimate only a sample nash equilibrium using this 
technique although this restriction can be removed at the expense 
of additional computation time 
one advantage of this method is that it can be applied to any 
data set and does not require the use of a simulator thus we can 
apply it when ds ∅ if a simulator is available we can generate 
additional data to build confidence in our initial estimates 
we tried the following methods for approximating payoff 
functions quadratic regression qr locally weighted average lwa 
and locally weighted linear regression lwlr we also used 
control variates to reduce the variance of payoff estimates as in our 
previous empirical game-theoretic analysis of tac scm- 
the quadratic regression model makes it possible to compute 
equilibria of the learned game analytically for the other methods 
we applied replicator dynamics to a discrete approximation of 
the learned game the expected total day- procurement in 
equilibrium was taken as the estimate of an outcome 
 search in strategy profile space 
when we have access to a simulator we can also use directed 
search through profile space to estimate the set of nash equilibria 
which we describe here after presenting some additional notation 
definition a strategic neighbor of a pure strategy profile 
a is a profile that is identical to a in all but one strategy we define 
snb a d as the set of all strategic neighbors of a available in 
the data set d similarly we define snb a ˜d to be all strategic 
neighbors of a not in d finally for any a ∈ snb a d we define 
the deviating agent as i a a 
definition the -bound ˆ of a pure strategy profile a is 
defined as maxa ∈snb a d max{ui a a a −ui a a a } we 
say that a is a candidate δ-equilibrium for δ ≥ ˆ 
when snb a ˜d ∅ i e all strategic neighbors are represented 
in the data a is confirmed as an ˆ-nash equilibrium 
our search method operates by exploring deviations from 
candidate equilibria we refer to it as bestfirstsearch as it selects 
with probability one a strategy profile a ∈ snb a ˜d that has the 
smallest ˆin d 
finally we define an estimator for a set of nash equilibria 
definition for a set k define co k to be the convex 
hull of k let bδ be the set of candidates at level δ we define 
ˆφ∗ 
 θ co {φ a a ∈ bδ} for a fixed δ to be an estimator of 
φ∗ 
 θ 
in words the estimate of a set of equilibrium outcomes is the 
convex hull of all aggregated strategy profiles with -bound below 
some fixed δ this definition allows us to exploit structure 
arising from the aggregation function if two profiles are close in terms 
of aggregation values they may be likely to have similar -bounds 
in particular if one is an equilibrium the other may be as well we 
present some theoretical support for this method of estimating the 
set of nash equilibria below 
since the game we are interested in is infinite it is necessary to 
terminate bestfirstsearch before exploring the entire space of 
strat 
for example we can use active learning techniques to improve 
the quality of payoff function approximation in this work we 
instead concentrate on search in strategy profile space 
egy profiles we currently determine termination time in a 
somewhat ad-hoc manner based on observations about the current set of 
candidate equilibria 
 data generation 
our data was collected by simulating tac scm games on a 
local version of the tac scm server which has a configuration 
setting for the storage cost agent strategies in simulated games 
were selected from the set { } in order to have 
positive probability of generating strategic neighbors 
a 
baseline data set do was generated by sampling randomly generated 
strategy profiles for each θ ∈ { } between 
and games were run for each profile after discarding games that 
had various flaws 
we used search to generate a simulated data 
set ds performing between and iterations of bestfirstsearch 
for each of the above settings of θ since simulation cost is 
extremely high a game takes nearly hour to run we were able to 
run a total of games over the span of more than six months 
for comparison to get the entire description of an empirical game 
defined by the restricted finite joint strategy space for each value 
of θ ∈ { } would have required at least 
games sampling each profile times 
 results 
 analysis of the baseline data set 
we applied the three learning methods described above to the 
baseline data set do additionally we generated an estimate of the 
nash equilibrium correspondence ˆφ∗ 
 θ by applying definition 
with δ e the results are shown in figure as we can see 
the correspondence ˆφ∗ 
 θ has little predictive power based on do 
and reveals no interesting structure about the game in contrast all 
three learning methods suggest that total day- procurement is a 
decreasing function of storage costs 
 
 
 
 
 
 
 
 
 
 
 
 
storage cost 
totalday- procurement 
lwa 
lwlr 
qr 
baselinemin 
baselinemax 
figure aggregate day- procurement estimates based on do 
the correspondence ˆφ∗ 
 θ is the interval between 
baselinemin and baselinemax 
 
generally search is terminated once the set of candidate equilibria 
is small enough to draw useful conclusions about the likely range 
of equilibrium strategies in the game 
 
of course we do not restrict our nash equilibrium estimates to 
stay in this discrete subset of 
 
for example if we detected that any agent failed during the 
game failures included crashes network connectivity problems 
and other obvious anomalies the game would be thrown out 
 
 analysis of search data 
to corroborate the initial evidence from the learning methods 
we estimated ˆφ∗ 
 θ again using δ e on the data set d 
{do ds} where ds is data generated through the application of 
bestfirstsearch the results of this estimate are plotted against the 
results of the learning methods trained on do 
 
in figure first 
we note that the addition of the search data narrows the range of 
potential equilibria substantially furthermore the actual point 
predictions of the learning methods and those based on -bounds 
after search are reasonably close combining the evidence gathered 
from these two very different approaches to estimating the outcome 
correspondence yields a much more compelling picture of the 
relationship between storage costs and day- procurement than either 
method used in isolation 
 
 
 
 
 
 
 
 
 
 
 
 
storage cost 
totayday- procurement 
lwa 
lwlr 
qr 
searchmin 
searchmax 
figure aggregate day- procurement estimates based on 
search in strategy profile space compared to function 
approximation techniques trained on do the correspondence ˆφ∗ 
 θ 
for d {do ds} is the interval between searchmin and 
searchmax 
this evidence supports the initial intuition that day- 
procurement should be decreasing with storage costs it also confirms that 
high levels of day- procurement are a rational response to the 
tournament setting of average storage cost which corresponds to 
θ the minimum prediction for aggregate procurement at 
this level of storage costs given by any experimental methods is 
approximately this is quite high as it corresponds to an 
expected commitment of of the total supplier capacity for the 
entire game the maximum prediction is considerably higher at 
in the actual competition aggregate day- procurement was 
equivalent to on the scale used here our predictions 
underestimate this outcome to some degree but show that any rational 
outcome was likely to have high day- procurement 
 extrapolating the solution correspondence 
we have reasonably strong evidence that the outcome 
correspondence is decreasing however the ultimate goal is to be able to 
either set the storage cost parameter to a value that would curb day- 
procurement in equilibrium or conclude that this is not possible 
to answer this question directly suppose that we set a 
conservative threshold α on aggregate day- procurement 
linear 
 
it is unclear how meaningful the results of learning would be if 
ds were added to the training data set indeed the additional data 
may actually increase the learning variance 
 
recall that designer s objective is to incentivize aggergate day- 
procurement that is below the threshold α our threshold here still 
represents a commitment of over of the suppliers capacity for 
extrapolation of the maximum of the outcome correspondence 
estimated from d yields θ 
the data for θ were collected in the same way as for other 
storage cost settings with randomly generated profiles followed 
by iterations of bestfirstsearch figure shows the detailed 
-bounds for all profiles in terms of their corresponding values of 
φ 
 e 
 e 
 e 
 e 
 e 
 e 
 e 
 e 
 e 
 e 
 e 
 
total day- procurement 
ε−boundfigure values of ˆ for profiles explored using search when 
θ strategy profiles explored are presented in terms of 
the corresponding values of φ a the gray region corresponds 
to ˆφ∗ 
 with δ m 
the estimated set of aggregate day- outcomes is very close to 
that for θ indicating that there is little additional benefit 
to raising storage costs above observe that even the lower 
bound of our estimated set of nash equilibria is well above the 
target day- procurement of furthermore payoffs to agents are 
almost always negative at θ consequently increasing the 
costs further would be undesirable even if day- procurement could 
eventually be curbed since we are reasonably confident that φ∗ 
 θ 
is decreasing in θ we also do not expect that setting θ somewhere 
between and will achieve the desired result 
we conclude that it is unlikely that day- procurement could ever 
be reduced to a desirable level using any reasonable setting of the 
storage cost parameter that our predictions tend to underestimate 
tournament outcomes reinforces this conclusion to achieve the 
desired reduction in day- procurement requires redesigning other 
aspects of the mechanism 
 probabilistic analysis 
our empirical analysis has produced evidence in support of the 
conclusion that no reasonable setting of storage cost was likely to 
sufficiently curb excessive day- procurement in tac scm 
all of this evidence has been in the form of simple interpolation and 
extrapolation of estimates of the nash equilibrium correspondence 
these estimates are based on simulating game instances and are 
subject to sampling noise contributed by the various stochastic 
elements of the game in this section we develop and apply methods 
for evaluating the sensitivity of our -bound calculations to such 
stochastic effects 
suppose that all agents have finite and small pure strategy sets 
a thus it is feasible to sample the entire payoff matrix of the 
game additionally suppose that noise is additive with zero-mean 
the entire game on average so in practice we would probably want 
the threshold to be even lower 
 
and finite variance that is ui a ui a ˜ξi a where ui a is 
the observed payoff to i when a was played ui a is the actual 
corresponding payoff and ˜ξi a is a mean-zero normal random 
variable we designate the known variance of ˜ξi a by σ 
i a thus 
we assume that ˜ξi a is normal with distribution n σ 
i a 
we take ¯ui a to be the sample mean over all ui a in d and 
follow chang and huang to assume that we have an improper 
prior over the actual payoffs ui a and sampling was independent 
for all i and a we also rely on their result that ui a ¯ui a 
¯ui a −zi a σi a 
p 
ni a are independent with posterior 
distributions n ¯ui a σ 
i a ni a where ni a is the number of 
samples taken of payoffs to i for pure profile a and zi a ∼ 
n 
we now derive a generic probabilistic bound that a profile a ∈ 
a is an -nash equilibrium if ui · ¯ui · are independent for all 
i ∈ i and a ∈ a we have the following result from this point on 
we omit conditioning on ¯ui · for brevity 
proposition 
pr 
„ 
max 
i∈i 
max 
b∈ai 
ui b a−i − ui a ≤ 
 
 
 
y 
i∈i 
z 
r 
y 
b∈ai\ai 
pr ui b a−i ≤ u fui a u du 
 
where fui a u is the pdf of n ¯ui a σi a 
the proofs of this and all subsequent results are in the appendix 
the posterior distribution of the optimum mean of n samples 
derived by chang and huang is 
pr ui a ≤ c − φ 
 p 
ni a ¯ui a − c 
σi a 
 
 
where a ∈ a and φ · is the n distribution function 
combining the results and we obtain a probabilistic 
confidence bound that a ≤ γ for a given γ 
now we consider cases of incomplete data and use the results 
we have just obtained to construct an upper bound restricted to 
profiles represented in data on the distribution of sup{φ∗ 
 θ } and 
inf{φ∗ 
 θ } assuming that both are attainable 
pr{sup{φ∗ 
 θ } ≤ x} ≤d 
pr{∃a ∈ d φ a ≤ x ∧ a ∈ n θ } ≤ 
x 
a∈d φ a ≤x 
pr{a ∈ n θ } 
x 
a∈d φ a ≤x 
pr{ a } 
where x is a real number and ≤d indicates that the upper bound 
accounts only for strategies that appear in the data set d since the 
events {∃a ∈ d φ a ≤ x ∧ a ∈ n θ } and {inf{φ∗ 
 θ } ≤ x} 
are equivalent this also defines an upper bound on the 
probability of {inf{φ∗ 
 θ } ≤ x} the values thus derived comprise the 
tables and 
φ∗ 
 θ θ θ θ 
 
 
 
 
table upper bounds on the distribution of inf{φ∗ 
 θ } 
restricted to d for θ ∈ { } when n θ is a set of nash 
equilibria 
φ∗ 
 θ θ θ θ 
 
 
 
 
table upper bounds on the distribution of inf{φ∗ 
 θ } 
restricted to d for θ ∈ { } when n θ is a set of 
nash equilibria 
tables and suggest that the existence of any equilibrium with 
φ a is unlikely for any θ that we have data for although 
this judgment as we mentioned is only with respect to the profiles 
we have actually sampled we can then accept this as another piece 
of evidence that the designer could not find a suitable setting of θ 
to achieve his objectives-indeed the designer seems unlikely to 
achieve his objective even if he could persuade participants to play 
a desirable equilibrium 
table also provides additional evidence that the agents in the 
 tac scm tournament were indeed rational in procuring large 
numbers of components at the beginning fo the game if we look at 
the third column of this table which corresponds to θ we 
can gather that no profile a in our data with φ a is very likely 
to be played in equilibrium 
the bounds above provide some general evidence but ultimately 
we are interested in a concrete probabilistic assessment of our 
conclusion with respect to the data we have sampled particularly we 
would like to say something about what happens for the settings of 
θ for which we have no data to derive an approximate 
probabilistic bound on the probability that no θ ∈ θ could have achieved the 
designer s objective let ∪j 
j θj be a partition of θ and assume 
that the function sup{φ∗ 
 θ } satisfies the lipschitz condition with 
lipschitz constant aj on each subset θj 
since we have 
determined that raising the storage cost above is undesirable due to 
secondary considerations we restrict attention to θ we 
now define each subset j to be the interval between two points for 
which we have produced data thus 
θ 
 
 
 
 
 
 
 
 
with j running between and corresponding to subintervals 
above we will further denote each θj by aj bj 
then the 
following proposition gives us an approximate upper bound 
on 
the probability that sup{φ∗ 
 θ } ≤ α 
proposition 
pr{ 
 
θ∈θ 
sup{φ θ } ≤ α} ≤d 
 x 
j 
x 
y z∈d y z≤cj 
 
  
x 
a φ a z 
pr{ a } 
 
a × 
× 
 
  
x 
a φ a y 
pr{ a } 
 
a 
where cj α aj bj − aj and ≤d indicates that the upper 
bound only accounts for strategies that appear in the data set d 
 
a function that satisfies the lipschitz condition is called lipschitz 
continuous 
 
the treatment for the interval is identical 
 
it is approximate in a sense that we only take into account 
strategies that are present in the data 
 
due to the fact that our bounds are approximate we cannot use 
them as a conclusive probabilistic assessment instead we take this 
as another piece of evidence to complement our findings 
even if we can assume that a function that we approximate from 
data is lipschitz continuous we rarely actually know the lipschitz 
constant for any subset of θ thus we are faced with a task of 
estimating it from data here we tried three methods of doing 
this the first one simply takes the highest slope that the function 
attains within the available data and uses this constant value for 
every subinterval this produces the most conservative bound and 
in many situations it is unlikely to be informative 
an alternative method is to take an upper bound on slope 
obtained within each subinterval using the available data this 
produces a much less conservative upper bound on probabilities 
however since the actual upper bound is generally greater for each 
subinterval the resulting probabilistic bound may be deceiving 
a final method that we tried is a compromise between the two 
above instead of taking the conservative upper bound based on 
data over the entire function domain θ we take the average of 
upper bounds obtained at each θj the bound at an interval is then 
taken to be the maximum of the upper bound for this interval and 
the average upper bound for all intervals 
the results of evaluating the expression for 
pr{ 
 
θ∈θ 
sup{φ∗ 
 θ } ≤ α} 
when α are presented in table in terms of our claims in 
maxj aj aj max{aj ave aj } 
 
table approximate upper bound on probability that some 
setting of θ ∈ will satisfy the designer objective with 
target α different methods of approximating the upper 
bound on slope in each subinterval j are used 
this work the expression gives an upper bound on the probability 
that some setting of θ i e storage cost in the interval will 
result in total day- procurement that is no greater in any 
equilibrium than the target specified by α and taken here to be as we 
had suspected the most conservative approach to estimating the 
upper bound on slope presented in the first column of the table 
provides us little information here however the other two 
estimation approaches found in columns two and three of table 
suggest that we are indeed quite confident that no reasonable setting of 
θ ∈ would have done the job given the tremendous 
difficulty of the problem this result is very strong 
still we must be 
very cautious in drawing too heroic a conclusion based on this 
evidence certainly we have not checked all the profiles but only 
a small proportion of them infinitesimal if we consider the 
entire continuous domain of θ and strategy sets nor can we expect 
ever to obtain enough evidence to make completely objective 
conclusions instead the approach we advocate here is to collect as 
much evidence as is feasible given resource constraints and make 
the most compelling judgment based on this evidence if at all 
possible 
 convergence results 
at this point we explore abstractly whether a design parameter 
choice based on payoff data can be asymptotically reliable 
 
since we did not have all the possible deviations for any profile 
available in the data the true upper bounds may be even lower 
as a matter of convenience we will use notation un i a to 
refer to a payoff function of player i based on an average over n 
i i d samples from the distribution of payoffs we also assume that 
un i a are independent for all a ∈ a and i ∈ i we will use 
the notation γn to refer to the game i r {ui n · } whereas γ 
will denote the underlying game i r {ui · } similarly we 
define n r to be r with respect to the game γn 
in this section we show that n s → s a s uniformly on 
the mixed strategy space for any finite game and furthermore that 
all mixed strategy nash equilibria in empirical games eventually 
become arbitrarily close to some nash equilibrium strategies in the 
underlying game we use these results to show that under certain 
conditions the optimal choice of the design parameter based on 
empirical data converges almost surely to the actual optimum 
theorem suppose that i ∞ a ∞ then n s → 
 s a s uniformly on s 
recall that n is a set of all nash equilibria of γ if we define 
nn γ {s ∈ s n s ≤ γ} we have the following corollary to 
theorem 
corollary for every γ there is m such that ∀n ≥ 
m n ⊂ nn γ a s 
proof since s for every s ∈ n we can find m large 
enough such that pr{supn≥m sups∈n n s γ} 
by the corollary for any game with a finite set of pure strategies 
and for any all nash equilibria lie in the set of empirical 
-nash equilibria if enough samples have been taken as we now 
show this provides some justification for our use of a set of profiles 
with a non-zero -bound as an estimate of the set of nash equilibria 
first suppose we conclude that for a particular setting of θ 
sup{ˆφ∗ 
 θ } ≤ α then since for any fixed n θ ⊂ 
nn θ when n is large enough 
sup{φ∗ 
 θ } sup 
s∈n θ 
φ s ≤ 
sup 
s∈nn θ 
φ s sup{ˆφ∗ 
 θ } ≤ α 
for any such n thus since we defined the welfare function of 
the designer to be i{sup{φ∗ 
 θ } ≤ α} in our domain of interest 
the empirical choice of θ satisfies the designer s objective thereby 
maximizing his welfare function 
alternatively suppose we conclude that inf{ˆφ∗ 
 θ } α for 
every θ in the domain then 
α inf{ˆφ∗ 
 θ } inf 
s∈nn θ 
φ s ≤ inf 
s∈n θ 
φ s ≤ 
≤ sup 
s∈n θ 
φ s sup{φ∗ 
 θ } 
for every θ and we can conclude that no setting of θ will satisfy 
the designer s objective 
now we will show that when the number of samples is large 
enough every nash equilibrium of γn is close to some nash 
equilibrium of the underlying game this result will lead us to consider 
convergence of optimizers based on empirical data to actual 
optimal mechanism parameter settings 
we first note that the function s is continuous in a finite game 
lemma let s be a mixed strategy set defined on a finite 
game then s → r is continuous 
 
for the exposition that follows we need a bit of additional 
notation first let z d be a metric space and x y ⊂ z and define 
directed hausdorff distance from x to y to be 
h x y sup 
x∈x 
inf 
y∈y 
d x y 
observe that u ⊂ x ⇒ h u y ≤ h x y further define 
bs x δ to be an open ball in s ⊂ z with center x ∈ s and 
radius δ now let nn denote all nash equilibria of the game γn 
and let 
nδ 
 
x∈n 
bs x δ 
that is the union of open balls of radius δ with centers at nash 
equilibria of γ note that h nδ n δ 
we can then prove the following general result 
theorem suppose i ∞ and a ∞ then almost 
surely h nn n converges to 
we will now show that in the special case when θ and a are 
finite and each γθ has a unique nash equilibrium the estimates 
ˆθ of optimal designer parameter converge to an actual optimizer 
almost surely 
let ˆθ arg maxθ∈θ w nn θ θ where n is the number of 
times each pure profile was sampled in γθ for every θ and let θ∗ 
 
arg maxθ∈θ w n θ θ 
theorem suppose n θ for all θ ∈ θ and suppose 
that θ and a are finite let w s θ be continuous at the unique 
s∗ 
 θ ∈ n θ for each θ ∈ θ then ˆθ is a consistent estimator of 
θ∗ 
if w n θ θ is defined as a supremum infimum or 
expectation over the set of nash equilibria in fact ˆθ → θ∗ 
a s in each of 
these cases 
the shortcoming of the above result is that within our 
framework the designer has no way of knowing or ensuring that γθ do 
indeed have unique equilibria however it does lend some 
theoretical justification for pursuing design in this manner and perhaps 
will serve as a guide for more general results in the future 
 related work 
the mechanism design literature in economics has typically 
explored existence of a mechanism that implements a social choice 
function in equilibrium additionally there is an extensive 
literature on optimal auction design of which the work by roger 
myerson is perhaps the most relevant in much of this work 
analytical results are presented with respect to specific utility 
functions and accounting for constraints such as incentive compatibility 
and individual rationality 
several related approaches to search for the best mechanism 
exist in the computer science literature conitzer and sandholm 
developed a search algorithm when all the relevant game 
parameters are common knowledge when payoff functions of players 
are unknown a search using simulations has been explored as an 
alternative one approach in that direction taken in and 
is to co-evolve the mechanism parameter and agent strategies 
using some notion of social utility and agent payoffs as fitness 
criteria an alternative to co-evolution explored in was to 
optimize a well-defined welfare function of the designer using genetic 
programming in this work the authors used a common learning 
strategy for all agents and defined an outcome of a game induced 
by a mechanism parameter as the outcome of joint agent learning 
most recently phelps et al compared two mechanisms based 
on expected social utility with expectation taken over an empirical 
distribution of equilibria in games defined by heuristic strategies 
as in 
 conclusion 
in this work we spent considerable effort developing general 
tactics for empirical mechanism design we defined a formal 
gametheoretic model of interaction between the designer and the 
participants of the mechanism as a two-stage game we also described in 
some generality the methods for estimating a sample nash 
equilibrium function when the data is extremely scarce or a nash 
equilibrium correspondence when more data is available our techniques 
are designed specifically to deal with problems in which both the 
mechanism parameter space and the agent strategy sets are infinite 
and only a relatively small data set can be acquired 
a difficult design issue in the tac scm game which the tac 
community has been eager to address provides us with a setting 
to test our methods in applying empirical game analysis to the 
problem at hand we are fully aware that our results are inherently 
inexact thus we concentrate on collecting evidence about the 
structure of the nash equilibrium correspondence in the end we 
can try to provide enough evidence to either prescribe a parameter 
setting or suggest that no setting is possible that will satisfy the 
designer in the case of tac scm our evidence suggests quite 
strongly that storage cost could not have been effectively adjusted 
in the tournament to curb excessive day- procurement 
without detrimental effects on overall profitability the success of our 
analysis in this extremely complex environment with high 
simulation costs makes us optimistic that our methods can provide 
guidance in making mechanism design decisions in other challenging 
domains the theoretical results confirm some intuitions behind 
the empirical mechanism design methods we have introduced and 
increases our confidence that our framework can be effective in 
estimating the best mechanism parameter choice in relatively general 
settings 
acknowledgments 
we thank terence kelly matthew rudary and satinder singh for 
helpful comments on earlier drafts of this work this work was 
supported in part by nsf grant iis- and the darpa real 
strategic reasoning program 
 references 
 r arunachalam and n m sadeh the supply chain trading 
agent competition electronic commerce research and 
applications - 
 m benisch a greenwald v naroditskiy and m tschantz 
a stochastic programming approach to scheduling in tac 
scm in fifth acm conference on electronic commerce 
pages - new york 
 y -p chang and w -t huang generalized confidence 
intervals for the largest value of some functions of 
parameters under normality statistica sinica - 
 
 d cliff evolution of market mechanism through a 
continuous space of auction-types in congress on 
evolutionary computation 
 d a cohn z ghahramani and m i jordan active 
learning with statistical models journal of artificial 
intelligence research - 
 v conitzer and t sandholm an algorithm for automatically 
designing deterministic mechanisms without payments in 
 
third international joint conference on autonomous agents 
and multi-agent systems pages - 
 d friedman evolutionary games in economics 
econometrica - may 
 r keener statistical theory a medley of core topics 
university of michigan department of statistics 
 c kiekintveld y vorobeychik and m p wellman an 
analysis of the supply chain management trading agent 
competition in ijcai- workshop on trading agent 
design and analysis edinburgh 
 a mas-colell m whinston and j green microeconomic 
theory oxford university press 
 r b myerson optimal auction design mathematics of 
operations research - february 
 s olafsson and j kim simulation optimization in 
e yucesan c -h chen j snowdon and j charnes editors 
 winter simulation conference 
 d pardoe and p stone tactex- a supply chain 
management agent sigecom exchanges - 
 s phelps s parsons and p mcburney automated agents 
versus virtual humans an evolutionary game theoretic 
comparison of two double-auction market designs in 
workshop on agent mediated electronic commerce vi 
 
 s phelps s parsons p mcburney and e sklar 
co-evolution of auction mechanisms and trading strategies 
towards a novel approach to microeconomic design in 
ecomas workshop 
 s phelps s parsons e sklar and p mcburney using 
genetic programming to optimise pricing rules for a 
double-auction market in workshop on agents for electronic 
commerce 
 y vorobeychik m p wellman and s singh learning 
payoff functions in infinite games in nineteenth 
international joint conference on artificial intelligence 
pages - 
 w e walsh r das g tesauro and j o kephart 
analyzing complex strategic interactions in multi-agent 
systems in aaai- workshop on game theoretic and 
decision theoretic agents 
 m p wellman j j estelle s singh y vorobeychik 
c kiekintveld and v soni strategic interactions in a supply 
chain game computational intelligence - 
february 
appendix 
a proofs 
a proof of proposition 
pr 
„ 
max 
i∈i 
max 
b∈ai\ai 
ui b a−i − ui a ≤ 
 
 
 
y 
i∈i 
eui a 
 
pr max 
b∈ai\ai 
ui b a−i − ui a ≤ ui a 
 
 
 
y 
i∈i 
z 
r 
y 
b∈ai\ai 
pr ui b a−i ≤ u fui a u du 
a proof of proposition 
first let us suppose that some function f x defined on ai bi 
satisfy lipschitz condition on ai bi with lipschitz constant ai 
then the following claim holds 
claim infx∈ ai bi f x ≥ f ai f bi − ai bi − ai 
to prove this claim note that the intersection of lines at f ai 
and f bi with slopes −ai and ai respectively will determine the 
lower bound on the minimum of f x on ai bi which is a lower 
bound on infimum of f x on ai bj the line at f ai is 
determined by f ai −aiai cl and the line at f bi is determined 
by f bi aibi cr thus the intercepts are cl f ai aiai 
and cr f bi aibi respectively let x∗ 
be the point at which 
these lines intersect then 
x∗ 
 − 
f x∗ 
 − cr 
a 
 
f x∗ 
 − cl 
a 
 
by substituting the expressions for cr and cl we get the desired 
result 
now subadditivity gives us 
pr{ 
 
θ∈θ 
sup{φ∗ 
 θ } ≤ α} ≤ 
 x 
j 
pr{ 
 
θ∈θj 
sup{φ∗ 
 θ } ≤ α} 
and by the claim 
pr{ 
 
θ∈θj 
sup{φ∗ 
 θ } ≤ α} 
 − pr{ inf 
θ∈θj 
sup{φ∗ 
 θ } α} ≤ 
pr{sup{φ∗ 
 aj } sup{φ∗ 
 bj } ≤ α aj bj − aj } 
since we have a finite number of points in the data set for each 
θ we can obtain the following expression 
pr{sup{φ∗ 
 aj } sup{φ∗ 
 bj } ≤ cj } d 
x 
y z∈d y z≤cj 
pr{sup{φ∗ 
 bj } y} pr{sup{φ∗ 
 aj } z} 
we can now restrict attention to deriving an upper bound on 
pr{sup{φ∗ 
 θ } y} for a fixed θ to do this observe that 
pr{sup{φ∗ 
 θ } y} ≤d pr{ 
 
a∈d φ a y 
 a } ≤ 
x 
a∈d φ a y 
pr{ a } 
by subadditivity and the fact that a profile a is a nash equilibrium 
if and only if a 
putting everything together yields the desired result 
a proof of theorem 
first we will need the following fact 
claim given a function fi x and a set x maxx∈x f x − 
maxx∈x f x ≤ maxx∈x f x − f x 
to prove this claim observe that 
 max 
x∈x 
f x − max 
x∈x 
f x 
 
maxx f x − maxx f x if maxx f x ≥ maxx f x 
maxx f x − maxx f x if maxx f x ≥ maxx f x 
in the first case 
max 
x∈x 
f x − max 
x∈x 
f x ≤ max 
x∈x 
 f x − f x ≤ 
≤ max 
x∈x 
 f x − f x 
 
similarly in the second case 
max 
x∈x 
f x − max 
x∈x 
f x ≤ max 
x∈x 
 f x − f x ≤ 
≤ max 
x∈x 
 f x − f x max 
x∈x 
 f x − f x 
thus the claim holds 
by the strong law of large numbers un i a → ui a a s for 
all i ∈ i a ∈ a that is 
pr{ lim 
n→∞ 
un i a ui a } 
or equivalently for any α and δ there is m i a 
such that 
pr{ sup 
n≥m i a 
 un i a − ui a 
δ 
 a 
} ≥ − α 
by taking m maxi∈i maxa∈a m i a we have 
pr{max 
i∈i 
max 
a∈a 
sup 
n≥m 
 un i a − ui a 
δ 
 a 
} ≥ − α 
thus by the claim for any n ≥ m 
sup 
n≥m 
 n s − s ≤ 
max 
i∈i 
max 
ai∈ai 
sup 
n≥m 
 un i ai s−i − ui ai s−i 
 sup 
n≥m 
max 
i∈i 
 un i s − ui s ≤ 
max 
i∈i 
max 
ai∈ai 
x 
b∈a−i 
sup 
n≥m 
 un i ai b − ui ai b s−i b 
 max 
i∈i 
x 
b∈a 
sup 
n≥m 
 un i b − ui b s b ≤ 
max 
i∈i 
max 
ai∈ai 
x 
b∈a−i 
sup 
n≥m 
 un i ai b − ui ai b 
 max 
i∈i 
x 
b∈a 
sup 
n≥m 
 un i b − ui b 
max 
i∈i 
max 
ai∈ai 
x 
b∈a−i 
 
δ 
 a 
 max 
i∈i 
x 
b∈a 
 
δ 
 a 
 ≤ δ 
with probability at least − α note that since s−i a and s a 
are bounded between and we were able to drop them from 
the expressions above to obtain a bound that will be valid 
independent of the particular choice of s furthermore since the above 
result can be obtained for an arbitrary α and δ we have 
pr{limn→∞ n s s } uniformly on s 
a proof of lemma 
we prove the result using uniform continuity of ui s and 
preservation of continuity under maximum 
claim a function f rk 
→ r defined by f t 
pk 
i ziti 
where zi are constants in r is uniformly continuous in t 
the claim follows because f t −f t 
pk 
i zi ti−ti ≤ 
pk 
i zi ti − ti an immediate result of this for our purposes is 
that ui s is uniformly continuous in s and ui ai s−i is uniformly 
continuous in s−i 
claim let f a b be uniformly continuous in b ∈ b for every 
a ∈ a with a ∞ then v b maxa∈a f a b is uniformly 
continuous in b 
to show this take γ and let b b ∈ b such that b − b 
δ a ⇒ f a b − f a b γ now take δ mina∈a δ a 
then whenever b − b δ 
 v b − v b max 
a∈a 
f a b − max 
a∈a 
f a b ≤ 
max 
a∈a 
 f a b − f a b γ 
now recall that s maxi maxai∈ai ui ai s−i − ui s by 
the claims above maxai∈ai ui ai s−i is uniformly continuous in 
s−i and ui s is uniformly continuous in s since the difference of 
two uniformly continuous functions is uniformly continuous and 
since this continuity is preserved under maximum by our second 
claim we have the desired result 
a proof of theorem 
choose δ first we need to ascertain that the following 
claim holds 
claim ¯ mins∈s\nδ 
 s exists and ¯ 
since nδ is an open subset of compact s it follows that s \ 
nδ is compact as we had also proved in lemma that s is 
continuous existence follows from the weierstrass theorem that 
¯ is clear since s if and only if s is a nash equilibrium 
of γ 
now by theorem for any α there is m such that 
pr{ sup 
n≥m 
sup 
s∈s 
 n s − s ¯} ≥ − α 
consequently for any δ 
pr{ sup 
n≥m 
h nn nδ δ} ≥ pr{∀n ≥ m nn ⊂ nδ} ≥ 
pr{ sup 
n≥m 
sup 
s∈n 
 s ¯} ≥ 
pr{ sup 
n≥m 
sup 
s∈s 
 n s − s ¯} ≥ − α 
since this holds for an arbitrary α and δ the desired result 
follows 
a proof of theorem 
fix θ and choose δ since w s θ is continuous at s∗ 
 θ 
given there is δ such that for every s that is within δ of 
s∗ 
 θ w s θ − w s∗ 
 θ θ by theorem we can find 
m θ large enough such that all s ∈ nn are within δ of s∗ 
 θ for 
all n ≥ m θ with probability consequently for any we 
can find m θ large enough such that with probability we have 
supn≥m θ sups ∈nn 
 w s θ − w s∗ 
 θ θ 
let us assume without loss of generality that there is a unique 
optimal choice of θ now since the set θ is finite there is also the 
second-best choice of θ if there is only one θ ∈ θ this discussion 
is moot anyway 
θ∗∗ 
 arg max 
θ\θ∗ 
w s∗ 
 θ θ 
suppose w l o g that θ∗∗ 
is also unique and let 
∆ w s∗ 
 θ∗ 
 θ∗ 
 − w s∗ 
 θ∗∗ 
 θ∗∗ 
 
then if we let ∆ and let m maxθ∈θ m θ where each 
m θ is large enough such that supn≥m θ sups ∈nn 
 w s θ − 
w s∗ 
 θ θ a s the optimal choice of θ based on any 
empirical equilibrium will be θ∗ 
with probability thus in 
particular given any probability distribution over empirical equilibria the 
best choice of θ will be θ∗ 
with probability similarly if we take 
supremum or infimum of w nn θ θ over the set of empirical 
equilibria in constructing the objective function 
 
 in stability properties of limit order dynamics 
eyal even-dar ∗ 
sham m kakade † 
michael kearns ‡ 
yishay mansour § 
abstract 
we study the stability properties of the dynamics of the 
standard continuous limit-order mechanism that is used in 
modern equity markets we ask whether such mechanisms 
are susceptible to butterfly effects - the infliction of large 
changes on common measures of market activity by only 
small perturbations of the order sequence we show that the 
answer depends strongly on whether the market consists of 
absolute traders who determine their prices independent 
of the current order book state or relative traders who 
determine their prices relative to the current bid and ask 
we prove that while the absolute trader model enjoys 
provably strong stability properties the relative trader model 
is vulnerable to great instability our theoretical results 
are supported by large-scale experiments using limit order 
data from inet a large electronic exchange for nasdaq 
stocks 
categories and subject descriptors 
j social and behavioral sciences economics 
general terms 
economics theory 
 introduction 
in recent years there has been an explosive increase in 
the automation of modern equity markets this increase 
has taken place both in the exchanges which are 
increasingly computerized and offer sophisticated interfaces for 
order placement and management and in the trading 
activity itself which is ever more frequently undertaken by 
software the so-called electronic communication networks or 
ecns that dominate trading in nasdaq stocks are a 
common example of the automation of the exchanges on the 
trading side computer programs now are entrusted not only 
with the careful execution of large block trades for clients 
 sometimes referred to on wall street as program trading 
but with the autonomous selection of stocks direction long 
or short and volumes to trade for profit commonly referred 
to as statistical arbitrage 
the vast majority of equity trading is done via the 
standard limit order market mechanism in this mechanism 
continuous trading takes place via the arrival of limit 
orders specifying whether the party wishes to buy or sell the 
volume desired and the price offered arriving limit orders 
that are entirely or partially executable with the best offers 
on the other side are executed immediately with any 
volume not immediately executable being placed in an queue 
 or book ordered by price on the appropriate side buy or 
sell a detailed description of the limit order mechanism is 
given in section while traders have always been able to 
view the prices at the top of the buy and sell books known 
as the bid and ask a relatively recent development in 
certain exchanges is the real-time revelation of the entire order 
book - the complete distribution of orders prices and 
volumes on both sides of the exchange with this revelation 
has come the opportunity - and increasingly the 
needfor modeling and exploiting limit order data and 
dynamics it is fair to say that market microstructure as this area 
is generally known is a topic commanding great interest 
both in the real markets and in the academic finance 
literature the opportunities and needs span the range from 
the optimized execution of large trades to the creation of 
stand-alone proprietary strategies that attempt to profit 
from high-frequency microstructure signals 
in this paper we investigate a previously unexplored but 
fundamental aspect of limit order microstructure the 
stability properties of the dynamics specifically we are 
interested in the following natural question to what extent 
are simple models of limit order markets either susceptible 
or immune to butterfly effects - that is the infliction of 
large changes in important activity statistics such as the 
 
number of shares traded or the average price per share by 
only minor perturbations of the order sequence 
to examine this question we consider two stylized but 
natural models of the limit order arrival process in the 
absolute price model buyers and sellers arrive with limit order 
prices that are determined independently of the current state 
of the market as represented by the order books though 
they may depend on all manner of exogenous information 
or shocks such as time news events announcements from 
the company whose shares are being traded private signals 
or state of the individual traders etc this process models 
traditional fundamentals-based trading in which market 
participants each have some inherent but possibly varying 
valuation for the good that in turn determines their limit 
price 
in contrast in the relative price model traders express 
their limit order prices relative to the best price offered in 
their respective book buy or sell thus a buyer would 
encode their limit order price as an offset ∆ which may be 
positive negative or zero from the current bid pb which is 
then translated to the limit price pb ∆ again in addition 
to now depending on the state of the order books prices 
may also depend on all manner of exogenous information 
the relative price model can be viewed as modeling traders 
who in addition to perhaps incorporating fundamental 
external information on the stock may also position their 
orders strategically relative to the other orders on their side 
of the book a common example of such strategic 
behavior is known as penny-jumping on wall street in which 
a trader who has in interest in buying shares quickly but 
still at a discount to placing a market order will 
deliberately position their order just above the current bid more 
generally the entire area of modern execution optimization 
 has come to rely heavily on the careful positioning 
of limit orders relative to the current order book state note 
that such positioning may depend on more complex features 
of the order books than just the current bid and ask but 
the relative model is a natural and simplified starting point 
we remark that an alternate view of the two models is that 
all traders behave in a relative manner but with absolute 
traders able to act only on a considerably slower time scale 
than the faster relative traders 
how do these two models differ clearly given any fixed 
sequence of arriving limit order prices we can choose to 
express these prices either as their original absolute values 
or we can run the order book dynamical process and 
transform each order into a relative difference with the top of 
its book and obtain identical results the differences arise 
when we consider the stability question introduced above 
intuitively in the absolute model a small perturbation in 
the arriving limit price sequence should have limited but 
still some effects on the subsequent evolution of the order 
books since prices are determined independently for the 
relative model this intuition is less clear it seems possible 
that a small perturbation could for example slightly 
modify the current bid which in turn could slightly modify the 
price of the next arriving order which could then slightly 
modify the price of the subsequent order and so on leading 
to an amplifying sequence of events 
our main results demonstrate that these two models do 
indeed have dramatically different stability properties we 
first show that for any fixed sequence of prices in the 
absolute model the modification of a single order has a bounded 
and extremely limited impact on the subsequent evolution 
of the books in particular we define a natural notion of 
distance between order books and show that small 
modifications can result in only constant distance to the original 
books for all subsequent time steps we then show that this 
implies that for almost any standard statistic of market 
activity - the executed volume the average price execution 
price and many others - the statistic can be influenced 
only infinitesimally by small perturbations 
in contrast we show that the relative model enjoys no 
such stability properties after giving specific worst-case 
relative price sequences in which small perturbations 
generate large changes in basic statistics for example altering the 
number of shares traded by a factor of two we proceed to 
demonstrate that the difference in stability properties of the 
two models is more than merely theoretical using 
extensive inet a major ecn for nasdaq stocks limit order 
data and order book reconstruction code we investigate the 
empirical stability properties when the data is interpreted 
as containing either absolute prices relative prices or 
mixtures of the two the theoretical predictions of stability and 
instability are strongly borne out by the subsequent 
experiments 
in addition to stability being of fundamental interest in 
any important dynamical system we believe that the 
results described here provide food for thought on the 
topics of market impact and the backtesting of quantitative 
trading strategies the attempt to determine hypothetical 
past performance using historical data they suggest that 
one s confidence that trading quietly and in small 
volumes will have minimal market impact is linked to an 
implicit belief in an absolute price model our results and the 
fact that in the real markets there is a large and increasing 
amount of relative behavior such as penny-jumping would 
seem to cast doubts on such beliefs similarly in a purely or 
largely relative-price world backtesting even low-frequency 
low-volume strategies could result in historical estimates of 
performance that are not only unrelated to future 
performance the usual concern but are not even accurate 
measures of a hypothetical past 
the outline of the paper follows in section we briefly 
review the large literature on market microstructure in 
section we describe the limit order mechanism and our 
formal models section presents our most important 
theoretical results the -modification theorem for the absolute 
price model this theorem is applied in section to derive a 
number of strong stability properties in the absolute model 
section presents specific examples establishing the 
worstcase instability of the relative model section contains 
the simulation studies that largely confirm our theoretical 
findings on inet market data 
 related work 
as was mentioned in the introduction market 
microstructure is an important and timely topic both in academic 
finance and on wall street and consequently has a large and 
varied recent literature here we have space only to 
summarize the main themes of this literature and to provide 
pointers to further readings to our knowledge the stability 
properties of detailed limit order microstructure dynamics 
have not been previously considered however see farmer 
and joshi for an example and survey of other price 
dynamic stability studies 
 
on the more theoretical side there is a rich line of work 
examining what might be considered the game-theoretic 
properties of limit order markets these works model traders 
and market-makers who provide liquidity by offering both 
buy and sell quotes and profit on the difference by utility 
functions incorporating tolerance for risks of price 
movement large positions and other factors and examine the 
resulting equilibrium prices and behaviors common 
findings predict negative price impacts for large trades and price 
effects for large inventory holdings by market-makers an 
excellent and comprehensive survey of results in this area 
can be found in 
there is a similarly large body of empirical work on 
microstructure major themes include the measurement of 
price impacts statistical properties of limit order books and 
attempts to establish the informational value of order books 
 a good overview of the empirical work can be found in 
 of particular note for our interests is which 
empirically studies the distribution of arriving limit order prices in 
several prominent markets this work takes a view of 
arriving prices analogous to our relative model and establishes 
a power-law form for the resulting distributions 
there is also a small but growing number of works 
examining market microstructure topics from a computer 
science perspective including some focused on the use of 
microstructure in algorithms for optimized trade execution 
kakade et al introduced limit order dynamics in 
competitive analysis for one-way and volume-weighted average 
price vwap trading some recent papers have applied 
reinforcement learning methods to trade execution using order 
book properties as state variables 
 microstructure preliminaries 
the following expository background material is adapted 
from the market mechanism we examine in this paper 
is driven by the simple and standard concept of a limit 
order suppose we wish to purchase shares of microsoft 
 msft stock in a limit order we specify not only the 
desired volume shares but also the desired price 
suppose that msft is currently trading at roughly 
a share see figure which shows an actual snapshot of an 
msft order book on inet but we are only willing to buy 
the shares at a share or lower we can choose to 
submit a limit order with this specification and our order 
will be placed in a queue called the buy order book which 
is ordered by price with the highest offered unexecuted buy 
price at the top often referred to as the bid if there are 
multiple limit orders at the same price they are ordered 
by time of arrival with older orders higher in the book 
in the example provided by figure our order would be 
placed immediately after the extant order for shares 
at though we offer the same price this order has 
arrived before ours similarly a sell order book for sell limit 
orders is maintained this time with the lowest sell price 
offered often referred to as the ask at its top 
thus the order books are sorted from the most 
competitive limit orders at the top high buy prices and low sell 
prices down to less competitive limit orders the bid and 
ask prices together are sometimes referred to as the inside 
market and the difference between them as the spread by 
definition the order books always consist exclusively of 
unexecuted orders - they are queues of orders hopefully 
waiting for the price to move in their direction 
figure sample inet order books for msft 
how then do orders get partially executed if a buy 
 sell respectively limit order comes in above the ask 
 below the bid respectively price then the order is matched 
with orders on the opposing books until either the incoming 
order s volume is filled or no further matching is possible 
in which case the remaining incoming volume is placed in 
the books 
for instance suppose in the example of figure a buy 
order for shares arrived with a limit price of 
this order would be partially filled by the two -share 
sell orders at in the sell books the -share sell 
order at and the -share sell order at for 
a total of shares executed the remaining shares 
of the incoming buy order would become the new bid of 
the buy book at it is important to note that the 
prices of executions are the prices specified in the limit orders 
already in the books not the prices of the incoming order 
that is immediately executed thus in this example the 
 executed shares would be at different prices note that 
this also means that in a pure limit order exchange such as 
inet market orders can be simulated by limit orders 
with extreme price values in exchanges such as inet any 
order can be withdrawn or canceled by the party that placed 
it any time prior to execution 
every limit order arrives atomically and instantaneously 
- there is a strict temporal sequence in which orders arrive 
and two orders can never arrive simultaneously this gives 
rise to the definition of the last price of the exchange which 
is simply the last price at which the exchange executed an 
order it is this quantity that is usually meant when people 
casually refer to the ticker price of a stock 
 formal definitions 
we now provide a formal model for the limit order 
pro 
cess described above in this model limit orders arrive in a 
temporal sequence with each order specifying its limit price 
and an indication of its type buy or sell like the actual 
exchanges we also allow cancellation of a standing 
 unexecuted order in the books any time prior to its execution 
without loss of generality we limit attention to a model in 
which every order is for a single share large order volumes 
can be represented by -share sequences 
definition let σ σ σn be a sequence of limit 
orders where each σi has the form ni ti vi here ni is an 
order identifier ti is the order type buy sell or cancel and 
vi is the limit order value in the case that ti is a cancel ni 
matches a previously placed order and vi is ignored 
we have deliberately called vi in the definition above the 
limit order value rather than price since our two models 
will differ in their interpretation of vi as being absolute or 
relative in the absolute model we do indeed interpret vi 
as simply being the price of the limit order in the 
relative model if the current order book configuration is a b 
 where a is the sell and b the buy book the price of the 
order is ask a vi if ti is sell and bid b vi if ti is buy 
where by ask x and bid x we denote the price of the 
order at the top of the book x note vi can be negative 
our main interest in this paper is the effects that the 
modification of a small number of limit orders can have on the 
resulting dynamics for simplicity we consider only 
modifications to the limit order values but our results generalize 
to any modification 
definition a k-modification of σ is a sequence σ 
such that for exactly k indices i ik vij vij 
 tij tij 
 
and nij nij 
 for every ij j ∈ { k} σ σ 
we now define the various quantities whose stability 
properties we examine in the absolute and relative models all of 
these are standard quantities of common interest in financial 
markets 
 volume σ number of shares executed traded in the 
sequence σ 
 average σ average execution price 
 close σ price of the last closing execution 
 lastbid σ bid at the end of the sequence 
 lastask σ ask at end of the sequence 
 the -modification theorem 
in this section we provide our most important technical 
result it shows that in the absolute model the effects that 
the modification of a single order has on the resulting 
evolution of the order books is extremely limited we then apply 
this result to derive strong stability results for all of the 
aforementioned quantities in the absolute model 
throughout this section we consider an arbitrary order 
sequence σ in the absolute model and any -modification 
σ of σ at any point index i in the two sequences we shall 
use a b to denote the sell and buy books respectively 
in σ and a b to denote the sell and buy books in σ 
for notational convenience we omit explicitly superscripting 
by the current index i we will shortly establish that at all 
times i a b and a b are very close 
although the order books are sorted by price we will use 
 for example a ∪ {a } a to indicate that a contains 
an order at some price a that is not present in a but that 
otherwise a and a are identical thus deleting the order 
at a in a would render the books the same similarly 
b ∪ {b } b ∪ {b } means b contains an order at price 
b not present in b b contains an order at price b not 
present in b and that otherwise b and b are identical 
using this notation we now define a set of stable system 
states where each state is composed from the order books 
of the original and the modified sequences shortly we show 
that if we change only one order s value price we remain 
in this set for any sequence of limit orders 
definition let ab be the set of all states a b 
and a b such that a a and b b let ¯ab be 
the set of states such that a ∪ {a } a ∪ {a } where 
a a and b b let a¯b be the set of states such that 
b ∪{b } b ∪{b } where b b and a a let ¯a¯b be 
the set of states in which a a ∪{a } and b b ∪{b } 
or in which a a ∪ {a } and b b ∪ {b } finally 
we define s ab ∪ ¯ab ∪ ¯ba ∪ ¯a¯b as the set of stable states 
theorem -modification theorem consider any 
sequence of orders σ and any -modification σ of σ then 
the order books a b and a b determined by σ and 
σ lie in the set s of stable states at all times 
ab 
¯a¯b 
a¯b¯ab 
figure diagram representing the set s of stable 
states and the possible movements transitions in it 
after the change 
the idea of the proof of this theorem is contained in 
figure which shows a state transition diagram labeled by the 
categories of stable states this diagram describes all 
transitions that can take place after the arrival of the order on 
which σ and σ differ the following establishes that 
immediately after the arrival of this differing order the state lies 
in s 
lemma if at any time the current books a b and 
 a b are in the set ab and thus identical then 
modifying the price of the next order keeps the state in s 
proof suppose the arriving order is a sell order and we 
change it from a to a assume without loss of generality 
that a a if neither order is executed immediately then 
we move to state ¯ab if both of them are executed then we 
stay in state ab and if only a is executed then we move to 
state ¯a¯b the analysis of an arriving buy order is similar 
following the arrival of their only differing order σ and 
σ are identical we now give a sequence of lemmas showing 
 
executed with two orders 
not executed in both 
arrivng buy order 
arriving buy order 
arriving buy order 
arriving sell order 
¯ab 
ab 
¯a¯b 
executed only with a 
 not a and a 
executed with a and a 
figure the state diagram when starting at state 
¯ab this diagram provides the intuition of lemma 
 
that following the initial difference covered by lemma 
the state remains in s forever on the remaining identical 
sequence we first show that from state ¯ab we remain in 
s regardless the next order the intuition of this lemma is 
demonstrated in figure 
lemma if the current state is in the set ¯ab then for 
any order the state will remain in s 
proof we first provide the analysis for the case of an 
arriving sell order note that in ¯ab the buy books are identical 
 b b thus either the arriving sell order is executed 
with the same buy order in both buy books or it is not 
executed in both buy books for the first case the buy 
books remain identical the bid is executed in both and the 
sell books remain unchanged for the second case the buy 
books remain unchanged and identical and the sell books 
have the new sell order added to both of them and thus 
still differ by one order 
next we provide an analysis of the more subtle case where 
the arriving item is a buy order for this case we need to 
take care of several different scenarios the first is when the 
top of both sell books the ask is identical then 
regardless of whether the new buy order is executed or not the 
state remains in ¯ab the analysis is similar to an arriving 
sell order 
we are left to deal with case where ask a and ask a 
are different here we discuss two subcases a ask a 
a and ask a a and b ask a a and ask a 
a here a and a are as in the definition of ¯ab in 
definition and a is some other price for subcase a by our 
assumption a a then either both asks get executed 
the sell books become identical and we move to state ab 
 neither ask is executed and we remain in state ¯ab or 
only ask a a is executed in which case we move to 
state ¯a¯b with a a ∪ {a } and b b ∪ {b } where 
b is the arriving buy order price for subcase b either 
 buy order is executed in neither sell book we remain in 
state ¯ab or the buy order is executed in both sell books 
and stay in state ¯ab with a ∪ {a } a ∪ {a } or only 
ask a a is executed and we move to state ¯a¯b 
lemma if the current state is in the set a¯b then for 
any order the state will remain in s 
lemma if the current configuration is in the set ¯a¯b 
then for any order the state will remain in s 
the proofs of these two lemmas are omitted but are 
similar in spirit to that of lemma the next and final lemma 
deals with cancellations 
lemma if the current order book state lies in s then 
following the arrival of a cancellation it remains in s 
proof when a cancellation order arrives one of the 
following possibilities holds the order is still in both sets of 
books it is not in either of them and it is only in one 
of them for the first two cases it is easy to see that the 
cancellation effect is identical on both sets of books and thus 
the state remains unchanged for the case when the order 
appears only in one set of books without loss of generality 
we assume that the cancellation cancels a buy order at b 
rather than removing b from the book we can change it to 
have price meaning this buy order will never be executed 
and is effectively canceled now regardless the state that we 
were in b is still only in one buy book but with a different 
price and thus we remain in the same state in s 
the proof of theorem follows from the above lemmas 
 absolute model stability 
in this section we apply the -modification theorem to 
show strong stability properties for the absolute model we 
begin with an examination of the executed volume 
lemma let σ be any sequence and σ be any 
 modification of σ then the set of the executed orders id 
numbers generated by the two sequences differs by at most 
 
proof by theorem we know that at each stage the 
books differ by at most two orders now since the union of 
the ids of the executed orders and the order books is always 
identical for both sequences this implies that the executed 
orders can differ by at most two 
corollary let σ be any sequence and σ be any 
kmodification of σ then the set of the executed orders id 
numbers generated by the two sequences differs by at most 
 k 
an order sequence σ is a k-extension of σ if σ can be 
obtained by deleting any k orders in σ 
lemma let σ be any sequence and let σ be any 
kextension of σ then the set of the executed orders generated 
by σ and σ differ by at most k 
this lemma is the key to obtain our main absolute model 
volume result below we use edit σ σ to denote the 
standard edit distance between the sequences σ and σ - the 
minimal number of substitutions insertions and deletions or 
orders needed to change σ to σ 
theorem let σ and σ be any absolute model order 
sequences then if edit σ σ ≤ k the set of the executed 
orders generated by σ and σ differ by at most k in 
particular volume σ − volume σ ≤ k 
proof we first define the sequence ˜σ which is the 
intersection of σ and σ since σ and σ are at most k apart we 
have that by k insertions we change ˜σ to either σ or σ and 
by lemma its set of executed orders is at most k from 
each thus the set of executed orders in σ and σ is at most 
 k apart 
 
 spread bounds 
theorem establishes strong stability for executed 
volume in the absolute model we now turn to the quantities 
that involve execution prices as opposed to volume alone 
- namely average σ close σ lastbid σ and lastask σ 
for these results unlike executed volume a condition must 
hold on σ in order for stability to occur this condition 
is expressed in terms of a natural measure of the spread of 
the market or the gap between the buyers and sellers we 
motivate this condition by first showing that without it by 
changing one order we can change average σ by any 
positive value x 
lemma there exists σ such that for any x ≥ 
there is a -modification σ of σ such that average σ 
average σ x 
proof let σ be a sequence of alternating sell and buy 
orders in which each seller offers p and each buyer p x 
and the first order is a sell then all executions take place 
at the ask which is always p and thus average σ p 
now suppose we modify only the first sell order to be at 
price p x this initial sell order will never be executed 
and now all executions take place at the bid which is always 
p x 
similar instability results can be shown to hold for the 
other price-based quantities this motivates the 
introduction of a quantity we call the second spread of the order 
books which is defined as the difference between the prices 
of the second order in the sell book and the second order in 
the buy book as opposed to the bid-ask difference which is 
commonly called the spread we note that in a liquid stock 
such as those we examine experimentally in section the 
second spread will typically be quite small and in fact almost 
always equal to the spread 
in this subsection we consider changes in the sequence 
only after an initialization period and sequences such that 
the second spread is always defined after the time we make a 
change we define s σ to be the maximum second spread 
in the sequence σ following the change 
theorem let σ be a sequence and let σ be any 
 modification of σ then 
 lastbid σ − lastbid σ ≤ s σ 
 lastask σ − lastask σ ≤ s σ 
where s σ is the maximum over the second spread in σ 
following the -modification 
proof we provide the proof for the last bid the proof 
for the last ask is similar the proof relies on theorem 
and considers states in the stable set s for states ab and ¯ab 
we have that the bid is identical let bid x sb x ask x 
be the bid the second highest buy order and the ask of a 
sequence x now recall that in state a¯b we have that the sell 
books are identical and that the two buy books are identical 
except one different order thus 
bid σ s σ ≥ sb σ s σ ≥ ask σ ask σ ≥ bid σ 
now it remains to bound bid σ here we use the fact that 
the bid of the modified sequence is at least the second 
highest buy order in the original sequence due to the fact that 
the books are different only in one order since 
bid σ ≥ sb σ ≥ ask σ − s σ ≥ bid σ − s σ 
we have that bid σ − bid σ ≤ s σ as desired 
in state ¯a¯b we have that for one sequence the books 
contain an additional buy order and an additional sell order 
first suppose that the books containing the additional 
orders are the original sequence σ now if the bid is not the 
additional order we are done otherwise we have the 
following 
bid σ ≤ ask σ ≤ sb σ s σ bid σ s σ 
where sb σ ≤ bid σ since the original buy book has only 
one additional order 
now assume that the books with the additional orders are 
for the modified sequence σ we have 
bid σ s σ ≥ ask σ ≥ ask σ ≥ bid σ 
where we used the fact that ask σ ≥ ask σ since the 
modified sequence has an additional order similarly we 
have that bid σ ≤ bid σ since the modified buy book 
contains an additional order 
we note that the proof of theorem actually establishes 
that the bid and ask of the original and modified sequences 
are within s σ at all times 
next we provide a technical lemma which relates the first 
spread of the modified sequence to the second spread of the 
original sequence 
lemma let σ be a sequence and let σ be any 
 modification of σ then the spread of σ is bounded by 
s σ 
proof by the -modification theorem we know that 
the books of the modified sequence and the original sequence 
can differ by at most one order in each book buy and sell 
therefore the second-highest buy order in the original 
sequence is always at most the bid in the modified sequence 
and the second-lowest sell order in the original sequence is 
always at least the ask of the modified sequence 
we are now ready to state a stability result for the average 
execution price in the absolute model it establishes that in 
highly liquid markets where the executed volume is large 
and the spread small the average price is highly stable 
theorem let σ be a sequence and let σ be any 
 modification of σ then 
 average σ − average σ ≤ 
 pmax s σ 
volume σ 
 s σ 
where pmax is the highest execution price in σ 
proof the proof will show that every execution in σ 
besides the execution of the modified order and the last 
execution has a matching execution in σ with a price different 
by at most s σ and will use the fact that pmax s σ is 
a bound on the price in σ 
referring to the proof of the -modification theorem 
suppose we are in state ¯a¯b where we have in one sequence 
 which can be either σ or σ an additional buy order b 
and an additional sell order a without loss of generality 
we assume that the sequence with the additional orders is 
σ if the next execution does not involve a or b then clearly 
we have the same execution in both σ and σ suppose 
that it involves a there are two possibilities either a is the 
modified order in which case we change the average price 
 
difference by pmax s σ volume σ and this can happen 
only once or a was executed before in σ and the executions 
both involve an order whose limit price is a by lemma 
the spread of both sequences is bounded by s σ which 
implies that the price of the execution in σ was at most 
a s σ while execution is in σ is at price a and thus the 
prices are different by at most s σ 
in states ¯ab a¯b as long as we have concurrent executions 
in the two sequences we know that the prices can differ 
by at most s σ if we have an execution only in one 
sequence we either match it in state ¯a¯b or charge it by 
 pmax s σ volume σ if we end at state ¯a¯b 
if we end in state ab ¯ab or a¯b then every execution in 
states ¯ab or a¯b were matched to an execution in state ¯a¯b if 
we end up in state ¯a¯b we have the one execution that is not 
matched and thus we charge it pmax s σ volume σ 
we next give a stability result for the closing price we 
first provide a technical lemma regarding the prices of 
consecutive executions 
lemma let σ be any sequence then the prices of 
two consecutive executions in σ differ by at most s σ 
proof suppose the first execution is taken at time t 
its price is bounded below by the current bid and above by 
the current ask now after this execution the bid is at least 
the second highest buy order at time t if the former bid 
was executed and no higher buy orders arrived and higher 
otherwise similarly the ask is at most the second lowest 
sell order at time t therefore the next execution price is 
at least the second bid at time t and at most the second ask 
at time t which is at most s σ away from the bid ask at 
time t 
lemma let σ be any sequence and let σ be a 
 modification of σ if the volume σ ≥ then 
 close σ − close σ ≤ s σ 
proof we first deal with case where the last execution 
occurs in both sequences simultaneously by theorem 
both the ask and the bid of σ and σ are at most s σ 
apart at every time t since the price of the last execution 
is their asks bids at time t we are done 
next we deal with the case where the last execution among 
the two sequences occurs only in σ in this case we know 
that either the previous execution happened simultaneously 
in both sequences at time t and thus all three executions are 
within the second spread of σ at time t the first execution 
in σ by definition the execution at σ from identical 
arguments as in the former case and the third by lemma 
otherwise the previous execution happened only in σ at 
time t in which case the two executions are within the the 
spread of σ at time t the execution of σ from the same 
arguments as before and the execution in σ must be inside 
its spread in time t 
if the last execution happens only in σ we know that 
the next execution of σ will be at most s σ away from 
its previous execution by lemma together with the 
fact that if an execution happens only in one sequence it 
implies that the order is in the spread of the second sequence 
as long as the sequences are -modification the proof is 
completed 
 spread bounds for k-modifications 
as in the case of executed volume we would like to extend 
the absolute model stability results for price-based 
quantities to the case where multiple orders are modified here our 
results are weaker and depend on the k-spread the distance 
between the kth highest buy order and the kth lowest sell 
order instead of the second spread looking ahead to 
section we note that in actual market data for liquid stocks 
this quantity is often very small as well we use sk σ to 
denote the k-spread as before we assume that the k-spread 
is always defined after an initialization period 
we first state the following generalization of lemma 
lemma let σ be a sequence and let σ be any 
 modification of σ for ≥ if s σ is always defined 
after the change then s σ ≤ s σ 
the proof is similar to the proof of lemma and 
omitted a simple application of this lemma is the following let 
σ be any sequence which is an -modification of σ then 
we have s σ ≤ s σ now using the above lemma and 
by simple induction we can obtain the following theorem 
theorem let σ be a sequence and let σ be any 
k-modification of σ then 
 lastbid σ − lastbid σ ≤ 
pk 
 s σ ≤ ksk σ 
 lastask σ −lastask σ ≤ 
pk 
 s σ ≤ ksk σ 
 close σ − close σ ≤ 
pk 
 s σ ≤ ksk σ 
 average σ − average σ ≤ 
pk 
 
 
 pmax s σ 
volume σ 
 s σ 
 
where s σ is the maximum over the -spread in σ following 
the first modification 
we note that while these bounds depend on deeper 
measures of spread for more modifications we are working in 
a -share order model thus in an actual market where 
single orders contain hundreds or thousands of shares the 
k-spread even for large k might be quite small and close to 
the standard -spread in liquid stocks 
 relative model instability 
in the relative model the underlying assumption is that 
traders try to exploit their knowledge of the books to 
strategically place their orders thus if a trader wants her buy 
order to be executed quickly she may position it above the 
current bid and be the first in the queue if the trader is 
patient and believes that the price trend is going to be 
downward she will place orders deeper in the buy book and so 
on 
while in the previous sections we showed stability results 
for the absolute model here we provide simple examples 
which show instability in the relative model for the 
executed volume last bid last ask average execution price and 
the last execution price in section we provide many 
simulations on actual market data that demonstrate that this 
instability is inherent to the relative model and not due 
to artificial constructions in the relative model we assume 
that for every sequence the ask and bid are always defined 
so the books have a non-empty initial configuration 
 
we begin by showing that in the relative model even a 
single modification can double the number of shares 
executed 
theorem there is a sequence σ and a -modification 
σ of σ such that volume σ ≥ volume σ 
proof for concreteness we assume that at the 
beginning the ask is and the bid is the sequence σ is 
composed from n buy orders with ∆ followed by n sell 
orders with ∆ and finally an alternating sequence of 
buy orders with ∆ and sell orders with ∆ − of 
length n since the books before the alternating sequence 
contain n sell orders at and n buy orders at we 
have that each pair of buy sell order in the alternating part 
is matched and executed but none of the initial n orders 
is executed and thus volume σ n now we change the 
first buy order to have ∆ after the first n orders 
there are still no executions however the books are 
different now there are n sell orders at n buy orders at 
and one buy order at now each order in the alternating 
sequence is executed with one of the former orders and we 
have volume σ n 
the next theorem shows that the spread-based stability 
results of section do not also hold in the relative model 
before providing the proof we give its intuition at the 
beginning the sell book contains only two prices which are far 
apart and both contain only two orders now several buy 
orders arrive at the original sequence they are not being 
executed while in the modified sequence they will be 
executed and leave the sell book with only the orders at the high 
price now many sell orders followed by many buy orders 
will arrive such that in the original sequence they will be 
executed only at the low price and in the modified sequence 
they will executed at the high price 
theorem for any positive numbers s and x there 
is sequence σ such that s σ s and a -modification σ 
of σ such that 
 close σ − close σ ≥ x 
 average σ − average σ ≥ x 
 lastbid σ − lastbid σ ≥ x 
 lastask σ − lastask σ ≥ x 
proof without loss of generality let us consider sequences 
in which all prices are integer-valued in which case the 
smallest possible value for the second spread is we provide 
the proof for the case s σ but the s σ case is 
similar 
we consider a sequence σ such that after an initialization 
period there have been no executions the buy book has 
 orders at price and the sell book has two orders at 
price and orders with value y where y is a positive 
integer that will be determined by the analysis the original 
sequence σ is a buy order with ∆ followed by two 
buy orders with ∆ then y sell orders with ∆ 
and then y buy orders with ∆ we first note that 
s σ there are y executions all at price the last 
bid is and the last ask is next we analyze a modified 
sequence we change the first buy order from ∆ to 
∆ therefore the next two buy orders with ∆ 
are executed and afterwards we have that the bid is and 
the ask is y now the y sell orders are accumulated at 
 y and after the next y buy orders the bid is at y− 
therefore at the end we have that lastbid σ y − 
lastask σ y close σ y and average σ 
y 
y 
 y 
y 
 setting y x we obtain the 
lemma for every property 
we note that while this proof was based on the fact that 
there are two consecutive orders in the books which are far 
 y apart we can provide a slightly more complicated 
example in which all orders are close at most apart yet still 
one change results in large differences 
 simulation studies 
the results presented so far paint a striking contrast 
between the absolute and relative price models while the 
absolute model enjoys provably strong stability over any fixed 
event sequence there exist at least specific sequences 
demonstrating great instability in the relative model the 
worstcase nature of these results raises the question of the extent 
to which such differences could actually occur in real 
markets in this section we provide indirect evidence on this 
question by presenting simulation results exploiting a rich 
source of real-market historical limit order sequence data 
by interpreting arriving limit order prices as either 
absolute values or by transforming them into differences with 
the current bid and ask relative model we can perform 
small modifications on the sequences and examine how 
different various outcomes volume traded average price etc 
would be from what actually occurred in the market these 
simulations provide an empirical counterpart to the theory 
we have developed we emphasize that all such simulations 
interpret the actual historical data as falling into either the 
absolute or relative model and are meaningful only within 
the confines of such an interpretation nevertheless we feel 
they provide valuable empirical insight into the potential 
 in stability properties of modern equity limit order 
markets and demonstrate that one s belief or hope in stability 
largely relies on an absolute model interpretation we also 
investigate the empirical behavior of mixtures of absolute 
and relative prices 
 data 
the historical data used in our simulations is 
commercially available limit order data from inet the previously 
mentioned electronic exchange for nasdaq stocks broadly 
speaking this data consists of practically every single event 
on inet regarding the trading of an individual 
stockevery arriving limit order price volume and sequence id 
number every execution and every cancellation of a 
standing order - all timestamped in milliseconds it is data 
sufficient to recreate the precise inet order book in a given 
stock on a given day and time 
we will report stability properties for three stocks 
amazon nvidia and qualcomm identified in the sequel by their 
tickers amzn nvda and qcom these three provide 
some range of liquidities with qcom having the greatest 
and nvda the least liquidity on inet and other trading 
properties we note that the qualitative results of our 
simulations were similar for several other stocks we examined 
 
 methodology 
for our simulations we employed order-book 
reconstruction code operating on the underlying raw data the basic 
format of each experiment was the following 
 run the order book reconstruction code on the 
original inet data and compute the quantity of interest 
 volume traded average price etc 
 make a small modification to a single order and 
recompute the resulting value of the quantity of interest 
in the absolute model case step is as simple as 
modifying the order in the original data and re-running the order 
book reconstruction for the relative model we must first 
pre-process the raw data and convert its prices to relative 
values then make the modification and re-run the order 
book reconstruction on the relative values 
the type of modification we examined was extremely small 
compared to the volume of orders placed in these stocks 
namely the deletion of a single randomly chosen order from 
the sequence although a deletion is not -modification 
its edit distance is and we can apply theorem for 
each trading day examined this single deleted order was 
selected among those arriving between am and pm and 
the quantities of interest were measured and compared at 
pm these times were chosen to include the busiest part of 
the trading day but avoid the half hour around the opening 
and closing of the official nasdaq market am and 
 pm respectively which are known to have different 
dynamics than the central portion of the day 
we run the absolute and relative model simulations on 
both the raw inet data and on a cleaned version of 
this data in the cleaned we remove all limit orders that 
were canceled in the actual market prior to their execution 
 along with the cancellations themselves the reason is 
that such cancellations may often be the first step in the 
repositioning of orders - that is cancellations of the 
order that are followed by the submission of a replacement 
order at a different price not removing canceled orders 
allows the possibility of modified simulations in which the 
same order 
is executed twice which may magnify 
instability effects again it is clear that neither the raw nor 
the cleaned data can perfectly reflect what would have 
happened under the deleted orders in the actual market 
however the results both from the raw data and the clean 
data are qualitatively similar the results mainly differ as 
expected in the executed volume where the instability 
results for the relative model are much more dramatic in the 
raw data 
 results 
we begin with summary statistics capturing our overall 
stability findings each row of the tables below contains a 
ticker e g amzn followed by either -r for the uncleaned 
or raw data or -c for the data with canceled orders 
removed for each of the approximately trading days 
in trials were run in which a randomly selected 
order was deleted from the inet event sequence for each 
quantity of interest volume executed average price closing 
price and last bid we show for the both the absolute and 
 
here same is in quotes since the two orders will actually 
have different sequence id numbers which is what makes 
such repositioning activity impossible to reliably detect in 
the data 
relative model the average percentage change in the quantity 
induced by the deletion 
the results confirm rather strikingly the qualitative 
conclusions of the theory we have developed in virtually every 
case stock raw or cleaned data and quantity the 
percentage change induced by a single deletion in the relative 
model is many orders of magnitude greater than in the 
absolute model and shows that indeed butterfly effects may 
occur in a relative model market as just one specific 
representative example notice that for qcom on the cleaned 
data the relative model effect of just a single deletion on the 
closing price is in excess of a full percentage point this is 
a variety of market impact entirely separate from the more 
traditional and expected kind generated by trading a large 
volume of shares 
stock date volume average 
rel abs rel abs 
amzn-r 
amzn-c 
nvda-r 
nvda-c 
qcom-r 
qcom-c 
stock date close lastbid 
rel abs rel abs 
amzn-r 
amzn-c 
nvda-r 
nvda-c 
qcom-r 
qcom-c 
in figure we examine how the change to one the 
quantities the average execution price grows with the 
introduction of greater perturbations of the event sequence in the two 
models rather than deleting only a single order between 
 am and pm in these experiments a growing number 
of randomly chosen deletions was performed and the 
percentage change to the average price measured as suggested 
by the theory we have developed for the absolute model the 
change to the average price grows linearly with the number 
of deletions and remains very small note the vastly 
different scales of the y-axis in the panels for the absolute and 
relative models in the figure for the relative model it is 
interesting to note that while small numbers of changes have 
large effects often causing average execution price changes 
well in excess of percent the effects of large numbers of 
changes levels off quite rapidly and consistently 
we conclude with an examination of experiments with a 
mixture model even if one accepts a world in which traders 
behave in either an absolute or relative manner one would 
be likely to claim that the market contains a mixture of both 
we thus ran simulations in which each arriving order in the 
inet event streams was treated as an absolute price with 
probability α and as a relative price with probability −α 
representative results for the average execution price in this 
mixture model are shown in figure for amzn and nvda 
perhaps as expected we see a monotonic decrease in the 
percentage change instability as the fraction of absolute 
traders increases with most of the reduction already being 
realized by the introduction of just a small population of 
absolute traders thus even in a largely relative-price world a 
 
 
 
 
 
 
 
 
x 
− qcom−r june absolute 
number of changes 
averageprice 
 
 
 
 
 
 
 
 
 
qcom−r june relative 
number of changes 
averageprice 
figure percentage change to the average 
execution price y-axis as a function of the number of 
deletions to the sequence x-axis the left panel is 
for the absolute model the right panel for the 
relative model and each curve corresponds to a single 
day of qcom trading in june curves 
represent averages over trials 
small minority of absolute traders can have a greatly 
stabilizing effect similar behavior is found for closing price and 
last bid 
 
 
 
 
 
 
 
 
 
amzn−r feburary 
α 
averageprice 
 
 
 
 
 
 
 
 
 
 
nvda−r june 
α 
averageprice 
figure percentage change to the average 
execution price y-axis vs probability of treating 
arriving inet orders as absolute prices x-axis each 
curve corresponds to a single day of trading during 
a month of curves represent averages over 
 trials 
for the executed volume in the mixture model however 
the findings are more curious in figure we show how 
the percentage change to the executed volume varies with 
the absolute trader fraction α for nvda data that is both 
raw and cleaned of cancellations we first see that for this 
quantity unlike the others the difference induced by the 
cleaned and uncleaned data is indeed dramatic as already 
suggested by the summary statistics table above but most 
intriguing is the fact that the stability is not monotonically 
increasing with α for either the cleaned or uncleaned 
datathe market with maximum instability is not a pure relative 
price market but occurs at some nonzero value for α it was 
in fact not obvious to us that sequences with this property 
could even be artificially constructed much less that they 
would occur as actual market data we have yet to find a 
satisfying explanation for this phenomenon and leave it to 
future research 
 acknowledgments 
we are grateful to yuriy nevmyvaka of lehman brothers 
in new york for the use of his inet order book 
reconstruction code and for valuable comments on the work presented 
 
 
 
 
 
 
 
 
 
 
nvda−c june 
α 
volume 
 
 
 
 
 
 
 
 
 
 
 
nvda−r june 
α 
volume 
figure percentage change to the executed volume 
 y-axis vs probability of treating arriving inet 
orders as absolute prices x-axis the left panel is 
for nvda using the raw data that includes 
cancellations while the right panel is on the cleaned data 
each curve corresponds to a single day of trading 
during june curves represent averages over 
 trials 
here yishay mansour was supported in part by the ist 
programme of the european community under the pascal 
network of excellence ist- - by a grant from 
the israel science foundation and an ibm faculty award 
 references 
 d bertsimas and a lo optimal control of execution 
costs journal of financial markets - 
 b biais l glosten and c spatt market 
microstructure a survey of microfoundations 
empirical results and policy implications journal of 
financial markets - 
 j -p bouchaud m mezard and m potters 
statistical properties of stock order books empirical 
results and models quantitative finance - 
 
 c cao o hansch and x wang the informational 
content of an open limit order book afa 
philadelphia meetings efa maastricht meetings 
paper no 
 r coggins a blazejewski and m aitken optimal 
trade execution of equities in a limit order market in 
international conference on computational 
intelligence for financial engineering pages - 
march 
 d farmer and s joshi the price dynamics of 
common trading strategies journal of economic 
behavior and organization - 
 j hasbrouck empirical market microstructure 
economic and statistical perspectives on the dynamics 
of trade in securities markets course notes 
stern school of business new york university 
 r kissell and m glantz optimal trading strategies 
amacom 
 s kakade m kearns y mansour and l ortiz 
competitive algorithms for vwap and limit order 
trading in proceedings of the acm conference on 
electronic commerce pages - 
 y nevmyvaka y feng and m kearns reinforcement 
learning for optimized trade execution preprint 
 
marginal contribution nets a compact representation 
scheme for coalitional games 
∗ 
samuel ieong 
† 
computer science department 
stanford university 
stanford ca 
sieong stanford edu 
yoav shoham 
computer science department 
stanford university 
stanford ca 
shoham stanford edu 
abstract 
we present a new approach to representing coalitional games 
based on rules that describe the marginal contributions of 
the agents this representation scheme captures 
characteristics of the interactions among the agents in a natural and 
concise manner we also develop efficient algorithms for two 
of the most important solution concepts the shapley value 
and the core under this representation the shapley value 
can be computed in time linear in the size of the input the 
emptiness of the core can be determined in time 
exponential only in the treewidth of a graphical interpretation of our 
representation 
categories and subject descriptors 
i distributed artificial intelligence multiagent 
systems j social and behavioral sciences 
economics f analysis of algorithms and problem 
complexity 
general terms 
algorithms economics 
 introduction 
agents can often benefit by coordinating their actions 
coalitional games capture these opportunities of 
coordination by explicitly modeling the ability of the agents to take 
joint actions as primitives as an abstraction coalitional 
games assign a payoff to each group of agents in the game 
this payoff is intended to reflect the payoff the group of 
agents can secure for themselves regardless of the actions 
of the agents not in the group these choices of primitives 
are in contrast to those of non-cooperative games of which 
agents are modeled independently and their payoffs depend 
critically on the actions chosen by the other agents 
 coalitional games and e-commerce 
coalitional games have appeared in the context of 
e-commerce in kleinberg et al use coalitional games to study 
recommendation systems in their model each individual 
knows about a certain set of items is interested in learning 
about all items and benefits from finding out about them 
the payoffs to groups of agents are the total number of 
distinct items known by its members given this coalitional 
game setting kleinberg et al compute the value of the 
private information of the agents is worth to the system using 
the solution concept of the shapley value definition can be 
found in section these values can then be used to 
determine how much each agent should receive for participating 
in the system 
as another example consider the economics behind 
supply chain formation the increased use of the internet as a 
medium for conducting business has decreased the costs for 
companies to coordinate their actions and therefore 
coalitional game is a good model for studying the supply chain 
problem suppose that each manufacturer purchases his raw 
materials from some set of suppliers and that the suppliers 
offer higher discount with more purchases the decrease in 
communication costs will let manufacturers find others 
interested in the same set of suppliers cheaper and facilitates 
formation of coalitions to bargain with the suppliers 
depending on the set of suppliers and how much from each 
supplier each coalition purchases we can assign payoffs to 
the coalitions depending on the discount it receives the 
resulting game can be analyzed using coalitional game 
theory and we can answer questions such as the stability of 
coalitions and how to fairly divide the benefits among the 
participating manufacturers a similar problem 
combinatorial coalition formation has previously been studied in 
 evaluation criteria for coalitional game 
representation 
to capture the coalitional games described above and 
perform computations on them we must first find a 
representation for these games the na¨ıve solution is to enumerate 
the payoffs to each set of agents therefore requiring space 
 
exponential in the number of agents in the game for the 
two applications described the number of agents in the 
system can easily exceed a hundred this na¨ıve approach will 
not be scalable to such problems therefore it is critical to 
find good representation schemes for coalitional games 
we believe that the quality of a representation scheme 
should be evaluated by four criteria 
expressivity the breadth of the class of coalitional games 
covered by the representation 
conciseness the space requirement of the representation 
efficiency the efficiency of the algorithms we can develop 
for the representation 
simplicity the ease of use of the representation by users 
of the system 
the ideal representation should be fully expressive i e it 
should be able to represent any coalitional games use as 
little space as possible have efficient algorithms for 
computation and be easy to use the goal of this paper is to 
develop a representation scheme that has properties close to 
the ideal representation 
unfortunately given that the number of degrees of 
freedom of coalitional games is o n 
 not all games can be 
represented concisely using a single scheme due to information 
theoretic constraints for any given class of games one may 
be able to develop a representation scheme that is tailored 
and more compact than a general scheme for example for 
the recommendation system game a highly compact 
representation would be one that simply states which agents know 
of which products and let the algorithms that operate on 
the representation to compute the values of coalitions 
appropriately for some problems however there may not be 
efficient algorithms for customized representations by 
having a general representation and efficient algorithms that go 
with it the representation will be useful as a prototyping 
tool for studying new economic situations 
 previous work 
the question of coalitional game representation has only 
been sparsely explored in the past in deng 
and papadimitriou focused on the complexity of different 
solution concepts on coalitional games defined on graphs 
while the representation is compact it is not fully 
expressive in conitzer and sandholm looked into the problem 
of determining the emptiness of the core in superadditive 
games they developed a compact representation scheme 
for such games but again the representation is not fully 
expressive either in conitzer and sandholm developed a 
fully expressive representation scheme based on 
decomposition our work extends and generalizes the representation 
schemes in through decomposing the game into a set of 
rules that assign marginal contributions to groups of agents 
we will give a more detailed review of these papers in section 
 after covering the technical background 
 summary of our contributions 
 we develop the marginal contribution networks 
representation a fully expressive representation scheme 
whose size scales according to the complexity of the 
interactions among the agents we believe that the 
representation is also simple and intuitive 
 we develop an algorithm for computing the shapley 
value of coalitional games under this representation 
that runs in time linear in the size of the input 
 under the graphical interpretation of the 
representation we develop an algorithm for determining the 
whether a payoff vector is in the core and the emptiness 
of the core in time exponential only in the treewidth 
of the graph 
 preliminaries 
in this section we will briefly review the basics of 
coalitional game theory and its two primary solution concepts 
the shapley value and the core 
we will also review 
previous work on coalitional game representation in more detail 
throughout this paper we will assume that the payoff to 
a group of agents can be freely distributed among its 
members this assumption is often known as the transferable 
utility assumption 
 technical background 
we can represent a coalition game with transferable utility 
by the pair n v where 
 n is the set of agents and 
 v n 
→ r is a function that maps each group of 
agents s ⊆ n to a real-valued payoff 
this representation is known as the characteristic form as 
there are exponentially many subsets it will take space 
exponential in the number of agents to describe a coalitional 
game 
an outcome in a coalitional game specifies the utilities 
the agents receive a solution concept assigns to each 
coalitional game a set of reasonable outcomes different 
solution concepts attempt to capture in some way outcomes 
that are stable and or fair two of the best known solution 
concepts are the shapley value and the core 
the shapley value is a normative solution concept it 
prescribes a fair way to divide the gains from cooperation 
when the grand coalition i e n is formed the division 
of payoff to agent i is the average marginal contribution of 
agent i over all possible permutations of the agents 
formally let φi v denote the shapley value of i under 
characteristic function v then 
φi v 
s⊂n 
s n − s − 
n 
 v s ∪ {i} − v s 
the shapley value is a solution concept that satisfies many 
nice properties and has been studied extensively in the 
economic and game theoretic literature it has a very useful 
axiomatic characterization 
efficiency eff a total of v n is distributed to the 
agents i e i∈n φi v v n 
symmetry sym if agents i and j are interchangeable 
then φi v φj v 
 
the materials and terminology are based on the textbooks 
by mas-colell et al and osborne and rubinstein 
 
as a notational convenience we will use the lower-case 
letter to represent the cardinality of a set denoted by the 
corresponding upper-case letter 
 
dummy dum if agent i is a dummy player i e his 
marginal contribution to all groups s are the same 
φi v v {i} 
additivity add for any two coalitional games v and 
w defined over the same set of agents n φi v w 
φi v φi w for all i ∈ n where the game v w is 
defined as v w s v s w s for all s ⊆ n 
we will refer to these axioms later in our proof of correctness 
of the algorithm for computing the shapley value under our 
representation in section 
the core is another major solution concept for coalitional 
games it is a descriptive solution concept that focuses on 
outcomes that are stable stability under core means that 
no set of players can jointly deviate to improve their payoffs 
formally let x s denote i∈s xi an outcome x ∈ rn 
is 
in the core if 
∀s ⊆ n x s ≥ v s 
the core was one of the first proposed solution concepts 
for coalitional games and had been studied in detail an 
important question for a given coalitional game is whether 
the core is empty in other words whether there is any 
outcome that is stable relative to group deviation for a 
game to have a non-empty core it must satisfy the property 
of balancedness defined as follows let s ∈ rn 
denote the 
characteristic vector of s given by 
 s i 
 if i ∈ s 
 otherwise 
let λs s⊆n be a set of weights such that each λs is in the 
range between and this set of weights λs s⊆n is a 
balanced collection if for all i ∈ n 
s⊆n 
λs s i 
a game is balanced if for all balanced collections of weights 
s⊆n 
λsv s ≤ v n 
by the bondereva-shapley theorem the core of a 
coalitional game is non-empty if and only if the game is 
balanced therefore we can use linear programming to 
determine whether the core of a game is empty 
maximize 
λ∈r n s⊆n λsv s 
subject to s⊆n λs s ∀i ∈ n 
λs ≥ ∀s ⊆ n 
 
if the optimal value of is greater than the value of the 
grand coalition then the core is empty unfortunately this 
program has an exponential number of variables in the 
number of players in the game and hence an algorithm that 
operates directly on this program would be infeasible in practice 
in section we will describe an algorithm that answers 
the question of emptiness of core that works on the dual of 
this program instead 
 previous work revisited 
deng and papadimitriou looked into the complexity of 
various solution concepts on coalitional games played on 
weighted graphs in in their representation the set of 
agents are the nodes of the graph and the value of a set of 
agents s is the sum of the weights of the edges spanned by 
them notice that this representation is concise since the 
space required to specify such a game is o n 
 however 
this representation is not general it will not be able to 
represent interactions among three or more agents for example 
it will not be able to represent the majority game where a 
group of agents s will have value of if and only if s n 
on the other hand there is an efficient algorithm for 
computing the shapley value of the game and for determining 
whether the core is empty under the restriction of positive 
edge weights however in the unrestricted case 
determining whether the core is non-empty is conp-complete 
conitzer and sandholm in considered coalitional games 
that are superadditive they described a concise 
representation scheme that only states the value of a coalition if the 
value is strictly superadditive more precisely the semantics 
of the representation is that for a group of agents s 
v s max 
{t t tn}∈π 
i 
v ti 
where π is the set of all possible partitions of s the value 
v s is only explicitly specified for s if v s is greater than 
all partitioning of s other than the trivial partition {s} 
while this representation can represent all games that are 
superadditive there are coalitional games that it cannot 
represent for example it will not be able to represent any 
games with substitutability among the agents an 
example of a game that cannot be represented is the unit game 
where v s as long as s ∅ under this 
representation the authors showed that determining whether the core 
is non-empty is conp-complete in fact even determining 
the value of a group of agents is np-complete 
in a more recent paper conitzer and sandholm described 
a representation that decomposes a coalitional game into a 
number of subgames whose sum add up to the original game 
 the payoffs in these subgames are then represented by 
their respective characteristic functions this scheme is fully 
general as the characteristic form is a special case of this 
representation for any given game there may be multiple 
ways to decompose the game and the decomposition may 
influence the computational complexity for computing the 
shapley value the authors showed that the complexity is 
linear in the input description in particular if the largest 
subgame as measured by number of agents is of size n and 
the number of subgames is m then their algorithm runs 
in o m n 
 time where the input size will also be o m n 
 
on the other hand the problem of determining whether a 
certain outcome is in the core is conp-complete 
 marginal contribution nets 
in this section we will describe the marginal contribution 
networks representation scheme we will show that the idea 
is flexible and we can easily extend it to increase its 
conciseness we will also show how we can use this scheme to 
represent the recommendation game from the introduction 
finally we will show that this scheme is fully expressive 
and generalizes the representation schemes in 
 rules and marginalcontributionnetworks 
the basic idea behind marginal contribution networks 
 mc-nets is to represent coalitional games using sets of 
rules the rules in mc-nets have the following syntactic 
 
form 
pattern → value 
a rule is said to apply to a group of agents s if s meets 
the requirement of the pattern in the basic scheme these 
patterns are conjunctions of agents and s meets the 
requirement of the given pattern if s is a superset of it the 
value of a group of agents is defined to be the sum over the 
values of all rules that apply to the group for example if 
the set of rules are 
{a ∧ b} → 
{b} → 
then v {a} v {b} and v {a b} 
mc-nets is a very flexible representation scheme and can 
be extended in different ways one simple way to extend 
it and increase its conciseness is to allow a wider class of 
patterns in the rules a pattern that we will use throughout 
the remainder of the paper is one that applies only in the 
absence of certain agents this is useful for expressing 
concepts such as substitutability or default values formally 
we express such patterns by 
{p ∧ p ∧ ∧ pm ∧ ¬n ∧ ¬n ∧ ∧ ¬nn} 
which has the semantics that such rule will apply to a group 
s only if {pi}m 
i ∈ s and {nj}n 
j ∈ s we will call 
the {pi}m 
i in the above pattern the positive literals and 
{nj}n 
j the negative literals note that if the pattern of 
a rule consists solely of negative literals we will consider 
that the empty set of agents will also satisfy such pattern 
and hence v ∅ may be non-zero in the presence of negative 
literals 
to demonstrate the increase in conciseness of 
representation consider the unit game described in section to 
represent such a game without using negative literals we 
will need n 
rules for n players we need a rule of value 
for each individual agent a rule of value − for each pair of 
agents to counter the double-counting a rule of value for 
each triplet of agents etc similar to the inclusion-exclusion 
principle on the other hand using negative literals we 
only need n rules value for the first agent value for the 
second agent in the absence of the first agent value for the 
third agent in the absence of the first two agents etc the 
representational savings can be exponential in the number 
of agents 
given a game represented as a mc-net we can interpret 
the set of rules that make up the game as a graph we call 
this graph the agent graph the nodes in the graph will 
represent the agents in the game and for each rule in the 
mcnet we connect all the agents in the rule together and assign 
a value to the clique formed by the set of agents notice that 
to accommodate negative literals we will need to annotate 
the clique appropriately this alternative view of mc-nets 
will be useful in our algorithm for core-membership in 
section 
we would like to end our discussion of the representation 
scheme by mentioning a trade-off between the 
expressiveness of patterns and the space required to represent them 
to represent a coalitional game in characteristic form one 
would need to specify all n 
− values there is no 
overhead on top of that since there is a natural ordering of the 
groups for mc-nets however specification of the rules 
requires specifying both the patterns and the values the 
patterns if not represented compactly may end up 
overwhelming the savings from having fewer values to specify 
the space required for the patterns also leads to a 
tradeoff between the expressiveness of the allowed patterns and 
the simplicity of representing them however we believe 
that for most naturally arising games there should be 
sufficient structure in the problem such that our representation 
achieves a net saving over the characteristic form 
 example recommendation game 
as an example we will use mc-net to represent the 
recommendation game discussed in the introduction for each 
product as the benefit of knowing about the product will 
count only once for each group we need to capture 
substitutability among the agents this can be captured by a 
scaled unit game suppose the value of the knowledge about 
product i is vi and there are ni agents denoted by {xj 
i } 
who know about the product the game for product i can 
then be represented as the following rules 
{x 
i } → vi 
{x 
i ∧ ¬x 
i } → vi 
 
{xni 
i ∧ ¬xni− 
i ∧ · · · ∧ ¬x 
i } → vi 
the entire game can then be built up from the sets of rules 
of each product the space requirement will be o mn∗ 
 
where m is the number of products in the system and n∗ 
is the maximum number of agents who knows of the same 
product 
 representation power 
we will discuss the expressiveness and conciseness of our 
representation scheme and compare it with the previous 
works in this subsection 
proposition marginal contribution networks 
constitute a fully expressive representation scheme 
proof consider an arbitrary coalitional game n v in 
characteristic form representation we can construct a set 
of rules to describe this game by starting from the singleton 
sets and building up the set of rules for any singleton set 
{i} we create a rule {i} → v i for any pair of agents {i j} 
we create a rule {i ∧ j} → v {i j} − v {i} − v {j} we 
can continue to build up rules in a manner similar to the 
inclusion-exclusion principle since the game is arbitrary 
mc-nets are fully expressive 
using the construction outlined in the proof we can show 
that our representation scheme can simulate the multi-issue 
representation scheme of in almost the same amount of 
space 
proposition marginal contribution networks use at 
most a linear factor in the number of agents more space 
than multi-issue representation for any game 
proof given a game in multi-issue representation we 
start by describing each of the subgames which are 
represented in characteristic form in with a set of rules 
 
we then build up the grand game by including all the rules 
from the subgames note that our representation may 
require a space larger by a linear factor due to the need to 
describe the patterns for each rule on the other hand our 
approach may have fewer than exponential number of rules 
for each subgame depending on the structure of these 
subgames and therefore may be more concise than multi-issue 
representation 
on the other hand there are games that require 
exponentially more space to represent under the multi-issue scheme 
compared to our scheme 
proposition marginal contribution networks are 
exponentially more concise than multi-issue representation for 
certain games 
proof consider a unit game over all the agents n as 
explained in this game can be represented in linear space 
using mc-nets with negative literals however as there is 
no decomposition of this game into smaller subgames it will 
require space o n 
 to represent this game under the 
multiissue representation 
under the agent graph interpretation of mc-nets we can 
see that mc-nets is a generalization of the graphical 
representation in namely from weighted graphs to weighted 
hypergraphs 
proposition marginal contribution networks can 
represent any games in graphical form under in the same 
amount of space 
proof given a game in graphical form g for each edge 
 i j with weight wij in the graph we create a rule {i j} → 
wij clearly this takes exactly the same space as the size of 
g and by the additive semantics of the rules it represents 
the same game as g 
 computing the shapley value 
given a mc-net we have a simple algorithm to compute 
the shapley value of the game considering each rule as a 
separate game we start by computing the shapley value of 
the agents for each rule for each agent we then sum up 
the shapley values of that agent over all the rules we first 
show that this final summing process correctly computes the 
shapley value of the agents 
proposition the shapley value of an agent in a marginal 
contribution network is equal to the sum of the shapley 
values of that agent over each rule 
proof for any group s under the mc-nets 
representation v s is defined to be the sum over the values of all the 
rules that apply to s therefore considering each rule as a 
game by the add axiom discussed in section the 
shapley value of the game created from aggregating all the rules 
is equal to the sum of the shapley values over the rules 
the remaining question is how to compute the shapley 
values of the rules we can separate the analysis into two 
cases one for rules with only positive literals and one for 
rules with mixed literals 
for rules that have only positive literals the shapley value 
of the agents is v m where v is the value of the rule and 
m is the number of agents in the rule this is a direct 
consequence of the sym axiom of the shapley value as 
the agents in a rule are indistinguishable from each other 
for rules that have both positive and negative literals we 
can consider the positive and the negative literals separately 
for a given positive literal i the rule will apply only if i 
occurs in a given permutation after the rest of the positive 
literals but before any of the negative literals formally let 
φi denote the shapley value of i p denote the cardinality of 
the positive set and n denote the cardinality of the negative 
set then 
φi 
 p − n 
 p n 
v 
v 
p p n 
n 
for a given negative literal j j will be responsible for 
cancelling the application of the rule if all positive literals come 
before the negative literals in the ordering and j is the first 
among the negative literals therefore 
φj 
p n − 
 p n 
 −v 
−v 
n p n 
p 
by the sym axiom all positive literals will have the value 
of φi and all negative literals will have the value of φj 
note that the sum over all agents in rules with mixed 
literals is this is to be expected as these rules contribute 
 to the grand coalition the fact that these rules have no 
effect on the grand coalition may appear odd at first but 
this is because the presence of such rules is to define the 
values of coalitions smaller than the grand coalition 
in terms of computational complexity given that the 
shapley value of any agent in a given rule can be computed in 
time linear in the pattern of the rule the total running time 
of the algorithm for computing the shapley value of the 
game is linear in the size of the input 
 answering core-related 
questions 
there are a few different but related computational 
problems associated with the solution concept of the core we 
will focus on the following two problems 
definition core-membership given a coalitional game 
and a payoff vector x determine if x is in the core 
definition core-non-emptiness given a coalitional 
game determine if the core is non-empty 
in the rest of the section we will first show that these 
two problems are conp-complete and conp-hard 
respectively and discuss some complexity considerations about 
these problems we will then review the main ideas of tree 
decomposition as it will be used extensively in our algorithm 
for core-membership next we will present the algorithm 
for core-membership and show that the algorithm runs 
in polynomial time for graphs of bounded treewidth we end 
by extending this algorithm to answer the question of 
corenon-emptiness in polynomial time for graphs of bounded 
treewidth 
 computational complexity 
the hardness of core-membership and 
core-nonemptiness follows directly from the hardness results of games 
over weighted graphs in 
 
proposition core-membership for games represented 
as marginal contribution networks is conp-complete 
proof core-membership in mc-nets is in the class 
of conp since any set of agents s of which v s x s 
will serve as a certificate to show that x does not belong to 
the core as for its hardness given any instance of 
coremembership for a game in graphical form of we can 
encode the game in exactly the same space using mc-net 
due to proposition since core-membership for games 
in graphical form is conp-complete core-membership in 
mc-nets is conp-hard 
proposition core-non-emptiness for games 
represented as marginal contribution networks is conp-hard 
proof the same argument for hardness between games 
in graphical frm and mc-nets holds for the problem of 
corenon-emptiness 
we do not know of a certificate to show that 
core-nonemptiness is in the class of conp as of now note that 
the obvious certificate of a balanced set of weights based 
on the bondereva-shapley theorem is exponential in size in 
 deng and papadimitriou showed the conp-completeness 
of core-non-emptiness via a combinatorial 
characterization namely that the core is non-empty if and only if 
there is no negative cut in the graph in mc-nets however 
there need not be a negative hypercut in the graph for the 
core to be empty as demonstrated by the following game 
 n { } 
v s 
 
 
 
 if s { } 
 if s { } { } { } or { } 
 otherwise 
 
applying the bondereva-shapley theorem if we let λ 
λ λ and λ this set of weights 
demonstrates that the game is not balanced and hence the core 
is empty on the other hand this game can be represented 
with mc-nets as follows weights on hyperedges 
w { } w { } w { } 
w { } w { } w { } − 
w { } 
w { } 
no matter how the set is partitioned the sum over the 
weights of the hyperedges in the cut is always non-negative 
to overcome the computational hardness of these 
problems we have developed algorithms that are based on tree 
decomposition techniques for core-membership our 
algorithm runs in time exponential only in the treewidth of the 
agent graph thus for graphs of small treewidth such as 
trees we have a tractable solution to determine if a payoff 
vector is in the core by using this procedure as a 
separation oracle i e a procedure for returning the inequality 
violated by a candidate solution to solving a linear 
program that is related to core-non-emptiness using the 
ellipsoid method we can obtain a polynomial time algorithm 
for core-non-emptiness for graphs of bounded treewidth 
 review of tree decomposition 
as our algorithm for core-membership relies heavily 
on tree decomposition we will first briefly review the main 
ideas in tree decomposition and treewidth 
definition a tree decomposition of a graph g v e 
is a pair x t where t i f is a tree and x {xi i ∈ 
i} is a family of subsets of v one for each node of t such 
that 
 i∈i xi v 
 for all edges v w ∈ e there exists an i ∈ i with 
v ∈ xi and w ∈ xi and 
 running intersection property for all i j k ∈ i if j 
is on the path from i to k in t then xi ∩ xk ⊆ xj 
the treewidth of a tree decomposition is defined as the 
maximum cardinality over all sets in x less one the treewidth 
of a graph is defined as the minimum treewidth over all tree 
decompositions of the graph 
given a tree decomposition we can convert it into a nice 
tree decomposition of the same treewidth and of size linear 
in that of t 
definition a tree decomposition t is nice if t is rooted 
and has four types of nodes 
leaf nodes i are leaves of t with xi 
introduce nodes i have one child j such that xi xj ∪ 
{v} of some v ∈ v 
forget nodes i have one child j such that xi xj \ {v} 
for some v ∈ xj 
join nodes i have two children j and k with xi xj 
xk 
an example of a partial nice tree decomposition together 
with a classification of the different types of nodes is in 
figure in the following section we will refer to nodes in the 
tree decomposition as nodes and nodes in the agent graph 
as agents 
 algorithm for core membership 
our algorithm for core-membership takes as an input 
a nice tree decomposition t of the agent graph and a payoff 
vector x by definition if x belongs to the core then for 
all groups s ⊆ n x s ≥ v s therefore the difference 
x s −v s measures how close the group s is to violating 
the core condition we call this difference the excess of group 
s 
definition the excess of a coalition s e s is defined 
as x s − v s 
a brute-force approach to determine if a payoff vector 
belongs to the core will have to check that the excesses of all 
groups are non-negative however this approach ignores the 
structure in the agent graph that will allow an algorithm to 
infer that certain groups have non-negative excesses due to 
 
this is based largely on the materials from a survey paper 
by bodlaender 
 
i 
j 
k l 
nm 
introduce node 
xj { } 
xk { } 
forget node 
xl { } 
introduce node 
xm { } xn { } 
leaf node 
join node 
xi { } 
join node 
figure example of a partial nice tree 
decomposition 
the excesses computed elsewhere in the graph tree 
decomposition is the key to take advantage of such inferences in a 
structured way 
for now let us focus on rules with positive literals 
suppose we have already checked that the excesses of all sets 
r ⊆ u are non-negative and we would like to check if the 
addition of an agent i to the set u will create a group with 
negative excess a na¨ıve solution will be to compute the 
excesses of all sets that include i the excess of the group 
 r ∪ {i} for any group r can be computed as follows 
e r ∪ {i} e r xi − v c 
where c is the cut between r and i and v c is the sum of 
the weights of the edges in the cut 
however suppose that from the tree decomposition we 
know that i is only connected to a subset of u say s which 
we will call the entry set to u ideally because i does not 
share any edges with members of ¯u u \ s we would 
hope that an algorithm can take advantage of this structure 
by checking only sets that are subsets of s ∪ {i} this 
computational saving may be possible since xi −v c in the 
update equation of does not depend on ¯u however we 
cannot simply ignore ¯u as members of ¯u may still influence 
the excesses of groups that include agent i through group 
s specifically if there exists a group t ⊃ s such that 
e t e s then even when e s ∪ {i} has non-negative 
excess e t ∪{i} may have negative excess in other words 
the excess available at s may have been drained away due 
to t this motivates the definition of the reserve of a group 
definition the reserve of a coalition s relative to a 
coalition u is the minimum excess over all coalitions between 
s and u i e all t s ⊆ t ⊆ u we denote this value by 
r s u we will refer to the group t that has the minimum 
excess as arg r s u we will also call u the limiting set of 
the reserve and s the base set of the reserve 
our algorithm works by keeping track of the reserves of 
all non-empty subsets that can be formed by the agents of a 
node at each of the nodes of the tree decomposition starting 
from the leaves of the tree and working towards the root 
at each node i our algorithm computes the reserves of all 
groups s ⊆ xi limited by the set of agents in the subtree 
rooted at i ti except those in xi\s the agents in xi\s 
are excluded to ensure that s is an entry set specifically 
s is the entry set to ti \ xi ∪ s 
to accomodate for negative literals we will need to make 
two adjustments firstly the cut between an agent m and a 
set s at node i now refers to the cut among agent m set s 
and set ¬ xi \ s and its value must be computed 
accordingly also when an agent m is introduced to a group at an 
introduce node we will also need to consider the change in 
the reserves of groups that do not include m due to possible 
cut involving ¬m and the group 
as an example of the reserve values we keep track of at a 
tree node consider node i of the tree in figure at node 
i we will keep track of the following 
r { } { } 
r { } { } 
r { } { } 
r { } { } 
r { } { } 
r { } { } 
r { } { } 
where the dots refer to the agents rooted under node m 
for notational use we will use ri s to denote r s u at 
node i where u is the set of agents in the subtree rooted at 
node i excluding agents in xi \ s we sometimes refer to 
these values as the r-values of a node the details of the 
r-value computations are in algorithm 
to determine if the payoff vector x is in the core during 
the r-value computation at each node we can check if all of 
the r-values are non-negative if this is so for all nodes in 
the tree the payoff vector x is in the core the correctness 
of the algorithm is due to the following proposition 
proposition the payoff vector x is not in the core if 
and only if the r-values at some node i for some group s is 
negative 
proof ⇐ if the reserve at some node i for some group 
s is negative then there exists a coalition t for which 
e t x t − v t hence x is not in the core 
 ⇒ suppose x is not in the core then there exists some 
group r∗ 
such that e r∗ 
 let xroot be the set of nodes 
at the root consider any set s ∈ xroot rroot s will have 
the base set of s and the limiting set of n \ xroot ∪ s 
the union over all of these ranges includes all sets u for 
which u ∩ xroot ∅ therefore if r∗ 
is not disjoint from 
xroot the r-value for some group in the root is negative 
if r∗ 
is disjoint from u consider the forest {ti} resulting 
from removal of all tree nodes that include agents in xroot 
 
algorithm subprocedures for core membership 
leaf-node i 
 ri xi ← e xi 
introduce-node i 
 j ← child of i 
 m ← xi \ xj {the introduced node} 
 for all s ⊆ xj s ∅ do 
 c ← all hyperedges in the cut of m s and ¬ xi \ s 
 ri s ∪ {x} ← rj s xm − v c 
 c ← all hyperedges in the cut of ¬m s and ¬ xi \s 
 ri s ← rj s − v c 
 end for 
 r {m} ← e {m} 
forget-node i 
 j ← child of i 
 m ← xj \ xi {the forgotten node} 
 for all s ⊆ xi s ∅ do 
 ri s min rj s rj s ∪ {m} 
 end for 
join-node i 
 {j k} ← {left right} child of i 
 for all s ⊆ xi s ∅ do 
 ri s ← rj s rk s − e s 
 end for 
by the running intersection property the sets of nodes in 
the trees ti s are disjoint thus if the set r∗ 
 i si for 
some si ∈ ti e r∗ 
 i e si implies some group 
s∗ 
i has negative excess as well therefore we only need to 
check the r-values of the nodes on the individual trees in the 
forest 
but for each tree in the forest we can apply the same 
argument restricted to the agents in the tree in the base 
case we have the leaf nodes of the original tree 
decomposition say for agent i if r∗ 
 {i} then r {i} e {i} 
therefore by induction if e r∗ 
 some reserve at some 
node would be negative 
we will next explain the intuition behind the correctness 
of the computations for the r-values in the tree nodes a 
detailed proof of correctness of these computations can be 
found in the appendix under lemmas and 
proposition the procedure in algorithm correctly 
compute the r-values at each of the tree nodes 
proof sketch we can perform a case analysis over 
the four types of tree nodes in a nice tree decomposition 
leaf nodes i the only reserve value to be computed is 
ri xi which equals r xi xi and therefore it is just 
the excess of group xi 
forget nodes i with child j let m be the forgotten node 
for any subset s ⊆ xi arg ri s must be chosen 
between the groups of s and s ∪ {m} and hence we 
choose between the lower of the two from the r-values 
at node j 
introduce nodes i with child j let m be the introduced 
node for any subset t ⊆ xi that includes m let s 
denote t \ {m} by the running intersection 
property there are no rules that involve m and agents of 
the subtree rooted at node i except those involving 
m and agents in xi as both the base set and the 
limiting set of the r-values of node j and node i 
differ by {m} for any group v that lies between the 
base set and the limiting set of node i the excess of 
group v will differ by a constant amount from the 
corresponding group v \ {m} at node j therefore 
the set arg ri t equals the set arg rj s ∪ {m} and 
ri t rj s xm − v cut where v cut is the value 
of the rules in the cut between m and s for any 
subset s ⊂ xi that does not include m we need to 
consider the values of rules that include ¬m as a literal 
in the pattern also when computing the reserve the 
payoff xm will not contribute to group s therefore 
together with the running intersection property as 
argued above we can show that ri s rj s − v cut 
join nodes i with left child j and right child k for any 
given set s ⊆ xi consider the r-values of that set 
at j and k if arg rj s or arg rk s includes agents 
not in s then argrj s and argrk s will be 
disjoint from each other due to the running intersection 
property therefore we can decompose arg ri s into 
three sets arg rj s \ s on the left s in the middle 
and arg rk s \ s on the right the reserve rj s 
will cover the excesses on the left and in the middle 
whereas the reserve rk s will cover those on the right 
and in the middle and so the excesses in the middle is 
double-counted we adjust for the double-counting by 
subtracting the excesses in the middle from the sum 
of the two reserves rj s and rk s 
finally note that each step in the computation of the 
rvalues of each node i takes time at most exponential in the 
size of xi hence the algorithm runs in time exponential only 
in the treewidth of the graph 
 algorithm for core non-emptiness 
we can extend the algorithm for core-membership into 
an algorithm for core-non-emptiness as described in 
section whether the core is empty can be checked using 
the optimization program based on the balancedness 
condition unfortunately that program has an exponential 
number of variables on the other hand the dual of the 
program has only n variables and can be written as follows 
minimize 
x∈rn 
n 
i xi 
subject to x s ≥ v s ∀s ⊆ n 
 
by strong duality optimal value of is equal to 
optimal value of the primal program described in section 
 therefore by the bondereva-shapley theorem if the 
optimal value of is greater than v n the core is empty 
we can solve the dual program using the ellipsoid method 
with core-membership as a separation oracle i e a 
procedure for returning a constraint that is violated note that 
a simple extension to the core-membership algorithm will 
allow us to keep track of the set t for which e t 
during the r-values computation and hence we can return the 
inequality about t as the constraint violated therefore 
core-non-emptiness can run in time polynomial in the 
running time of core-membership which in turn runs in 
 
time exponential only in the treewidth of the graph note 
that when the core is not empty this program will return 
an outcome in the core 
 concluding remarks 
we have developed a fully expressive representation scheme 
for coalitional games of which the size depends on the 
complexity of the interactions among the agents our focus 
on general representation is in contrast to the approach 
taken in we have also developed an efficient 
algorithm for the computation of the shapley values for this 
representation while core-membership for mc-nets is 
conp-complete we have developed an algorithm for 
coremembership that runs in time exponential only in the treewidth 
of the agent graph we have also extended the algorithm 
to solve core-non-emptiness other than the algorithm 
for core-non-emptiness in under the restriction of 
non-negative edge weights and that in for 
superadditive games when the value of the grand coalition is given 
we are not aware of any explicit description of algorithms 
for core-related problems in the literature 
the work in this paper is related to a number of areas 
in computer science especially in artificial intelligence for 
example the graphical interpretation of mc-nets is closely 
related to markov random fields mrfs of the bayes nets 
community they both address the issue of of conciseness 
of representation by using the combinatorial structure of 
weighted hypergraphs in fact kearns et al first apply 
these idea to games theory by introducing a representation 
scheme derived from bayes net to represent non-cooperative 
games the representational issues faced in coalitional 
games are closely related to the problem of expressing 
valuations in combinatorial auctions the or-bid 
language for example is strongly related to superadditivity 
the question of the representation power of different 
patterns is also related to boolean expression complexity 
we believe that with a better understanding of the 
relationships among these related areas we may be able to develop 
more efficient representations and algorithms for coalitional 
games 
finally we would like to end with some ideas for 
extending the work in this paper one direction to increase the 
conciseness of mc-nets is to allow the definition of 
equivalent classes of agents similar to the idea of extending bayes 
nets to probabilistic relational models the concept of 
symmetry is prevalent in games and the use of classes of agents 
will allow us to capture symmetry naturally and concisely 
this will also address the problem of unpleasing assymetric 
representations of symmetric games in our representation 
along the line of exploiting symmetry as the agents within 
the same class are symmetric with respect to each other we 
can extend the idea above by allowing functional description 
of marginal contributions more concretely we can specify 
the value of a rule as dependent on the number of agents 
of each relevant class the use of functions will allow 
concise description of marginal diminishing returns mdrs 
without the use of functions the space needed to describe 
mdrs among n agents in mc-nets is o n with the use 
of functions the space required can be reduced to o 
another idea to extend mc-nets is to augment the 
semantics to allow constructs that specify certain rules cannot be 
applied simultaneously this is useful in situations where a 
certain agent represents a type of exhaustible resource and 
therefore rules that depend on the presence of the agent 
should not apply simultaneously for example if agent i in 
the system stands for coal we can either use it as fuel for 
a power plant or as input to a steel mill for making steel 
but not for both at the same time currently to represent 
such situations we have to specify rules to cancel out the 
effects of applications of different rules the augmented 
semantics can simplify the representation by specifying when 
rules cannot be applied together 
 acknowledgment 
the authors would like to thank chris luhrs bob 
mcgrew eugene nudelman and qixiang sun for fruitful 
discussions and the anonymous reviewers for their helpful 
comments on the paper 
 references 
 h l bodlaender treewidth algorithmic techniques 
and results in proc nd symp on mathematical 
foundation of copmuter science pages - 
springer-verlag lncs 
 v conitzer and t sandholm complexity of 
determining nonemptiness of the core in proc th 
int joint conf on artificial intelligence pages 
 - 
 v conitzer and t sandholm computing shapley 
values manipulating value division schemes and 
checking core membership in multi-issue domains in 
proc th nat conf on artificial intelligence pages 
 - 
 x deng and c h papadimitriou on the complexity 
of cooperative solution concepts math oper res 
 - may 
 y fujishima k leyton-brown and y shoham 
taming the computational complexity of 
combinatorial auctions optimal and approximate 
approaches in proc th int joint conf on 
artificial intelligence pages - 
 m kearns m l littman and s singh graphical 
models for game theory in proc th conf on 
uncertainty in artificial intelligence pages - 
 
 j kleinberg c h papadimitriou and p raghavan 
on the value of private information in proc th 
conf on theoretical aspects of rationality and 
knowledge pages - 
 c li and k sycara algoirthms for combinatorial 
coalition formation and payoff division in an electronic 
marketplace technical report robotics insititute 
carnegie mellon university november 
 a mas-colell m d whinston and j r green 
microeconomic theory oxford university press new 
york 
 n nisan bidding and allocation in combinatorial 
auctions in proc nd acm conf on electronic 
commerce pages - 
 m j osborne and a rubinstein a course in game 
theory the mit press cambridge massachusetts 
 
 i wegener the complexity of boolean functions 
john wiley sons new york october 
 
appendix 
we will formally show the correctness of the r-value 
computation in algorithm of introduce nodes and join nodes 
lemma the procedure for computing the r-values of 
introduce nodes in algorithm is correct 
proof let node m be the newly introduced agent at i 
let u denote the set of agents in the subtree rooted at i 
by the running intersection property all interactions the 
hyperedges between m and u must be in node i for all 
s ⊆ xi m ∈ s let r denote u \ xi ∪ s and q denote 
 r \ {m} 
ri s r s r 
 min 
t s⊆t ⊆r 
e t 
 min 
t s⊆t ⊆r 
x t − v t 
 min 
t s⊆t ⊆r 
x t \ {m} xm − v t \ {m} − v cut 
 min 
t s\{m}⊆t ⊆q 
e t xm − v cut 
 rj s xm − v cut 
the argument for sets s ⊆ xi m ∈ s is symmetric except 
xm will not contribute to the reserve due to the absence of 
m 
lemma the procedure for computing the r-values of 
join nodes in algorithm is correct 
proof consider any set s ⊆ xi let uj denote the 
subtree rooted at the left child rj denote uj \ xj ∪ s 
and qj denote uj \ xj let uk rk and qk be defined 
analogously for the right child let r denote u \ xi ∪ s 
ri s r s r 
 min 
t s⊆t ⊆r 
x t − v t 
 min 
t s⊆t ⊆r 
x s x t ∩ qj x t ∩ qk 
− v s − v cut s t ∩ qj − v cut s t ∩ qk 
 min 
t s⊆t ⊆r 
x t ∩ qj − v cut s t ∩ qj 
 min 
t s⊆t ⊆r 
x t ∩ qk − v cut s t ∩ qk 
 x s − v s 
 min 
t s⊆t ⊆r 
x t ∩ qj x s − v cut s t ∩ qj − v s 
 min 
t s⊆t ⊆r 
x t ∩ qk x s − v cut s t ∩ qk − v s 
− x s − v s 
 min 
t s⊆t ⊆r 
e t ∩ rj min 
t s⊆t ⊆r 
e t ∩ rk − e s 
 min 
t s⊆t ⊆rj 
e t min 
t s⊆t ⊆rk 
e t − e s 
 rj s rk s − e s 
where is true as t ∩ qj and t ∩ qk are disjoint due 
to the running intersection property of tree decomposition 
and hence the minimum of the sum can be decomposed into 
the sum of the minima 
 
the sequential auction problem on ebay 
an empirical analysis and a solution 
∗ 
adam i juda 
division of engineering and applied sciences 
harvard university 
harvard business school 
ajuda hbs edu 
david c parkes 
division of engineering and applied sciences 
harvard university 
parkes eecs harvard edu 
abstract 
bidders on ebay have no dominant bidding strategy when 
faced with multiple auctions each offering an item of 
interest as seen through an analysis of auctions on 
ebay for a dell e fp lcd monitor some bidders win 
auctions at prices higher than those of other available 
auctions while others never win an auction despite placing bids 
in losing efforts that are greater than the closing prices of 
other available auctions these misqueues in strategic 
behavior hamper the efficiency of the system and in so doing 
limit the revenue potential for sellers this paper proposes a 
novel options-based extension to ebay s proxy-bidding 
system that resolves this strategic issue for buyers in 
commoditized markets an empirical analysis of ebay provides a 
basis for computer simulations that investigate the market 
effects of the options-based scheme and demonstrates that 
the options-based scheme provides greater efficiency than 
ebay while also increasing seller revenue 
categories and subject descriptors 
j computer applications social and behavioral 
sciences-economics 
general terms 
algorithms design economics 
 introduction 
electronic markets represent an application of information 
systems that has generated significant new trading 
opportunities while allowing for the dynamic pricing of goods in 
addition to marketplaces such as ebay electronic marketplaces 
are increasingly used for business-to-consumer auctions e g 
to sell surplus inventory 
many authors have written about a future in which 
commerce is mediated by online automated trading agents 
 there is still little evidence of automated trading 
in e-markets though we believe that one leading place of 
resistance is in the lack of provably optimal bidding 
strategies for any but the simplest of market designs without 
this we do not expect individual consumers or firms to 
be confident in placing their business in the hands of an 
automated agent 
one of the most common examples today of an electronic 
marketplace is ebay where the gross merchandise volume 
 i e the sum of all successfully closed listings during 
was b among items listed on ebay many are essentially 
identical this is especially true in the consumer 
electronics category which accounted for roughly b of ebay s 
gross merchandise volume in this presence of 
essentially identical items can expose bidders and sellers to risks 
because of the sequential auction problem 
for example alice may want an lcd monitor and could 
potentially bid in either a o clock or o clock ebay 
auction while alice would prefer to participate in whichever 
auction will have the lower winning price she cannot 
determine beforehand which auction that may be and could 
end up winning the wrong auction this is a problem of 
multiple copies 
another problem bidders may face is the exposure 
problem as investigated by bykowsky et al exposure 
problems exist when buyers desire a bundle of goods but may 
only participate in single-item auctions 
for example if 
alice values a video game console by itself for a video 
game by itself for and both a console and game for 
alice must determine how much of the of synergy value 
she might include in her bid for the console alone both 
problems arise in ebay as a result of sequential auctions of 
single items coupled with patient bidders with substitutes 
or complementary valuations 
why might the sequential auction problem be bad 
complex games may lead to bidders employing costly strategies 
and making mistakes potential bidders who do not wish 
to bear such costs may choose not to participate in the 
 
the exposure problem has been primarily investigated by 
bykowsky et al in the context of simultaneous single-item 
auctions the problem is also a familiar one of online 
decision making 
 
market inhibiting seller revenue opportunities 
additionally among those bidders who do choose to participate the 
mistakes made may lead to inefficient allocations further 
limiting revenue opportunities 
we are interested in creating modifications to ebay-style 
markets that simplify the bidder problem leading to simple 
equilibrium strategies and preferably better efficiency and 
revenue properties 
 options proxies a proposed solution 
retail stores have developed policies to assist their 
customers in addressing sequential purchasing problems 
return policies alleviate the exposure problem by allowing 
customers to return goods at the purchase price price 
matching alleviates the multiple copies problem by allowing buyers 
to receive from sellers after purchase the difference between 
the price paid for a good and a lower price found elsewhere 
for the same good furthermore price matching 
can reduce the impact of exactly when a seller brings an 
item to market as the price will in part be set by others 
selling the same item these two retail policies provide the 
basis for the scheme proposed in this paper 
we extend the proxy bidding technology currently 
employed by ebay our super-proxy extension will take 
advantage of a new real options-based market infrastructure 
that enables simple yet optimal bidding strategies the 
extensions are computationally simple handle temporal 
issues and retain seller autonomy in deciding when to enter 
the market and conduct individual auctions 
a seller sells an option for a good which will ultimately 
lead to either a sale of the good or the return of the option 
buyers interact through a proxy agent defining a value on 
all possible bundles of goods in which they have interest 
together with the latest time period in which they are 
willing to wait to receive the good s the proxy agents use 
this information to determine how much to bid for options 
and follow a dominant bidding strategy across all relevant 
auctions a proxy agent exercises options held when the 
buyer s patience has expired choosing options that 
maximize a buyer s payoff given the reported valuation all other 
options are returned to the market and not exercised the 
options-based protocol makes truthful and immediate 
revelation to a proxy a dominant strategy for buyers whatever 
the future auction dynamics 
we conduct an empirical analysis of ebay collecting data 
on over four months of bids for dell lcd screens model 
e fp starting in the summer of lcd screens are 
a high-ticket item for which we demonstrate evidence of 
the sequential bidding problem we first infer a 
conservative model for the arrival time departure time and value of 
bidders on ebay for lcd screens during this period this 
model is used to simulate the performance of the 
optionsbased infrastructure in order to make direct comparisons to 
the actual performance of ebay in this market 
we also extend the work of haile and tamer to 
estimate an upper bound on the distribution of value of ebay 
bidders taking into account the sequential auction 
problem when making the adjustments using this estimate one 
can approximate how much greater a bidder s true value is 
 
prior work has shown price matching as a potential 
mechanism for colluding firms to set monopoly prices however 
in our context auction prices will be matched which are 
not explicitly set by sellers but rather by buyers bids 
from the maximum bid they were observed to have placed 
on ebay based on this approximation revenue generated 
in a simulation of the options-based scheme exceeds 
revenue on ebay for the comparable population and sequence of 
auctions by while the options-based scheme 
demonstrates itself as being more efficient 
 related work 
a number of authors have analyzed the 
multiple copies problem often times in the context of 
categorizing or modeling sniping behavior for reasons other 
than those first brought forward by ockenfels and roth 
 these papers perform equilibrium analysis in simpler 
settings assuming bidders can participate in at most two 
auctions peters severinov extend these models to 
allow buyers to consider an arbitrary number of auctions and 
characterize a perfect bayesian equilibrium however their 
model does not allow auctions to close at distinct times and 
does not consider the arrival and departure of bidders 
previous work have developed a data-driven approach 
toward developing a taxonomy of strategies employed by 
bidders in practice when facing multi-unit auctions but have 
not considered the sequential bidding problem 
previous work has also sought to provide agents with smarter 
bidding strategies unfortunately it seems hard 
to design artificial agents with equilibrium bidding 
strategies even for a simple simultaneous ascending price auction 
iwasaki et al have considered the role of options in 
the context of a single monolithic auction design to help 
bidders with marginal-increasing values avoid exposure in 
a multi-unit homogeneous item auction problem in other 
contexts options have been discussed for selling coal mine 
leases or as leveled commitment contracts for use in a 
decentralized market place most similar to our work 
gopal et al use options for reducing the risks of buyers 
and sellers in the sequential auction problem however their 
work uses costly options and does not remove the sequential 
bidding problem completely 
work on online mechanisms and online auctions 
 considers agents that can dynamically arrive and depart 
across time we leverage a recent price-based 
characterization by hajiaghayi et al to provide a dominant strategy 
equilibrium for buyers within our options-based protocol 
the special case for single-unit buyers is equivalent to the 
protocol of hajiaghayi et al albeit with an options-based 
interpretation 
jiang and leyton-brown use machine learning 
techniques for bid identification in online auctions 
 ebay and the dell e fp 
the most common type of auction held on ebay is a 
singleitem proxy auction auctions open at a given time and 
remain open for a set period of time usually one week 
bidders bid for the item by giving a proxy a value ceiling the 
proxy will bid on behalf of the bidder only as much as is 
necessary to maintain a winning position in the auction up to 
the ceiling received from the bidder bidders may 
communicate with the proxy multiple times before an auction closes 
in the event that a bidder s proxy has been outbid a bidder 
may give the proxy a higher ceiling to use in the auction 
ebay s proxy auction implements an incremental version of 
a vickrey auction with the item sold to the highest bidder 
for the second-highest bid plus a small increment 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
number of auctions 
numberofbidders 
auctions available 
auctions in which bid 
figure histogram of number of lcd auctions 
available to each bidder and number of lcd auctions in which 
a bidder participates 
the market analyzed in this paper is that of a specific 
model of an lcd monitor a dell lcd model e fp 
this market was selected for a variety of reasons including 
 the mean price of the monitor was with 
standard deviation so we believe it reasonable to 
assume that bidders on the whole are only interested in 
acquiring one copy of the item on ebay 
 the volume transacted is fairly high at approximately 
 units sold per month 
 the item is not usually bundled with other items 
 the item is typically sold as new and so suitable for 
the price-matching of the options-based scheme 
raw auction information was acquired via a perl script 
the script accesses the ebay search engine 
and returns all 
auctions containing the terms  dell and  lcd that have 
closed within the past month 
data was stored in a text 
file for post-processing to isolate the auctions in the 
domain of interest queries were made against the titles of ebay 
auctions that closed between may through 
october 
figure provides a general sense of how many lcd 
auctions occur while a bidder is interested in pursuing a 
monitor 
 bidders had more than one auction 
available between when they first placed a bid on ebay and the 
 
for reference dell s october mail order catalogue 
quotes the price of the monitor as being without a 
desktop purchase and as part of a desktop purchase 
upgrade 
 
http search ebay com 
 
the search is not case-sensitive 
 
specifically the query found all auctions where the title 
contained all of the following strings  dell  lcd and 
 e fp while excluding all auctions that contained any 
of the following strings  dimension  ghz  desktop  p 
and  gb the exclusion terms were incorporated so that the 
only auctions analyzed would be those selling exclusively the 
lcd of interest for example the few bundled auctions 
selling both a dell dimension desktop and the e fp lcd 
are excluded 
 
as a reference most auctions close on ebay between noon 
and midnight edt with almost two auctions for the dell 
lcd monitor closing each hour on average during peak time 
periods bidders have an average observed patience of 
days with a standard deviation of days 
latest closing time of an auction in which they bid with an 
average of auctions available figure also illustrates 
the number of auctions in which each bidder participates 
only of bidders who had more than one auction 
available are observed to bid in more than one auction bidding 
in auctions on average a simple regression analysis 
shows that bidders tend to submit maximal bids to an 
auction that are higher after spending twice as much time 
in the system as well as bids that are higher in each 
subsequent auction 
among the bidders that won exactly one monitor and 
participated in multiple auctions paid more than 
 more than the closing price of another auction in which 
they bid paying on average more standard deviation 
 than the closing price of the cheapest auction in which 
they bid but did not win furthermore among the 
bidders that never won an item despite participating in 
multiple auctions placed a losing bid in one auction 
that was more than higher than the closing price of 
another auction in which they bid submitting a losing bid on 
average more standard deviation than the 
closing price of the cheapest auction in which they bid but did 
not win although these measures do not say a bidder that 
lost could have definitively won because we only consider 
the final winning price and not the bid of the winner to her 
proxy or a bidder that won could have secured a better 
price this is at least indicative of some bidder mistakes 
 modeling the sequential 
auction problem 
while the ebay analysis was for simple bidders who 
desire only a single item let us now consider a more general 
scenario where people may desire multiple goods of different 
types possessing general valuations over those goods 
consider a world with buyers sometimes called bidders 
b and k different types of goods g gk let t { } 
denote time periods let l denote a bundle of goods 
represented as a vector of size k where lk ∈ { } denotes 
the quantity of good type gk in the bundle 
the type of a 
buyer i ∈ b is ai di vi with arrival time ai ∈ t departure 
time di ∈ t and private valuation vi l ≥ for each bundle 
of goods l received between ai and di and zero value 
otherwise the arrival time models the period in which a buyer 
first realizes her demand and enters the market while the 
departure time models the period in which a buyer loses 
interest in acquiring the good s in settings with general 
valuations we need an additional assumption an upper bound 
on the difference between a buyer s arrival and departure 
denoted δmax buyers have quasi-linear utilities so that 
the utility of buyer i receiving bundle l and paying p in 
some period no later than di is ui l p vi l − p each 
seller j ∈ s brings a single item kj to the market has no 
intrinsic value and wants to maximize revenue seller j has 
an arrival time aj which models the period in which she 
is first interested in listing the item while the departure 
time dj models the latest period in which she is willing to 
consider having an auction for the item close a seller will 
receive payment by the end of the reported departure of the 
winning buyer 
 
we extend notation whereby a single item k of type gk 
refers to a vector l lk 
 
we say an individual auction in a sequence is locally 
strategyproof lsp if truthful bidding is a dominant strategy 
for a buyer that can only bid in that auction consider the 
following example to see that lsp is insufficient for the 
existence of a dominant bidding strategy for buyers facing a 
sequence of auctions 
example alice values one ton of sand with one ton 
of stone at bob holds a vickrey auction for one ton 
of sand on monday and a vickrey auction for one ton of 
stone on tuesday alice has no dominant bidding strategy 
because she needs to know the price for stone on tuesday to 
know her maximum willingness to pay for sand on monday 
definition the sequential auction problem given 
a sequence of auctions despite each auction being locally 
strategyproof a bidder has no dominant bidding strategy 
consider a sequence of auctions generally auctions 
selling the same item will be uncertainly-ordered because a 
buyer will not know the ordering of closing prices among 
the auctions define the interesting bundles for a buyer as 
all bundles that could maximize the buyer s profit for some 
combination of auctions and bids of other buyers 
within 
the interesting bundles say that an item has uncertain 
marginal value if the marginal value of an item depends on the 
other goods held by the buyer 
say that an item is 
oversupplied if there is more than one auction offering an item 
of that type say two bundles are substitutes if one of those 
bundles has the same value as the union of both bundles 
proposition given locally strategyproof single-item 
auctions the sequential auction problem exists for a bidder if 
and only if either of the following two conditions is true 
within the set of interesting bundles a there are two 
bundles that are substitutes b there is an item with uncertain 
marginal value or c there is an item that is over-supplied 
 a bidder faces competitors bids that are conditioned on 
the bidder s past bids 
proof sketch ⇐ a bidder does not have a dominant 
strategy when a she does not know which bundle among 
substitutes to pursue b she faces the exposure problem 
or c she faces the multiple copies problem additionally 
a bidder does not have a dominant strategy when she does 
not how to optimally influence the bids of competitors ⇒ 
by contradiction a bidder has a dominant strategy to bid 
its constant marginal value for a given item in each auction 
available when conditions and are both false 
for example the following buyers all face the sequential 
auction problem as a result of condition a b and c 
respectively a buyer who values one ton of sand for 
or one ton of stone for but not both sand and stone 
a buyer who values one ton of sand for one ton of 
stone for and one ton of sand and one ton of stone for 
 and can participate in an auction for sand before an 
auction for stone a buyer who values one ton of sand for 
 and can participate in many auctions selling sand 
 
assume that the empty set is an interesting bundle 
 
formally an item k has uncertain marginal value if {m 
m vi q − vi q − k ∀q ⊆ l ∈ interestingbundle q ⊇ 
k} 
 
formally two bundles a and b are substitutes if vi a ∪ 
b max vi a vi b where a ∪ b l where lk 
max ak bk 
 super proxies and options 
the novel solution proposed in this work to resolve the 
sequential auction problem consists of two primary 
components richer proxy agents and options with price matching 
in finance a real option is a right to acquire a real good at 
a certain price called the exercise price for instance alice 
may obtain from bob the right to buy sand from him at 
an exercise price of an option provides the right to 
purchase a good at an exercise price but not the obligation 
this flexibility allows buyers to put together a collection of 
options on goods and then decide which to exercise 
options are typically sold at a price called the option 
price however options obtained at a non-zero option price 
cannot generally support a simple dominant bidding 
strategy as a buyer must compute the expected value of an 
option to justify the cost this computation requires a 
model of the future which in our setting requires a model 
of the bidding strategies and the values of other bidders 
this is the very kind of game-theoretic reasoning that we 
want to avoid 
instead we consider costless options with an option price 
of zero this will require some care as buyers are weakly 
better off with a costless option than without one whatever 
its exercise price however multiple bidders pursuing 
options with no intention of exercising them would cause the 
efficiency of an auction for options to unravel this is the 
role of the mandatory proxy agents which intermediate 
between buyers and the market a proxy agent forces a link 
between the valuation function used to acquire options and 
the valuation used to exercise options if a buyer tells her 
proxy an inflated value for an item she runs the risk of 
having the proxy exercise options at a price greater than her 
value 
 buyer proxies 
 acquiring options 
after her arrival a buyer submits her valuation ˆvi 
 perhaps untruthfully to her proxy in some period ˆai ≥ ai 
along with a claim about her departure time ˆdi ≥ ˆai all 
transactions are intermediated via proxy agents each 
auction is modified to sell an option on that good to the 
highest bidding proxy with an initial exercise price set to the 
second-highest bid received 
when an option in which a buyer is interested becomes 
available for the first time the proxy determines its bid 
by computing the buyer s maximum marginal value for the 
item and then submits a bid in this amount a proxy does 
not bid for an item when it already holds an option the 
bid price is 
bidt 
i k max 
l 
 ˆvi l k − ˆvi l 
by having a proxy compute a buyer s maximum marginal 
value for an item and then bidding only that amount a 
buyer s proxy will win any auction that could possibly be of 
benefit to the buyer and only lose those auctions that could 
never be of value to the buyer 
 
the system can set a reserve price for each good provided 
that the reserve is universal for all auctions selling the same 
item without a universal reserve price price matching is 
not possible because of the additional restrictions on prices 
that individual sellers will accept 
 
buyer type monday tuesday 
molly mon tues nancy nancy → polly 
nancy mon tues - polly 
polly mon tues 
-table three-buyer example with each wanting a 
single item and one auction occurring on monday and 
tuesday xy implies an option with exercise price x and 
bookkeeping that a proxy has prevented y from 
currently possessing an option → is the updating of 
exercise price and bookkeeping 
when a proxy wins an auction for an option the proxy 
will store in its local memory the identity which may be 
a pseudonym of the proxy not holding an option because 
of the proxy s win i e the proxy that it  bumped from 
winning if any this information will be used for price 
matching 
 pricing options 
sellers agree by joining the market to allow the proxy 
representing a buyer to adjust the exercise price of an option 
that it holds downwards if the proxy discovers that it could 
have achieved a better price by waiting to bid in a later 
auction for an option on the same good to assist in the 
implementation of the price matching scheme each proxy 
tracks future auctions for an option that it has already won 
and will determine who would be bidding in that auction 
had the proxy delayed its entry into the market until this 
later auction the proxy will request price matching from 
the seller that granted it an option if the proxy discovers that 
it could have secured a lower price by waiting to reiterate 
the proxy does not acquire more than one option for any 
good rather it reduces the exercise price on its already 
issued option if a better deal is found 
the proxy is able to discover these deals by asking each 
future auction to report the identities of the bidders in that 
auction together with their bids this needs to be enforced 
by ebay as the central authority the highest bidder in this 
later auction across those whose identity is not stored in 
the proxy s memory for the given item is exactly the bidder 
against whom the proxy would be competing had it delayed 
its entry until this auction if this high bid is lower than the 
current option price held the proxy price matches down 
to this high bid price 
after price matching one of two adjustments will be made 
by the proxy for bookkeeping purposes if the winner of 
the auction is the bidder whose identity has been in the 
proxy s local memory the proxy will replace that local 
information with the identity of the bidder whose bid it just 
price matched as that is now the bidder the proxy has 
prevented from obtaining an option if the auction winner s 
identity is not stored in the proxy s local memory the 
memory may be cleared in this case the proxy will simply price 
match against the bids of future auction winners on this 
item until the proxy departs 
example table molly s proxy wins the 
monday auction submitting a bid of and receiving an option 
for molly s proxy adds nancy to its local memory as 
nancy s proxy would have won had molly s proxy not bid 
on tuesday only nancy s and polly s proxy bid as molly s 
proxy holds an option with nancy s proxy winning an 
opbuyer type monday tuesday 
truth 
molly mon mon 
 nancynancy mon tues - polly 
polly mon tues 
-misreport 
molly mon mon 
-nancy mon tues molly molly → φ 
polly mon tues - φ 
misreport 
match low 
molly mon mon 
-nancy mon tues → 
polly mon tues - 
table examples demonstrating why bookkeeping will 
lead to a truthful system whereas simply matching to the 
lowest winning price will not 
tion for and noting that it bumped polly s proxy at this 
time molly s proxy will price match its option down to 
and replace nancy with polly in its local memory as per the 
price match algorithm as polly would be holding an option 
had molly never bid 
 exercising options 
at the reported departure time the proxy chooses which 
options to exercise therefore a seller of an option must 
wait until period ˆdw for the option to be exercised and 
receive payment where w was the winner of the option 
for 
bidder i in period ˆdi the proxy chooses the option s that 
maximize the reported utility of the buyer 
θ∗ 
t argmax 
θ⊆θ 
 ˆvi γ θ − π θ 
where θ is the set of all options held γ θ are the goods 
corresponding to a set of options and π θ is the sum of 
exercise prices for a set of options all other options are 
returned 
no options are exercised when no combination 
of options have positive utility 
 why bookkeep and not match winning price 
one may believe that an alternative method for 
implementing a price matching scheme could be to simply have 
proxies match the lowest winning price they observe after 
winning an option however as demonstrated in table 
such a simple price matching scheme will not lead to a 
truthful system 
the first scenario in table demonstrates the outcome 
if all agents were to truthfully report their types molly 
 
while this appears restrictive on the seller we believe it not 
significantly different than what sellers on ebay currently 
endure in practice an auction on ebay closes at a specific 
time but a seller must wait until a buyer relinquishes 
payment before being able to realize the revenue an amount of 
time that could easily be days if payment is via a money 
order sent through courier to much longer if a buyer is 
slow but not overtly delinquent in remitting her payment 
 
presumably an option returned will result in the seller 
holding a new auction for an option on the item it still 
possesses however the system will not allow a seller to 
re-auction an option until δmax after the option had first 
been issued in order to maintain a truthful mechanism 
 
would win the monday auction and receive an option with 
an exercise price of subsequently exercising that option 
at the end of monday and nancy would win the tuesday 
auction and receive an option with an exercise price of 
 subsequently exercising that option at the end of tuesday 
the second scenario in table demonstrates the outcome 
if nancy were to misreport her value for the good by 
reporting an inflated value of using the proposed bookkeeping 
method nancy would win the monday auction and receive 
an option with an exercise price of on tuesday polly 
would win the auction and receive an option with an exercise 
price of nancy s proxy would observe that the highest 
bid submitted on tuesday among those proxies not stored 
in local memory is polly s bid of and so nancy s proxy 
would price match the exercise price of its option down to 
 note that the exercise price nancy s proxy has obtained 
at the end of tuesday is the same as when she truthfully 
revealed her type to her proxy 
the third scenario in table demonstrates the outcome if 
nancy were to misreport her value for the good by reporting 
an inflated value of if the price matching scheme were 
for proxies to simply match their option price to the 
lowest winning price at any time while they are in the system 
nancy would win the monday auction and receive an option 
with an exercise price of on tuesday polly would win 
the auction and receive an option with an exercise price of 
 nancy s proxy would observe that the lowest price on 
tuesday was and so nancy s proxy would price match 
the exercise price of its option down to note that the 
exercise price nancy s proxy has obtained at the end of 
tuesday is lower than when she truthfully revealed her type to 
the proxy 
therefore a price matching policy of simply matching the 
lowest price paid may not elicit truthful information from 
buyers 
 complexity of algorithm 
an xor-valuation of size m for buyer i is a set of m 
terms l 
 v 
i lm 
 vm 
i that maps distinct 
bundles to values where i is interested in acquiring at most one 
such bundle for any bundle s vi s maxlm⊆s vm 
i 
theorem given an xor-valuation which possesses 
m terms there is an o km 
 algorithm for computing all 
maximum marginal values where k is the number of 
different item types in which a buyer may be interested 
proof for each item type recall equation which 
defines the maximum marginal value of an item for each 
bundle l in the m-term valuation vi l k may be found 
by iterating over the m terms therefore the number of 
terms explored to determine the maximum marginal value 
for any item is o m 
 and so the total number of 
bundle comparisons to be performed to calculate all maximum 
marginal values is o km 
 
theorem the total memory required by a proxy for 
implementing price matching is o k where k is the 
number of distinct item types the total work performed by a 
proxy to conduct price matching in each auction is o 
proof by construction of the algorithm the proxy stores 
one maximum marginal value for each item for bidding of 
which there are o k at most one buyer s identity for each 
item of which there are o k and one current option 
exercise price for each item of which there are o k for 
each auction the proxy either submits a precomputed bid 
or price matches both of which take o work 
 truthful bidding to the proxy agent 
proxies transform the market into a direct revelation 
mechanism where each buyer i interacts with the proxy only 
once 
and does so by declaring a bid bi which is 
defined as an announcement of her type ˆai ˆdi ˆvi where 
the announcement may or may not be truthful we 
denote all received bids other than i s as b−i given bids 
b bi b−i the market determines allocations xi b and 
payments pi b ≥ to each buyer using an online 
algorithm 
a dominant strategy equilibrium for buyers requires that 
vi xi bi b−i −pi bi b−i ≥ vi xi bi b−i −pi bi b−i ∀bi 
bi ∀b−i 
we now establish that it is a dominant strategy for a buyer 
to reveal her true valuation and true departure time to her 
proxy agent immediately upon arrival to the system the 
proof builds on the price-based characterization of 
strategyproof single-item online auctions in hajiaghayi et al 
define a monotonic and value-independent price function 
psi ai di l v−i which can depend on the values of other 
agents v−i price psi ai di l v−i will represent the price 
available to agent i for bundle l in the mechanism if it 
announces arrival time ai and departure time di the price 
is independent of the value vi of agent i but can depend on 
ai di and l as long as it satisfies a monotonicity condition 
definition price function psi ai di l v−i is 
monotonic if psi ai di l v−i ≤ psi ai di l v−i for all ai ≤ 
ai all di ≥ di all bundles l ⊆ l and all v−i 
lemma an online combinatorial auction will be 
strategyproof with truthful reports of arrival departure and value 
a dominant strategy when there exists a monotonic and 
value-independent price function psi ai di l v−i such that 
for all i and all ai di ∈ t and all vi agent i is allocated 
bundle l∗ 
 argmaxl vi l − psi ai di l v−i in period di 
and makes payment psi ai di l∗ 
 v−i 
proof agent i cannot benefit from reporting a later 
departure ˆdi because the allocation is made in period ˆdi and 
the agent would have no value for this allocation agent 
i cannot benefit from reporting a later arrival ˆai ≥ ai or 
earlier departure ˆdi ≤ di because of price monotonicity 
finally the agent cannot benefit from reporting some ˆvi vi 
because its reported valuation does not change the prices 
it faces and the mechanism maximizes its utility given its 
reported valuation and given the prices 
lemma at any given time there is at most one buyer 
in the system whose proxy does not hold an option for a 
given item type because of buyer i s presence in the system 
and the identity of that buyer will be stored in i s proxy s 
local memory at that time if such a buyer exists 
proof by induction consider the first proxy that a 
buyer prevents from winning an option either a the 
 
for analysis purposes we view the mechanism as an opaque 
market so that the buyer cannot condition her bid on bids 
placed by others 
 
bumped proxy will leave the system having never won an 
option or b the bumped proxy will win an auction in the 
future if a the buyer s presence prevented exactly that 
one buyer from winning an option but will have not 
prevented any other proxies from winning an option as the 
buyer s proxy will not bid on additional options upon 
securing one and will have had that bumped proxy s identity 
in its local memory by definition of the algorithm if b 
the buyer has not prevented the bumped proxy from 
winning an option after all but rather has prevented only the 
proxy that lost to the bumped proxy from winning if any 
whose identity will now be stored in the proxy s local 
memory by definition of the algorithm for this new identity in 
the buyer s proxy s local memory either scenario a or b 
will be true ad infinitum 
given this we show that the options-based 
infrastructure implements a price-based auction with a monotonic and 
value-independent price schedule to every agent 
theorem truthful revelation of valuation arrival and 
departure is a dominant strategy for a buyer in the 
optionsbased market 
proof first define a simple agent-independent price 
function pk 
i t v−i as the highest bid by the proxies not 
holding an option on an item of type gk at time t not 
including the proxy representing i herself and not 
including any proxies that would have already won an option had 
i never entered the system i e whose identity is stored 
in i s proxy s local memory ∞ if no supply at t this 
set of proxies is independent of any declaration i makes to 
its proxy as the set explicitly excludes the at most one 
proxy see lemma that i has prevented from holding 
an option and each bid submitted by a proxy within this 
set is only a function of their own buyer s declared 
valuation see equation furthermore i cannot influence 
the supply she faces as any options returned by bidders 
due to a price set by i s proxy s bid will be re-auctioned 
after i has departed the system therefore pk 
i t v−i is 
independent of i s declaration to its proxy next define 
psk 
i ˆai ˆdi v−i minˆai≤τ≤ ˆdi 
 pk 
i τ v−i possibly ∞ as the 
minimum price over pk 
i t v−i which is clearly monotonic 
by construction of price matching this is exactly the price 
obtained by a proxy on any option that it holds at 
departure define psi ˆai ˆdi l v−i 
èk k 
k psk 
i ˆai ˆdi v−i lk 
which is monotonic in ˆai ˆdi and l since psk 
i ˆai ˆdi v−i is 
monotonic in ˆai and ˆdi and weakly greater than zero for 
each k given the set of options held at ˆdi which may be 
a subset of those items with non-infinite prices the proxy 
exercises options to maximize the reported utility left to 
show is that all bundles that could not be obtained with 
options held are priced sufficiently high as to not be 
preferred for each such bundle either there is an item priced 
at ∞ in which case the bundle would not be desired or 
there must be an item in that bundle for which the proxy 
does not hold an option that was available in all auctions 
for such an item there must have been a distinct bidder 
with a bid greater than bidt 
i k which subsequently results 
in psk 
i ˆai ˆdi v−i bidt 
i k and so the bundle without k 
would be preferred to the bundle 
theorem the super proxy options-based scheme is 
individually-rational for both buyers and sellers 
price σ price value surplus 
ebay 
options 
table average price paid standard deviation of 
prices paid average bidder value among winners and 
average winning bidder surplus on ebay for dell e fp 
lcd screens as well as the simulated options-based 
market using worst-case estimates of bidders true value 
proof by construction the proxy exercises the profit 
maximizing set of options obtained or no options if no set 
of options derives non-negative surplus therefore buyers 
are guaranteed non-negative surplus by participating in the 
scheme for sellers the price of each option is based on a 
non-negative bid or zero 
 evaluating the options proxy 
infrastructure 
a goal of the empirical benchmarking and a reason to 
collect data from ebay is to try and build a realistic model 
of buyers from which to estimate seller revenue and other 
market effects under the options-based scheme 
we simulate a sequence of auctions that match the timing 
of the dell lcd auctions on ebay 
when an auction 
successfully closes on ebay we simulate a vickrey auction for 
an option on the item sold in that period auctions that do 
not successfully close on ebay are not simulated we 
estimate the arrival departure and value of each bidder on ebay 
from their observed behavior 
arrival is estimated as the 
first time that a bidder interacts with the ebay proxy while 
departure is estimated as the latest closing time among ebay 
auctions in which a bidder participates 
we initially adopt a particularly conservative estimate for 
bidder value estimating it as the highest bid a bidder was 
observed to make on ebay table compares the 
distribution of closing prices on ebay and in the simulated options 
scheme while the average revenue in both schemes is 
virtually the same in the options scheme vs 
on ebay the winners in the options scheme tend to value 
the item won more than the winners on ebay in 
the options scheme vs on ebay 
 bid identification 
we extend the work of haile and tamer to sequential 
auctions to get a better view of underlying bidder values 
rather than assume for bidders an equilibrium behavior as 
in standard econometric techniques haile and tamer do not 
attempt to model how bidders true values get mapped into a 
bid in any given auction rather in the context of repeated 
 
when running the simulations the results of the first and 
final ten days of auctions are not recorded to reduce edge 
effects that come from viewing a discrete time window of a 
continuous process 
 
for the bidders that won multiple times on ebay we 
have each one bid a constant marginal value for each 
additional item in each auction until the number of options 
held equals the total number of lcds won on ebay with 
each option available for price matching independently this 
bidding strategy is not a dominant strategy falling outside 
the type space possible for buyers on which the proof of 
truthfulness has been built but is believed to be the most 
appropriate first order action for simulation 
 
 
 
 
 
 
 
 
value 
cdf observed max bids upper bound of true value 
figure cdf of maximum bids observed and upper 
bound estimate of the bidding population s distribution 
for maximum willingness to pay the true population 
distribution lies below the estimated upper bound 
single-item auctions with distinct bidder populations haile 
and tamer make only the following two assumptions when 
estimating the distribution of true bidder values 
 bidders do not bid more than they are willing to pay 
 bidders do not allow an opponent to win at a price 
they are willing to beat 
from the first of their two assumptions given the bids placed 
by each bidder in each auction haile and tamer derive a 
method for estimating an upper bound of the bidding 
population s true value distribution i e the bound that lies 
above the true value distribution from the second of their 
two assumptions given the winning price of each auction 
haile and tamer derive a method for estimating a lower 
bound of the bidding population s true value distribution 
it is only the upper-bound of the distribution that we 
utilize in our work 
haile and tamer assume that bidders only participate in 
a single auction and require independence of the bidding 
population from auction to auction neither assumption is 
valid here the former because bidders are known to bid 
in more than one auction and the latter because the set 
of bidders in an auction is in all likelihood not a true i i d 
sampling of the overall bidding population in particular 
those who win auctions are less likely to bid in successive 
auctions while those who lose auctions are more likely to 
remain bidders in future auctions 
in applying their methods we make the following 
adjustments 
 within a given auction each individual bidder s true 
willingness to pay is assumed weakly greater than the 
maximum bid that bidder submits across all auctions 
for that item either past or future 
 when estimating the upper bound of the value 
distribution if a bidder bids in more than one auction 
randomly select one of the auctions in which the 
bidder bid and only utilize that one observation during 
the estimation 
 
in current work we assume that removing duplicate 
bidders is sufficient to make the buying populations 
independent i i d draws from auction to auction if one believes 
that certain portions of the population are drawn to 
cerprice σ price value surplus 
ebay 
options 
table average price paid standard deviation of 
prices paid average bidder value among winners and 
average winning bidder surplus on ebay for dell e fp 
lcd screens as well as in the simulated options-based 
market using an adjusted haile and tamer estimate of 
bidders true values being higher than their 
maximum observed bid 
figure provides the distribution of maximum bids placed 
by bidders on ebay as well as the estimated upper bound of 
the true value distribution of bidders based on the extended 
haile and tamer method 
as can be seen the smallest 
relative gap between the two curves meaningfully occurs near 
the th percentile where the upper bound is times 
the maximum bid therefore adopted as a less 
conservative model of bidder values is a uniform scaling factor of 
 
we now present results from this less conservative 
analysis table shows the distribution of closing prices in 
auctions on ebay and in the simulated options scheme the 
mean price in the options scheme is now significantly higher 
 greater than the prices on ebay in the options 
scheme vs on ebay while the standard deviation 
of closing prices is lower among the options scheme auctions 
 in the options scheme vs on ebay therefore not 
only is the expected revenue stream higher but the lower 
variance provides sellers a greater likelihood of realizing that 
higher revenue 
the efficiency of the options scheme remains higher than 
on ebay the winners in the options scheme now have an 
average estimated value higher at 
in an effort to better understand this efficiency we 
formulated a mixed integer program mip to determine a 
simple estimate of the allocative efficiency of ebay the 
mip computes the efficient value of the oﬄine problem with 
full hindsight on all bids and all supply 
using a scaling 
of the total value allocated to ebay winners is 
estimated at while the optimal value from the mip 
is this suggests an allocative efficiency of 
while the typical value of a winner on ebay is an 
average value of was possible 
note the options-based 
tain auctions then further adjustments would be required 
in order to utilize these techniques 
 
the estimation of the points in the curve is a 
minimization over many variables many of which can have 
smallnumbers bias consequently haile and tamer suggest using 
a weighted average over all terms yi of 
è 
i yi 
exp yiρ èj exp yj ρ 
to 
approximate the minimum while reducing the small 
number effects we used ρ − and removed observations 
of auctions with bidders or more as they occurred very 
infrequently however some small numbers bias still 
demonstrated itself with the plateau in our upper bound estimate 
around a value of 
 
buyers who won more than one item on ebay are cloned 
so that they appear to be multiple bidders of identical type 
 
as long as one believes that every bidder s true value is a 
constant factor α away from their observed maximum bid 
the efficiency calculation holds for any value of α in 
practice this belief may not be reasonable for example if 
losing bidders tend to have true values close to their observed 
 
scheme comes very close to achieving this level of efficiency 
 at efficient in this estimate even though it operates 
without the benefit of hindsight 
finally although the typical winning bidder surplus 
decreases between ebay and the options-based scheme some 
surplus redistribution would be possible because the total 
market efficiency is improved 
 discussion 
the biggest concern with our scheme is that proxy agents 
who may be interested in many different items may acquire 
many more options than they finally exercise this can lead 
to efficiency loss notice that this is not an issue when 
bidders are only interested in a single item as in our empirical 
study or have linear-additive values on items 
to fix this we would prefer to have proxy agents use more 
caution in acquiring options and use a more adaptive bidding 
strategy than that in equation for instance if a proxy 
is already holding an option with an exercise price of on 
some item for which it has value of and it values some 
substitute item at the proxy could reason that in no 
circumstance will it be useful to acquire an option on the 
second item 
we formulate a more sophisticated bidding strategy along 
these lines let θt be the set of all options a proxy for bidder 
i already possesses at time t let θt ⊆ θt be a subset of 
those options the sum of whose exercise prices are π θt 
and the goods corresponding to those options being γ θt 
let π θt ˆvi γ θt − π θt be the reported available 
surplus associated with a set of options let θ∗ 
t be the set 
of options currently held that would maximize the buyer s 
surplus i e θ∗ 
t argmaxθt⊆θt 
π θt 
let the maximal willingness to pay for an item k represent 
a price above which the agent knows it would never exercise 
an option on the item given the current options held this 
can be computed as follows 
bidt 
i k max 
l 
 min ˆvi l k − π θ∗ 
t ˆvi l k − ˆvi l 
 
where ˆvi l k −π θ∗ 
t considers surplus already held ˆvi l 
k −ˆvi l considers the marginal value of a good and taking 
the max considers the overall use of pursuing the good 
however and somewhat counter intuitively we are not 
able to implement this bidding scheme without forfeiting 
truthfulness the π θ∗ 
t term in equation i e the amount 
of guaranteed surplus bidder i has already obtained can be 
influenced by proxy j s bid therefore bidder j may have 
the incentive to misrepresent her valuation to her proxy if 
she believes doing so will cause i to bid differently in the 
future in a manner beneficial to j consider the following 
example where the proxy scheme is refined to bid the 
maximum willingness to pay 
example alice values either one ton of sand or one 
ton of stone for bob values either one ton of sand 
or one ton of stone for all bidders have a patience 
maximum bids while ebay winners have true values much 
greater than their observed maximum bids then downward 
bias is introduced in the efficiency calculation at present 
 
the increase in ebay winner surplus between tables and 
 is to be expected as the α scaling strictly increases the 
estimated value of the ebay winners while holding the prices 
at which they won constant 
of days on day one a sand auction is held where alice s 
proxy bids and bob s bids alice s proxy wins 
an option to purchase sand for on day two a stone 
auction is held where alice s proxy bids as she has 
already obtained a guaranteed of surplus from winning 
a sand option and so reduces her stone bid by this amount 
and bob s bids either alice s proxy or bob s proxy 
will win the stone option at the end of the second day 
alice s proxy holds an option with an exercise price of 
to obtain a good valued for and so obtains in 
surplus 
now consider what would have happened had alice 
declared that she valued only stone 
example alice declares valuing only stone for 
bob values either one ton of sand or one ton of stone for 
 all bidders have a patience of days on day one a 
sand auction is held where bob s proxy bids bob s 
proxy wins an option to purchase sand for on day two 
a stone auction is held where alice s proxy bids and 
bob s bids as he has already obtained a guaranteed 
of surplus from winning a sand option and so reduces his 
stone bid by this amount alice s proxy wins the stone 
option for at the end of the second day alice s proxy 
holds an option with an exercise price of to obtain a good 
valued for and so obtains in surplus 
by misrepresenting her valuation i e excluding her value 
of sand alice was able to secure higher surplus by guiding 
bob s bid for stone to an area of immediate further 
work by the authors is to develop a more sophisticated proxy 
agent that can allow for bidding of maximum willingness to 
pay equation while maintaining truthfulness 
an additional practical concern with our proxy scheme 
is that we assume an available trusted and well understood 
method to characterize goods and presumably the quality 
of goods we envision this happening in practice by 
sellers defining a classification for their item upon entering the 
market for instance via a upc code just as in ebay this 
would allow an opportunity for sellers to improve revenue by 
overstating the quality of their item new vs like new 
and raises the issue of how well a reputation scheme could 
address this 
 conclusions 
we introduced a new sales channel consisting of an 
optionsbased and proxied auction protocol to address the 
sequential auction problem that exists when bidders face 
multiple auctions for substitutes and complements goods our 
scheme provides bidders with a simple dominant and 
truthful bidding strategy even though the market remains open 
and dynamic 
in addition to exploring more sophisticated proxies that 
bid in terms of maximum willingness to pay future work 
should aim to better model seller incentives and resolve the 
strategic problems facing sellers for instance does the 
options scheme change seller incentives from what they 
currently are on ebay 
acknowledgments 
we would like to thank pai-ling yin helpful comments 
have been received from william simpson attendees at 
har 
vard university s econcs and itm seminars and 
anonymous reviewers thank you to aaron l roth and 
kangxing jin for technical support all errors and omissions 
remain our own 
 references 
 p anthony and n r jennings developing a bidding 
agent for multiple heterogeneous auctions acm 
trans on internet technology 
 r bapna p goes a gupta and y jin user 
heterogeneity and its impact on electronic auction 
market design an empirical exploration mis 
quarterly - 
 d bertsimas j hawkins and g perakis optimal 
bidding in on-line auctions working paper 
 c boutilier m goldszmidt and b sabata 
sequential auctions for the allocation of resources with 
complementarities in proc th international joint 
conference on artificial intelligence ijcai- 
pages - 
 a byde c preist and n r jennings decision 
procedures for multiple auctions in proc st int 
joint conf on autonomous agents and multiagent 
systems aamas- 
 m m bykowsky r j cull and j o ledyard 
mutually destructive bidding the fcc auction 
design problem journal of regulatory economics 
 - 
 y chen c narasimhan and z j zhang consumer 
heterogeneity and competitive price-matching 
guarantees marketing science - 
 a k dixit and r s pindyck investment under 
uncertainty princeton university press 
 r gopal s thompson y a tung and a b 
whinston managing risks in multiple online auctions 
an options approach decision sciences 
 - 
 a greenwald and j o kephart shopbots and 
pricebots in proc th international joint 
conference on artificial intelligence ijcai- 
pages - 
 p a haile and e tamer inference with an 
incomplete model of english auctions journal of 
political economy 
 m t hajiaghayi r kleinberg m mahdian and 
d c parkes online auctions with re-usable goods in 
proc acm conf on electronic commerce 
 k hendricks i onur and t wiseman preemption 
and delay in ebay auctions university of texas at 
austin working paper 
 a iwasaki m yokoo and k terada a robust open 
ascending-price multi-unit auction protocol against 
false-name bids decision support systems - 
 
 e g james d hess price-matching policies an 
empirical case managerial and decision economics 
 - 
 a x jiang and k leyton-brown estimating 
bidders valuation distributions in online auctions in 
workshop on game theory and decision theory 
 gtdt at ijcai 
 r lavi and n nisan competitive analysis of 
incentive compatible on-line auctions in proc nd 
acm conf on electronic commerce ec- 
 y j lin price matching in a model of equilibrium 
price dispersion southern economic journal 
 - 
 d lucking-reiley and d f spulber 
business-to-business electronic commerce journal of 
economic perspectives - 
 a ockenfels and a roth last-minute bidding and 
the rules for ending second-price auctions evidence 
from ebay and amazon auctions on the internet 
american economic review - 
 m peters and s severinov internet auctions with 
many traders journal of economic theory 
 forthcoming 
 r porter mechanism design for online real-time 
scheduling in proceedings of the th acm conference 
on electronic commerce pages - acm press 
 
 m h rothkopf and r engelbrecht-wiggans 
innovative approaches to competitive mineral leasing 
resources and energy - 
 t sandholm and v lesser leveled commitment 
contracts and strategic breach games and economic 
behavior - 
 t w sandholm and v r lesser issues in 
automated negotiation and electronic commerce 
extending the contract net framework in proc st 
international conference on multi-agent systems 
 icmas- pages - 
 h s shah n r joshi a sureka and p r 
wurman mining for bidding strategies on ebay 
lecture notes on artificial intelligence 
 m stryszowska late and multiple bidding in 
competing second price internet auctions 
euroconference on auctions and market design 
theory evidence and applications 
 j t -y wang is last minute bidding bad ucla 
working paper 
 r zeithammer an equilibrium model of a dynamic 
auction marketplace working paper university of 
chicago 
 
weak monotonicity suffices for truthfulness 
on convex domains 
michael saks 
∗ 
dept of mathematics 
rutgers university 
 frelinghuysen road 
piscataway nj 
saks math rutgers edu 
lan yu 
† 
dept of computer science 
rutgers university 
 frelinghuysen road 
piscataway nj 
lanyu paul rutgers edu 
abstract 
weak monotonicity is a simple necessary condition for a 
social choice function to be implementable by a truthful 
mechanism roberts showed that it is sufficient for all 
social choice functions whose domain is unrestricted lavi 
mu alem and nisan proved the sufficiency of weak 
monotonicity for functions over order-based domains and gui 
muller and vohra proved sufficiency for order-based 
domains with range constraints and for domains defined by 
other special types of linear inequality constraints here we 
show the more general result conjectured by lavi mu alem 
and nisan that weak monotonicity is sufficient for 
functions defined on any convex domain 
categories and subject descriptors 
j social and behavioral sciences economics k 
 computers and society electronic 
commerce-payment schemes 
general terms 
theory economics 
 introduction 
social choice theory centers around the general problem of 
selecting a single outcome out of a set a of alternative 
outcomes based on the individual preferences of a set p of 
players a method for aggregating player preferences to select 
one outcome is called a social choice function in this paper 
we assume that the range a is finite and that each player s 
preference is expressed by a valuation function which 
assigns to each possible outcome a real number representing 
the benefit the player derives from that outcome the 
ensemble of player valuation functions is viewed as a 
valuation matrix with rows indexed by players and columns by 
outcomes 
a major difficulty connected with social choice functions 
is that players can not be required to tell the truth about 
their preferences since each player seeks to maximize his 
own benefit he may find it in his interest to misrepresent 
his valuation function an important approach for dealing 
with this problem is to augment a given social choice 
function with a payment function which assigns to each player 
a positive or negative payment as a function of all of the 
individual preferences by carefully choosing the payment 
function one can hope to entice each player to tell the truth 
a social choice function augmented with a payment function 
is called a mechanism 
and the mechanism is said to 
implement the social choice function a mechanism is truthful 
 or to be strategyproof or to have a dominant strategy if 
each player s best strategy knowing the preferences of the 
others is always to declare his own true preferences a 
social choice function is truthfully implementable or truthful 
if it has a truthful implementation the property of 
truthful implementability is sometimes called dominant strategy 
incentive compatibility this framework leads naturally to 
the question which social choice functions are truthful 
this question is of the following general type given a 
class of functions here social choice functions and a 
property that holds for some of them here truthfulness 
characterize the property the definition of the property itself 
provides a characterization so what more is needed here 
are some useful notions of characterization 
 recognition algorithm give an algorithm which given 
an appropriate representation of a function in the class 
determines whether the function has the property 
 parametric representation give an explicit parametrized 
family of functions and show that each function in the 
 
the usual definition of mechanism is more general than this 
 see chapter c or the mechanisms we consider 
here are usually called direct revelation mechanisms 
 
family has the property and that every function with 
the property is in the family 
a third notion applies in the case of hereditary properties 
of functions a function g is a subfunction of function f or 
f contains g if g is obtained by restricting the domain of 
f a property p of functions is hereditary if it is preserved 
under taking subfunctions truthfulness is easily seen to be 
hereditary 
 sets of obstructions for a hereditary property p a 
function g that does not have the property is an 
obstruction to the property in the sense that any function 
containing g doesn t have the property an obstruction 
is minimal if every proper subfunction has the 
property a set of obstructions is complete if every function 
that does not have the property contains one of them 
as a subfunction the set of all functions that don t 
satisfy p is a complete but trivial and uninteresting 
set of obstructions one seeks a set of small ideally 
minimal obstructions 
we are not aware of any work on recognition algorithms 
for the property of truthfulness but there are significant 
results concerning parametric representations and obstruction 
characterizations of truthfulness it turns out that the 
domain of the function i e the set of allowed valuation 
matrices is crucial for functions with unrestricted domain i e 
whose domain is the set of all real matrices there are very 
good characterizations of truthfulness for general domains 
however the picture is far from complete typically the 
domains of social choice functions are specified by a system of 
constraints for example an order constraint requires that 
one specified entry in some row be larger than another in 
the same row a range constraint places an upper or lower 
bound on an entry and a zero constraint forces an entry to 
be these are all examples of linear inequality constraints 
on the matrix entries 
building on work of roberts lavi mu alem and 
nisan defined a condition called weak monotonicity 
 wmon independently in the context of multi-unit 
auctions bikhchandani chatterji and sen identified the 
same condition and called it nondecreasing in marginal 
utilities ndmu the definition of w-mon can be formulated 
in terms of obstructions for some specified simple set f of 
functions each having domains of size a function satisfies 
w-mon if it contains no function from f the functions 
in f are not truthful and therefore w-mon is a 
necessary condition for truthfulness lavi mu alem and nisan 
 showed that w-mon is also sufficient for truthfulness 
for social choice functions whose domain is order-based i e 
defined by order constraints and zero constraints and gui 
muller and vohra extended this to other domains the 
domain constraints considered in both papers are special 
cases of linear inequality constraints and it is natural to 
ask whether w-mon is sufficient for any domain defined by 
such constraints lavi mu alem and nisan conjectured 
that w-mon suffices for convex domains the main result 
of this paper is an affirmative answer to this conjecture 
theorem for any social choice function having 
convex domain and finite range weak monotonicity is necessary 
and sufficient for truthfulness 
using the interpretation of weak monotonicity in terms 
of obstructions each having domain size this provides a 
complete set of minimal obstructions for truthfulness within 
the class of social choice functions with convex domains 
the two hypotheses on the social choice function that 
the domain is convex and that the range is finite can not 
be omitted as is shown by the examples given in section 
 related work 
there is a simple and natural parametrized set of 
truthful social choice functions called affine maximizers roberts 
 showed that for functions with unrestricted domain 
every truthful function is an affine maximizer thus providing 
a parametrized representation for truthful functions with 
unrestricted domain there are many known examples of 
truthful functions over restricted domains that are not affine 
maximizers see and each of these 
examples has a special structure and it seems plausible that 
there might be some mild restrictions on the class of all 
social choice functions such that all truthful functions obeying 
these restrictions are affine maximizers lavi mu alem and 
nisan obtained a result in this direction by showing that 
for order-based domains under certain technical 
assumptions every truthful social choice function is almost an 
affine maximizer 
there are a number of results about truthfulness that 
can be viewed as providing obstruction characterizations 
although the notion of obstruction is not explicitly discussed 
for a player i a set of valuation matrices is said to be 
i-local if all of the matrices in the set are identical except for 
row i call a social choice function i-local if its domain is 
ilocal and call it local if it is i-local for some i the following 
easily proved fact is used extensively in the literature 
proposition the social choice function f is truthful 
if and only if every local subfunction of f is truthful 
this implies that the set of all local non-truthful functions 
comprises a complete set of obstructions for truthfulness 
this set is much smaller than the set of all non-truthful 
functions but is still far from a minimal set of obstructions 
rochet rozenshtrom and gui muller and vohra 
 identified a necessary and sufficient condition for 
truthfulness see lemma below called the nonnegative cycle 
property this condition can be viewed as providing a 
minimal complete set of non-truthful functions as is required 
by proposition each function in the set is local 
furthermore it is one-to-one in particular its domain has size at 
most the number of possible outcomes a 
as this complete set of obstructions consists of minimal 
non-truthful functions this provides the optimal obstruction 
characterization of non-truthful functions within the class of 
all social choice functions but by restricting attention to 
interesting subclasses of social choice functions one may hope 
to get simpler sets of obstructions for truthfulness within 
that class 
the condition of weak monotonicity mentioned earlier can 
be defined by a set of obstructions each of which is a local 
function of domain size exactly thus the results of lavi 
mu alem and nisan and of gui muller and vohra 
give a very simple set of obstructions for truthfulness within 
certain subclasses of social choice functions theorem 
extends these results to a much larger subclass of functions 
 
 weak monotonicity and the nonnegative 
cycle property 
by proposition a function is truthful if and only if each 
of its local subfunctions is truthful therefore to get a set 
of obstructions for truthfulness it suffices to obtain such a 
set for local functions 
the domain of an i-local function consists of matrices that 
are fixed on all rows but row i fix such a function f and 
let d ⊆ ra 
be the set of allowed choices for row i since 
f depends only on row i and row i is chosen from d we 
can view f as a function from d to a therefore f is a 
social choice function having one player we refer to such a 
function as a single player function 
associated to any single player function f with domain d 
we define an edge-weighted directed graph hf whose vertex 
set is the image of f for convenience we assume that f 
is surjective and so this image is a for each a b ∈ a 
x ∈ f− 
 a there is an edge ex a b from a to b with weight 
x a − x b the weight of a set of edges is just the sum of 
the weights of the edges we say that f satisfies 
 the nonnegative cycle property if every directed cycle 
has nonnegative weight 
 the nonnegative two-cycle property if every directed 
cycle between two vertices has nonnegative weight 
we say a local function g satisfies nonnegative cycle 
property nonnegative two-cycle property if its associated single 
player function f does 
the graph hf has a possibly infinite number of edges 
between any two vertices we define gf to be the 
edgeweighted directed graph with exactly one edge from a to b 
whose weight δab is the infimum possibly −∞ of all of the 
edge weights ex a b for x ∈ f− 
 a it is easy to see that hf 
has the nonnegative cycle property nonnegative two-cycle 
property if and only if gf does gf is called the outcome 
graph of f 
the weak monotonicity property mentioned earlier can 
be defined for arbitrary social choice functions by the 
condition that every local subfunction satisfies the nonnegative 
two-cycle property the following result was obtained by 
rochet in a slightly different form and rediscovered by 
rozenshtrom and gui muller and vohra 
lemma a local social choice function is truthful if and 
only if it has the nonnegative cycle property thus a social 
choice function is truthful if and only if every local 
subfunction satisfies the nonnegative cycle property 
in light of this theorem follows from 
theorem for any surjective single player function f 
d −→ a where d is a convex subset of ra 
and a is finite 
the nonnegative two-cycle property implies the nonnegative 
cycle property 
this is the result we will prove 
 overview of the proof of theorem 
let d ⊆ ra 
be convex and let f d −→ a be a single 
player function such that gf has no negative two-cycles we 
want to conclude that gf has no negative cycles for two 
vertices a b let δ∗ 
ab denote the minimum weight of any path 
from a to b clearly δ∗ 
ab ≤ δab our proof shows that the 
δ∗ 
-weight of every cycle is exactly from which theorem 
follows 
there seems to be no direct way to compute δ∗ 
and so we 
proceed indirectly based on geometric considerations we 
identify a subset of paths in gf called admissible paths and 
a subset of admissible paths called straight paths we prove 
that for any two outcomes a b there is a straight path from 
a to b lemma and corollary and all straight paths 
from a to b have the same weight which we denote ρab 
 theorem we show that ρab ≤ δab lemma and that 
the ρ-weight of every cycle is the key step to this proof 
is showing that the ρ-weight of every directed triangle is 
 lemma 
it turns out that ρ is equal to δ∗ 
 corollary although 
this equality is not needed in the proof of theorem 
to expand on the above summary we give the definitions 
of an admissible path and a straight path these are 
somewhat technical and rely on the geometry of f we first 
observe that without loss of generality we can assume that 
d is topologically closed section in section for each 
a ∈ a we enlarge the set f− 
 a to a closed convex set 
da ⊆ d in such a way that for a b ∈ a with a b da and 
db have disjoint interiors we define an admissible path to 
be a sequence of outcomes a ak such that each of the 
sets ij daj ∩ daj is nonempty section an 
admissible path is straight if there is a straight line that meets one 
point from each of the sets i ik− in order section 
finally we mention how the hypotheses of convex domain 
and finite range are used in the proof both hypotheses are 
needed to show the existence of a straight path from a 
to b for all a b lemma that the ρ-weight of a directed 
triangle is lemma the convex domain hypothesis is 
also needed for the convexity of the sets da section the 
finite range hypothesis is also needed to reduce theorem to 
the case that d is closed section and to prove that every 
straight path from a to b has the same δ-weight theorem 
 
 reduction to closed domain 
we first reduce the theorem to the case that d is closed 
write dc 
for the closure of d since a is finite dc 
 
∪a∈a f− 
 a c 
 thus for each v ∈ dc 
− d there is an 
a a v ∈ a such that v ∈ f− 
 a c 
 extend f to the 
function g on dc 
by defining g v a v for v ∈ dc 
− 
d and g v f v for v ∈ d it is easy to check that 
δab g δab f for all a b ∈ a and therefore it suffices to 
show that the nonnegative two-cycle property for g implies 
the nonnegative cycle property for g 
henceforth we assume d is convex and closed 
 a dissection of the domain 
in this section we construct a family of closed convex sets 
{da a ∈ a} with disjoint interiors whose union is d and 
satisfying f− 
 a ⊆ da for each a ∈ a 
let ra {v ∀b ∈ a v a − v b ≥ δab} ra is a closed 
polyhedron containing f− 
 a the next proposition 
implies that any two of these polyhedra intersect only on their 
boundary 
proposition let a b ∈ a if v ∈ ra ∩rb then v a − 
v b δab −δba 
 
da 
db 
dc 
dd 
de 
v 
w 
x 
y 
z 
u 
p 
figure a -dimensional domain with outcomes 
proof v ∈ ra implies v a − v b ≥ δab and v ∈ rb 
implies v b −v a ≥ δba which by the nonnegative two-cycle 
property implies v a − v b ≤ δab thus v a − v b δab 
and by symmetry v b − v a δba 
finally we restrict the collection of sets {ra a ∈ a} 
to the domain d by defining da ra ∩ d for each a ∈ 
a clearly da is closed and convex and contains f− 
 a 
therefore 
s 
a∈a da d also by proposition any point 
v in da ∩ db satisfies v a − v b δab −δba 
 paths and d-sequences 
a path of size k is a sequence −→a a ak with each 
ai ∈ a possibly with repetition we call −→a an a 
ak path for a path −→a we write −→a for the size of −→a −→a is 
simple if the ai s are distinct 
for b c ∈ a we write pbc for the set of b c -paths and 
spbc for the set of simple b c -paths the δ-weight of path 
−→a is defined by 
δ −→a 
k− x 
i 
δaiai 
a d-sequence of order k is a sequence −→u u uk 
with each ui ∈ d possibly with repetition we call −→u a 
 u uk -sequence for a d-sequence −→u we write ord u for 
the order of −→u for v w ∈ d we write svw 
for the set of 
 v w -sequences 
a compatible pair is a pair −→a −→u where −→a is a path 
and −→u is a d-sequence satisfying ord −→u −→a and for 
each i ∈ k both ui− and ui belong to dai 
we write c −→a for the set of d-sequences −→u that are 
compatible with −→a we say that −→a is admissible if c −→a 
is nonempty for −→u ∈ c −→a we define 
∆−→a −→u 
 −→a − 
x 
i 
 ui ai − ui ai 
for v w ∈ d and b c ∈ a we define cvw 
bc to be the set of 
compatible pairs −→a −→u such that −→a ∈ pbc and −→u ∈ svw 
 
to illustrate these definitions figure gives the 
dissection of a domain a -dimensional plane into five regions 
da db dc dd de d-sequence v w x y z is compatible 
with both path a b c e and path a b d e d-sequence 
 v w u y z is compatible with a unique path a b d e 
d-sequence x w p y z is compatible with a unique path 
 b a d e hence a b c e a b d e and b a d e are 
admissible paths however path a c d or path b e is not 
admissible 
proposition for any compatible pair −→a −→u ∆−→a −→u 
δ −→a 
proof let k ord −→u −→a by the definition of a 
compatible pair ui ∈ dai ∩ dai for i ∈ k − ui ai − 
ui ai δaiai from proposition therefore 
∆−→a −→u 
k− x 
i 
 ui ai − ui ai 
k− x 
i 
δaiai δ −→a 
lemma let b c ∈ a and let −→a −→a ∈ pbc if c −→a ∩ 
c −→a ∅ then δ −→a δ −→a 
proof let −→u be a d-sequence in c −→a ∩ c −→a by 
proposition δ −→a ∆−→a −→u and δ −→a ∆−→a −→u it 
suffices to show ∆−→a −→u ∆−→a −→u 
let k ord −→u −→a −→a since 
∆−→a −→u 
k− x 
i 
 ui ai − ui ai 
 u a 
k− x 
i 
 ui ai − ui− ai − uk− ak 
 u b 
k− x 
i 
 ui ai − ui− ai − uk− c 
∆−→a −→u − ∆−→a −→u 
 
k− x 
i 
 ui ai − ui− ai − ui ai − ui− ai 
 
k− x 
i 
 ui ai − ui ai − ui− ai − ui− ai 
noticing both ui− and ui belong to dai ∩ dai 
 we have by 
proposition 
ui− ai − ui− ai δaiai 
 ui ai − ui ai 
hence ∆−→a −→u − ∆−→a −→u 
 linear d-sequences and straight 
paths 
for v w ∈ d we write vw for the closed line segment 
joining v and w 
a d-sequence −→u of order k is linear provided that there 
is a sequence of real numbers λ ≤ λ ≤ ≤ λk 
such that ui − λi u λiuk in particular each ui 
belongs to u uk for v w ∈ d we write lvw 
for the set of 
linear v w -sequences 
for b c ∈ a and v w ∈ d we write lcvw 
bc for the set of 
compatible pairs −→a −→u such that −→a ∈ pbc and −→u ∈ lvw 
 
for a path −→a we write l −→a for the set of linear 
sequences compatible with −→a we say that −→a is straight if 
l −→a ∅ 
for example in figure d-sequence v w x y z is 
linear while v w u y z x w p y z and x v w y z are 
not hence path a b c e and a b d e are both straight 
however path b a d e is not straight 
 
lemma let b c ∈ a and v ∈ db w ∈ dc there is 
a simple path −→a and d-sequence −→u such that −→a −→u ∈ 
lcvw 
bc furthermore for any such path −→a δ −→a ≤ v b − 
v c 
proof by the convexity of d any sequence of points on 
vw is a d-sequence 
if b c singleton path −→a b and d-sequence −→u 
 v w are obviously compatible δ −→a v b − v c 
so assume b c if db ∩dc ∩vw ∅ we pick an arbitrary 
x from this set and let −→a b c ∈ spbc −→u v x w ∈ 
lvw 
 again it is easy to check the compatibility of −→a −→u 
since v ∈ db v b − v c ≥ δbc δ −→a 
for the remaining case b c and db ∩dc ∩vw ∅ notice 
v w otherwise v w ∈ db ∩ dc ∩ vw so we can define 
λx for every point x on vw to be the unique number in 
such that x − λx v λxw for convenience we write 
x ≤ y for λx ≤ λy 
let ia da ∩ vw for each a ∈ a since d ∪a∈ada we 
have vw ∪a∈aia moreover by the convexity of da and 
vw ia is a possibly trivial closed interval 
we begin by considering the case that ib and ic are each 
a single point that is ib {v} and ic {w} 
let s be a minimal subset of a satisfying ∪s∈sis vw 
for each s ∈ s is is maximal i e not contained in any 
other it for t ∈ s in particular the intervals {is s ∈ 
s} have all left endpoints distinct and all right endpoints 
distinct and the order of the left endpoints is the same as 
that of the right endpoints let k s and index s 
as a ak− in the order defined by the right endpoints 
denote the interval iai by li ri thus l l lk− 
r r rk− and the fact that these intervals cover 
vw implies l v rk− w and for all ≤ i ≤ k − 
li ≤ ri which further implies li ri now we define 
the path −→a a a ak− ak with a b ak c 
and a a ak− as above define the linear d-sequence 
−→u u u uk by u u v uk w and for 
 ≤ i ≤ k− ui ri it follows immediately that −→a −→u ∈ 
lcvw 
bc neither b nor c is in s since lb rb and lc rc thus 
−→a is simple 
finally to show δ −→a ≤ v b − v c we note 
v b − v c v a − v ak 
k− x 
i 
 v ai − v ai 
and 
δ −→a ∆−→a −→u 
k− x 
i 
 ui ai − ui ai 
 v a − v a 
k− x 
i 
 ri ai − ri ai 
for two outcomes d e ∈ a let us define fde z z d −z e 
for all z ∈ d it suffices to show faiai ri ≤ faiai v for 
 ≤ i ≤ k − 
fact for d e ∈ a fde z is a linear function of z 
furthermore if x ∈ dd and y ∈ de with x y then 
fde x x d − x e ≥ δde ≥ −δed ≥ − y e − y d 
fde y therefore fde z is monotonically nonincreasing along 
the line 
←→ 
xy as z moves in the direction from x to y 
applying this fact with d ai e ai x li and y ri 
gives the desired conclusion this completes the proof for 
the case that ib {v} and ic {w} 
for general ib ic rb lc otherwise db ∩ dc ∩ vw ib ∩ 
ic ∅ let v rb and w lc clearly we can apply the 
above conclusion to v ∈ db w ∈ dc and get a compatible 
pair −→a −→u ∈ lcv w 
bc with −→a simple and δ −→a ≤ v b − 
v c define the linear d-sequence −→u by u v uk w 
and ui ui for i ∈ k − −→a −→u ∈ lcvw 
bc is evident 
moreover applying the above fact with d b e c x v 
and y w we get v b − v c ≥ v b − v c ≥ δ −→a 
corollary for any b c ∈ a there is a straight b 
c path 
the main result of this section theorem says that for 
any b c ∈ a every straight b c -path has the same δ-weight 
to prove this we first fix v ∈ db and w ∈ dc and show 
 lemma that every straight b c -path compatible with 
some linear v w -sequence has the same δ-weight ρbc v w 
we then show in theorem that ρbc v w is the same for 
all choices of v ∈ db and w ∈ dc 
lemma for b c ∈ a there is a function ρbc db × 
dc −→ r satisfying that for any −→a −→u ∈ lcvw 
bc δ −→a 
ρbc v w 
proof let −→a −→u −→a −→u ∈ lcvw 
bc it suffices to 
show δ −→a δ −→a to do this we construct a linear 
 v w -sequence −→u and paths −→a ∗ 
 −→a ∗∗ 
∈ pbc both 
compatible with −→u satisfying δ −→a ∗ 
 δ −→a and δ −→a ∗∗ 
 δ −→a 
lemma implies δ −→a ∗ 
 δ −→a ∗∗ 
 which will complete the 
proof 
let −→a ord −→u k and −→a ord −→u l we 
select −→u to be any linear v w -sequence u u ut such 
that −→u and −→u are both subsequences of −→u i e there 
are indices i i · · · ik t and j 
j · · · jl t such that −→u ui ui uik and 
−→u uj uj ujl we now construct a b c -path 
−→a ∗ 
compatible with −→u such that δ −→a ∗ 
 δ −→a an 
analogous construction gives −→a ∗∗ 
compatible with −→u such 
that δ −→a ∗∗ 
 δ −→a this will complete the proof 
−→a ∗ 
is defined as follows for ≤ j ≤ t a∗ 
j ar where 
r is the unique index satisfying ir− j ≤ ir since both 
uir− ur− and uir ur belong to dar 
 uj ∈ dar 
for 
ir− ≤ j ≤ ir by the convexity of dar 
 the compatibility of 
 −→a ∗ 
 −→u follows immediately clearly a∗ 
 a b and a∗ 
t 
ak c so −→a ∗ 
∈ pbc furthermore as δa∗ 
j a∗ 
j 
 δarar 
 
for each r ∈ k ir− j ir 
δ −→a ∗ 
 
k− x 
r 
δa∗ 
ir 
a∗ 
ir 
 
k− x 
r 
δarar 
 δ −→a 
we are now ready for the main theorem of the section 
theorem ρbc is a constant map on db × dc thus 
for any b c ∈ a every straight b c -path has the same 
δweight 
proof for a path −→a v w is compatible with −→a if 
there is a linear v w -sequence compatible with −→a we 
write cp −→a for the set of pairs v w compatible with 
−→a ρbc is constant on cp −→a because for each v w ∈ 
cp −→a ρbc v w δ −→a by lemma we also haves 
−→a ∈spbc 
cp −→a db ×dc since a is finite spbc the set 
of simple paths from b to c is finite as well 
 
next we prove that for any path −→a cp −→a is closed 
let vn 
 wn 
 n ∈ n be a convergent sequence in cp −→a 
and let v w be the limit we want to show that v w ∈ 
cp −→a for each n ∈ n since vn 
 wn 
 ∈ cp −→a there is 
a linear vn 
 wn 
 -sequence un 
compatible with −→a i e there 
are λn 
 ≤ λn 
 ≤ ≤ λn 
k k −→a such that 
un 
j − λn 
j vn 
 λn 
j wn 
 j k since for each 
n λn 
 λn 
 λn 
 λn 
k belongs to the closed bounded set 
 k 
we can choose an infinite subset i ⊆ n such that the 
sequence λn 
 n ∈ i converges let λ λ λ λk be 
the limit clearly λ ≤ λ ≤ · · · ≤ λk 
define the linear v w -sequence −→u by uj − λj v 
λj w j k then for each j ∈ { k} uj is 
the limit of the sequence un 
j n ∈ i for j each un 
j 
belongs to the closed set daj so uj ∈ daj similarly for j 
k each un 
j belongs to the closed set daj so uj ∈ daj 
hence −→a −→u is compatible implying v w ∈ cp −→a 
now we have db × dc covered by finitely many closed 
subsets on each of them ρbc is a constant 
suppose for contradiction that there are v w v w ∈ 
db × dc such that ρbc v w ρbc v w 
l { − λ v λv − λ w λw λ ∈ } 
is a line segment in db ×dc by the convexity of db dc let 
l { x y ∈ l ρbc x y ρbc v w } 
and l l − l clearly v w ∈ l v w ∈ l let 
p {−→a ∈ spbc δ −→a ρbc v w } 
l 
`s 
−→a ∈p cp −→a 
´ 
∩ l l 
s 
−→a ∈spbc−p cp −→a 
 
∩ l 
are closed by the finiteness of p this is a contradiction 
since it is well known and easy to prove that a line segment 
can not be expressed as the disjoint union of two nonempty 
closed sets 
summarizing corollary lemma and theorem we 
have 
corollary for any b c ∈ a there is a real number 
ρbc with the property that there is at least one straight 
 b c -path of δ-weight ρbc and every straight b c -path 
has δ-weight ρbc 
 proof of theorem 
lemma ρbc ≤ δbc for all b c ∈ a 
proof for contradiction suppose ρbc − δbc 
by the definition of δbc there exists v ∈ f− 
 b ⊆ db with 
v b − v c δbc ρbc pick an arbitrary w ∈ dc 
by lemma there is a compatible pair −→a −→u ∈ lcvw 
bc 
with δ −→a ≤ v b − v c since −→a is a straight b c -path 
ρbc δ −→a ≤ v b − v c leading to a contradiction 
define another edge-weighted complete directed graph gf 
on vertex set a where the weight of arc a b is ρab 
immediately from lemma the weight of every directed cycle in 
gf is bounded below by its weight in gf to prove theorem 
 it suffices to show the zero cycle property of gf i e 
every directed cycle has weight zero we begin by considering 
two-cycles 
lemma ρbc ρcb for all b c ∈ a 
proof let −→a be a straight b c -path compatible with 
linear sequence −→u let −→a be the reverse of −→a and −→u the 
reverse of −→u obviously −→a −→u is compatible as well and 
thus −→a is a straight c b -path therefore 
ρbc ρcb δ −→a δ −→a 
k− x 
i 
δaiai 
k− x 
i 
δai ai 
 
k− x 
i 
 δaiai δai ai 
where the final equality uses proposition 
next for three cycles we first consider those compatible 
with linear triples 
lemma if there are collinear points u ∈ da v ∈ db 
w ∈ dc a b c ∈ a ρab ρbc ρca 
proof first we prove for the case where v is between u 
and w from lemma there are compatible pairs −→a −→u ∈ 
lcuv 
ab −→a −→u ∈ lcvw 
bc let −→a ord −→u k and 
 −→a ord −→u l we paste −→a and −→a together as 
−→a a a a ak− ak a al c 
−→u and −→u as 
−→u u u u uk v u u ul w 
clearly −→a −→u ∈ lcuw 
ac and 
δ −→a 
k− x 
i 
δaiai 
 δak 
a 
 
l− x 
i 
δai ai 
 δ −→a δbb δ −→a 
 δ −→a δ −→a 
therefore ρac δ −→a δ −→a δ −→a ρab ρbc 
moreover ρac −ρca by lemma so we get ρab ρbc 
ρca 
now suppose w is between u and v by the above 
argument we have ρac ρcb ρba and by lemma 
ρab ρbc ρca −ρba − ρcb − ρac 
the case that u is between v and w is similar 
now we are ready for the zero three-cycle property 
lemma ρab ρbc ρca for all a b c ∈ a 
proof let s { a b c ρab ρbc ρca } and 
for contradiction suppose s ∅ s is finite for each 
a ∈ a choose va ∈ da arbitrarily and let t be the convex 
hull of {va a ∈ a} for each a b c ∈ s let rabc 
da × db × dc ∩ t 
 clearly each rabc is nonempty and 
compact moreover by lemma no u v w ∈ rabc is 
collinear 
define f d 
→ r by f u v w v−u w−v u−w 
for a b c ∈ s the restriction of f to the compact set rabc 
attains a minimum m a b c at some point u v w ∈ rabc 
by the continuity of f i e there exists a triangle ∆uvw of 
minimum perimeter within t with u ∈ da v ∈ db w ∈ dc 
choose a∗ 
 b∗ 
 c∗ 
 ∈ s so that m a∗ 
 b∗ 
 c∗ 
 is minimum 
and let u∗ 
 v∗ 
 w∗ 
 ∈ ra∗b∗c∗ be a triple achieving it pick 
an arbitrary point p in the interior of ∆u∗ 
v∗ 
w∗ 
 by the 
convexity of domain d there is d ∈ a such that p ∈ dd 
 
consider triangles ∆u∗ 
pw∗ 
 ∆w∗ 
pv∗ 
and ∆v∗ 
pu∗ 
 since 
each of them has perimeter less than that of ∆u∗ 
v∗ 
w∗ 
and 
all three triangles are contained in t by the minimality of 
∆u∗ 
v∗ 
w∗ 
 a∗ 
 d c∗ 
 c∗ 
 d b∗ 
 b∗ 
 d a∗ 
 ∈ s thus 
ρa∗d ρdc∗ ρc∗a∗ 
ρc∗d ρdb∗ ρb∗c∗ 
ρb∗d ρda∗ ρa∗b∗ 
summing up the three equalities 
 ρa∗d ρdc∗ ρc∗d ρdb∗ ρb∗d ρda∗ 
 ρc∗a∗ ρb∗c∗ ρa∗b∗ 
which yields a contradiction 
ρa∗b∗ ρb∗c∗ ρc∗a∗ 
with the zero two-cycle and three-cycle properties the 
zero cycle property of gf is immediate as noted earlier 
this completes the proof of theorem 
theorem every directed cycle of gf has weight zero 
proof clearly zero two-cycle and three-cycle properties 
imply triangle equality ρab ρbc ρac for all a b c ∈ a for 
a directed cycle c a a aka by inductively applying 
triangle equality we have 
pk− 
i ρaiai ρa ak therefore 
the weight of c is 
k− x 
i 
ρaiai ρaka ρa ak ρaka 
as final remarks we note that our result implies the 
following strengthenings of theorem 
corollary for any b c ∈ a every admissible b 
c path has the same δ-weight ρbc 
proof first notice that for any b c ∈ a if db ∩dc ∅ 
δbc ρbc to see this pick v ∈ db ∩ dc arbitrarily 
obviously path −→a b c is compatible with linear sequence 
−→u v v v and is thus a straight b c -path hence 
ρbc δ −→a δbc 
now for any b c ∈ a and any b c -path −→a with c −→a 
∅ let −→u ∈ c −→a since ui ∈ dai ∩ dai for i ∈ −→a − 
δ −→a 
 −→a − 
x 
i 
δaiai 
 −→a − 
x 
i 
ρaiai 
which by theorem −ρa −→a a ρa a −→a 
 ρbc 
corollary for any b c ∈ a ρbc is equal to δ∗ 
bc the 
minimum δ-weight over all b c -paths 
proof clearly ρbc ≥ δ∗ 
bc by corollary on the other 
hand for every b c -path −→a b a a ak c by 
lemma 
δ −→a 
k− x 
i 
δaiai ≥ 
k− x 
i 
ρaiai 
which by theorem −ρaka ρa ak ρbc hence ρbc ≤ 
δ∗ 
bc which completes the proof 
 counterexamples to stronger 
forms of theorem 
theorem applies to social choice functions with convex 
domain and finite range we now show that neither of these 
hypotheses can be omitted our examples are single player 
functions 
the first example illustrates that convexity can not be 
omitted we present an untruthful single player social choice 
function with three outcomes a b c satisfying w-mon on a 
path-connected but non-convex domain the domain is the 
boundary of a triangle whose vertices are x − y 
 − and z − x and the open line segment 
zx is assigned outcome a y and the open line segment xy 
is assigned outcome b and z and the open line segment 
yz is assigned outcome c clearly δab −δba δbc 
−δcb δca −δac − w-mon the nonnegative 
twocycle property holds since there is a negative cycle δab 
δbc δca − by lemma this is not a truthful choice 
function 
we now show that the hypothesis of finite range can not 
be omitted we construct a family of single player social 
choice functions each having a convex domain and an infinite 
number of outcomes and satisfying weak monotonicity but 
not truthfulness 
our examples will be specified by a positive integer n and 
an n × n matrix m satisfying the following properties 
m is non-singular m is positive semidefinite there 
are distinct i i ik ∈ n satisfying 
k− x 
j 
 m ij ij − m ij ij m ik ik − m ik i 
here is an example matrix with n and i i i 
 
 
  
 − 
− 
 − 
 
a 
let e e en denote the standard basis of rn 
 let 
sn denote the convex hull of {e e en} which is the 
set of vectors in rn 
with nonnegative coordinates that sum 
to the range of our social choice function will be the 
set sn and the domain d will be indexed by sn that is 
d {yλ λ ∈ sn} where yλ is defined below the function 
f maps yλ to λ 
next we specify yλ by definition d must be a set of 
functions from sn to r for λ ∈ sn the domain element 
yλ sn −→ r is defined by yλ α λt 
mα the 
nonsingularity of m guarantees that yλ yµ for λ µ ∈ sn 
it is easy to see that d is a convex subset of the set of all 
functions from sn to r 
the outcome graph gf is an infinite graph whose vertex 
set is the outcome set a sn for outcomes λ µ ∈ a the 
edge weight δλµ is equal to 
δλµ inf{v λ − v µ f v λ} 
 yλ λ − yλ µ λt 
mλ − λt 
mµ λt 
m λ − µ 
we claim that gf satisfies the nonnegative two-cycle 
property w-mon but has a negative cycle and hence is not 
truthful 
for outcomes λ µ ∈ a 
δλµ δµλ λt 
m λ−µ µt 
m µ−λ λ−µ t 
m λ−µ 
 
which is nonnegative since m is positive semidefinite hence 
the nonnegative two-cycle property holds next we show 
that gf has a negative cycle let i i ik be a 
sequence of indices satisfying property of m we claim 
ei ei eik ei is a negative cycle since 
δeiej et 
i m ei − ej m i i − m i j 
for any i j ∈ k the weight of the cycle 
k− x 
j 
δeij 
eij 
 δeik 
ei 
 
k− x 
j 
 m ij ij − m ij ij m ik ik − m ik i 
which completes the proof 
finally we point out that the third property imposed on 
the matrix m has the following interpretation let r m 
{r r rn} be the set of row vectors of m and let hm be 
the single player social choice function with domain r m 
and range { n} mapping ri to i property is 
equivalent to the condition that the outcome graph ghm has a 
negative cycle by lemma this is equivalent to the 
condition that hm is untruthful 
 future work 
as stated in the introduction the goal underlying the 
work in this paper is to obtain useful and general 
characterizations of truthfulness 
let us say that a set d of p × a real valuation matrices 
is a wm-domain if any social choice function on d 
satisfying weak monotonicity is truthful in this paper we showed 
that for finite a any convex d is a wm-domain typically 
the domains of social choice functions considered in 
mechanism design are convex but there are interesting examples 
with non-convex domains e g combinatorial auctions with 
unknown single-minded bidders it is intriguing to find the 
most general conditions under which a set d of real 
matrices is a wm-domain we believe that convexity is the main 
part of the story i e a wm-domain is after excluding some 
exceptional cases essentially a convex set 
turning to parametric representations let us say a set 
d of p × a matrices is an am-domain if any truthful 
social choice function with domain d is an affine maximizer 
roberts theorem says that the unrestricted domain is an 
am-domain what are the most general conditions under 
which a set d of real matrices is an am-domain 
acknowledgments 
we thank ron lavi for helpful discussions and the two 
anonymous referees for helpful comments 
 references 
 a archer and e tardos truthful mechanisms for 
one-parameter agents in ieee symposium on 
foundations of computer science pages - 
 y bartal r gonen and n nisan incentive 
compatible multi unit combinatorial auctions in 
tark proceedings of the th conference on 
theoretical aspects of rationality and knowledge pages 
 - acm press 
 s bikhchandani s chatterjee and a sen incentive 
compatibility in multi-unit auctions technical report 
ucla department of economics dec 
 a goldberg j hartline a karlin m saks and 
a wright competitive auctions 
 h gui r muller and r vohra dominant strategy 
mechanisms with multidimensional types technical 
report maastricht meteor maastricht 
research school of economics of technology and 
organization available at 
http ideas repec org p dgr umamet html 
 r lavi a mu alem and n nisan towards a 
characterization of truthful combinatorial auctions in 
focs proceedings of the th annual ieee 
symposium on foundations of computer science page 
 ieee computer society 
 d lehmann l o callaghan and y shoham truth 
revelation in approximately efficient combinatorial 
auctions j acm - 
 a mas-colell m whinston and j green 
microeconomic theory oxford university press 
 n nisan algorithms for selfish agents lecture notes 
in computer science - 
 k roberts the characterization of implementable 
choice rules aggregation and revelation of preferences 
j-j laffont ed north holland publishing company 
 j -c rochet a necessary and sufficient condition for 
rationalizability in a quasi-linear context journal of 
mathematical economics - 
 i rozenshtrom dominant strategy implementation 
with quasi-linear preferences master s thesis dept of 
economics the hebrew university jerusalem israel 
 
 
information markets vs opinion pools 
an empirical comparison 
yiling chen 
chao-hsien chu 
tracy mullen 
school of information sciences technology 
the pennsylvania state university 
university park pa 
{ychen chu tmullen} ist psu edu 
david m pennock 
yahoo research labs 
 n pasadena ave rd floor 
pasadena ca 
pennockd yahoo-inc com 
abstract 
in this paper we examine the relative forecast accuracy of 
information markets versus expert aggregation we 
leverage a unique data source of almost people s subjective 
probability judgments on us national football league 
games and compare with the market probabilities given 
by two different information markets on exactly the same 
events we combine assessments of multiple experts via 
linear and logarithmic aggregation functions to form pooled 
predictions prices in information markets are used to 
derive market predictions our results show that at the same 
time point ahead of the game information markets provide 
as accurate predictions as pooled expert assessments in 
screening pooled expert predictions we find that arithmetic 
average is a robust and efficient pooling function 
weighting expert assessments according to their past performance 
does not improve accuracy of pooled predictions and 
logarithmic aggregation functions offer bolder predictions than 
linear aggregation functions the results provide insights 
into the predictive performance of information markets and 
the relative merits of selecting among various opinion 
pooling methods 
categories and subject descriptors 
j computer applications social and behavioral 
sciences-economics 
general terms 
economics performance 
 introduction 
forecasting is a ubiquitous endeavor in human societies 
for decades scientists have been developing and exploring 
various forecasting methods which can be roughly divided 
into statistical and non-statistical approaches statistical 
approaches require not only the existence of enough 
historical data but also that past data contains valuable 
information about the future event when these conditions can 
not be met non-statistical approaches that rely on 
judgmental information about the future event could be better 
choices one widely used non-statistical method is to elicit 
opinions from experts since experts are not generally in 
agreement many belief aggregation methods have been 
proposed to combine expert opinions together and form a 
single prediction these belief aggregation methods are called 
opinion pools which have been extensively studied in 
statistics and management sciences and 
applied in many domains such as group decision making 
and risk analysis 
with the fast growth of the internet information markets 
have recently emerged as a promising non-statistical 
forecasting tool information markets sometimes called 
prediction markets idea markets or event markets are 
markets designed for aggregating information and making 
predictions about future events to form the predictions 
information markets tie payoffs of securities to outcomes of 
events for example in an information market to predict 
the result of a us professional national football league 
 nfl game say new england vs carolina the security 
pays a certain amount of money per share to its holders if 
and only if new england wins the game otherwise it pays 
off nothing the security price before the game reflects the 
consensus expectation of market traders about the 
probability of new england winning the game such markets 
are becoming very popular the iowa electronic markets 
 iem are real-money futures markets to predict 
economic and political events such as elections the 
hollywood stock exchange hsx is a virtual play-money 
exchange for trading securities to forecast future box office 
proceeds of new movies and the outcomes of entertainment 
awards etc tradesports com a real-money betting 
exchange registered in ireland hosts markets for sports 
political entertainment and financial events the foresight 
exchange fx allows traders to wager play money on 
unresolved scientific questions or other claims of public 
interest and newsfutures com s world news exchange has 
 
popular sports and financial betting markets also grounded 
in a play-money currency 
despite the popularity of information markets one of the 
most important questions to ask is how accurately can 
information markets predict previous research in general 
shows that information markets are remarkably accurate 
the political election markets at iem predict the election 
outcomes better than polls prices in hsx 
and fx have been found to give as accurate or more 
accurate predictions than judgment of individual experts 
 however information markets have not been 
calibrated against opinion pools except for servan-schreiber 
et al in which the authors compare two information 
markets against arithmetic average of expert opinions since 
information markets in nature offer an adaptive and 
selforganized mechanism to aggregate opinions of market 
participants it is interesting to compare them with existing 
opinion pooling methods to evaluate the performance of 
information markets from another perspective the 
comparison will provide beneficial guidance for practitioners to 
choose the most appropriate method for their needs 
this paper contributes to the literature in two ways 
as an initial attempt to compare information markets with 
opinion pools of multiple experts it leads to a better 
understanding of information markets and their promise as an 
alternative institution for obtaining accurate forecasts 
in screening opinion pools to be used in the comparison we 
cast insights into relative performances of different opinion 
pools in terms of prediction accuracy we compare two 
information markets with several linear and logarithmic 
opinion pools linop and logop at predicting the results of 
nfl games our results show that at the same time point 
ahead of the game information markets provide as accurate 
predictions as our carefully selected opinion pools in 
selecting the opinion pools to be used in our comparison we 
find that arithmetic average is a robust and efficient pooling 
function weighting expert assessments according to their 
past performances does not improve the prediction accuracy 
of opinion pools and logop offers bolder predictions than 
linop the remainder of the paper is organized as follows 
section reviews popular opinion pooling methods 
section introduces the basics of information markets data 
sets and our analysis methods are described in section 
we present results and analysis in section followed by 
conclusions in section 
 review of opinion pools 
clemen and winkler classify opinion pooling 
methods into two broad categories mathematical approaches 
and behavioral approaches in mathematical approaches 
the opinions of individual experts are expressed as 
subjective probability distributions over outcomes of an 
uncertain event they are combined through various 
mathematical methods to form an aggregated probability 
distribution genest and zidek and french provide 
comprehensive reviews of mathematical approaches mathematical 
approaches can be further distinguished into axiomatic 
approaches and bayesian approaches axiomatic approaches 
apply prespecified functions that map expert opinions 
expressed as a set of individual probability distributions to 
a single aggregated probability distribution these 
pooling functions are justified using axioms or certain desirable 
properties two of the most common pooling functions are 
the linear opinion pool linop and the logarithmic opinion 
pool logop using linop the aggregate probability 
distribution is a weighted arithmetic mean of individual 
probability distributions 
p θ 
n 
i 
wipi θ 
where pi θ is expert i s probability distribution of uncertain 
event θ p θ represents the aggregate probability 
distribution wi s are weights for experts which are usually 
nonnegative and sum to and n is the number of experts using 
logop the aggregate probability distribution is a weighted 
geometric mean of individual probability distributions 
p θ k 
n 
i 
pi θ wi 
 
where k is a normalization constant to ensure that the pooled 
opinion is a probability distribution other axiomatic 
pooling methods often are extensions of linop logop 
or both winkler and morris establish the 
early framework of bayesian aggregation methods bayesian 
approaches assume as if there is a decision maker who has a 
prior probability distribution over event θ and a likelihood 
function over expert opinions given the event this decision 
maker takes expert opinions as evidence and updates its 
priors over the event and opinions according to bayes rule the 
resulted posterior probability distribution of θ is the pooled 
opinion 
behavioral approaches have been widely studied in the 
field of group decision making and organizational 
behavior the important assumption of behavioral approaches is 
that through exchanging opinions or information experts 
can eventually reach an equilibrium where further 
interaction won t change their opinions one of the best known 
behavioral approaches is the delphi technique 
typically this method and its variants do not allow open 
discussion but each expert has chance to judge opinions of other 
experts and is given feedback experts then can reassess 
their opinions and repeat the process until a consensus or a 
smaller spread of opinions is achieved some other 
behavioral methods such as the nominal group technique 
promote open discussions in controlled environments 
each approach has its pros and cons axiomatic 
approaches are easy to use but they don t have a normative 
basis to choose weights in addition several impossibility 
results e g genest show that no aggregation 
function can satisfy all desired properties of an opinion pool 
unless the pooled opinion degenerates to a single individual 
opinion which effectively implies a dictator bayesian 
approaches are nicely based on the normative bayesian 
framework however it is sometimes frustratingly difficult to 
apply because it requires either constructing an obscenely 
complex joint prior over the event and opinions often 
impractical even in terms of storage space complexity not 
to mention from an elicitation standpoint or making 
strong assumptions about the prior like conditional 
independence of experts behavior approaches allow experts to 
dynamically improve their information and revise their 
opinions during interactions but many of them are not fixed or 
completely specified and can t guarantee convergence or 
repeatability 
 
 how information markets work 
much of the enthusiasm for information markets stems 
from hayek hypothesis and efficient market 
hypothesis hayek in his classic critique of central planning in 
 s claims that the price system in a competitive market 
is a very efficient mechanism to aggregate dispersed 
information among market participants the efficient market 
hypothesis further states that in an efficient market the 
price of a security almost instantly incorporates all 
available information the market price summarizes all relevant 
information across traders hence is the market participants 
consensus expectation about the future value of the security 
empirical evidence supports both hypotheses to a large 
extent thus when associating the value of a 
security with the outcome of an uncertain future event market 
price by revealing the consensus expectation of the security 
value can indirectly predict the outcome of the event this 
idea gives rise to information markets 
for example if we want to predict which team will win 
the nfl game between new england and carolina an 
information market can trade a security if new england 
defeats carolina whose payoff per share at the end of the 
game is specified as follow 
 if new england wins the game 
 otherwise 
the security price should roughly equal the expected payoff 
of the security in an efficient market the time value of 
money usually can be ignored because durations of most 
information markets are short assuming exposure to risk is 
roughly equal for both outcomes or that there are sufficient 
effectively risk-neutral speculators in the market the price 
should not be biased by the risk attitudes of various players 
in the market thus 
p pr patriots win × − pr patriots win × 
where p is the price of the security if new england 
defeats carolina and pr patriots win is the probability 
that new england will win the game observing the security 
price p before the game we can derive pr patriots win 
which is the market participants collective prediction about 
how likely it is that new england will win the game 
the above security is a winner-takes-all contract it is 
used when the event to be predicted is a discrete random 
variable with disjoint outcomes in this case binary its 
price predicts the probability that a specific outcome will be 
realized when the outcome of a prediction problem can be 
any value in a continuous interval we can design a security 
that pays its holder proportional to the realized value this 
kind of security is what wolfers and zitzewitz called 
an index contract it predicts the expected value of a 
future outcome many other aspects of a future event such as 
median value of outcome can also be predicted in 
information markets by designing and trading different securities 
wolfers and zitzewitz provide a summary of the main 
types of securities traded in information markets and what 
statistical properties they can predict in practice 
conceiving a security for a prediction problem is only one of the 
many decisions in designing an effective information 
market spann and skiera propose an initial framework for 
designing information markets 
 design of analysis 
 data sets 
our data sets cover nfl games held between 
september th and december th nfl games are 
very suitable for our purposes because two online 
exchanges and one online prediction contest already exist that 
provide data on both information markets and the 
opinions of self-identified experts for the same set of games 
the popularity of nfl games in the united states provides 
natural incentives for people to participate in information 
markets and or the contest which increases liquidity of 
information markets and improves the quality and number 
of opinions in the contest intense media coverage and 
analysis of the profiles and strengths of teams and 
individual players provide the public with much information so that 
participants of information markets and the contest can be 
viewed as knowledgeable regarding to the forecasting goal 
information market data was acquired by using a 
specially designed crawler program from tradesports com s 
football-nfl markets and newsfutures com s sports 
exchange for each nfl game both tradesports and 
newsfutures have a winner-takes-all information market to 
predict the game outcome we introduce the design of the 
two markets according to spann and skiera s three steps for 
designing an information market as below 
 choice of forecasting goal markets at both 
tradesports and newsfutures aim at predicting which one 
of the two teams will win a nfl football game they 
trade similar winner-takes-all securities that pay off 
 if a team wins the game and if it loses the game 
small differences exist in how they deal with ties in 
the case of a tie tradesports will unwind all trades 
that occurred and refund all exchange fees but the 
security is worth in newsfutures since the 
probability of a tie is usually very low much less the 
prices at both markets effectively represent the market 
participants consensus assessment of the probability 
that the team will win 
 incentive for participation and information 
revelation tradesports and newsfutures use different 
incentives for participation and information revelation 
tradesports is a real-money exchange a trader needs 
to open and fund an account with a minimum of 
to participate in the market both profits and losses 
can occur as a result of trading activity on the 
contrary a trader can register at newsfutures for free and 
get units of sport exchange virtual money at the 
time of registration traders at newsfutures will not 
incur any real financial loss they can accumulate 
virtual money by trading securities the virtual money 
can then be used to bid for a few real prizes at 
newsfutures online shop 
 financial market design both markets at 
tradesports and newsfutures use the continuous double 
auction as their trading mechanism tradesports charges 
a small fee on each security transaction and expiry 
while newsfutures does not 
we can see that the main difference between two information 
markets is real money vs virtual money servan-schreiber 
 
et al have compared the effect of money on the 
performance of the two information markets and concluded that 
the prediction accuracy of the two markets are at about the 
same level not intending to compare these two markets 
we still use both markets in our analysis to ensure that our 
findings are not accidental 
we obtain the opinions of self-identified experts for 
nfl games from the probabilityfootball online contest 
one of several probabilitysports contests the contest is 
free to enter participants of the contest are asked to enter 
their subjective probability that a team will win a game 
by noon on the day of the game importantly the contest 
evaluates the participants performance via the quadratic 
scoring rule 
s − × prob lose 
 
where s represents the score that a participant earns for the 
game and prob lose is the probability that the participant 
assigns to the actual losing team the quadratic score is 
one of a family of so-called proper scoring rules that have 
the property that an expert s expected score is maximized 
when the expert reports probabilities truthfully for 
example for a game team a vs team b if a player assigns to 
both team a and b his her score for the game is no matter 
which team wins if he she assigns to team a and to 
team b showing that he is confident in team a s winning 
he she will score points for the game if team a wins and 
lose points if team b wins this quadratic scoring rule 
rewards bold predictions that are right but penalizes bold 
predictions that turn out to be wrong the top players 
measured by accumulated scores over all games win the prizes 
of the contest the suggested strategy at the contest 
website is to make picks for each game that match as closely 
as possible the probabilities that each team will win this 
strategy is correct if the participant seeks to maximize 
expected score however as prizes are awarded only to the top 
few winners participants goals are to maximize the 
probability of winning not maximize expected score resulting in a 
slightly different and more risk-seeking optimization 
still 
as far as we are aware this data offer the closest thing 
available to true subjective probability judgments from so many 
people over so many public events that have corresponding 
information markets 
 methods of analysis 
in order to compare the prediction accuracy of 
information markets and that of opinion pools we proceed to derive 
predictions from market data of tradesports and 
newsfutures form pooled opinions using expert data from 
probabilityfootball contest and specify the performance measures 
to be used 
 deriving predictions 
for information markets deriving predictions is 
straightforward we can take the security price and divide it by 
 to get the market s prediction of the probability that 
a team will win to match the time when participants at 
the probabilityfootball contest are required to report their 
probability assessments we derive predictions using the last 
trade price before noon on the day of the game for more 
 
ideally prizes would be awarded by lottery in proportion 
to accumulated score 
than half of the games this time is only about an hour 
earlier than the game starting time while it is several hours 
earlier for other games two sets of market predictions are 
derived 
 nf prediction equals newsfutures last trade price 
before noon of the game day divided by 
 ts prediction equals tradesports last trade price 
before noon of the game day divided by 
we apply linop and logop to probabilityfootball data 
to obtain aggregate expert predictions the reason that we 
do not consider other aggregation methods include data 
from probabilityfootball is only suitable for mathematical 
pooling methods-we can rule out behavioral approaches 
 bayesian aggregation requires us to make assumptions 
about the prior probability distribution of game outcomes 
and the likelihood function of expert opinions given the 
large number of games and participants making reasonable 
assumptions is difficult and for axiomatic approaches 
previous research has shown that simpler aggregation 
methods often perform better than more complex methods 
because the output of logop is indeterminate if there are 
probability assessments of both and and because 
assessments of and are dictatorial using logop we add 
a small number to an expert opinion if it is and 
subtract from it if it is 
in pooling opinions we consider two influencing factors 
weights of experts and number of expert opinions to be 
pooled for weights of experts we experiment with equal 
weights and performance-based weights the 
performancebased weights are determined according to previous 
accumulated score in the contest the score for each game is 
calculated according to equation the scoring rule used in 
the probabilityfootball contest for the first week since no 
previous scores are available we choose equal weights for 
later weeks we calculate accumulated past scores for each 
player because the cumulative scores can be negative we 
shift everyone s score if needed to ensure the weights are 
non-negative thus 
wi 
cumulative scorei shift 
n 
j cumulative scorej shift 
 
where shift equals if the smallest cumulative scorej is 
non-negative and equals the absolute value of the 
smallest cumulative scorej otherwise for simplicity we call 
performance-weighted opinion pool as weighted and equally 
weighted opinion pool as unweighted we will use them 
interchangeably in the remaining of the paper 
as for the number of opinions used in an opinion pool 
we form different opinion pools with different number of 
experts only the best performing experts are selected for 
example to form an opinion pool with expert opinions 
we choose the top participants since there is no 
performance record for the first week we use opinions of all 
participants in the first week for week we select opinions of 
 individuals whose scores in the first week are among the 
top for week individuals whose cumulative scores 
of week and are among the top s are selected experts 
are chosen in a similar way for later weeks thus the top 
 participants can change from week to week 
the possible opinion pools varied in pooling functions 
weighting methods and number of expert opinions are shown 
 
table pooled expert predictions 
 symbol description 
 lin-all-u unweighted equally weighted linop 
of all experts 
 lin-all-w weighted performance-weighted 
linop of all experts 
 lin-n-u unweighted equally weighted linop 
with n experts 
 lin-n-w weighted performance-weighted 
linop with n experts 
 log-all-u unweighted equally weighted logop 
of all experts 
 log-all-w weighted performance-weighted 
logop of all experts 
 log-n-u unweighted equally weighted logop 
with n experts 
 log-n-w weighted performance-weighted 
logop with n experts 
in table lin represents linear and log represents 
logarithmic n is the number of expert opinions that are 
pooled and all indicates that all opinions are combined 
we use u to symbolize unweighted equally weighted 
opinion pools w is used for weighted performance-weighted 
opinion pools lin-all-u the equally weighted linop with 
all participants is basically the arithmetic mean of all 
participants opinions log-all-u is simply the geometric mean 
of all opinions 
when a participant did not enter a prediction for a 
particular game that participant was removed from the opinion 
pool for that game this contrasts with the 
probabilityfootball average reported on the contest website and used 
by servan-schreiber et al where unreported 
predictions were converted to probability predictions 
 performance measures 
we use three common metrics to assess prediction 
accuracy of information markets and opinion pools these 
measures have been used by servan-schreiber et al in 
evaluating the prediction accuracy of information markets 
 absolute error prob lose 
where prob lose is the probability assigned to the 
eventual losing team absolute error simply measures 
the difference between a perfect prediction for 
winning team and the actual prediction a prediction 
with lower absolute error is more accurate 
 quadratic score − × prob lose 
 
quadratic score is the scoring function that is used in 
the probabilityfootball contest it is a linear 
transformation of squared error prob lose 
 which is one of 
the mostly used metrics in evaluating forecasting 
accuracy quadratic score can be negative a prediction 
with higher quadratic score is more accurate 
 logarithmic score log prob w in 
where prob w in is the probability assigned to the 
eventual winning team the logarithmic score like 
the quadratic score is a proper scoring rule a 
prediction with higher less negative logarithmic score is 
more accurate 
 empirical results 
 performance of opinion pools 
depending on how many opinions are used there can be 
numerous different opinion pools we first examine the 
effect of number of opinions on prediction accuracy by forming 
opinion pools with the number of expert opinions varying 
from to in the probabilityfootball competition not 
all registered participants provide their probability 
assessments for every game is the smallest number of 
participants for all games for each game we sort experts 
according to their accumulated quadratic score in previous 
weeks predictions of the best performing n participants are 
picked to form an opinion pool with n experts 
figure shows the prediction accuracy of linop and 
logop in terms of mean values of the three performance 
measures across all games we can see the following trends 
in the figure 
 unweighted opinion pools and performance-weighted 
opinion pools have similar levels of prediction 
accuracy especially for linop 
 for linop increasing the number of experts in 
general increases or keeps the same the level of prediction 
accuracy when there are more than experts the 
prediction accuracy of linop is stable regarding the 
number of experts 
 logop seems more accurate than linop in terms of 
mean absolute error but using all other performance 
measures linop outperforms logop 
 for logop increasing the number of experts increases 
the prediction accuracy at the beginning but the 
curves including the points with all experts for mean 
quadratic score and mean logarithmic score have slight 
bell-shapes which represent a decrease in prediction 
accuracy when the number of experts is very large 
the curves for mean absolute error on the other hand 
show a consistent increase of accuracy 
the first and second trend above imply that when using 
linop the simplest way which has good prediction 
accuracy is to average the opinions of all experts weighting 
does not seem to improve performance selecting experts 
according to past performance also does not help it is a 
very interesting observation that even if many participants 
of the probabilityfootball contest do not provide accurate 
individual predictions they have negative quadratic scores 
in the contest including their opinions into the opinion pool 
still increases the prediction accuracy one explanation of 
this phenomena could be that biases of individual judgment 
can offset with each other when opinions are diverse which 
makes the pooled prediction more accurate 
the third trend presents a controversy the relative 
prediction accuracy of logop and linop flips when using 
different accuracy measures to investigate this disagreement 
we plot the absolute error of log-all-u and lin-all-u for each 
game in figure when the absolute error of an opinion 
 
 all 
 
 
 
 
 
 
 
 
 
 
 
number of expert opinions 
meanabsoluteerror 
unweighted linear 
weighted linear 
unweighted logarithmic 
weighted logarithmic 
lin−all−u 
lin−all−w 
log−all−u 
log−all−w 
 a mean absolute error 
 all 
 
 
 
 
 
 
 
 
 
 
 
number of expert opinions 
meanquadraticscore 
unweighted linear 
weighted linear 
unweighted logarithmic 
weighted logarithmic 
lin−all−u 
lin−all−w 
log−all−u 
log−all−w 
 all 
− 
− 
− 
− 
− 
− 
− 
− 
− 
− 
number of expert opinions 
meanlogarithmicscore 
unweighted linear 
weighted linear 
unweighted logarithmic 
weighted logarithmic 
lin−all−u 
lin−all−w 
log−all−u 
log−all−w 
 b mean quadratic score c mean logarithmic score 
figure prediction accuracy of opinion pools 
pool for a game is less than it means that the team 
favored by the opinion pool wins the game if it is greater than 
 the underdog wins compared with lin-all-u log-all-u 
has lower absolute error when it is less than and greater 
absoluter error when it is greater than which indicates 
that predictions of log-all-u are bolder more close to or 
than those of lin-all-u this is due to the nature of linear 
and logarithmic aggregating functions because quadratic 
score and logarithmic score penalize bold predictions that 
are wrong logop is less accurate when measured in these 
terms 
similar reasoning accounts for the fourth trend when 
there are more than experts increasing number of 
experts used in logop improves the prediction accuracy 
measured by absolute error but worsens the accuracy measured 
by the other two metrics examining expert opinions we 
find that participants who rank lower are more frequent in 
offering extreme predictions or than those ranking high 
in the list when we increase the number of experts in an 
opinion pool we are incorporating more extreme predictions 
into it the resulting logop is bolder and hence has lower 
mean quadratic score and mean logarithmic score 
 comparison of information markets and 
opinion pools 
through the first screening of various opinion pools we 
select lin-all-u log-all-u log-all-w and log- -u to 
compare with predictions from information markets lin-all-u 
as shown in figure can represent what linop can achieve 
however the performance of logop is not consistent when 
evaluated using different metrics log-all-u and log-all-w 
offer either the best or the worst predictions log- -u the 
logop with the top performing experts provides more 
stable predictions we use all of the three to stand for the 
performance of logop in our later comparison 
if a prediction of the probability that a team will win a 
game either from an opinion pool or an information 
market is higher than we say that the team is the predicted 
favorite for the game table presents the number and 
percentage of games that predicted favorites actually win out 
of a total of games all four opinion pools correctly 
predict a similar number and percentage of games as nf and 
ts since nf ts and the four opinion pools form their 
predictions using information available at noon of the game 
 
table number and percentage of games that predicted favorites win 
nf ts lin-all-u log-all-u log-all-w log- -u 
number 
percentage 
table mean of prediction accuracy measures 
absolute error quadratic score logarithmic score 
nf 
 - 
 
ts 
 - 
 
lin-all-u 
 - 
 
log-all-u 
 - 
 
log-all-w 
 - 
 
log- -u 
 - 
 
 numbers in parentheses are standard errors 
 best value for each metric is shown in bold 
 
 
 
 
 
 
 
 
 
 
 
 
absolute error of lin−all−u 
absoluteerroroflog−all−u 
 degree line 
absolute error 
figure absolute error lin-all-u vs log-all-u 
day information markets and opinion pools have 
comparable potential at the same time point 
we then take a closer look at prediction accuracy of 
information markets and opinion pools using the three 
performance measures table displays mean values of these 
measures over games numbers in parentheses are 
standard errors which estimate the standard deviation of the 
mean to take into consideration of skewness of 
distributions we also report median values of accuracy measures in 
table judged by the mean values of accuracy measures 
in table all methods have similar accuracy levels with 
nf and ts slightly better than the opinion pools however 
the median values of accuracy measures indicate that 
logall-u and log-all-w opinion pools are more accurate than 
all other predictions 
we employ the randomization test to study whether 
the differences in prediction accuracy presented in table 
and table are statistically significant the basic idea of 
randomization test is that by randomly swapping 
predictions of two methods numerous times an empirical 
distribution for the difference of prediction accuracy can be 
constructed using this empirical distribution we are then able 
to evaluate that at what confidence level the observed 
difference reflects a real difference for example the mean 
absolute error of nf is higher than that of log-all-u by 
 as shown in table to test whether this 
difference is statistically significant we shuﬄe predictions from 
two methods randomly label half of predictions as nf and 
the other half as log-all-u and compute the difference of 
mean absolute error of the newly formed nf and log-all-u 
data the above procedure is repeated times the 
 differences of mean absolute error results in an 
empirical distribution of the difference comparing our observed 
difference with this distribution we find that the 
observed difference is greater than of the empirical 
differences this leads us to conclude that the difference of 
mean absolute error between nf and log-all-u is not 
statistically significant if we choose the level of significance to 
be 
table and table are results of randomization test for 
mean and median differences respectively each cell of the 
table is for two different prediction methods represented by 
name of the row and name of the column the first lines 
of table cells are results for absolute error the second and 
third lines are dedicated to quadratic score and logarithmic 
score respectively we can see that in terms of mean values 
of accuracy measures the differences of all methods are not 
statistically significant to any reasonable degree when it 
 
table median of prediction accuracy measures 
absolute error quadratic score logarithmic score 
nf - 
ts - 
lin-all-u - 
log-all-u - 
log-all-w - 
log- -u - 
 best value for each metric is shown in bold 
table statistical confidence of mean differences in prediction accuracy 
ts lin-all-u log-all-u log-all-w log- -u 
nf 
 
 
 
ts 
 
 
 
lin-all-u 
 
 
 
log-all-u 
 
 
 
log-all-w 
 
 
 
 in each table cell row accounts for absolute error row for quadratic score 
and row for logarithmic score 
comes to median values of prediction accuracy log-all-u 
outperforms lin-all-u at a high confidence level 
these results indicate that differences in prediction 
accuracy between information markets and opinion pools are not 
statistically significant this may seem to contradict the 
result of servan-schreiber et al in which newsfutures s 
information markets have been shown to provide 
statistically significantly more accurate predictions than the 
 unweighted average of all probabilityfootball opinions the 
discrepancy emerges in dealing with missing data not all 
 registered probabilityfootball participants offer 
probability assessments for each game when a participant does 
not provide a probability assessment for a game the 
contest considers their prediction as this makes sense in 
the context of the contest since always yields quadratic 
score the probabilityfootball average reported on the 
contest website and used by servan-schreiber et al includes 
these estimates instead we remove participants from 
games that they do not provide assessments pooling only 
the available opinions together our treatment increases the 
prediction accuracy of lin-all-u significantly 
 conclusions 
with the fast growth of the internet information markets 
have recently emerged as an alternative tool for predicting 
future events previous research has shown that information 
markets give as accurate or more accurate predictions than 
individual experts and polls however information 
markets as an adaptive mechanism to aggregate different 
opinions of market participants have not been calibrated against 
many belief aggregation methods in this paper we compare 
prediction accuracy of information markets with linear and 
logarithmic opinion pools linop and logop using 
predictions from two markets and individuals regarding the 
outcomes of american football games during the 
nfl season in screening for representative opinion pools to 
compare with information markets we investigate the effect 
of weights and number of experts on prediction accuracy 
our results on both the comparison of information markets 
and opinion pools and the relative performance of different 
opinion pools are summarized as below 
 at the same time point ahead of the events 
information markets offer as accurate predictions as our 
selected opinion pools 
we have selected four opinion pools to represent the 
prediction accuracy level that linop and logop can 
achieve with all four performance metrics our two 
information markets obtain similar prediction accuracy 
as the four opinion pools 
 
table statistical confidence of median differences in prediction accuracy 
ts lin-all-u log-all-u log-all-w log- -u 
nf 
 
 
 
ts 
 
 
 
lin-all-u 
 
 
 
log-all-u 
 
 
 
log-all-w 
 
 
 
 in each table cell row accounts for absolute error row for quadratic score 
and row for logarithmic score 
 confidence above is shown in bold 
 the arithmetic average of all opinions lin-all-u is a 
simple robust and efficient opinion pool 
simply averaging across all experts seems resulting in 
better predictions than individual opinions and 
opinion pools with a few experts it is quite robust in the 
sense that even if the included individual predictions 
are less accurate averaging over all opinions still gives 
better or equally good predictions 
 weighting expert opinions according to past 
performance does not seem to significantly improve 
prediction accuracy of either linop or logop 
comparing performance-weighted opinion pools with 
equally weighted opinion pools we do not observe much 
difference in terms of prediction accuracy since we 
only use one performance-weighting method 
calculating the weights according to past accumulated quadratic 
score that participants earned this might due to the 
weighting method we chose 
 logop yields bolder predictions than linop 
logop yields predictions that are closer to the 
extremes or 
an information markets is a self-organizing mechanism 
for aggregating information and making predictions 
compared with opinion pools it is less constrained by space 
and time and can eliminate the efforts to identify experts 
and decide belief aggregation methods but the advantages 
do not compromise their prediction accuracy to any extent 
on the contrary information markets can provide real-time 
predictions which are hardly achievable through resorting 
to experts in the future we are interested in further 
exploring 
 performance comparison of information markets with 
other opinion pools and mathematical aggregation 
procedures 
in this paper we only compare information markets 
with two simple opinion pools linear and logarithmic 
it will be meaningful to investigate their relative 
prediction accuracy with other belief aggregation methods 
such as bayesian approaches there are also a number 
of theoretical expert algorithms with proven worst-case 
performance bounds whose average-case or 
practical performance would be instructive to investigate 
 whether defining expertise more narrowly can improve 
predictions of opinion pools 
in our analysis we broadly treat participants of the 
probabilityfootball contest as experts in all games if 
we define expertise more narrowly selecting experts 
in certain football teams to predict games involving 
these teams will the predictions of opinion pools be 
more accurate 
 the possibility of combining information markets with 
other forecasting methods to achieve better prediction 
accuracy 
chen fine and huberman use an information 
market to determine the risk attitude of participants 
and then perform a nonlinear aggregation of their 
predictions based on their risk attitudes the nonlinear 
aggregation mechanism is shown to outperform both 
the market and the best individual participants it 
is worthy of more attention whether information 
markets as an alternative forecasting method can be used 
together with other methods to improve our 
predictions 
 acknowledgments 
we thank brian galebach the owner and operator of the 
probabilitysports and probabilityfootball websites for 
providing us with such unique and valuable data we thank 
varsha dani lance fortnow omid madani sumit 
sang 
hai and the anonymous reviewers for useful insights and 
pointers 
the authors acknowledge the support of the penn state 
ebusiness research center 
 references 
 http us newsfutures com 
 http www biz uiowa edu iem 
 http www hsx com 
 http www ideosphere com fx 
 http www probabilityfootball com 
 http www probabilitysports com 
 http www tradesports com 
 a h ashton and r h ashton aggregating 
subjective forecasts some empirical results 
management science - 
 r p batchelor and p dua forecaster diversity and 
the benefits of combining forecasts management 
science - 
 n cesa-bianchi y freund d haussler d p 
helmbold r e schapire and m k warmuth how 
to use expert advice journal of the acm 
 - 
 k chen l fine and b huberman predicting the 
future information system frontier - 
 r t clemen and r l winkler combining 
probability distributions from experts in risk analysis 
risk analysis - 
 r m cook experts in uncertainty opinion and 
subjective probability in science oxford university 
press new york 
 a l delbecq a h van de ven and d h 
gustafson group techniques for program planners 
a guide to nominal group and delphi processes 
scott foresman and company glenview il 
 e f fama efficient capital market a review of 
theory and empirical work journal of finance 
 - 
 r forsythe and f lundholm information 
aggregation in an experimental market econometrica 
 - 
 r forsythe f nelson g r neumann and 
j wright forecasting elections a market alternative 
to polls in t r palfrey editor contemporary 
laboratory experiments in political economy pages 
 - university of michigan press ann arbor mi 
 
 r forsythe f nelson g r neumann and 
j wright anatomy of an experimental political stock 
market american economic review - 
 
 r forsythe t a rietz and t w ross wishes 
expectations and actions a survey on price formation 
in election stock markets journal of economic 
behavior and organization - 
 s french group consensus probability distributions 
a critical survey bayesian statistics - 
 c genest a conflict between two axioms for 
combining subjective distributions journal of the 
royal statistical society - 
 c genest pooling operators with the marginalization 
property canadian journal of statistics 
 - 
 c genest k j mcconway and m j schervish 
characterization of externally bayesian pooling 
operators annals of statistics - 
 c genest and j v zidek combining probability 
distributions a critique and an annotated 
bibliography statistical science - 
 s j grossman an introduction to the theory of 
rational expectations under asymmetric information 
review of economic studies - 
 f a hayek the use of knowledge in society 
american economic review - 
 j c jackwerth and m rubinstein recovering 
probability distribution from options prices journal 
of finance - 
 h a linstone and m turoff the delphi method 
techniques and applications addison-wesley 
reading ma 
 p a morris decision analysis expert use 
management science - 
 p a morris combining expert judgments a 
bayesian approach management science 
 - 
 p a morris an axiomatic approach to expert 
resolution management science - 
 e w noreen computer-intensive methods for 
testing hypotheses an introduction wiley and sons 
inc new york 
 d m pennock s lawrence c l giles and f a 
nielsen the real power of artificial markets science 
 - february 
 d m pennock s lawrence f a nielsen and c l 
giles extracting collective probabilistic forecasts from 
web games in proceedings of the th acm sigkdd 
international conference on knowledge discovery and 
data mining pages - san francisco ca 
 c plott and s sunder rational expectations and the 
aggregation of diverse information in laboratory 
security markets econometrica - 
 e servan-schreiber j wolfers d m pennock and 
b galebach prediction markets does money 
matter electronic markets - 
 m spann and b skiera internet-based virtual stock 
markets for business forecasting management science 
 - 
 m west bayesian aggregation journal of the royal 
statistical society series a general - 
 
 r l winkler the consensus of subjective probability 
distributions management science b -b 
 
 j wolfers and e zitzewitz prediction markets 
journal of economic perspectives - 
 
 
bid expressiveness and clearing algorithms in 
multiattribute double auctions 
yagil engel michael p wellman and kevin m lochner 
university of michigan computer science engineering 
 hayward st ann arbor mi - usa 
{yagil wellman klochner} umich edu 
abstract 
we investigate the space of two-sided multiattribute auctions 
focusing on the relationship between constraints on the offers traders 
can express through bids and the resulting computational problem 
of determining an optimal set of trades we develop a formal 
semantic framework for characterizing expressible offers and show 
conditions under which the allocation problem can be separated 
into first identifying optimal pairwise trades and subsequently 
optimizing combinations of those trades we analyze the bilateral 
matching problem while taking into consideration relevant results 
from multiattribute utility theory network flow models we 
develop for computing global allocations facilitate classification of 
the problem space by computational complexity and provide 
guidance for developing solution algorithms experimental trials help 
distinguish tractable problem classes for proposed solution 
techniques 
categories and subject descriptors f theory of 
computation analysis of algorithms and problem complexity j 
 computer applications social and behavioral 
sciences-economics 
general terms algorithms economics 
 background 
a multiattribute auction is a market-based mechanism where 
goods are described by vectors of features or attributes 
 such mechanisms provide traders with the ability to negotiate 
over a multidimensional space of potential deals delaying 
commitment to specific configurations until the most promising candidates 
are identified for example in a multiattribute auction for 
computers the good may be defined by attributes such as processor speed 
memory and hard disk capacity agents have varying preferences 
 or costs associated with the possible configurations for example 
a buyer may be willing to purchase a computer with a ghz 
processor mb of memory and a gb hard disk for a price no 
greater than or the same computer with gb of memory for 
a price no greater than 
existing research in multiattribute auctions has focused 
primarily on one-sided mechanisms which automate the process whereby 
a single agent negotiates with multiple potential trading partners 
 models of procurement typically assume 
the buyer has a value function v ranging over the possible 
configurations x and that each seller i can similarly be associated 
with a cost function ci over this domain the role of the auction is 
to elicit these functions possibly approximate or partial versions 
and identify the surplus-maximizing deal in this case such an 
outcome would be arg maxi x v x − ci x this problem can be 
translated into the more familiar auction for a single good without 
attributes by computing a score for each attribute vector based on 
the seller valuation function and have buyers bid scores analogs 
of the classic first- and second-price auctions correspond to 
firstand second-score auctions 
in the absence of a published buyer scoring function agents on 
both sides may provide partial specifications of the deals they are 
willing to engage research on such auctions has for example 
produced iterative mechanisms for eliciting cost functions 
incrementally other efforts focus on the optimization problem facing 
the bid taker for example considering side constraints on the 
combination of trades comprising an overall deal side constraints 
have also been analyzed in the context of combinatorial auctions 
 
our emphasis is on two-sided multiattribute auctions where 
multiple buyers and sellers submit bids and the objective is to construct 
a set of deals maximizing overall surplus previous research on 
such auctions includes works by fink et al and gong 
both of which consider a matching problem for continuous double 
auctions cdas where deals are struck whenever a pair of 
compatible bids is identified 
in a call market in contrast bids accumulate until designated 
times e g on a periodic or scheduled basis at which the auction 
clears by determining a comprehensive match over the entire set 
of bids because the optimization is performed over an aggregated 
scope call markets often enjoy liquidity and efficiency advantages 
over cdas 
clearing a multiattribute cda is much like clearing a one-sided 
multiattribute auction because nothing happens between bids the 
problem is to match a given new bid say an offer to buy with the 
existing bids on the other sell side multiattribute call markets are 
potentially much more complex constructing an optimal overall 
matching may require consideration of many different 
combina 
in the interim between clears call markets may also disseminate 
price quotes providing summary information about the state of the 
auction such price quotes are often computed based on 
hypothetical clears and so the clearing algorithm may be invoked more 
frequently than actual market clearing operations 
 
tions of trades among the various potential trading-partner 
pairings the problem can be complicated by restrictions on overall 
assignments as expressed in side constraints 
the goal of the present work is to develop a general framework 
for multiattribute call markets to enable investigation of design 
issues and possibilities in particular we use the framework to 
explore tradeoffs between expressive power of agent bids and 
computational properties of auction clearing we conduct our 
exploration independent of any consideration of strategic issues bearing 
on mechanism design as with analogous studies of combinatorial 
auctions we intend that tradeoffs quantified in this work can 
be combined with incentive factors within a comprehensive overall 
approach to multiattribute auction design 
we provide the formal semantics of multiattribute offers in our 
framework in the next section we abstract where appropriate 
from the specific language used to express offers characterizing 
expressiveness semantically in terms of what deals may be offered 
this enables us to identify some general conditions under which 
the problem of multilateral matching can be decomposed into 
bilateral matching problems we then develop a family of network 
flow problems that capture corresponding classes of multiattribute 
call market optimizations experimental trials provide preliminary 
confirmation that the network formulations provide useful structure 
for implementing clearing algorithms 
 multiattribute offers 
 basic definitions 
the distinguishing feature of a multiattribute auction is that the 
goods are defined by vectors of attributes x x xm 
xj ∈ xj a configuration is a particular attribute vector x ∈ 
x 
qm 
j xj the outcome of the auction is a set of bilateral 
trades trade t takes the form t x q b s π signifying that 
agent b buys q units of configuration x from seller s for 
payment π for convenience we use the notation xt to denote the 
configuration associated with trade t and similarly for other 
elements of t for a set of trades t we denote by ti that subset of t 
involving agent i i e b i or s i let t denote the set of all 
possible trades 
a bid expresses an agent s willingness to participate in trades 
we specify the semantics of a bid in terms of offer sets let ot 
i ⊆ 
ti denote agent i s trade offer set intuitively this represents the 
trades in which i is willing to participate however since the 
outcome of the auction is a set of trades several of which may involve 
agent i we must in general consider willingness to engage in trade 
combinations accordingly we introduce the combination offer set 
of agent i oc 
i ⊆ ti 
 specifying offer sets 
a fully expressive bid language would allow specification of 
arbitrary combination offer sets we instead consider a more limited 
class which while restrictive still captures most forms of 
multiattribute bidding proposed in the literature our bids directly specify 
part of the agent s trade offer set and include further directives 
controlling how this can be extended to the full trade and combination 
offer sets 
for example one way to specify a trade buy offer set would 
be to describe a set of configurations and quantities along with the 
maximal payment one would exchange for each x q specified 
this description could be by enumeration or any available means 
of defining such a mapping 
an explicit set of trades in the offer set generally entails inclusion 
of many more implicit trades we assume payment monotonicity 
which means that agents always prefer more money that is for 
π π 
 x q i s π ∈ ot 
i ⇒ x q i s π ∈ ot 
i 
 x q b i π ∈ ot 
i ⇒ x q b i π ∈ ot 
i 
we also assume free disposal which dictates that for all i q 
q 
 x q i s π ∈ ot 
i ⇒ x q i s π ∈ ot 
i 
 x q b i π ∈ ot 
i ⇒ x q b i π ∈ ot 
i 
note that the conditions for agents in the role of buyers and sellers 
are analogous henceforth for expository simplicity we present 
all definitions with respect to buyers only leaving the definition 
for sellers as understood allowing agents bids to comprise offers 
from both buyer and seller perspectives is also straightforward 
an assertion that offers are divisible entails further implicit 
members in the trade offer set 
definition divisible offer agent i s offer is 
divisible down to q iff 
∀q q q x q i s π ∈ ot 
i ⇒ x q i s q 
q 
π ∈ ot 
i 
we employ the shorthand divisible to mean divisible down to 
the definition above specifies arbitrary divisibility it would 
likewise be possible to define divisibility with respect to integers or to 
any given finite granularity note that when offers are divisible it 
suffices to specify one offer corresponding to the maximal quantity 
one is willing to trade for any given configuration trading partner 
and per-unit payment called the price 
at the extreme of indivisibility are all-or-none offers 
definition aon offer agent i s offer is all-or-none 
 aon iff 
 x q i s π ∈ ot 
i ∧ x q i s π ∈ ot 
i ⇒ q q ∨ π π 
in many cases the agent will be indifferent with respect to 
different trading partners in that event it may omit the partner element 
from trades directly specified in its offer set and simply assert that 
its offer is anonymous 
definition anonymity agent i s offer is anonymous 
iff ∀s s b b x q i s π ∈ ot 
i ⇔ x q i s π ∈ ot 
i ∧ 
 x q b i π ∈ ot 
i ⇔ x q b i π ∈ ot 
i 
because omitting trading partner qualifications simplifies the 
exposition we generally assume in the following that all offers are 
anonymous unless explicitly specified otherwise extending to the 
non-anonymous case is conceptually straightforward we employ 
the wild-card symbol ∗ in place of an agent identifier to indicate 
that any agent is acceptable 
to specify a trade offer set a bidder directly specifies a set of 
willing trades along with any regularity conditions e g 
divisibility anonymity that implicitly extend the set the full trade offer 
set is then defined by the closure of this direct set with respect to 
payment monotonicity free disposal and any applicable 
divisibility assumptions 
we next consider the specification of combination offer sets 
without loss of generality we restrict each trade set t ∈ oc 
i to 
include at most one trade for any combination of configuration and 
trading partner multiple such trades are equivalent to one net trade 
aggregating the quantities and payments the key question is to 
what extent the agent is willing to aggregate deals across 
configurations or trading partners one possibility is disallowing any 
aggregation 
 
definition no aggregation the no-aggregation 
combinations are given by ona 
i {∅} ∪ {{t} t ∈ ot 
i } agent i s 
offer exhibits non-aggregation iff oc 
i ona 
i 
we require in general that oc 
i ⊇ ona 
i 
a more flexible policy is to allow aggregation across trading 
partners keeping configuration constant 
definition partner aggregation suppose a 
particular trade is offered in the same context set of additional trades 
t with two different sellers s and s that is 
{ x q i s π } ∪ t ∈ oc 
i ∧ { x q i s π } ∪ t ∈ oc 
i 
agent i s offer allows seller aggregation iff in all such cases 
{ x q i s π x q − q i s π − π } ∪ t ∈ oc 
i 
in other words we may create new trade offer combinations by 
splitting the common trade quantity and payment not necessarily 
proportionately between the two sellers 
in some cases it might be reasonable to form combinations by 
aggregating different configurations 
definition configuration aggregation suppose 
agent i offers in the same context the same quantity of two not 
necessarily different configurations x and x that is 
{ x q i ∗ π } ∪ t ∈ oc 
i ∧ { x q i ∗ π } ∪ t ∈ oc 
i 
agent i s offer allows configuration aggregation iff in all such cases 
 and analogously when it is a seller 
{ x q i ∗ 
q 
q 
π x q − q i ∗ 
q − q 
q 
π } ∪ t ∈ oc 
i 
note that combination offer sets can accommodate offerings of 
configuration bundles however classes of bundles formed by 
partner or configuration aggregation are highly regular covering only a 
specific type of bundle formed by splitting a desired quantity across 
configurations this is quite restrictive compared to the general 
combinatorial case 
 willingness to pay 
an agent s offer trade set implicitly defines the agent s 
willingness to pay for any given configuration and quantity we assume 
anonymity to avoid conditioning our definitions on trading partner 
definition willingness to pay agent i s willingness 
to pay for quantity q of configuration x is given by 
ˆub 
i x q max π s t x q i ∗ π ∈ ot 
i 
we use the symbol ˆu to recognize that willingness to pay can be 
viewed as a proxy for the agent s utility function measured in 
monetary units the superscript b distinguishes the buyer s 
willingnessto-pay function from a seller s willingness to accept ˆus 
i x q 
defined as the minimum payment seller i will accept for q units of 
configuration x we omit the superscript where the distinction is 
inessential or clear from context 
definition trade quantity bounds agent i s 
minimum trade quantity for configuration x is given by 
qi x min q s t ∃π x q i ∗ π ∈ ot 
i 
the agent s maximum trade quantity for x is 
¯qi x max q s t 
∃π x q i ∗ π ∈ ot 
i ∧ ¬∃q q x q i ∗ π ∈ ot 
i 
when the agent has no offers involving x we take qi x ¯qi x 
 
it is useful to define a special case where all configurations are 
offered in the same quantity range 
definition configuration parity agent i s offers 
exhibit configuration parity iff 
qi x ∧ qi x ⇒ qi x qi x ∧ ¯qi x ¯qi x 
under configuration parity we drop the arguments from trade 
quantity bounds yielding the constants ¯q and q which apply to all offers 
definition linear pricing agent i s offers exhibit 
linear pricing iff for all qi x ≤ q ≤ ¯qi x 
ˆui x q 
q 
¯qi x 
ˆui x ¯qi x 
note that linear pricing assumes divisibility down to qi x given 
linear pricing we can define the unit willingness to pay ˆui x 
ˆui x ¯qi x ¯qi x and take ˆui x q qˆui x for all qi x ≤ 
q ≤ ¯qi x 
in general an agent s willingness to pay may depend on a context 
of other trades the agent is engaging in 
definition willingness to pay in context agent 
i s willingness to pay for quantity q of configuration x in the 
context of other trades t is given by 
ˆub 
i x q t max π s t { x q i s π } ∪ ti ∈ oc 
i 
lemma if oc 
i is either non aggregating or exhibits linear 
pricing then 
ˆub 
i x q t ˆub 
i x q 
 multiattribute allocation 
definition trade surplus the surplus of trade t 
 x q b s π is given by 
σ t ˆub 
b x q − ˆus 
s x q 
note that the trade surplus does not depend on the payment which 
is simply a transfer from buyer to seller 
definition trade unit surplus the unit surplus 
of trade t x q b s π is given by σ 
 t σ t q 
under linear pricing we can equivalently write σ 
 t ˆub 
b x − 
ˆus 
s x 
definition surplus of a trade in context the 
surplus of trade t x q b s π in the context of other trades t 
σ t t is given by 
ˆub 
b x q t − ˆus 
s x q t 
definition gmap the global multiattribute 
allocation problem gmap is to find the set of acceptable trades 
maximizing total surplus 
max 
t ∈ t 
x 
t∈t 
σ t t \ {t} s t ∀i ti ∈ oc 
i 
definition mmp the multiattribute matching 
problem mmp is to find a best trade for a given pair of traders 
mmp b s arg max 
t∈ot 
b 
∩ot 
s 
σ t 
if ot 
b ∩ ot 
s is empty we say that mmp has no solution 
 
proofs of all the following results are provided in an extended 
version of this paper available from the authors 
theorem suppose all agents offers exhibit no aggregation 
 definition then the solution to gmap consists of a set of 
trades each of which is a solution to mmp for its specified pair 
of traders 
theorem suppose that each agent s offer set satisfies one 
of the following not necessarily the same sets of conditions 
 no aggregation and configuration parity definitions and 
 divisibility linear pricing and configuration parity 
 definitions and with combination offer set defined as the 
minimal set consistent with configuration aggregation 
 definition 
then the solution to gmap consists of a set of trades each of 
which employs a configuration that solves mmp for its specified 
pair of traders 
let mmpd 
 b s denote a modified version of mmp where ot 
b 
and ot 
s are extended to assume divisibility i e the offer sets are 
taken to be their closures under definition then we can extend 
theorem to allow aggregating agents to maintain aon or 
minquantity offers as follows 
theorem suppose offer sets as in theorem except that 
agents i satisfying configuration aggregation need be divisible only 
down to qi rather than down to then the solution to gmap 
consists of a set of trades each of which employs the same 
configuration as a solution to mmpd 
for its specified pair of traders 
theorem suppose agents b and s exhibit configuration 
parity divisibility and linear pricing and there exists configuration x 
such that ˆub x − ˆus x then t ∈ mmpd 
 b s iff 
xt arg max 
x 
{ˆub x − ˆus x } 
qt min ¯qb ¯qs 
 
the preceding results signify that under certain conditions we 
can divide the global optimization problem into two parts first find 
a bilateral trade that maximizes unit surplus for each pair of traders 
 or total surplus in the non-aggregation case and then use the 
results to find a globally optimal set of trades in the following two 
sections we investigate each of these subproblems 
 utility representation and mmp 
we turn next to consider the problem of finding a best deal 
between pairs of traders the complexity of mmp depends pivotally 
on the representation by bids of offer sets an issue we have 
postponed to this point 
note that issues of utility representation and mmp apply to a 
broad class of multiattribute mechanisms beyond the multiattribute 
call markets we emphasize for example the complexity results 
contained in this section apply equally to the bidding problem faced 
by sellers in reverse auctions given a published buyer scoring 
function 
the simplest representation of an offer set is a direct 
enumeration of configurations and associated quantities and payments this 
approach treats the configurations as atomic entities making no use 
 
that is for such an agent i oc 
i is the closure under configuration 
aggregation of ona 
i 
of attribute structure a common and inexpensive enhancement is 
to enable a trader to express sets of configurations by specifying 
subsets of the domains of component attributes associating a 
single quantity and payment with a set of configurations expresses 
indifference among them hence we refer to such a set as an 
indifference range 
indifference ranges include the case of attributes 
with a natural ordering in which a bid specifies a minimum or 
maximum acceptable attribute level the use of indifference ranges 
can be convenient for mmp the compatibility of two indifference 
ranges is simply found by testing set intersection for each attribute 
as demonstrated by the decision-tree algorithm of fink et al 
alternatively bidders may specify willingness-to-pay functions 
ˆu in terms of compact functional forms enumeration based 
representations even when enhanced with indifference ranges are 
ultimately limited by the exponential size of attribute space 
functional forms may avoid this explosion but only if ˆu reflects 
structure among the attributes moreover even given a compact 
specification of ˆu we gain computational benefits only if we can 
perform the matching without expanding the ˆu values of an 
exponential number of configuration points 
 additive forms 
one particularly useful multiattribute representation is known as 
the additive scoring function though this form is widely used in 
practice and in the academic literature it is important to stress the 
assumptions behind it the theory of multiattribute representation 
is best developed in the context where ˆu is interpreted as a 
utility function representing an underlying preference order we 
present the premises of additive utility theory in this section and 
discuss some generalizations in the next 
definition a set of attributes y ⊂ x is preferentially 
independent pi of its complement z x \ y if the conditional 
preference order over y given a fixed level z 
of z is the same 
regardless of the choice of z 
 
in other words the preference order over the projection of x on 
the attributes in y is the same for any instantiation of the attributes 
in z 
definition x {x xm} is mutually preferentially 
independent mpi if any subset of x is preferentially independent 
of its complement 
theorem a preference order over set of attributes 
x has an additive utility function representation 
u x xm 
mx 
i 
ui xi 
iff x is mutually preferential independent 
a utility function over outcomes including money is quasi-linear 
if the function can be represented as a function over non-monetary 
attributes plus payments π interpreting ˆu as a utility function over 
non-monetary attributes is tantamount to assuming quasi-linearity 
even when quasi-linearity is assumed however mpi over 
nonmonetary attributes is not sufficient for the quasi-linear utility 
function to be additive for this we also need that each of the pairs 
 π xi for any attribute xi would be pi of the rest of the attributes 
 
these should not be mistaken with indifference curves which 
express dependency between the attributes indifference curves can 
be expressed by the more elaborate utility representations discussed 
below 
 
this by maut in turn implies that the set of attributes including 
money is mpi and the utility function can be represented as 
u x xm π 
mx 
i 
ui xi π 
given that form a willingness-to-pay function reflecting u can be 
represented additively as 
ˆu x 
mx 
i 
ui xi 
in many cases the additivity assumption provides practically 
crucial simplification of offer set elicitation in addition to 
compactness additivity dramatically simplifies mmp if both sides provide 
additive ˆu representations the globally optimal match reduces to 
finding the optimal match separately for each attribute 
a common scenario in procurement has the buyer define an 
additive scoring function while suppliers submit enumerated offer 
points or indifference ranges this model is still very amenable to 
mmp for each element in a supplier s enumerated set we optimize 
each attribute by finding the point in the supplier s allowable range 
that is most preferred by the buyer 
a special type of scoring more particularly cost function was 
defined by bichler and kalagnanam and called a configurable 
offer this idea is geared towards procurement auctions assuming 
suppliers are usually comfortable with expressing their preferences 
in terms of cost that is quasi-linear in every attribute they can 
specify a price for a base offer and additional cost for every change in 
a specific attribute level this model is essentially a pricing out 
approach for this case mmp can still be optimized on a 
per-attribute basis a similar idea has been applied to one-sided 
iterative mechanisms in which sellers refine prices on a 
perattribute basis at each iteration 
 multiattribute utility theory 
under mpi the tradeoffs between the attributes in each subset 
cannot be affected by the value of other attributes for example 
when buying a pc a weaker cpu may increase the importance of 
the ram compared to say the type of keyboard such 
relationships cannot be expressed under an additive model 
multiattribute utility theory maut develops various compact 
representations of utility functions that are based on weaker 
structural assumptions there are several challenges in adapting 
these techniques to multiattribute bidding first as noted above 
the theory is developed for utility functions which may behave 
differently from willingness-to-pay functions second computational 
efficiency of matching has not been an explicit goal of most work 
in the area third adapting such representations to iterative 
mechanisms may be more challenging 
one representation that employs somewhat weaker assumptions 
than additivity yet retains the summation structure is the 
generalized additive ga decomposition 
u x 
jx 
j 
fj xj 
 xj 
∈ xj 
 
where the xj 
are potentially overlapping sets of attributes together 
exhausting the space x 
a key point from our perspective is that the complexity of the 
matching is similar to the complexity of optimizing a single 
function since the sum function is in the form as well recent work 
by gonzales and perny provides an elicitation process for ga 
decomposable preferences under certainty as well as an 
optimization algorithm for the ga decomposed function the complexity of 
exact optimization is exponential in the induced width of the graph 
however to become operational for multiattribute bidding this 
decomposition must be detectable and verifiable by statements over 
preferences with respect to price outcomes we are exploring this 
topic in ongoing work 
 solving gmap under allocation 
constraints 
theorems and establish conditions under which gmap 
solutions must comprise elements from constituent mmp solutions 
in sections and we show how to compute these gmap 
solutions given the mmp solutions under these conditions in these 
settings traders that aggregate partners also aggregate 
configurations hence we refer to them simply as aggregating or 
nonaggregating section suggests a means to relax the linear 
pricing restriction employed in these constructions section 
provides strategies for allowing traders to aggregate partners and 
restrict configuration aggregation at the same time 
 notation and graphical representation 
our clearing algorithms are based on network flow formulations 
of the underlying optimization problem the network model is 
based on a bipartite graph in which nodes on the left side represent 
buyers and nodes on the right represent sellers we denote the sets 
of buyers and sellers by b and s respectively 
we define two graph families one for the case of non-aggregating 
traders called single-unit and the other for the case of 
aggregating traders called multi-unit 
for both types a single directed 
arc is placed from a buyer i ∈ b to a seller j ∈ s if and only if 
mmp i j is nonempty we denote by t i the set of potential 
trading partners of trader i i e the nodes connected to buyer or 
seller i in the bipartite graph 
in the single-unit case we define the weight of an arc i j as 
wij σ mmp i j note that free disposal lets a buy offer 
receive a larger quantity than desired and similarly for sell offers 
for the multi-unit case the weights are wij σ 
 mmp i j 
and we associate the quantity ¯qi with the node for trader i we also 
use the notation qij for the mathematical formulations to denote 
partial fulfillment of qt for t mmp i j 
 handling indivisibility and aggregation 
constraints 
under the restrictions of theorems or and when the 
solution to mmp is given gmap exhibits strong similarity to the 
problem of clearing double auctions with assignment constraints 
 a match in our bipartite representation corresponds to a 
potential trade in which assignment constraints are satisfied network 
flow formulations have been shown to model this problem under 
the assumption of indivisibility and aggregation for all traders the 
novelty in this part of our work is the use of generalized network 
flow formulations for more complex cases where aggregation and 
divisibility may be controlled by traders 
initially we examine the simple case of no aggregation 
 theorem observe that the optimal allocation is simply the solution 
to the well known weighted assignment problem on the 
singleunit bipartite graph described above the set of matches that 
maximizes the total weight of arcs corresponds to the set of trades that 
maximizes total surplus note that any form of in divisibility can 
 
in the next section we introduce a hybrid form of graph 
accommodating mixes of the two trader categories 
 
also be accommodated in this model via the constituent mmp 
subproblems 
the next formulation solves the case in which all traders fall 
under case of theorem -that is all traders are aggregating and 
divisible and exhibit linear pricing this case can be represented 
using the following linear program corresponding to our multi-unit 
graph 
max 
x 
i∈b j∈s 
wij qij 
s t 
x 
i∈t j 
qij ≤ ¯qj j ∈ s 
x 
j∈t i 
qij ≤ ¯qi i ∈ b 
qij ≥ j ∈ s i ∈ b 
recall that the qij variables in the solution represent the number 
of units that buyer i procures from seller j this formulation is 
known as the network transportation problem with inequality 
constraints for which efficient algorithms are available it is a 
well known property of the transportation problem and flow 
problems on pure networks in general that given integer input values 
the optimal solution is guaranteed to be integer as well figure 
demonstrates the transformation of a set of bids to a transportation 
problem instance 
figure multi-unit matching with two boolean attributes 
 a bids with offers to buy in the left column and offers to 
sell at right q p indicates an offer to trade q units at price p 
per unit configurations are described in terms of constraints 
on attribute values b corresponding multi-unit assignment 
model w represents arc weights unit surplus s represents 
source exogenous flow and t represents sink quantity 
the problem becomes significantly harder when aggregation is 
given as an option to bidders requiring various enhancements to 
the basic multi-unit bipartite graph described above in general 
we consider traders that are either aggregating or not with either 
divisible or aon offers 
initially we examine a special case which at the same time 
demonstrates the hardness of the problem but still carries computational 
advantages we designate one side e g buyers as restrictive aon 
and non-aggregating and the other side sellers as unrestrictive 
 divisible and aggregating this problem can be represented using 
the following integer programming formulation 
max 
x 
i∈b j∈s 
wij qij 
s t 
x 
i∈t j 
¯qiqij ≤ ¯qj j ∈ s 
x 
j∈t i 
qij ≤ i ∈ b 
qij ∈ { } j ∈ s i ∈ b 
 
this formulation is a restriction of the generalized assignment 
problem gap although gap is known to be np-hard it can 
be solved relatively efficiently by exact or approximate algorithms 
gap is more general than the formulation above as it allows 
buyside quantities ¯qi above to be different for each potential seller 
that this formulation is np-hard as well even the case of a single 
seller corresponds to the knapsack problem illustrates the drastic 
increase in complexity when traders with different constraints are 
admitted to the same problem instance 
other than the special case above we found no advantage in 
limiting aon constraints when traders may specify aggregation 
constraints therefore the next generalization allows any combination 
of the two boolean constraints that is any trader chooses among 
four bid types 
ni bid aon and not aggregating 
ad bid allows aggregation and divisibility 
ai bid aon allows aggregation quantity can be aggregated across 
configurations as long as it sums to the whole amount 
nd no aggregation divisibility one trade but smaller quantities 
are acceptable 
to formulate an integer programming representation for the 
problem we introduce the following variables boolean variables 
ri and rj indicate whether buyer i and seller j participate in the 
solution used for aon traders another indicator variable yij 
applied to non-aggregating buyer i and seller j is one iff i trades 
with j for aggregating traders yij is not constrained 
max 
x 
i∈b j∈s 
wij qij a 
s t 
x 
j∈t i 
qij ¯qiri i ∈ aib b 
x 
j∈t i 
qij ≤ ¯qiri i ∈ adb c 
x 
i∈t j 
qij ¯qirj j ∈ ais d 
x 
i∈t j 
qij ≤ qj rj j ∈ ads e 
xij ≤ ¯qiyij i ∈ ndb j ∈ t i f 
xij ≤ ¯qj yij j ∈ nis i ∈ t j g 
x 
j∈t i 
yij ≤ ri i ∈ nib ∪ ndb h 
x 
i∈t j 
yij ≤ rj j ∈ nis ∪ nds i 
int qij j 
yij rj ri ∈ { } k 
 
figure generalized network flow model b is a buyer in 
ad b ∈ ni b ∈ ai b ∈ nd v is a seller in nd 
v ∈ ai v ∈ ad the g values represent arc gains 
problem has additional structure as a generalized min-cost 
flow problem with integral flow 
a generalized flow network is 
a network in which each arc may have a gain factor in addition 
to the pure network parameters which are flow limits and costs 
flow in an arc is then multiplied by its gain factor so that the flow 
that enters the end node of an arc equals the flow that entered from 
its start node multiplied by the gain factor of the arc the network 
model can in turn be translated into an ip formulation that captures 
such structure 
the generalized min-cost flow problem is well-studied and has 
a multitude of efficient algorithms the faster algorithms are 
polynomial in the number of arcs and the logarithm of the maximal 
gain that is performance is not strongly polynomial but is 
polynomial in the size of the input the main benefit of this graphical 
formulation to our matching problem is that it provides a very 
efficient linear relaxation integer programming algorithms such as 
branch-and-bound use solutions to the linear relaxation instance to 
bound the optimal integer solution since network flow algorithms 
are much faster than arbitrary linear programs generalized network 
flow simplex algorithms have been shown to run in practice only 
or times slower than pure network min-cost flow we expect 
a branch-and-bound solver for the matching problem to show 
improved performance when taking advantage of network flow 
modeling 
the network flow formulation is depicted in figure 
nonrestrictive traders are treated as in figure for a non-aggregating 
buyer a single unit from the source will saturate up to one of the 
yij for all j and be multiplied by ¯qi if i ∈ nd the end node of 
yij will function as a sink that may drain up to ¯qi of the entering 
flow for i ∈ ni we use an indicator arc ri on which the 
flow is multiplied by ¯qi trader i trades the full quantity iff ri 
at the seller side the end node of a qij arc functions as a source 
for sellers j ∈ nd in order to let the flow through yij 
arcs be 
or ¯qj the flow is then multiplied by 
¯qj 
so flows enter an end 
node which can drain either or units for sellers j ∈ ni arcs rj 
ensure aon similarly to arcs rj for buyers 
having established this framework we are ready to 
accommo 
constraint j could be omitted yielding computational savings 
if non-integer quantities are allowed here and henceforth we 
assume the harder problem where divisibility is with respect to 
integers 
date more flexible versions of side constraints the first 
generalization is to replace the boolean aon constraint with divisibility down 
to q the minimal quantity in our network flow instance we simply 
need to turn the node of the constrained trader i e g the node b 
in figure to a sink that can drain up to ¯qi − qi units of flow the 
integer program can be also easily changed to accommodate 
this extension 
using gains we can also apply batch size constraints if a trader 
specifies a batch size β we change the gain on the r arcs to β and 
set the available flow of its origin to the maximal number of batches 
¯qi β 
 nonlinear pricing 
a key assumption in handling aggregation up to this point is 
linear pricing which enables us to limit attention to a single unit 
price divisibility without linear pricing allows expression of 
concave willingness-to-pay functions corresponding to convex 
preference relations bidders may often wish to express non-convex offer 
sets for example due to fixed costs or switching costs in 
production settings 
we consider nonlinear pricing in the form of enumerated 
payment schedules-that is defining values ˆu x q for a select set of 
quantities q for the indivisible case these points are distinguished 
in the offer set by satisfying the following 
∃π x q i ∗ π ∈ ot 
i ∧ ¬∃q q x q i ∗ π ∈ ot 
i 
 cf definition which defines the maximum quantity ¯q as the 
largest of these for the divisible case the distinguished quantities 
are those where the unit price changes which can be formalized 
similarly 
to handle nonlinear pricing we augment the network to include 
flow possibilities corresponding to each of the enumerated 
quantities plus additional structure to enforce exclusivity among them 
in other words the network treats the offer for a given quantity as 
in section and embeds this in an xor relation to ensure that 
each trader picks only one of these quantities since for each such 
quantity choice we can apply theorem or the solution we get 
is in fact the solution to gmap 
the network representation of the xor relation which can be 
embedded into the network of figure is depicted in figure for 
a trader i with k xor quantity points we define dummy variables 
zk 
i k k since we consider trades between every pair 
of quantity points we also have qk 
ij 
 k k for buyer 
i ∈ ai with xor points at quantities ¯qk 
i we replace b with the 
following constraints 
x 
j∈t i 
qk 
ij 
 ¯qk 
i zk 
i k k 
kx 
k 
zk 
i ri 
zk 
i ∈ { } k k 
 
 homogeneity constraints 
the model handles constraints over the aggregation of 
quantities from different trading partners when aggregation is allowed 
the formulation permits trades involving arbitrary combinations of 
configurations a homogeneity constraint restricts such 
combinations by requiring that configurations aggregated in an overall 
deal must agree on some or all attributes 
 
figure extending the network flow model to express an xor 
over quantities b has xor points for or units 
in the presence of homogeneity constraints we can no longer 
apply the convenient separation of gmap into mmp plus global 
bipartite optimization as the solution to gmap may include trades 
not part of any mmp solution for example let buyer b specify 
an offer for maximum quantity of various acceptable 
configurations with a homogeneity constraint over the attribute color 
this means b is willing to aggregate deals over different trading 
partners and configurations as long as all are the same color if 
seller s can provide blue units or green units and seller s can 
provide only green units we may prefer that b and s trade on 
green units even if the local surplus of a blue trade is greater 
let {x xh} be attributes that some trader constrains to 
be homogeneous to preserve the network flow framework we 
need to consider for each trader every point in the product domain 
of these homogeneous attributes thus for every assignment ˆx 
to the homogeneous attributes we compute mmp b s under the 
constraint that configurations are consistent with ˆx we apply the 
same approach as in section solve the global optimization 
such that the alternative ˆx assignments for each trader are combined 
under xor semantics thus enforcing homogeneity constraints 
the size of this network is exponential in the number of 
homogeneous attributes since we need a node for each point in the product 
domain of all the homogeneous attributes of each trader 
hence 
this solution method will only be tractable in applications were the 
traders can be limited to a small number of homogeneous attributes 
it is important to note that the graph needs to include a node only 
for each point that potentially matches a point of the other side it 
is therefore possible to make the problem tractable by limiting one 
of the sides to a less expressive bidding language and by that limit 
the set of potential matches for example if sellers submit bounded 
sets of xor points we only need to consider the points in the 
combined set offered by the sellers and the reduction to network flow 
is polynomial regardless of the number of homogeneous attributes 
if such simplifications do not apply it may be preferable to solve 
the global problem directly as a single optimization problem we 
provide the formulation for the special case of divisibility with 
respect to integers and configuration parity let i index buyers j 
sellers and h homogeneous attributes variable xh 
ij 
∈ xh 
represents the value of attribute xh in the trade between buyer i and 
seller j integer variable qij represents the quantity of the trade 
 zero for no trade between i and j 
 
if traders differ on which attributes they express such constraints 
we can limit consideration to the relevant alternatives the 
complexity will still be exponential but in the maximum number of 
homogeneous attributes for any pair of traders 
max 
x 
i∈b j∈s 
 ˆub 
i xij qij − ˆus 
j xij qij 
x 
j∈s 
qij ≤ ¯qi i ∈ b 
x 
i∈b 
qij ≤ ¯qj j ∈ s 
xh 
 j 
 xh 
 j 
 · · · x b j 
j ∈ s h ∈ { h} 
xh 
i 
 xh 
i 
 · · · xi s 
i ∈ b h ∈ { h} 
 
table summarizes the mapping we presented from allocation 
constraints to the complexity of solving gmap configuration 
parity is assumed for all cases but the first 
 experimental results 
we approach the experimental aspect of this work with two 
objectives first we seek a general idea of the sizes and types of 
clearing problems that can be solved under given time constraints we 
also look to compare the performance of a straightforward integer 
program as in with an integer program that is based on the 
network formulations developed here since we used cplex a 
commercial optimization tool the second objective could be achieved 
to the extent that cplex can take advantage of network structure 
present in a model 
we found that in addition to the problem size in terms of number 
of traders the number of aggregating traders plays a crucial role in 
determining complexity when most of the traders are aggregating 
problems of larger sizes can be solved quickly for example our 
ip model solved instances with buyers and sellers where 
 of them are aggregating in less than two minutes when the 
aggregating ratio was reduced to for the same data solution 
time was just under five minutes 
these results motivated us to develop a new network model 
rather than treat non-aggregating traders as a special case the new 
model takes advantage of the single-unit nature of non-aggregating 
trades treating the aggregating traders as a special case this new 
model outperformed our other models on most problem instances 
exceptions being those where aggregating traders constitute a vast 
majority at least 
this new model figure has a single node for each non 
aggregating trader with a single-unit arc designating a match to another 
non-aggregating trader an aggregating trader has a node for each 
potential match connected via y arcs to a mutual source node 
unlike the previous model we allow fractional flow for this case 
representing the traded fraction of the buyer s total quantity 
we tested all three models on random data in the form of 
bipartite graphs encoding mmp solutions in our experiments each 
trader has a maximum quantity uniformly distributed over 
and minimum quantity uniformly distributed from zero to maximal 
quantity each buyer seller pair is selected as matching with 
probability with matches assigned a surplus uniformly distributed 
over whereas the size of the problem is defined by the 
number of traders on each side the problem complexity depends 
on the product b × s the tests depicted in figures - are 
for the worst case b s with each data point averaged over 
six samples in the figures the direct ip is designated sw 
our first network model figure nw and our revised network 
model figure nw 
 
traded quantity remains integer 
 
aggregation hom attr divisibility linear pricing technique complexity 
no aggregation n a any not required assignment problem polynomial 
all aggregate none down to required transpor problem polynomial 
one side none aggr side div aggr side gap np-hard 
optional none down to q batch required generalized ntwrk flow np-hard 
optional bounded down to q batch bounded size schdl generalized ntwrk flow np-hard 
optional not bounded down to q batch not required nonlinear opt depends on ˆu x q 
table mapping from combinations of allocation constraints to the solution methods of gmap one side means that one side 
aggregates and divisible and the other side is restrictive batch means that traders may submit batch sizes 
figure generalized network flow model b is a buyer in 
ad b ∈ ai b ∈ ni b ∈ nd v is a seller in ad 
v ∈ ai v ∈ nd the g values represent arc gains and w 
values represent weights 
figure average performance of models when of traders 
aggregate 
figure average performance of models when of traders 
aggregate 
figure average performance of models when of traders 
aggregate 
 
figure performance of models when varying percentage of 
aggregating traders 
figure shows how the various models are affected by a change 
in the percentage of aggregating traders holding problem size fixed 
due to the integrality constraints we could not test available 
algorithms specialized for network-flow problems on our test 
problems thus we cannot fully evaluate the potential gain attributable 
to network structure however the model we built based on the 
insight from the network structure clearly provided a significant 
speedup even without using a special-purpose algorithm model 
nw provided speedups of a factor of - over the model sw 
this was consistent throughout the problem sizes including the 
smaller sizes for which the speedup is not visually apparent on the 
chart 
 conclusions 
the implementation and deployment of market exchanges 
requires the development of bidding languages information feedback 
policies and clearing algorithms that are suitable for the target 
domain while paying heed to the incentive properties of the resulting 
mechanisms for multiattribute exchanges the space of feasible 
such mechanisms is constrained by computational limitations 
imposed by the clearing process the extent to which the space of 
feasible mechanisms may be quantified a priori will facilitate the 
search for such exchanges in the full mechanism design problem 
in this work we investigate the space of two-sided multiattribute 
auctions focusing on the relationship between constraints on the 
offers traders can express through bids and the resulting 
computational problem of determining an optimal set of trades we 
developed a formal semantic framework for characterizing expressible 
offers and introduced some basic classes of restrictions our key 
technical results identify sets of conditions under which the 
overall matching problem can be separated into first identifying 
optimal pairwise trades and subsequently optimizing combinations of 
those trades based on these results we developed network flow 
models for the overall clearing problem which facilitate 
classification of problem versions by computational complexity and provide 
guidance for developing solution algorithms and relaxing bidding 
constraints 
 acknowledgments 
this work was supported in part by nsf grant iis- and 
the stiet program under nsf igert grant we are 
 
all tests were performed on intel ghz processors with 
kb cache test that did not complete by the one-hour time limit 
were recorded as seconds 
grateful to comments from an anonymous reviewer some of the 
underlying ideas were developed while the first two authors worked 
at tradingdynamics inc and ariba inc in - cf us 
patent we thank yoav shoham kumar ramaiyer and 
gopal sundaram for fruitful discussions about multiattribute 
auctions in that time frame 
 references 
 r k ahuja t l magnanti and j b orlin network flows 
prentice-hall 
 f bacchus and a grove graphical models for preference and 
utility in eleventh conference on uncertainty in artificial 
intelligence pages - montreal 
 m bichler the future of e-markets multi-dimensional market 
mechanisms cambridge u press new york ny usa 
 m bichler and j kalagnanam configurable offers and winner 
determination in multi-attribute auctions european journal of 
operational research - 
 m bichler m kaukal and a segev multi-attribute auctions for 
electronic procurement in proceedings of the st ibm iac 
workshop on internet based negotiation technologies 
 c boutilier t sandholm and r shields eliciting bid taker 
non-price preferences in combinatorial auctions in nineteenth 
natl conf on artificial intelligence pages - san jose 
 f branco the design of multidimensional auctions rand journal 
of economics - 
 y -k che design competition through multidimensional auctions 
rand journal of economics - 
 g debreu topological methods in cardinal utility theory in 
k arrow s karlin and p suppes editors mathematical methods 
in the social sciences stanford university press 
 n economides and r a schwartz electronic call market trading 
journal of portfolio management 
 y engel and m p wellman multiattribute utility representation for 
willingness-to-pay functions tech report univ of michigan 
 e fink j johnson and j hu exchange market for complex goods 
theory and experiments netnomics - 
 m l fisher r jaikumar and l n van wassenhove a multiplier 
adjustment method for the generalized assignment problem 
management science - 
 j gong exchanges for complex commodities search for optimal 
matches master s thesis university of south florida 
 c gonzales and p perny gai networks for decision making under 
certainty in ijcai- workshop on preferences edinburgh 
 j r kalagnanam a j davenport and h s lee computational 
aspects of clearing continuous call double auctions with assignment 
constraints and indivisible demand electronic commerce research 
 - 
 r l keeney and h raiffa decisions with multiple objectives 
preferences and value tradeoffs wiley 
 n nisan bidding and allocation in combinatorial auctions in 
second acm conference on electronic commerce pages - 
minneapolis mn 
 d c parkes and j kalagnanam models for iterative multiattribute 
procurement auctions management science - 
 t sandholm and s suri side constraints and non-price attributes in 
markets in ijcai- workshop on distributed constraint 
reasoning seattle 
 l j schvartzman and m p wellman market-based allocation with 
indivisible bids in aamas- workshop on agent-mediated 
electronic commerce utrecht 
 j shachat and j t swarthout procurement auctions for 
differentiated goods technical report economics 
working paper archive at wustl oct 
 a v sunderam and d c parkes preference elicitation in proxied 
multiattribute auctions in fourth acm conference on electronic 
commerce pages - san diego 
 p r wurman m p wellman and w e walsh a parametrization 
of the auction design space games and economic behavior 
 - 
 
a dynamic pari-mutuel market for hedging wagering and 
information aggregation 
david m pennock 
yahoo research labs 
 n pasadena ave rd floor 
pasadena ca usa 
pennockd yahoo-inc com 
abstract 
i develop a new mechanism for risk allocation and 
information speculation called a dynamic pari-mutuel market dpm 
a dpm acts as hybrid between a pari-mutuel market and 
a continuous double auction cda inheriting some of the 
advantages of both like a pari-mutuel market a dpm 
offers infinite buy-in liquidity and zero risk for the market 
institution like a cda a dpm can continuously react to 
new information dynamically incorporate information into 
prices and allow traders to lock in gains or limit losses by 
selling prior to event resolution the trader interface can be 
designed to mimic the familiar double auction format with 
bid-ask queues though with an addition variable called the 
payoff per share the dpm price function can be viewed 
as an automated market maker always offering to sell at 
some price and moving the price appropriately according 
to demand since the mechanism is pari-mutuel i e 
redistributive it is guaranteed to pay out exactly the amount 
of money taken in i explore a number of variations on the 
basic dpm analyzing the properties of each and solving in 
closed form for their respective price functions 
categories and subject descriptors 
j computer applications social and behavioral 
sciences-economics 
general terms 
algorithms design economics theory 
 introduction 
a wide variety of financial and wagering mechanisms have 
been developed to support hedging i e insuring against 
exposure to uncertain events and or speculative trading on 
uncertain events the dominant mechanism used in 
financial circles is the continuous double auction cda or in 
some cases the cda with market maker cdawmm the 
primary mechanism used for sports wagering is a bookie or 
bookmaker who essentially acts exactly as a market maker 
horse racing and jai alai wagering traditionally employ the 
pari-mutuel mechanism though there is no formal or 
logical separation between financial trading and wagering the 
two endeavors are socially considered distinct recently 
there has been a move to employ cdas or cdawmms for 
all types of wagering including on sports horse racing 
political events world news and many other uncertain events 
and a simultaneous and opposite trend to use bookie systems 
for betting on financial markets these trends highlight the 
interchangeable nature of the mechanisms and further blur 
the line between investing and betting some companies at 
the forefront of these movements are growing exponentially 
with some industry observers declaring the onset of a 
revolution in the wagering business 
each mechanism has pros and cons for the market 
institution and the participating traders a cda only matches 
willing traders and so poses no risk whatsoever for the 
market institution but a cda can suffer from illiquidity in the 
form huge bid-ask spreads or even empty bid-ask queues if 
trading is light and thus markets are thin a successful cda 
must overcome a chicken-and-egg problem traders are 
attracted to liquid markets but liquid markets require a large 
number of traders a cdawmm and the similar bookie 
mechanism have built-in liquidity but at a cost the market 
maker itself usually affiliated with the market institution is 
exposed to significant risk of large monetary losses both the 
cda and cdawmm offer incentives for traders to leverage 
information continuously as soon as that information 
becomes available as a result prices are known to capture 
the current state of information exceptionally well 
pari-mutuel markets effectively have infinite liquidity 
anyone can place a bet on any outcome at any time without 
the need for a matching offer from another bettor or a 
market maker pari-mutuel markets also involve no risk for the 
market institution since they only redistribute money from 
losing wagers to winning wagers however pari-mutuel 
mar 
http www wired com news ebiz html 
 
kets are not suitable for situations where information arrives 
over time since there is a strong disincentive for placing bets 
until either all information is revealed or the market 
is about to close for this reason pari-mutuel prices prior 
to the market s close cannot be considered a reflection of 
current information pari-mutuel market participants 
cannot buy low and sell high they cannot cash out gains or 
limit losses before the event outcome is revealed because 
the process whereby information arrives continuously over 
time is the rule rather than the exception the applicability 
of the standard pari-mutuel mechanism is questionable in a 
large number of settings 
in this paper i develop a new mechanism suitable for 
hedging speculating and wagering called a dynamic 
parimutuel market dpm a dpm can be thought of as a 
hybrid between a pari-mutuel market and a cda a dpm is 
indeed pari-mutuel in nature meaning that it acts only to 
redistribute money from some traders to others and so 
exposes the market institution to no volatility no risk a 
constant pre-determined subsidy is required to start the 
market the subsidy can in principle be arbitrarily small and 
might conceivably come from traders via antes or 
transaction fees rather than the market institution though a 
nontrivial outside subsidy may actually encourage trading 
and information aggregation a dpm has the infinite 
liquidity of a pari-mutuel market traders can always purchase 
shares in any outcome at any time at some price 
automatically set by the market institution a dpm is also able 
to react to and incorporate information arriving over time 
like a cda the market institution changes the price for 
particular outcomes based on the current state of wagering 
if a particular outcome receives a relatively large number of 
wagers its price increases if an outcome receives relatively 
few wagers its price decreases prices are computed 
automatically using a price function which can differ depending 
on what properties are desired the price function 
determines the instantaneous price per share for an infinitesimal 
quantity of shares the total cost for purchasing n shares 
is computed as the integral of the price function from to 
n the complexity of the price function can be hidden from 
traders by communicating only the ask prices for various lots 
of shares e g lots of shares as is common practice in 
cdas and cdawmms dpm prices do reflect current 
information and traders can cash out in an aftermarket to lock 
in gains or limit losses before the event outcome is revealed 
while there is always a market maker willing to accept buy 
orders there is not a market maker accepting sell orders 
and thus no guaranteed liquidity for selling instead selling 
is accomplished via a standard cda mechanism traders 
can always hedge-sell by purchasing the opposite outcome 
than they already own 
 background and related work 
 pari-mutuel markets 
pari-mutuel markets are common at horse races 
 dog races and jai alai games in a pari-mutuel 
market people place wagers on which of two or more mutually 
exclusive and exhaustive outcomes will occur at some time 
in the future after the true outcome becomes known all 
of the money that is lost by those who bet on the incorrect 
outcome is redistributed to those who bet on the correct 
outcome in direct proportion to the amount they wagered 
more formally if there are k mutually exclusive and 
exhaustive outcomes e g k horses exactly one of which will win 
and m m mk dollars are bet on each outcome and 
outcome i occurs then everyone who bet on an outcome 
j i loses their wager while everyone who bet on outcome 
i receives 
pk 
j mj mi dollars for every dollar they wagered 
that is every dollar wagered on i receives an equal share of 
all money wagered an equivalent way to think about the 
redistribution rule is that every dollar wagered on i is 
refunded then receives an equal share of all remaining money 
bet on the losing outcomes or 
p 
j i mj mi dollars 
in practice the market institution e g the racetrack 
first takes a certain percent of the total amount wagered 
usually about in the united states then redistributes 
whatever money remains to the winners in proportion to 
their amount bet 
consider a simple example with two outcomes a and b 
the outcomes are mutually exclusive and exhaustive 
meaning that pr a ∧ b and pr a pr b suppose 
 is bet on a and on b now suppose that a 
occurs e g horse a wins the race people who wagered on 
b lose their money or in total people who wagered 
on a win and each receives a proportional share of the total 
 wagered ignoring fees specifically each wager 
on a entitles its owner a share of the or 
every dollar bet in a pari-mutuel market has an equal 
payoff regardless of when the wager was placed or how much 
money was invested in the various outcomes at the time the 
wager was placed the only state that matters is the final 
state the final amounts wagered on all the outcomes when 
the market closes and the identity of the correct outcome 
as a result there is a disincentive to place a wager early 
if there is any chance that new information might become 
available moreover there are no guarantees about the 
payoff rate of a particular bet except that it will be nonnegative 
if the correct outcome is chosen payoff rates can fluctuate 
arbitrarily until the market closes so a second reason not 
to bet early is to wait to get a better sense of the final 
payout rates this is in contrast to cdas and cdawmms like 
the stock market where incentives exist to invest as soon as 
new information is revealed 
pari-mutuel bettors may be allowed to switch their 
chosen outcome or even cancel their bet prior to the market s 
close however they cannot cash out of the market early 
to either lock in gains or limit losses if new information 
favors one outcome over another as is possible in a cda 
or a cdawmm if bettors can cancel or change their bets 
then an aftermarket to sell existing wagers is not sensible 
every dollar wagered is worth exactly up until the 
market s close-no one would buy at greater than and no one 
would sell at less than pari-mutuel bettors must wait 
until the outcome is revealed to realize any profit or loss 
unlike a cda in a pari-mutuel market anyone can place 
a wager of any amount at any time-there is in a sense 
infinite liquidity for buying a cdawmm also has built-in 
liquidity but at the cost of significant risk for the market 
maker in a pari-mutuel market since money is only 
redistributed among bettors the market institution itself has no 
risk the main drawback of a pari-mutuel market is that 
it is useful only for capturing the value of an uncertain 
asset at some instant in time it is ill-suited for situations 
where information arrives over time continuously updating 
the estimated value of the asset-situations common in 
al 
most all trading and wagering scenarios there is no notion 
of buying low and selling high as occurs in a cda where 
buying when few others are buying and the price is low is 
rewarded more than buying when many others are buying 
 and the price is high perhaps for this reason in most 
dynamic environments financial mechanisms like the cda 
that can react in real-time to changing information are more 
typically employed to facilitate speculating and hedging 
since a pari-mutuel market can estimate the value of an 
asset at a single instant in time a repeated pari-mutuel 
market where distinct pari-mutuel markets are run at 
consecutive intervals could in principle capture changing 
information dynamics but running multiple consecutive 
markets would likely thin out trading in each individual market 
also in each individual pari-mutuel market the incentives 
would still be to wait to bet until just before the ending 
time of that particular market this last problem might 
be mitigated by instituting a random stopping rule for each 
individual pari-mutuel market 
in laboratory experiments pari-mutuel markets have shown 
a remarkable ability to aggregate and disseminate 
information dispersed among traders at least for a single snapshot 
in time a similar ability has been recognized at real 
racetracks 
 financial markets 
in the financial world wagering on the outcomes of 
uncertain future propositions is also common the typical market 
mechanism used is the continuous double auction cda 
the term securities market in economics and finance 
generically encompasses a number of markets where speculating 
on uncertain events is possible examples include stock 
markets like nasdaq options markets like the cboe 
futures markets like the cme other derivatives markets 
insurance markets political stock markets idea futures 
markets decision markets and even market games 
 securities markets generally have an economic 
and social value beyond facilitating speculation or wagering 
they allow traders to hedge risk or to insure against 
undesirable outcomes so if a particular outcome has disutility 
for a trader he or she can mitigate the risk by wagering 
for the outcome to arrange for compensation in case the 
outcome occurs in this sense buying automobile insurance 
is effectively a bet that an accident or other covered event 
will occur similarly buying a put option which is useful 
as a hedge for a stockholder is a bet that the underlying 
stock will go down in practice agents engage in a mixture 
of hedging and speculating and there is no clear dividing 
line between the two like pari-mutuel markets often 
prices in financial markets are excellent information 
aggregators yielding very accurate forecasts of future events 
 
a cda constantly matches orders to buy an asset with 
orders to sell if at any time one party is willing to buy 
one unit of the asset at a bid price of pbid while another 
party is willing to sell one unit of the asset at an ask price of 
pask and pbid is greater than or equal to pask then the two 
parties transact at some price between pbid and pask if 
the highest bid price is less than the lowest ask price then no 
transactions occur in a cda the bid and ask prices rapidly 
change as new information arrives and traders reassess the 
value of the asset since the auctioneer only matches willing 
bidders the auctioneer takes on no risk however buyers 
can only buy as many shares as sellers are willing to sell for 
any transaction to occur there must be a counterparty on 
the other side willing to accept the trade 
as a result when few traders participate in a cda it may 
become illiquid meaning that not much trading activity 
occurs the spread between the highest bid price and the 
lowest ask price may be very large or one or both queues may 
be completely empty discouraging trading 
one way to 
induce liquidity is to provide a market maker who is willing 
to accept a large number of buy and sell orders at particular 
prices we call this mechanism a cda with market maker 
 cdawmm 
conceptually the market maker is just like 
any other trader but typically is willing to accept a much 
larger volume of trades the market maker may be a 
person or may be an automated algorithm adding a market 
maker to the system increases liquidity but exposes the 
market maker to risk now instead of only matching trades the 
system actually takes on risk of its own and depending on 
what happens in the future may lose considerable amounts 
of money 
 wagering markets 
the typical las vegas bookmaker or oddsmaker functions 
much like a market maker in a cda in this case the 
market institution the book or house sets the odds 
initially 
according to expert opinion and later in response to the 
relative level of betting on the various outcomes unlike in a 
pari-mutuel environment whenever a wager is placed with 
a bookmaker the odds or terms for that bet are fixed at the 
time of the bet the bookmaker profits by offering different 
odds for the two sides of the bet essentially defining a 
bidask spread while odds may change in response to changing 
information any bets made at previously set odds remain 
in effect according to the odds at the time of the bet this 
is precisely in analogy to a cdawmm one difference 
between a bookmaker and a market maker is that the former 
usually operates in a take it or leave it mode bettors 
cannot place their own limit orders on a common queue they 
can in effect only place market orders at prices defined by 
the bookmaker still the bookmaker certainly reacts to 
bettor demand like a market maker the bookmaker exposes 
itself to significant risk sports betting markets have also 
been shown to provide high quality aggregate forecasts 
 
 market scoring rule 
hanson s market scoring rule msr is a new 
mechanism for hedging and speculating that shares some 
properties in common with a dpm like a dpm an msr can be 
conceptualized as an automated market maker always 
willing to accept a trade on any event at some price an msr 
requires a patron to subsidize the market the patron s final 
loss is variable and thus technically implies a degree of risk 
though the maximum loss is bounded an msr maintains 
a probability distribution over all events at any time any 
 
thin markets do occur often in practice and can be 
seen in a variety of the less popular markets available on 
http tradesports com or in some financial options 
markets for example 
 
a very clear example of a cdawmm is the interactive 
betting market on http wsex com 
 
or alternatively the bookmaker sets the game line in order 
to provide even-money odds 
 
trader who believes the probabilities are wrong can change 
any part of the distribution by accepting a lottery ticket that 
pays off according to a scoring rule e g the logarithmic 
scoring rule as long as that trader also agrees to pay 
off the most recent person to change the distribution in the 
limit of a single trader the mechanism behaves like a 
scoring rule suitable for polling a single agent for its probability 
distribution in the limit of many traders it produces a 
combined estimate since the market essentially always has a 
complete set of posted prices for all possible outcomes the 
mechanism avoids the problem of thin markets or illiquidity 
an msr is not pari-mutuel in nature as the patron in 
general injects a variable amount of money into the system an 
msr provides a two-sided automated market maker while 
a dpm provides a one-sided automated market maker in 
an msr the vector of payoffs across outcomes is fixed at 
the time of the trade while in a dpm the vector of payoffs 
across outcomes depends both on the state of wagering at 
the time of the trade and the state of wagering at the 
market s close while the mechanisms are quite different-and 
so trader acceptance and incentives may strongly differ-the 
properties and motivations of dpms and msrs are quite 
similar 
hanson shows how msrs are especially well suited for 
allowing bets on a combinatorial number of outcomes the 
patron s payment for subsidizing trading on all n 
possible 
combinations of n events is no larger than the sum of 
subsidizing the n event marginals independently the mechanism 
was planned for use in the policy analysis market pam a 
futures market in middle east related outcomes and funded 
by darpa until a media firestorm killed the project 
as of this writing the founders of pam were considering 
reopening under private control 
 a dynamic pari-mutuel market 
 high-level description 
in contrast to a standard pari-mutuel market where each 
dollar always buys an equal share of the payoff in a dpm 
each dollar buys a variable share in the payoff depending on 
the state of wagering at the time of purchase so a wager 
on a at a time when most others are wagering on b offers a 
greater possible profit than a wager on a when most others 
are also wagering on a 
a natural way to communicate the changing payoff of a 
bet is to say that at any given time a certain amount of 
money will buy a certain number of shares in one outcome 
the other purchasing a share entitles its owner to an equal 
stake in the winning pot should the chosen outcome occur 
the payoff is variable because when few people are betting 
on an outcome shares will generally be cheaper than at a 
time when many people are betting that outcome there 
is no pre-determined limit on the number of shares new 
shares can be continually generated as trading proceeds 
for simplicity all analyses in this paper consider the 
binary outcome case generalizing to multiple discrete 
outcomes should be straightforward denote the two outcomes 
a and b the outcomes are mutually exclusive and 
ex 
see http hanson gmu edu policyanalysismarket html for 
more information or http dpennock com pam html for 
commentary 
 
http www policyanalysismarket com 
haustive denote the instantaneous price per share of a as 
p and the price per share of b as p denote the payoffs 
per share as p and p respectively these four numbers 
p p p p are the key numbers that traders must track 
and understand note that the price is set at the time of the 
wager the payoff per share is finalized only after the event 
outcome is revealed 
at any time a trader can purchase an infinitesimal 
quantity of shares of a at price p and similarly for b however 
since the price changes continuously as shares are purchased 
the cost of buying n shares is computed as the integral of a 
price function from to n the use of continuous functions 
and integrals can be hidden from traders by aggregating the 
automated market maker s sell orders into discrete lots of 
say shares each these ask orders can be 
automatically entered into the system by the market institution so 
that traders interact with what looks like a more familiar 
cda we examine this interface issue in more detail below 
in section 
for our analysis we introduce the following additional 
notation denote m as the total amount of money wagered 
on a m as the total amount of money wagered on b 
t m m as the total amount of money wagered on 
both sides n as the total number of shares purchased of 
a and n as the total number of shares purchased of b 
there are many ways to formulate the price function 
several natural price functions are outlined below each is 
motivated as the unique solution to a particular constraint on 
price dynamics 
 advantages and disadvantages 
to my knowledge a dpm is the only known mechanism 
for hedging and speculating that exhibits all three of the 
following properties guaranteed liquidity no risk 
for the market institution and continuous incorporation 
of information a standard pari-mutuel fails a cda 
fails a cdawmm the bookmaker mechanism and an 
msr all fail even though technically an msr exposes 
its patron to risk i e a variable future payoff the 
patron s maximum loss is bounded so the distinction between 
a dpm and an msr in terms of these three properties is 
more technical than practical 
dpm traders can cash out of the market early just like 
stock market traders to lock in a profit or limit a loss an 
action that is simply not possible in a standard pari-mutuel 
a dpm also has some drawbacks the payoff for a 
wager depends both on the price at the time of the trade and 
on the final payoff per share at the market s close this 
contrasts with the cda variants where the payoff vector 
across possible future outcomes is fixed at the time of the 
trade so a trader s strategic optimization problem is 
complicated by the need to predict the final values of p and 
p if p changes according to a random walk then traders 
can take the current p as an unbiased estimate of the 
final p greatly decreasing the complexity of their 
optimization if p does not change according to a random walk 
the mechanism still has utility as a mechanism for hedging 
and speculating though optimization may be difficult and 
determining a measure of the market s aggregate opinion of 
the probabilities of a and b may be difficult we discuss 
the implications of random walk behavior further below in 
section in the discussion surrounding assumption 
a second drawback of a dpm is its one-sided nature 
 
while an automated market maker always stands ready to 
accept buy orders there is no corresponding market maker 
to accept sell orders traders must sell to each other 
using a standard cda mechanism for example by posting an 
ask order at a price at or below the market maker s current 
ask price traders can also always hedge-sell by 
purchasing shares in the opposite outcome from the market maker 
thereby hedging their bet if not fully liquidating it 
 redistribution rule 
in a standard pari-mutuel market payoffs can be 
computed in either of two equivalent ways each winning 
wager receives a refund of the initial paid plus an equal 
share of all losing wagers or each winning wager 
receives an equal share of all wagers winning or losing 
because each dollar always earns an equal share of the payoff 
the two formulations are precisely the same 
 
mlose 
mwin 
 
mwin mlose 
mwin 
 
in a dynamic pari-mutuel market because each dollar is 
not equally weighted the two formulations are distinct and 
lead to significantly different price functions and 
mechanisms each with different potentially desirable properties 
we consider each case in turn the next section analyzes 
case where only losing money is redistributed section 
examines case where all money is redistributed 
 dpm i losing money 
redistributed 
for the case where the initial payments on winning bets 
are refunded and only losing money is redistributed the 
respective payoffs per share are simply 
p 
m 
n 
p 
m 
n 
 
so if a occurs shareholders of a receive all of their 
initial payment back plus p dollars per share owned while 
shareholders of b lose all money wagered similarly if b 
occurs shareholders of b receive all of their initial payment 
back plus p dollars per share owned while shareholders of 
a lose all money wagered 
without loss of generality i will analyze the market from 
the perspective of a deriving prices and payoffs for a only 
the equations for b are symmetric 
the trader s per-share expected value for purchasing an 
infinitesimal quantity of shares of a is 
e shares 
 pr a · e p a − − pr a · p 
e shares 
 pr a · e 
 
m 
n 
˛ 
˛ 
˛ 
˛ a 
 
− − pr a · p 
where is an infinitesimal quantity of shares of a pr a 
is the trader s belief in the probability of a and p is the 
instantaneous price per share of a for an infinitesimal 
quantity of shares e p a is the trader s expectation of the 
payoff per share of a after the market closes and given that 
a occurs this is a subtle point the value of p does not 
matter if b occurs since in this case shares of a are 
worthless and the current value of p does not necessarily matter 
as this may change as trading continues so in order to 
determine the expected value of shares of a the trader must 
estimate what he or she expects the payoff per share to be 
in the end after the market closes if a occurs 
if e shares a risk-neutral trader should purchase 
shares of a how many shares this depends on the price 
function determining p in general p increases as more 
shares are purchased the risk-neutral trader should 
continue purchasing shares until e shares a 
riskaverse trader will generally stop purchasing shares before 
driving e shares all the way to zero assuming 
riskneutrality the trader s optimization problem is to choose a 
number of shares n ≥ of a to purchase in order to 
maximize 
e n shares pr a ·n·e p a − −pr a · 
z n 
 
p n dn 
 
it s easy to see that the same value of n can be solved for by 
finding the number of shares required to drive e shares 
to zero that is find n ≥ satisfying 
 pr a · e p a − − pr a · p n 
if such a n exists otherwise n 
 market probability 
as traders who believe that e shares of a 
purchase shares of a and traders who believe that e shares 
of b purchase shares of b the prices p and p 
change according to a price function as prescribed below 
the current prices in a sense reflect the market s opinion as 
a whole of the relative probabilities of a and b 
assuming an efficient marketplace the market as a whole 
considers e shares since the mechanisms is a zero sum 
game for example if market participants in aggregate felt 
that e shares then there would be net demand for 
a driving up the price of a until e shares define 
mpr a to be the market probability of a or the 
probability of a inferred by assuming that e shares we 
can consider mpr a to be the aggregate probability of a 
as judged by the market as a whole mpr a is the solution 
to 
 mpr a · e p a − − mpr a · p 
solving we get 
mpr a 
p 
p e p a 
 
at this point we make a critical assumption in order to 
greatly simplify the analysis we assume that 
e p a p 
that is we assume that the current value for the payoff per 
share of a is the same as the expected final value of the 
payoff per share of a given that a occurs this is certainly true 
for the last infinitesimal wager before the market closes 
it s not obvious however that the assumption is true well 
before the market s close basically we are assuming that 
the value of p moves according to an unbiased random 
walk the current value of p is the best expectation of 
its future value i conjecture that there are reasonable 
market efficiency conditions under which assumption is true 
though i have not been able to prove that it arises naturally 
from rational trading we examine scenarios below in which 
 
assumption seems especially plausible nonetheless the 
assumption effects our analysis only regardless of whether 
 is true each price function derived below implies a 
welldefined zero-sum game in which traders can play if traders 
can assume that is true then their optimization 
problem is greatly simplified however optimizing does 
not depend on the assumption and traders can still optimize 
by strategically projecting the final expected payoff in 
whatever complicated way they desire so the utility of dpm for 
hedging and speculating does not necessarily hinge on the 
truth of assumption on the other hand the ability to 
easily infer an aggregate market consensus probability from 
market prices does depend on 
 price functions 
a variety of price functions seem reasonable each 
exhibiting various properties and implying differing market 
probabilities 
 price function i price of a equals payoff of b 
one natural price function to consider is to set the price 
per share of a equal to the payoff per share of b and set 
the price per share of b equal to the payoff per share of a 
that is 
p p 
p p 
enforcing this relationship reduces the dimensionality of 
the system from four to two simplifying the interface traders 
need only track two numbers instead of four the 
relationship makes sense since new information supporting a 
should encourage purchasing of shares a driving up both 
the price of a and the payoff of b and driving down the 
price of b and the payoff of a in this setting assumption 
 seems especially reasonable since if an efficient market 
hypothesis leads prices to follow a random walk than payoffs 
must also follow a random walk 
the constraints lead to the following derivation of the 
market probability 
mpr a p mpr b p 
mpr a p mpr b p 
mpr a 
mpr b 
 
p 
p 
mpr a 
mpr b 
 
m 
n 
m 
n 
mpr a 
mpr b 
 
m n 
m n 
mpr a 
m n 
m n m n 
 
the constraints specify the instantaneous 
relationship between payoff and price from this we can derive 
how prices change when non-infinitesimal shares are 
purchased let n be the number of shares purchased and let m 
be the amount of money spent purchasing n shares note 
that p dm dn the instantaneous price per share and 
m 
r n 
 
p n dn substituting into equation we get 
p p 
dm 
dn 
 
m m 
n 
dm 
m m 
 
dn 
n 
z 
dm 
m m 
 
z 
dn 
n 
ln m m 
n 
n 
 c 
m m 
h 
e 
n 
n − 
i 
 
equation gives the cost of purchasing n shares the 
instantaneous price per share as a function of n is 
p n 
dm 
dn 
 
m 
n 
e 
n 
n 
note that p m n p as required the derivation 
of the price function p n for b is analogous and the results 
are symmetric 
the notion of buying infinitesimal shares or 
integrating costs over a continuous function are probably foreign 
to most traders a more standard interface can be 
implemented by discretizing the costs into round lots of shares 
for example lots of shares then ask orders of 
shares each at the appropriate price can be automatically 
placed by the market institution for example the 
market institution can place an ask order for shares at 
price m another ask order for shares at price 
 m −m a third ask for shares at m − 
m etc in this way the market looks more 
familiar to traders like a typical cda with a number of ask 
orders at various prices automatically available a trader 
buying less than shares would pay a bit more than if 
the true cost were computed using but the discretized 
interface would probably be more intuitive and transparent 
to the majority of traders 
the above equations assume that all money that comes in 
is eventually returned or redistributed in other words the 
mechanism is a zero sum game and the market institution 
takes no portion of the money this could be generalized so 
that the market institution always takes a certain amount 
or a certain percent or a certain amount per transaction or 
a certain percent per transaction before money in returned 
or redistributed 
finally note that the above price function is undefined 
when the amount bet or the number of shares are zero so 
the system must begin with some positive amount on both 
sides and some positive number of shares outstanding on 
both sides these initial amounts can be arbitrarily small in 
principle but the size of the initial subsidy may affect the 
incentives of traders to participate also the smaller the 
initial amounts the more each new dollar effects the prices 
the initialization amounts could be funded as a subsidy from 
the market institution or a patron which i ll call a seed 
wager or from a portion of the fees charged which i ll call 
an ante wager 
 price function ii price of a proportional to 
money on a 
a second price function can be derived by requiring the 
ratio of prices to be equal to the ratio of money wagered 
 
that is 
p 
p 
 
m 
m 
 
in other words the price of a is proportional to the amount 
of money wagered on a and similarly for b this seems like 
a particularly natural way to set the price since the more 
money that is wagered on one side the cheaper becomes a 
share on the other side in exactly the same proportion 
using equation along with and we can derive 
the implied market probability 
m 
m 
 
p 
p 
 
mpr a 
mpr b 
· m 
n 
mpr b 
mpr a 
· m 
n 
 
 mpr a 
 mpr b 
· 
m n 
m n 
 mpr a 
 mpr b 
 
 m 
n 
 m n 
mpr a 
mpr b 
 
m 
√ 
n 
m 
√ 
n 
mpr a 
m 
√ 
n 
m 
√ 
n m 
√ 
n 
 
we can solve for the instantaneous price as follows 
p 
mpr a 
mpr b 
· p 
 
m 
√ 
n 
m 
√ 
n 
· 
m 
n 
 
m 
√ 
n n 
 
working from the above instantaneous price we can 
derive the implied cost function m as a function of the number 
n of shares purchased as follows 
dm 
dn 
 
m m 
√ 
n n 
√ 
n 
z 
dm 
m m 
 
z 
dn 
√ 
n n 
√ 
n 
ln m m 
 
n 
 n n n 
 
 c 
m m 
 
e 
 
r 
n n 
n 
− 
r 
n 
n − 
 
 
from this we get the price function 
p n 
dm 
dn 
 
m 
p 
 n n n 
e 
 
r 
n n 
n 
− 
r 
n 
n 
 
note that as required p m 
√ 
n n and p p 
 m m if one uses the above price function then the 
market dynamics will be such that the ratio of the 
 instantaneous prices of a and b always equals the ratio of the 
amounts wagered on a and b which seems fairly natural 
note that as before the mechanism can be modified to 
collect transaction fees of some kind also note that seed or 
ante wagers are required to initialize the system 
 dpm ii all money redistributed 
above we examined the policy of refunding winning 
wagers and redistributing only losing wagers in this section 
we consider the second policy mentioned in section all 
money from all wagers are collected and redistributed to 
winning wagers 
for the case where all money is redistributed the 
respective payoffs per share are 
p 
m m 
n 
 
t 
n 
p 
m m 
n 
 
t 
n 
 
where t m m is the total amount of money wagered 
on both sides so if a occurs shareholders of a lose their 
initial price paid but receive p dollars per share owned 
shareholders of b simply lose all money wagered similarly 
if b occurs shareholders of b lose their initial price paid 
but receive p dollars per share owned shareholders of a 
lose all money wagered 
in this case the trader s per-share expected value for 
purchasing an infinitesimal quantity of shares of a is 
e shares 
 pr a · e p a − p 
a risk-neutral trader optimizes by choosing a number of 
shares n ≥ of a to purchase in order to maximize 
e n shares pr a · n · e p a − 
z n 
 
p n dn 
 pr a · n · e p a − m 
the same value of n can be solved for by finding the number 
of shares required to drive e shares to zero that is 
find n ≥ satisfying 
 pr a · e p a − p n 
if such a n exists otherwise n 
 market probability 
in this case mpr a the aggregate probability of a as 
judged by the market as a whole is the solution to 
 mpr a · e p a − p 
solving we get 
mpr a 
p 
e p a 
 
as before we make the simplifying assumption that 
the expected final payoff per share equals the current payoff 
per share the assumption is critical for our analysis but 
may not be required for a practical implementation 
 price functions 
for the case where all money is distributed the 
constraints that keep the price of a equal to the payoff of b 
and vice versa do not lead to the derivation of a coherent 
price function 
a reasonable price function can be derived from the 
constraint employed in section where we require that 
the ratio of prices to be equal to the ratio of money wagered 
that is p p m m in other words the price of a is 
proportional to the amount of money wagered on a and 
similarly for b 
 
using equations and we can derive the implied 
market probability 
m 
m 
 
p 
p 
 
mpr a 
mpr b 
· 
t 
n 
· 
n 
t 
 
mpr a 
mpr b 
· 
n 
n 
mpr a 
mpr b 
 
m n 
m n 
mpr a 
m n 
m n m n 
 
interestingly this is the same market probability derived in 
section for the case of losing-money redistribution with 
the constraints that the price of a equal the payoff of b and 
vice versa 
the instantaneous price per share for an infinitesimal 
quantity of shares is 
p 
 m 
 m m 
m n m n 
 
m m 
n m 
m 
n 
working from the above instantaneous price we can 
derive the number of shares n that can be purchased for m 
dollars as follows 
dm 
dn 
 
m m m 
n n m 
m m 
n 
dn 
dm 
 
n n m 
m m 
n 
m m m 
 
· · · 
n 
m n − n 
t 
 
n t m 
m 
ln 
 
t m m 
m t m 
 
 
note that we solved for n m rather than m n i could not 
find a closed-form solution for m n as was derived for the 
two other cases above still n m can be used to determine 
how many shares can be purchased for m dollars and the 
inverse function can be approximated to any degree 
numerically from n m we can also compute the price function 
p m 
dm 
dn 
 
 m m m t 
denom 
 
where 
denom m m m n m − m m n 
 t m m n ln 
 
t m m 
m t m 
 
note that as required p p m m if one uses 
the above price function then the market dynamics will be 
such that the ratio of the instantaneous prices of a and b 
always equals the ratio of the amounts wagered on a and 
b 
this price function has another desirable property it acts 
such that the expected value of wagering on a and 
simultaneously wagering on b equals zero assuming that 
is e of a of b the derivation is omitted 
 comparing dpm i and ii 
the main advantage of refunding winning wagers dpm 
i is that every bet on the winning outcome is guaranteed 
to at least break even the main disadvantage of 
refunding winning wagers is that shares are not homogenous each 
share of a for example is actually composed of two distinct 
parts the refund or a lottery ticket that pays p if a 
occurs where p is the price paid per share and one share 
of the final payoff p if a occurs this complicates the 
implementation of an aftermarket to cash out of the market 
early which we will examine below in section when all 
money is redistributed dpm ii shares are homogeneous 
each share entitles its owner to an equal slice of the final 
payoff because shares are homogenous the 
implementation of an aftermarket is straightforward as we shall see in 
section on the other hand because initial prices paid 
are not refunded for winning bets there is a chance that if 
prices swing wildly enough a wager on the correct outcome 
might actually lose money traders must be aware that if 
they buy in at an excessively high price that later tumbles 
allowing many others to get in at a much lower price they 
may lose money in the end regardless of the outcome from 
informal experiments i don t believe this eventuality would 
be common but nonetheless it requires care in 
communicating to traders the possible risks one potential fix would be 
for the market maker to keep track of when the price is going 
too low endangering an investor on the correct outcome at 
this point the market maker could artificially stop lowering 
the price sell orders in the aftermarket might still come in 
below the market maker s price but in this way the system 
could ensure that every wager on the correct outcome at 
least breaks even 
 other variations 
a simple ascending price function would set p αm 
and p αm where α in this case prices would only 
go up for the case of all money being redistributed this 
would eliminate the possibility of losing money on a wager on 
the correct outcome even though the market maker s price 
only rises the going price may fall well below the market 
maker s price as ask orders are placed in the aftermarket 
i have derived price functions for several other cases using 
the same methodology above each price function may have 
its own desirable properties but it s not clear which is best 
or even that a single best method exists further analyses 
and more importantly empirical investigations are required 
to answer these questions 
 aftermarkets 
a key advantage of dpm over a standard pari-mutuel 
market is the ability to cash out of the market before it 
closes in order to take a profit or limit a loss this is 
accomplished by allowing traders to place ask orders on the 
same queue as the market maker so traders can sell the 
shares that they purchased at or below the price set by the 
market maker or traders can place a limit sell order at any 
price buyers will purchase any existing shares for sale at 
the lower prices first before purchasing new shares from the 
market maker 
 aftermarket for dpm ii 
for the second main case explored above where all money 
 
is redistributed allowing an aftermarket is simple in fact 
aftermarket may be a poor descriptor buying and selling 
are both fully integrated into the same mechanism every 
share is worth precisely the same amount so traders can 
simply place ask orders on the same queue as the market maker 
in order to sell their shares new buyers will accept the 
lowest ask price whether it comes from the market maker or 
another trader in this way traders can cash out early and 
walk away with their current profit or loss assuming they 
can find a willing buyer 
 aftermarket for dpm i 
when winning wagers are refunded and only losing wagers 
are redistributed each share is potentially worth a different 
amount depending on how much was paid for it so it is not 
as simple a matter to set up an aftermarket however an 
aftermarket is still possible in fact much of the complexity 
can be hidden from traders so it looks nearly as simple as 
placing a sell order on the queue 
in this case shares are not homogenous each share of a 
is actually composed of two distinct parts the refund of 
p · a dollars and the payoff of p · a dollars where p 
is the per-share price paid and a is the indicator function 
equalling if a occurs and otherwise one can 
imagine running two separate aftermarkets where people can sell 
these two respective components however it is possible to 
automate the two aftermarkets by automatically bundling 
them together in the correct ratio and selling them in the 
central dpm in this way traders can cash out by placing 
sell orders on the same queue as the dpm market maker 
effectively hiding the complexity of explicitly having two 
separate aftermarkets the bundling mechanism works as 
follows suppose the current price for share of a is p a 
buyer agrees to purchase the share at p the buyer pays 
p dollars and receives p · a p · a dollars if there is 
enough inventory in the aftermarkets the buyer s share is 
constructed by bundling together p · a from the first 
aftermarket and p · a from the second aftermarket the seller 
in the first aftermarket receives p mpr a dollars and the 
seller in the second aftermarket receives p mpr b dollars 
 pseudo aftermarket for dpm i 
there is an alternative pseudo aftermarket that s 
possible for the case of dpm i that does not require bundling 
consider a share of a purchased for the share is 
composed of · a and p · a now suppose the current price 
has moved from to per share and the trader wants to 
cash out at a profit the trader can sell share at market 
price share for receiving all of the initial 
investment back and retaining share of a the share is 
worth either some positive amount or nothing depending 
on the outcome and the final payoff so the trader is left 
with shares worth a positive expected value and all of his or 
her initial investment the trader has essentially cashed out 
and locked in his or her gains now suppose instead that 
the price moves downward from to per share the 
trader decides to limit his or her loss by selling the share for 
 the buyer gets the share plus · a the buyer s price 
refunded the trader seller gets the plus what remains 
of the original price refunded or · a the trader s loss 
is now limited to at most instead of if a occurs the 
trader breaks even if b occurs the trader loses 
also note that-in either dpm formulation-traders can 
always hedge sell by buying the opposite outcome without 
the need for any type of aftermarket 
 conclusions 
i have presented a new market mechanism for wagering 
on or hedging against a future uncertain event called a 
dynamic pari-mutuel market dpm the mechanism 
combines the infinite liquidity and risk-free nature of a 
parimutuel market with the dynamic nature of a cda making 
it suitable for continuous information aggregation to my 
knowledge all existing mechanisms-including the standard 
pari-mutuel market the cda the cdawmm the bookie 
mechanism and the msr-exhibit at most two of the three 
properties an msr is the closest to a dpm in terms of 
these properties if not in terms of mechanics given some 
natural constraints on price dynamics i have derived in 
closed form the implied price functions which encode how 
prices change continuously as shares are purchased the 
interface for traders looks much like the familiar cda with 
the system acting as an automated market maker willing 
to accept an infinite number of buy orders at some price 
i have explored two main variations of a dpm one where 
only losing money is redistributed and one where all money 
is redistributed each has its own pros and cons and each 
supports several reasonable price functions i have described 
the workings of an aftermarket so that traders can cash out 
of the market early like in a cda to lock in their gains 
or limit their losses an operation that is not possible in a 
standard pari-mutuel setting 
 future work 
this paper reports the results of an initial investigation of 
the concept of a dynamic pari-mutuel market many avenues 
for future work present themselves including the following 
 random walk conjecture the most important 
question mark in my mind is whether the random walk 
assumption can be proven under reasonable market 
efficiency conditions and if not how severely it effects 
the practicality of the system 
 incentive analysis formally what are the 
incentives for traders to act on new information and when 
how does the level of initial subsidy effect trader 
incentives 
 laboratory experiments and field tests this 
paper concentrated on the mathematics and algorithmics 
of the mechanism however the true test of the 
mechanism s ability to serve as an instrument for hedging 
wagering or information aggregation is to test it with 
real traders in a realistic environment in reality how 
do people behave when faced with a dpm mechanism 
 dpm call market i have derived the price functions 
to react to wagers on one outcome at a time the 
mechanism could be generalized to accept orders on 
both sides then update the prices wholistically rather 
than by assuming a particular sequence on the wagers 
 real-valued variables i believe the mechanisms in 
this paper can easily be generalized to multiple discrete 
 
outcomes and multiple real-valued outcomes that 
always sum to some constant value e g multiple 
percentage values that must sum to however the 
generalization to real-valued variables with arbitrary 
range is less clear and open for future development 
 compound combinatorial betting i believe that 
dpm may be well suited for compound or 
combinatorial betting for many of the same reasons 
that market scoring rules are well suited for the 
task dpm may also have some computational 
advantages over msr though this remains to be seen 
acknowledgments 
i thank dan fain gary flake lance fortnow and robin 
hanson 
 references 
 mukhtar m ali probability and utility estimates for 
racetrack bettors journal of political economy 
 - 
 peter bossaerts leslie fine and john ledyard 
inducing liquidity in thin financial markets through 
combined-value trading mechanisms european 
economic review - 
 kay-yut chen leslie r fine and bernardo a 
huberman forecasting uncertain events with small 
groups in third acm conference on electronic 
commerce ec pages - 
 sandip debnath david m pennock c lee giles and 
steve lawrence information incorporation in online 
in-game sports betting markets in fourth acm 
conference on electronic commerce ec 
 robert forsythe and russell lundholm information 
aggregation in an experimental market econometrica 
 - 
 robert forsythe forrest nelson george r neumann 
and jack wright anatomy of an experimental 
political stock market american economic review 
 - 
 robert forsythe thomas a rietz and thomas w 
ross wishes expectations and actions a survey on 
price formation in election stock markets journal of 
economic behavior and organization - 
 
 lance fortnow joe kilian david m pennock and 
michael p wellman betting boolean-style a 
framework for trading in securities based on logical 
formulas in proceedings of the fourth annual acm 
conference on electronic commerce pages - 
 
 john m gandar william h dare craig r brown 
and richard a zuber informed traders and price 
variations in the betting market for professional 
basketball games journal of finance 
liii - 
 robin hanson decision markets ieee intelligent 
systems - 
 robin hanson combinatorial information market 
design information systems frontiers 
 robin d hanson could gambling save science 
encouraging an honest consensus social 
epistemology - 
 jens carsten jackwerth and mark rubinstein 
recovering probability distributions from options 
prices journal of finance - 
 joseph b kadane and robert l winkler separating 
probability elicitation from utilities journal of the 
american statistical association - 
 
 david m pennock steve lawrence c lee giles and 
finn ˚arup nielsen the real power of artificial 
markets science - february 
 david m pennock steve lawrence finn ˚arup 
nielsen and c lee giles extracting collective 
probabilistic forecasts from web games in seventh 
international conference on knowledge discovery and 
data mining pages - 
 c r plott j wit and w c yang parimutuel 
betting markets as information aggregation devices 
experimental results technical report social science 
working paper california institute of 
technology april 
 charles r plott markets as information gathering 
tools southern economic journal - 
 charles r plott and shyam sunder rational 
expectations and the aggregation of diverse 
information in laboratory security markets 
econometrica - 
 charles polk robin hanson john ledyard and 
takashi ishikida policy analysis market an 
electronic commerce application of a combinatorial 
information market in proceedings of the fourth 
annual acm conference on electronic commerce 
pages - 
 r roll orange juice and weather american 
economic review - 
 richard n rosett gambling and rationality journal 
of political economy - 
 carsten schmidt and axel werwatz how accurate do 
markets predict the outcome of an event the euro 
 soccer championships experiment technical 
report - max planck institute for research 
into economic systems 
 wayne w snyder horse racing testing the efficient 
markets model journal of finance - 
 
 richard h thaler and william t ziemba anomalies 
parimutuel betting markets racetracks and lotteries 
journal of economic perspectives - 
 martin weitzman utility analysis and group 
behavior an empirical study journal of political 
economy - 
 robert l winkler and allan h murphy good 
probability assessors j applied meteorology 
 - 
 
privacy in electronic commerce and the economics of 
immediate gratification 
alessandro acquisti 
h john heinz iii school of public policy and management 
carnegie mellon university 
acquisti andrew cmu edu 
abstract 
dichotomies between privacy attitudes and behavior have 
been noted in the literature but not yet fully explained we 
apply lessons from the research on behavioral economics to 
understand the individual decision making process with 
respect to privacy in electronic commerce we show that it is 
unrealistic to expect individual rationality in this context 
models of self-control problems and immediate gratification 
offer more realistic descriptions of the decision process and 
are more consistent with currently available data in 
particular we show why individuals who may genuinely want to 
protect their privacy might not do so because of 
psychological distortions well documented in the behavioral literature 
we show that these distortions may affect not only  na¨ıve 
individuals but also  sophisticated ones and we prove that 
this may occur also when individuals perceive the risks from 
not protecting their privacy as significant 
categories and subject descriptors 
j social and behavioral sciences economics k 
 public policy issues privacy 
general terms 
economics security human factors 
 privacy and electronic 
commerce 
privacy remains an important issue for electronic 
commerce a pricewaterhousecoopers study in showed 
that nearly two thirds of the consumers surveyed would 
shop more online if they knew retail sites would not do 
anything with their personal information a federal trade 
commission study reported in that sixty-seven 
percent of consumers were very concerned about the privacy 
of the personal information provided on-line more 
recently a february harris interactive survey found that 
the three biggest consumer concerns in the area of on-line 
personal information security were companies trading 
personal data without permission the consequences of insecure 
transactions and theft of personal data according to 
a jupiter research study in billion in on-line 
sales will be lost by - up from billion in 
online retail sales would be approximately twenty-four percent 
higher in if consumers fears about privacy and 
security were addressed effectively although the media 
hype has somewhat diminished risks and costs have 
notas evidenced by the increasing volumes of electronic spam 
and identity theft 
surveys in this field however as well as experiments and 
anecdotal evidence have also painted a different picture 
 have found evidence that even privacy 
concerned individuals are willing to trade-off privacy for 
convenience or bargain the release of very personal information in 
exchange for relatively small rewards the failure of several 
on-line services aimed at providing anonymity for internet 
users offers additional indirect evidence of the reluctance 
by most individuals to spend any effort in protecting their 
personal information 
the dichotomy between privacy attitudes and behavior 
has been highlighted in the literature preliminary 
interpretations of this phenomenon have been provided 
 still missing are an explanation grounded in 
economic or psychological theories an empirical validation of 
the proposed explanation and of course the answer to the 
most recurring question should people bother at all about 
privacy 
in this paper we focus on the first question we formally 
analyze the individual decision making process with respect 
to privacy and its possible shortcomings we focus on 
individual mis conceptions about their handling of risks they 
face when revealing private information we do not address 
the issue of whether people should actually protect 
themselves we will comment on that in section where we will 
also discuss strategies to empirically validate our theory 
we apply lessons from behavioral economics traditional 
economics postulates that people are forward-looking and 
bayesian updaters they take into account how current 
behavior will influence their future well-being and preferences 
for example study rational models of addiction this 
approach can be compared to those who see in the decision 
 
not to protect one s privacy a rational choice given the 
 supposedly low risks at stake however developments in the 
area of behavioral economics have highlighted various forms 
of psychological inconsistencies self-control problems 
hyperbolic discounting present-biases etc that clash with 
the fully rational view of the economic agent in this 
paper we draw from these developments to reach the following 
conclusions 
 we show that it is unlikely that individuals can act 
rationally in the economic sense when facing privacy 
sensitive decisions 
 we show that alternative models of personal behavior 
and time-inconsistent preferences are compatible with 
the dichotomy between attitudes and behavior and can 
better match current data for example they can 
explain the results presented by at the acm ec 
 conference in their experiment self-proclaimed 
privacy advocates were found to be willing to reveal 
varying amounts of personal information in exchange 
for small rewards 
 in particular we show that individuals may have a 
tendency to under-protect themselves against the privacy 
risks they perceive and over-provide personal 
information even when wary of perceived risks involved 
 we show that the magnitude of the perceived costs of 
privacy under certain conditions will not act as 
deterrent against behavior the individual admits is risky 
 we show following similar studies in the economics of 
immediate gratification that even  sophisticated 
individuals may under certain conditions become 
 privacy myopic 
our conclusion is that simply providing more 
information and awareness in a self-regulative environment is not 
sufficient to protect individual privacy improved 
technologies by lowering costs of adoption and protection certainly 
can help however more fundamental human behavioral 
responses must also be addressed if privacy ought to be 
protected 
in the next section we propose a model of rational agents 
facing privacy sensitive decisions in section we show the 
difficulties that hinder any model of privacy decision 
making based on full rationality in section we show how 
behavioral models based on immediate gratification bias can 
better explain the attitudes-behavior dichotomy and match 
available data in section we summarize and discuss our 
conclusions 
 a model of rationality in 
privacy decision making 
some have used the dichotomy between privacy attitudes 
and behavior to claim that individuals are acting rationally 
when it comes to privacy under this view individuals may 
accept small rewards for giving away information because 
they expect future damages to be even smaller when 
discounted over time and with their probability of occurrence 
here we want to investigate what underlying assumptions 
about personal behavior would support the hypothesis of 
full rationality in privacy decision making 
since economists have been interested in 
privacy but only recently formal models have started 
appearing while these studies focus on market 
interactions between one agent and other parties here we are 
interested in formalizing the decision process of the single 
individual we want to see if individuals can be 
economically rational forward-lookers bayesian updaters utility 
maximizers and so on when it comes to protect their own 
personal information 
the concept of privacy once intended as the right to be 
left alone has transformed as our society has become 
more information oriented in an information society the self 
is expressed defined and affected through and by 
information and information technology the boundaries between 
private and public become blurred privacy has therefore 
become more a class of multifaceted interests than a single 
unambiguous concept hence its value may be discussed if 
not ascertained only once its context has also been 
specified this most often requires the study of a network of 
relations between a subject certain information related to 
the subject other parties that may have various linkages 
of interest or association with that information or that 
subject and the context in which such linkages take place 
to understand how a rational agent could navigate through 
those complex relations in equation we abstract the 
decision process of an idealized rational economic agent who 
is facing privacy trade-offs when completing a certain 
transaction 
max 
d 
ut δ ve a pd 
 a γ ve t pd 
 t − cd 
t 
in equation δ and γ are unspecified functional forms 
that describe weighted relations between expected payoffs 
from a set of events v and the associated probabilities of 
occurrence of those events p more precisely the utility u of 
completing a transaction t the transaction being any action 
- not necessarily a monetary operation - possibly involving 
exposure of personal information is equal to some function 
of the expected payoff ve a from maintaining or not 
certain information private during that transaction and the 
probability of maintaining or not maintaining that 
information private when using technology d pd 
 a − pd 
 a 
plus some function of the expected payoff ve t from 
completing or non completing the transaction possibly 
revealing personal information and the probability of completing 
 or not completing that transaction with a certain 
technology d pd 
 t − pd 
 t minus the cost of using the 
technology t cd 
t 
the technology d may or may not be privacy enhancing 
since the payoffs in equation can be either positive or 
negative equation embodies the duality implicit in privacy 
issues there are both costs and benefits gained from 
revealing or from protecting personal information and the costs 
and benefits from completing a transaction ve t might be 
distinct from the costs and benefits from keeping the 
associated information private ve a for instance revealing 
one s identity to an on-line bookstore may earn a discount 
viceversa it may also cost a larger bill because of price 
discrimination protecting one s financial privacy by not 
divulging credit card information on-line may protect against 
future losses and hassles related to identity theft but it may 
 
see also 
 
make one s on-line shopping experience more cumbersome 
and therefore more expensive 
the functional parameters δ and γ embody the variable 
weights and attitudes an individual may have towards 
keeping her information private for example her privacy 
sensitivity or her belief that privacy is a right whose respect 
should be enforced by the government and completing 
certain transactions note that ve and p could refer to sets of 
payoffs and the associated probabilities of occurrence the 
payoffs are themselves only expected because regardless of 
the probability that the transaction is completed or the 
information remains private they may depend on other sets of 
events and their associated probabilities ve and pd 
 in 
other words can be read as multi-variate parameters inside 
which are hidden several other variables expectations and 
functions because of the complexity of the privacy network 
described above 
over time the probability of keeping certain information 
private for instance will not only depend on the chosen 
technology d but also on the efforts by other parties to 
appropriate that information these efforts may be function 
among other things of the expected value of that 
information to those parties the probability of keeping 
information private will also depend on the environment in which 
the transaction is taking place similarly the expected 
benefit from keeping information private will also be a 
collection over time of probability distributions dependent on 
several parameters imagine that the probability of keeping 
your financial transactions private is very high when you 
use a bank in bermuda still the expected value from 
keeping your financial information confidential will depend on a 
number of other factors 
a rational agent would in theory choose the technology 
d that maximizes her expected payoff in equation maybe 
she would choose to complete the transaction under the 
protection of a privacy enhancing technology maybe she would 
complete the transaction without protection maybe she 
would not complete the transaction at all d for 
example the agent may consider the costs and benefits of 
sending an email through an anonymous mix-net system 
 and compare those to the costs and benefits of sending 
that email through a conventional non-anonymous 
channel the magnitudes of the parameters in equation will 
change with the chosen technology mix-net systems may 
decrease the expected losses from privacy intrusions 
nonanonymous email systems may promise comparably higher 
reliability and possibly reduced costs of operations 
 rationality and psychological 
distortions in privacy 
equation is a comprehensive while intentionally generic 
road-map for navigation across privacy trade-offs that no 
human agent would be actually able to use 
we hinted to some difficulties as we noted that several 
layers of complexities are hidden inside concepts such as the 
expected value of maintaining certain information private 
and the probability of succeeding doing so more precisely 
an agent will face three problems when comparing the 
tradeoffs implicit in equation incomplete information about all 
parameters bounded power to process all available 
information no deviation from the rational path towards 
utilitymaximization those three problems are precisely the same 
issues real people have to deal with on an everyday basis as 
they face privacy-sensitive decisions we discuss each 
problem in detail 
 incomplete information what information has the 
individual access to as she prepares to take privacy sensitive 
decisions for instance is she aware of privacy invasions and 
the associated risks what is her knowledge of the existence 
and characteristics of protective technologies 
economic transactions are often characterized by 
incomplete or asymmetric information different parties involved 
may not have the same amount of information about the 
transaction and may be uncertain about some important 
aspects of it incomplete information will affect almost all 
parameters in equation and in particular the estimation 
of costs and benefits costs and benefits associated with 
privacy protection and privacy intrusions are both 
monetary and immaterial monetary costs may for instance 
include adoption costs which are probably fixed and usage 
costs which are variable of protective technologies - if the 
individual decides to protect herself or they may include 
the financial costs associated to identity theft if the 
individual s information turns out not to have been adequately 
protected immaterial costs may include learning costs of 
a protective technology switching costs between different 
applications or social stigma when using anonymizing 
technologies and many others likewise the benefits from 
protecting or not protecting personal information may also be 
easy to quantify in monetary terms the discount you receive 
for revealing personal data or be intangible the feeling of 
protection when you send encrypted emails 
it is difficult for an individual to estimate all these 
values through information technology privacy invasions can 
be ubiquitous and invisible many of the payoffs associated 
with privacy protection or intrusion may be discovered or 
ascertained only ex post through actual experience consider 
for instance the difficulties in using privacy and encrypting 
technologies described in 
in addition the calculations implicit in equation depend 
on incomplete information about the probability 
distribution of future events some of those distributions may be 
predicted after comparable data - for example the 
probability that a certain credit card transaction will result in fraud 
today could be calculated using existing statistics the 
probability distributions of other events may be very 
difficult to estimate because the environment is too 
dynamicfor example the probability of being subject to identity theft 
 years in the future because of certain data you are 
releasing now and the distributions of some other events may be 
almost completely subjective - for example the probability 
that a new and practical form of attack on a currently 
secure cryptosystem will expose all of your encrypted personal 
communications a few years from now 
this leads to a related problem bounded rationality 
 bounded rationality is the individual able to 
calculate all the parameters relevant to her choice or is she 
limited by bounded rationality 
in our context bounded rationality refers to the inability 
to calculate and compare the magnitudes of payoffs 
associated with various strategies the individual may choose in 
privacy-sensitive situations it also refers to the inability to 
process all the stochastic information related to risks and 
probabilities of events leading to privacy costs and benefits 
 
in traditional economic theory the agent is assumed to 
have both rationality and unbounded  computational power 
to process information but human agents are unable to 
process all information in their hands and draw accurate 
conclusions from it in the scenario we consider once 
an individual provides personal information to other parties 
she literally loses control of that information that loss of 
control propagates through other parties and persists for 
unpredictable spans of time being in a position of 
information asymmetry with respect to the party with whom she 
is transacting decisions must be based on stochastic 
assessments and the magnitudes of the factors that may affect 
the individual become very difficult to aggregate calculate 
and compare 
bounded rationality will affect the 
calculation of the parameters in equation and in particular δ 
γ ve and pt the cognitive costs involved in trying to 
calculate the best strategy could therefore be so high that 
the individual may just resort to simple heuristics 
 psychological distortions eventually even if an 
individual had access to complete information and could 
appropriately compute it she still may find it difficult to 
follow the rational strategy presented in equation a vast 
body of economic and psychological literature has by now 
confirmed the impact of several forms of psychological 
distortions on individual decision making privacy seems to 
be a case study encompassing many of those distortions 
hyperbolic discounting under insurance self-control 
problems immediate gratification and others the traditional 
dichotomy between attitude and behavior observed in 
several aspects of human psychology and studied in the social 
psychology literature since and may also appear in 
the privacy space because of these distortions 
for example individuals have a tendency to discount 
 hyperbolically future costs or benefits in economics 
hyperbolic discounting implies inconsistency of personal 
preferences over time - future events may be discounted at 
different discount rates than near-term events hyperbolic 
discounting may affect privacy decisions for instance when we 
heavily discount the low probability of high future risks 
such as identity theft 
related to hyperbolic discounting 
is the tendency to underinsure oneself against certain risks 
 
in general individuals may put constraints on future 
behavior that limit their own achievement of maximum utility 
people may genuinely want to protect themselves but 
because of self-control bias they will not actually take those 
steps and opt for immediate gratification instead 
people tend to underappreciate the effects of changes in their 
states and hence falsely project their current preferences 
over consumption onto their future preferences far more 
than suggesting merely that people mispredict future tastes 
this projection bias posits a systematic pattern in these 
mispredictions which can lead to systematic errors in 
dynamicchoice environments p 
 
the negative utility coming from future potential 
misuses of somebody s personal information could be a random 
shock whose probability and scope are extremely variable 
for example a small and apparently innocuous piece of 
information might become a crucial asset or a dangerous 
liability in the right context 
 
a more rigorous description and application of hyperbolic 
discounting is provided in section 
in addition individuals suffer from optimism bias 
the misperception that one s risks are lower than those of 
other individuals under similar conditions optimism bias 
may lead us to believe that we will not be subject to privacy 
intrusions 
individuals encounter difficulties when dealing with 
cumulative risks for instance shows that while young 
smokers appreciate the long term risks of smoking they do not 
fully realize the cumulative relation between the low risks of 
each additional cigarette and the slow building up of a 
serious danger difficulties with dealing with cumulative risks 
apply to privacy because our personal information once 
released can remain available over long periods of time and 
since it can be correlated to other data the  anonymity sets 
 in which we wish to remain hidden get smaller as 
a result the whole risk associated with revealing different 
pieces of personal information is more than the sum of the 
individual risks associated with each piece of data 
also it is easier to deal with actions and effects that 
are closer to us in time actions and effects that are in 
the distant future are difficult to focus on given our limited 
foresight perspective as the foresight changes so does 
behavior even when preferences remain the same this 
phenomenon may also affects privacy decisions since the 
costs of privacy protection may be immediate but the 
rewards may be invisible absence of intrusions and spread 
over future periods of time 
to summarize whenever we face privacy sensitive 
decisions we hardly have all data necessary for an informed 
choice but even if we had we would be likely unable to 
process it and even if we could process it we may still end 
behaving against our own better judgment in what follows 
we present a model of privacy attitudes and behavior based 
on some of these findings and in particular on the plight of 
immediate gratification 
 privacy and the economics of 
immediate gratification 
the problem of immediate gratification which is related 
to the concepts of time inconsistency hyperbolic 
discounting and self-control bias is so described by o donoghue and 
rabin p a person s relative preference for 
wellbeing at an earlier date over a later date gets stronger as the 
earlier date gets closer p eople have self-control 
problems caused by a tendency to pursue immediate gratification 
in a way that their  long-run selves do not appreciate for 
example if you were given only two alternatives on monday 
you may claim you will prefer working hours on saturday 
to hours and half on sunday but as saturday comes you 
will be more likely to prefer postponing work until sunday 
this simple observation has rather important consequences 
in economic theory where time-consistency of preferences is 
the dominant model consider first the traditional model 
of utility that agents derive from consumption the model 
states that utility discounts exponentially over time 
ut 
t 
τ t 
δτ 
uτ 
in equation the cumulative utility u at time t is the 
discounted sum of all utilities from time t the present until 
time t the future δ is the discount factor with a value 
 
period period period period 
benefits from selling period 
costs from selling period 
benefits from selling period 
costs from selling period 
benefits from selling period 
costs from selling period 
table fictional expected payoffs from joining loyalty program 
between and a value of would imply that the 
individual discounts so heavily that the utility from future periods 
is worth zero today a value of would imply that the 
individual is so patient she does not discount future utilities 
the discount factor is used in economics to capture the fact 
that having say one dollar one year from now is valuable 
but not as much as having that dollar now in equation 
if all uτ were constant - for instance - and δ was 
then at time t that is now u would be worth but 
u would be worth 
modifying the traditional model of utility discounting 
and then have proposed a model which takes into 
account possible time-inconsistency of preferences consider 
equation 
ut ut ut ut δt 
ut β 
t 
τ t 
δτ 
uτ 
assume that δ β ∈ δ is the discount factor for 
intertemporal utility as in equation β is the parameter 
that captures an individual s tendency to gratify herself 
immediately a form of time-inconsistent preferences when 
β is the model maps the traditional time-consistent 
utility model and equation is identical to equation but 
when β is zero the individual does not care for anything but 
today in fact any β smaller than represents self-control 
bias 
the experimental literature has convincingly proved that 
human beings tend to have self-control problems even when 
they claim otherwise we tend to avoid and postpone 
undesirable activities even when this will imply more effort 
tomorrow and we tend to over-engage in pleasant activities 
even though this may cause suffering or reduced utility in 
the future 
this analytical framework can be applied to the study 
of privacy attitudes and behavior protecting your privacy 
sometimes means protecting yourself from a clear and present 
hassle telemarketers or people peeping through your 
window and seeing how you live - see but sometimes it 
represents something akin to getting an insurance against 
future and only uncertain risks in surveys completed at 
time t subjects asked about their attitude towards 
privacy risks may mentally consider some costs of protecting 
themselves at a later time t s and compare those to the 
avoided costs of privacy intrusions in an even more distant 
future t s n their alternatives at survey time are 
represented in equation 
min 
wrt x 
du β e cs p δs 
x e cs n i δs n 
 − x 
x is a dummy variable that can take values or it 
represents the individual s choice - which costs the 
individual opts to face the expected cost of protecting herself at 
time s e cs p in which case x or the expected costs 
of being subject to privacy intrusions at a later time s n 
e cs n i 
the individual is trying to minimize the disutility du of 
these costs with respect to x because she discounts the 
two future events with the same discount factor although 
at different times for certain values of the parameters the 
individual may conclude that paying to protect herself is 
worthy in particular this will happen when 
e cs p δs 
 e cs n i δs n 
 
now consider what happens as the moment t s comes 
now a real price should be paid in order to enjoy some form 
of protection say starting to encrypt all of your emails to 
protect yourself from future intrusions now the individual 
will perceive a different picture 
min 
wrt x 
dus δe cs p x βe cn i δn 
 − x 
note that nothing has changed in the equation certainly 
not the individual s perceived risks except time if β the 
parameter indicating the degree of self-control problems is 
less than one chances are that the individual now will 
actually choose not to protect herself this will in fact happen 
when 
δe cs p βe cn i δn 
 
note that disequalities and may be simultaneously 
met for certain β at survey time the individual 
honestly claimed she wanted to protect herself in 
principlethat is some time in the future but as she is asked to 
make an effort to protect herself right now she chooses to 
run the risk of privacy intrusion 
similar mathematical arguments can be made for the 
comparison between immediate costs with immediate benefits 
 subscribing to a  no-call list to stop telemarketers from 
harassing you at dinner and immediate costs with only 
future expected rewards insuring yourself against identity 
theft or protecting yourself from frauds by never using your 
credit card on-line particularly when expected future 
rewards or avoided risks are also intangible the immaterial 
consequences of living or not in a dossier society or the 
chilling effects or lack thereof of being under surveillance 
the reader will have noticed that we have focused on 
perceived expected costs e c rather than real costs we 
do not know the real costs and we do not claim that the 
 
individual does but we are able to show that under 
certain conditions even costs perceived as very high as during 
periods of intense privacy debate will be ignored 
we can provide some fictional numerical examples to make 
the analysis more concrete we present some scenarios 
inspired by the calculations in 
imagine an economy with just periods table each 
individual can enroll in a supermarket s loyalty program by 
revealing personal information if she does so the individual 
gets a discount of during the period of enrollment only to 
pay one unit each time thereafter because of price 
discrimination based on the information she revealed we make no 
attempt at calibrating the realism of this obviously abstract 
example the point we are focusing on is how time 
inconsistencies may affect individual behavior given the expected 
costs and benefits of certain actions 
depending on which 
period the individual chooses for  selling her data we have 
the undiscounted payoffs represented in table 
imagine that the individual is contemplating these 
options and discounting them according to equation 
suppose that δ for all types of individuals this means that 
for simplicity we do not consider intertemporal discounting 
but β for time-inconsistent individuals and β for 
everybody else the time-consistent individual will choose 
to join the program at the very last period and rip off a 
benefit of - the individual with immediate 
gratification problems for whom β will instead perceive the 
benefits from joining now or in period as equivalent 
and will join the program now thus actually making herself 
worse off 
 also suggest that in addition to the distinction 
between time-consistent individuals and individuals with 
timeinconsistent preferences we should also distinguish 
timeinconsistent individuals who are na¨ıve from those who are 
sophisticated na¨ıve time-inconsistent individuals are not 
aware of their self-control problems - for example they are 
those who always plan to start a diet next week 
sophisticated time-inconsistent individuals suffer of immediate 
gratification bias but are at least aware of their inconsistencies 
people in this category choose their behavior today correctly 
estimating their future time-inconsistent behavior 
now consider how this difference affects decisions in 
another scenario represented in table an individual is 
considering the adoption of a certain privacy enhancing 
technology it will cost her some money both to protect herself 
and not to protect herself if she decides to protect herself 
the cost will be the amount she pays - for example - for some 
technology that shields her personal information if she 
decides not to protect herself the cost will be the expected 
consequences of privacy intrusions 
we assume that both these aggregate costs increase over 
time although because of separate dynamics as time goes 
by more and more information about the individual has 
been revealed and it becomes more costly to be protected 
against privacy intrusions at the same time however 
intrusions become more frequent and dangerous 
 
one may claim that loyalty cards keep on providing 
benefits over time here we make the simplifying assumption 
that such benefits are not larger than the future costs 
incurred after having revealed one s tastes we also assume 
that the economy ends in period for all individuals 
regardless of when they chose to join the loyalty program 
in period the individual may protect herself by spending 
 or she may choose to face a risk of privacy intrusion the 
following period expected to cost in the second period 
assuming that no intrusion has yet taken place she may 
once again protect herself by spending a little more or 
she may choose to face a risk of privacy intrusion the next 
 third period expected to cost in the third period she 
could protect herself for or face an expected cost of in 
the following last period 
here too we make no attempt at calibrating the values in 
table again we focus on the different behavior driven by 
heterogeneity in time-consistency and sophistication versus 
na¨ıvete we assume that β for individuals with no 
self control problems and β for everybody else we 
assume for simplicity that δ for all 
the time-consistent individuals will obviously choose to 
protect themselves as soon as possible 
in the first period na¨ıve time-inconsistent individuals will 
compare the costs of protecting themselves then or face a 
privacy intrusion in the second period because ∗ 
 they will prefer to wait until the following period to 
protect themselves but in the second period they will be 
comparing ∗ - and so they will postpone their 
protection again they will keep on doing so facing higher 
and higher risks eventually they will risk to incur the 
highest perceived costs of privacy intrusions note again that 
we are simply assuming that individuals believe there are 
privacy risks and that they increase over time we will come 
back to this concept later on 
time-inconsistent but sophisticated individuals on the 
other side will adopt a protective technology in period 
and pay by period in fact they will correctly realize 
that if they wait till period which they are tempted to 
do because ∗ their self-control bias will lead 
them to postpone adopting the technology once more 
 because ∗ therefore they predict they would 
incur the expected cost ∗ which is larger than 
 the cost of protecting oneself in period in period 
however they correctly predict that they will not wait to protect 
themselves further than period so they wait till period 
 because ∗ at which time they will adopt a 
protective technology see also 
to summarize time-inconsistent people tend not to fully 
appreciate future risks and if na¨ıve also their inability to 
deal with them this happens even if they are aware of those 
risks and they are aware that those risks are increasing as 
we learnt from the second scenario time inconsistency can 
lead individuals to accept higher and higher risks 
individuals may tend to downplay the fact that single actions present 
low risks but their repetition forms a huge liability it is a 
deceiving aspect of privacy that its value is truly 
appreciated only after privacy itself is lost this dynamics captures 
the essence of privacy and the so-called anonymity sets 
 where each bit of information we reveal can be linked to 
others so that the whole is more than the sum of the parts 
in addition show that when costs are immediate 
time-inconsistent individuals tend to procrastinate when 
benefits are immediate they tend to preoperate in our 
context things are even more interesting because all privacy 
decisions involve at the same time costs and benefits so 
we opt against using ecash in order to save us the costs 
of switching from credit cards but we accept the risk that 
our credit card number on the internet could be used 
ma 
period period period period 
protection costs 
expected intrusion costs 
table fictional costs of protecting privacy and expected costs of privacy intrusions over time 
liciously and we give away our personal information to 
supermarkets in order to gain immediate discounts - which 
will likely turn into price discrimination in due time 
we have shown in the second scenario above how 
sophisticated but time-inconsistent individuals may choose to 
protect their information only in period sophisticated 
people with self-control problems may be at a loss sometimes 
even when compared to na¨ıve people with time 
inconsistency problems how many privacy advocates do use 
privacy enhancing technologies all the time the reasoning 
is that sophisticated people are aware of their self-control 
problems and rather than ignoring them they incorporate 
them into their decision process this may decrease their 
own incentive to behave in the optimal way now 
sophisticated privacy advocates might realize that protecting 
themselves from any possible privacy intrusion is unrealistic and 
so they may start misbehaving now and may get used to 
that a form of coherent arbitrariness this is consistent 
with the results by presented at the acm ec 
conference found that privacy advocates were also willing 
to reveal personal information in exchange for monetary 
rewards 
it is also interesting to note that these inconsistencies are 
not caused by ignorance of existing risks or confusion about 
available technologies individuals in the abstract scenarios 
we described are aware of their perceived risks and costs 
however under certain conditions the magnitude of those 
liabilities is almost irrelevant the individual will take very 
slowly increasing risks which become steps towards huge 
liabilities 
 discussion 
applying models of self-control bias and immediate 
gratification to the study of privacy decision making may offer 
a new perspective on the ongoing privacy debate we have 
shown that a model of rational privacy behavior is 
unrealistic while models based on psychological distortions offer 
a more accurate depiction of the decision process we have 
shown why individuals who genuinely would like to protect 
their privacy may not do so because of psychological 
distortions well documented in the behavioral economics 
literature we have highlighted that these distortions may affect 
not only na¨ıve individuals but also sophisticated ones 
surprisingly we have also found that these inconsistencies may 
occur when individuals perceive the risks from not 
protecting their privacy as significant 
additional uncertainties risk aversion and varying 
attitudes towards losses and gains may be confounding elements 
in our analysis empirical validation is necessary to calibrate 
the effects of different factors 
an empirical analysis may start with the comparison of 
available data on the adoption rate of privacy technologies 
that offer immediate refuge from minor but pressing privacy 
concerns for example  do not call marketing lists with 
data on the adoption of privacy technologies that offer less 
obviously perceivable protection from more dangerous but 
also less visible privacy risks for example identity theft 
insurances however only an experimental approach over 
different periods of time in a controlled environment may 
allow us to disentangle the influence of several factors surveys 
alone cannot suffice since we have shown why survey-time 
attitudes will rarely match decision-time actions an 
experimental verification is part of our ongoing research agenda 
the psychological distortions we have discussed may be 
considered in the ongoing debate on how to deal with the 
privacy problem industry self-regulation users self protection 
 through technology or other strategies or government s 
intervention the conclusions we have reached suggest that 
individuals may not be trusted to make decisions in their 
best interests when it comes to privacy this does not mean 
that privacy technologies are ineffective on the contrary 
our results by aiming at offering a more realistic model of 
user-behavior could be of help to technologists in their 
design of privacy enhancing tools however our results also 
imply that technology alone or awareness alone may not 
address the heart of the privacy problem improved 
technologies with lower costs of adoption and protection and 
more information about risks and opportunities certainly 
can help however more fundamental human behavioral 
mechanisms must also be addressed self-regulation even 
in presence of complete information and awareness may not 
be trusted to work for the same reasons a combination of 
technology awareness and regulative policies - calibrated 
to generate and enforce liabilities and incentives for the 
appropriate parties - may be needed for privacy-related welfare 
increase as in other areas of an economy see on a related 
analysis 
observing that people do not want to pay for privacy or do 
not care about privacy therefore is only a half truth people 
may not be able to act as economically rational agents when 
it comes to personal privacy and the question whether do 
consumers care is a different question from does privacy 
matter whether from an economic standpoint privacy 
ought to be protected or not is still an open question it is 
a question that involves defining specific contexts in which 
the concept of privacy is being invoked but the value of 
privacy eventually goes beyond the realms of economic 
reasoning and cost benefit analysis and ends up relating to one s 
views on society and freedom still even from a purely 
economic perspective anecdotal evidence suggest that the costs 
of privacy from spam to identity theft lost sales intrusions 
and the like are high and increasing 
 acknowledgments 
the author gratefully acknowledges carnegie mellon 
university s berkman development fund that partially 
supported this research the author also wishes to thank jens 
grossklags charis kaskiris and three anonymous referees 
for their helpful comments 
 
 references 
 a acquisti r dingledine and p syverson on the 
economics of anonymity in financial 
cryptographyfc pages - springer verlag lncs 
 
 a acquisti and j grossklags losses gains and 
hyperbolic discounting an experimental approach to 
information security attitudes and behavior in nd 
annual workshop on economics and information 
security - weis 
 a acquisti and h r varian conditioning prices on 
purchase history technical report university of 
california berkeley presented at the european 
economic association conference venice it august 
 http www heinz cmu edu  acquisti 
papers privacy pdf 
 g a akerlof the market for  lemons quality 
uncertainty and the market mechanism quarterly 
journal of economics - 
 g s becker and k m murphy a theory of rational 
addiction journal of political economy - 
 
 b d brunk understanding the privacy space first 
monday http firstmonday org issues 
issue brunk index html 
 g calzolari and a pavan optimal design of privacy 
policies technical report gremaq university of 
toulouse 
 d chaum untraceable electronic mail return 
addresses and digital pseudonyms communications 
of the acm - 
 d chaum blind signatures for untraceable payments 
in advances in cryptology - crypto pages 
 - plenum press 
 r k chellappa and r sin personalization versus 
privacy an empirical examination of the online 
consumer s dilemma in informs meeting 
 f t commission privacy online fair information 
practices in the electronic marketplace 
http www ftc gov reports privacy 
privacy pdf 
 community banker association of indiana identity 
fraud expected to triple by 
http www cbai org newsletter december 
identity fraud de htm 
 s corey professional attitudes and actual behavior 
journal of educational psychology - 
 
 c diaz s seys j claessens and b preneel 
towards measuring anonymity in p syverson and 
r dingledine editors privacy enhancing 
technologies - pet springer verlag 
 ebusinessforum com emarketer the great online 
privacy debate http www ebusinessforum 
com index asp doc id layout rich story 
 federal trade commission identity theft heads the 
ftc s top consumer fraud complaints of 
http www ftc gov opa idtheft htm 
 r gellman privacy consumers and costs - how the 
lack of privacy costs consumers and why business 
studies of privacy costs are biased and incomplete 
 
http www epic org reports dmfprivacy html 
 i -h harn k -l hui t s lee and i p l png 
online information privacy measuring the 
cost-benefit trade-off in rd international 
conference on information systems 
 harris interactive first major post- privacy 
survey finds consumers demanding companies do more 
to protect privacy public wants company privacy 
policies to be independently verified 
http www harrisinteractive com news 
allnewsbydate asp newsid 
 p jehiel and a lilico smoking today and stopping 
tomorrow a limited foresight perspective technical 
report department of economics ucla 
 jupiter research seventy percent of us consumers 
worry about online privacy but few take protective 
action http 
 www jmm com xp jmm press pr xml 
 h kunreuther causes of underinsurance against 
natural disasters geneva papers on risk and 
insurance 
 d laibson essays on hyperbolic discounting mit 
department of economics ph d dissertation 
 r lapiere attitudes versus actions social forces 
 - 
 g lowenstein t o donoghue and m rabin 
projection bias in predicting future utility technical 
report carnegie mellon university cornell university 
and university of california berkeley 
 a odlyzko privacy economics and price 
discrimination on the internet in fifth international 
conference on electronic commerce pages - 
acm 
 t o donoghue and m rabin choice and 
procrastination quartely journal of economics 
 - the page referenced in the text 
refers to the working paper version 
 r a posner an economic theory of privacy 
regulation pages - 
 r a posner the economics of privacy american 
economic review - 
 privacy rights clearinghouse nowhere to turn 
victims speak out on identity theft http 
 www privacyrights org ar idtheft htm 
 m rabin and t o donoghue the economics of 
immediate gratification journal of behavioral 
decision making - 
 a serjantov and g danezis towards an information 
theoretic metric for anonymity in p syverson and 
r dingledine editors privacy enhancing 
technologies - pet springer verlag lncs 
 
 a shostack paying for privacy consumers and 
infrastructures in nd annual workshop on 
economics and information security - weis 
 
 h a simon models of bounded rationality the mit 
press cambridge ma 
 
 p slovic what does it mean to know a cumulative 
risk adolescents perceptions of short-term and 
long-term consequences of smoking journal of 
behavioral decision making - 
 s spiekermann j grossklags and b berendt 
e-privacy in nd generation e-commerce privacy 
preferences versus actual behavior in rd acm 
conference on electronic commerce - ec pages 
 - 
 g j stigler an introduction to privacy in economics 
and politics journal of legal studies - 
 p syverson the paradoxical value of privacy in nd 
annual workshop on economics and information 
security - weis 
 c r taylor private demands and demands for 
privacy dynamic pricing and the market for customer 
information department of economics duke 
university duke economics working paper - 
 
 t vila r greenstadt and d molnar why we can t 
be bothered to read privacy policies models of 
privacy economics as a lemons market in nd annual 
workshop on economics and information 
securityweis 
 s warren and l brandeis the right to privacy 
harvard law review - 
 n d weinstein optimistic biases about personal 
risks science - 
 a whitten and j d tygar why johnny can t 
encrypt a usability evaluation of pgp in th 
usenix security symposium 
 
scouts promoters and connectors the roles of ratings 
in nearest neighbor collaborative filtering 
bharath kumar mohan 
dept of csa 
indian institute of science 
bangalore india 
mbk csa iisc ernet in 
benjamin j keller 
dept of computer science 
eastern michigan university 
ypsilanti mi usa 
bkeller emich edu 
naren ramakrishnan 
dept of computer science 
virginia tech blacksburg 
va usa 
naren cs vt edu 
abstract 
recommender systems aggregate individual user ratings into 
predictions of products or services that might interest 
visitors the quality of this aggregation process crucially 
affects the user experience and hence the effectiveness of 
recommenders in e-commerce we present a novel study that 
disaggregates global recommender performance metrics into 
contributions made by each individual rating allowing us 
to characterize the many roles played by ratings in 
nearestneighbor collaborative filtering in particular we formulate 
three roles-scouts promoters and connectors-that 
capture how users receive recommendations how items get 
recommended and how ratings of these two types are 
themselves connected resp these roles find direct uses in 
improving recommendations for users in better targeting of 
items and most importantly in helping monitor the health 
of the system as a whole for instance they can be used 
to track the evolution of neighborhoods to identify rating 
subspaces that do not contribute or contribute negatively 
to system performance to enumerate users who are in 
danger of leaving and to assess the susceptibility of the system 
to attacks such as shilling we argue that the three rating 
roles presented here provide broad primitives to manage a 
recommender system and its community 
categories and subject descriptors 
h information systems applications types of 
systems-decision support j computer applications 
social and behavioral sciences 
general terms 
algorithms human factors 
 introduction 
recommender systems have become integral to e-commerce 
providing technology that suggests products to a visitor 
based on previous purchases or rating history 
collaborative filtering a common form of recommendation predicts 
a user s rating for an item by combining other ratings of 
that user with other users ratings significant research has 
been conducted in implementing fast and accurate 
collaborative filtering algorithms designing interfaces for 
presenting recommendations to users and studying the 
robustness of these algorithms however with the 
exception of a few studies on the influence of users little 
attention has been paid to unraveling the inner workings 
of a recommender in terms of the individual ratings and the 
roles they play in making good recommendations such an 
understanding will give an important handle to monitoring 
and managing a recommender system to engineer 
mechanisms to sustain the recommender and thereby ensure its 
continued success 
our motivation here is to disaggregate global recommender 
performance metrics into contributions made by each 
individual rating allowing us to characterize the many roles 
played by ratings in nearest-neighbor collaborative filtering 
we identify three possible roles scouts to connect the user 
into the system to receive recommendations promoters to 
connect an item into the system to be recommended and 
 connectors to connect ratings of these two kinds viewing 
ratings in this way we can define the contribution of a 
rating in each role both in terms of allowing recommendations 
to occur and in terms of influence on the quality of 
recommendations in turn this capability helps support scenarios 
such as 
 situating users in better neighborhoods a user s 
ratings may inadvertently connect the user to a 
neighborhood for which the user s tastes may not be a perfect 
match identifying ratings responsible for such bad 
recommendations and suggesting new items to rate can 
help situate the user in a better neighborhood 
 targeting items recommender systems suffer from 
lack of user participation especially in cold-start 
scenarios involving newly arrived items identifying 
users who can be encouraged to rate specific items 
helps ensure coverage of the recommender system 
 monitoring the evolution of the recommender system 
and its stakeholders a recommender system is 
constantly under change growing with new users and 
 
items shrinking with users leaving the system items 
becoming irrelevant and parts of the system under 
attack tracking the roles of a rating and its evolution 
over time provides many insights into the health of the 
system and how it could be managed and improved 
these include being able to identify rating subspaces 
that do not contribute or contribute negatively to 
system performance and could be removed to 
enumerate users who are in danger of leaving or have left 
the system and to assess the susceptibility of the 
system to attacks such as shilling 
as we show the characterization of rating roles presented 
here provides broad primitives to manage a recommender 
system and its community the rest of the paper is 
organized as follows background on nearest-neighbor 
collaborative filtering and algorithm evaluation is discussed in 
section section defines and discusses the roles of a rating 
and section defines measures of the contribution of a 
rating in each of these roles in section we illustrate the use 
of these roles to address the goals outlined above 
 background 
 algorithms 
nearest-neighbor collaborative filtering algorithms either 
use neighborhoods of users or neighborhoods of items to 
compute a prediction an algorithm of the first kind is 
called user-based and one of the second kind is called 
itembased in both families of algorithms neighborhoods are 
formed by first computing the similarity between all pairs 
of users for user-based or items for item-based 
predictions are then computed by aggregating ratings which in a 
user-based algorithm involves aggregating the ratings of the 
target item by the user s neighbors and in an item-based 
algorithm involves aggregating the user s ratings of items 
that are neighbors of the target item algorithms within 
these families differ in the definition of similarity 
formation of neighborhoods and the computation of predictions 
we consider a user-based algorithm based on that defined 
for grouplens with variations from herlocker et al 
and an item-based algorithm similar to that of sarwar et 
al 
the algorithm used by resnick et al defines the 
similarity of two users u and v as the pearson correlation of 
their common ratings 
sim u v 
p 
i∈iu∩iv 
 ru i − ¯ru rv i − ¯rv 
qp 
i∈iu 
 ru i − ¯ru 
qp 
i∈iv 
 rv i − ¯rv 
 
where iu is the set of items rated by user u ru i is user u s 
rating for item i and ¯ru is the average rating of user u 
 similarly for v similarity computed in this manner is typically 
scaled by a factor proportional to the number of common 
ratings to reduce the chance of making a recommendation 
made on weak connections 
sim u v 
max iu ∩ iv γ 
γ 
· sim u v 
where γ ≈ is a constant used as a lower limit in scaling 
these new similarities are then used to define a static 
neighborhood nu for each user u consisting of the top k users 
most similar to user u a prediction for user u and item 
i is computed by a weighted average of the ratings by the 
neighbors 
pu i ¯ru 
p 
v∈v sim u v rv i − ¯rv 
p 
v∈v sim u v 
 
where v nu ∩ ui is the set of users most similar to u who 
have rated i 
the item-based algorithm we use is the one defined by 
sarwar et al in this algorithm similarity is defined as 
the adjusted cosine measure 
sim i j 
p 
u∈ui∩uj 
 ru i − ¯ru ru j − ¯ru 
qp 
u∈ui 
 ru i − ¯ru 
qp 
u∈uj 
 ru j − ¯ru 
 
where ui is the set of users who have rated item i as for 
the user-based algorithm the similarity weights are adjusted 
proportionally to the number of users that have rated the 
items in common 
sim i j 
max ui ∩ uj γ 
γ 
· sim i j 
given the similarities the neighborhood ni of an item i is 
defined as the top k most similar items for that item a 
prediction for user u and item i is computed as the weighted 
average 
pu i ¯ri 
p 
j∈j sim i j ru j − ¯rj 
p 
j∈j sim i j 
 
where j ni ∩ iu is the set of items rated by u that are 
most similar to i 
 evaluation 
recommender algorithms have typically been evaluated 
using measures of predictive accuracy and coverage 
studies on recommender algorithms notably herlocker et al 
and sarwar et al typically compute predictive accuracy 
by dividing a set of ratings into training and test sets and 
compute the prediction for an item in the test set using the 
ratings in the training set a standard measure of predictive 
accuracy is mean absolute error mae which for a test set 
t { u i } is defined as 
mae 
p 
 u i ∈t pu i − ru i 
 t 
 
coverage has a number of definitions but generally refers 
to the proportion of items that can be predicted by the 
algorithm 
a practical issue with predictive accuracy is that users 
typically are presented with recommendation lists and not 
individual numeric predictions recommendation lists are 
lists of items in decreasing order of prediction sometimes 
stated in terms of star-ratings and so predictive accuracy 
may not be reflective of the accuracy of the list so instead 
we can measure recommendation or rank accuracy which 
indicates the extent to which the list is in the correct 
order herlocker et al discuss a number of rank accuracy 
measures which range from kendall s tau to measures that 
consider the fact that users tend to only look at a prefix of 
the list kendall s tau measures the number of inversions 
when comparing ordered pairs in the true user ordering of 
 
jim 
tom 
jeff 
my cousin vinny 
the matrix 
star wars 
the mask 
figure ratings in simple movie recommender 
items and the recommended order and is defined as 
τ 
c − d 
p 
 c d tr c d tp 
 
where c is the number of pairs that the system predicts in 
the correct order d the number of pairs the system 
predicts in the wrong order tr the number of pairs in the true 
ordering that have the same ratings and tp is the 
number of pairs in the predicted ordering that have the same 
ratings a shortcoming of the tau metric is that it is 
oblivious to the position in the ordered list where the 
inversion occurs for instance an inversion toward the end 
of the list is given the same weight as one in the beginning 
one solution is to consider inversions only in the top few 
items in the recommended list or to weight inversions based 
on their position in the list 
 roles of a rating 
our basic observation is that each rating plays a 
different role in each prediction in which it is used consider a 
simplified movie recommender system with three users jim 
jeff and tom and their ratings for a few movies as shown 
in fig for this initial discussion we will not consider the 
rating values involved the recommender predicts whether 
tom will like the mask using the other already available 
ratings how this is done depends on the algorithm 
 an item-based collaborative filtering algorithm 
constructs a neighborhood of movies around the mask by 
using the ratings of users who rated the mask and 
other movies similarly e g jim s ratings of the matrix 
and the mask and jeff s ratings of star wars and the 
mask tom s ratings of those movies are then used to 
make a prediction for the mask 
 a user-based collaborative filtering algorithm would 
construct a neighborhood around tom by tracking other 
users whose rating behaviors are similar to tom s e g 
tom and jeff have rated star wars tom and jim have 
rated the matrix the prediction of tom s rating for 
the mask is then based on the ratings of jeff and tim 
although the nearest-neighbor algorithms aggregate the 
ratings to form neighborhoods used to compute predictions we 
can disaggregate the similarities to view the computation of 
a prediction as simultaneously following parallel paths of 
ratings so irrespective of the collaborative filtering 
algorithm used we can visualize the prediction of tom s rating 
of the mask as walking through a sequence of ratings in 
jim 
tom 
jeff 
the matrix 
star wars 
the mask 
q 
q q 
p 
p 
p 
figure ratings used to predict the mask for tom 
jim 
tom 
jeff 
the matrix 
star wars 
the mask 
q 
q 
q 
p p 
p 
jerry 
r 
r 
figure prediction of the mask for tom in which a 
rating is used more than once 
this example two paths were used for this prediction as 
depicted in fig p p p and q q q note that these 
paths are undirected and are all of length only the order 
in which the ratings are traversed is different between the 
item-based algorithm e g p p p q q q and the 
user-based algorithm e g p p p q q q a rating 
can be part of many paths for a single prediction as shown 
in fig where three paths are used for a prediction two 
of which follow p p p p and p r r 
predictions in a collaborative filtering algorithms may 
involve thousands of such walks in parallel each playing a part 
in influencing the predicted value each prediction path 
consists of three ratings playing roles that we call scouts 
promoters and connectors to illustrate these roles consider 
the path p p p in fig used to make a prediction of 
the mask for tom 
 the rating p tom → star wars makes a connection 
from tom to other ratings that can be used to predict 
tom s rating for the mask this rating serves as a 
scout in the bipartite graph of ratings to find a path 
that leads to the mask 
 the rating p jeff → star wars helps the system 
recommend the mask to tom by connecting the scout to 
the promoter 
 the rating p jeff → the mask allows connections to 
the mask and therefore promotes this movie to tom 
formally given a prediction pu a of a target item a for user 
u a scout for pu a is a rating ru i such that there exists a 
user v with ratings rv a and rv i for some item i a promoter 
for pu a is a rating rv a for some user v such that there exist 
ratings rv i and ru i for an item i and a connector for pu a 
 
jim 
tom 
jeff 
jerry 
my cousin vinny 
the matrix 
star wars 
the mask 
jurasic park 
figure scouts promoters and connectors 
is a rating rv i by some user v and rating i such that there 
exists ratings ru i and rv a the scouts connectors and 
promoters for the prediction of tom s rating of the mask 
are p and q p and q and p and q respectively each 
of these roles has a value in the recommender to the user 
the user s neighborhood and the system in terms of allowing 
recommendations to be made 
 roles in detail 
ratings that act as scouts tend to help the recommender 
system suggest more movies to the user though the extent 
to which this is true depends on the rating behavior of other 
users for example in fig the rating tom → star wars 
helps the system recommend only the mask to him while 
tom → the matrix helps recommend the mask jurassic 
park and my cousin vinny tom makes a connection to 
jim who is a prolific user of the system by rating the 
matrix however this does not make the matrix the best 
movie to rate for everyone for example jim is benefited 
equally by both the mask and the matrix which allow the 
system to recommend star wars to him his rating of the 
mask is the best scout for jeff and jerry s only scout is his 
rating of star wars this suggests that good scouts allow 
a user to build similarity with prolific users and thereby 
ensure they get more from the system 
while scouts represent beneficial ratings from the 
perspective of a user promoters are their duals and are of benefit 
to items in fig my cousin vinny benefits from jim s 
rating since it allows recommendations to jeff and tom 
the mask is not so dependent on just one rating since the 
ratings by jim and jeff help it on the other hand jerry s 
rating of star wars does not help promote it to any other 
user we conclude that a good promoter connects an item to 
a broader neighborhood of other items and thereby ensures 
that it is recommended to more users 
connectors serve a crucial role in a recommender system 
that is not as obvious the movies my cousin vinny and 
jurassic park have the highest recommendation potential 
since they can be recommended to jeff jerry and tom based 
on the linkage structure illustrated in fig beside the 
fact that jim rated these movies these recommendations are 
possible only because of the ratings jim → the matrix and 
jim → the mask which are the best connectors a 
connector improves the system s ability to make recommendations 
with no explicit gain for the user 
note that every rating can be of varied benefit in each of 
these roles the rating jim → my cousin vinny is a poor 
scout and connector but is a very good promoter the 
rating jim → the mask is a reasonably good scout a very 
good connector and a good promoter finally the rating 
jerry → star wars is a very good scout but is of no value 
as a connector or promoter as illustrated here a rating 
can have different value in each of the three roles in terms of 
whether a recommendation can be made or not we could 
measure this value by simply counting the number of times 
a rating is used in each role which alone would be 
helpful in characterizing the behavior of a system but we can 
also measure the contribution of each rating to the quality 
of recommendations or health of the system since every 
prediction is a combined effort of several recommendation 
paths we are interested in discerning the influence of each 
rating and hence each path in the system towards the 
system s overall error we can understand the dynamics of 
the system at a finer granularity by tracking the influence 
of a rating according to the role played the next section 
describes the approach to measuring the values of a rating 
in each role 
 contributions of ratings 
as we ve seen a rating may play different roles in different 
predictions and in each prediction contribute to the quality 
of a prediction in different ways our approach can use any 
numeric measure of a property of system health and assigns 
credit or blame to each rating proportional to its influence 
in the prediction by tracking the role of each rating in a 
prediction we can accumulate the credit for a rating in each 
of the three roles and also track the evolution of the roles 
of rating over time in the system 
this section defines the methodology for computing the 
contribution of ratings by first defining the influence of a 
rating and then instantiating the approach for predictive 
accuracy and then rank accuracy we also demonstrate how 
these contributions can be aggregated to study the 
neighborhood of ratings involved in computing a user s 
recommendations note that although our general formulation for rating 
influence is algorithm independent due to space 
considerations we present the approach for only item-based 
collaborative filtering the definition for user-based algorithms is 
similar and will be presented in an expanded version of this 
paper 
 influence of ratings 
recall that an item-based approach to collaborative 
filtering relies on building item neighborhoods using the 
similarity of ratings by the same user as described earlier 
similarity is defined by the adjusted cosine measure equations 
and a set of the top k neighbors is maintained for all 
items for space and computational efficiency a prediction 
of item i for a user u is computed as the weighted deviation 
from the item s mean rating as shown in equation the 
list of recommendations for a user is then the list of items 
sorted in descending order of their predicted values 
we first define impact a i j the impact a user a has in 
determining the similarity between two items i and j this 
is the change in the similarity between i and j when a s 
rating is removed and is defined as 
impact a i j 
 sim i j − sim¯a i j 
p 
w∈cij 
 sim i j − sim ¯w i j 
where cij {u ∈ u ∃ ru i ru j ∈ r u } is the set of coraters 
 
of items i and j users who rate both i and j r u is the set 
of ratings provided by user u and sim¯a i j is the similarity 
of i and j when the ratings of user a are removed 
sim¯a i j 
p 
v∈u\{a} ru i − ¯ru ru j − ¯ru 
qp 
u∈u\{a} ru i − ¯ru 
qp 
u∈u\{a} ru j − ¯ru 
 
and adjusted for the number of raters 
sim¯a i j 
max ui ∩ uj − γ 
γ 
· sim i j 
if all coraters of i and j rate them identically we define the 
impact as 
impact a i j 
 
 cij 
since 
p 
w∈cij 
 sim i j − sim ¯w i j 
the influence of each path u j v i ru j rv j rv i in 
the prediction of pu i is given by 
influence u j v i 
sim i j 
p 
l∈ni∩iu 
sim i l 
· impact v i j 
it follows that the sum of influences over all such paths for 
a given set of endpoints is 
 role values for predictive accuracy 
the value of a rating in each role is computed from the 
influence depending on the evaluation measure employed 
here we illustrate the approach using predictive accuracy as 
the evaluation metric 
in general the goodness of a prediction decides whether 
the ratings involved must be credited or discredited for their 
role for predictive accuracy the error in prediction e 
 pu i − ru i is mapped to a comfort level using a mapping 
function m e anecdotal evidence suggests that users are 
unable to discern errors less than for a rating scale of 
to and so an error less than is considered 
acceptable but anything larger is not we hence define m e as 
 − e binned to an appropriate value in − − 
for each prediction pu i m e is attributed to all the paths 
that assisted the computation of pu i proportional to their 
influences this tribute m e ∗influence u j v i is in turn 
inherited by each of the ratings in the path ru j rv j rv i 
with the credit blame accumulating to the respective roles 
of ru j as a scout rv j as a connector and rv i as a 
promoter in other words the scout value sf ru j the 
connector value cf rv j and the promoter value pf rv i are 
all incremented by the tribute amount over a large 
number of predictions scouts that have repeatedly resulted in 
big error rates have a big negative scout value and vice 
versa similarly with the other roles every rating is thus 
summarized by its triple sf cf pf 
 role values for rank accuracy 
we now define the computation of the contribution of 
ratings to observed rank accuracy for this computation we 
must know the user s preference order for a set of items for 
which predictions can be computed we assume that we 
have a test set of the users ratings of the items presented 
in the recommendation list for every pair of items rated 
by a user in the test data we check whether the predicted 
order is concordant with his preference we say a pair i j 
is concordant with error whenever one of the following 
holds 
 if ru i ru j then pu i − pu j 
 if ru i ru j then pu i − pu j or 
 if ru i ru j then pu i − pu j ≤ 
similarly a pair i j is discordant with error if it is not 
concordant our experiments described below use an error 
tolerance of 
all paths involved in the prediction of the two items in 
a concordant pair are credited and the paths involved in 
a discordant pair are discredited the credit assigned to a 
pair of items i j in the recommendation list for user u is 
computed as 
c i j 
 
t 
t 
· 
c d 
if i j are concordant 
− t 
t 
· 
c d 
if i j are discordant 
 
where t is the number of items in the user s test set whose 
ratings could be predicted t is the number of items rated 
by user u in the test set c is the number of concordances 
and d is the number of discordances the credit c is then 
divided among all paths responsible for predicting pu i and 
pu j proportional to their influences we again add the role 
values obtained from all the experiments to form a triple 
 sf cf pf for each rating 
 aggregating rating roles 
after calculating the role values for individual ratings we 
can also use these values to study neighborhoods and the 
system here we consider how we can use the role values 
to characterize the health of a neighborhood consider the 
list of top recommendations presented to a user at a 
specific point in time the collaborative filtering algorithm 
traversed many paths in his neighborhood through his scouts 
and other connectors and promoters to make these 
recommendations we call these ratings the recommender 
neighborhood of the user the user implicitly chooses this 
neighborhood of ratings through the items he rates apart from 
the collaborative filtering algorithm the health of this 
neighborhood completely influences a user s satisfaction with the 
system we can characterize a user s recommender 
neighborhood by aggregating the individual role values of the 
ratings involved weighted by the influence of individual ratings 
in determining his recommended list different sections of 
the user s neighborhood wield varied influence on his 
recommendation list for instance ratings reachable through 
highly rated items have a bigger say in the recommended 
items 
our aim is to study the system and classify users with 
respect to their positioning in a healthy or unhealthy 
neighborhood a user can have a good set of scouts but may 
be exposed to a neighborhood with bad connectors and 
promoters he can have a good neighborhood but his bad 
scouts may ensure the neighborhood s potential is rendered 
useless we expect that users with good scouts and good 
neighborhoods will be most satisfied with the system in the 
future 
a user s neighborhood is characterized by a triple that 
represents the weighted sum of the role values of individual 
ratings involved in making recommendations consider a 
user u and his ordered list of recommendations l an item i 
 
in the list is weighted inversely as k i depending on its 
position in the list in our studies we use k i 
p 
position i 
several paths of ratings ru j rv j rv i are involved in 
predicting pu i which ultimately decides its position in l each 
with influence u j v i 
the recommender neighborhood of a user u is 
characterized by the triple sfn u cfn u pfn u where 
sfn u 
x 
i∈l 
p 
 ru j rv j rv i sf ru j influence u j v i 
k i 
 
cfn u and pfn u are defined similarly this triple 
estimates the quality of u s recommendations based on the past 
track record of the ratings involved in their respective roles 
 experimentation 
as we have seen we can assign role values to each rating 
when evaluating a collaborative filtering system in this 
section we demonstrate the use of this approach to our overall 
goal of defining an approach to monitor and manage the 
health of a recommender system through experiments done 
on the movielens million rating dataset in particular we 
discuss results relating to identifying good scouts 
promoters and connectors the evolution of rating roles and the 
characterization of user neighborhoods 
 methodology 
our experiments use the movielens million rating dataset 
which consists of ratings by users of movies the 
ratings are in the range to and are labeled with the time 
the rating was given as discussed before we consider only 
the item-based algorithm here with item neighborhoods of 
size and due to space considerations only present role 
value results for rank accuracy 
since we are interested in the evolution of the rating role 
values over time the model of the recommender system is 
built by processing ratings in their arrival order the 
timestamping provided by movielens is hence crucial for the 
analyses presented here we make assessments of rating roles at 
intervals of ratings and processed the first 
ratings in the dataset giving rise to snapshots we 
incrementally update the role values as the time ordered 
ratings are merged into the model to keep the experiment 
computationally manageable we define a test dataset for 
each user as the time ordered ratings are merged into the 
model we label a small randomly selected percentage 
as test data at discrete epochs i e after processing every 
 ratings we compute the predictions for the ratings 
in the test data and then compute the role values for the 
ratings used in the predictions one potential criticism of 
this methodology is that the ratings in the test set are never 
evaluated for their roles we overcome this concern by 
repeating the experiment using different random seeds the 
probability that every rating is considered for evaluation is 
then considerably high − n 
 where n is the number 
of times the experiment is repeated with different random 
seeds the results here are based on n repetitions 
the item-based collaborative filtering algorithm s 
performance was ordinary with respect to rank accuracy fig 
shows a plot of the precision and recall as ratings were 
merged in time order into the model the recall was always 
high but the average precision was just about 
 
 
 
 
 
 
 
 
 
 
 
 
ratings merged into model 
value 
precision 
recall 
figure precision and recall for the item-based 
collaborative filtering algorithm 
 inducing good scouts 
the ratings of a user that serve as scouts are those that 
allow the user to receive recommendations we claim that 
users with ratings that have respectable scout values will 
be happier with the system than those with ratings with 
low scout values note that the item-based algorithm 
discussed here produces recommendation lists with nearly half 
of the pairs in the list discordant from the user s preference 
whether all of these discordant pairs are observable by the 
user is unclear however certainly this suggests that there 
is a need to be able to direct users to items whose ratings 
would improve the lists 
the distribution of the scout values for most users 
ratings are gaussian with mean zero fig shows the 
frequency distribution of scout values for a sample user at a 
given snapshot we observe that a large number of ratings 
never serve as scouts for their users a relatable scenario 
is when amazon s recommender makes suggestions of books 
or items based on other items that were purchased as gifts 
with simple relevance feedback from the user such ratings 
can be isolated as bad scouts and discounted from future 
predictions removing bad scouts was found to be 
worthwhile for individual users but the overall performance 
improvement was only marginal 
an obvious question is whether good scouts can be formed 
by merely rating popular movies as suggested by rashid et 
al they show that a mix of popularity and rating 
entropy identifies the best items to suggest to new users 
when evaluated using mae following their intuition we 
would expect to see a higher correlation between 
popularityentropy and good scouts we measured the pearson 
correlation coefficient between aggregated scout values for a movie 
with the popularity of a movie number of times it is rated 
and with its popularity variance measure at different 
snapshots of the system note that the scout values were initially 
anti-correlated with popularity fig but became 
moderately correlated as the system evolved both popularity and 
popularity variance performed similarly a possible 
explanation is that there has been insufficient time for the popular 
movies to accumulate ratings 
 
- 
 
 
 
 
 
 
 
- - - - 
scout value 
frequency 
figure distribution of scout values for a sample 
user 
- 
- 
 
 
 
 
 
 
popularity 
pop var 
figure correlation between aggregated scout 
value and item popularity computed at different 
intervals 
- 
- 
- 
 
 
 
 
 
 
 
figure correlation between aggregated promoter 
value and user prolificity computed at different 
intervals 
table movies forming the best scouts 
best scouts conf pop 
being john malkovich 
star wars episode iv - a new hope 
princess bride the 
sixth sense the 
matrix the 
ghostbusters 
casablanca 
insider the 
american beauty 
terminator judgment day 
fight club 
shawshank redemption the 
run lola run lola rennt 
terminator the 
usual suspects the 
aliens 
north by northwest 
fugitive the 
end of days 
raiders of the lost ark 
schindler s list 
back to the future 
toy story 
alien 
abyss the 
 a space odyssey 
dogma 
little mermaid the 
table movies forming the worst scouts 
worst scouts conf pop 
harold and maude 
grifters the 
sting the 
godfather part iii the 
lawrence of arabia 
high noon 
women on the verge of a 
grapes of wrath the 
duck soup 
arsenic and old lace 
midnight cowboy 
to kill a mockingbird 
four weddings and a funeral 
good the bad and the ugly the 
it s a wonderful life 
player the 
jackie brown 
boat the das boot 
manhattan 
truth about cats dogs the 
ghost 
lone star 
big chill the 
 
by studying the evolution of scout values we can identify 
movies that consistently feature in good scouts over time 
we claim these movies will make viable scouts for other 
users we found the aggregated scout values for all movies 
in intervals of ratings each a movie is said to induce 
a good scout if the movie was in the top of the sorted 
list and to induce a bad scout if it was in bottom of 
the same list movies appearing consistently high over time 
are expected to remain up there in the future the effective 
confidence in a movie m is 
cm 
tm − bm 
n 
 
where tm is the number of times it appeared in the top 
 bm the number of times it appeared in the bottom 
 and n is the number of intervals considered using 
this measure the top few movies expected to induce the 
best scouts are shown in table movies that would be bad 
scout choices are shown in table with their associated 
confidences the popularities of the movies are also displayed 
although more popular movies appear in the list of good 
scouts these tables show that a blind choice of scout based 
on popularity alone can be potentially dangerous 
interestingly the best scout- being john malkovich -is about a 
puppeteer who discovers a portal into a movie star a movie 
that has been described variously on amazon com as  makes 
you feel giddy  seriously weird  comedy with depth  silly 
 strange and  inventive indicating whether someone likes 
this movie or not goes a long way toward situating the user 
in a suitable neighborhood with similar preferences 
on the other hand several factors may have made a movie 
a bad scout like the sharp variance in user preferences in 
the neighborhood of a movie two users may have the 
same opinion about lawrence of arabia but they may 
differ sharply about how they felt about the other movies they 
saw bad scouts ensue when there is deviation in behavior 
around a common synchronization point 
 inducing good promoters 
ratings that serve to promote items in a collaborative 
filtering system are critical to allowing a new item be 
recommended to users so inducing good promoters is important 
for cold-start recommendation we note that the frequency 
distribution of promoter values for a sample movie s ratings 
is also gaussian similar to fig this indicates that the 
promotion of a movie is benefited most by the ratings of a 
few users and are unaffected by the ratings of most users 
we find a strong correlation between a user s number of 
ratings and his aggregated promoter value fig depicts the 
evolution of the pearson correlation co-efficient between the 
prolificity of a user number of ratings versus his 
aggregated promoter value we expect that conspicuous shills 
by recommending wrong movies to users will be 
discredited with negative aggregate promoter values and should be 
identifiable easily 
given this observation the obvious rule to follow when 
introducing a new movie is to have it rated directly by 
prolific users who posses high aggregated promoter values a 
new movie is thus cast into the neighborhood of many other 
movies improving its visibility note though that a user 
may have long stopped using the system tracking 
promoter values consistently allows only the most active recent 
users to be considered 
 inducing good connectors 
given the way scouts connectors and promoters are 
characterized it follows that the movies that are part of the best 
scouts are also part of the best connectors similarly the 
users that constitute the best promoters are also part of the 
best connectors good connectors are induced by 
ensuring a user with a high promoter value rates a movie with a 
high scout value in our experiments we find that a rating s 
longest standing role is often as a connector a rating with a 
poor connector value is often seen due to its user being a bad 
promoter or its movie being a bad scout such ratings can 
be removed from the prediction process to bring marginal 
improvements to recommendations in some selected 
experiments we observed that removing a set of badly behaving 
connectors helped improve the system s overall performance 
by the effect was even higher on a few select users 
who observed an improvement of above in precision 
without much loss in recall 
 monitoring the evolution of rating roles 
one of the more significant contributions of our work is 
the ability to model the evolution of recommender systems 
by studying the changing roles of ratings over time the role 
and value of a rating can change depending on many factors 
like user behavior redundancy shilling effects or 
properties of the collaborative filtering algorithm used studying 
the dynamics of rating roles in terms of transitions between 
good bad and negligible values can provide insights into the 
functioning of the recommender system we believe that a 
continuous visualization of these transitions will improve the 
ability to manage a recommender system 
we classify different rating states as good bad or 
negligible consider a user who has rated movies in a 
particular interval of which are part of the test set if a 
scout has a value greater than it indicates that it is 
uniquely involved in at least concordant predictions which 
we will say is good thus a threshold of is chosen to 
bin a rating as good bad or negligible in terms of its scout 
connector and promoter value for instance a rating r at 
time t with role value triple − is classified as 
 scout connector promoter − where indicates good 
 indicates negligible and − indicates bad 
the positive credit held by a rating is a measure of its 
contribution to the betterment of the system and the discredit 
is a measure of its contribution to the detriment of the 
system even though the positive roles and the negative roles 
make up a very small percentage of all ratings their 
contribution supersedes their size for example even though only 
 of all ratings were classified as good scouts they hold 
 of all positive credit in the system similarly the bad 
scouts were just of all ratings but hold of all 
discredit note that good and bad scouts together comprise 
only of the ratings implying that the 
majority of the ratings are negligible role players as scouts 
 more on this later likewise good connectors were 
of the system and hold of all positive credit the bad 
connectors of the system hold of all discredit 
good promoters of the system hold of all credit 
while bad promoters hold of all discredit this 
reiterates that a few ratings influence most of the system s 
performance hence it is important to track transitions 
between them regardless of their small numbers 
 
across different snapshots a rating can remain in the 
same state or change a good scout can become a bad scout 
a good promoter can become a good connector good and 
bad scouts can become vestigial and so on it is not 
practical to expect a recommender system to have no ratings in 
bad roles however it suffices to see ratings in bad roles 
either convert to good or vestigial roles similarly observing 
a large number of good roles become bad ones is a sign of 
imminent failure of the system 
we employ the principle of non-overlapping episodes 
to count such transitions a sequence such as 
 → → → 
is interpreted as the transitions 
 
 
 
instead of 
 
 
 
see for further details about this counting procedure 
thus a rating can be in one of possible states and there 
are about 
possible transitions we make a further 
simplification and utilize only states indicating whether the 
rating is a scout promoter or connector and whether it 
has a positive negative or negligible role ratings that 
serve multiple purposes are counted using multiple episode 
instantiations but the states themselves are not duplicated 
beyond the restricted states in this model a transition 
such as is counted as 
 scout scout 
 scout connector 
 scout promoter 
 connector scout 
 connector scout 
 connector promoter 
 promoter scout 
 promoter connector 
 promoter promoter 
of these transitions like px q where p q x ∈ 
{ −} are considered uninteresting and only the rest are 
counted 
fig depicts the major transitions counted while 
processing the first ratings from the movielens dataset 
only transitions with frequency greater than or equal to 
are shown the percentages for each state indicates the 
number of ratings that were found to be in those states 
we consider transitions from any state to a good state as 
healthy from any state to a bad state as unhealthy and 
from any state to a vestigial state as decaying 
from fig we can observe 
 the bulk of the ratings have negligible values 
irrespective of their role the majority of the transitions 
involve both good and bad ratings becoming 
negligible 
scout 
 
 
scout 
scout 
 
connector 
 
 
connector 
connector 
 
promoter 
 
 
promoter 
promoter 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
healthy 
unhealthy 
decaying 
figure transitions among rating roles 
 the number of good ratings is comparable to the bad 
ratings and ratings are seen to switch states often 
except in the case of scouts see below 
 the negative and positive scout states are not 
reachable through any transition indicating that these 
ratings must begin as such and cannot be coerced into 
these roles 
 good promoters and good connectors have a much 
longer survival period than scouts transitions that 
recur to these states have frequencies of and 
when compared to just for scouts good 
connectors are the slowest to decay whereas good scouts 
decay the fastest 
 healthy percentages are seen on transitions between 
promoters and connectors as indicated earlier there 
are hardly any transitions from promoters connectors 
to scouts this indicates that over the long run a 
user s rating is more useful to others movies or other 
users than to the user himself 
 the percentages of healthy transitions outweigh the 
unhealthy ones - this hints that the system is healthy 
albeit only marginally 
note that these results are conditioned by the static nature 
of the dataset which is a set of ratings over a fixed window 
of time however a diagram such as fig is clearly useful 
for monitoring the health of a recommender system for 
instance acceptable limits can be imposed on different types 
of transitions and if a transition fails to meet the 
threshold the recommender system or a part of it can be brought 
under closer scrutiny furthermore the role state transition 
diagram would also be the ideal place to study the effects of 
shilling a topic we will consider in future research 
 characterizing neighborhoods 
earlier we saw that we can characterize the neighborhood 
of ratings involved in creating a recommendation list l for 
 
a user in our experiment we consider lists of length 
and sample the lists of about of users through the 
evolution of the model at intervals of ratings each and 
compute their neighborhood characteristics to simplify our 
presentation we consider the percentage of the sample that 
fall into one of the following categories 
 inactive user sfn u 
 good scouts good neighborhood 
 sfn u ∧ cfn u ∧ pfn u 
 good scouts bad neighborhood 
 sfn u ∧ cfn u ∨ pfn u 
 bad scouts good neighborhood 
 sfn u ∧ cfn u ∧ pfn u 
 bad scouts bad neighborhood 
 sfn u ∧ cfn u ∨ pfn u 
from our sample set of users we found that users 
were inactive of the remaining users we found users 
had good scouts and a good neighborhood had bad scouts 
and a good neighborhood had good scouts and a bad 
neighborhood and had bad scouts and a bad 
neighborhood thus we conjecture that users are in 
danger of leaving the system 
as a remedy users with bad scouts and a good 
neighborhood can be asked to reconsider rating of some movies 
hoping to improve the system s recommendations the 
system can be expected to deliver more if they engineer some 
good scouts users with good scouts and a bad 
neighborhood are harder to address this situation might entail 
selectively removing some connector-promoter pairs that are 
causing the damage handling users with bad scouts and 
bad neighborhoods is a more difficult challenge 
such a classification allows the use of different strategies 
to better a user s experience with the system depending on 
his context in future work we intend to conduct field 
studies and study the improvement in performance of different 
strategies for different contexts 
 conclusions 
to further recommender system acceptance and 
deployment we require new tools and methodologies to manage 
an installed recommender and develop insights into the roles 
played by ratings a fine-grained characterization in terms 
of rating roles such as scouts promoters and connectors 
as done here helps such an endeavor although we have 
presented results on only the item-based algorithm with list 
rank accuracy as the metric the same approach outlined 
here applies to user-based algorithms and other metrics 
in future research we plan to systematically study the 
many algorithmic parameters tolerances and cutoff 
thresholds employed here and reason about their effects on the 
downstream conclusions we also aim to extend our 
formulation to other collaborative filtering algorithms study 
the effect of shilling in altering rating roles conduct field 
studies and evaluate improvements in user experience by 
tweaking ratings based on their role values finally we plan 
to develop the idea of mining the evolution of rating role 
patterns into a reporting and tracking system for all aspects 
of recommender system health 
 references 
 cosley d lam s albert i konstan j and 
riedl j is seeing believing how recommender 
system interfaces affect user s opinions in proc 
chi pp - 
 herlocker j l konstan j a borchers a 
and riedl j an algorithmic framework for 
performing collaborative filtering in proc sigir 
 pp - 
 herlocker j l konstan j a terveen 
l g and riedl j t evaluating collaborative 
filtering recommender systems acm transactions 
on information systems vol pp - 
 konstan j a personal communication 
 lam s k and riedl j shilling recommender 
systems for fun and profit in proceedings of the th 
international world wide web conference 
acm press pp - 
 laxman s sastry p s and unnikrishnan 
k p discovering frequent episodes and learning 
hidden markov models a formal connection ieee 
transactions on knowledge and data engineering 
vol - 
 mclaughlin m r and herlocker j l a 
collaborative filtering algorithm and evaluation 
metric that accurately model the user experience in 
proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pp - 
 o mahony m hurley n j kushmerick n 
and silvestre g collaborative recommendation 
a robustness analysis acm transactions on 
internet technology vol nov pp - 
 rashid a m albert i cosley d lam s 
mcnee s konstan j a and riedl j getting 
to know you learning new user preferences in 
recommender systems in proceedings of the 
conference on intelligent user interfaces iui 
 pp - 
 rashid a m karypis g and riedl j 
influence in ratings-based recommender systems 
an algorithm-independent approach in proc of the 
siam international conference on data mining 
 
 resnick p iacovou n sushak m 
bergstrom p and riedl j grouplens an 
open architecture for collaborative filtering of 
netnews in proceedings of the conference on 
computer supported collaborative work cscw 
 acm press pp - 
 sarwar b karypis g konstan j and reidl 
j item-based collaborative filtering 
recommendation algorithms in proceedings of the 
tenth international world wide web conference 
 www pp - 
 schein a popescu a ungar l and 
pennock d methods and metrics for cold-start 
recommendation in proc sigir 
pp - 
 
communication complexity of common voting rules∗ 
vincent conitzer 
carnegie mellon university 
 forbes avenue 
pittsburgh pa usa 
conitzer cs cmu edu 
tuomas sandholm 
carnegie mellon university 
 forbes avenue 
pittsburgh pa usa 
sandholm cs cmu edu 
abstract 
we determine the communication complexity of the 
common voting rules the rules sorted by their communication 
complexity from low to high are plurality plurality with 
runoff single transferable vote stv condorcet approval 
bucklin cup maximin borda copeland and ranked pairs 
for each rule we first give a deterministic communication 
protocol and an upper bound on the number of bits 
communicated in it then we give a lower bound on even the 
nondeterministic communication requirements of the 
voting rule the bounds match for all voting rules except stv 
and maximin 
categories and subject descriptors 
f theory of computation analysis of algorithms 
and problem complexity j computer applications 
social and behavioral sciences-economics 
general terms 
algorithms economics theory 
 introduction 
one key factor in the practicality of any preference 
aggregation rule is its communication burden to successfully 
aggregate the agents preferences it is usually not necessary 
for all the agents to report all of their preference information 
clever protocols that elicit the agents preferences partially 
and sequentially have the potential to dramatically reduce 
the required communication this has at least the following 
advantages 
 it can make preference aggregation feasible in settings 
where the total amount of preference information is 
too large to communicate 
 even when communicating all the preference 
information is feasible reducing the communication 
requirements lessens the burden placed on the agents this is 
especially true when the agents rather than knowing 
all their preferences in advance need to invest effort 
 such as computation or information gathering to 
determine their preferences 
 it preserves some of the agents privacy 
most of the work on reducing the communication burden 
in preference aggregation has focused on resource allocation 
settings such as combinatorial auctions in which an 
auctioneer auctions off a number of possibly distinct items 
in a single event because in a combinatorial auction 
bidders can have separate valuations for each of an 
exponential number of possible bundles of items this is a setting 
in which reducing the communication burden is especially 
crucial this can be accomplished by supplementing the 
auctioneer with an elicitor that incrementally elicits parts 
of the bidders preferences on an as-needed basis based on 
what the bidders have revealed about their preferences so 
far as suggested by conen and sandholm for example 
the elicitor can ask for a bidder s value for a specific bundle 
 value queries which of two bundles the bidder prefers 
 order queries which bundle he ranks kth or what the rank of 
a given bundle is rank queries which bundle he would 
purchase given a particular vector of prices demand queries 
etc -until at least the final allocation can be determined 
experimentally this yields drastic savings in preference 
revelation furthermore if the agents valuation functions 
are drawn from certain natural subclasses the elicitation 
problem can be solved using only polynomially many queries 
even in the worst case for a review 
of preference elicitation in combinatorial auctions see 
ascending combinatorial auctions are a well-known special 
form of preference elicitation where the elicitor asks demand 
queries with increasing prices finally resource 
 
allocation problems have also been studied from a 
communication complexity viewpoint thereby deriving lower bounds 
on the required communication for example nisan and 
segal show that exponential communication is required even 
to obtain a surplus greater than that obtained by 
auctioning off all objects as a single bundle segal also studies 
social choice rules in general and shows that for a large 
class of social choice rules supporting budget sets must be 
revealed such that if every agent prefers the same outcome 
in her budget set this proves the optimality of that 
outcome segal then uses this characterization to prove bounds 
on the communication required in resource allocation as well 
as matching settings 
in this paper we will focus on the communication 
requirements of a generally applicable subclass of social choice 
rules commonly known as voting rules in a voting setting 
there is a set of candidate outcomes over which the voters 
express their preferences by submitting a vote typically 
a ranking of the candidates and the winner that is the 
chosen outcome is determined based on these votes the 
communication required by voting rules can be large either 
because the number of voters is large such as for example 
in national elections or because the number of candidates 
is large for example the agents can vote over allocations of 
a number of resources or both prior work has studied 
elicitation in voting studying how computationally hard it 
is to decide whether a winner can be determined with the 
information elicited so far as well as how hard it is to find the 
optimal sequence of queries given perfect suspicions about 
the voters preferences in addition that paper discusses 
strategic game-theoretic issues introduced by elicitation 
in contrast in this paper we are concerned with the 
worst-case number of bits that must be communicated to 
execute a given voting rule when nothing is known in advance 
about the voters preferences we determine the 
communication complexity of the common voting rules for each rule 
we first give an upper bound on the deterministic 
communication complexity by providing a communication protocol 
for it and analyzing how many bits need to be transmitted 
in this protocol segal s results do not apply to most 
voting rules because most voting rules are not 
intersectionmonotonic or even monotonic 
 for many of the voting 
rules under study it turns out that one cannot do better 
than simply letting each voter immediately communicate all 
her potentially relevant information however for some 
rules such as plurality with runoff stv and cup there is 
a straightforward multistage communication protocol that 
with some analysis can be shown to significantly outperform 
the immediate communication of all potentially relevant 
information finally for some rules such as the condorcet 
and bucklin rules we need to introduce a more complex 
communication protocol to achieve the best possible upper 
 
for two of the rules that we study that are 
intersectionmonotonic namely the approval and condorcet rules 
segal s results can in fact be used to give alternative proofs 
of our lower bounds we only give direct proofs for these 
rules here because these direct proofs are among the 
easier ones in this paper the alternative proofs are nontrivial 
even given segal s results and a space constraint applies 
however we hope to also include the alternative proofs in a 
later version 
bound after obtaining the upper bounds we show that 
they are tight by giving matching lower bounds on even the 
nondeterministic communication complexity of each voting 
rule there are two exceptions stv for which our upper 
and lower bounds are apart by a factor log m and maximin 
for which our best deterministic upper bound is also a factor 
log m above the nondeterministic lower bound although 
we give a nondeterministic upper bound that matches the 
lower bound 
 review of voting rules 
in this section we review the common voting rules that 
we study in this paper a voting rule 
is a function 
mapping a vector of the n voters votes i e preferences over 
candidates to one of the m candidates the winner in the 
candidate set c in some cases such as the condorcet rule 
the rule may also declare that no winner exists we do not 
concern ourselves with what happens in case of a tie 
between candidates our lower bounds hold regardless of how 
ties are broken and the communication protocols used for 
our upper bounds do not attempt to break the ties all of 
the rules that we study are rank-based rules which means 
that a vote is defined as an ordering of the candidates with 
the exception of the plurality rule for which a vote is a 
single candidate and the approval rule for which a vote is a 
subset of the candidates 
we will consider the following voting rules for rules that 
define a score the candidate with the highest score wins 
 scoring rules let α α αm be a vector of 
integers such that α ≥ α ≥ αm for each voter a 
candidate receives α points if it is ranked first by the voter 
α if it is ranked second etc the score sα of a candidate 
is the total number of points the candidate receives the 
borda rule is the scoring rule with α m− m− 
the plurality rule is the scoring rule with α 
 single transferable vote stv the rule proceeds through 
a series of m − rounds in each round the candidate with 
the lowest plurality score that is the least number of voters 
ranking it first among the remaining candidates is 
eliminated and each of the votes for that candidate transfer 
to the next remaining candidate in the order given in that 
vote the winner is the last remaining candidate 
 plurality with run-off in this rule a first round 
eliminates all candidates except the two with the highest plurality 
scores votes are transferred to these as in the stv rule 
and a second round determines the winner from these two 
 approval each voter labels each candidate as either 
approved or disapproved the candidate approved by the 
greatest number of voters wins 
 condorcet for any two candidates i and j let n i j be 
the number of voters who prefer i to j if there is a candidate 
i that is preferred to any other candidate by a majority of 
the voters that is n i j n j i for all j i-that is i 
wins every pairwise election then candidate i wins 
 
the term voting protocol is often used to describe the same 
concept but we seek to draw a sharp distinction between 
the rule mapping preferences to outcomes and the 
communication elicitation protocol used to implement this rule 
 
 maximin aka simpson the maximin score of i is 
s i minj i n i j -that is i s worst performance in a 
pairwise election the candidate with the highest maximin 
score wins 
 copeland for any two distinct candidates i and j let 
c i j if n i j n j i c i j if n i j 
n j i and c i j if n i j n j i the copeland 
score of candidate i is s i j i c i j 
 cup sequential binary comparisons the cup rule is 
defined by a balanced 
binary tree t with one leaf per 
candidate and an assignment of candidates to leaves each leaf 
gets one candidate each non-leaf node is assigned the 
winner of the pairwise election of the node s children the 
candidate assigned to the root wins 
 bucklin for any candidate i and integer l let b i l 
be the number of voters that rank candidate i among the 
top l candidates the winner is arg mini min{l b i l 
n } that is if we say that a voter approves her top 
l candidates then we repeatedly increase l by until some 
candidate is approved by more than half the voters and this 
candidate is the winner 
 ranked pairs this rule determines an order on all 
the candidates and the winner is the candidate at the top 
of this order sort all ordered pairs of candidates i j by 
n i j the number of voters who prefer i to j starting 
with the pair i j with the highest n i j we lock in 
the result of their pairwise election i j then we move 
to the next pair and we lock the result of their pairwise 
election we continue to lock every pairwise result that does 
not contradict the ordering established so far 
we emphasize that these definitions of voting rules do not 
concern themselves with how the votes are elicited from the 
voters all the voting rules including those that are 
suggestively defined in terms of rounds are in actuality just 
functions mapping the vector of all the voters votes to a 
winner nevertheless there are always many different ways 
of eliciting the votes or the relevant parts thereof from the 
voters for example in the plurality with runoff rule one 
way of eliciting the votes is to ask every voter to declare her 
entire ordering of the candidates up front alternatively we 
can first ask every voter to declare only her most preferred 
candidate then we will know the two candidates in the 
runoff and we can ask every voter which of these two 
candidates she prefers thus we distinguish between the voting 
rule the mapping from vectors of votes to outcomes and the 
communication protocol which determines how the relevant 
parts of the votes are actually elicited from the voters the 
goal of this paper is to give efficient communication 
protocols for the voting rules just defined and to prove that there 
do not exist any more efficient communication protocols 
it is interesting to note that the choice of the 
communication protocol may affect the strategic behavior of the voters 
multistage communication protocols may reveal to the 
voters some information about how the other voters are voting 
 for example in the two-stage communication protocol just 
given for plurality with runoff in the second stage voters 
 
balanced means that the difference in depth between two 
leaves can be at most one 
will know which two candidates have the highest plurality 
scores in general when the voters receive such 
information it may give them incentives to vote differently than 
they would have in a single-stage communication protocol 
in which all voters declare their entire votes simultaneously 
of course even the single-stage communication protocol is 
not strategy-proof 
for any reasonable voting rule by the 
gibbard-satterthwaite theorem however this does 
not mean that we should not be concerned about adding 
even more opportunities for strategic voting in fact many 
of the communication protocols introduced in this paper do 
introduce additional opportunities for strategic voting but 
we do not have the space to discuss this here in prior 
work we do give an example where an elicitation 
protocol for the approval voting rule introduces strategic voting 
and give principles for designing elicitation protocols that 
do not introduce strategic problems 
now that we have reviewed voting rules we move on to a 
brief review of communication complexity theory 
 review of some communication 
complexity theory 
in this section we review the basic model of a 
communication problem and the lower-bounding technique of 
constructing a fooling set the basic model of a communication 
problem is due to yao for an overview see kushilevitz 
and nisan 
each player ≤ i ≤ n knows only input xi together 
they seek to compute f x x xn in a deterministic 
protocol for computing f in each stage one of the players 
announces to all other players a bit of information based 
on her own input and the bits announced so far 
eventually the communication terminates and all players know 
f x x xn the goal is to minimize the worst-case 
 over all input vectors number of bits sent the 
deterministic communication complexity of a problem is the 
worstcase number of bits sent in the best correct deterministic 
protocol for it in a nondeterministic protocol the next 
bit to be sent can be chosen nondeterministically for the 
purposes of this paper we will consider a nondeterministic 
protocol correct if for every input vector there is some 
sequence of nondeterministic choices the players can make so 
that the players know the value of f when the protocol 
terminates the nondeterministic communication complexity 
of a problem is the worst-case number of bits sent in the 
best correct nondeterministic protocol for it 
we are now ready to give the definition of a fooling set 
definition a fooling set is a set of input vectors 
{ x 
 x 
 x 
n x 
 x 
 x 
n xk 
 xk 
 xk 
n such 
that for any i f xi 
 xi 
 xi 
n f for some constant f 
but for any i j there exists some vector r r rn ∈ 
{i j}n 
such that f xr 
 xr 
 xrn 
n f that is we can 
mix the inputs from the two input vectors to obtain a vector 
with a different function value 
it is known that if a fooling set of size k exists then log k 
is a lower bound on the communication complexity even 
the nondeterministic communication complexity 
 
a strategy-proof protocol is one in which it is in the players 
best interest to report their preferences truthfully 
 
for the purposes of this paper f is the voting rule that 
maps the votes to the winning candidate and xi is voter 
i s vote the information that the voting rule would require 
from the voter if there were no possibility of multistage 
communication-i e the most preferred candidate 
 plurality the approved candidates approval or the ranking of 
all the candidates all other protocols however when we 
derive our lower bounds f will only signify whether a 
distinguished candidate a wins that is f is if a wins and 
 otherwise this will strengthen our lower bound results 
 because it implies that even finding out whether one given 
candidate wins is hard 
thus a fooling set in our 
context is a set of vectors of votes so that a wins does not win 
with each of them but for any two different vote vectors in 
the set there is a way of taking some voters votes from the 
first vector and the others votes from the second vector so 
that a does not win wins 
to simplify the proofs of our lower bounds we make 
assumptions such as the number of voters n is odd in many 
of these proofs therefore technically we do not prove the 
lower bound for number of candidates number of voters 
pairs m n that do not satisfy these assumptions for 
example if we make the above assumption then we technically 
do not prove the lower bound for any pair m n in which 
n is even nevertheless we always prove the lower bound 
for a representative set of m n pairs for example for 
every one of our lower bounds it is the case that for infinitely 
many values of m there are infinitely many values of n such 
that the lower bound is proved for the pair m n 
 results 
we are now ready to present our results for each voting 
rule we first give a deterministic communication protocol 
for determining the winner to establish an upper bound 
then we give a lower bound on the nondeterministic 
communication complexity even on the complexity of deciding 
whether a given candidate wins which is an easier 
question the lower bounds match the upper bounds in all but 
two cases the stv rule upper bound o n log m 
 lower 
bound ω n log m and the maximin rule upper bound 
o nm log m although we do give a nondeterministic 
protocol that is o nm lower bound ω nm 
when we discuss a voting rule in which the voters rank the 
candidates we will represent a ranking in which candidate c 
is ranked first c is ranked second etc as c c cm 
 
one possible concern is that in the case where ties are 
possible it may require much communication to verify whether 
a specific candidate a is among the winners but little 
communication to produce one of the winners however all the 
fooling sets we use in the proofs have the property that if 
a wins then a is the unique winner therefore in these 
fooling sets if one knows any one of the winners then one 
knows whether a is a winner thus computing one of the 
winners requires at least as much communication as 
verifying whether a is among the winners in general when a 
communication problem allows multiple correct answers for 
a given vector of inputs this is known as computing a 
relation rather than a function however as per the above 
we can restrict our attention to a subset of the domain where 
the voting rule truly is a single-valued function and hence 
lower bounding techniques for functions rather than 
relations will suffice 
sometimes for the purposes of a proof the internal ranking 
of a subset of the candidates does not matter and in this 
case we will not specify it for example if s {c c } 
then c s c indicates that either the ranking c 
c c c or the ranking c c c c can be used 
for the proof 
we first give a universal upper bound 
theorem the deterministic communication 
complexity of any rank-based voting rule is o nm log m 
proof this bound is achieved by simply having 
everyone communicate their entire ordering of the candidates 
 indicating the rank of an individual candidate requires only 
o log m bits so each of the n voters can simply indicate 
the rank of each of the m candidates 
the next lemma will be useful in a few of our proofs 
lemma if m divides n then log n −m log n m ≥ 
n log m − 
proof if n m that is n m then this 
expression simplifies to log n we have log n 
n 
i 
log i ≥ 
n 
x 
log i dx which using integration by parts is equal to 
n log n − n − n log n − n log m − n log m − 
 so we can assume that n m ≥ we observe that 
log n 
n 
i 
log i 
n m− 
i 
m 
j 
log im j ≥ 
n m− 
i 
m 
j 
log im 
 m 
n m− 
i 
log im and that m log n m m 
n m 
i 
log i 
therefore log n − m log n m ≥ m 
n m− 
i 
log im − 
m 
n m 
i 
log i m 
n m− 
i 
log im i −log n m m n m− 
 log m−log n log m n log m−m log n now using the 
fact that n m ≥ we have m log n n m n log m n m 
n m n log m log n m ≤ n log m log thus 
log n − m log n m ≥ n log m − m log n ≥ n log m − 
n log m log n log m − 
theorem the deterministic communication 
complexity of the plurality rule is o n log m 
proof indicating one of the candidates requires only 
o log m bits so each voter can simply indicate her most 
preferred candidate 
theorem the nondeterministic communication 
complexity of the plurality rule is ω n log m even to decide 
whether a given candidate a wins 
proof we will exhibit a fooling set of size n 
 n 
m 
 m 
where 
n n− taking the logarithm of this gives log n − 
m log n m so the result follows from lemma the 
fooling set will consist of all vectors of votes satisfying the 
following constraints 
 for any ≤ i ≤ n voters i− and i vote the same 
 
 every candidate receives equally many votes from the 
first n n − voters 
 the last voter voter n votes for a 
candidate a wins with each one of these vote vectors 
because of the extra vote for a from the last voter given that 
m divides n let us see how many vote vectors there are in 
the fooling set we need to distribute n voter pairs evenly 
over m candidates for a total of n m voter pairs per 
candidate and there are precisely n 
 n 
m 
 m 
ways of doing this 
all that remains to show is that for any two distinct vectors 
of votes in the fooling set we can let each of the voters vote 
according to one of these two vectors in such a way that a 
loses let i be a number such that the two vote vectors 
disagree on the candidate for which voters i − and i vote 
without loss of generality suppose that in the first vote 
vector these voters do not vote for a but for some other 
candidate b instead now construct a new vote vector by 
taking votes i − and i from the first vote vector and 
the remaining votes from the second vote vector then b 
receives n m votes in this newly constructed vote 
vector whereas a receives at most n m votes so a is not 
the winner in the newly constructed vote vector and hence 
we have a correct fooling set 
theorem the deterministic communication 
complexity of the plurality with runoff rule is o n log m 
proof first let every voter indicate her most preferred 
candidate using log m bits after this the two candidates 
in the runoff are known and each voter can indicate which 
one she prefers using a single additional bit 
theorem the nondeterministic communication 
complexity of the plurality with runoff rule is ω n log m even 
to decide whether a given candidate a wins 
proof we will exhibit a fooling set of size n 
 n 
m 
 m 
where m m and n n − taking the 
logarithm of this gives log n − m log n m so the result 
follows from lemma divide the candidates into m pairs 
 c d c d cm dm where c a and d b 
the fooling set will consist of all vectors of votes satisfying 
the following constraints 
 for any ≤ i ≤ n voters i − and i − rank 
the candidates ck i a c − {a ck i } for some 
candidate ck i if ck i a then the vote is simply 
a c − {a} 
 for any ≤ i ≤ n voters i − and i rank the 
candidates dk i a c − {a dk i } that is their most 
preferred candidate is the candidate that is paired with 
the candidate that the previous two voters vote for 
 
an intuitive proof of this is the following we can count 
the number of permutations of n elements as follows first 
divide the elements into m buckets of size n m so that if 
x is placed in a lower-indexed bucket than y then x will 
be indexed lower in the eventual permutation then decide 
on the permutation within each bucket for which there are 
 n m choices per bucket it follows that n equals the 
number of ways to divide n elements into m buckets of size 
n m times n m m 
 
 every candidate is ranked at the top of equally many 
of the first n n − votes 
 voter n n− ranks the candidates a c−{a} 
 voter n n ranks the candidates b c − {b} 
candidate a wins with each one of these vote vectors 
because of the last two votes candidates a and b are one vote 
ahead of all the other candidates and continue to the runoff 
and at this point all the votes that had another candidate 
ranked at the top transfer to a so that a wins the runoff 
given that m divides n let us see how many vote vectors 
there are in the fooling set we need to distribute n groups 
of four voters evenly over the m pairs of candidates and 
 as in the proof of theorem there are n 
 n 
m 
 m 
ways of 
doing this all that remains to show is that for any two 
distinct vectors of votes in the fooling set we can let each 
of the voters vote according to one of these two vectors in 
such a way that a loses let i be a number such that ck i is 
not the same in both of these two vote vectors that is c 
k i 
 ck i in the first vote vector is not equal to c 
k i ck i in 
the second vote vector without loss of generality suppose 
c 
k i a now construct a new vote vector by taking votes 
 i − i − i − i from the first vote vector and the 
remaining votes from the second vote vector in this newly 
constructed vote vector c 
k i and d 
k i each receive n m 
votes in the first round whereas a receives at most n m 
votes so a does not continue to the runoff in the newly 
constructed vote vector and hence we have a correct fooling 
set 
theorem the nondeterministic communication 
complexity of the borda rule is ω nm log m even to decide 
whether a given candidate a wins 
proof we will exhibit a fooling set of size m n 
where 
m m− and n n− this will prove the theorem 
because m is ω m log m so that log m n 
 n log m 
is ω nm log m for every vector π π πn consisting 
of n orderings of all candidates other than a and another 
fixed candidate b technically the orderings take the form of 
a one-to-one function πi { m } → c − {a b} with 
πi j c indicating that candidate c is the jth in the order 
represented by πi let the following vector of votes be an 
element of the fooling set 
 for ≤ i ≤ n let voters i − and i − rank the 
candidates a b πi πi πi m 
 for ≤ i ≤ n let voters i − and i rank the 
candidates πi m πi m − πi b a 
 let voter n n − rank the candidates a 
b π π π m where π is an 
arbitrary order of the candidates other than a and b 
which is the same for every element of the fooling set 
 let voter n n rank the candidates π m 
π m − π a b 
we observe that this fooling set has size m n 
 and that 
candidate a wins in each vector of votes in the fooling set to 
 
see why we observe that for any ≤ i ≤ n votes i− and 
 i − rank the candidates in the exact opposite way from 
votes i − and i which under the borda rule means they 
cancel out and the last two votes give one more point to a 
than to any other candidate-besides b who gets two fewer 
points than a all that remains to show is that for any two 
distinct vectors of votes in the fooling set we can let each of 
the voters vote according to one of these two vectors in such 
a way that a loses let the first vote vector correspond to 
the vector π 
 π 
 π 
n and let the second vote vector 
correspond to the vector π 
 π 
 π 
n for some i we 
must have π 
i π 
i so that for some candidate c ∈ {a b} 
 π 
i − 
 c π 
i − 
 c that is c is ranked higher in π 
i than 
in π 
i now construct a new vote vector by taking votes 
 i− and i− from the first vote vector and the remaining 
votes from the second vote vector a s borda score remains 
unchanged however because c is ranked higher in π 
i than 
in π 
i c receives at least more points from votes i− and 
 i − in the newly constructed vote vector than it did in 
the second vote vector it follows that c has a higher borda 
score than a in the newly constructed vote vector so a is 
not the winner in the newly constructed vote vector and 
hence we have a correct fooling set 
theorem the nondeterministic communication 
complexity of the copeland rule is ω nm log m even to decide 
whether a given candidate a wins 
proof we will exhibit a fooling set of size m n 
where 
m m − and n n − this will prove the 
theorem because m is ω m log m so that log m n 
 
n log m is ω nm log m we write the set of candidates 
as the following disjoint union c {a b} ∪ l ∪ r where 
l {l l lm } and r {r r rm } for every 
vector π π πn consisting of n permutations of the 
integers through m πi { m } → { m } 
let the following vector of votes be an element of the fooling 
set 
 for ≤ i ≤ n let voter i − rank the candidates 
a b lπi rπi lπi rπi 
lπi m rπi m 
 for ≤ i ≤ n let voter i rank the candidates 
rπi m lπi m rπi m − lπi m − 
rπi lπi b a 
 let voter n − n rank the candidates a b 
l r l r lm rm 
 let voter n n rank the candidates rm lm 
rm − lm − r l a b 
we observe that this fooling set has size m n 
 and 
that candidate a wins in each vector of votes in the 
fooling set every pair of candidates is tied in their pairwise 
election with the exception that a defeats b so that a wins 
the election by half a point all that remains to show is 
that for any two distinct vectors of votes in the fooling 
set we can let each of the voters vote according to one 
of these two vectors in such a way that a loses let the 
first vote vector correspond to the vector π 
 π 
 π 
n 
and let the second vote vector correspond to the vector 
 π 
 π 
 π 
n for some i we must have π 
i π 
i so that 
for some j ∈ { m } we have π 
i − 
 j π 
i − 
 j 
now construct a new vote vector by taking vote i− from 
the first vote vector and the remaining votes from the 
second vote vector a s copeland score remains unchanged let 
us consider the score of lj we first observe that the rank of 
lj in vote i − in the newly constructed vote vector is at 
least higher than it was in the second vote vector because 
 π 
i − 
 j π 
i − 
 j let d 
 lj be the set of candidates 
in l ∪ r that voter i − ranked lower than lj in the first 
vote vector d 
 lj {c ∈ l ∪ r lj 
 
 i− c} and let 
d 
 lj be the set of candidates in l ∪ r that voter i − 
ranked lower than lj in the second vote vector d 
 lj 
{c ∈ l ∪ r lj 
 
 i− c} then it follows that in the 
newly constructed vote vector lj defeats all the candidates 
in d 
 lj − d 
 lj in their pairwise elections because lj 
receives an extra vote in each one of these pairwise elections 
relative to the second vote vector and loses to all the 
candidates in d 
 lj − d 
 lj because lj loses a vote in each one 
of these pairwise elections relative to the second vote 
vector and ties with everyone else but d 
 lj − d 
 lj ≥ 
and hence d 
 lj − d 
 lj − d 
 lj − d 
 lj ≥ hence 
in the newly constructed vote vector lj has at least two 
more pairwise wins than pairwise losses and therefore has 
at least more point than if lj had tied all its pairwise 
elections thus lj has a higher copeland score than a in the 
newly constructed vote vector so a is not the winner in the 
newly constructed vote vector and hence we have a correct 
fooling set 
theorem the nondeterministic communication 
complexity of the maximin rule is o nm 
proof the nondeterministic protocol will guess which 
candidate w is the winner and for each other candidate 
c which candidate o c is the candidate against whom c 
receives its lowest score in a pairwise election then let 
every voter communicate the following 
 for each candidate c w whether she prefers c to w 
 for each candidate c w whether she prefers c to o c 
we observe that this requires the communication of n m− 
 bits if the guesses were correct then letting n d e be 
the number of voters preferring candidate d to candidate e 
we should have n c o c n w c for any c w c w 
which will prove that w wins the election 
theorem the nondeterministic communication 
complexity of the maximin rule is ω nm even to decide whether 
a given candidate a wins 
proof we will exhibit a fooling set of size n m 
where 
m m − and n n − let b be a candidate other 
than a for every vector s s sn consisting of n 
subsets si ⊆ c − {a b} let the following vector of votes be 
an element of the fooling set 
 for ≤ i ≤ n let voters i − and i − rank the 
candidates si a c − si ∪ {a b} b 
 for ≤ i ≤ n let voters i − and i rank the 
candidates b c − si ∪ {a b} a si 
 
 let voter n n rank the candidates a b 
c − {a b} 
we observe that this fooling set has size m 
 n 
 n m 
 
and that candidate a wins in each vector of votes in the 
fooling set in every one of a s pairwise elections a is ranked 
higher than its opponent by n n n votes 
all that remains to show is that for any two distinct vectors 
of votes in the fooling set we can let each of the voters vote 
according to one of these two vectors in such a way that 
a loses let the first vote vector correspond to the vector 
 s 
 s 
 s 
n and let the second vote vector correspond 
to the vector s 
 s 
 s 
n for some i we must have 
s 
i s 
i so that either s 
i s 
i or s 
i s 
i without loss 
of generality suppose s 
i s 
i and let c be some candidate 
in s 
i − s 
i now construct a new vote vector by taking 
votes i − and i − from the first vote vector and the 
remaining votes from the second vote vector in this newly 
constructed vote vector a is ranked higher than c by only 
 n − voters for the following reason whereas voters i− 
and i − do not rank c higher than a in the second vote 
vector because c ∈ s 
i voters i − and i − do rank 
c higher than a in the first vote vector because c ∈ s 
i 
moreover in every one of b s pairwise elections b is ranked 
higher than its opponent by at least n voters so a has 
a lower maximin score than b therefore a is not the winner 
in the newly constructed vote vector and hence we have a 
correct fooling set 
theorem the deterministic communication 
complexity of the stv rule is o n log m 
 
proof consider the following communication protocol 
let each voter first announce her most preferred candidate 
 o n log m communication in the remaining rounds we 
will keep track of each voter s most preferred candidate 
among the remaining candidates which will be enough to 
implement the rule when candidate c is eliminated let 
each of the voters whose most preferred candidate among the 
remaining candidates was c announce their most preferred 
candidate among the candidates remaining after c s 
elimination if candidate c was the ith candidate to be eliminated 
 that is there were m − i candidates remaining before 
c s elimination it follows that at most n m − i voters 
had candidate c as their most preferred candidate among 
the remaining candidates and thus the number of bits to be 
communicated after the elimination of the ith candidate is 
o n m−i log m 
thus the total communication in 
this communication protocol is o n log m 
m− 
i 
 n m − i 
 log m of course 
m− 
i 
 m − i 
m 
i 
 i which is 
o log m substituting into the previous expression we find 
that the communication complexity is o n log m 
 
theorem the nondeterministic communication 
complexity of the stv rule is ω n log m even to decide whether 
a given candidate a wins 
proof we omit this proof because of space constraint 
 
actually o n m − i log m − i is also correct 
but it will not improve the bound 
theorem the deterministic communication 
complexity of the approval rule is o nm 
proof approving or disapproving of a candidate requires 
only one bit of information so every voter can simply 
approve or disapprove of every candidate for a total 
communication of nm bits 
theorem the nondeterministic communication 
complexity of the approval rule is ω nm even to decide whether 
a given candidate a wins 
proof we will exhibit a fooling set of size n m 
where 
m m − and n n − for every vector 
 s s sn consisting of n subsets si ⊆ c − {a} let 
the following vector of votes be an element of the fooling 
set 
 for ≤ i ≤ n let voters i − and i − approve 
si ∪ {a} 
 for ≤ i ≤ n let voters i − and i approve c − 
 si ∪ {a} 
 let voter n n approve {a} 
we observe that this fooling set has size m 
 n 
 n m 
 
and that candidate a wins in each vector of votes in the 
fooling set a is approved by n voters whereas each 
other candidate is approved by only n voters all that 
remains to show is that for any two distinct vectors of votes 
in the fooling set we can let each of the voters vote 
according to one of these two vectors in such a way that a 
loses let the first vote vector correspond to the vector 
 s 
 s 
 s 
n and let the second vote vector correspond 
to the vector s 
 s 
 s 
n for some i we must have 
s 
i s 
i so that either s 
i s 
i or s 
i s 
i without loss 
of generality suppose s 
i s 
i and let b be some candidate 
in s 
i − s 
i now construct a new vote vector by taking 
votes i − and i − from the first vote vector and the 
remaining votes from the second vote vector in this newly 
constructed vote vector a is still approved by n votes 
however b is approved by n votes for the following 
reason whereas voters i− and i− do not approve b in 
the second vote vector because b ∈ s 
i voters i − and 
 i − do approve b in the first vote vector because b ∈ s 
i 
it follows that b s score in the newly constructed vote vector 
is b s score in the second vote vector n plus two so a 
is not the winner in the newly constructed vote vector and 
hence we have a correct fooling set 
interestingly an ω m lower bound can be obtained even 
for the problem of finding a candidate that is approved by 
more than one voter 
theorem the deterministic communication 
complexity of the condorcet rule is o nm 
proof we maintain a set of active candidates s which 
is initialized to c at each stage we choose two of the active 
candidates say the two candidates with the lowest indices 
and we let each voter communicate which of the two 
candidates she prefers such a stage requires the communication 
of n bits one per voter the candidate preferred by fewer 
 
voters the loser of the pairwise election is removed from 
s if the pairwise election is tied both candidates are 
removed after at most m − iterations only one candidate 
is left or zero candidates are left in which case there is no 
condorcet winner let a be the remaining candidate to 
find out whether candidate a is the condorcet winner let 
each voter communicate for every candidate c a whether 
she prefers a to c this requires the communication of at 
most n m − bits this is enough to establish whether 
a won each of its pairwise elections and thus whether a is 
the condorcet winner 
theorem the nondeterministic communication 
complexity of the condorcet rule is ω nm even to decide whether 
a given candidate a wins 
proof we will exhibit a fooling set of size n m 
where 
m m − and n n − for every vector 
 s s sn consisting of n subsets si ⊆ c − {a} let 
the following vector of votes be an element of the fooling 
set 
 for ≤ i ≤ n let voter i − rank the candidates 
si a c − si 
 for ≤ i ≤ n let voter i rank the candidates c − 
si a si 
 let voter n n rank the candidates a c −{a} 
we observe that this fooling set has size m 
 n 
 n m 
 
and that candidate a wins in each vector of votes in the 
fooling set a wins each of its pairwise elections by a single 
vote all that remains to show is that for any two distinct 
vectors of votes in the fooling set we can let each of the 
voters vote according to one of these two vectors in such a 
way that a loses let the first vote vector correspond to 
the vector s 
 s 
 s 
n and let the second vote vector 
correspond to the vector s 
 s 
 s 
n for some i we 
must have s 
i s 
i so that either s 
i s 
i or s 
i s 
i 
without loss of generality suppose s 
i s 
i and let b be 
some candidate in s 
i − s 
i now construct a new vote 
vector by taking vote i − from the first vote vector and 
the remaining votes from the second vote vector in this 
newly constructed vote vector b wins its pairwise election 
against a by one vote vote i − ranks b above a in the 
newly constructed vote vector because b ∈ s 
i whereas in 
the second vote vector vote i − ranked a above b because 
b ∈ s 
i so a is not the condorcet winner in the newly 
constructed vote vector and hence we have a correct fooling 
set 
theorem the deterministic communication 
complexity of the cup rule is o nm 
proof consider the following simple communication 
protocol first let all the voters communicate for every one of 
the matchups in the first round which of its two candidates 
they prefer after this the matchups for the second round 
are known so let all the voters communicate which 
candidate they prefer in each matchup in the second round-etc 
because communicating which of two candidates is preferred 
requires only one bit per voter and because there are only 
m − matchups in total this communication protocol 
requires o nm communication 
theorem the nondeterministic communication 
complexity of the cup rule is ω nm even to decide whether a 
given candidate a wins 
proof we will exhibit a fooling set of size n m 
where 
m m − and n n − given that m is a 
power of so that one candidate gets a bye that is does not 
face an opponent in the first round let a be the candidate 
with the bye of the m first-round matchups let lj denote 
the one left candidate in the jth matchup and let rj be 
the other right candidate let l {lj ≤ j ≤ m } 
and r {rj ≤ j ≤ m } so that c l ∪ r ∪ {a} 
 
 
 
 
 
 
 
l r l r l r 
a 
m m 
figure the schedule for the cup rule used in the 
proof of theorem 
for every vector s s sn consisting of n subsets 
si ⊆ r let the following vector of votes be an element of 
the fooling set 
 for ≤ i ≤ n let voter i − rank the candidates 
si l a r − si 
 for ≤ i ≤ n let voter i rank the candidates r − 
si l a si 
 let voters n n− n n− n n− 
rank the candidates l a r 
 let voters n n − n n − rank the 
candidates a r l r l rm lm 
 let voters n n − n n rank the 
candidates rm lm rm − lm − r l a 
we observe that this fooling set has size m 
 n 
 n m 
 
also candidate a wins in each vector of votes in the fooling 
set for the following reasons each candidate rj defeats its 
opponent lj in the first round for any ≤ i ≤ n the 
net effect of votes i − and i on the pairwise election 
between rj and lj is zero votes n − n − n − prefer 
lj to rj but votes n − n − n − n all prefer rj to lj 
moreover a defeats every rj in their pairwise election for 
any ≤ i ≤ n the net effect of votes i − and i on the 
pairwise election between a and rj is zero votes n − n 
prefer rj to a but votes n − n − n − n − n − all 
prefer a to rj it follows that a will defeat all the candidates 
that it faces 
all that remains to show is that for any two distinct 
vectors of votes in the fooling set we can let each of the voters 
vote according to one of these two vectors in such a way that 
a loses let the first vote vector correspond to the vector 
 
 s 
 s 
 s 
n and let the second vote vector correspond 
to the vector s 
 s 
 s 
n for some i we must have 
s 
i s 
i so that either s 
i s 
i or s 
i s 
i without 
loss of generality suppose s 
i s 
i and let rj be some 
candidate in s 
i − s 
i now construct a new vote vector by 
taking vote i from the first vote vector and the remaining 
votes from the second vote vector we note that whereas 
in the second vote vector vote i preferred rj to lj because 
rj ∈ r−s 
i in the newly constructed vote vector this is no 
longer the case because rj ∈ s 
i it follows that whereas 
in the second vote vector rj defeated lj in the first round 
by one vote in the newly constructed vote vector lj 
defeats rj in the first round thus at least one lj advances 
to the second round after defeating its opponent rj now 
we observe that in the newly constructed vote vector any 
lk wins its pairwise election against any rq with q k this 
is because among the first n votes at least n − prefer lk 
to rq votes n − n − n − prefer lk to rq and because 
q k either votes n − n − prefer lk to rq if k q 
or votes n − n prefer lk to rq if k q thus at least 
n n n votes prefer lk to rq moreover 
any lk wins its pairwise election against a this is because 
only votes n − and n − prefer a to lk it follows that 
after the first round any surviving candidate lk can only 
lose a matchup against another surviving lk so that one of 
the lk must win the election so a is not the winner in the 
newly constructed vote vector and hence we have a correct 
fooling set 
theorem the deterministic communication 
complexity of the bucklin rule is o nm 
proof let l be the minimum integer for which there is 
a candidate who is ranked among the top l candidates by 
more than half the votes we will do a binary search for l 
at each point we will have a lower bound ll which is smaller 
than l initialized to and an upper bound lh which is at 
least l initialized to m while lh − ll we continue 
by finding out whether lh − l is smaller than l after 
which we can update the bounds 
to find out whether a number k is smaller than l we 
determine every voter s k most preferred candidates 
every voter can communicate which candidates are among her 
k most preferred candidates using m bits for each 
candidate indicate whether the candidate is among the top k or 
not but because the binary search requires log m iterations 
this gives us an upper bound of o log m nm which is not 
strong enough however if ll k lh and we already 
know a voter s ll most preferred candidates as well as her lh 
most preferred candidates then the voter no longer needs to 
communicate whether the ll most preferred candidates are 
among her k most preferred candidates because they must 
be and she no longer needs to communicate whether the 
m−lh least preferred candidates are among her k most 
preferred candidates because they cannot be thus the voter 
needs to communicate only m−ll − m−lh lh −ll bits 
in any given stage because each stage lh − ll is roughly 
halved each voter in total communicates only roughly 
m m m ≤ m bits 
theorem the nondeterministic communication 
complexity of the bucklin rule is ω nm even to decide whether 
a given candidate a wins 
proof we will exhibit a fooling set of size n m 
where 
m m− and n n we write the set of candidates 
as the following disjoint union c {a} ∪ l ∪ r where 
l {l l lm } and r {r r rm } for any 
subset s ⊆ { m } let l s {li i ∈ s} and let 
r s {ri i ∈ s} for every vector s s sn 
consisting of n sets si ⊆ { m } let the following 
vector of votes be an element of the fooling set 
 for ≤ i ≤ n let voter i − rank the candidates 
l si r − r si a l − l si r si 
 for ≤ i ≤ n let voter i rank the candidates l − 
l si r si a l si r − r si 
we observe that this fooling set has size m 
 n 
 n m 
 
and that candidate a wins in each vector of votes in the 
fooling set for the following reason each candidate in c − {a} 
is ranked among the top m candidates by exactly half the 
voters which is not enough to win thus we need to look 
at the voters top m candidates and a is ranked m th 
by all voters all that remains to show is that for any two 
distinct vectors of votes in the fooling set we can let each of 
the voters vote according to one of these two vectors in such 
a way that a loses let the first vote vector correspond to 
the vector s 
 s 
 s 
n and let the second vote vector 
correspond to the vector s 
 s 
 s 
n for some i we 
must have s 
i s 
i so that either s 
i s 
i or s 
i s 
i 
without loss of generality suppose s 
i s 
i and let j be 
some integer in s 
i − s 
i now construct a new vote vector 
by taking vote i − from the first vote vector and the 
remaining votes from the second vote vector in this newly 
constructed vote vector a is still ranked m th by all 
votes however lj is ranked among the top m candidates 
by n n votes this is because whereas vote 
 i − does not rank lj among the top m candidates in the 
second vote vector because j ∈ s 
i we have lj ∈ l s 
i 
vote i − does rank lj among the top m candidates in the 
first vote vector because j ∈ s 
i we have lj ∈ l s 
i so a 
is not the winner in the newly constructed vote vector and 
hence we have a correct fooling set 
theorem the nondeterministic communication 
complexity of the ranked pairs rule is ω nm log m even to 
decide whether a given candidate a wins 
proof we omit this proof because of space constraint 
 discussion 
one key obstacle to using voting for preference 
aggregation is the communication burden that an election places 
on the voters by lowering this burden it may become 
feasible to conduct more elections over more issues in the 
limit this could lead to a shift from representational 
government to a system in which most issues are decided by 
referenda-a veritable e-democracy in this paper we 
analyzed the communication complexity of the common voting 
rules knowing which voting rules require little 
communication is especially important when the issue to be voted 
on is of low enough importance that the following is true 
the parties involved are willing to accept a rule that tends 
 
to produce outcomes that are slightly less representative of 
the voters preferences if this rule reduces the 
communication burden on the voters significantly the following table 
summarizes the results we obtained 
rule lower bound upper bound 
plurality ω n log m o n log m 
plurality w runoff ω n log m o n log m 
stv ω n log m o n log m 
condorcet ω nm o nm 
approval ω nm o nm 
bucklin ω nm o nm 
cup ω nm o nm 
maximin ω nm o nm 
borda ω nm log m o nm log m 
copeland ω nm log m o nm log m 
ranked pairs ω nm log m o nm log m 
communication complexity of voting rules sorted from low 
to high all of the upper bounds are deterministic with 
the exception of maximin for which the best deterministic 
upper bound we proved is o nm log m all of the lower 
bounds hold even for nondeterministic communication and 
even just for determining whether a given candidate a is 
the winner 
one area of future research is to study what happens when 
we restrict our attention to communication protocols that 
do not reveal any strategically useful information this 
restriction may invalidate some of the upper bounds that we 
derived using multistage communication protocols also all 
of our bounds are worst-case bounds it may be possible to 
outperform these bounds when the distribution of votes has 
additional structure 
when deciding which voting rule to use for an election 
there are many considerations to take into account the 
voting rules that we studied in this paper are the most 
common ones that have survived the test of time one way to 
select among these rules is to consider recent results on 
complexity the table above shows that from a communication 
complexity perspective plurality plurality with runoff and 
stv are preferable however plurality has the undesirable 
property that it is computationally easy to manipulate by 
voting strategically plurality with runoff is np-hard 
to manipulate by a coalition of weighted voters or by an 
individual that faces correlated uncertainty about the 
others votes stv is np-hard to manipulate in those 
settings as well but also by an individual with perfect 
knowledge of the others votes when the number of 
candidates is unbounded therefore stv is more robust 
although it may require slightly more worst-case 
communication as per the table above yet other selection criteria 
are the computational complexity of determining whether 
enough information has been elicited to declare a winner 
and that of determining the optimal sequence of queries 
 references 
 lawrence ausubel and paul milgrom ascending auctions 
with package bidding frontiers of theoretical economics 
 no article 
 john bartholdi iii and james orlin single transferable 
vote resists strategic voting social choice and welfare 
 - 
 john bartholdi iii craig tovey and michael trick the 
computational difficulty of manipulating an election social 
choice and welfare - 
 avrim blum jeffrey jackson tuomas sandholm and 
martin zinkevich preference elicitation and query learning 
journal of machine learning research - 
 wolfram conen and tuomas sandholm preference 
elicitation in combinatorial auctions extended abstract in 
proceedings of the acm conference on electronic 
commerce acm-ec pages - 
 vincent conitzer jerome lang and tuomas sandholm 
how many candidates are needed to make elections hard to 
manipulate in theoretical aspects of rationality and 
knowledge tark pages - 
 vincent conitzer and tuomas sandholm complexity of 
manipulating elections with few candidates in proceedings 
of the national conference on artificial intelligence 
 aaai pages - 
 vincent conitzer and tuomas sandholm vote elicitation 
complexity and strategy-proofness in proceedings of the 
national conference on artificial intelligence aaai 
pages - 
 sven de vries james schummer and rakesh v vohra on 
ascending auctions for heterogeneous objects draft 
 allan gibbard manipulation of voting schemes 
econometrica - 
 benoit hudson and tuomas sandholm effectiveness of 
query types and policies for preference elicitation in 
combinatorial auctions in international conference on 
autonomous agents and multi-agent systems aamas 
pages - 
 e kushilevitz and n nisan communication complexity 
cambridge university press 
 sebasti´en lahaie and david parkes applying learning 
algorithms to preference elicitation in proceedings of the 
acm conference on electronic commerce 
 noam nisan and ilya segal the communication 
requirements of efficient allocations and supporting prices 
journal of economic theory forthcoming 
 david parkes ibundle an efficient ascending price bundle 
auction in proceedings of the acm conference on 
electronic commerce acm-ec pages - 
 tuomas sandholm an implementation of the contract net 
protocol based on marginal cost calculations in 
proceedings of the national conference on artificial 
intelligence aaai pages - 
 tuomas sandholm and craig boutilier preference 
elicitation in combinatorial auctions in peter cramton 
yoav shoham and richard steinberg editors 
combinatorial auctions chapter mit press 
 paolo santi vincent conitzer and tuomas sandholm 
towards a characterization of polynomial preference 
elicitation with value queries in combinatorial auctions in 
conference on learning theory colt pages - 
 mark satterthwaite strategy-proofness and arrow s 
conditions existence and correspondence theorems for 
voting procedures and social welfare functions journal of 
economic theory - 
 ilya segal the communication requirements of social choice 
rules and supporting budget sets draft presented at 
the dimacs workshop on computational issues in 
auction design rutgers university new jersey usa 
 peter wurman and michael wellman akba a 
progressive anonymous-price combinatorial auction in 
proceedings of the acm conference on electronic 
commerce acm-ec pages - 
 a c yao some complexity questions related to distributed 
computing in proceedings of the th acm symposium on 
theory of computing stoc pages - 
 martin zinkevich avrim blum and tuomas sandholm on 
polynomial-time preference elicitation with value queries 
in proceedings of the acm conference on electronic 
commerce acm-ec pages - 
 
efficiency and nash equilibria in a scrip system for p p 
networks 
eric j friedman 
school of operations 
research and industrial 
engineering 
cornell university 
ejf  cornell edu 
joseph y halpern 
computer science dept 
cornell university 
halpern cs cornell edu 
ian kash 
computer science dept 
cornell university 
kash cs cornell edu 
abstract 
a model of providing service in a p p network is analyzed 
it is shown that by adding a scrip system a mechanism that 
admits a reasonable nash equilibrium that reduces free 
riding can be obtained the effect of varying the total amount 
of money scrip in the system on efficiency i e social 
welfare is analyzed and it is shown that by maintaining the 
appropriate ratio between the total amount of money and 
the number of agents efficiency is maximized the work 
has implications for many online systems not only p p 
networks but also a wide variety of online forums for which scrip 
systems are popular but formal analyses have been lacking 
categories and subject descriptors 
c computer-communication networks distributed 
systems i artificial intelligence distributed 
artificial intelligence-multiagent systems j social and 
behavioral sciences economics k computers and 
society electronic commerce 
general terms 
economics theory 
 introduction 
a common feature of many online distributed systems is 
that individuals provide services for each other 
peer-topeer p p networks such as kazaa or bittorrent 
have proved popular as mechanisms for file sharing and 
applications such as distributed computation and file storage 
are on the horizon systems such as seti home provide 
computational assistance systems such as slashdot 
provide content evaluations and advice forums in which people 
answer each other s questions having individuals provide 
each other with service typically increases the social welfare 
the individual utilizing the resources of the system derives a 
greater benefit from it than the cost to the individual 
providing it however the cost of providing service can still be 
nontrivial for example users of kazaa and bittorrent may 
be charged for bandwidth usage in addition in some 
filesharing systems there is the possibility of being sued which 
can be viewed as part of the cost thus in many systems 
there is a strong incentive to become a free rider and 
benefit from the system without contributing to it this is not 
merely a theoretical problem studies of the gnutella 
network have shown that almost percent of users share 
no files and nearly percent of responses are from the top 
 percent of sharing hosts 
having relatively few users provide most of the service 
creates a point of centralization the disappearance of a small 
percentage of users can greatly impair the functionality of 
the system moreover current trends seem to be leading 
to the elimination of the altruistic users on which these 
systems rely these heavy users are some of the most 
expensive customers isps have thus as the amount of traffic has 
grown isps have begun to seek ways to reduce this traffic 
some universities have started charging students for 
excessive bandwidth usage others revoke network access for it 
 a number of companies have also formed whose service 
is to detect excessive bandwidth usage 
these trends make developing a system that encourages 
a more equal distribution of the work critical for the 
continued viability of p p networks and other distributed online 
systems a significant amount of research has gone into 
designing reputation systems to give preferential treatment 
to users who are sharing files some of the p p networks 
currently in use have implemented versions of these 
techniques however these approaches tend to fall into one of 
two categories either they are barter-like or reputational 
by barter-like we mean that each agent bases its decisions 
only on information it has derived from its own interactions 
perhaps the best-known example of a barter-like system is 
bittorrent where clients downloading a file try to find other 
clients with parts they are missing so that they can trade 
thus creating a roughly equal amount of work since the 
barter is restricted to users currently interested in a 
single file this works well for popular files but tends to have 
problems maintaining availability of less popular ones an 
example of a barter-like system built on top of a more 
traditional file-sharing system is the credit system used by emule 
 
 each user tracks his history of interactions with other 
users and gives priority to those he has downloaded from in 
the past however in a large system the probability that 
a pair of randomly-chosen users will have interacted before 
is quite small so this interaction history will not be 
terribly helpful anagnostakis and greenwald present a more 
sophisticated version of this approach but it still seems to 
suffer from similar problems 
a number of attempts have been made at providing 
general reputation systems e g the basic idea 
is to aggregate each user s experience into a global number 
for each individual that intuitively represents the system s 
view of that individual s reputation however these 
attempts tend to suffer from practical problems because they 
implicitly view users as either good or bad assume that 
the good users will act according to the specified protocol 
and that there are relatively few bad users unfortunately 
if there are easy ways to game the system once this 
information becomes widely available rational users are likely to 
make use of it we cannot count on only a few users being 
bad in the sense of not following the prescribed protocol 
for example kazaa uses a measure of the ratio of the 
number of uploads to the number of downloads to identify good 
and bad users however to avoid penalizing new users they 
gave new users an average rating users discovered that they 
could use this relatively good rating to free ride for a while 
and once it started to get bad they could delete their stored 
information and effectively come back as a new user thus 
circumventing the system see for a discussion and 
for a formal analysis of this whitewashing thus kazaa s 
reputation system is ineffective 
this is a simple case of a more general vulnerability of 
such systems to sybil attacks where a single user 
maintains multiple identities and uses them in a coordinated 
fashion to get better service than he otherwise would recent 
work has shown that most common reputation systems are 
vulnerable in the worst case to such attacks however 
the degree of this vulnerability is still unclear the 
analyses of the practical vulnerabilities and the existence of such 
systems that are immune to such attacks remains an area of 
active research e g 
simple economic systems based on a scrip or money seem 
to avoid many of these problems are easy to implement and 
are quite popular see e g however they 
have a different set of problems perhaps the most common 
involve determining the amount of money in the system 
roughly speaking if there is too little money in the system 
relative to the number of agents then relatively few users 
can afford to make request on the other hand if there is 
too much money then users will not feel the need to 
respond to a request they have enough money already a 
related problem involves handling newcomers if newcomers 
are each given a positive amount of money then the system 
is open to sybil attacks perhaps not surprisingly scrip 
systems end up having to deal with standard economic woes 
such as inflation bubbles and crashes 
in this paper we provide a formal model in which to 
analyze scrip systems we describe a simple scrip system 
and show that under reasonable assumptions for each fixed 
amount of money there is a nontrivial nash equilibrium 
involving threshold strategies where an agent accepts a request 
if he has less than k for some threshold k 
an interesting 
aspect of our analysis is that in equilibrium the 
distribution of users with each amount of money is the distribution 
that maximizes entropy subject to the money supply 
constraint this allows us to compute the money supply that 
maximizes efficiency social welfare given the number of 
agents it also leads to a solution for the problem of 
dealing with newcomers we simply assume that new users come 
in with no money and adjust the price of service which is 
equivalent to adjusting the money supply to maintain the 
ratio that maximizes efficiency while assuming that new 
users come in with no money will not work in all settings 
we believe the approach will be widely applicable in 
systems where the goal is to do work new users can acquire 
money by performing work it should also work in 
kazaalike system where a user can come in with some resources 
 e g a private collection of mp s 
the rest of the paper is organized as follows in section 
we present our formal model and observe that it can be used 
to understand the effect of altruists in section we 
examine what happens in the game under nonstrategic play if all 
agents use the same threshold strategy we show that in 
this case the system quickly converges to a situation where 
the distribution of money is characterized by maximum 
entropy using this analysis we show in section that under 
minimal assumptions there is a nontrivial nash equilibrium 
in the game where all agents use some threshold strategy 
moreover we show in section that the analysis leads to 
an understanding of how to choose the amount of money 
in the system or equivalently the cost to fulfill a request 
so as to maximize efficiency and also shows how to handle 
new users in section we discuss the extent to which our 
approach can handle sybils and collusion we conclude in 
section 
 the model 
to begin we formalize providing service in a p p network 
as a non-cooperative game unlike much of the modeling in 
this area our model will model the asymmetric interactions 
in a file sharing system in which the matching of players 
 those requesting a file with those who have that particular 
file is a key part of the system this is in contrast with 
much previous work which uses random matching in a 
prisoner s dilemma such models were studied in the economics 
literature and first applied to online reputations in 
 an application to p p is found in 
this random-matching model fails to capture some salient 
aspects of a number of important settings when a request 
is made there are typically many people in the network who 
can potentially satisfy it especially in a large p p network 
but not all can for example some people may not have 
the time or resources to satisfy the request the 
randommatching process ignores the fact that some people may not 
be able to satisfy the request presumably if the person 
matched with the requester could not satisfy the match he 
would have to defect moreover it does not capture the fact 
that the decision as to whether to volunteer to satisfy 
the request should be made before the matching process 
not after that is the matching process does not capture 
 
although we refer to our unit of scrip as the dollar these 
are not real dollars nor do we view them as convertible to 
dollars 
 
the fact that if someone is unwilling to satisfy the request 
there will doubtless be others who can satisfy it finally the 
actions and payoffs in the prisoner s dilemma game do not 
obviously correspond to actual choices that can be made 
for example it is not clear what defection on the part of 
the requester means in our model we try to deal with all 
these issues 
suppose that there are n agents at each round an agent 
is picked uniformly at random to make a request each other 
agent is able to satisfy this request with probability β at 
all times independent of previous behavior the term β is 
intended to capture the probability that an agent is busy or 
does not have the resources to fulfill the request assuming 
that β is time-independent does not capture the intution 
that being an unable to fulfill a request at time t may well 
be correlated with being unable to fulfill it at time t we 
believe that in large systems we should be able to drop the 
independence assumption but we leave this for future work 
in any case those agents that are able to satisfy the request 
must choose whether or not to volunteer to satisfy it if 
at least one agent volunteers the requester gets a benefit 
of util the job is done and one of volunteers is chosen 
at random to fulfill the request the agent that fulfills the 
request pays a cost of α as is standard in the literature 
we assume that agents discount future payoffs by a factor of 
δ per time unit this captures the intuition that a util now is 
worth more than a util tomorrow and allows us to compute 
the total utility derived by an agent in an infinite game 
lastly we assume that with more players requests come 
more often thus we assume that the time between rounds 
is n this captures the fact that the systems we want 
to model are really processing many requests in parallel so 
we would expect the number of concurrent requests to be 
proportional to the number of users 
let g n δ α β denote this game with n agents a 
discount factor of δ a cost to satisfy requests of α and a 
probability of being able to satisfy requests of β when the 
latter two parameters are not relevant we sometimes write 
g n δ 
we use the following notation throughout the paper 
 pt 
denotes the agent chosen in round t 
 bt 
i ∈ { } denotes whether agent i can satisfy the 
request in round t bt 
i with probability β and 
bt 
i is independent of bt 
i for all t t 
 v t 
i ∈ { } denotes agent i s decision about whether 
to volunteer in round t indicates volunteering v t 
i 
is determined by agent i s strategy 
 vt 
∈ {j v t 
j bt 
j } denotes the agent chosen to satisfy 
the request this agent is chosen uniformly at random 
from those who are willing v t 
j and able bt 
j 
to satisfy the request 
 ut 
i denotes agent i s utility in round t 
a standard agent is one whose utility is determined as 
discussed in the introduction namely the agent gets 
 
for large n our model converges to one in which players 
make requests in real time and the time between a player s 
requests are exponentially distributed with mean in 
addition the time between requests served by a single player 
is also exponentially distributed 
a utility of for a fulfilled request and utility −α for 
fulfilling a request thus if i is a standard agent then 
ut 
i 
 
 
 
 if i pt and 
p 
j i v t 
j bt 
j 
−α if i vt 
 otherwise 
 ui 
p∞ 
t δt n 
ut 
i denotes the total utility for agent 
i it is the discounted total of agent i s utility in each 
round note that the effective discount factor is δ n 
since an increase in n leads to a shortening of the time 
between rounds 
now that we have a model of making and satisfying 
requests we use it to analyze free riding take an altruist to 
be someone who always fulfills requests agent i might 
rationally behave altruistically if agent i s utility function has 
the following form for some α 
ut 
i 
 
 
 
 if i pt and 
p 
j i v t 
j bt 
j 
α if i vt 
 otherwise 
thus rather than suffering a loss of utility when satisfying 
a request an agent derives positive utility from satisfying 
it such a utility function is a reasonable representation of 
the pleasure that some people get from the sense that they 
provide the music that everyone is playing for such 
altruistic agents playing the strategy that sets v t 
i for all t 
is dominant while having a nonstandard utility function 
might be one reason that a rational agent might use this 
strategy there are certainly others for example a naive 
user of filesharing software with a good connection might 
well follow this strategy all that matters for the 
following discussion is that there are some agents that use this 
strategy for whatever reason 
as we have observed such users seem to exist in some 
large systems suppose that our system has a altruists 
intuitively if a is moderately large they will manage to satisfy 
most of the requests in the system even if other agents do 
no work thus there is little incentive for any other agent 
to volunteer because he is already getting full advantage of 
participating in the system based on this intuition it is a 
relatively straightforward calculation to determine a value 
of a that depends only on α β and δ but not the number 
n of players in the system such that the dominant strategy 
for all standard agents i is to never volunteer to satisfy any 
requests i e v t 
i for all t 
proposition there exists an a that depends only on 
α β and δ such that in g n δ α β with at least a altruists 
not volunteering in every round is a dominant strategy for 
all standard agents 
proof consider the strategy for a standard player j in 
the presence of a altruists even with no money player 
j will get a request satisfied with probability − − β a 
just through the actions of these altruists thus even if j is 
chosen to make a request in every round the most additional 
expected utility he can hope to gain by having money isp∞ 
k − β a 
δk 
 − β a 
 − δ if − β a 
 − δ α 
or equivalently if a log −β α − δ never volunteering 
is a dominant strategy 
consider the following reasonable values for our 
parameters β so that each player can satisfy of the 
requests α a low but non-negligible cost δ day 
 
 which corresponds to a yearly discount factor of 
approximately and an average of request per day per player 
then we only need a while this is a large number 
it is small relative to the size of a large p p network 
current systems all have a pool of users behaving like our 
altruists this means that attempts to add a reputation 
system on top of an existing p p system to influence users 
to cooperate will have no effect on rational users to have 
a fair distribution of work these systems must be 
fundamentally redesigned to eliminate the pool of altruistic users 
in some sense this is not a problem at all in a system 
with altruists the altruists are presumably happy as are 
the standard agents who get almost all their requests 
satisfied without having to do any work indeed current p p 
network work quite well in terms of distributing content to 
people however as we said in the introduction there is 
some reason to believe these altruists may not be around 
forever thus it is worth looking at what can be done to 
make these systems work in their absence for the rest of 
this paper we assume that all agents are standard and try 
to maximize expected utility 
we are interested in equilibria based on a scrip system 
each time an agent has a request satisfied he must pay the 
person who satisfied it some amount for now we assume 
that the payment is fixed for simplicity we take the amount 
to be we denote by m the total amount of money in 
the system we assume that m otherwise no one will 
ever be able to get paid 
in principle agents are free to adopt a very wide 
variety of strategies they can make decisions based on the 
names of other agents or use a strategy that is heavily 
history dependant and mix these strategies freely to aid our 
analysis we would like to be able to restrict our attention to 
a simpler class of strategies the class of strategies we are 
interested in is easy to motivate the intuitive reason for 
wanting to earn money is to cater for the possibility that an 
agent will run out before he has a chance to earn more on 
the other hand a rational agent with plenty of mone would 
not want to work because by the time he has managed to 
spend all his money the util will have less value than the 
present cost of working the natural balance between these 
two is a threshold strategy let sk be the strategy where 
an agent volunteers whenever he has less than k dollars and 
not otherwise note that s is the strategy where the agent 
never volunteers while everyone playing s is a nash 
equilibrium nobody can do better by volunteering if no one else 
is willing to it is an uninteresting one as we will show 
in section it is sufficient to restrict our attention to this 
class of strategies 
we use kt 
i to denote the amount of money agent i has 
at time t clearly kt 
i kt 
i unless agent i has a request 
satisfied in which case kt 
i kt 
i − or agent i fulfills a 
request in which case kt 
i kt 
i formally 
kt 
i 
 
 
 
kt 
i − if i pt 
 
p 
j i v t 
j bt 
j and kt 
i 
kt 
i if i vt 
and kt 
pt 
kt 
i otherwise 
the threshold strategy sk is the strategy such that 
v t 
i 
 
 if kt 
pt and kt 
i k 
 otherwise 
 the game under nonstrategic 
play 
before we consider strategic play we examine what 
happens in the system if everyone just plays the same strategy 
sk our overall goal is to show that there is some 
distribution over money i e the fraction of people with each 
amount of money such that the system converges to this 
distribution in a sense to be made precise shortly 
suppose that everyone plays sk for simplicity assume 
that everyone has at most k dollars we can make this 
assumption with essentially no loss of generality since if 
someone has more than k dollars he will just spend money 
until he has at most k dollars after this point he will never 
acquire more than k thus eventually the system will be 
in such a state if m ≥ kn no agent will ever be willing to 
work thus for the purposes of this section we assume that 
m kn 
from the perspective of a single agent in stochastic 
equilibrium the agent is undergoing a random walk 
however the parameters of this random walk depend on the 
random walks of the other agents and it is quite complicated 
to solve directly thus we consider an alternative analysis 
based on the evolution of the system as a whole 
if everyone has at most k dollars then the amount of 
money that an agent has is an element of { k} if there 
are n agents then the state of the game can be described 
by identifying how much money each agent has so we can 
represent it by an element of sk n { k}{ n} 
 since 
the total amount of money is constant not all of these states 
can arise in the game for example the state where each 
player has is impossible to reach in any game with money 
in the system let ms s 
p 
i∈{ n} s i denote the total 
mount of money in the game at state s where s i is the 
number of dollars that agent i has in state s we want 
to consider only those states where the total money in the 
system is m namely 
sk n m {s ∈ sk n ms s m} 
under the assumption that all agents use strategy sk the 
evolution of the system can be treated as a markov chain 
mk n m over the state space sk n m it is possible to move 
from one state to another in a single round if by choosing 
a particular agent to make a request and a particular agent 
to satisfy it the amounts of money possesed by each agent 
become those in the second state therefore the 
probability of a transition from a state s to t is unless there exist 
two agents i and j such that s i t i for all i ∈ {i j} 
t i s i and t j s j − in this case the 
probability of transitioning from s to t is the probability of j 
being chosen to spend a dollar and has someone willing and 
able to satisfy his request n − − β {i s i k} −ij 
 
multiplied by the probability of i being chosen to satisfy his 
request {i s i k} − ij ij is if j has k dollars 
and otherwise it is just a correction for the fact that j 
cannot satisfy his own request 
let ∆k 
denote the set of probability distributions on { k} 
we can think of an element of ∆k 
as describing the fraction 
of people with each amount of money this is a useful way 
of looking at the system since we typically don t care who 
has each amount of money but just the fraction of people 
that have each amount as before not all elements of ∆k 
are possible given our constraint that the total amount of 
 
money is m rather than thinking in terms of the total 
amount of money in the system it will prove more useful to 
think in terms of the average amount of money each player 
has of course the total amount of money in a system 
with n agents is m iff the average amount that each player 
has is m m n let ∆k 
m denote all distributions d ∈ ∆k 
such that e d m i e 
pk 
j d j j m given a state 
s ∈ sk n m let ds 
∈ ∆k 
m denote the distribution of money 
in s our goal is to show that if n is large then there is 
a distribution d∗ 
∈ ∆k 
m such that with high probability 
the markov chain mk n m will almost always be in a state 
s such that ds 
is close to d∗ 
 thus agents can base their 
decisions about what strategy to use on the assumption that 
they will be in such a state 
we can in fact completely characterize the distribution 
d∗ 
 given a distribution d ∈ ∆k 
 let 
h d − 
x 
{j d j } 
d j log d j 
denote the entropy of d if ∆ is a closed convex set of 
distributions then it is well known that there is a unique 
distribution in ∆ at which the entropy function takes its maximum 
value in ∆ since ∆k 
m is easily seen to be a closed convex set 
of distributions it follows that there is a unique distribution 
in ∆k 
m that we denote d∗ 
k m whose entropy is greater than 
that of all other distributions in ∆k 
m we now show that 
for n sufficiently large the markov chain mk n m is almost 
surely in a state s such that ds 
is close to d∗ 
k m n the 
statement is correct under a number of senses of close 
for definiteness we consider the euclidean distance given 
 let sk n m denote the set of states s in sk n mn such 
that 
pk 
j ds 
 j − d∗ 
k m 
 
given a markov chain m over a state space s and s ⊆ s 
let xt s s be the random variable that denotes that m is in 
a state of s at time t when started in state s 
theorem for all all k and all m there exists 
n such that for all n n and all states s ∈ sk n mn there 
exists a time t∗ 
 which may depend on k n m and such 
that for t t∗ 
 we have pr xt s sk n m − 
proof sketch suppose that at some time t pr xt s s 
is uniform for all s then the probability of being in a set of 
states is just the size of the set divided by the total number of 
states a standard technique from statistical mechanics is to 
show that there is a concentration phenomenon around the 
maximum entropy distribution more precisely using 
a straightforward combinatorial argument it can be shown 
that the fraction of states not in sk n m is bounded by 
p n ecn 
 where p is a polynomial this fraction clearly 
goes to as n gets large thus for sufficiently large n 
pr xt s sk n m − if pr xt s s is uniform 
it is relatively straightforward to show that our markov 
chain has a limit distribution π over sk n mn such that for 
all s s ∈ sk n mn limt→∞ pr xt s s πs let pij denote 
the probability of transitioning from state i to state j it 
is easily verified by an explicit computation of the 
transition probabilities that pij pji for all states i and j it 
immediatly follows from this symmetry that πs πs so 
π is uniform after a sufficient amount of time the 
distribution will be close enough to π that the probabilities are 
again bounded by constant which is sufficient to complete 
the theorem 
 
euclidean distance 
 
 
 
 
 
numberofsteps 
figure distance from maximum-entropy 
distribution with agents 
 
number of agents 
 
 
 
 
 
maximumdistance 
figure maximum distance from 
maximumentropy distribution over 
timesteps 
 
number of agents 
 
 
 
 
timetodistance 
figure average time to get within of the 
maximum-entropy distribution 
 
we performed a number of experiments that show that 
the maximum entropy behavior described in theorem 
arises quickly for quite practical values of n and t the 
first experiment showed that even if n we reach 
the maximum-entropy distribution quickly we averaged 
runs of the markov chain for k where there is enough 
money for each agent to have starting from a very extreme 
distribution every agent has either or and considered 
the average time needed to come within various distances 
of the maximum entropy distribution as figure shows 
after steps on average the euclidean distance from 
the average distribution of money to the maximum-entropy 
distribution is after steps the distance is down 
to note that this is really only real time units since 
with players we have transactions per time unit 
we then considered how close the distribution stays to 
the maximum entropy distribution once it has reached it 
to simplify things we started the system in a state whose 
distribution was very close to the maximum-entropy 
distribution and ran it for 
steps for various values of n 
as figure shows the system does not move far from the 
maximum-entropy distribution once it is there for 
example if n the system is never more than distance 
from the maximum-entropy distribution if n it is 
never more than from the maximum-entropy 
distribution 
finally we considered how more carefully how quickly 
the system converges to the maximum-entropy distribution 
for various values of n there are approximately kn 
 
possible states so the convergence time could in principle be 
quite large however we suspect that the markov chain 
that arises here is rapidly mixing which means that it will 
converge significantly faster see for more details about 
rapid mixing we believe that the actually time needed is 
o n this behavior is illustrated in figure which shows 
that for our example chain again averaged over runs 
after n steps the euclidean distance between the actual 
distribution of money in the system and the maximum-entropy 
distribution is less than 
 the game under strategic play 
we have seen that the system is well behaved if the agents 
all follow a threshold strategy we now want to show that 
there is a nontrivial nash equilibrium where they do so that 
is a nash equilibrium where all the agents use sk for some 
k this is not true in general if δ is small then agents 
have no incentive to work intuitively if future utility is 
sufficiently discounted then all that matters is the present 
and there is no point in volunteering to work with small 
δ s is the only equilibrium however we show that for δ 
sufficiently large there is another equilibrium in threshold 
strategies we do this by first showing that if every other 
agent is playing a threshold strategy then there is a best 
response that is also a threshold strategy although not 
necessarily the same one we then show that there must be 
some mixed threshold strategy for which this best response 
is the same strategy it follows that this tuple of threshold 
strategies is a nash equilibrium 
as a first step we show that for all k if everyone other 
than agent i is playing sk then there is a threshold 
strategy sk that is a best response for agent i to prove this 
we need to assume that the system is close to the 
steadystate distribution i e the maximum-entropy distribution 
however as long as δ is sufficiently close to we can ignore 
what happens during the period that the system is not in 
steady state 
we have thus far considered threshold strategies of the 
form sk where k is a natural number this is a discrete set 
of strategies for a later proof it will be helpful to have 
a continuous set of strategies if γ k γ where k is 
a natural number and ≤ γ let sγ be the strategy 
that performs sk with probability − γ and sk with 
probability γ note that we are not considering arbitrary 
mixed threshold strategies here but rather just mixing 
between adjacent strategies for the sole purpose of making out 
strategies continuous in a natural way theorem 
applies to strategies sγ the same proof goes through without 
change where γ is an arbitrary nonnegative real number 
theorem fix a strategy sγ and an agent i there 
exists δ∗ 
 and n∗ 
such that if δ δ∗ 
 n n∗ 
 and every 
agent other than i is playing sγ in game g n δ then there 
is an integer k such that the best response for agent i is sk 
either k is unique that is there is a unique best response 
that is also a threshold strategy or there exists an integer 
k such that sγ is a best response for agent i for all γ in 
the interval k k and these are the only best responses 
among threshold strategies 
proof sketch if δ is sufficiently large we can ignore 
what happens before the system converges to the 
maximumentropy distribution if n is sufficiently large then the 
strategy played by one agent will not affect the distribution of 
money significantly thus the probability of i moving from 
one state dollar amount to another depends only on i s 
strategy since we can take the probability that i will be 
chosen to make a request and the probability that i will 
be chosen to satisfy a request to be constant thus from 
i s point of view the system is a markov decision process 
 mdp and i needs to compute the optimal policy 
 strategy for this mdp it follows from standard results 
theorem that there is an optimal policy that is a 
threshold policy 
the argument that the best response is either unique or 
there is an interval of best responses follows from a more 
careful analysis of the value function for the mdp 
we remark that there may be best responses that are not 
threshold strategies all that theorem shows is that 
among best responses there is at least one that is a threshold 
strategy since we know that there is a best response that 
is a threshold strategy we can look for a nash equilibrium 
in the space of threshold strategies 
theorem for all m there exists δ∗ 
 and n∗ 
such 
that if δ δ∗ 
and n n∗ 
 there exists a nash equilibrium in 
the game g n δ where all agents play sγ for some integer 
γ 
proof it follows easily from the proof theorem that 
if br δ γ is the minimal best response threshold strategy if 
all the other agents are playing sγ and the discount factor is 
δ then for fixed δ br δ · is a step function it also follows 
 
formally we need to define the strategies when the system 
is far from equilibrium however these far from stochastic 
equilibrium strategies will not affect the equilibrium 
behavior when n is large and deviations from stochastic 
equilibrium are extremely rare 
 
from the theorem that if there are two best responses then 
a mixture of them is also a best response therefore if we 
can join the steps by a vertical line we get a best-response 
curve it is easy to see that everywhere that this 
bestresponse curve crosses the diagonal y x defines a nash 
equilibrium where all agents are using the same threshold 
strategy as we have already observed one such 
equilibrium occurs at if there are only m in the system we can 
restrict to threshold strategies sk where k ≤ m since 
no one can have more than m all strategies sk for k m 
are equivalent to sm these are just the strategies where 
the agent always volunteers in response to request made by 
someone who can pay clearly br δ sm ≤ m for all δ so 
the best response function is at or below the equilibrium at 
m if k ≤ m n every player will have at least k dollars 
and so will be unwilling to work and the best response is 
just consider k∗ 
 the smallest k such that k m n it 
is not hard to show that for k∗ 
there exists a δ∗ 
such that 
for all δ ≥ δ∗ 
 br δ k∗ 
 ≥ k∗ 
 it follows by continuity that if 
δ ≥ δ∗ 
 there must be some γ such that br δ γ γ this 
is the desired nash equilibrium 
this argument also shows us that we cannot in general 
expect fixed points to be unique if br δ k∗ 
 k∗ 
and 
br δ k k then our argument shows there must be 
a second fixed point in general there may be multiple fixed 
points even when br δ k∗ 
 k∗ 
 as illustrated in the figure 
 with n and m 
 
strategy of rest of agents 
 
 
 
 
 
 
bestresponse 
figure the best response function for n 
and m 
theorem allows us to restrict our design to agents 
using threshold strategies with the confidence that there will 
be a nontrivial equilibrium however it does not rule out 
the possibility that there may be other equilibria that do 
not involve threshold stratgies it is even possible although 
it seems unlikely that some of these equilibria might be 
better 
 social welfare and scalabity 
our theorems show that for each value of m and n for 
sufficiently large δ there is a nontrivial nash equilibrium 
where all the agents use some threshold strategy sγ m n 
from the point of view of the system designer not all 
equilibria are equally good we want an equilibrium where as few 
as possible agents have when they get a chance to make a 
request so that they can pay for the request and relatively 
few agents have more than the threshold amount of money 
 so that there are always plenty of agents to fulfill the 
request there is a tension between these objectives it is not 
hard to show that as the fraction of agents with increases 
in the maximum entropy distribution the fraction of agents 
with the maximum amount of money decreases thus our 
goal is to understand what the optimal amount of money 
should be in the system given the number of agents that 
is we want to know the amount of money m that maximizes 
efficiency i e the total expected utility if all the agents use 
sγ m n 
we first observe that the most efficient equilibrium 
depends only on the ratio of m to n not on the actual values 
of m and n 
theorem there exists n∗ 
such that for all games 
g n δ and g n δ where n n n∗ 
 if m n m n 
then sγ m n sγ m n 
proof fix m n r theorem shows that the 
maximum-entropy distribution depends only on k and the 
ratio m n not on m and n separately thus given r for 
each choice of k there is a unique maximum entropy 
distribution dk r the best response br δ k depends only on the 
distribution dk r not m or n thus the nash equilibrium 
depends only on the ratio r that is for all choices of m 
and n such that n is sufficiently large so that theorem 
applies and m n r the equilibrium strategies are the 
same 
in light of theorem the system designer should ensure 
that there is enough money m in the system so that the 
ratio between m n is optimal we are currently exploring 
exactly what the optimal ratio is as our very preliminary 
results for β show in figure the ratio appears to be 
monotone increasing in δ which matches the intuition that 
we should provide more patient agents with the opportunity 
to save more money additionally it appears to be relatively 
smooth which suggests that it may have a nice analytic 
solution 
 
discount rate ∆ 
 
 
 
 
 
optimalratioofmn 
figure optimal average amount of money to the 
nearest for β 
we remark that in practice it may be easier for the 
designer to vary the price of fulfilling a request rather than 
 
if there are multiple equilibria we take sγ m n to be the 
nash equilibrium that has highest efficiency for fixed m and 
n 
 
injecting money in the system this produces the same 
effect for example changing the cost of fulfilling a request 
from to is equivalent to halving the amount of money 
that each agent has similarly halving the the cost of 
fulfilling a request is equivalent to doubling the amount of money 
that everyone has with a fixed amount of money m there 
is an optimal product nc of the number of agents and the 
cost c of fulfilling a request 
theorem also tells us how to deal with a dynamic pool 
of agents our system can handle newcomers relatively 
easily simply allow them to join with no money this gives 
existing agents no incentive to leave and rejoin as 
newcomers we then change the price of fulfilling a request so that 
the optimal ratio is maintained this method has the nice 
feature that it can be implemented in a distributed fashion 
if all nodes in the system have a good estimate of n then 
they can all adjust prices automatically alternatively the 
number of agents in the system can be posted in a 
public place approaches that rely on adjusting the amount 
of money may require expensive system-wide computations 
 see for an example and must be carefully tuned to 
avoid creating incentives for agents to manipulate the 
system by which this is done 
note that in principle the realization that the cost of 
fulfilling a request can change can affect an agent s 
strategy for example if an agent expects the cost to increase 
then he may want to defer volunteering to fulfill a request 
however if the number of agents in the system is always 
increasing then the cost always decreases so there is never 
any advantage in waiting 
there may be an advantage in delaying a request but it 
is far more costly in terms of waiting costs than in 
providing service since we assume the need for a service is often 
subject to real waiting costs while the need to supply the 
service is merely to augment a money supply related 
issues are discussed in 
we ultimately hope to modify the mechanism so that the 
price of a job can be set endogenously within the system 
 as in real-world economies with agents bidding for jobs 
rather than there being a fixed cost set externally however 
we have not yet explored the changes required to implement 
this change thus for now we assume that the cost is set 
as a function of the number of agents in the system and 
that there is no possibility for agents to satisfy a request for 
less than the official cost or for requesters to offer to pay 
more than it 
 sybils and collusion 
in a naive sense our system is essentially sybil-proof to 
get d dollars his sybils together still have to perform d units 
of work moreover since newcomers enter the system with 
 there is no benefit to creating new agents simply to take 
advantage of an initial endowment nevertheless there are 
some less direct ways that an agent could take advantage 
of sybils first by having more identities he will have a 
greater probability of getting chosen to make a request it 
is easy to see that this will lead to the agent having higher 
total utility however this is just an artifact of our model 
to make our system simple to analyze we have assumed 
that request opportunities came uniformly at random in 
practice requests are made to satisfy a desire our model 
implicitly assumed that all agents are equally likely to have 
a desire at any particular time having sybils should not 
increase the need to have a request satisfied indeed it would 
be reasonable to assume that sybils do not make requests at 
all 
second having sybils makes it more likely that one of the 
sybils will be chosen to fulfill a request this can allow a 
user to increase his utility by setting a lower threshold that 
is to use a strategy sk where k is smaller than the k used 
by the nash equilibrium strategy intuitively the need for 
money is not as critical if money is easier to obtain 
unlike the first concern this seems like a real issue it seems 
reasonable to believe that when people make a decision 
between a number of nodes to satisfy a request they do so 
at random at least to some extent even if they look for 
advertised node features to help make this decision sybils 
would allow a user to advertise a wide range of features 
third an agent can drive down the cost of fulfilling a 
request by introducing many sybils similarly he could 
increase the cost and thus the value of his money by making 
a number of sybils leave the system concievably he could 
alternate between these techniques to magnify the effects of 
work he does we have not yet calculated the exact effect of 
this change it interacts with the other two effects of having 
sybils that we have already noted given the number of 
sybils that would be needed to cause a real change in the 
perceived size of a large p p network the practicality of this 
attack depends heavily on how much sybils cost an attacker 
and what resources he has available 
the second point raised regarding sybils also applies to 
collusion if we allow money to be loaned if k agents 
collude they can agree that if one runs out of money another 
in the group will loan him money by pooling their money 
in this way the k agents can again do better by setting a 
higher threshold note that the loan mechanism doesn t 
need to be built into the system the agents can simply use 
a fake transaction to transfer the money these appear 
to be the main avenues for collusive attacks but we are still 
exploring this issue 
 conclusion 
we have given a formal analysis of a scrip system and 
have shown that the existence of a nash equilibrium where 
all agents use a threshold strategy moreover we can 
compute efficiency of equilibrium strategy and optimize the price 
 or money supply to maximize efficiency thus our 
analysis provides a formal mechanisms for solving some important 
problems in implementing scrip systems it tells us that with 
a fixed population of rational users such systems are very 
unlikely to become unstable thus if this stability is 
common belief among the agents we would not expect inflation 
bubbles or crashes because of agent speculation however 
we cannot rule out the possibility that that agents may have 
other beliefs that will cause them to speculate our 
analysis also tells us how to scale the system to handle an influx 
of new users without introducing these problems scale the 
money supply to keep the average amount of money constant 
 or equivalently adjust prices to achieve the same goal 
there are a number of theoretical issues that are still open 
including a characterization of the multiplicity of 
equilibria - are there usually in addition we expect that one 
should be able to compute analytic estimates for the best 
response function and optimal pricing which would allow us 
to understand the relationship between pricing and various 
parameters in the model 
 
it would also be of great interest to extend our analysis 
to handle more realistic settings we mention a few possible 
extensions here 
 we have assumed that the world is homogeneous in a 
number of ways including request frequency utility 
and ability to satisfy requests it would be 
interesting to examine how relaxing any of these assumptions 
would alter our results 
 we have assumed that there is no cost to an agent 
to be a member of the system suppose instead that 
we imposed a small cost simply for being present in 
the system to reflect the costs of routing messages and 
overlay maintainance this modification could have a 
significant impact on sybil attacks 
 we have described a scrip system that works when 
there are no altruists and have shown that no system 
can work once there there are sufficiently many 
altruists what happens between these extremes 
 one type of irrational behavior encountered with 
scrip systems is hoarding there are some similarities 
between hoarding and altruistic behavior while an 
altruist provide service for everyone a hoarder will 
volunteer for all jobs in order to get more money and 
rarely request service so as not to spend money it 
would be interesting to investigate the extent to which 
our system is robust against hoarders clearly with 
too many hoarders there may not be enough money 
remaining among the non-hoarders to guarantee that 
typically a non-hoarder would have enough money to 
satisfy a request 
 finally in p p filesharing systems there are 
overlapping communities of various sizes that are significantly 
more likely to be able to satisfy each other s requests 
it would be interesting to investigate the effect of such 
communities on the equilibrium of our system 
there are also a number of implementation issues that 
would have to be resolved in a real system for example we 
need to worry about the possibility of agents counterfeiting 
money or lying about whether service was actually provided 
karma provdes techniques for dealing with both of these 
issues and a number of others but some of karma s 
implementation decisions point to problems for our model for 
example it is prohibitively expensive to ensure that bank 
account balances can never go negative a fact that our model 
does not capture another example is that karma has nodes 
serve as bookkeepers for other nodes account balances like 
maintaining a presence in the network this imposes a cost 
on the node but unlike that responsibility it can be easily 
shirked karma suggests several ways to incentivize nodes 
to perform these duties we have not investigated whether 
these mechanisms be incorporated without disturbing our 
equilibrium 
 acknowledgements 
we would like to thank emin gun sirer shane 
henderson jon kleinberg and anonymous referees for helpful 
suggestions ef ik and jh are supported in part by nsf 
under grant itr- jh is also supported in part 
by nsf under grants ctc- and iis- by 
onr under grant n - - - by the dod 
multidisciplinary university research initiative muri program 
administered by the onr under grants n - - - and 
n - - - and by afosr under grant 
f - - 
 references 
 e adar and b a huberman free riding on 
gnutella first monday 
 k g anagnostakis and m greenwald 
exchange-based incentive mechanisms for peer-to-peer 
file sharing in international conference on distributed 
computing systems icdcs pages - 
 bittorrent inc bittorrent web site 
http www bittorent com 
 a cheng and e friedman sybilproof reputation 
mechanisms in workshop on economics of 
peer-to-peer systems p pecon pages - 
 
 cornell information technologies cornell s 
ccommodity internet usage statistics 
http www cit cornell edu computer students 
bandwidth charts html 
 j r douceur the sybil attack in international 
workshop on peer-to-peer systems iptps pages 
 - 
 g ellison cooperation in the prisoner s dilemma 
with anonymous random matching review of 
economic studies - 
 emule project emule web site 
http www emule-project net 
 m feldman k lai i stoica and j chuang robust 
incentive techniques for peer-to-peer networks in 
acm conference on electronic commerce ec 
pages - 
 e j friedman and d c parkes pricing wifi at 
starbucks issues in online mechanism design in ec 
 proceedings of the th acm conference on 
electronic commerce pages - acm press 
 
 e j friedman and p resnick the social cost of 
cheap pseudonyms journal of economics and 
management strategy - 
 r guha r kumar p raghavan and a tomkins 
propagation of trust and distrust in conference on 
the world wide web www pages - 
 m gupta p judge and m h ammar a reputation 
system for peer-to-peer networks in network and 
operating system support for digital audio and 
video nossdav pages - 
 z gyongi p berkhin h garcia-molina and 
j pedersen link spam detection based on mass 
estimation technical report stanford university 
 
 j ioannidis s ioannidis a d keromytis and 
v prevelakis fileteller paying and getting paid for 
file storage in financial cryptography pages - 
 
 e t jaynes where do we stand on maximum 
entropy in r d levine and m tribus editors the 
maximum entropy formalism pages - mit 
press cambridge mass 
 
 s d kamvar m t schlosser and h garcia-molina 
the eigentrust algorithm for reputation management 
in p p networks in conference on the world wide 
web www pages - 
 m kandori social norms and community 
enforcement review of economic studies - 
 
 logisense corporation logisense web site 
http www logisense com tm p p html 
 l lovasz and p winkler mixing of random walks 
and other diffusions on a graph in surveys in 
combinatorics walker ed london 
mathematical society lecture note series 
cambridge university press 
 open source technology group slashdot 
faqcomments and moderation 
http slashdot org faq com-mod shtml cm 
 osmb llc gnutella web site 
http www gnutella com 
 m l puterman markov decision processes wiley 
 
 seti home seti home web page 
http setiathome ssl berkeley edu 
 sharman networks ltd kazaa web site 
http www kazaa com 
 v vishnumurthy s chandrakumar and e sirer 
karma a secure economic framework for peer-to-peer 
resource sharing in workshop on economics of 
peer-to-peer systems p pecon 
 l xiong and l liu building trust in decentralized 
peer-to-peer electronic communities in internation 
conference on electronic commerce research 
 icecr 
 h zhang a goel r govindan k mason and b v 
roy making eigenvector-based reputation systems 
robust to collusion in workshop on algorithms and 
models for the web-graph waw pages - 
 
 
multi-attribute coalitional games∗ 
samuel ieong 
† 
computer science department 
stanford university 
stanford ca 
sieong cs stanford edu 
yoav shoham 
computer science department 
stanford university 
stanford ca 
shoham cs stanford edu 
abstract 
we study coalitional games where the value of cooperation 
among the agents are solely determined by the attributes 
the agents possess with no assumption as to how these 
attributes jointly determine this value this framework 
allows us to model diverse economic interactions by picking 
the right attributes we study the computational 
complexity of two coalitional solution concepts for these 
gamesthe shapley value and the core we show how the positive 
results obtained in this paper imply comparable results for 
other games studied in the literature 
categories and subject descriptors 
i distributed artificial intelligence multiagent 
systems j social and behavioral sciences 
economics f analysis of algorithms and problem 
complexity 
general terms 
algorithms economics 
 introduction 
when agents interact with one another the value of their 
contribution is determined by what they can do with their 
skills and resources rather than simply their identities 
consider the problem of forming a soccer team for a team to 
be successful a team needs some forwards midfielders 
defenders and a goalkeeper the relevant attributes of the 
players are their skills at playing each of the four positions 
the value of a team depends on how well its players can play 
these positions at a finer level we can extend the model 
to consider a wider range of skills such as passing 
shooting and tackling but the value of a team remains solely a 
function of the attributes of its players 
consider an example from the business world 
companies in the metals industry are usually vertically-integrated 
and diversified they have mines for various types of ores 
and also mills capable of processing and producing 
different kinds of metal they optimize their production profile 
according to the market prices for their products for 
example when the price of aluminum goes up they will 
allocate more resources to producing aluminum however each 
company is limited by the amount of ores it has and its 
capacities in processing given kinds of ores two or more 
companies may benefit from trading ores and processing 
capacities with one another to model the metal industry the 
relevant attributes are the amount of ores and the 
processing capacities of the companies given the exogenous input 
of market prices the value of a group of companies will be 
determined by these attributes 
many real-world problems can be likewise modeled by 
picking the right attributes as attributes apply to both 
individual agents and groups of agents we propose the use 
of coalitional game theory to understand what groups may 
form and what payoffs the agents may expect in such models 
coalitional game theory focuses on what groups of agents 
can achieve and thus connects strongly with e-commerce 
as the internet economies have significantly enhanced the 
abilities of business to identify and capitalize on profitable 
opportunities of cooperation our goal is to understand 
the computational aspects of computing the solution 
concepts stable and or fair distribution of payoffs formally 
defined in section for coalitional games described using 
attributes our contributions can be summarized as follows 
 we define a formal representation for coalitional games 
based on attributes and relate this representation to 
others proposed in the literature we show that when 
compared to other representations there exists games 
for which a multi-attribute description can be 
exponentially more succinct and for no game it is worse 
 given the generality of the model positive results carry 
over to other representations we discuss two positive 
results in the paper one for the shapley value and one 
for the core and show how these imply related results 
in the literature 
 
 we study an approximation heuristic for the shapley 
value when its exact values cannot be found efficiently 
we provide an explicit bound on the maximum error 
of the estimate and show that the bound is 
asymptotically tight we also carry out experiments to evaluate 
how the heuristic performs on random instances 
 related work 
coalitional game theory has been well studied in 
economics a vast amount of literature have focused 
on defining and comparing solution concepts and 
determining their existence and properties the first algorithmic 
study of coalitional games as far as we know is performed 
by deng and papadimitriou in they consider coalitional 
games defined on graphs where the players are the vertices 
and the value of coalition is determined by the sum of the 
weights of the edges spanned by these players this can be 
efficiently modeled and generalized using attributes 
as a formal representation multi-attribute coalitional games 
is closely related to the multi-issue representation of conitzer 
and sandholm and our work on marginal contribution 
networks both of these representations are based on 
dividing a coalitional game into subgames termed issues 
in and rules in and aggregating the subgames via 
linear combination the key difference in our work is the 
unrestricted aggregation of subgames the aggregation could 
be via a polynomial function of the attributes or even by 
treating the subgames as input to another computational 
problem such as a min-cost flow problem the relationship 
of these models will be made clear after we define the 
multiattribute representation in section 
another representation proposed in the literature is one 
specialized for superadditive games by conitzer and 
sandholm this representation is succinct but to find the 
values of some coalitions may require solving an np-hard 
problem while it is possible for multi-attribute coalitional 
games to efficiently represent these games it necessarily 
requires the solution to an np-hard problem in order to find 
out the values of some coalitions in this paper we stay 
within the boundary of games that admits efficient 
algorithm for determining the value of coalitions we will 
therefore not make further comparisons with 
the model of coalitional games with attributes has been 
considered in the works of shehory and kraus they model 
the agents as possessing capabilities that indicates their 
proficiencies in different areas and consider how to efficiently 
allocate tasks and the dynamics of coalition formation 
 our work differs significantly as our focus is on 
reasoning about solution concepts our model also covers a wider 
scope as attributes generalize the notion of capabilities 
yokoo et al have also considered a model of coalitional 
games where agents are modeled by sets of skills and these 
skills in turn determine the value of coalitions there are 
two major differences between their work and ours firstly 
yokoo et al assume that each skill is fundamentally different 
from another hence no two agents may possess the same 
skill also they focus on developing new solution concepts 
that are robust with respect to manipulation by agents our 
focus is on reasoning about traditional solution concepts 
 
we acknowledge that random instances may not be typical 
of what happens in practice but given the generality of our 
model it provides the most unbiased view 
our work is also related to the study of cooperative games 
with committee control in these games there is usually 
an underlying set of resources each controlled by a 
 possibly overlapping set of players known as the committee 
engaged in a simple game defined in section 
multiattribute coalitional games generalize these by considering 
relationship between the committee and the resources 
beyond simple games we note that when restricted to simple 
games we derive similar results to that in 
 preliminaries 
in this section we will review the relevant concepts of 
coalitional game theory and its two most important solution 
concepts - the shapley value and the core we will then 
define the computational questions that will be studied in 
the second half of the paper 
 coalitional games 
throughout this paper we assume that payoffs to groups 
of agents can be freely distributed among its members this 
transferable utility assumption is commonly made in 
coalitional game theory the canonical representation of a 
coalitional game with transferable utility is its characteristic form 
definition a coalition game with transferable utility in 
characteristic form is denoted by the pair n v where 
 n is the set of agents and 
 v n 
→ r is a function that maps each group of 
agents s ⊆ n to a real-valued payoff 
a group of agents in a game is known as a coalition and the 
entire set of agents is known as the grand coalition 
an important class of coalitional games is the class of 
monotonic games 
definition a coalitional game is monotonic if for all 
s ⊂ t ⊆ n v s ≤ v t 
another important class of coalitional games is the class 
of simple games in a simple game a coalition either wins 
in which case it has a value of or loses in which case it 
has a value of it is often used to model voting situations 
simple games are often assumed to be monotonic i e if s 
wins then for all t ⊇ s t also wins this coincides with 
the notion of using simple games as a model for voting if a 
simple game is monotonic then it is fully described by the 
set of minimal winning coalitions i e coalitions s for which 
v s but for all coalitions t ⊂ s v t 
an outcome in a coalitional game specifies the utilities 
the agents receive a solution concept assigns to each 
coalitional game a set of reasonable outcomes different 
solution concepts attempt to capture in some way outcomes 
that are stable and or fair two of the best known solution 
concepts are the shapley value and the core 
the shapley value is a normative solution concept that 
prescribes a fair way to divide the gains from cooperation 
when the grand coalition is formed the division of payoff 
to agent i is the average marginal contribution of agent i 
over all possible permutations of the agents formally 
definition the shapley value of agent i φi v in game 
n v is given by the following formula 
φi v 
s⊆n\{i} 
 s n − s − 
 n 
 v s ∪ {i} − v s 
 
the core is a descriptive solution concept that focuses on 
outcomes that are stable stability under core means that 
no set of players can jointly deviate to improve their payoffs 
definition an outcome x ∈ r n 
is in the core of the 
game n v if for all s ⊆ n 
i∈s 
xi ≥ v s 
note that the core of a game may be empty i e there may 
not exist any payoff vector that satisfies the stability 
requirement for the given game 
 computational problems 
we will study the following three problems related to 
solution concepts in coalitional games 
problem shapley value given a description of the 
coalitional game and an agent i compute the shapley value 
of agent i 
problem core membership given a description of 
the coalitional game and a payoff vector x such that 
è 
i∈n xi 
v n determine if 
è 
i∈s xi ≥ v s for all s ⊆ n 
problem core non-emptiness given a description 
of the coalitional game determine if there exists any payoff 
vector x such that 
è 
i∈s xi ≥ v s for all s ⊆ n andè 
i∈n xi v n 
note that the complexity of the above problems depends 
on the how the game is described all these problems will 
be easy if the game is described by its characteristic form 
but only so because the description takes space 
exponential in the number of agents and hence simple brute-force 
approach takes time polynomial to the input description 
to properly understand the computational complexity 
questions we have to look at compact representation 
 formal model 
in this section we will give a formal definition of 
multiattribute coalitional games and show how it is related to 
some of the representations discussed in the literature we 
will also discuss some limitations to our proposed approach 
 multi-attribute coalitional games 
a multi-attribute coalitional game macg consists of 
two parts a description of the attributes of the agents 
which we termed an attribute model and a function that 
assigns values to combination of attributes together they 
induce a coalitional game over the agents we first define 
the attribute model 
definition an attribute model is a tuple n m a where 
 n denotes the set of agents of size n 
 m denotes the set of attributes of size m 
 a ∈ rm×n 
 the attribute matrix describes the values 
of the attributes of the agents with aij denoting the 
value of attribute i for agent j 
we can directly define a function that maps combinations 
of attributes to real values however for many problems 
we can describe the function more compactly by computing 
it in two steps we first compute an aggregate value for 
each attribute then compute the values of combination of 
attributes using only the aggregated information formally 
definition an aggregating function or aggregator takes 
as input a row of the attribute matrix and a coalition s and 
summarizes the attributes of the agents in s with a single 
number we can treat it as a mapping from rn 
× n 
→ r 
aggregators often perform basic arithmetic or logical 
operations for example it may compute the sum of the 
attributes or evaluate a boolean expression by treating the 
agents i ∈ s as true and j ∈ s as false analogous to the 
notion of simple games we call an aggregator simple if its 
range is { } for any aggregator there is a set of relevant 
agents and a set of irrelevant agents an agent i is 
irrelevant to aggregator aj 
if aj 
 s ∪ {i} aj 
 s for all s ⊆ n 
a relevant agent is one not irrelevant 
given the attribute matrix an aggregator assigns a value 
to each coalition s ⊆ n thus each aggregator defines a 
game over n for aggregator aj 
 we refer to this induced 
game as the game of attribute j and denote it with aj 
 a 
when the attribute matrix is clear from the context we may 
drop a and simply denote the game as aj 
 we may refer to 
the game as the aggregator when no ambiguities arise 
we now define the second step of the computation with 
the help of aggregators 
definition an aggregate value function takes as input 
the values of the aggregators and maps these to a real value 
in this paper we will focus on having one aggregator per 
attribute therefore in what follows we will refer to the 
aggregate value function as a function over the attributes 
note that when all aggregators are simple the aggregate 
value function implicitly defines a game over the attributes 
as it assigns a value to each set of attributes t ⊆ m we 
refer to this as the game among attributes 
we now define multi-attribute coalitional game 
definition a multi-attribute coalitional game is defined 
by the tuple n m a a w where 
 n m a is an attribute model 
 a is a set of aggregators one for each attribute we can 
treat the set together as a vector function mapping 
rm×n 
× n 
→ rm 
 w rm 
→ r is an aggregate value function 
this induces a coalitional game with transferable payoffs 
n v with players n and the value function defined by 
v s w a a s 
note that macg as defined is fully capable of 
representing any coalitional game n v we can simply take the set 
of attributes as equal to the set of agents i e m n an 
identity matrix for a aggregators of sums and the 
aggregate value function w to be v 
 
 an example 
let us illustrate how macg can be used to represent a 
game with a simple example suppose there are four types 
of resources in the world gold silver copper and iron that 
each agent is endowed with some amount of these resources 
and there is a fixed price for each of the resources in the 
market this game can be described using macg with an 
attribute matrix a where aij denote the amount of resource 
i that agent j is endowed for each resource the 
aggregator sums together the amount of resources the agents have 
finally the aggregate value function takes the dot product 
between the market price vector and the aggregate vector 
note the inherent flexibility in the model only limited 
work would be required to update the game as the market 
price changes or when a new agent arrives 
 relationship with other representations 
as briefly discussed in section macg is closely related 
to two other representations in the literature the 
multiissue representation of conitzer and sandholm and our 
work on marginal contribution nets to make their 
relationships clear we first review these two representations 
we have changed the notations from the original papers to 
highlight their similarities 
definition a multi-issue representation is given as a 
vector of coalitional games v v vm each possibly 
with a varying set of agents say n nm the 
coalitional game n v induced by multi-issue representation has 
player set n 
ëm 
i ni and for each coalition s ⊆ n 
v s 
èm 
i v s ∩ ni the games vi are assumed to be 
represented in characteristic form 
definition a marginal contribution net is given as a 
set of rules r r rm where rule ri has a weight wi 
and a pattern pi that is a conjunction over literals 
 positive or negative the agents are represented as literals a 
coalition s is said to satisfy the pattern pi if we treat the 
agents i ∈ s as true an agent j ∈ s as false pi s 
evaluates to true denote the set of literals involved in rule i 
by ni the coalitional game n v induced by a marginal 
contribution net has player set n 
ëm 
i ni and for each 
coalition s ⊆ n v s 
è 
i pi s true wi 
from these definitions we can see the relationships among 
these three representations clearly an issue of a multi-issue 
representation corresponds to an attribute in macg 
similarly a rule of a marginal contribution net corresponds to 
an attribute in macg the aggregate value functions are 
simple sums and weighted sums for the respective 
representations therefore it is clear that macg will be no less 
succinct than either representation 
however macg differs in two important way firstly 
there is no restriction on the operations performed by the 
aggregate value function over the attributes this is an 
important generalization over the linear combination of issues 
or rules in the other two approaches in particular there are 
games for which macg can be exponentially more compact 
the proof of the following proposition can be found in the 
appendix 
proposition consider the parity game n v where 
coalition s ⊆ n has value v s if s is odd and v s 
 otherwise macg can represent the game in o n space 
both multi-issue representation and marginal contribution 
nets requires o n 
 space 
a second important difference of macg is that the 
attribute model and the value function is cleanly separated 
as suggested in the example in section this often 
allows us more efficient update of the values of the game as it 
changes also the same attribute model can be evaluated 
using different value functions and the same value function 
can be used to evaluate different attribute model therefore 
macg is very suitable for representing multiple games we 
believe the problems of updating games and representing 
multiple games are interesting future directions to explore 
 limitation of one aggregator per attribute 
before focusing on one aggregator per attribute for the 
rest of the paper it is natural to wonder if any is lost per 
such restriction the unfortunate answer is yes best 
illustrated by the following consider again the problem of 
forming a soccer team discussed in the introduction where 
we model the attributes of the agents as their ability to take 
the four positions of the field and the value of a team 
depends on the positions covered if we first aggregate each 
of the attribute individually we will lose the distributional 
information of the attributes in other words we will not 
be able to distinguish between two teams one of which has 
a player for each position the other has one player who can 
play all positions but the rest can only play the same one 
position 
this loss of distributional information can be recovered 
by using aggregators that take as input multiple rows of 
the attribute matrix rather than just a single row 
alternatively if we leave such attributes untouched we can leave 
the burden of correctly evaluating these attributes to the 
aggregate value function however for many problems that we 
found in the literature such as the transportation domain 
of and the flow game setting of the distribution of 
attributes does not affect the value of the coalitions in 
addition the problem may become unmanageably complex as 
we introduce more complicated aggregators therefore we 
will focus on the representation as defined in definition 
 shapley value 
in this section we focus on computational issues of 
finding the shapley value of a player in macg we first set 
up the problem with the use of oracles to avoid 
complexities arising from the aggregators we then show that when 
attributes are linearly separable the shapley value can be 
efficiently computed this generalizes the proofs of related 
results in the literature for the non-linearly separable case 
we consider a natural heuristic for estimating the shapley 
value and study the heuristic theoretically and empirically 
 problem setup 
we start by noting that computing the shapley value for 
simple aggregators can be hard in general in particular we 
can define aggregators to compute weighted majority over 
its input set of agents as noted in finding the shapley 
value of a weighted majority game is p-hard therefore 
discussion of complexity of shapley value for macg with 
unrestricted aggregators is moot 
instead of placing explicit restriction on the aggregator 
we assume that the shapley value of the aggregator can be 
 
answered by an oracle for notation let φi u denote the 
shapley value for some game u we make the following 
assumption 
assumption for each aggregator aj 
in a macg there 
is an associated oracle that answers the shapley value of the 
game of attribute j in other words φi aj 
 is known 
for many aggregators that perform basic operations over 
its input polynomial time oracle for shapley value exists 
this include operations such as sum and symmetric 
functions when the attributes are restricted to { } also when 
only few agents have an effect on the aggregator brute-force 
computation for shapley value is feasible therefore the 
above assumption is reasonable for many settings in any 
case such abstraction allows us to focus on the aggregate 
value function 
 linearly separable attributes 
when the aggregate value function can be written as a 
linear function of the attributes the shapley value of the 
game can be efficiently computed 
theorem given a game n v represented as a macg 
n m a a w if the aggregate value function can be 
written as a linear function of its attributes i e 
w a a s 
m 
j 
cj aj 
 a s 
the shapley value of agent i in n v is given by 
φi v 
m 
j 
cj φi aj 
 
proof first we note that shapley value satisfies an 
additivity axiom 
the shapley value satisfies additivity namely 
φi a b φi a φi b where n a b is 
a game defined to be a b s a s b s 
for all s ⊆ n 
it is also clear that shapley value satisfies scaling namely 
φi αv αφi v 
where αv s αv s for all s ⊆ n 
since the aggregate value function can be expressed as a 
weighted sum of games of attributes 
φi v φi w a φi 
m 
j 
cjaj 
 
m 
j 
cjφi aj 
 
many positive results regarding efficient computation of 
shapley value in the literature depends on some form of 
linearity examples include the edge-spanning game on graphs 
by deng and papadimitriou the multi-issue 
representation of and the marginal contribution nets of the 
key to determine if the shapley value can be efficiently 
computed depends on the linear separability of attributes once 
this is satisfied as long as the shapley value of the game of 
attributes can be efficiently determined the shapley value 
of the entire game can be efficiently computed 
corollary the shapley value for the edge-spanning 
game of games in multi-issue representation and 
games in marginal contribution nets can be computed in 
polynomial time 
 polynomial combination of attributes 
when the aggregate value function cannot be expressed 
as a linear function of its attributes computing the shapley 
value exactly is difficult here we will focus on aggregate 
value function that can be expressed as some polynomial 
of its attributes if we do not place a limit on the degree 
of the polynomial and the game n v is not necessarily 
monotonic the problem is p-hard 
theorem computing the shapley value of a macg 
n m a a w when w can be an arbitrary polynomial of 
the aggregates a is p-hard even when the shapley value 
of each aggregator can be efficiently computed 
the proof is via reduction from three-dimensional matching 
and details can be found in the appendix 
even if we restrict ourselves to monotonic games and 
non-negative coefficients for the polynomial aggregate value 
function computing the exact shapley value can still be 
hard for example suppose there are two attributes all 
agents in some set b ⊆ n possess the first attribute and all 
agents in some set c ⊆ n possess the second and b and c 
are disjoint for a coalition s ⊆ n the aggregator for the 
first evaluates to if and only if s ∩ b ≥ b and similarly 
the aggregator for the second evaluates to if and only if 
 s ∩ c ≥ c let the cardinality of the sets b and c be b 
and c we can verify that the shapley value of an agent i in 
b equals 
φi 
 
b 
b − 
i 
 b 
i 
 c 
c − 
 
 b c 
c i− 
 
c − c 
b c − c − i 
the equation corresponds to a weighted sum of probability 
values of hypergeometric random variables the 
correspondence with hypergeometric distribution is due to sampling 
without replacement nature of shapley value as far as we 
know there is no close-form formula to evaluate the sum 
above in addition as the number of attributes involved 
increases we move to multi-variate hypergeometric random 
variables and the number of summands grow exponentially 
in the number of attributes therefore it is highly unlikely 
that the exact shapley value can be determined efficiently 
therefore we look for approximation 
 approximation 
first we need a criteria for evaluating how well an 
estimate ˆφ approximates the true shapley value φ we 
consider the following three natural criteria 
 maximum underestimate maxi φi ˆφi 
 maximum overestimate maxi 
ˆφi φi 
 total variation 
 
è 
i φi − ˆφi or alternatively 
maxs 
è 
i∈s φi − 
è 
i∈s 
ˆφi 
the total variation criterion is more meaningful when we 
normalize the game to having a value of for the grand 
coalition i e v n we can also define additive 
analogues of the under- and overestimates especially when the 
games are normalized 
 
we will assume for now that the aggregate value 
function is a polynomial over the attributes with non-negative 
coefficients we will also assume that the aggregators are 
simple we will evaluate a specific heuristic that is 
analogous to equation suppose the aggregate function can 
be written as a polynomial with p terms 
w a a s 
p 
j 
cj aj 
 a s aj 
 a s · · · aj kj 
 a s 
 
for term j the coefficient of the term is cj its degree kj 
and the attributes involved in the term are j j kj 
we compute an estimate ˆφ to the shapley value as 
ˆφi 
p 
j 
kj 
l 
cj 
kj 
φi aj l 
 
the idea behind the estimate is that for each term we divide 
the value of the term equally among all its attributes this 
is represented by the factor 
cj 
kj 
 then for for each attribute 
of an agent we assign the player a share of value from the 
attribute this share is determined by the shapley value 
of the simple game of that attribute without considering 
the details of the simple games this constitutes a fair but 
blind rule of sharing 
 theoretical analysis of heuristic 
we can derive a simple and tight bound for the maximum 
 multiplicative underestimate of the heuristic estimate 
theorem given a game n v represented as a macg 
n m a a w suppose w can be expressed as a 
polynomial function of its attributes cf equation let k 
maxjkj i e the maximum degree of the polynomial let ˆφ 
denote the estimated shapley value using equation and 
φ denote the true shapley value for all i ∈ n φi ≥ k ˆφi 
proof we bound the maximum underestimate 
term-byterm let tj be the j-th term of the polynomial we note 
that the term can be treated as a game among attributes 
as it assigns a value to each coalition s ⊆ n without loss 
of generality renumber attributes j through j kj as 
through kj 
tj s cj 
kj 
l 
al 
 a s 
to make the equations less cluttered let 
b n s 
 s n − s − 
 n 
and for a game a contribution of agent i to group s i ∈ s 
∆i a s a s ∪ {i} − a s 
the true shapley value of the game tj is 
φi tj cj 
s⊆n\{i} 
b n s ∆i tj s 
for each coalition s i ∈ s ∆i tj s if and only if for 
at least one attribute say l∗ 
 ∆i al∗ 
 s therefore if 
we sum over all the attributes we would have included l∗ 
for sure 
φi tj ≤ cj 
kj 
j s⊆n\{i} 
b n s ∆i aj 
 s 
 kj 
kj 
j 
cj 
kj 
φi aj 
 
 kj 
ˆφi t 
summing over the terms we see that the worst case 
underestimate is by the maximum degree 
without loss of generality since the bound is 
multiplicative we can normalize the game to having v n as a 
corollary because we cannot overestimate any set by more 
than k times we obtain a bound on the total variation 
corollary the total variation between the estimated 
shapley value and the true shapley value for k-degree bounded 
polynomial aggregate value function is k− 
k 
 
we can show that this bound is tight 
example consider a game with n players and k 
attributes let the first n− agents be a member of the first 
 k − attributes and that the corresponding aggregator 
returns if any one of the first k − agents is present 
let the n-th agent be the sole member of the k-th attribute 
the estimated shapley will assign a value of k− 
k 
 
n− 
to the 
first n − agents and 
k 
to the n-th agent however the 
true shapley value of the n-th agent tends to as n → ∞ 
and the total variation approaches k− 
k 
 
in general we cannot bound how much ˆφ may 
overestimate the true shapley value the problem is that ˆφi may 
be non-zero for agent i even though may have no influence 
over the outcome of a game when attributes are multiplied 
together as illustrated by the following example 
example consider a game with players and 
attributes and let the first agent be a member of both 
attributes and the other agent a member of the second 
attribute for a coalition s the first aggregator evaluates to 
 if agent ∈ s and the second aggregator evaluates to if 
both agents are in s while agent is not a dummy with 
respect to the second attribute it is a dummy with respect 
to the product of the attributes agent will be assigned a 
value of 
 
by the estimate 
as mentioned for simple monotonic games a game is fully 
described by its set of minimal winning coalitions when 
the simple aggregators are represented as such it is possible 
to check in polynomial time for agents turning dummies 
after attributes are multiplied together therefore we can 
improve the heuristic estimate in this special case 
 empirical evaluation 
due to a lack of benchmark problems for coalitional games 
we have tested the heuristic on random instances we 
believe more meaningful results can be obtained when we have 
real instances to test this heuristic on 
our experiment is set up as follows we control three 
parameters of the experiment the number of players − 
 
 
 
 
 
 
 
 
 
 
 
no of players 
totalvariationdistance 
 
 
 
 
 a effect of max degree 
 
 
 
 
 
 
 
 
 
 
no of players 
totalvariationdistance 
 
 
 
 b effect of number of attributes 
figure experimental results 
the number of attributes − and the maximum degree 
of the polynomial − for each attribute we randomly 
sample one to three minimal winning coalitions we then 
randomly generate a polynomial of the desired maximum 
degree with a random number − of terms each with 
a random positive weight we normalize each game to have 
v n the results of the experiments are shown in 
figure the y-axis of the graphs shows the total variation 
and the x-axis the number of players each datapoint is an 
average of approximately random samples 
figure a explores the effect of the maximum degree 
and the number of players when the number of attributes is 
fixed at six as expected the total variation increases as 
the maximum degree increases on the other hand there is 
only a very small increase in error as the number of players 
increases the error is nowhere near the theoretical 
worstcase bound of 
 
to 
 
for polynomials of degrees to 
figure b explores the effect of the number of attributes 
and the number of players when the maximum degree of the 
polynomial is fixed at three we first note that these three 
lines are quite tightly clustered together suggesting that the 
number of attributes has relatively little effect on the error 
of the estimate as the number of attributes increases the 
total variation decreases we think this is an interesting 
phenomenon this is probably due to the precise construct 
required for the worst-case bound and so as more attributes 
are available we have more diverse terms in the polynomial 
and the diversity pushes away from the worst-case bound 
 core-related questions 
in this section we look at the complexity of the two 
computational problems related to the core core 
nonemptiness and core membership we show that the 
nonemptiness of core of the game among attributes and the 
cores of the aggregators imply non-emptiness of the core of 
the game induced by the macg we also show that there 
appears to be no such general relationship that relates the 
core memberships of the game among attributes games of 
attributes and game induced by macg 
 problem setup 
there are many problems in the literature for which the 
questions of core non-emptiness and core 
membership are known to be hard for example for the 
edgespanning game that deng and papadimitriou studied 
both of these questions are conp-complete as macg can 
model the edge-spanning game in the same amount of space 
these hardness results hold for macg as well 
as in the case for computing shapley value we attempt 
to find a way around the hardness barrier by assuming the 
existence of oracles and try to build algorithms with these 
oracles first we consider the aggregate value function 
assumption for a macg n m a a w we assume 
there are oracles that answers the questions of core 
nonemptiness and core membership for the aggregate value 
function w 
when the aggregate value function is a non-negative linear 
function of its attributes the core is always non-empty and 
core membership can be determined efficiently 
the concept of core for the game among attributes makes 
the most sense when the aggregators are simple games we 
will further assume that these simple games are monotonic 
assumption for a macg n m a a w we assume 
all aggregators are monotonic and simple we also assume 
there are oracles that answers the questions of core 
nonemptiness and core membership for the aggregators 
we consider this a mild assumption recall that monotonic 
simple games are fully described by their set of minimal 
winning coalitions cf section if the aggregators are 
represented as such core non-emptiness and core 
membership can be checked in polynomial time this is due to the 
following well-known result regarding simple games 
lemma a simple game n v has a non-empty core 
if and only if it has a set of veto players say v such that 
v s for all s ⊇ v further a payoff vector x is in the 
core if and only if xi for all i ∈ v 
 core non-emptiness 
there is a strong connection between the non-emptiness 
of the cores of the games among attributes games of the 
attributes and the game induced by a macg 
theorem given a game n v represented as a macg 
n m a a w if the core of the game among attributes 
 
m w is non-empty and the cores of the games of 
attributes are non-empty then the core of n v is non-empty 
proof let u be an arbitrary payoff vector in the core of 
the game among attributes m w for each attribute j 
let θj 
be an arbitrary payoff vector in the core of the game 
of attribute j by lemma each attribute j must have a 
set of veto players let this set be denoted by pj 
 for each 
agent i ∈ n let yi 
è 
j ujθj 
i we claim that this vector y 
is in the core of n v consider any coalition s ⊆ n 
v s w a a s ≤ 
j s⊇p j 
uj 
this is true because an aggregator cannot evaluate to 
without all members of the veto set for any attribute j by 
lemma 
è 
i∈p j θj 
i therefore 
j s⊇p j 
uj 
j s⊇p j 
uj 
i∈p j 
θj 
i 
 
i∈s j s⊇p j 
ujθj 
i 
≤ 
i∈s 
yi 
note that the proof is constructive and hence if we are 
given an element in the core of the game among attributes 
we can construct an element of the core of the coalitional 
game from theorem we can obtain the following 
corollaries that have been previously shown in the literature 
corollary the core of the edge-spanning game of 
is non-empty when the edge weights are non-negative 
proof let the players be the vertices and their 
attributes the edges incident on them for each attribute 
there is a veto set - namely both endpoints of the edges 
as previously observed an aggregate value function that is a 
non-negative linear function of its aggregates has non-empty 
core therefore the precondition of theorem is satisfied 
and the edge-spanning game with non-negative edge weights 
has a non-empty core 
corollary theorem of the core of a flow 
game with committee control where each edge is controlled 
by a simple game with a veto set of players is non-empty 
proof we treat each edge of the flow game as an 
attribute and so each attribute has a veto set of players the 
core of a flow game without committee has been shown 
to be non-empty in we can again invoke theorem to 
show the non-emptiness of core for flow games with 
committee control 
however the core of the game induced by a macg may 
be non-empty even when the core of the game among 
attributes is empty as illustrated by the following example 
example suppose the minimal winning coalition of all 
aggregators in a macg n m a a w is n then v s 
for all coalitions s ⊂ n as long as v n ≥ any 
nonnegative vector x that satisfies 
è 
i∈n xi v n is in the 
core of n v 
complementary to the example above when all the 
aggregators have empty cores the core of n v is also empty 
theorem given a game n v represented as a macg 
n m a a w if the cores of all aggregators are empty 
v n and for each i ∈ n v {i} ≥ then the core of 
n v is empty 
proof suppose the core of n v is non-empty let x 
be a member of the core and pick an agent i such that xi 
 however for each attribute since the core is empty by 
lemma there are at least two disjoint winning coalitions 
pick the winning coalition sj 
that does not include i for 
each attribute j let s∗ 
 
ë 
j sj 
 because s∗ 
is winning 
for all coalitions v s∗ 
 v n however 
v n 
j∈n 
xj xi 
j ∈n 
xj ≥ xi 
j∈s∗ 
xj 
j∈s∗ 
xj 
therefore v s∗ 
 
è 
j∈s∗ xj contradicting the fact that x 
is in the core of n v 
we do not have general results regarding the problem of 
core non-emptiness when some of the aggregators have 
non-empty cores while others have empty cores we suspect 
knowledge about the status of the cores of the aggregators 
alone is insufficient to decide this problem 
 core membership 
since it is possible for the game induced by the macg 
to have a non-empty core when the core of the aggregate 
value function is empty example we try to explore the 
problem of core membership assuming that the core of 
both the game among attributes m w and the underlying 
game n v is known to be non-empty and see if there 
is any relationship between their members one reasonable 
requirement is whether a payoff vector x in the core of n v 
can be decomposed and re-aggregated to a payoff vector y 
in the core of m w formally 
definition we say that a vector x ∈ rn 
≥ can be 
decomposed and re-aggregated into a vector y ∈ rm 
≥ if there 
exists z ∈ rm×n 
≥ such that 
yi 
n 
j 
zij for all i 
xj 
m 
i 
zij for all j 
we may refer z as shares 
when there is no restriction on the entries of z it is 
always possible to decompose a payoff vector x in the core of 
n v to a payoff vector y in the core of m w however it 
seems reasonable to restrict that if an agent j is irrelevant to 
the aggregator i i e i never changes the outcome of 
aggregator j then zij should be restricted to be unfortunately 
this restriction is already too strong 
example consider a macg n m a a w with two 
players and three attributes suppose agent is irrelevant 
to attribute and agent is irrelevant to attributes and 
 for any set of attributes t ⊆ m let w be defined as 
w t 
 if t or 
 if t 
 if t 
 
since the core of a game with a finite number of players forms 
a polytope we can verify that the set of vectors 
 and fully characterize the core c of m w 
on the other hand the vector is in the core of n v 
this vector cannot be decomposed and re-aggregated to a 
vector in c under the stated restriction 
because of the apparent lack of relationship among 
members of the core of n v and that of m w we believe an 
algorithm for testing core membership will require more 
input than just the veto sets of the aggregators and the 
oracle of core membership for the aggregate value function 
 concluding remarks 
multi-attribute coalitional games constitute a very 
natural way of modeling problems of interest its space 
requirement compares favorably with other representations 
discussed in the literature and hence it serves well as a 
prototype to study computational complexity of coalitional 
game theory for a variety of problems positive results 
obtained under this representation can easily be translated to 
results about other representations some of these corollary 
results have been discussed in sections and 
an important direction to explore in the future is the 
question of efficiency in updating a game and how to 
evaluate the solution concepts without starting from scratch as 
pointed out at the end of section macg is very 
naturally suited for updates representation results regarding 
efficiency of updates and algorithmic results regarding how 
to compute the different solution concepts from updates will 
both be very interesting 
our work on approximating the shapley value when the 
aggregate value function is a non-linear function of the 
attributes suggests more work to be done there as well given 
the natural probabilistic interpretation of the shapley value 
we believe that a random sampling approach may have 
significantly better theoretical guarantees 
 references 
 j m bilbao j r fern´andez and j j l´opez 
complexity in cooperative game theory 
http www esi us es  mbilbao 
 v conitzer and t sandholm complexity of 
determining nonemptiness of the core in proc th 
int joint conf on artificial intelligence pages 
 - 
 v conitzer and t sandholm computing shapley 
values manipulating value division schemes and 
checking core membership in multi-issue domains in 
proc th nat conf on artificial intelligence pages 
 - 
 i j curiel j j derks and s h tijs on balanced 
games and games with committee control or 
spectrum - 
 x deng and c h papadimitriou on the complexity 
of cooperative solution concepts math oper res 
 - may 
 m r garey and d s johnson computers and 
intractability a guide to the theory of 
np-completeness w h freeman new york 
 s ieong and y shoham marginal contribution nets 
a compact representation scheme for coalitional 
games in proc th acm conf on electronic 
commerce pages - 
 e kalai and e zemel totally balanced games and 
games of flow math oper res - 
 a mas-colell m d whinston and j r green 
microeconomic theory oxford university press new 
york 
 m j osborne and a rubinstein a course in game 
theory the mit press cambridge massachusetts 
 
 l s shapley a value for n-person games in h w 
kuhn and a w tucker editors contributions to the 
theory of games ii number in annals of 
mathematical studies pages - princeton 
university press 
 o shehory and s kraus task allocation via coalition 
formation among autonomous agents in proc th 
int joint conf on artificial intelligence pages - 
 
 o shehory and s kraus a kernel-oriented model for 
autonomous-agent coalition-formation in general 
environments implentation and results in proc th 
nat conf on artificial intelligence pages - 
 
 j von neumann and o morgenstern theory of 
games and economic behvaior princeton university 
press 
 m yokoo v conitzer t sandholm n ohta and 
a iwasaki coalitional games in open anonymous 
environments in proc th nat conf on artificial 
intelligence pages - 
appendix 
we complete the missing proofs from the main text here 
to prove proposition we need the following lemma 
lemma marginal contribution nets when all coalitions 
are restricted to have values or have the same 
representation power as an and or circuit with negation at the 
literal level i e ac 
circuit of depth two 
proof if a rule assigns a negative value in the marginal 
contribution nets we can write the rule by a corresponding 
set of at most n rules where n is the number of agents such 
that each of which has positive values through application of 
de morgan s law with all values of the rules non-negative 
we can treat the weighted summation step of marginal 
contribution nets can be viewed as an or and each rule as 
a conjunction over literals possibly negated this exactly 
match up with an and or circuit of depth two 
proof proposition the parity game can be 
represented with a macg using a single attribute aggregator 
of sum and an aggregate value function that evaluates that 
sum modulus two 
as a boolean function parity is known to require an 
exponential number of prime implicants by lemma a prime 
implicant is the exact analogue of a pattern in a rule of 
marginal contribution nets therefore to represent the parity 
function a marginal contribution nets must be an 
exponential number of rules 
finally as shown in a marginal contribution net is at 
worst a factor of o n less compact than multi-issue 
representation therefore multi-issue representation will also 
 
take exponential space to represent the parity game this 
is assuming that each issue in the game is represented in 
characteristic form 
proof theorem an instance of three-dimensional 
matching is as follows given set p ⊆ w × x × y 
where w x y are disjoint sets having the same number q 
of elements does there exist a matching p ⊆ p such that 
 p q and no two elements of p agree in any 
coordinate for notation let p {p p pk} we construct 
a macg n m a a w as follows 
 m let attributes to q correspond to elements in w 
 q to q correspond to elements in x q to q 
corresponds to element in y and let there be a special 
attribute q 
 n let player i corresponds to pi and let there be a 
special player 
 a let aji if the element corresponding to 
attribute j is in pi thus for the first k columns there 
are exactly three non-zero entries we also set 
a q 
 a for each aggregator j aj 
 a s if and only if 
sum of row j of a s equals 
 w product over all aj 
 
in the game n v that corresponds to this construction 
v s if and only if all attributes are covered exactly 
once therefore for ∈ t ⊆ n v t ∪ { } − v t if 
and only if t covers attributes to q exactly once since 
all such t if exists must be of size q the number of 
threedimensional matchings is given by 
φ v 
 k 
q k − q 
 
the dynamics of viral marketing ∗ 
jure leskovec 
† 
carnegie mellon university 
pittsburgh pa 
jure cs cmu edu 
lada a adamic 
‡ 
university of michigan 
ann arbor mi 
ladamic umich edu 
bernardo a huberman 
hp labs 
palo alto ca 
bernardo huberman hp com 
abstract 
we present an analysis of a person-to-person 
recommendation network consisting of million people who made 
million recommendations on half a million products we 
observe the propagation of recommendations and the 
cascade sizes which we explain by a simple stochastic model 
we then establish how the recommendation network grows 
over time and how effective it is from the viewpoint of the 
sender and receiver of the recommendations while on 
average recommendations are not very effective at inducing 
purchases and do not spread very far we present a model 
that successfully identifies product and pricing categories for 
which viral marketing seems to be very effective 
categories and subject descriptors 
j social and behavioral sciences economics 
general terms 
economics 
 introduction 
with consumers showing increasing resistance to 
traditional forms of advertising such as tv or newspaper ads 
marketers have turned to alternate strategies including 
viral marketing viral marketing exploits existing social 
networks by encouraging customers to share product 
information with their friends previously a few in depth 
studies have shown that social networks affect the adoption of 
individual innovations and products for a review see 
or but until recently it has been difficult to measure 
how influential person-to-person recommendations actually 
are over a wide range of products we were able to directly 
measure and model the effectiveness of recommendations by 
studying one online retailer s incentivised viral marketing 
program the website gave discounts to customers 
recommending any of its products to others and then tracked the 
resulting purchases and additional recommendations 
although word of mouth can be a powerful factor 
influencing purchasing decisions it can be tricky for advertisers 
to tap into some services used by individuals to 
communicate are natural candidates for viral marketing because the 
product can be observed or advertised as part of the 
communication email services such as hotmail and yahoo had 
very fast adoption curves because every email sent through 
them contained an advertisement for the service and because 
they were free hotmail spent a mere on traditional 
marketing and still grew from zero to million users in 
months google s gmail captured a significant part of 
market share in spite of the fact that the only way to sign 
up for the service was through a referral 
most products cannot be advertised in such a direct way 
at the same time the choice of products available to 
consumers has increased manyfold thanks to online retailers 
who can supply a much wider variety of products than 
traditional brick-and-mortar stores not only is the variety 
of products larger but one observes a  fat tail 
phenomenon where a large fraction of purchases are of relatively 
obscure items on amazon com somewhere between to 
 percent of unit sales fall outside of its top ranked 
products rhapsody a streaming-music service streams 
more tracks outside than inside its top tunes 
effectively advertising these niche products using traditional 
advertising approaches is impractical therefore using more 
targeted marketing approaches is advantageous both to the 
merchant and the consumer who would benefit from 
learning about new products 
the problem is partly addressed by the advent of 
online product and merchant reviews both at retail sites such 
as ebay and amazon and specialized product comparison 
sites such as epinions and cnet quantitative marketing 
techniques have been proposed and the rating of 
products and merchants has been shown to effect the likelihood 
of an item being bought of further help to the 
consumer are collaborative filtering recommendations of the 
form people who bought x also bought y feature 
these refinements help consumers discover new products 
 
and receive more accurate evaluations but they cannot 
completely substitute personalized recommendations that one 
receives from a friend or relative it is human nature to be 
more interested in what a friend buys than what an 
anonymous person buys to be more likely to trust their opinion 
and to be more influenced by their actions our friends are 
also acquainted with our needs and tastes and can make 
appropriate recommendations a lucid marketing survey 
found that of individuals consulted friends and relatives 
before purchasing home electronics - more than the half who 
used search engines to find product information 
several studies have attempted to model just this kind 
of network influence richardson and domingos used 
epinions trusted reviewer network to construct an 
algorithm to maximize viral marketing efficiency assuming that 
individuals probability of purchasing a product depends on 
the opinions on the trusted peers in their network kempe 
kleinberg and tardos evaluate the efficiency of several 
algorithms for maximizing the size of influence set given 
various models of adoption while these models address the 
question of maximizing the spread of influence in a network 
they are based on assumed rather than measured influence 
effects 
in contrast in our study we are able to directly observe 
the effectiveness of person to person word of mouth 
advertising for hundreds of thousands of products for the first time 
we find that most recommendation chains do not grow very 
large often terminating with the initial purchase of a 
product however occasionally a product will propagate through 
a very active recommendation network we propose a simple 
stochastic model that seems to explain the propagation of 
recommendations moreover the characteristics of 
recommendation networks influence the purchase patterns of their 
members for example individuals likelihood of 
purchasing a product initially increases as they receive additional 
recommendations for it but a saturation point is quickly 
reached interestingly as more recommendations are sent 
between the same two individuals the likelihood that they 
will be heeded decreases we also propose models to identify 
products for which viral marketing is effective we find that 
the category and price of product plays a role with 
recommendations of expensive products of interest to small well 
connected communities resulting in a purchase more often 
we also observe patterns in the timing of recommendations 
and purchases corresponding to times of day when people 
are likely to be shopping online or reading email we report 
on these and other findings in the following sections 
 the recommendation network 
 dataset description 
our analysis focuses on the recommendation referral 
program run by a large retailer the program rules were as 
follows each time a person purchases a book music or a 
movie he or she is given the option of sending emails 
recommending the item to friends the first person to purchase 
the same item through a referral link in the email gets a 
discount when this happens the sender of the 
recommendation receives a credit on their purchase 
the recommendation dataset consists of 
recommendations made among distinct users the 
data was collected from june to may in 
total products were recommended of them 
belonging to main product groups books dvds music 
and videos in addition to recommendation data we also 
crawled the retailer s website to obtain product categories 
reviews and ratings for all products of the products in our 
data set were discontinued the retailer no longer 
provided any information about them 
although the data gives us a detailed and accurate view 
of recommendation dynamics it does have its limitations 
the only indication of the success of a recommendation 
is the observation of the recipient purchasing the product 
through the same vendor we have no way of knowing if 
the person had decided instead to purchase elsewhere 
borrow or otherwise obtain the product the delivery of the 
recommendation is also somewhat different from one person 
simply telling another about a product they enjoy possibly 
in the context of a broader discussion of similar products 
the recommendation is received as a form email including 
information about the discount program someone reading 
the email might consider it spam or at least deem it less 
important than a recommendation given in the context of 
a conversation the recipient may also doubt whether the 
friend is recommending the product because they think the 
recipient might enjoy it or are simply trying to get a 
discount for themselves finally because the recommendation 
takes place before the recommender receives the product 
it might not be based on a direct observation of the 
product nevertheless we believe that these recommendation 
networks are reflective of the nature of word of mouth 
advertising and give us key insights into the influence of social 
networks on purchasing decisions 
 recommendation network statistics 
for each recommendation the dataset included the 
product and product price sender id receiver id the sent date 
and a buy-bit indicating whether the recommendation 
resulted in a purchase and discount the sender and receiver 
id s were shadowed we represent this data set as a 
directed multi graph the nodes represent customers and 
a directed edge contains all the information about the 
recommendation the edge i j p t indicates that i 
recommended product p to customer j at time t 
the typical process generating edges in the 
recommendation network is as follows a node i first buys a product p at 
time t and then it recommends it to nodes j jn the 
j nodes can they buy the product and further recommend 
it the only way for a node to recommend a product is to 
first buy it note that even if all nodes j buy a product 
only the edge to the node jk that first made the purchase 
 within a week after the recommendation will be marked 
by a buy-bit because the buy-bit is set only for the first 
person who acts on a recommendation we identify additional 
purchases by the presence of outgoing recommendations for 
a person since all recommendations must be preceded by a 
purchase we call this type of evidence of purchase a 
buyedge note that buy-edges provide only a lower bound on the 
total number of purchases without discounts it is possible 
for a customer to not be the first to act on a 
recommendation and also to not recommend the product to others 
unfortunately this was not recorded in the data set we 
consider however the buy-bits and buy-edges as proxies for 
the total number of purchases through recommendations 
for each product group we took recommendations on all 
products from the group and created a network table 
 
 
x 
 
 
 
 
 
 
 
 
x 
 
number of nodes 
sizeofgiantcomponent 
by month 
quadratic fit 
 
 
 
 
x 
 
m month 
n 
 nodes 
 
 
m 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
kp 
 recommendations by a person for a product 
n x k 
p 
 
level 
γ 
level 
γ 
level 
γ 
level 
γ 
level 
γ 
 a network growth b recommending by level 
figure a the size of the largest connected 
component of customers over time the inset shows the 
linear growth in the number of customers n over 
time b the number of recommendations sent by 
a user with each curve representing a different depth 
of the user in the recommendation chain a power 
law exponent γ is fitted to all but the tail 
 first columns shows the sizes of various product group 
recommendation networks with p being the total number 
of products in the product group n the total number of 
nodes spanned by the group recommendation network and 
e the number of edges recommendations the column eu 
shows the number of unique edges - disregarding multiple 
recommendations between the same source and recipient 
in terms of the number of different items there are by far 
the most music cds followed by books and videos there 
is a surprisingly small number of dvd titles on the other 
hand dvds account for more half of all recommendations in 
the dataset the dvd network is also the most dense 
having about recommendations per node while books and 
music have about recommendations per node and videos 
have only a bit more than recommendation per node 
music recommendations reached about the same number 
of people as dvds but used more than times fewer 
recommendations to achieve the same coverage of the nodes book 
recommendations reached by far the most people - 
million notice that all networks have a very small number 
of unique edges for books videos and music the number 
of unique edges is smaller than the number of nodes - this 
suggests that the networks are highly disconnected 
figure a shows the fraction of nodes in largest weakly 
connected component over time notice the component is 
very small even if we compose a network using all the 
recommendations in the dataset the largest connected 
component contains less than of the nodes and the 
second largest component has only nodes still some 
smaller communities numbering in the tens of thousands 
of purchasers of dvds in categories such as westerns 
classics and japanese animated films anime had connected 
components spanning about of their members 
the insert in figure a shows the growth of the 
customer base over time surprisingly it was linear adding on 
average new users each month which is an 
indication that the service itself was not spreading epidemically 
further evidence of non-viral spread is provided by the 
relatively high percentage of users who made their first 
recommendation without having previously received one 
back to table given the total number of 
recommendations e and purchases bb be influenced by 
recommendations we can estimate how many recommendations need 
to be independently sent over the network to induce a new 
purchase using this metric books have the most influential 
recommendations followed by dvds and music for books 
one out of recommendations resulted in a purchase for 
dvds it increases to recommendations per purchase and 
further increases to for music and for video 
even with these simple counts we can make the first few 
observations it seems that some people got quite 
heavily involved in the recommendation program and that they 
tended to recommend a large number of products to the 
same set of friends since the number of unique edges is so 
small this shows that people tend to buy more dvds and 
also like to recommend them to their friends while they 
seem to be more conservative with books one possible 
reason is that a book is bigger time investment than a dvd 
one usually needs several days to read a book while a dvd 
can be viewed in a single evening 
one external factor which may be affecting the 
recommendation patterns for dvds is the existence of referral websites 
 www dvdtalk com on these websites people who want to 
buy a dvd and get a discount would ask for 
recommendations this way there would be recommendations made 
between people who don t really know each other but rather 
have an economic incentive to cooperate we were not able 
to find similar referral sharing sites for books or cds 
 forward recommendations 
not all people who make a purchase also decide to give 
recommendations so we estimate what fraction of people 
that purchase also decide to recommend forward to obtain 
this information we can only use the nodes with purchases 
that resulted in a discount 
the last columns of table show that only about a 
third of the people that purchase also recommend the 
product forward the ratio of forward recommendations is much 
higher for dvds than for other kinds of products videos 
also have a higher ratio of forward recommendations while 
books have the lowest this shows that people are most keen 
on recommending movies while more conservative when 
recommending books and music 
figure b shows the cumulative out-degree distribution 
that is the number of people who sent out at least kp 
recommendations for a product it shows that the deeper an 
individual is in the cascade if they choose to make 
recommendations they tend to recommend to a greater number 
of people on average the distribution has a higher 
variance this effect is probably due to only very heavily 
recommended products producing large enough cascades to 
reach a certain depth we also observe that the probability 
of an individual making a recommendation at all which can 
only occur if they make a purchase declines after an initial 
increase as one gets deeper into the cascade 
 identifying cascades 
as customers continue forwarding recommendations they 
contribute to the formation of cascades in order to 
identify cascades i e the causal propagation of 
recommendations we track successful recommendations as they influence 
purchases and further recommendations we define a 
recommendation to be successful if it reached a node before its first 
purchase we consider only the first purchase of an item 
because there are many cases when a person made multiple 
 
group p n e eu bb be purchases forward percent 
book 
dvd 
music 
video 
total 
table product group recommendation statistics p number of products n number of nodes e number of 
edges recommendations eu number of unique edges bb number of buy bits be number of buy edges last 
 columns of the table fraction of people that purchase and also recommend forward purchases number 
of nodes that purchased forward nodes that purchased and then also recommended the product 
 
 
 a medical book b japanese graphic novel 
figure examples of two product recommendation networks a first aid study guide first aid for the 
usmle step b japanese graphic novel manga oh my goddess mara strikes back 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
number of recommendations 
count 
 e x− 
r 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
number of purchases 
count 
 e x− 
r 
 
 a recommendations b purchases 
figure distribution of the number of 
recommendations and number of purchases made by a node 
purchases of the same product and in between those 
purchases she may have received new recommendations in this 
case one cannot conclude that recommendations following 
the first purchase influenced the later purchases 
each cascade is a network consisting of customers nodes 
who purchased the same product as a result of each other s 
recommendations edges we delete late recommendations 
- all incoming recommendations that happened after the 
first purchase of the product this way we make the 
network time increasing or causal - for each node all incoming 
edges recommendations occurred before all outgoing edges 
now each connected component represents a time obeying 
propagation of recommendations 
figure shows two typical product recommendation 
networks a a medical study guide and b a japanese graphic 
novel throughout the dataset we observe very similar 
patters most product recommendation networks consist of a 
large number of small disconnected components where we 
do not observe cascades then there is usually a small 
number of relatively small components with recommendations 
successfully propagating 
this observation is reflected in the heavy tailed 
distribution of cascade sizes see figure having a power-law 
exponent close to for dvds in particular 
we also notice bursts of recommendations figure b 
some nodes recommend to many friends forming a star like 
pattern figure shows the distribution of the 
recommendations and purchases made by a single node in the 
recommendation network notice the power-law distributions and 
long flat tails the most active person made 
recommendations and purchased different items finally 
we also sometimes observe  collisions where nodes receive 
recommendations from two or more sources a detailed 
enumeration and analysis of observed topological cascade 
patterns for this dataset is made in 
 the recommendation propagation model 
a simple model can help explain how the wide variance we 
observe in the number of recommendations made by 
individuals can lead to power-laws in cascade sizes figure the 
model assumes that each recipient of a recommendation will 
forward it to others if its value exceeds an arbitrary 
threshold that the individual sets for herself since exceeding this 
value is a probabilistic event let s call pt the probability 
that at time step t the recommendation exceeds the 
thresh 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 e x− 
r 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 e x− 
r 
 
 
 
 
 
 
 
 
 
 
 
 
 
 e x− 
r 
 
 
 
 
 
 
 
 
 
 
 
 
 
 e x− 
r 
 
 a book b dvd c music d video 
figure size distribution of cascades size of cascade vs count bold line presents a power-fit 
old in that case the number of recommendations nt at 
time t is given in terms of the number of 
recommendations at an earlier time by 
nt ptnt 
where the probability pt is defined over the unit interval 
notice that because of the probabilistic nature of the 
threshold being exceeded one can only compute the final 
distribution of recommendation chain lengths which we now 
proceed to do 
subtracting from both sides of this equation the term nt 
and diving by it we obtain 
n t − nt 
nt 
 pt − 
summing both sides from the initial time to some very 
large time t and assuming that for long times the numerator 
is smaller than the denominator a reasonable assumption 
we get 
dn 
n 
 pt 
the left hand integral is just ln n and the right hand 
side is a sum of random variables which in the limit of a very 
large uncorrelated number of recommendations is normally 
distributed central limit theorem 
this means that the logarithm of the number of messages 
is normally distributed or equivalently that the number of 
messages passed is log-normally distributed in other words 
the probability density for n is given by 
p n 
 
n 
√ 
 πσ 
exp 
− ln n − μ 
 σ 
 
which for large variances describes a behavior whereby 
the typical number of recommendations is small the mode 
of the distribution but there are unlikely events of large 
chains of recommendations which are also observable 
furthermore for large variances the lognormal 
distribution can behave like a power law for a range of values in 
order to see this take the logarithms on both sides of the 
equation equivalent to a log-log plot and one obtains 
ln p n − ln n − ln 
√ 
 πσ − 
 ln n − μ 
 σ 
 
so for large σ the last term of the right hand side goes 
to zero and since the the second term is a constant one 
obtains a power law behavior with exponent value of 
minus one there are other models which produce power-law 
distributions of cascade sizes but we present ours for its 
simplicity since it does not depend on network topology 
or critical thresholds in the probability of a recommendation 
being accepted 
 success of recommendations 
so far we only looked into the aggregate statistics of the 
recommendation network next we ask questions about 
the effectiveness of recommendations in the 
recommendation network itself first we analyze the probability of 
purchasing as one gets more and more recommendations next 
we measure recommendation effectiveness as two people 
exchange more and more recommendations lastly we 
observe the recommendation network from the perspective of 
the sender of the recommendation does a node that makes 
more recommendations also influence more purchases 
 probability of buying versus number of 
incoming recommendations 
first we examine how the probability of purchasing changes 
as one gets more and more recommendations one would 
expect that a person is more likely to buy a product if she gets 
more recommendations on the other had one would also 
think that there is a saturation point - if a person hasn t 
bought a product after a number of recommendations they 
are not likely to change their minds after receiving even more 
of them so how many recommendations are too many 
figure shows the probability of purchasing a product 
as a function of the number of incoming recommendations 
on the product as we move to higher numbers of incoming 
recommendations the number of observations drops rapidly 
for example there were million cases with incoming 
recommendation on a book and only cases where a 
person got incoming recommendations on a particular book 
the maximum was incoming recommendations for these 
reasons we cut-off the plot when the number of observations 
becomes too small and the error bars too large 
figure a shows that overall book recommendations 
are rarely followed even more surprisingly as more and 
more recommendations are received their success decreases 
we observe a peak in probability of buying at incoming 
recommendations and then a slow drop 
for dvds figure b we observe a saturation around 
incoming recommendations this means that after a person 
gets recommendations on a particular dvd they 
become immune to them - their probability of buying does 
not increase anymore the number of observations is 
million at incoming recommendation and at 
incoming recommendations the maximal number of received 
recommendations is and that person did not buy 
 
 
 
 
 
 
 
 
 
incoming recommendations 
probabilityofbuying 
 
 
 
 
 
 
incoming recommendations 
probabilityofbuying 
 a books b dvd 
figure probability of buying a book dvd given a number of incoming recommendations 
 
 
 
 
 
 
x 
− 
exchanged recommendations 
probabilityofbuying 
 
 
 
 
 
 
 
exchanged recommendations 
probabilityofbuying 
 a books b dvd 
figure the effectiveness of recommendations 
with the total number of exchanged 
recommendations 
 success of subsequent recommendations 
next we analyze how the effectiveness of 
recommendations changes as two persons exchange more and more 
recommendations a large number of exchanged 
recommendations can be a sign of trust and influence but a sender of too 
many recommendations can be perceived as a spammer a 
person who recommends only a few products will have her 
friends attention but one who floods her friends with all 
sorts of recommendations will start to loose her influence 
we measure the effectiveness of recommendations as a 
function of the total number of previously exchanged 
recommendations between the two nodes we construct the 
experiment in the following way for every recommendation 
r on some product p between nodes u and v we first 
determine how many recommendations were exchanged between 
u and v before recommendation r then we check whether v 
the recipient of recommendation purchased p after 
recommendation r arrived for the experiment we consider only 
node pairs u v where there were at least a total of 
recommendations sent from u to v we perform the experiment 
using only recommendations from the same product group 
figure shows the probability of buying as a function of 
the total number of exchanged recommendations between 
two persons up to that point for books we observe that 
the effectiveness of recommendation remains about constant 
up to exchanged recommendations as the number of 
exchanged recommendations increases the probability of 
buying starts to decrease to about half of the original value and 
then levels off for dvds we observe an immediate and 
consistent drop this experiment shows that recommendations 
start to lose effect after more than two or three are passed 
between two people we performed the experiment also for 
video and music but the number of observations was too 
low and the measurements were noisy 
 success of outgoing recommendations 
in previous sections we examined the data from the 
viewpoint of the receiver of the recommendation now we look 
from the viewpoint of the sender the two interesting 
questions are how does the probability of getting a credit 
change with the number of outgoing recommendations and 
given a number of outgoing recommendations how many 
purchases will they influence 
one would expect that recommendations would be the 
most effective when recommended to the right subset of 
friends if one is very selective and recommends to too few 
friends then the chances of success are slim one the other 
hand recommending to everyone and spamming them with 
recommendations may have limited returns as well 
the top row of figure shows how the average number 
of purchases changes with the number of outgoing 
recommendations for books music and videos the number of 
purchases soon saturates it grows fast up to around 
outgoing recommendations and then the trend either slows or 
starts to drop dvds exhibit different behavior with the 
expected number of purchases increasing throughout but 
if we plot the probability of getting a credit as a 
function of the number of outgoing recommendations as in the 
bottom row of figure we see that the success of dvd 
recommendations saturates as well while books videos and 
music have qualitatively similar trends the difference in 
the curves for dvd recommendations points to the presence 
of collisions in the dense dvd network which has 
recommendations per node and around per product - an 
order of magnitude more than other product groups this 
means that many different individuals are recommending to 
the same person and after that person makes a purchase 
even though all of them made a  successful recommendation 
 
 
 
 
 
 
 
 
outgoing recommendations 
numberofpurchases 
 
 
 
 
 
 
 
 
 
outgoing recommendations 
numberofpurchases 
 
 
 
 
 
 
outgoing recommendations 
numberofpurchases 
 
 
 
 
 
 
 
outgoing recommendations 
numberofpurchases 
 
 
 
 
 
 
 
outgoing recommendations 
probabilityofcredit 
 
 
 
 
 
 
 
 
outgoing recommendations 
probabilityofcredit 
 
 
 
 
 
 
 
outgoing recommendations 
probabilityofcredit 
 
 
 
 
 
 
outgoing recommendations 
probabilityofcredit 
 a books b dvd c music d video 
figure top row number of resulting purchases given a number of outgoing recommendations bottom 
row probability of getting a credit given a number of outgoing recommendations 
 
 
 
 
 
 
 
 
 
lag day 
proportionofpurchases 
 
 
 
 
 
 
 
lag day 
proportionofpurchases 
 a books b dvd 
figure the time between the recommendation 
and the actual purchase we use all purchases 
by our definition only one of them receives a credit 
 timing of recommendations and 
purchases 
the recommendation referral program encourages people 
to purchase as soon as possible after they get a 
recommendation since this maximizes the probability of getting a 
discount we study the time lag between the recommendation 
and the purchase of different product groups effectively how 
long it takes a person to both receive a recommendation 
consider it and act on it 
we present the histograms of the thinking time i e the 
difference between the time of purchase and the time the last 
recommendation was received for the product prior to the 
purchase figure we use a bin size of day around 
 of book and dvd purchases occurred within a day after 
the last recommendation was received for dvds 
purchases occur more than a week after last recommendation 
while this drops to for books in contrast if we consider 
the lag between the purchase and the first recommendation 
only of dvd purchases are made within a day while the 
proportion stays the same for books this reflects a greater 
likelihood for a person to receive multiple recommendations 
for a dvd than for a book at the same time dvd 
recommenders tend to send out many more recommendations 
only one of which can result in a discount individuals then 
often miss their chance of a discount which is reflected in 
the high ratio of recommended dvd purchases that 
did not a get discount see table columns bb and be in 
contrast for books only of purchases through 
recommendations did not receive a discount 
we also measure the variation in intensity by time of day 
for three different activities in the recommendation system 
recommendations figure a all purchases figure b 
and finally just the purchases which resulted in a discount 
 figure c each is given as a total count by hour of day 
the recommendations and purchases follow the same 
pattern the only small difference is that purchases reach a 
sharper peak in the afternoon after pm pacific time pm 
eastern time the purchases that resulted in a discount 
look like a negative image of the first two figures this means 
that most of discounted purchases happened in the morning 
when the traffic number of purchases recommendations on 
the retailer s website was low this makes a lot of sense since 
most of the recommendations happened during the day and 
if the person wanted to get the discount by being the first 
one to purchase she had the highest chances when the traffic 
on the website was the lowest 
 recommendation effectiveness 
by book category 
social networks are a product of the contexts that bring 
people together some contexts result in social ties that 
are more effective at conducting an action for example 
in small world experiments where participants attempt to 
reach a target individual through their chain of 
acquaintances profession trumped geography which in turn was 
more useful in locating a target than attributes such as 
religion or hobbies in the context of product 
recommendations we can ask whether a recommendation for a work 
of fiction which may be made by any friend or neighbor is 
 
 
 
 
 
 
 
 
x 
 
hour of the day 
recommendtions 
 
 
 
 
 
 
x 
 
hour of the day 
allpurchases 
 
 
 
 
 
 
 
 
 
hour of the day 
discountedpurchases 
 a recommendations b purchases c purchases with discount 
figure time of day for purchases and recommendations a shows the distribution of recommendations 
over the day b shows all purchases and c shows only purchases that resulted in getting discount 
more or less influential than a recommendation for a 
technical book which may be made by a colleague at work or 
school 
table shows recommendation trends for all top level 
book categories by subject an analysis of other product 
types can be found in the extended version of the paper for 
clarity we group the results by different category types 
fiction personal leisure professional technical and 
nonfiction other fiction encompasses categories such as sci-fi 
and romance as well as children s and young adult books 
personal leisure encompasses everything from gardening 
photography and cooking to health and religion 
first we compare the relative number of 
recommendations to reviews posted on the site column cav rp of 
table surprisingly we find that the number of people 
making personal recommendations was only a few times greater 
than the number of people posting a public review on the 
website we observe that fiction books have relatively few 
recommendations compared to the number of reviews while 
professional and technical books have more 
recommendations than reviews this could reflect several factors one is 
that people feel more confident reviewing fiction than 
technical books another is that they hesitate to recommend a 
work of fiction before reading it themselves since the 
recommendation must be made at the point of purchase yet 
another explanation is that the median price of a work of 
fiction is lower than that of a technical book this means 
that the discount received for successfully recommending a 
mystery novel or thriller is lower and hence people have less 
incentive to send recommendations 
next we measure the per category efficacy of 
recommendations by observing the ratio of the number of purchases 
occurring within a week following a recommendation to the 
number of recommenders for each book subject category 
 column b of table on average only of the 
recommenders of a book received a discount because their 
recommendation was accepted and another made a 
recommendation that resulted in a purchase but not a discount 
we observe marked differences in the response to 
recommendation for different categories of books fiction in general 
is not very effectively recommended with only around 
of recommenders succeeding the efficacy was a bit higher 
 around for non-fiction books dealing with personal and 
leisure pursuits but is significantly higher in the professional 
and technical category medical books have nearly double 
the average rate of recommendation acceptance this could 
be in part attributed to the higher median price of medical 
books and technical books in general as we will see in 
section a higher product price increases the chance that a 
recommendation will be accepted 
recommendations are also more likely to be accepted for 
certain religious categories for christian living and 
theology and for bibles in contrast books not tied 
to organized religions such as ones on the subject of new 
age and occult spirituality have lower 
recommendation effectiveness these results raise the interesting 
possibility that individuals have greater influence over one 
another in an organized context for example through a 
professional contact or a religious one there are exceptions of 
course for example japanese anime dvds have a strong 
following in the us and this is reflected in their frequency 
and success in recommendations another example is that of 
gardening in general recommendations for books relating 
to gardening have only a modest chance of being accepted 
which agrees with the individual prerogative that 
accompanies this hobby at the same time orchid cultivation can be 
a highly organized and social activity with frequent  shows 
and online communities devoted entirely to orchids perhaps 
because of this the rate of acceptance of orchid book 
recommendations is twice as high as those for books on vegetable 
or tomato growing 
 modeling the recommendation 
success 
we have examined the properties of recommendation 
network in relation to viral marketing but one question still 
remains what determines the product s viral marketing 
success we present a model which characterizes product 
categories for which recommendations are more likely to be 
accepted we use a regression of the following product 
attributes to correlate them with recommendation success 
 r number of recommendations 
 ns number of senders of recommendations 
 nr number of recipients of recommendations 
 p price of the product 
 v number of reviews of the product 
 t average product rating 
 
category np n cc rp vav cav pm b ∗ 
rp 
books general 
fiction 
children s books 
literature fiction 
mystery and thrillers 
science fiction fantasy 
romance 
teens 
comics graphic novels 
horror 
personal leisure 
religion and spirituality 
health mind and body 
history 
home and garden 
entertainment 
arts and photography 
travel 
sports 
parenting and families 
cooking food and wine 
outdoors nature 
professional technical 
professional technical 
business and investing 
science 
computers and internet 
medicine 
engineering 
law 
nonfiction-other 
nonfiction 
reference 
biographies and memoirs 
table statistics by book category np number of products in category n number of customers cc percentage 
of customers in the largest connected component rp av reviews in - rp av reviews 
 st months vav average star rating cav average number of people recommending product cav rp 
ratio of recommenders to reviewers pm median price b ratio of the number of purchases resulting from a 
recommendation to the number of recommenders the symbol denotes statistical significance at the 
level at the level 
from the original set of half a million products we 
compute a success rate s for the products that had at 
least one purchase made through a recommendation and for 
which a price was given in section we defined 
recommendation success rate s as the ratio of the total number 
purchases made through recommendations and the number 
of senders of the recommendations we decided to use this 
kind of normalization rather than normalizing by the total 
number of recommendations sent in order not to penalize 
communities where a few individuals send out many 
recommendations figure b since the variables follow a heavy 
tailed distribution we use the following model 
s exp 
i 
βi log xi i 
where xi are the product attributes as described on 
previous page and i is random error 
we fit the model using least squares and obtain the 
coefficients βi shown on table with the exception of the 
average rating they are all significant the only two 
attributes with a positive coefficient are the number of 
recommendations and price this shows that more expensive 
and more recommended products have a higher success rate 
the number of senders and receivers have large negative 
coefficients showing that successfully recommended products 
are more likely to be not so widely popular they have 
relatively many recommendations with a small number of 
senders and receivers which suggests a very dense 
recommendation network where lots of recommendations were 
exchanged between a small community of people 
these insights could be to marketers - personal 
recommendations are most effective in small densely connected 
communities enjoying expensive products 
 
variable coefficient βi 
const - 
r 
ns - 
nr - 
p 
v - 
t - 
r 
 
table regression using the log of the 
recommendation success rate ln s as the dependent variable 
for each coefficient we provide the standard error 
and the statistical significance level 
 discussion and conclusion 
although the retailer may have hoped to boost its 
revenues through viral marketing the additional purchases that 
resulted from recommendations are just a drop in the bucket 
of sales that occur through the website nevertheless we 
were able to obtain a number of interesting insights into how 
viral marketing works that challenge common assumptions 
made in epidemic and rumor propagation modeling 
firstly it is frequently assumed in epidemic models that 
individuals have equal probability of being infected every 
time they interact contrary to this we observe that the 
probability of infection decreases with repeated interaction 
marketers should take heed that providing excessive 
incentives for customers to recommend products could backfire 
by weakening the credibility of the very same links they are 
trying to take advantage of 
traditional epidemic and innovation diffusion models also 
often assume that individuals either have a constant 
probability of  converting every time they interact with an 
infected individual or that they convert once the fraction of 
their contacts who are infected exceeds a threshold in both 
cases an increasing number of infected contacts results in 
an increased likelihood of infection instead we find that 
the probability of purchasing a product increases with the 
number of recommendations received but quickly saturates 
to a constant and relatively low probability this means 
individuals are often impervious to the recommendations of 
their friends and resist buying items that they do not want 
in network-based epidemic models extremely highly 
connected individuals play a very important role for example 
in needle sharing and sexual contact networks these nodes 
become the super-spreaders by infecting a large number 
of people but these models assume that a high degree node 
has as much of a probability of infecting each of its neighbors 
as a low degree node does in contrast we find that there 
are limits to how influential high degree nodes are in the 
recommendation network as a person sends out more and 
more recommendations past a certain number for a product 
the success per recommendation declines this would seem 
to indicate that individuals have influence over a few of their 
friends but not everybody they know 
we also presented a simple stochastic model that allows 
for the presence of relatively large cascades for a few 
products but reflects well the general tendency of 
recommendation chains to terminate after just a short number of steps 
we saw that the characteristics of product reviews and 
effectiveness of recommendations vary by category and price 
with more successful recommendations being made on 
technical or religious books which presumably are placed in the 
social context of a school workplace or place of worship 
finally we presented a model which shows that smaller 
and more tightly knit groups tend to be more conducive to 
viral marketing so despite the relative ineffectiveness of the 
viral marketing program in general we found a number of 
new insights which we hope will have general applicability 
to marketing strategies and to future models of viral 
information spread 
 references 
 anonymous profiting from obscurity what the long 
tail means for the economics of e-commerce 
economist 
 e brynjolfsson y hu and m d smith consumer 
surplus in the digital economy estimating the value 
of increased product variety at online booksellers 
management science 
 k burke as consumer attitudes shift so must 
marketing strategies 
 j chevalier and d mayzlin the effect of word of 
mouth on sales online book reviews 
 p erd¨os and a r´enyi on the evolution of random 
graphs publ math inst hung acad sci 
 d gruhl r guha d liben-nowell and a tomkins 
information diffusion through blogspace in www 
 
 s jurvetson what exactly is viral marketing red 
herring - 
 d kempe j kleinberg and e tardos maximizing 
the spread of infuence in a social network in acm 
sigkdd international conference on knowledge 
discovery and data mining kdd 
 p killworth and h bernard reverse small world 
experiment social networks - 
 j leskovec a singh and j kleinberg patterns of 
influence in a recommendation network in 
pacific-asia conference on knowledge discovery and 
data mining pakdd 
 g linden b smith and j york amazon com 
recommendations item-to-item collaborative filtering 
ieee internet computing - 
 a l montgomery applying quantitative marketing 
techniques to the internet interfaces - 
 p resnick and r zeckhauser trust among strangers 
in internet transactions empirical analysis of ebays 
reputation system in the economics of the internet 
and e-commerce elsevier science 
 m richardson and p domingos mining 
knowledge-sharing sites for viral marketing in acm 
sigkdd international conference on knowledge 
discovery and data mining kdd 
 e m rogers diffusion of innovations free press 
new york fourth edition 
 d strang and s a soule diffusion in organizations 
and social movements from hybrid corn to poison 
pills annual review of sociology - 
 j travers and s milgram an experimental study of 
the small world problem sociometry 
 d watts a simple model of global cascades on 
random networks pnas - apr 
 
robust solutions for combinatorial auctions 
∗ 
alan holland 
cork constraint computation centre 
department of computer science 
university college cork ireland 
a holland  c ucc ie 
barry o sullivan 
cork constraint computation centre 
department of computer science 
university college cork ireland 
b osullivan  c ucc ie 
abstract 
bids submitted in auctions are usually treated as enforceable 
commitments in most bidding and auction theory literature in reality 
bidders often withdraw winning bids before the transaction when 
it is in their best interests to do so given a bid withdrawal in a 
combinatorial auction finding an alternative repair solution of 
adequate revenue without causing undue disturbance to the 
remaining winning bids in the original solution may be difficult or even 
impossible we have called this the bid-taker s exposure 
problem when faced with such unreliable bidders it is preferable for 
the bid-taker to preempt such uncertainty by having a solution that 
is robust to bid withdrawal and provides a guarantee that possible 
withdrawals may be repaired easily with a bounded loss in revenue 
in this paper we propose an approach to addressing the 
bidtaker s exposure problem firstly we use the weighted super 
solutions framework from the field of constraint programming 
to solve the problem of finding a robust solution a weighted super 
solution guarantees that any subset of bids likely to be withdrawn 
can be repaired to form a new solution of at least a given revenue by 
making limited changes secondly we introduce an auction model 
that uses a form of leveled commitment contract which 
we have called mutual bid bonds to improve solution reparability 
by facilitating backtracking on winning bids by the bid-taker we 
then examine the trade-off between robustness and revenue in 
different economically motivated auction scenarios for different 
constraints on the revenue of repair solutions we also demonstrate 
experimentally that fewer winning bids partake in robust solutions 
thereby reducing any associated overhead in dealing with extra 
bidders robust solutions can also provide a means of selectively 
discriminating against distrusted bidders in a measured manner 
categories and subject descriptors 
j social and behavioral sciences economics k computers 
∗this work has received support from science foundation 
ireland under grant number pi c the authors wish to thank 
brahim hnich and the anonymous reviewers for their helpful 
comments 
and society electronic commerce i artificial intelligence 
problem solving control methods and search 
general terms 
algorithms economics reliability 
 introduction 
a combinatorial auction ca provides an efficient means of 
allocating multiple distinguishable items amongst bidders whose 
perceived valuations for combinations of items differ such 
auctions are gaining in popularity and there is a proliferation in their 
usage across various industries such as telecoms b b procurement 
and transportation 
revenue is the most obvious optimization criterion for such 
auctions but another desirable attribute is solution robustness in terms 
of combinatorial auctions a robust solution is one that can 
withstand bid withdrawal a break by making changes easily to form 
a repair solution of adequate revenue a brittle solution to a ca 
is one in which an unacceptable loss in revenue is unavoidable if 
a winning bid is withdrawn in such situations the bid-taker may 
be left with a set of items deemed to be of low value by all other 
bidders these bidders may associate a higher value for these items 
if they were combined with items already awarded to others hence 
the bid-taker is left in an undesirable local optimum in which a form 
of backtracking is required to reallocate the items in a manner that 
results in sufficient revenue we have called this the bid-taker s 
exposure problem that bears similarities to the exposure 
problem faced by bidders seeking multiple items in separate single-unit 
auctions but holding little or no value for a subset of those items 
however reallocating items may be regarded as disruptive to a 
solution in many real-life scenarios consider a scenario where 
procurement for a business is conducted using a ca it would be 
highly undesirable to retract contracts from a group of suppliers 
because of the failure of a third party a robust solution that is 
tolerant of such breaks is preferable robustness may be regarded as a 
preventative measure protecting against future uncertainty by 
sacrificing revenue in place of solution stability and reparability we 
assume a probabilistic approach whereby the bid-taker has 
knowledge of the reliability of bidders from which the likelihood of an 
incomplete transaction may be inferred 
repair solutions are required for bids that are seen as brittle 
 i e likely to break repairs may also be required for sets of 
bids deemed brittle we propose the use of the weighted super 
 
solutions wss framework for constraint programming that 
is ideal for establishing such robust solutions as we shall see 
this framework can enforce constraints on solutions so that 
possible breakages are reparable 
this paper is organized as follows section presents the 
winner determination problem wdp for combinatorial auctions 
outlines some possible reasons for bid withdrawal and shows how 
simply maximizing expected revenue can lead to intolerable revenue 
losses for risk-averse bid-takers this motivates the use of robust 
solutions and section introduces a constraint programming cp 
framework weighted super solutions that finds such 
solutions we then propose an auction model in section that enhances 
reparability by introducing mandatory mutual bid bonds that may 
be seen as a form of leveled commitment contract 
section presents an extensive empirical evaluation of the approach 
presented in this paper in the context of a number of well-known 
combinatorial auction distributions with very encouraging results 
section discusses possible extensions and questions raised by our 
research that deserve future work finally in section a number 
of concluding remarks are made 
 combinatorial auctions 
before presenting the technical details of our solution to the 
bid-taker s exposure problem we shall present a brief survey 
of combinatorial auctions and existing techniques for handling bid 
withdrawal 
combinatorial auctions involve a single bid-taker allocating 
multiple distinguishable items amongst a group of bidders the 
bidtaker has a set of m items for sale m { m} and 
bidders submit a set of bids b {b b bn} a bid is a tuple 
bj sj pj where sj ⊆ m is a subset of the items for sale and 
pj ≥ is a price the wdp for a ca is to label all bids as either 
winning or losing so as to maximize the revenue from winning bids 
without allocating any item to more than one bid the following is 
the integer programming formulation for the wdp 
max 
n 
j 
pjxj s t 
j i∈sj 
xj ≤ ∀i ∈ { m} xj ∈ { } 
this problem is np-complete and inapproximable 
and is otherwise known as the set packing problem the above 
problem formulation assumes the notion of free disposal this 
means that the optimal solution need not necessarily sell all of the 
items if the auction rules stipulate that all items must be sold the 
problem becomes a set partition problem the wdp has been 
extensively studied in recent years the fastest search algorithms 
that find optimal solutions e g cabob can in practice 
solve very large problems involving thousands of bids very quickly 
 the problem of bid withdrawal 
we assume an auction protocol with a three stage process 
involving the submission of bids winner determination and finally 
a transaction phase we are interested in bid withdrawals that 
occur between the announcement of winning bids and the end of the 
transaction phase all bids are valid until the transaction is 
complete so we anticipate an expedient transaction process 
 
 
in some instances the transaction period may be so lengthy that 
consideration of non-winning bids as still being valid may not be 
fair breaks that occur during a lengthy transaction phase are more 
difficult to remedy and may require a subsequent auction for 
example if the item is a service contract for a given period of time and 
the break occurs after partial fulfilment of this contract the other 
an example of a winning bid withdrawal occurred in an fcc 
spectrum auction withdrawals or breaks may occur for 
various reasons bid withdrawal may be instigated by the bid-taker 
when quality of service agreements are broken or payment 
deadlines are not met we refer to bid withdrawal by the bid-taker as 
item withdrawal in this paper to distinguish between the actions 
of a bidder and the bid-taker harstad and rothkopf outlined 
several possibilities for breaks in single item auctions that include 
 an erroneous initial valuation bid 
 unexpected events outside the winning bidder s control 
 a desire to have the second-best bid honored 
 information obtained or events that occurred after the auction 
but before the transaction that reduces the value of an item 
 the revelation of competing bidders valuations infers reduced 
profitability a problem known as the winner s curse 
kastner et al examined how to handle perturbations given 
a solution whilst minimizing necessary changes to that solution 
these perturbations may include bid withdrawals change of 
valuation items of a bid or the submission of a new bid they looked at 
the problem of finding incremental solutions to restructure a 
supply chain whose formation is determined using combinatorial 
auctions following a perturbation in the optimal solution they 
proceed to impose involuntary item withdrawals from winning 
bidders they formulated an incremental integer linear program ilp 
that sought to maximize the valuation of the repair solution whilst 
preserving the previous solution as much as possible 
 being proactive against bid withdrawal 
when a bid is withdrawn there may be constraints on how the 
solution can be repaired if the bid-taker was freely able to revoke 
the awarding of items to other bidders then the solution could be 
repaired easily by reassigning all the items to the optimal solution 
without the withdrawn bid alternatively the bidder who reneged 
upon a bid may have all his other bids disqualified and the items 
could be reassigned based on the optimum solution without that 
bidder present however the bid-taker is often unable to freely 
reassign the items already awarded to other bidders when items 
cannot be withdrawn from winning bidders following the failure of 
another bidder to honor his bid repair solutions are restricted to the 
set of bids whose items only include those in the bid s that were 
reneged upon we are free to award items to any of the previously 
unsuccessful bids when finding a repair solution 
when faced with uncertainty over the reliability of bidders a 
possible approach is to maximize expected revenue this approach 
does not make allowances for risk-averse bid-takers who may view 
a small possibility of very low revenue as unacceptable 
consider the example in table and the optimal expected 
revenue in the situation where a single bid may be withdrawn there 
are three submitted bids for items a and b the third being a 
combination bid for the pair of items at a value of the optimal 
solution has a value of with the first and second bids as 
winners when we consider the probabilities of failure in the fourth 
column the problem of which solution to choose becomes more 
difficult 
computing the expected revenue for the solution with the first 
and second bids winning the items denoted gives 
 × × × × × × × 
bidders valuations for the item may have decreased in a non-linear 
fashion 
 
table example combinatorial auction 
items 
bids a b ab withdrawal prob 
x 
x 
x 
if a single bid is withdrawn there is probability of of a revenue 
of given the fact that we cannot withdraw an item from the 
other winning bidder the expected revenue for is 
 × × 
we can therefore surmise that the second solution is preferable to 
the first based on expected revenue 
determining the maximum expected revenue in the presence of 
such uncertainty becomes computationally infeasible however as 
the number of brittle bids grows a wdp needs to be solved for all 
possible combinations of bids that may fail the possible loss in 
revenue for breaks is also not tightly bounded using this approach 
therefore a large loss may be possible for a small number of breaks 
consider the previous example where the bid amount for x 
becomes the expected revenue of becomes 
greater than that of there are some bid-takers 
who may prefer the latter solution because the revenue is never less 
than but the former solution returns revenue of only with 
probability a risk-averse bid-taker may not tolerate such a 
possibility preferring to sacrifice revenue for reduced risk 
if we modify our repair search so that a solution of at least a 
given revenue is guaranteed the search for a repair solution 
becomes a satisfiability test rather than an optimization problem the 
approaches described above are in contrast to that which we 
propose in the next section our approach can be seen as preventative 
in that we find an initial allocation of items to bidders which is 
robust to bid withdrawal possible losses in revenue are bounded by 
a fixed percentage of the true optimal allocation perturbations to 
the original solution are also limited so as to minimize disruption 
we regard this as the ideal approach for real-world combinatorial 
auctions 
definition robust solution for a ca a robust 
solution for a combinatorial auction is one where any subset of 
successful bids whose probability of withdrawal is greater than or 
equal to α can be repaired by reassigning items at a cost of at most 
β to other previously losing bids to form a repair solution 
constraints on acceptable revenue e g being a minimum 
percentage of the optimum are defined in the problem model and are 
thus satisfied by all solutions the maximum cost of repair β may 
be a fixed value that may be thought of as a fund for 
compensating winning bidders whose items are withdrawn from them when 
creating a repair solution alternatively β may be a function of 
the bids that were withdrawn section will give an example of 
such a mechanism in the following section we describe an ideal 
constraint-based framework for the establishment of such robust 
solutions 
 finding robust solutions 
in constraint programming cp a constraint satisfaction 
problem csp is modeled as a set of n variables x {x xn} 
a set of domains d {d x d xn } where d xi is 
the set of finite possible values for variable xi and a set c 
{c cm} of constraints each restricting the assignments of 
some subset of the variables in x constraint satisfaction involves 
finding values for each of the problem variables such that all 
constraints are satisfied its main advantages are its declarative nature 
and flexibility in tackling problems with arbitrary side constraints 
constraint optimization seeks to find a solution to a csp that 
optimizes some objective function a common technique for solving 
constraint optimization problems is to use branch-and-bound 
techniques that avoid exploring sub-trees that are known not to contain 
a better solution than the best found so far an initial bound can be 
determined by finding a solution that satisfies all constraints in c 
or by using some heuristic methods 
a classical super solution ss is a solution to a csp in which 
if a small number of variables lose their values repair solutions 
are guaranteed with only a few changes thus providing solution 
robustness it is a generalization of both fault tolerance in 
cp and supermodels in propositional satisfiability sat 
an a b -super solution is one in which if at most a variables lose 
their values a repair solution can be found by changing at most b 
other variables 
super solutions for combinatorial auctions minimize the number 
of bids whose status needs to be changed when forming a repair 
solution only a particular set of variables in the solution may 
be subject to change and these are said to be members of the 
breakset for each combination of brittle assignments in the break-set a 
repair-set is required that comprises the set of variables whose 
values must change to provide another solution the cardinality of the 
repair set is used to measure the cost of repair in reality 
changing some variable assignments in a repair solution incurs a lower 
cost than others thereby motivating the use of a different metric for 
determining the legality of repair sets 
the weighted super solution wss framework considers 
the cost of repair required rather than simply the number of 
assignments modified to form an alternative solution for cas this 
may be a measure of the compensation penalties paid to winning 
bidders to break existing agreements robust solutions are 
particularly desirable for applications where unreliability is a problem 
and potential breakages may incur severe penalties weighted 
super solutions offer a means of expressing which variables are 
easily re-assigned and those that incur a heavy cost hebrard et 
al describe how some variables may fail such as machines in a 
job-shop problem and others may not a wss generalizes this 
approach so that there is a probability of failure associated with each 
assignment and sets of variables whose assignments have 
probabilities of failure greater than or equal to a threshold value α require 
repair solutions 
a wss measures the cost of repairing or reassigning other 
variables using inertia as a metric inertia is a measure of a variable s 
aversion to change and depends on its current assignment future 
assignment and the breakage variable s 
it may be desirable to reassign items to different bidders in order 
to find a repair solution of satisfactory revenue compensation may 
have to be paid to bidders who lose items during the formation of a 
repair solution the inertia of a bid reflects the cost of changing its 
state for winning bids this may reflect the necessary compensation 
penalty for the bid-taker to break the agreement if such breaches 
are permitted whereas for previously losing bids this is a free 
operation the total amount of compensation payable to bidders may 
depend upon other factors such as the cause of the break there is 
a limit to how much these overall repair costs should be and this is 
given by the value β this value may not be known in advance and 
 
algorithm wss int level double α double β boolean 
begin 
if level number of variables then return true 
choose unassigned variable x 
foreach value v in the domain of x do 
assign x v 
if problem is consistent then 
foreach combination of brittle assignments a do 
if ¬reparable a β then return false 
if wss level then return true 
unassign x 
return false 
end 
may depend upon the break therefore β may be viewed as the 
fund used to compensate winning bidders for the unilateral 
withdrawal of their bids by the bid-taker in summary an α β -wss 
allows any set of variables whose probability of breaking is greater 
than or equal to α be repaired with changes to the original robust 
solution with a cost of at most β 
the depth-first search for a wss see pseudo-code description 
in algorithm maintains arc-consistency at each node of the 
tree as search progresses the reparability of each previous 
assignment is verified at each node by extending a partial repair 
solution to the same depth as the current partial solution this may 
be thought of as maintaining concurrent search trees for repairs 
a repair solution is provided for every possible set of break 
variables a the wss algorithm attempts to extend the current partial 
assignment by choosing a variable and assigning it a value 
backtracking may then occur for one of two reasons we cannot extend 
the assignment to satisfy the given constraints or the current 
partial assignment cannot be associated with a repair solution whose 
cost of repair is less than β should a break occur the procedure 
reparable searches for partial repair solutions using 
backtracking and attempts to extend the last repair found just as in 
 b super solutions the differences being that a repair is provided 
for a set of breakage variables rather than a single variable and the 
cost of repair is considered a summation operator is used to 
determine the overall cost of repair if a fixed bound upon the size of 
any potential break-set can be formed the wss algorithm is 
npcomplete for a more detailed description of the wss search 
algorithm the reader is referred to since a complete description of 
the algorithm is beyond the scope of this paper 
example we shall step through the example given in 
table when searching for a wss each bid is represented by a 
single variable with domain values of and the former representing 
bid-failure and the latter bid-success the probability of failure of 
the variables are when they are assigned to and 
otherwise the problem is initially solved using an ilp solver such as 
lp solve or cplex and the optimal revenue is found to be 
 a fixed percentage of this revenue can be used as a threshold 
value for a robust solution and its repairs the bid-taker wishes to 
have a robust solution so that if a single winning bid is withdrawn 
a repair solution can be formed without withdrawing items from 
any other winning bidder this example may be seen as searching 
for a -weighted super solution β is because no funds are 
available to compensate the withdrawal of items from winning 
bidders the bid-taker is willing to compromise on revenue but only 
by say of the optimal value 
bids and cannot both succeed since they both require item 
a so a constraint is added precluding the assignment in which both 
variables take the value similarly bids and cannot both win 
so another constraint is added between these two variables 
therefore in this example the set of csp variables is v {x x x } 
whose domains are all { } the constraints are x x ≤ 
x x ≤ and xi∈v aixi ≥ where ai reflects the 
relevant bid-amounts for the respective bid variables in order to find a 
robust solution of optimal revenue we seek to maximize the sum of 
these amounts max xi∈v aixi 
when all variables are set to see figure a branch this is 
not a solution because the minimum revenue of has not been 
met so we try assigning bid to branch this is a valid 
solution but this variable is brittle because there is a chance that 
this bid may be withdrawn see table therefore we need to 
determine if a repair can be formed should it break the search for a 
repair begins at the first node see figure b notice that value 
has been removed from bid because this search tree is simulating 
the withdrawal of this bid when bid is set to branch the 
maximum revenue solution in the remaining subtree has revenue of 
only therefore search is discontinued at that node of the tree 
bid and bid are both assigned to branches and and 
the total cost of both these changes is still because no 
compensation needs to be paid for bids that change from losing to winning 
with bid now losing branch this gives a repair solution of 
 hence is reparable and therefore a wss we continue 
our search in figure a however because we are seeking a robust 
solution of optimal revenue 
when bid is assigned to branch we seek a partial repair 
for this variable breaking branch is not considered since it offers 
insufficient revenue the repair search sets bid to in a separate 
search tree not shown and control is returned to the search for 
a wss bid is set to branch but this solution would not 
produce sufficient revenue so bid is then set to branch we 
then attempt to extend the repair for bid not shown this fails 
because the repair for bid cannot assign bid to because the 
cost of repairing such an assignment would be ∞ given that the 
auction rules do not permit the withdrawal of items from winning 
bids a repair for bid breaking is therefore not possible because 
items have already been awarded to bid a repair solution with 
bid assigned to does not produce sufficient revenue when bid 
is assigned to the inability to withdraw items from winning bids 
implies that is an irreparable solution when the minimum 
tolerable revenue is greater than the italicized comments and 
dashed line in figure a illustrate the search path for a wss if 
both of these bids were deemed reparable 
section introduces an alternative auction model that will allow 
the bid-taker to receive compensation for breakages and in turn use 
this payment to compensate other bidders for withdrawal of items 
from winning bids this will enable the reallocation of items and 
permit the establishment of as a second wss for this 
example 
 mutual bid bonds a 
backtracking mechanism 
some auction solutions are inherently brittle and it may be 
impossible to find a robust solution if we can alter the rules of an 
auction so that the bid-taker can retract items from winning 
bidders then the reparability of solutions to such auctions may be 
improved in this section we propose an auction model that permits 
bid and item withdrawal by the bidders and bid-taker respectively 
we propose a model that incorporates mutual bid bonds to enable 
solution reparability for the bid-taker a form of insurance against 
 
 
 
 
 
 
 
 
 
insufficient 
revenue 
find repair solution 
for bid breakage 
find partial repair for 
bid breakage 
insufficient 
revenue 
 a extend partial repair 
for bid breakage 
 b find partial repair 
for bid breakage 
bid 
bid 
bid 
find repair solutions for 
bid breakages 
 
 
 
 
 
 
 
 
insufficient 
revenue 
 a search for wss 
 
 
 
 
 
 
 
 
insufficient 
revenue 
insufficient 
revenue 
bid 
bid 
bid 
inertia 
inertia 
inertia 
 
 
 
 b search for a repair for bid breakage 
figure search tree for a wss without item withdrawal 
the winner s curse for the bidder whilst also compensating bidders 
in the case of item withdrawal from winning bids we propose that 
such winner s curse bid-taker s exposure insurance comprise 
a fixed percentage κ of the bid amount for all bids such mutual 
bid bonds are mandatory for each bid in our model 
 the conditions 
attached to the bid bonds are that the bid-taker be allowed to annul 
winning bids item withdrawal when repairing breaks elsewhere 
in the solution in the interests of fairness compensation is paid 
to bidders from whom items are withdrawn and is equivalent to the 
penalty that would have been imposed on the bidder should he have 
withdrawn the bid 
combinatorial auctions impose a heavy computational burden 
on the bidder so it is important that the hedging of risk should be 
a simple and transparent operation for the bidder so as not to 
further increase this burden unnecessarily we also contend that it 
is imperative that the bidder knows the potential penalty for 
withdrawal in advance of bid submission this information is essential 
for bidders when determining how aggressive they should be in 
their bidding strategy bid bonds are commonplace in procurement 
for construction projects usually they are mandatory for all bids 
are a fixed percentage κ of the bid amount and are unidirectional 
in that item withdrawal by the bid-taker is not permitted mutual 
bid bonds may be seen as a form of leveled commitment contract 
in which both parties may break the contract for the same fixed 
penalty such contracts permit unilateral decommitment for 
prespecified penalties sandholm et al showed that this can increase 
the expected payoffs of all parties and enables deals that would be 
impossible under full commitment 
in practice a bid bond typically ranges between and of the 
 
making the insurance optional may be beneficial in some 
instances if a bidder does not agree to the insurance it may be 
inferred that he may have accurately determined the valuation for the 
items and therefore less likely to fall victim to the winner s curse 
the probability of such a bid being withdrawn may be less so a 
repair solution may be deemed unnecessary for this bid on the other 
hand it decreases the reparability of solutions 
bid amount if the decommitment penalties are the same 
for both parties in all bids κ does not influence the reparability of 
a given set of bids it merely influences the levels of penalties and 
compensation transacted by agents low values of κ incur low bid 
withdrawal penalties and simulate a dictatorial bid-taker who does 
not adequately compensate bidders for item withdrawal andersson 
and sandholm found that myopic agents reach a higher social 
welfare quicker if they act selfishly rather than cooperatively when 
penalties in leveled commitment contracts are low increased levels 
of bid withdrawal are likely when the penalties are low also 
high values of κ tend towards full-commitment and reduce the 
advantages of such winner s curse bid-taker s exposure 
insurance the penalties paid are used to fund a reassignment of 
items to form a repair solution of sufficient revenue by 
compensating previously successful bidders for withdrawal of the items from 
them 
example consider the example given in table once more 
where the bids also comprise a mutual bid bond of of the bid 
amount if a bid is withdrawn the bidder forfeits this amount and 
the bid-taker can then compensate winning bidders whose items are 
withdrawn when trying to form a repair solution later the search 
for repair solutions for breaks to bid and bid appear in figures 
 a and b respectively 
 
when bid breaks there is a compensation penalty paid to the 
bid-taker equal to that can be used to fund a reassignment of the 
items we therefore set β to and this becomes the maximum 
expenditure allowed to withdraw items from winning bidders β 
may also be viewed as the size of the fund available to facilitate 
backtracking by the bid-taker when we extend the partial repair 
for bid so that bid loses an item branch the overall cost of 
repair increases to due to this item withdrawal by the bid-taker 
 
the actual implementation of wss search checks previous 
solutions to see if they can repair breaks before searching for a new 
repair solution is a solution that has already been found 
so the search for a repair in this example is not strictly necessary 
but is described for pedagogical reasons 
 
 
 
 
 
 
bid 
bid 
bid 
insufficient revenue 
inertia 
 
inertia 
 
inertia 
 
 
 
 
 
 
 a search for a repair for bid breakage 
 
 
 
 
 
bid 
bid 
bid 
insufficient revenue 
inertia 
 
inertia 
 
inertia 
 
 
 
 
 
 
 b search for a repair for bid breakage 
figure repair search tree for breaks and κ 
and is just within the limit given by β in figure a the search path 
follows the dashed line and sets bid to be branch the repair 
solutions for bids and can be extended further by assigning bid 
to branches and therefore may be considered 
a robust solution recall that previously this was not the case 
using mutual bid bonds thus increases reparability and allows a 
robust solution of revenue as opposed to as was previously 
the case 
 experiments 
we have used the combinatorial auction test suite cats 
to generate sample auction data we generated instances of 
problems in which there are items for sale and - bids 
that may be dominated in some instances 
 such dominated bids 
can participate in repair solutions although they do not feature in 
optimal solutions cats uses economically motivated bidding 
patterns to generate auction data in various scenarios to motivate the 
research presented in this paper we use sensitivity analysis to 
examine the brittleness of optimal solutions and hence determine the 
types of auctions most likely to benefit from a robust solution we 
then establish robust solutions for cas using the wss framework 
 sensitivity analysis for the wdp 
we have performed sensitivity analysis of the following four 
distributions airport take-off landing slots matching electronic 
components arbitrary property spectrum-rights regions 
and transportation paths these distributions were chosen 
because they describe a broad array of bidding patterns in different 
application domains 
the method used is as follows we first of all determined the 
optimal solution using lp solve a mixed integer linear program 
solver we then simulated a single bid withdrawal and re-solved 
the problem with the other winning bids remaining fixed i e there 
were no involuntary dropouts the optimal repair solution was 
then determined this process is repeated for all winning bids in 
the overall optimal solution thus assuming that all bids are brittle 
figure shows the average revenue of such repair solutions as a 
percentage of the optimum also shown is the average worst-case 
scenario over auctions we also implemented an auction rule 
that disallows bids from the reneging bidder participate in a repair 
 
figure a illustrates how the paths distribution is inherently 
the most robust distribution since when any winning bid is 
withdrawn the solution can be repaired to achieve over of the 
 
the cats flags included int prices with the bid alpha parameter 
set to 
 
we assumed that all bids in a given xor bid with the same 
dummy item were from the same bidder 
optimal revenue on average for auctions with more than bids 
there are some cases however when such withdrawals result in 
solutions whose revenue is significantly lower than optimum even 
in auctions with as many as bids there are occasions when a 
single bid withdrawal can result in a drop in revenue of over 
although the average worst-case drop in revenue is only 
figure b shows how the matching distribution is more brittle on 
average than paths and also has an inferior worst-case revenue on 
average this trend continues as the regions-npv figure c 
and arbitrary-npv figure d distributions are more 
brittle still these distributions are clearly sensitive to bid withdrawal 
when no other winning bids in the solution may be involuntarily 
withdrawn by the bid-taker 
 robust solutions using wss 
in this section we focus upon both the arbitrary-npv and 
regions-npv distributions because the sensitivity analysis 
indicated that these types of auctions produce optimal solutions that 
tend to be most brittle and therefore stand to benefit most from 
solution robustness we ignore the auctions with bids because 
the sensitivity analysis has indicated that these auctions are 
inherently robust with a very low average drop in revenue following a bid 
withdrawal they would also be very computationally expensive 
given the extra complexity of finding robust solutions 
a pure cp approach needs to be augmented with global 
constraints that incorporate operations research techniques to increase 
pruning sufficiently so that thousands of bids may be examined 
global constraints exploit special-purpose filtering algorithms to 
improve performance there are a number of ways to speed 
up the search for a weighted super solution in a ca although this 
is not the main focus of our current work polynomial matching 
algorithms may be used in auctions whose bid length is short such 
as those for airport landing take-off slots for example the integer 
programming formulation of the wdp stipulates that a bid either 
loses or wins if we relax this constraint so that bids can partially 
win this corresponds to the linear relaxation of the problem and is 
solvable in polynomial time at each node of the search tree we 
can quickly solve the linear relaxation of the remaining problem in 
the subtree below the current node to establish an upper bound on 
remaining revenue if this upper bound plus revenue in the parent 
tree is less than the current lower bound on revenue search at that 
node can cease the continuous lp relaxation thus provides a 
vital speed-up in the search for weighted super solutions which we 
have exploited in our implementation the lp formulation is as 
follows 
max 
xi∈v 
aixi 
 
 
 
 
 
 
 
 
revenue ofoptimum 
bids 
average repair solution revenue 
worst-case repair solution revenue 
 a paths 
 
 
 
 
 
 
 
revenue ofoptimum 
bids 
average repair solution revenue 
worst-case repair solution revenue 
 b matching 
 
 
 
 
 
 
 
revenue ofoptimum 
bids 
average repair solution revenue 
worst-case repair solution revenue 
 c regions-npv 
 
 
 
 
 
 
 
revenue ofoptimum 
bids 
average repair solution revenue 
worst-case repair solution revenue 
 d arbitrary-npv 
figure sensitivity of bid distributions to single bid withdrawal 
s t 
j i∈sj 
xj ≤ ∀i ∈ { m} xj ≥ xj ∈ r 
additional techniques that are outlined in can aid the 
scalability of a cp approach but our main aim in these experiments is 
to examine the robustness of various auction distributions and 
consider the tradeoff between robustness and revenue the wss solver 
we have developed is an extension of the super solution solver 
presented in this solver is in turn based upon the efc 
constraint solver 
combinatorial auctions are easily modeled as a constraint 
optimization problems we have chosen the branch-on-bids 
formulation because in tests it worked faster than a branch-on-items 
formulation for the arbitrary-npv and regions-npv 
distributions all variables are binary and our search mechanism uses a 
reverse lexicographic value ordering heuristic this complements our 
dynamic variable ordering heuristic that selects the most promising 
unassigned variable as the next one in the search tree we use the 
product of the solution of the lp relaxation and the degree of a 
variable to determine the likelihood of its participation in a robust 
solution high values in the lp solution are a strong indication of 
variables most likely to form a high revenue solution whilst the a 
variable s degree reflects the number of other bids that overlap in 
terms of desired items bids for large numbers of items tend to be 
more robust which is why we weight our robust solution search in 
this manner we found this heuristic to be slightly more effective 
than the lp solution alone as the number of bids in the auction 
increases however there is an increase in the inherent robustness 
of solutions so the degree of a variable loses significance as the 
auction size increases 
 results 
our experiments simulate three different constraints on repair 
solutions the first is that no winning bids are withdrawn by the 
bid-taker and a repair solution must return a revenue of at least 
of the optimal overall solution secondly we relaxed the revenue 
constraint to of optimum thirdly we allowed backtracking 
by the bid-taker on winning bids using mutual bid bonds but 
maintaining the revenue constraint at of optimum 
prior to finding a robust solution we solved the wdp optimally 
using lp solve we then set the minimum tolerable revenue 
for a solution to be then of the revenue of this 
optimal solution we assumed that all bids were brittle thus a repair 
solution is required for every bid in the solution initially we 
assume that no backtracking was permitted on assignments of items 
to other winning bids given a bid withdrawal elsewhere in the 
solution table shows the percentage of optimal solutions that are 
robust for minimum revenue constraints for repair solutions of 
and of optimal revenue relaxing the revenue constraint on 
repair solutions to of the optimum revenue greatly increases the 
number of optimal solutions that are robust we also conducted 
experiments on the same auctions in which backtracking by the 
bid-taker is permitted using mutual bid bonds this significantly 
improves the reparability of optimal solutions whilst still 
maintaining repair solutions of of optimum an interesting feature 
of the arbitrary-npv distribution is that optimal solutions can 
become more brittle as the number of bids increases the reason 
for this is that optimal solutions for larger auctions have more 
winning bids some of the optimal solutions for the smallest auctions 
with bids have only one winning bidder if this bid is 
withdrawn it is usually easy to find a new repair solution within of 
the previous optimal revenue also repair solutions for bids that 
contain a small number of items may be made difficult by the fact 
that a reduced number of bids cover only a subset of those items a 
mitigating factor is that such bids form a smaller percentage of the 
revenue of the optimal solution on average 
we also implemented a rule stipulating that any losing bids from 
 
table optimal solutions that are inherently robust 
 bids 
min revenue 
arbitrary-npv 
repair ≥ 
repair ≥ 
mbb repair ≥ ≥ 
regions-npv 
repair ≥ 
repair ≥ 
mbb repair ≥ ≥ 
table occurrence of robust solutions 
 bids 
min revenue 
arbitrary-npv 
repair ≥ 
repair ≥ 
mbb repair ≥ 
regions-npv 
repair ≥ 
repair ≥ 
mbb repair ≥ 
a withdrawing bidder cannot participate in a repair solution this 
acts as a disincentive for strategic withdrawal and was also used 
previously in the sensitivity analysis in some auctions a robust 
solution may not exist table shows the percentage of auctions that 
support robust solutions for the arbitrary-npv and regions 
-npv distributions it is clear that finding robust solutions for the 
former distribution is particularly difficult for auctions with and 
 bids when revenue constraints are of optimum this 
difficulty was previously alluded to by the low percentage of optimal 
solutions that were robust for these auctions relaxing the revenue 
constraint helps increase the percentage of auctions in which robust 
solutions are achievable to and respectively this 
improves the reparability of all solutions thereby increasing the 
average revenue of the optimal robust solution it is somewhat 
counterintuitive to expect a reduction in reparability of auction solutions as 
the number of bids increases because there tends to be an increased 
number of solutions above a revenue threshold in larger auctions 
the mbb auction model performs very well however and ensures 
that robust solutions are achievable for such inherently brittle 
auctions without sacrificing over of optimal revenue to achieve 
repair solutions 
figure shows the average revenue of the optimal robust 
solution as a percentage of the overall optimum repair solutions found 
for a wss provide a lower bound on possible revenue following a 
bid withdrawal note that in some instances it is possible for a 
repair solution to have higher revenue than the original solution 
when backtracking on winning bids by the bid-taker is disallowed 
this can only happen when the repair solution includes two or more 
bids that were not in the original otherwise the repair bids would 
participate in the optimal robust solution in place of the bid that 
was withdrawn a wss guarantees minimum levels of revenue for 
repair solutions but this is not to say that repair solutions cannot be 
improved upon it is possible to use an incremental algorithm to 
 
 
 
 
 
 
revenue ofoptimum 
bids 
repair revenue min optimal 
repair revenue min optimal 
mbb repair revenue min optimal 
 a regions-npv 
 
 
 
 
 
 
revenue ofoptimum 
bids 
repair revenue min optimal 
repair revenue min optimal 
mbb repair revenue min optimal 
 b arbitrary-npv 
figure revenue of optimal robust solutions 
determine an optimal repair solution following a break whilst safe 
in the knowledge that in advance of any possible bid withdrawal 
we can establish a lower bound on the revenue of a repair kastner 
et al have provided such an incremental ilp formulation 
mutual bid bonds facilitate backtracking by the bid-taker on 
already assigned items this improves the reparability of all 
possible solutions thus increasing the revenue of the optimal robust 
solution on average figure shows the increase in revenue of 
robust solutions in such instances the revenues of repair 
solutions are bounded by at least of the optimum in our 
experiments thereby allowing a direct comparison with robust solutions 
already found using the same revenue constraint but not providing 
for backtracking it is immediately obvious that such a mechanism 
can significantly increase revenue whilst still maintaining solution 
robustness 
table shows the number of winning bids participating in 
optimal and optimal robust solutions given the three different 
constraints on repairing solutions listed at the beginning of this 
section as the number of bids increases more of the optimal overall 
solutions are robust this leads to a convergence in the number of 
winning bids the numbers in brackets are derived from the 
sensitivity analysis of optimal solutions that reveals the fact that almost 
all optimal solutions for auctions of bids are robust we can 
therefore infer that the average number of winning bids in 
revenuemaximizing robust solutions converges towards that of the optimal 
overall solutions 
a notable side-effect of robust solutions is that fewer bids 
participate in the solutions it can be clearly seen from table that when 
revenue constraints on repair solutions are tight there are fewer 
winning bids in the optimal robust solution on average this is 
particularly pronounced for smaller auctions in both distributions 
this can win benefits for the bid-taker such as reduced overheads in 
dealing with fewer suppliers although mbbs aid solution 
repara 
table number of winning bids 
 bids 
solution 
arbitrary-npv 
optimal 
repair ≥ ≈ 
repair ≥ 
mbb ≥ ≈ 
regions-npv 
optimal 
repair ≥ ≈ 
repair ≥ 
mbb ≥ ≈ 
bility the number of bids in the solutions increases on average 
this is to be expected because a greater fraction of these solutions 
are in fact optimal as we saw in table 
 discussion and future work 
bidding strategies can become complex in 
non-incentive-compatible mechanisms where winner determination is no longer 
necessarily optimal the perceived reparability of a bid may influence the 
bid amount with reparable bids reaching a lower equilibrium point 
and perceived irreparable bids being more aggressive 
penalty payments for bid withdrawal also create an incentive for 
more aggressive bidding by providing a form of insurance against 
the winner s curse if a winning bidder s revised valuation for 
a set of items drops by more than the penalty for withdrawal of 
the bid then it is in his best interests to forfeit the item s and pay 
the penalty should the auction rules state that the bid-taker will 
refuse to sell the items to any of the remaining bidders in the event 
of a withdrawal then insurance against potential losses will 
stimulate more aggressive bidding however in our case we are seeking 
to repair the solution with the given bids a side-effect of such a 
policy is to offset the increased aggressiveness by incentivizing 
reduced valuations in expectation that another bidder s successful bid 
is withdrawn harstad and rothkopf examined the conditions 
required to ensure an equilibrium position in which bidding was at 
least as aggressive as if no bid withdrawal was permitted given this 
countervailing incentive to under-estimate a valuation three 
major results arose from their study of bid withdrawal in a single item 
auction 
 equilibrium bidding is more aggressive with withdrawal for 
sufficiently small probabilities of an award to the second 
highest bidder in the event of a bid withdrawal 
 equilibrium bidding is more aggressive with withdrawal if 
the number of bidders is large enough 
 for many distributions of costs and estimates equilibrium 
bidding is more aggressive with withdrawal if the variability 
of the estimating distribution is sufficiently large 
it is important that mutual bid bonds do not result in depressed 
bidding in equilibrium an analysis of the resultant behavior of 
bidders must incorporate the possibility of a bidder winning an item 
and having it withdrawn in order for the bid-taker to formulate a 
repair solution after a break elsewhere harstad and rothkopf have 
analyzed bidder aggressiveness using a strictly game-theoretic 
model in which the only reason for bid withdrawal is the winner s 
curse they assumed all bidders were risk-neutral but surmised 
that it is entirely possible for the bid-taker to collect a risk premium 
from risk-averse bidders with the offer of such insurance 
combinatorial auctions with mutual bid bonds add an extra incentive to 
bid aggressively because of the possibility of being compensated 
for having a winning bid withdrawn by a bid-taker this is militated 
against by the increased probability of not having items withdrawn 
in a repair solution we leave an in-depth analysis of the sufficient 
conditions for more aggressive bidding for future work 
whilst the wss framework provides ample flexibility and 
expressiveness scalability becomes a problem for larger auctions 
although solutions to larger auctions tend to be naturally more 
robust some bid-takers in such auctions may require robustness a 
possible extension of our work in this paper may be to examine the 
feasibility of reformulating integer linear programs so that the 
solutions are robust hebrard et al examined reformulation of 
csps for finding super solutions alternatively it may be 
possible to use a top-down approach by looking at the k-best solutions 
sequentially in terms of revenue and performing sensitivity 
analysis upon each solution until a robust one is found in procurement 
settings the principle of free disposal is often discounted and all 
items must be sold this reduces the number of potential solutions 
and thereby reduces the reparability of each solution the impact 
of such a constraint on revenue of robust solutions is also left for 
future work 
there is another interesting direction this work may take namely 
robust mechanism design porter et al introduced the notion of 
fault tolerant mechanism design in which agents have private 
information regarding costs for task completion but also their 
probabilities of failure when the bid-taker has combinatorial 
valuations for task completions it may be desirable to assign the same 
task to multiple agents to ensure solution robustness it is desirable 
to minimize such potentially redundant task assignments but not to 
the detriment of completed task valuations this problem could be 
modeled using the wss framework in a similar manner to that of 
combinatorial auctions 
in the case where no robust solutions are found it is possible 
to optimize robustness instead of revenue by finding a solution 
of at least a given revenue that minimizes the probability of an 
irreparable break in this manner the least brittle solution of adequate 
revenue may be chosen 
 conclusion 
fairness is often cited as a reason for choosing the optimal 
solution in terms of revenue only robust solutions militate against 
bids deemed brittle therefore bidders must earn a reputation for 
being reliable to relax the reparability constraint attached to their 
bids this may be seen as being fair to long-standing business 
partners whose reliability is unquestioned internet-based auctions 
are often seen as unwelcome price-gouging exercises by suppliers 
in many sectors traditional business partnerships are 
being severed by increased competition amongst suppliers quality 
of service can suffer because of the increased focus on short-term 
profitability to the detriment of the bid-taker in the long-term 
robust solutions can provide a means of selectively discriminating 
against distrusted bidders in a measured manner as 
combinatorial auction deployment moves from large value auctions with a 
small pool of trusted bidders e g spectrum-rights sales towards 
lower value auctions with potentially unknown bidders e g 
supply chain management solution robustness becomes more 
relevant as well as being used to ensure that the bid-taker is not 
left vulnerable to bid withdrawal it may also be used to cement 
relationships with preferred possibly incumbent suppliers 
 
we have shown that it is possible to attain robust solutions for 
cas with only a small loss in revenue we have also illustrated 
how such solutions tend to have fewer winning bids than overall 
optimal solutions thereby reducing any overheads associated with 
dealing with more bidders we have also demonstrated that 
introducing mutual bid bonds a form of leveled commitment contract 
can significantly increase the revenue of optimal robust solutions 
by improving reparability we contend that robust solutions 
using such a mechanism can allow a bid-taker to offer the possibility 
of bid withdrawal to bidders whilst remaining confident about 
postrepair revenue and also facilitating increased bidder aggressiveness 
 references 
 martin andersson and tuomas sandholm leveled 
commitment contracts with myopic and strategic agents 
journal of economic dynamics and control - 
 special issue on agent-based computational 
economics 
 fahiem bacchus and george katsirelos efc solver 
www cs toronto edu ˜gkatsi efc efc html 
 michael berkelaar kjell eikland and peter notebaert 
lp solve version 
http groups yahoo com group lp solve 
 rina dechter constraint processing morgan kaufmann 
 
 sven devries and rakesh vohra combinatorial auctions a 
survey informs journal on computing pages - 
 
 jim ericson reverse auctions bad idea line sept 
 matthew l ginsberg andrew j parkes and amitabha roy 
supermodels and robustness in proceedings of aaai- 
pages - madison wi 
 ronald m harstad and michael h rothkopf withdrawable 
bids as winner s curse insurance operations research 
 - november-december 
 emmanuel hebrard brahim hnich and toby walsh robust 
solutions for constraint satisfaction and optimization in 
proceedings of the european conference on artificial 
intelligence pages - 
 emmanuel hebrard brahim hnich and toby walsh super 
solutions in constraint programming in proceedings of 
cp-ai-or pages - 
 gail hohner john rich ed ng grant reid andrew j 
davenport jayant r kalagnanam ho soo lee and chae 
an combinatorial and quantity-discount procurement 
auctions benefit mars incorporated and its suppliers 
interfaces - 
 alan holland and barry o sullivan super solutions for 
combinatorial auctions in ercim-colognet constraints 
workshop csclp springer lnai lausanne 
switzerland 
 alan holland and barry o sullivan weighted super 
solutions for constraint programs december technical 
report no ucc-cs- - - 
 selective insurance business insurance 
http www selectiveinsurance com psapps 
 business ins bonds asp bc 
 ryan kastner christina hsieh miodrag potkonjak and 
majid sarrafzadeh on the sensitivity of incremental 
algorithms for combinatorial auctions in wecwis pages 
 - june 
 kevin leyton-brown mark pearson and yoav shoham 
towards a universal test suite for combinatorial auction 
algorithms in acm conference on electronic commerce 
pages - 
 associated general contractors of america associated 
general contractors of america white paper on reverse 
auctions for procurement of construction 
http www agc org content public pdf 
 member resources 
reverseauctionwhitepaper pdf 
 national society of professional engineers a basic guide to 
surety bonds http www nspe org pracdiv 
 - surebond asp 
 martin pesendorfer and estelle cantillon combination 
bidding in multi-unit auctions harvard business school 
working draft 
 ryan porter amir ronen yoav shoham and moshe 
tennenholtz mechanism design with execution uncertainty 
in proceedings of uai- pages - 
 jean-charles r´egin global constraints and filtering 
algorithms in constraint and integer 
programmingtowards a unified methodology chapter pages - 
kluwer academic publishers 
 michael h rothkopf and aleksandar peke˘c combinatorial 
auction design management science - 
november 
 michael h rothkopf aleksandar peke˘c and ronald m 
harstad computationally manageable combinatorial 
auctions management science - 
 daniel sabin and eugene c freuder contradicting 
conventional wisdom in constraint satisfaction in a cohn 
editor proceedings of ecai- pages - 
 tuomas sandholm algorithm for optimal winner 
determination in combinatorial auctions artificial 
intelligence - - 
 tuomas sandholm and victor lesser leveled commitment 
contracts and strategic breach games and economic 
behavior - january 
 tuomas sandholm and victor lesser leveled commitment 
contracting a backtracking instrument for multiagent 
systems ai magazine - 
 tuomas sandholm sandeep sikka and samphel norden 
algorithms for optimizing leveled commitment contracts in 
proceedings of the ijcai- pages - morgan 
kaufmann publishers inc 
 tuomas sandholm and yunhong zhou surplus equivalence 
of leveled commitment contracts artificial intelligence 
 - 
 william e walsh michael p wellman and fredrik ygge 
combinatorial auctions for supply chain formation in acm 
conference on electronic commerce pages - 
 rainier weigel and christian bliek on reformulation of 
constraint satisfaction problems in proceedings of ecai- 
pages - 
 margaret w wiener access spectrum bid withdrawal 
http wireless fcc gov auctions 
 releases da pdf july 
 
negotiation-range mechanisms 
exploring the limits of truthful efficient markets 
yair bartal 
∗ 
school of computer science 
and engineering 
the hebrew university of 
jerusalem israel 
yair cs huji ac il 
rica gonen 
school of computer science 
and engineering 
the hebrew university of 
jerusalem israel 
rgonen cs huji ac il 
pierfrancesco la mura 
leipzig graduate school of 
management 
leipzig germany 
plamura hhl de 
abstract 
this paper introduces a new class of mechanisms based on 
negotiation between market participants this model allows 
us to circumvent myerson and satterthwaite s impossibility 
result and present a bilateral market mechanism that is 
efficient individually rational incentive compatible and 
budget balanced in the single-unit heterogeneous setting the 
underlying scheme makes this combination of desirable 
qualities possible by reporting a price range for each buyer-seller 
pair that defines a zone of possible agreements while the 
final price is left open for negotiation 
categories and subject descriptors 
j social and behavioral sciences economics k 
 computers and society electronic 
commerce-payment schemes 
general terms 
algorithms economics theory 
 introduction 
in this paper we introduce the concept of negotiation 
based mechanisms in the context of the theory of efficient 
truthful markets a market consists of multiple buyers and 
sellers who wish to exchange goods the market s main 
objective is to produce an allocation of sellers goods to buyers 
as to maximize the total gain from trade 
a commonly studied model of participant behavior is taken 
from the field of economic mechanism design in 
this model each player has a private valuation function that 
assigns real values to each possible allocation the 
algorithm motivates players to participate truthfully by handing 
payments to them 
the mechanism in an exchange collects buyer bids and 
seller bids and clears the exchange by computing i a set of 
trades and ii the payments made and received by players 
in designing a mechanism to compute trades and payments 
we must consider the bidding strategies of self-interested 
players i e rational players that follow expected-utility 
maximizing strategies we set allocative efficiency as our 
primary goal that is the mechanism must compute a set 
of trades that maximizes gain from trade in addition we 
require individual rationality ir so that all players have 
positive expected utility to participate budget balance bb 
so that the exchange does not run at a loss and incentive 
compatibility ic so that reporting the truth is a dominant 
strategy for each player 
unfortunately myerson and satterthwaite s well 
known result demonstrates that in bilateral trade it is 
impossible to simultaneously achieve perfect efficiency bb and 
ir using an ic mechanism a unique approach to 
overcome myerson and satterthwaite s impossibility result was 
attempted by parkes kalagnanam and eso this result 
designs both a regular and a combinatorial bilateral trade 
mechanism which imposes bb and ir that approximates 
truth revelation and allocation efficiency 
in this paper we circumvent myerson and satterthwaite s 
impossibility result by introducing a new model of 
negotiationrange markets a negotiation-range mechanism does not 
produce payment prices for the market participants rather 
is assigns each buyer-seller pair a price range called zone 
of possible agreements zopa the buyer is provided with 
the high end of the range and the seller with the low end 
of the range this allows the trading parties to engage in 
negotiation over the final price with guarantee that the deal 
is beneficial for both of them the negotiation process is not 
considered part of the mechanism but left up to the 
interested parties or to some external mechanism to perform in 
effect a negotiation-range mechanism operates as a mediator 
between the market participants offering them the grounds 
to be able to finalize the terms of the trade by themselves 
this concept is natural to many real-world market 
environments such as the real estate market 
 
we focus on the single-unit heterogeneous setting n 
sellers offer one unique good each by placing sealed bids 
specifying their willingness to sell and m buyers interested in 
buying a single good each placing sealed bids specifying 
their willingness to pay for each good they may be 
interested in 
our main result is a single-unit heterogeneous bilateral 
trade negotiation-range mechanism zopas that is 
efficient individually rational incentive compatible and budget 
balanced 
our result does not contradict myerson and 
satterthwaite s important theorem myerson-satterthwaite s proof 
relies on a theorem assuring that in two different efficient 
ic markets if the sellers have the same upper bound utility 
then they will receive the same prices in each market and 
if the buyers have the same lower bound utility then they 
will receive the same prices in each market our single-unit 
heterogeneous mechanism bypasses myerson and 
satterthwaite s theorem by producing a price range defined by a 
seller s floor and a buyer s ceiling for each pair of matched 
players in our market mechanism the seller s upper bound 
utility may be the same while the seller s floor is different 
and the buyer s lower bound utility may be the same while 
the buyer s ceiling is different moreover the final price is 
not fixed by the mechanism at all instead it is determined 
by an independent negotiation between the buyer and seller 
more specifically in a negotiation-range mechanism the 
range of prices each matched pair is given is resolved by a 
negotiation stage where a final price is determined this 
negotiation stage is crucial for our mechanism to be ic 
intuitively a negotiation range mechanism is incentive 
compatible if truth telling promises the best zopa from the 
point of view of the player in question that is he would 
tell the truth if this strategy maximizes the upper and lower 
bounds on his utility as expressed by the zopa boundaries 
yet when carefully examined it turns out that it is 
impossible by that this goal will always hold this is simply 
because such a mechanism could be easily modified to 
determine final prices for the players e g by taking the average of 
the range s boundaries here the negotiation stage come 
into play we show that if the above utility maximizing 
condition does not hold then it is the case that the player 
cannot influence the negotiation bound that is assigned to 
his deal partner no matter what value he declares this 
means that the only thing that he may achieve by reporting 
a false valuation is modifying his own negotiation bound 
something that he could alternatively achieve by reporting 
his true valuation and incorporating the effect of the 
modified negotiation bound into his negotiation strategy this 
eliminates the benefit of reporting false valuations and 
allows our mechanism to compute the optimal gain from trade 
according to the players true values 
the problem of computing the optimal allocation which 
maximizes gain from trade can be conceptualized as the 
problem of finding the maximum weighted matching in a 
weighted bipartite graph connecting buyers and sellers where 
each edge in the graph is assigned a weight equal to the 
difference between the respective buyer bid and seller bid it 
is well known that this problem can be solved efficiently in 
polynomial time 
vcg ic payment schemes support efficient and 
ir bilateral trade but not simultaneously bb our particular 
approach adapts the vcg payment scheme to achieve 
budget balance the philosophy of the vcg payment schemes 
in bilateral trade is that the buyer pays the seller s 
opportunity cost of not selling the good to another buyer and not 
keeping the good to herself the seller is paid in addition to 
the buyer s price a compensation for the damage the 
mechanism did to the seller by not extracting the buyer s full 
bid our philosophy is a bit different the seller is paid at 
least her opportunity cost of not selling the good to another 
buyer and not keeping the good for herself the buyer pays 
at most his alternate seller s opportunity cost of not selling 
the good to another buyer and not keeping the alternate 
good for herself 
the rest of this paper is organized as follows in 
section we describe our model and definitions in section 
we present the single-unit heterogeneous negotiation-range 
mechanism and show that it is efficient ir ic and bb 
finally we conclude with a discussion in section 
 negotiation markets 
preliminaries 
let π denote the set of players n the set of n 
selling players and m the set of m buying players where 
π n ∪ m let ψ { t} denote the set of goods 
let ti ∈ {− }t 
denote an exchange vector for a trade 
such that player i buys goods {a ∈ ψ ti a } and sells 
goods {a ∈ ψ ti a − } let t t t π denote 
the complete trade between all players we view t as 
describing the allocation of goods by the mechanism to the 
buyers and sellers 
in the single-unit heterogeneous setting every good 
belongs to specific seller and every buyer is interested in 
buying one good the buyer may bid for several or all goods 
at the end of the auction every good is either assigned to 
one of the buyers who bid for it or kept unsold by the seller 
it is convenient to assume the sets of buyers and sellers are 
disjoint though it is not required i e n ∩ m ∅ each 
seller i is associated with exactly one good ai for which she 
has true valuation ci which expresses the price at which it 
is beneficial for her to sell the good if the seller reports a 
false valuation at an attempt to improve the auction results 
for her this valuation is denoted ˆci a buyer has a 
valuation vector describing his valuation for each of the goods 
according to their owner specifically vj k denotes buyer 
j s valuation for good ak similarly if he reports a false 
valuation it is denoted ˆvj k 
if buyer j is matched by the mechanism with seller i then 
ti ai − and tj ai notice that in our setting for 
every k i ti ak and tj ai and also for every 
z j tz ai 
for a matched buyer j - seller i pair the gain from trade 
on the deal is defined as vj i − ci given and allocation t 
the gain from trade associated with t is 
v 
j∈m i∈n 
 vj i − ci · tj ai 
let t∗ 
denote the optimal allocation which maximizes the 
gain from trade computed according to the players true 
valuations let v ∗ 
denote the optimal gain from trade 
associated with this allocation 
when players report false valuations we use ˆt∗ 
and ˆv ∗ 
to 
denote the optimal allocation and gain from trade 
respectively when computed according to the reported valuations 
 
we are interested in the design of negotiation-range 
mechanisms in contrast to a standard auction mechanism where 
the buyer and seller are provided with the prices they should 
pay the goal of a negotiation-range mechanism is to provide 
the player s with a range of prices within which they can 
negotiate the final terms of the deal by themselves the 
mechanism would provide the buyer with the upper bound 
on the range and the seller with the lower bound on the 
range this gives each of them a promise that it will be 
beneficial for them to close the deal but does not provide 
information about the other player s terms of negotiation 
definition negotiation range zone of possible 
agreements zopa between a matched buyer and seller the 
zopa is a range l h ≤ l ≤ h where h is an upper 
bound ceiling price for the buyer and l is a lower bound 
 floor price for the seller 
definition negotiation-range mechanism a 
mechanism that computes a zopa l h for each matched buyer 
and seller in t∗ 
 and provides the buyer with the upper bound 
h and the seller with the lower bound l 
the basic assumption is that participants in the auction 
are self-interested players that is their main goal is to 
maximize their expected utility the utility for a buyer who 
does not participate in the trade is if he does win some 
good his utility is the surplus between his valuation for that 
good and the price he pays for a seller if she keeps the good 
unsold her utility is just her valuation of the good and the 
surplus is if she gets to sell it her utility is the price she 
is paid for it and the surplus is the difference between this 
price and her valuation 
since negotiation-range mechanisms assign bounds on the 
range of prices rather than the final price it is useful to 
define the upper and lower bounds on the player s utilities 
defined by the range s limits 
definition consider a buyer j - seller i pair matched 
by a negotiation-range mechanism and let l h be their 
associated negotiation range 
 the buyer s top utility is vj i − l and the buyer s 
bottom utility is vj i − h 
 the seller s top utility is h with surplus h − ci and 
the seller s bottom utility is l with surplus l − ci 
 the single-unit heterogeneous 
mechanism zopas 
 description of the mechanism 
zopas is a negotiation-range mechanism it finds the 
optimal allocation t∗ 
and uses it to define a zopa for each 
buyer-seller pair 
the first stage in applying the mechanism is for the buyers 
and sellers to submit their sealed bids the mechanism then 
allocates buyers to sellers by computing the allocation t ∗ 
 
which results in the optimal gain from trade v ∗ 
 and defines 
a zopa for each buyer-seller pair finally buyers and sellers 
use the zopa to negotiate a final price 
computing t∗ 
involves solving the maximum weighted 
bipartite matching problem for the complete bipartite graph 
kn m constructed by placing the buyers on one side of the 
find the optimal allocation t ∗ 
compute the maximum weighted bipartite 
matching for the bipartite graph 
of buyers and sellers and edge weights 
equal to the gain from trade 
calculate sellers floors 
for every buyer j allocated good ai 
find the optimal allocation t−j ∗ 
li vj i v−j ∗ 
− v ∗ 
calculate buyers ceilings 
for every buyer j allocated good ai 
find the optimal allocation t −i 
 ∗ 
find the optimal allocation t −i 
−j ∗ 
hj vj i v −i 
−j ∗ 
− v −i 
 ∗ 
negotiation phase 
for every buyer j 
and every seller i of good ai 
report to seller i her floor li 
and identify her matched buyer j 
report to buyer j his ceiling hj 
and identify his matched seller i 
i j negotiate the good s final price 
figure the zopas mechanism 
graph the seller on another and giving the edge between 
buyer j and seller i weight equal to vj i − ci the 
maximum weighted matching problem in solvable in polynomial 
time e g using the hungarian method this results in 
a matching between buyers and sellers that maximizes gain 
from trade 
the next step is to compute for each buyer-seller pair a 
seller s floor which provides the lower bound of the zopa 
for this pair and assigns it to the seller 
a seller s floor is computed by calculating the difference 
between the total gain from trade when the buyer is excluded 
and the total gain from trade of the other participants when 
the buyer is included the vcg principle 
let t−j ∗ 
denote the gain from trade of the optimal 
allocation when buyer j s bids are discarded denote by v−j ∗ 
the total gain from trade in the allocation t−j ∗ 
 
definition seller floor the lowest price the seller 
should expect to receive communicated to the seller by the 
mechanism the seller floor for player i who was matched 
with buyer j on good ai i e tj ai is computed as 
li vj i v−j ∗ 
− v ∗ 
 
the seller is instructed not to accept less than this price from 
her matched buyer 
next the mechanism computes for each buyer-seller pair a 
buyer ceiling which provides the upper bound of the zopa 
for this pair and assigns it to the buyer 
each buyer s ceiling is computed by removing the buyer s 
matched seller and calculating the difference between the 
total gain from trade when the buyer is excluded and the 
total gain from trade of the other participants when the 
 
buyer is included let t−i 
 ∗ 
denote the gain from trade 
of the optimal allocation when seller i is removed from the 
trade denote by v −i 
 ∗ 
the total gain from trade in the 
allocation t−i 
 ∗ 
 
let t−i 
−j ∗ 
denote the gain from trade of the optimal 
allocation when seller i is removed from the trade and buyer j s 
bids are discarded denote by v −i 
−j ∗ 
the total gain from 
trade in the allocation t −i 
−j ∗ 
 
definition buyer ceiling the highest price the seller 
should expect to pay communicated to the buyer by the 
mechanism the buyer ceiling for player j who was matched with 
seller i on good ai i e tj ai is computed as 
hj vj i v −i 
−j ∗ 
− v −i 
 ∗ 
 
the buyer is instructed not to pay more than this price to 
his matched seller 
once the negotiation range lower bound and upper bound 
are computed for every matched pair the mechanism reports 
the lower bound price to the seller and the upper bound price 
to the buyer at this point each buyer-seller pair negotiates 
on the final price and concludes the deal 
a schematic description the zopas mechanism is given 
in figure 
 analysis of the mechanism 
in this section we analyze the properties of the zopas 
mechanism 
theorem the zopas market negotiation-range 
mechanism is an incentive-compatible bilateral trade 
mechanism that is efficient individually rational and budget 
balanced 
clearly zopas is an efficient polynomial time mechanism 
let us show it satisfies the rest of the properties in the 
theorem 
claim zopas is individually rational i e the 
mechanism maintains nonnegative utility surplus for all 
participants 
proof if a participant does not trade in the optimal 
allocation then his utility surplus is zero by definition 
consider a pair of buyer j and seller i which are matched in the 
optimal allocation t ∗ 
 then the buyer s utility is at least 
vj i − hj recall that hj vj i v −i 
−j ∗ 
− v −i 
 ∗ 
 so 
that vj i − hj v −i 
 ∗ 
− v −i 
−j ∗ 
 since the optimal gain 
from trade which includes j is higher than that which does 
not we have that the utility is nonnegative vj i − hj ≥ 
now consider the seller i her utility surplus is at least 
li − ci recall that li vj i v−j ∗ 
− v ∗ 
 if we 
removed from the optimal allocation t ∗ 
the contribution of 
the buyer j - seller i pair we are left with an allocation 
which excludes j and has value v ∗ 
− vj i − ci this 
implies that v−j ∗ 
≥ v ∗ 
− vj i ci which implies that 
li − ci ≥ 
the fact that zopas is a budget-balanced mechanism 
follows from the following lemma which ensures the validity 
of the negotiation range i e that every seller s floor is below 
her matched buyer s ceiling this ensures that they can close 
the deal at a final price which lies in this range 
lemma for every buyer j- seller i pair matched by the 
mechanism li ≤ hj 
proof recall that li vj i v−j ∗ 
− v ∗ 
and hj 
vj i v −i 
−j ∗ 
− v −i 
 ∗ 
 to prove that li ≤ hj it is enough 
to show that 
 v −i 
 ∗ 
 v−j ∗ 
≤ v ∗ 
 v −i 
−j ∗ 
 
the proof of is based on a method which we apply 
several times in our analysis we start with the 
allocations t−i 
 ∗ 
and t−j ∗ 
which together have value equal 
to v −i 
 ∗ 
 v−j ∗ 
 we now use them to create a pair of 
new valid allocations by using the same pairs that were 
matched in the original allocations this means that the 
sum of values of the new allocations is the same as the 
original pair of allocations we also require that one of the new 
allocations does not include buyer j or seller i this means 
that the sum of values of these new allocations is at most 
v ∗ 
 v −i 
−j ∗ 
 which proves 
let g be the bipartite graph where the nodes on one side 
of g represent the buyers and the nodes on the other side 
represent the sellers and edge weights represent the gain 
from trade for the particular pair the different allocations 
represent bipartite matchings in g it will be convenient for 
the sake of our argument to think of the edges that belong 
to each of the matchings as being colored with a specific 
color representing this matching 
assign color to the edges in the matching t −i 
 ∗ 
and 
assign color to the edges in the matching t−j ∗ 
 we claim 
that these edges can be recolored using colors and so that 
the new coloring represents allocations t represented by 
color and t−i 
−j represented by color this implies 
the that inequality holds figure illustrates the graph 
g and the colorings of the different matchings 
define an alternating path p starting at j let s be 
the seller matched to j in t −i 
 ∗ 
 if none exists then p is 
empty let b be the buyer matched to s in t−j ∗ 
 s be 
the seller matched to b in t−i 
 ∗ 
 b be the buyer matched 
to s in t−j ∗ 
 and so on this defines an alternating 
path p starting at j whose edges colors alternate between 
colors and starting with this path ends either in a 
seller who is not matched in t−j ∗ 
or in a buyer who is not 
matched in t−i 
 ∗ 
 since all sellers in this path are matched 
in t−i 
 ∗ 
 we have that seller i does not belong to p this 
ensures that edges in p may be colored by alternating colors 
 and starting with since except for the first edge all 
others do not involve i or j and thus may be colored and 
be part of an allocation t −i 
−j 
we are left to recolor the edges that do not belong to p 
since none of these edges includes j we have that the edges 
that were colored which are part of t −i 
 ∗ 
 may now be 
colored and be included in the allocation t −i 
−j it is 
also clear that the edges that were colored which are part 
of t−j ∗ 
 may now be colored and be included in the 
allocation t this completes the proof of the lemma 
 incentive compatibility 
the basic requirement in mechanism design is for an 
exchange mechanism to be incentive compatible this means 
that its payment structure enforces that truth-telling is the 
players weakly dominant strategy that is that the 
strategy by which the player is stating his true valuation results 
 
 
js 
s b 
s b 
b s 
s b 
s b 
s b 
s b 
 
figure alternating path argument for lemma 
 validity of the negotiation range and claim 
 part of buyer s ic proof 
colors 
bidders 
 
unmatchedmatched 
figure key to figure 
in bigger or equal utility as any other strategy the 
utility surplus is defined as the absolute difference between the 
player s bid and his price 
negotiation-range mechanisms assign bounds on the range 
of prices rather than the final price and therefore the player s 
valuation only influences the minimum and maximum bounds 
on his utility for a buyer the minimum bottom utility 
would be based on the top of the negotiation range 
 ceiling and the maximum top utility would be based on the 
bottom of the negotiation range floor for a seller it s the 
other way around therefore the basic natural requirement 
from negotiation-range mechanisms would be that stating 
the player s true valuation results in both the higher 
bottom utility and higher top utility for the player compared 
with other strategies unfortunately this requirement is 
still too strong and it is impossible by that this will 
always hold therefore we slightly relax it as follows we 
require this holds when the false valuation based strategy 
changes the player s allocation when the allocation stays 
unchanged we require instead that the player would not be 
able to change his matched player s bound e g a buyer 
cannot change the seller s floor this means that the only 
thing he can influence is his own bound something that he 
can alternatively achieve through means of negotiation 
the following formally summarizes our incentive 
compatibility requirements from the negotiation-range mechanism 
buyer s incentive compatibility 
 let j be a buyer matched with seller i by the 
mechanism according to valuation vj and the 
negotiationrange assigned is li hj assume that when the 
mechanism is applied according to valuation ˆvj seller 
k i is matched with j and the negotiation-range 
assigned is ˆlk ˆhj then 
vj i − hj ≥ vj k − ˆhj 
vj i − li ≥ vj k − ˆlk 
 let j be a buyer not matched by the mechanism 
according to valuation vj assume that when the 
mechanism is applied according to valuation ˆvj seller k i 
is matched with j and the negotiation-range assigned 
is ˆlk ˆhj then 
vj k − ˆhj ≤ vj k − ˆlk ≤ 
 let j be a buyer matched with seller i by the 
mechanism according to valuation vj and let the assigned 
bottom of the negotiation range seller s floor be li 
assume that when the mechanism is applied according 
to valuation ˆvj the matching between i and j remains 
unchanged and let the assigned bottom of the 
negotiation range seller s floor be ˆli then 
ˆli li 
notice that the first inequality of always holds for a valid 
negotiation range mechanism lemma 
seller s incentive compatibility 
 let i be a seller not matched by the mechanism 
according to valuation ci assume that when the mechanism 
 
is applied according to valuation ˆci buyer z j is 
matched with i and the negotiation-range assigned is 
 ˆli ˆhz then 
ˆli − ci ≤ ˆhz − ci ≤ 
 let i be a buyer matched with buyer j by the 
mechanism according to valuation ci and let the assigned top 
of the negotiation range buyer s ceiling be hj 
assume that when the mechanism is applied according 
to valuation ˆci the matching between i and j remains 
unchanged and let the assigned top of the negotiation 
range buyer s ceiling be ˆhj then 
ˆhj hj 
notice that the first inequality of always holds for a valid 
negotiation range mechanism lemma 
observe that in the case of sellers in our setting the case 
expressed by requirement is the only case in which the 
seller may change the allocation to her benefit in particular 
it is not possible for seller i who is matched in t ∗ 
to change 
her buyer by reporting a false valuation this fact simply 
follows from the observation that reducing the seller s 
valuation increases the gain from trade for the current allocation 
by at least as much than any other allocation whereas 
increasing the seller s valuation decreases the gain from trade 
for the current allocation by exactly the same amount as any 
other allocation in which it is matched therefore the only 
case the optimal allocation may change is when in the new 
allocation i is not matched in which case her utility surplus 
is 
theorem zopas is an incentive compatible 
negotiationrange mechanism 
proof we begin with the incentive compatibility for 
buyers 
consider a buyer j who is matched with seller i according 
to his true valuation v consider that j is reporting instead 
a false valuation ˆv which results in a different allocation in 
which j is matched with seller k i the following claim 
shows that a buyer j which changed his allocation due to 
a false declaration of his valuation cannot improve his top 
utility 
claim let j be a buyer matched to seller i in t ∗ 
 and 
let k i be the seller matched to j in ˆt∗ 
 then 
vj i − hj ≥ vj k − ˆhj 
proof recall that hj vj i v −i 
−j ∗ 
− v −i 
 ∗ 
and 
ˆhj ˆvj k ˆv −k 
−j ∗ 
− ˆv −k 
 ∗ 
 therefore vj i − hj 
 v −i 
 ∗ 
− v −i 
−j ∗ 
and vj k − ˆhj vj k − ˆvj k ˆv −k 
 ∗ 
− 
 ˆv −k 
−j ∗ 
 
it follows that in order to prove we need to show 
 ˆv −k 
 ∗ 
 v −i 
−j ∗ 
≤ v −i 
 ∗ 
 ˆv −k 
−j ∗ 
 ˆvj k − vj k 
consider first the case were j is matched to i in ˆt−k 
 ∗ 
 if 
we remove this pair and instead match j with k we obtain 
a matching which excludes i if the gain from trade on the 
new pair is taken according to the true valuation then we 
get 
 ˆv −k 
 ∗ 
− ˆvj i − ci vj k − ck ≤ v −i 
 ∗ 
 
now since the optimal allocation ˆt∗ 
matches j with k rather 
than with i we have that 
 v −i 
−j ∗ 
 ˆvj i − ci ≤ ˆv ∗ 
 ˆv −k 
−j ∗ 
 ˆvj k − ck 
where we have used that ˆv −i 
−j ∗ 
 v −i 
−j ∗ 
since these 
allocations exclude j adding up these two inequalities implies 
 in this case 
it is left to prove when j is not matched to i in ˆt−k 
 ∗ 
 
in fact in this case we prove the stronger inequality 
 ˆv −k 
 ∗ 
 v −i 
−j ∗ 
≤ v −i 
 ∗ 
 ˆv −k 
−j ∗ 
 
it is easy to see that indeed implies since it follows 
from the fact that k is assigned to j in ˆt∗ 
that ˆvj k ≥ 
vj k the proof of works as follows we start with the 
allocations ˆt−k 
 ∗ 
and t−i 
−j ∗ 
which together have value 
equal to ˆv −k 
 ∗ 
 v −i 
−j ∗ 
 we now use them to create a 
pair of new valid allocations by using the same pairs that 
were matched in the original allocations this means that 
the sum of values of the new allocations is the same as the 
original pair of allocations we also require that one of the 
new allocations does not include seller i and is based on the 
true valuation v while the other allocation does not include 
buyer j or seller k and is based on the false valuation ˆv this 
means that the sum of values of these new allocations is at 
most v −i 
 ∗ 
 ˆv −k 
−j ∗ 
 which proves 
let g be the bipartite graph where the nodes on one side 
of g represent the buyers and the nodes on the other side 
represent the sellers and edge weights represent the gain 
from trade for the particular pair the different allocations 
represent bipartite matchings in g it will be convenient for 
the sake of our argument to think of the edges that belong 
to each of the matchings as being colored with a specific 
color representing this matching 
assign color to the edges in the matching ˆt−k 
 ∗ 
and 
assign color to the edges in the matching t −i 
−j ∗ 
 we claim 
that these edges can be recolored using colors and so that 
the new coloring represents allocations t −i 
 represented 
by color and ˆt−k 
−j represented by color this implies 
the that inequality holds figure illustrates the graph 
g and the colorings of the different matchings 
define an alternating path p starting at j let s i be 
the seller matched to j in ˆt−k 
 ∗ 
 if none exists then p is 
empty let b be the buyer matched to s in t−i 
−j ∗ 
 s be 
the seller matched to b in ˆt−k 
 ∗ 
 b be the buyer matched 
to s in t−i 
−j ∗ 
 and so on this defines an alternating 
path p starting at j whose edges colors alternate between 
colors and starting with this path ends either in 
a seller who is not matched in t −i 
−j ∗ 
or in a buyer who 
is not matched in ˆt−k 
 ∗ 
 since all sellers in this path are 
matched in ˆt−k 
 ∗ 
 we have that seller k does not belong to 
p since in this case s i and the rest of the sellers in p 
are matched in t−i 
−j ∗ 
we have that seller i as well does not 
belong to p this ensures that edges in p may be colored 
by alternating colors and starting with since s i 
we may use color for the first edge and thus assign it to 
the allocation t−i 
 all other edges do not involve i j 
or k and thus may be either colored and be part of an 
allocation ˆt−k 
−j or colored and be part of an allocation 
 t−i 
 in an alternating fashion 
we are left to recolor the edges that do not belong to p 
since none of these edges includes j we have that the edges 
 
that were colored which are part of ˆt−k 
 ∗ 
 may now be 
colored and be included in the allocation ˆt−k 
−j it is 
also clear that the edges that were colored which are part 
of t−i 
−j ∗ 
 may now be colored and be included in the 
allocation t−i 
 this completes the proof of and the 
claim 
the following claim shows that a buyer j which changed 
his allocation due to a false declaration of his valuation 
cannot improve his bottom utility the proof is basically the 
standard vcg argument 
claim let j be a buyer matched to seller i in t ∗ 
 and 
k i be the seller matched to j in ˆt∗ 
 then 
vj i − li ≥ vj k − ˆlk 
proof recall that li vj i v−j ∗ 
− v ∗ 
 and ˆlk 
ˆvj k ˆv−j ∗ 
− ˆv ∗ 
 ˆvj k v−j ∗ 
− ˆv ∗ 
 therefore 
vj i − li v ∗ 
− v−j ∗ 
and vj k − ˆlk vj k − ˆvj k 
ˆv ∗ 
− v−j ∗ 
 
it follows that in order to prove we need to show 
v ∗ 
≥ vj k − ˆvj k ˆv ∗ 
 
the scenario of this claim occurs when j understates his 
value for ai or overstated his value for ak consider these 
two cases 
 ˆvj k vj k since ak was allocated to j in the 
allocation ˆt∗ 
we have that using the allocation of ˆt∗ 
according to the true valuation gives an allocation of 
value u satisfying ˆv ∗ 
− ˆvj k vj k ≤ u ≤ v ∗ 
 
 ˆvj k vj k and ˆvj i vj i in this case 
reduces to v ∗ 
≥ ˆv ∗ 
 since j is not allocated i in ˆt∗ 
we have that ˆt∗ 
is an allocation that uses only true 
valuations from the optimality of t ∗ 
we conclude 
that v ∗ 
≥ ˆv ∗ 
 
another case in which a buyer may try to improve his 
utility is when he does not win any good by stating his true 
valuation he may give a false valuation under which he 
wins some good the following claim shows that doing this 
is not beneficial to him 
claim let j be a buyer not matched in t ∗ 
 and 
assume seller k is matched to j in ˆt∗ 
 then 
vj k − ˆlk ≤ 
proof the scenario of this claim occurs if j did not 
buy in the truth-telling allocation and overstates his value 
for ak ˆvj k vj k in his false valuation recall that 
ˆlk ˆvj k ˆv−j ∗ 
− ˆv ∗ 
 thus we need to show that 
 ≥ vj k − ˆvj k ˆv ∗ 
− v−j ∗ 
 since j is not allocated 
in t∗ 
then v−j ∗ 
 v ∗ 
 since j is allocated ak in ˆt∗ 
we have that using the allocation of ˆt∗ 
according to the 
true valuation gives an allocation of value u satisfying ˆv ∗ 
− 
ˆvj k vj k ≤ u ≤ v ∗ 
 thus we can conclude that ≥ 
vj k − ˆvj k ˆv ∗ 
− v−j ∗ 
 
finally the following claim ensures that a buyer cannot 
influence the floor bound of the zopa for the good he wins 
claim let j be a buyer matched to seller i in t ∗ 
 and 
assume that ˆt∗ 
 t∗ 
 then ˆli li 
proof recall that li vj i v−j ∗ 
− v ∗ 
 and ˆli 
ˆvj i ˆv−j ∗ 
− ˆv ∗ 
 ˆvj i v−j ∗ 
− ˆv ∗ 
 therefore we 
need to show that ˆv ∗ 
 v ∗ 
 ˆvj i − vj i 
since j is allocated ai in t∗ 
 we have that using the 
allocation of t∗ 
according to the false valuation gives an allocation 
of value u satisfying v ∗ 
− vj i ˆvj i ≤ u ≤ ˆv ∗ 
 
similarly since j is allocated ai in ˆt∗ 
 we have that using the 
allocation of ˆt∗ 
according to the true valuation gives an 
allocation of value u satisfying ˆv ∗ 
− ˆvj i vj i ≤ u ≤ v ∗ 
 
which together with the previous inequality completes the 
proof 
this completes the analysis of the buyer s incentive 
compatibility we now turn to prove the seller s incentive 
compatibility properties of our mechanism 
the following claim handles the case where a seller that 
was not matched in t ∗ 
falsely understates her valuation such 
that she gets matched n ˆt∗ 
 
claim let i be a seller not matched in t ∗ 
 and assume 
buyer z is matched to i in ˆt∗ 
 then 
ˆhz − ci ≤ 
proof recall that ˆhz vz i ˆv −i 
−z ∗ 
− ˆv −i 
 ∗ 
 
since i is not matched in t ∗ 
and ˆt−i 
 ∗ 
involves only true 
valuations we have that ˆv −i 
 ∗ 
 v ∗ 
 since i is matched 
with z in ˆt∗ 
it can be obtained by adding the buyer z - seller 
i pair to ˆt−i 
−z ∗ 
 it follows that ˆv ∗ 
 ˆv −i 
−z ∗ 
 vz i − ˆci 
thus we have that ˆhz ˆv ∗ 
 ˆci − v ∗ 
 now since i is 
matched in ˆt∗ 
 using this allocation according to the true 
valuation gives an allocation of value u satisfying ˆv ∗ 
 ˆci − 
ci ≤ u ≤ v ∗ 
 therefore ˆhz −ci ˆv ∗ 
 ˆci −v ∗ 
−ci ≤ 
finally the following simple claim ensures that a seller 
cannot influence the ceiling bound of the zopa for the good 
she sells 
claim let i be a seller matched to buyer j in t ∗ 
 and 
assume that ˆt∗ 
 t∗ 
 then ˆhj hj 
proof since ˆv −i 
−j ∗ 
 v −i 
−j ∗ 
and ˆv −i 
 ∗ 
 v −i 
 ∗ 
it 
follows that 
ˆhj vj i ˆv −i 
−j ∗ 
− ˆv −i 
 ∗ 
 vj i v −i 
−j ∗ 
− v −i 
 ∗ 
 hj 
 conclusions and extensions 
in this paper we suggest a way to deal with the 
impossibility of producing mechanisms which are efficient 
individually rational incentive compatible and budget balanced 
to this aim we introduce the concept of negotiation-range 
mechanisms which avoid the problem by leaving the final 
determination of prices to a negotiation between the buyer 
and seller the goal of the mechanism is to provide the 
initial range zopa for negotiation in a way that it will be 
beneficial for the participants to close the proposed deals 
we present a negotiation range mechanism that is efficient 
individually rational incentive compatible and budget 
balanced the zopa produced by our mechanism is based 
 
on a natural adaptation of the vcg payment scheme in a 
way that promises valid negotiation ranges which permit a 
budget balanced allocation 
the basic question that we aimed to tackle seems very 
exciting which properties can we expect a market 
mechanism to achieve are there different market models and 
requirements from the mechanisms that are more feasible 
than classic mechanism design goals 
in the context of our negotiation-range model is 
natural to further study negotiation based mechanisms in more 
general settings a natural extension is that of a 
combinatorial market unfortunately finding the optimal allocation 
in a combinatorial setting is np-hard and thus the problem 
of maintaining bb is compounded by the problem of 
maintaining ic when efficiency is approximated 
applying the approach in this paper to develop 
negotiationrange mechanisms for combinatorial markets even in 
restricted settings seems a promising direction for research 
 references 
 y bartal r gonen and n nisan incentive 
compatible multi-unit combinatorial auctions 
proceeding of th tark pp - june 
 e h clarke multipart pricing of public goods in 
journal public choice volume pages - 
 j feigenbaum c papadimitriou and s shenker 
sharing the cost of multicast transmissions journal 
of computer and system sciences 
 a fiat a goldberg j hartline and a karlin 
competitive generalized auctions proceeding of th 
acm symposium on theory of computing 
 r gonen and d lehmann optimal solutions for 
multi-unit combinatorial auctions branch and 
bound heuristics proceeding of acm conference on 
electronic commerce ec pages - october 
 
 r gonen and d lehmann linear programming 
helps solving large multi-unit combinatorial 
auctions in proceeding of informs 
november 
 t groves incentives in teams in journal 
econometrica volume pages - 
 r lavi a mu alem and n nisan towards a 
characterization of truthful combinatorial auctions 
proceeding of th annual ieee symposium on 
foundations of computer science 
 d lehmann l i o callaghan and y shoham 
truth revelation in rapid approximately efficient 
combinatorial auctions in proceedings of the first 
acm conference on electronic commerce pages 
 - november 
 r myerson m satterthwaite efficient mechanisms 
for bilateral trading journal of economic theory 
 pages - 
 n nisan and a ronen algorithmic mechanism 
design in proceeding of th acm symposium on 
theory of computing 
 d c parkes j kalagnanam and m eso achieving 
budget-balance with vickrey-based payment schemes 
in exchanges proceeding of th international joint 
conference on artificial intelligence pages - 
 
 w vickrey counterspeculation auctions and 
competitive sealed tenders in journal of finance 
 volume pages - 
 
a price-anticipating resource allocation mechanism 
for distributed shared clusters 
michal feldman∗ 
mfeldman sims berkeley edu 
kevin lai† 
kevin lai hp com 
li zhang† 
l zhang hp com 
abstract 
in this paper we formulate the fixed budget resource allocation 
game to understand the performance of a distributed 
marketbased resource allocation system multiple users decide how to 
distribute their budget bids among multiple machines 
according to their individual preferences to maximize their individual 
utility we look at both the efficiency and the fairness of the 
allocation at the equilibrium where fairness is evaluated through 
the measures of utility uniformity and envy-freeness we show 
analytically and through simulations that despite being highly 
decentralized such a system converges quickly to an equilibrium 
and unlike the social optimum that achieves high efficiency but 
poor fairness the proposed allocation scheme achieves a nice 
balance of high degrees of efficiency and fairness at the equilibrium 
categories and subject descriptors 
c computer-communication networks distributed 
systems c performance of systems f analysis 
of algorithms and problem complexity 
nonnumerical algorithms and problems j social and behavioral 
sciences economics 
general terms 
algorithms performance design economics 
 introduction 
the primary advantage of distributed shared clusters like 
the grid and planetlab is their ability to pool 
together shared computational resources this allows increased 
throughput because of statistical multiplexing and the bursty 
utilization pattern of typical users sharing nodes that are 
dispersed in the network allows lower delay because 
applications can store data close to users finally sharing allows 
greater reliability because of redundancy in hosts and 
network connections 
however resource allocation in these systems remains the 
major challenge the problem is how to allocate a shared 
resource both fairly and efficiently where efficiency is the 
ratio of the achieved social welfare to the social optimal 
with the presence of strategic users who act in their own 
interests 
several non-economic allocation algorithms have been 
proposed but these typically assume that task values i e their 
importance are the same or are inversely proportional to 
the resources required or are set by an omniscient 
administrator however in many cases task values vary 
significantly are not correlated to resource requirements and are 
difficult and time-consuming for an administrator to set 
instead we examine a market-based resource allocation 
system others are described in that allows 
users to express their preferences for resources through a 
bidding mechanism 
in particular we consider a price-anticipating scheme 
in which a user bids for a resource and receives the ratio of 
his bid to the sum of bids for that resource this 
proportional scheme is simpler more scalable and more 
responsive than auction-based schemes previous 
work has analyzed price-anticipating schemes in the context 
of allocating network capacity for flows for users with 
unlimited budgets in this work we examine a price-anticipating 
scheme in the context of allocating computational 
capacity for users with private preferences and limited budgets 
resulting in a qualitatively different game as discussed in 
section 
in this paper we formulate the fixed budget resource 
allocation game and study the existence and performance of 
the nash equilibria of this game for evaluating the nash 
equilibria we consider both their efficiency measuring how 
close the social welfare at equilibrium is to the social 
optimum and fairness measuring how different the users 
utilities are although rarely considered in previous game 
theoretical study we believe fairness is a critical metric for a 
resource allocation schemes because the perception of 
unfairness will cause some users to reject a system with more 
efficient but less fair resource allocation in favor of one with 
less efficient more fair resource allocation we use both 
utility uniformity and envy-freeness to measure fairness 
utility uniformity which is common in computer science work 
measures the closeness of utilities of different users 
envyfreeness which is more from the economic perspective 
measures the happiness of users with their own resources 
compared to the resources of others 
our contributions are as follows 
 we analyze the existence and performance of 
 
nash equilibria using analysis we show that there is 
always a nash equilibrium in the fixed budget game if the 
utility functions satisfy a fairly weak and natural condition 
of strong competitiveness we also show the worst case 
performance bounds for m players the efficiency at equilibrium 
is ω 
√ 
m the utility uniformity is ≥ m and the 
envyfreeness ≥ 
√ 
 − ≈ although these bounds are quite 
low the simulations described below indicate these bounds 
are overly pessimistic 
 we describe algorithms that allow strategic users 
to optimize their utility as part of the fixed budget 
game analysis we show that strategic users with linear 
utility functions can calculate their bids using a best response 
algorithm that quickly results in an allocation with high 
efficiency with little computational and communication 
overhead we present variations of the best response algorithm 
for both finite and infinite parallelism tasks in addition we 
present a local greedy adjustment algorithm that converges 
more slowly than best response but allows for non-linear or 
unformulatable utility functions 
 we show that the price-anticipating resource 
allocation mechanism achieves a high degree of 
efficiency and fairness using simulation we find that 
although the socially optimal allocation results in perfect 
efficiency it also results in very poor fairness likewise 
allocating according to only users preference weights results in a 
high fairness but a mediocre efficiency intuition would 
suggest that efficiency and fairness are exclusive surprisingly 
the nash equilibrium reached by each user iteratively 
applying the best response algorithm to adapt his bids achieves 
nearly the efficiency of the social optimum and nearly the 
fairness of the weight-proportional allocation the efficiency 
is ≥ the utility uniformity is ≥ and the 
envyfreeness is ≥ independent of the number of users in 
the system in addition the time to converge to the 
equilibrium is ≤ iterations when all users use the best response 
strategy the local adjustment algorithm performs similarly 
when there is sufficient competitiveness but takes to 
iterations to stabilize 
as a result we believe that shared distributed systems 
based on the fixed budget game can be highly decentralized 
yet achieve a high degree of efficiency and fairness 
the rest of the paper is organized as follows we 
describe the model in section and derive the performance 
at the nash equilibria for the infinite parallelism model in 
section in section we describe algorithms for users 
to optimize their own utility in the fixed budget game in 
section we describe our simulator and simulation results 
we describe related work in section we conclude by 
discussing some limit of our model and future work in section 
 the model 
price-anticipating resource allocation we study 
the problem of allocating a set of divisible resources or 
machines suppose that there are m users and n machines 
each machine can be continuously divided for allocation 
to multiple users an allocation scheme ω r rm 
where ri ri · · · rin with rij representing the share of 
machine j allocated to user i satisfies that for any ≤ i ≤ m 
and ≤ j ≤ n rij ≥ and 
pm 
i rij ≤ let ω denote the 
set of all the allocation schemes 
we consider the price anticipating mechanism in which 
each user places a bid to each machine and the price of the 
machine is determined by the total bids placed formally 
suppose that user i submits a non-negative bid xij to 
machine j the price of machine j is then set to yj 
pn 
i xij 
the total bids placed on the machine j consequently user i 
receives a fraction of rij 
xij 
yj 
of j when yj i e when 
there is no bid on a machine the machine is not allocated 
to anyone we call xi xi xin the bidding vector of 
user i 
the additional consideration we have is that each user i 
has a budget constraint xi therefore user i s total bids 
have to sum up to his budget i e 
pn 
j xij xi the 
budget constraints come from the fact that the users do not 
have infinite budget 
utility functions each user i s utility is represented 
by a function ui of the fraction ri rin the user receives 
from each machine given the problem domain we consider 
we assume that each user has different and relatively 
independent preferences for different machines therefore the 
basic utility function we consider is the linear utility 
function ui ri · · · rin wi ri · · · winrin where wij ≥ 
is user i s private preference also called his weight on 
machine j for example suppose machine has a faster cpu 
but less memory than machine and user runs cpu 
bounded applications while user runs memory bounded 
applications as a result w w and w w 
our definition of utility functions corresponds to the user 
having enough jobs or enough parallelism within jobs to 
utilize all the machines consequently the user s goal is to grab 
as much of a resource as possible we call this the infinite 
parallelism model in practice a user s application may have 
an inherent limit on parallelization e g some computations 
must be done sequentially or there may be a system limit 
 e g the application s data is being served from a file server 
with limited capacity to model this we also consider the 
more realistic finite parallelism model where the user s 
parallelism is bounded by ki and the user s utility ui is the 
sum of the ki largest wijrij in this model the user only 
submits bids to up to ki machines our abstraction is to 
capture the essense of the problem and facilitate our 
analysis in section we discuss the limit of the above definition 
of utility functions 
best response as typically we assume the users are 
selfish and strategic - they all act to maximize their own 
utility defined by their utility functions from the 
perspective of user i if the total bids of the other users placed on 
each machine j is yj then the best response of user i to the 
system is the solution of the following optimization problem 
maximize ui 
xij 
xij yj 
 subject to 
pn 
j xij xi and xij ≥ 
the difficulty of the above optimization problem depends 
on the formulation of ui we will show later how to solve 
it for the infinite parallelism model and provide a heuristic 
for finite parallelism model 
nash equilibrium by the assumption that the user is 
selfish each user s bidding vector is the best response to the 
system the question we are most interested in is whether 
there exists a collection of bidding vectors one for each user 
such that each user s bidding vector is the best response to 
those of the other users such a state is known as the nash 
equilibrium a central concept in game theory formally 
the bidding vectors x xm is a nash equilibrium if for 
 
any ≤ i ≤ m xi is the best response to the system or for 
any other bidding vector xi 
ui x xi xm ≥ ui x xi xm 
the nash equilibrium is desirable because it is a stable 
state at which no one has incentive to change his strategy 
but a game may not have an equilibrium indeed a nash 
equilibrium may not exist in the price anticipating scheme 
we define above this can be shown by a simple example of 
two players and two machines for example let u r r 
r and u r r r r then player should never bid 
on machine because it has no value to him now player 
has to put a positive bid on machine to claim the machine 
but there is no lower limit resulting in the non-existence of 
the nash equilibrium we should note that even the mixed 
strategy equilibrium does not exist in this example clearly 
this happens whenever there is a resource that is wanted 
by only one player to rule out this case we consider those 
strongly competitive games 
under the infinite parallelism 
model a game is called strongly competitive if for any ≤ 
j ≤ n there exists an i k such that wij wkj under 
such a condition we have that see for a proof 
theorem there always exists a pure strategy nash 
equilibrium in a strongly competitive game 
given the existence of the nash equilibrium the next 
important question is the performance at the nash equilibrium 
which is often measured by its efficiency and fairness 
efficiency price of anarchy for an allocation scheme 
ω ∈ ω denote by u ω 
p 
i ui ri the social welfare under 
ω let u∗ 
 maxω∈ω u ω denote the optimal social welfare 
- the maximum possible aggregated user utilities the 
efficiency at an allocation scheme ω is defined as π ω u ω 
u∗ 
let ω denote the set of the allocation at the nash 
equilibrium when there exists nash equilibrium i e ω ∅ 
define the efficiency of a game q to be π q minω∈ω π ω 
it is usually the case that π i e there is an efficiency 
loss at a nash equilibrium this is the price of anarchy 
paid for not having central enforcement of the user s good 
behavior this price is interesting because central control 
results in the best possible outcome but is not possible in 
most cases 
fairness while the definition of efficiency is standard 
there are multiple ways to define fairness we consider two 
metrics one is by comparing the users utilities the utility 
uniformity τ ω of an allocation scheme ω is defined to be 
mini ui ω 
maxi ui ω 
 the ratio of the minimum utility and the 
maximum utility among the users such definition or utility 
discrepancy defined similarly as maxi ui ω 
mini ui ω 
 is used 
extensively in computer science literature under this 
definition the utility uniformity τ q of a game q is defined to 
be τ q minω∈ω τ ω 
the other metric extensively studied in economics is the 
concept of envy-freeness unlike the utility uniformity 
metric the enviness concerns how the user perceives the 
value of the share assigned to him compared to the shares 
other users receive within such a framework define the 
envy-freeness of an allocation scheme ω by ρ ω mini j 
ui ri 
ui rj 
 
 alternatives include adding a reservation price or limiting the 
lowest allowable bid to each machine these alternatives 
however introduce the problem of coming up with the right price 
or limit 
when ρ ω ≥ the scheme is known as an envy-free 
allocation scheme likewise the envy-freeness ρ q of a game 
q is defined to be ρ q minω∈ω ρ ω 
 nash equilibrium 
in this section we present some theoretical results 
regarding the performance at nash equilibrium under the infinite 
parallelism model we assume that the game is strongly 
competitive to guarantee the existence of equilibria for a 
meaningful discussion of efficiency and fairness we assume 
that the users are symmetric by requiring that xi andpn 
j wij for all the ≤ i ≤ m or informally we 
require all the users have the same budget and they have 
the same utility when they own all the resources this 
precludes the case when a user has an extremely high budget 
resulting in very low efficiency or low fairness at equilibrium 
we first provide a characterization of the equilibria by 
definition the bidding vectors x xm is a nash 
equilibrium if and only if each player s strategy is the best 
response to the group s bids since ui is a linear function 
and the domain of each users bids { xi xin 
p 
j xij 
xi and xij ≥ } is a convex set the optimality condition is 
that there exists λi such that 
∂ui 
∂xij 
 wij 
yj − xij 
y 
j 
 
 λi if xij and 
 λi if xij 
 
or intuitively at an equilibrium each user has the same 
marginal value on machines where they place positive bids 
and has lower marginal values on those machines where they 
do not bid 
under the infinite parallelism model it is easy to compute 
the social optimum u∗ 
as it is achieved when we allocate 
each machine wholly to the person who has the maximum 
weight on the machine i e u∗ 
 
pn 
j max ≤i≤m wij 
 two-player games 
we first show that even in the simplest nontrivial case 
when there are two users and two machines the game has 
interesting properties we start with two special cases to 
provide some intuition about the game the weight 
matrices are shown in figure a and b which correspond 
respectively to the equal-weight and opposite-weight games 
let x and y denote the respective bids of users and on 
machine denote by s x y and δ − s s 
equal-weight game in figure both users have equal 
valuations for the two machines by the optimality 
condition for the bid vectors to be in equilibrium they need to 
satisfy the following equations according to 
α 
y 
 x y 
 − α 
 − y 
 − x − y 
α 
x 
 x y 
 − α 
 − x 
 − x − y 
by simplifying the above equations we obtain that δ 
 − α and x y α thus there exists a unique nash 
equilibrium of the game where the two users have the same 
bidding vector at the equilibrium the utility of each user 
is and the social welfare is on the other hand the 
social optimum is clearly thus the equal-weight game 
is ideal as the efficiency utility uniformity and the 
envyfreeness are all 
 
m m 
u α − α 
u α − α 
m m 
u α − α 
u − α α 
 a equal weight game b opposite weight game 
figure two special cases of two-player games 
opposite-weight game the situation is different for 
the opposite game in which the two users put the exact 
opposite weights on the two machines assume that α ≥ 
 similarly for the bid vectors to be at the equilibrium 
they need to satisfy 
α 
y 
 x y 
 − α 
 − y 
 − x − y 
 − α 
x 
 x y 
 α 
 − x 
 − x − y 
by simplifying the above equations we have that each 
nash equilibrium corresponds to a nonnegative root of the 
cubic equation f δ δ 
− cδ 
 cδ − where c 
 
 α −α 
− 
clearly δ is a root of f δ when δ we have 
that x α y − α which is the symmetric equilibrium 
that is consistent with our intuition - each user puts a 
bid proportional to his preference of the machine at this 
equilibrium u − α − α u∗ 
 α and u u∗ 
 
 α 
α 
 − which is minimized when α 
√ 
 
 
with the 
minimum value of 
√ 
 − ≈ however when α is 
large enough there exist two other roots corresponding to 
less intuitive asymmetric equilibria 
intuitively the asymmetric equilibrium arises when user 
values machine a lot but by placing even a relatively small 
bid on machine he can get most of the machine because 
user values machine very little and thus places an even 
smaller bid in this case user gets most of machine and 
almost half of machine 
the threshold is at when f i e when c 
 α −α 
 
 this solves to α 
√ 
 
 
≈ those asymmetric 
equilibria at δ are bad as they yield lower efficiency 
than the symmetric equilibrium let δ be the minimum 
root when α → c → ∞ and δ c o c → 
then x y → thus u → u∗ 
→ and u u∗ 
→ 
from the above simple game we already observe that the 
nash equilibrium may not be unique which is different from 
many congestion games in which the nash equilibrium is 
unique 
for the general two player game we can show that 
is actually the worst efficiency bound with a proof in 
further at the asymmetric equilibrium the utility 
uniformity approaches when α → this is the worst possible 
for two player games because as we show in section a 
user s utility at any nash equilibrium is at least m in the 
m-player game 
another consequence is that the two player game is 
always envy-free suppose that the two user s shares are r 
 r r n and r r r n respectively then 
u r u r u r r u because 
ri ri for all ≤ i ≤ n again by that u r ≥ 
we have that u r ≥ u r i e any equilibrium 
allocation is envy-free 
theorem for a two player game π q ≥ τ q ≥ 
 and ρ q all the bounds are tight in the worst case 
 multi-player game 
for large numbers of players the loss in social welfare 
can be unfortunately large the following example shows 
the worst case bound consider a system with m n 
 n 
players and n machines of the players there are n 
who 
have the same weights on all the machines i e n on each 
machine the other n players have weight each on a 
different machine and or a sufficiently small on all the 
other machines clearly u∗ 
 n the following allocation 
is an equilibrium the first n 
players evenly distribute their 
money among all the machines the other n player invest all 
of their money on their respective favorite machine hence 
the total money on each machine is n at this 
equilibrium each of the first n 
players receives 
n 
 n 
n 
 
n n 
on 
each machine resulting in a total utility of n 
· 
n n 
 
the other n players each receives 
n 
on their favorite 
machine resulting in a total utility of n · 
n 
 therefore 
the total utility of the equilibrium is while the social 
optimum is n θ 
√ 
m this bound is the worst possible 
what about the utility uniformity of the multi-player 
allocation game we next show that the utility uniformity of 
the m-player allocation game cannot exceed m 
let s sn be the current total bids on the n 
machines excluding user i user i can ensure a utility of m 
by distributing his budget proportionally to the current bids 
that is user i by bidding sij xi 
pn 
i si on machine j 
obtains a resource level of 
rij 
sij 
sij sj 
 
sj 
pn 
i si 
sj 
pn 
i si sj 
 
 
 
pn 
i si 
 
where 
pn 
j sj 
pm 
j xj − xi m − 
therefore rij 
 m− 
 
m 
 the total utility of user i 
is 
nx 
j 
rijwij m 
nx 
j 
wij m 
since each user s utility cannot exceed the minimal 
possible uniformity is m 
while the utility uniformity can be small the envy-freeness 
on the other hand is bounded by a constant of 
√ 
 − ≈ 
 as shown in to summarize we have that 
theorem for the m-player game q π q ω 
√ 
m 
τ q ≥ m and ρ q ≥ 
√ 
 − all of these bounds are 
tight in the worst case 
 algorithms 
in the previous section we present the performance bounds 
of the game under the infinite parallelism model however 
the more interesting questions in practice are how the 
equilibrium can be reached and what is the performance at the 
nash equilibrium for the typical distribution of utility 
functions in particular we would like to know if the intuitive 
strategy of each player constantly re-adjusting his bids 
according to the best response algorithm leads to the 
equilibrium to answer these questions we resort to simulations 
in this section we present the algorithms that we use to 
compute or approximate the best response and the social 
optimum in our experiments we consider both the infinite 
parallelism and finite parallelism model 
 
 infinite parallelism model 
as we mentioned before it is easy to compute the social 
optimum under the infinite parallelism model - we simply 
assign each machine to the user who likes it the most we 
now present the algorithm for computing the best response 
recall that for weights w wn total bids y yn and 
the budget x the best response is to solve the following 
optimization problem 
maximize u 
pn 
j wj 
xj 
xj yj 
subject to 
pn 
j xj x and xj ≥ 
to compute the best response we first sort 
wj 
yj 
in 
decreasing order without loss of generality suppose that 
w 
y 
≥ 
w 
y 
≥ 
wn 
yn 
 
suppose that x∗ 
 x∗ 
 x∗ 
n is the optimum solution 
we show that if x∗ 
i then for any j i x∗ 
j too 
suppose this were not true then 
∂u 
∂xj 
 x∗ 
 wj 
yj 
 x∗ 
j yj 
 wj 
yj 
y 
j 
 
wj 
yj 
≤ 
wi 
yi 
 
∂u 
∂xi 
 x∗ 
 
thus it contradicts with the optimality condition 
suppose that k max{i x∗ 
i } again by the optimality 
condition there exists λ such that wi 
yi 
 x∗ 
i yi λ for ≤ i ≤ k 
and x∗ 
i for i k equivalently we have that 
x∗ 
i 
r 
wiyi 
λ 
− yi for ≤ i ≤ k and x∗ 
i for i k 
replacing them in the equation 
pn 
i x∗ 
i x we can 
solve for λ 
 
pk 
i 
√ 
wiyi 
 x 
pk 
i yi thus 
x∗ 
i 
√ 
wiyi 
pk 
i 
√ 
wiyi 
 x 
kx 
i 
yi − yi 
the remaining question is how to determine k it is the 
largest value such that x∗ 
k thus we obtain the 
following algorithm to compute the best response of a user 
 sort the machines according to wi 
yi 
in decreasing order 
 compute the largest k such that 
√ 
wkyk 
pk 
i 
√ 
wiyi 
 x 
kx 
i 
yi − yk ≥ 
 set xj for j k and for ≤ j ≤ k set 
xj 
√ 
wjyj 
pk 
i 
√ 
wiyi 
 x 
kx 
i 
yi − yj 
the computational complexity of this algorithm is o n log n 
dominated by the sorting in practice the best response can 
be computed infrequently e g once a minute so for a 
typically powerful modern host this cost is negligible 
the best response algorithm must send and receive o n 
messages because each user must obtain the total bids from 
each host in practice this is more significant than the 
computational cost note that hosts only reveal to users the sum 
of the bids on them as a result hosts do not reveal the 
private preferences and even the individual bids of one user to 
another 
 finite parallelism model 
recall that in the finite parallelism model each user i only 
places bids on at most ki machines of course the infinite 
parallelism model is just a special case of finite parallelism 
model in which ki n for all the i s in the finite parallelism 
model computing the social optimum is no longer trivial 
due to bounded parallelism it can instead be computed by 
using the maximum matching algorithm 
consider the weighted complete bipartite graph g u × 
v where u {ui ≤ i ≤ m and ≤ ≤ ki} v 
{ n} with edge weight wij assigned to the edge ui vj 
a matching of g is a set of edges with disjoint nodes and 
the weight of a matching is the total weights of the edges in 
the matching as a result the following lemma holds 
lemma the social optimum is the same as the 
maximum weight matching of g 
thus we can use the maximum weight matching 
algorithm to compute the social optimum the maximum weight 
matching is a classical network problem and can be solved 
in polynomial time we choose to implement the 
hungarian algorithm because of its simplicity there 
may exist a more efficient algorithm for computing the 
maximum matching by exploiting the special structure of g this 
remains an interesting open question 
however we do not know an efficient algorithm to 
compute the best response under the finite parallelism model 
instead we provide the following local search heuristic 
suppose we again have n machines with weights w wn 
and total bids y yn let the user s budget be x and 
the parallelism bound be k our goal is to compute an 
allocation of x to up to k machines to maximize the user s 
utility 
for a subset of machines a denote by x a the best 
response on a without parallelism bound and by u a the 
utility obtained by the best response algorithm the local 
search works as follows 
 set a to be the k machines with the highest wi yi 
 compute u a by the infinite parallelism best response 
algorithm sec on a 
 for each i ∈ a and each j ∈ a repeat 
 let b a − {i} {j} compute u b 
 if u b u a let a ← b and goto 
 output x a 
intuitively by the local search heuristic we test if we can 
swap a machine in a for one not in a to improve the best 
response utility if yes we swap the machines and repeat the 
process otherwise we have reached a local maxima and 
output that value we suspect that the local maxima that 
this algorithm finds is also the global maximum with 
respect to an individual user and that this process stop after 
a few number of iterations but we are unable to establish 
it however in our simulations this algorithm quickly 
converges to a high ≥ efficiency 
 
 local greedy adjustment 
the above best response algorithms only work for the 
linear utility functions described earlier in practice 
utility functions may have more a complicated form or even 
worse a user may not have a formulation of his utility 
function we do assume that the user still has a way to measure 
his utility which is the minimum assumption necessary for 
any market-based resource allocation mechanism in these 
situations users can use a more general strategy the local 
greedy adjustment method which works as follows a user 
finds the two machines that provide him with the highest 
and lowest marginal utility he then moves a fixed small 
amount of money from the machine with low marginal 
utility to the machine with the higher one this strategy aims 
to adjust the bids so that the marginal values at each 
machine being bid on are the same this condition guarantees 
the allocation is the optimum when the utility function is 
concave the tradeoff for local greedy adjustment is that it 
takes longer to stabilize than best-response 
 simulation results 
while the analytic results provide us with worst-case 
analysis for the infinite parallelism model in this section we 
employ simulations to study the properties of the nash 
equilibria in more realistic scenarios and for the finite parallelism 
model first we determine whether the user bidding process 
converges and if so what the rate of convergence is 
second in cases of convergence we look at the performance at 
equilibrium using the efficiency and fairness metrics defined 
above 
iterative method in our simulations each user starts 
with an initial bid vector and then iteratively updates his 
bids until a convergence criterion described below is met 
the initial bid is set proportional to the user s weights on 
the machines we experiment with two update methods the 
best response methods as described in section and 
and the local greedy adjustment method as described in 
section 
convergence criteria convergence time measures how 
quickly the system reaches equilibrium it is particularly 
important in the highly dynamic environment of distributed 
shared clusters in which the system s conditions may change 
before reaching the equilibrium thus a high convergence 
rate may be more significant than the efficiency at the 
equilibrium 
there are several different criteria for convergence the 
strongest criterion is to require that there is only negligible 
change in the bids of each user the problem with this 
criterion is that it is too strict users may see negligible change 
in their utilities but according to this definition the 
system has not converged the less strict utility gap criterion 
requires there to be only negligible change in the users 
utility given users concern for utility this is a more natural 
definition indeed in practice the user is probably not 
willing to re-allocate their bids dramatically for a small utility 
gain therefore we use the utility gap criterion to measure 
convergence time for the best response update method i e 
we consider that the system has converged if the utility gap 
of each user is smaller than in our experiments 
however this criterion does not work for the local greedy 
adjustment method because users of that method will 
experience constant fluctuations in utility as they move money 
around for this method we use the marginal utility gap 
criterion we compare the highest and lowest utility 
margins on the machines if the difference is negligible then we 
consider the system to be converged 
in addition to convergence to the equilibrium we also 
consider the criterion from the system provider s view the 
social welfare stabilization criterion under this criterion 
a system has stabilized if the change in social welfare is 
≤ individual users utility may not have converged this 
criterion is useful to evaluate how quickly the system as a 
whole reaches a particular efficiency level 
user preferences we experiment with two models of 
user preferences random distribution and correlated 
distribution with random distribution users weights on the 
different machines are independently and identically 
distributed according the uniform distribution in practice 
users preferences are probably correlated based on factors 
like the hosts location and the types of applications that 
users run to capture these correlations we associate with 
each user and machine a resource profile vector where each 
dimension of the vector represents one resource e g cpu 
memory and network bandwidth for a user i with a 
profile pi pi pi pik represents user i s need for 
resource k for machine j with profile qj qj qj 
qjk represents machine j s strength with respect to resource 
k then wij is the dot product of user i s and machine 
j s resource profiles i e wij pi · qj 
p 
k pikqjk by 
using these profiles we compress the parameter space and 
introduce correlations between users and machines 
in the following simulations we fix the number of 
machines to and vary the number of users from to 
 but we only report the results for the range of − 
users since the results remain similar for a larger number of 
users sections and present the simulation results 
when we apply the infinite parallelism and finite parallelism 
models respectively if the system converges we report the 
number of iterations until convergence a convergence time 
of iterations indicates non-convergence in which case 
we report the efficiency and fairness values at the point we 
terminate the simulation 
 infinite parallelism 
in this section we apply the infinite parallelism model 
which assumes that users can use an unlimited number of 
machines we present the efficiency and fairness at the 
equilibrium compared to two baseline allocation methods 
social optimum and weight-proportional in which users 
distribute their bids proportionally to their weights on the 
machines which may seem a reasonable distribution method 
intuitively 
we present results for the two user preference models 
with uniform preferences users weights for the different 
machines are independently and identically distributed 
according to the uniform distribution u ∼ and are 
normalized thereafter in correlated preferences each user s 
and each machine s resource profile vector has three 
dimensions and their values are also taken from the uniform 
distribution u ∼ 
convergence time figure shows the convergence 
time efficiency and fairness of the infinite parallelism model 
under uniform left and correlated right preferences plots 
 a and b show the convergence and stabilization time 
of the best-response and local greedy adjustment methods 
 
 
 
 
 
 
 
convergencetime iterations 
number of users 
uniform preferences 
 a 
best-response 
greedy convergence 
greedy stabilization 
 
 
 
 
 
 
number of users 
correlated preferences 
 b 
best-response 
greedy convergence 
greedy stabilization 
 
 
 
 
 
 
 
efficiency 
number of users 
 c 
nash equilibrium 
weight-proportional 
social optimum 
 
 
 
 
 
 
 
number of users 
 d 
nash equilibrium 
weight-proportional 
social optimum 
 
 
 
 
 
 
 
 
 
 
 
 
utilityuniformity 
number of users 
 e 
nash equilibrium 
weight-proportional 
social optimum 
 
 
 
 
 
 
 
 
 
 
 
 
number of users 
 f 
nash equilibrium 
weight proportional 
social optimum 
 
 
 
 
 
 
 
envy-freeness 
number of users 
 g 
nash equilibrium 
weight proportional 
social optimum 
 
 
 
 
 
 
 
number of users 
 h 
nash equilibrium 
weight proportional 
social optimum 
figure efficiency utility uniformity enviness and convergence time as a function of the number of users under the 
infinite parallelism model with uniform and correlated preferences n 
 
 
 
 
 
 
 
 
efficiency 
iteration number 
best-response 
greedy 
figure efficiency level over time under the infinite 
parallelism model number of users n 
the best-response algorithm converges within a few number 
of iterations for any number of users in contrast the local 
greedy adjustment algorithm does not converge even within 
 iterations when the number of users is smaller than 
but does converge for a larger number of users we believe 
that for small numbers of users there are dependency cycles 
among the users that prevent the system from converging 
because one user s decisions affects another user whose 
decisions affect another user etc regardless the local greedy 
adjustment method stabilizes within iterations 
figure presents the efficiency over time for a system 
with users it demonstrates that while both adjustment 
methods reach the same social welfare the best-response 
algorithm is faster 
in the remainder of this paper we will refer to the nash 
equilibrium independent of the adjustment method used to 
reach it 
efficiency figure c and d present the efficiency as 
a function of the number of users we present the efficiency 
at equilibrium and use the social optimum and the 
weightproportional static allocation methods for comparison 
social optimum provides an efficient allocation by definition 
for both user preference models the efficiency at the 
equilibrium is approximately independent of the number of 
users which is only slightly worse than the social optimum 
the efficiency at the equilibrium is ≈ improvement over 
the weight-proportional allocation method for uniform 
preferences and ≈ improvement for correlated preferences 
fairness figure e and f present the utility 
uniformity as a function of the number of users and figures g 
and h present the envy-freeness while the social optimum 
yields perfect efficiency it has poor fairness the 
weightproportional method achieves the highest fairness among the 
three allocation methods but the fairness at the equilibrium 
is close 
the utility uniformity is slightly better at the equilibrium 
under uniform preferences than under correlated 
preferences since when users preferences are more 
aligned users happiness is more likely going to be at the 
expense of each other although utility uniformity decreases 
in the number of users it remains reasonable even for a large 
number of users and flattens out at some point at the 
social optimum utility uniformity can be infinitely poor as 
some users may be allocated no resources at all the same is 
true with respect to envy-freeness the difference between 
uniform and correlated preferences is best demonstrated in 
the social optimum results when the number of users is 
small it may be possible to satisfy all users to some extent 
if their preferences are not aligned but if they are aligned 
even with a very small number of users some users get no 
resources thus both utility uniformity and envy-freeness go 
to zero as the number of users increases it becomes almost 
impossible to satisfy all users independent of the existence 
of correlation 
these results demonstrate the tradeoff between the 
different allocation methods the efficiency at the equilibrium is 
lower than the social optimum but it performs much 
better with respect to fairness the equilibrium allocation is 
completely envy-free under uniform preferences and almost 
envy-free under correlated preferences 
 finite parallelism 
 
 
 
 
 
 
convergencetime iterations 
number of users 
 machines user 
 machines user 
figure convergence time under the finite parallelism 
model n 
 
 
 
 
 
 
efficiency 
iteration number 
 -machines user users 
 -machines user users 
figure efficiency level over time under the finite 
parallelism model with local search algorithm n 
we also consider the finite parallelism model and use the 
local search algorithm as described in section to 
adjust user s bids we again experimented with both the 
uniform and correlated preferences distributions and did not 
find significant differences in the results so we present the 
simulation results for only the uniform distribution 
in our experiments the local search algorithm stops quickly 
- it usually discovers a local maximum within two 
iterations as mentioned before we cannot prove that a local 
maximum is the global maximum but our experiments 
indicate that the local search heuristic leads to high efficiency 
 
convergence time let ∆ denote the parallelism bound 
that limits the maximum number of machines each user can 
bid on we experiment with ∆ and ∆ in both 
cases we use machines and vary the number of users 
figure shows that the system does not always converge 
but if it does the convergence happens quickly the 
nonconvergence occurs when the number of users is between 
and for ∆ between and for ∆ we 
believe that the non-convergence is caused by moderate 
competition no competition allows the system to equilibrate 
quickly because users do not have to change their bids in 
reaction to changes in others bids high competition also 
allows convergence because each user s decision has only a 
small impact on other users so the system is more stable 
and can gradually reach convergence however when there 
is moderate competition one user s decisions may cause 
dramatic changes in another s decisions and cause large 
fluctuations in bids in both cases of non-convergence the ratio of 
competitors per machine δ m×∆ n for m users and n 
machines is in the interval although the system does 
not converge in these bad ranges the system nontheless 
achieves and maintains a high level of overall efficiency after 
a few iterations as shown in figure 
performance in figure we present the efficiency 
utility uniformity and envy-freeness at the nash 
equilibrium for the finite parallelism model when the system does 
not converge we measure performance by taking the 
minimum value we observe after running for many iterations 
when ∆ there is a performance drop in particular 
with respect to the fairness metrics in the range between 
 and users where it does not converge for a larger 
number of users the system converges and achieves a lower 
level of utility uniformity but a high degree of efficiency and 
envy-freeness similar to those under the infinite parallelism 
model as described above this is due the competition ratio 
falling into the head-to-head range when the parallelism 
bound is large ∆ the performance is closer to the 
infinite parallelism model and we do not observe this drop 
in performance 
 related work 
there are two main groups of related work in resource 
allocation those that incorporate an economic mechanism 
and those that do not 
one non-economic approach is scheduling surveyed by 
pinedo examples of this approach are queuing in 
first-come first-served fcfs order queueing using the 
resource consumption of tasks e g and scheduling 
using combinatorial optimization these all assume that 
the values and resource consumption of tasks are reported 
accurately which does not apply in the presence of strategic 
users we view scheduling and resource allocation as two 
separate functions resource allocation divides a resource 
among different users while scheduling takes a given 
allocation and orders a user s jobs 
examples of the economic approach are spawn work 
by stoica et al the millennium resource allocator 
work by wellman et al bellagio and tycoon 
spawn and the work by wellman et al uses a reservation 
abstraction similar to the way airline seats are allocated 
unfortunately reservations have a high latency to acquire 
resources unlike the price-anticipating scheme we consider 
the tradeoff of the price-anticipating schemes is that users 
have uncertainty about exactly how much of the resources 
they will receive 
bellagio uses the share centralized allocator share 
allocates resources using a centralized combinatorial auction 
that allows users to express preferences with 
complementarities solving the np-complete combinatorial auction 
problem provides an optimally efficient allocation the 
priceanticipating scheme that we consider does not explicitly 
operate on complementarities thereby possibly losing some 
efficiency but it also avoids the complexity and overhead of 
combinatorial auctions 
there have been several analyses of 
variations of price-anticipating allocation schemes in the 
context of allocation of network capacity for flows their 
methodology follows the study of congestion potential games 
 by relating the nash equilibrium to the solution of a 
 usually convex global optimization problem but those 
techniques no longer apply to our game because we model 
users as having fixed budgets and private preferences for 
machines for example unlike those games there may 
exist multiple nash equilibria in our game milchtaich 
studied congestion games with private preferences but the 
technique in is specific to the congestion game 
 conclusions 
this work studies the performance of a market-based 
mechanism for distributed shared clusters using both analyatical 
and simulation methods we show that despite the worst 
case bounds the system can reach a high performance level 
at the nash equilibrium in terms of both efficiency and 
fairness metrics in addition with a few exceptions under 
the finite parallelism model the system reaches equilibrium 
quickly by using the best response algorithm and when the 
number of users is not too small by the greedy local 
adjustment method 
while our work indicates that the price-anticipating scheme 
may work well for resource allocation for shared clusters 
there are many interesting directions for future work one 
direction is to consider more realistic utility functions for 
example we assume that there is no parallelization cost and 
there is no performance degradation when multiple users 
share the same machine in practice both assumptions may 
not be correct for examples the user must copy code and 
data to a machine before running his application there and 
there is overhead for multiplexing resources on a single 
machine when the job size is large enough and the degree 
of multiplexing is sufficiently low we can probably ignore 
those effects but those costs should be taken into account 
for a more realistic modeling another assumption is that 
users have infinite work so the more resources they can 
acquire the better in practice users have finite work one 
approach to address this is to model the user s utility 
according to the time to finish a task rather than the amount 
of resources he receives 
another direction is to study the dynamic properties of 
the system when the users needs change over time 
according to some statistical model in addition to the usual 
questions concerning repeated games it would also be important 
to understand how users should allocate their budgets wisely 
over time to accomodate future needs 
 
 
 
 
 
 
 
 
number of users 
 a limit machines user 
efficiency 
utility uniformity 
envy-freeness 
 
 
 
 
 
 
 
number of users 
 b limit machines user 
efficiency 
utility uniformity 
envy-freeness 
figure efficiency utility uniformity and envy-freeness under the finite parallelism model n 
 acknowledgements 
we thank bernardo huberman lars rasmusson eytan 
adar and moshe babaioff for fruitful discussions we also 
thank the anonymous reviewers for their useful comments 
 references 
 http planet-lab org 
 a auyoung b n chun a c snoeren and a vahdat 
resource allocation in federated distributed computing 
infrastructures in proceedings of the st workshop on 
operating system and architectural support for the 
on-demand it infrastructure 
 b chun c ng j albrecht d c parkes and a vahdat 
computational resource exchanges for distributed 
resource allocation 
 b n chun and d e culler market-based proportional 
resource sharing for clusters technical report csd- 
university of california at berkeley computer science 
division january 
 m feldman k lai and l zhang a price-anticipating 
resource allocation mechanism for distributed shared 
clusters technical report arxiv 
http arxiv org abs cs dc 
 d ferguson y yemimi and c nikolaou microeconomic 
algorithms for load balancing in distributed computer 
systems in international conference on distributed 
computer systems pages - 
 i foster and c kesselman globus a metacomputing 
infrastructure toolkit the international journal of 
supercomputer applications and high performance 
computing - summer 
 m l fredman and r e tarjan fibonacci heaps and 
their uses in improved network optimization algorithms 
journal of the acm - 
 h n gabow data structures for weighted matching and 
nearest common ancestors with linking in proceedings of 
 st annual acm-siam symposium on discrete 
algorithms pages - 
 b hajek and s yang strategic buyers in a sum bid 
game for flat networks manuscript http 
 tesla csl uiuc edu  hajek papers hajekyang pdf 
 
 r johari and j n tsitsiklis efficiency loss in a network 
resource allocation game mathematics of operations 
research 
 f p kelly charging and rate control for elastic traffic 
european transactions on telecommunications - 
 
 f p kelly and a k maulloo rate control in 
communication networks shadow prices proportional 
fairness and stability operational research society 
 - 
 h w kuhn the hungarian method for the assignment 
problem naval res logis quart - 
 k lai l rasmusson s sorkin l zhang and b a 
huberman tycoon an implemention of a distributed 
market-based resource allocation system manuscript 
http www hpl hp com research tycoon papers and 
presentations 
 i milchtaich congestion games with player-specific 
payoff functions games and economic behavior 
 - 
 d monderer and l s sharpley potential games games 
and economic behavior - 
 c papadimitriou algorithms games and the internet in 
proceedings of rd stoc 
 c h papadimitriou and k steiglitz combinatorial 
optimization dover publications inc 
 m pinedo scheduling prentice hall 
 o regev and n nisan the popcorn market online 
markets for computational resources in proceedings of 
 st international conference on information and 
computation economies pages - 
 r w rosenthal a class of games possessing 
pure-strategy nash equilibria internation journal of 
game theory - 
 s sanghavi and b hajek optimal allocation of a 
divisible good to strategic buyers manuscript http 
 tesla csl uiuc edu  hajek papers optdivisible pdf 
 
 i stoica h abdel-wahab and a pothen a 
microeconomic scheduler for parallel computers in 
proceedings of the workshop on job scheduling strategies 
for parallel processing pages - april 
 h r varian equity envy and efficiency journal of 
economic theory - 
 c a waldspurger t hogg b a huberman j o 
kephart and s stornetta spawn a distributed 
computational economy ieee transactions on software 
engineering - february 
 m p wellman w e walsh p r wurman and j k 
mackie-mason auction protocols for decentralized 
scheduling games and economic behavior - 
 
 a wierman and m harchol-balter classifying scheduling 
policies with respect to unfairness in an m gi in 
proceedings of the acm sigmetrics conference 
on measurement and modeling of computer systems 
 l zhang on the efficiency and fairness of a fixed budget 
resource allocation game manuscript 
 
on the computational power of iterative auctions∗ 
 extended abstract 
liad blumrosen 
school of engineering and computer science 
the hebrew university of jerusalem 
jerusalem israel 
liad cs huji ac il 
noam nisan 
school of engineering and computer science 
the hebrew university of jerusalem 
jerusalem israel 
noam cs huji ac il 
abstract 
we embark on a systematic analysis of the power and 
limitations of iterative combinatorial auctions most existing 
iterative combinatorial auctions are based on repeatedly 
suggesting prices for bundles of items and querying the bidders 
for their demand under these prices we prove a large 
number of results showing the boundaries of what can be 
achieved by auctions of this kind we first focus on auctions 
that use a polynomial number of demand queries and then 
we analyze the power of different kinds of ascending-price 
auctions 
categories and subject descriptors 
f theory of computation analysis of algorithms 
and problem complexity j computer applications 
social and behavioral sciences-economics 
general terms 
algorithms economics theory 
 introduction 
combinatorial auctions have recently received a lot of 
attention in a combinatorial auction a set m of m 
nonidentical items are sold in a single auction to n competing 
bidders the bidders have preferences regarding the bundles 
of items that they may receive the preferences of bidder i 
are specified by a valuation function vi m 
→ r 
 where 
vi s denotes the value that bidder i attaches to winning the 
bundle of items s we assume free disposal i e that the 
vi s are monotone non-decreasing the usual goal of the 
auctioneer is to optimize the social welfare 
p 
i vi si where the 
allocation s sn must be a partition of the items 
applications include many complex resource allocation problems 
and in fact combinatorial auctions may be viewed as the 
common abstraction of many complex resource allocation 
problems combinatorial auctions face both economic and 
computational difficulties and are a central problem in the 
recently active border of economic theory and computer 
science a forthcoming book addresses many of the issues 
involved in the design and implementation of combinatorial 
auctions 
the design of a combinatorial auction involves many 
considerations in this paper we focus on just one central 
issue the communication between bidders and the allocation 
mechanism - preference elicitation transferring all 
information about bidders preferences requires an infeasible 
 exponential in m amount of communication thus 
direct revelation auctions in which bidders simply declare 
their preferences to the mechanism are only practical for 
very small auction sizes or for very limited families of bidder 
preferences we have therefore seen a multitude of suggested 
iterative auctions in which the auction protocol repeatedly 
interacts with the different bidders aiming to adaptively 
elicit enough information about the bidders preferences as 
to be able to find a good optimal or close to optimal 
allocation 
most of the suggested iterative auctions proceed by 
maintaining temporary prices for the bundles of items and 
repeatedly querying the bidders as to their preferences between the 
bundles under the current set of prices and then updating 
the set of bundle prices according to the replies received 
 e g effectively such an iterative 
auction accesses the bidders preferences by repeatedly making 
the following type of demand query to bidders query to 
bidder i a vector of bundle prices p {p s }s⊆m answer 
a bundle of items s ⊆ m that maximizes vi s − p s 
these types of queries are very natural in an economic 
setting as they capture the revealed preferences of the 
bidders some auctions called item-price or linear-price 
auctions specify a price pi for each item and the price of any 
given bundle s is always linear p s 
p 
i∈s pi other 
auctions called bundle-price auctions allow specifying 
arbitrary non-linear prices p s for bundles another 
important differentiation between models of iterative auctions is 
 
based on whether they use anonymous or non-anonymous 
prices in some auctions the prices that are presented to the 
bidders are always the same anonymous prices in other 
auctions non-anonymous different bidders may face 
different discriminatory vectors of prices in ascending-price 
auctions forcing prices to be anonymous may be a 
significant restriction 
in this paper we embark on a systematic analysis of the 
computational power of iterative auctions that are based 
on demand queries we do not aim to present auctions for 
practical use but rather to understand the limitations and 
possibilities of these kinds of auctions in the first part of 
this paper our main question is what can be done using a 
polynomial number of these types of queries that is 
polynomial in the main parameters of the problem n m and the 
number of bits t needed for representing a single value vi s 
note that from an algorithmic point of view we are talking 
about sub-linear time algorithms the input size here is 
really n m 
− numbers - the descriptions of the valuation 
functions of all bidders there are two aspects to 
computational efficiency in these settings the first is the 
communication with the bidders i e the number of queries made and 
the second is the usual computational tractability our 
lower bounds will depend only on the number of 
queriesand hold independently of any computational assumptions 
like p np our upper bounds will always be 
computationally efficient both in terms of the number of queries 
and in terms of regular computation as mentioned this 
paper concentrates on the single aspect of preference elicitation 
and on its computational consequences and does not address 
issues of incentives this strengthens our lower bounds but 
means that the upper bounds require evaluation from this 
perspective also before being used in any real combinatorial 
auction 
the second part of this paper studies the power of 
ascending -price auctions ascending auctions are iterative 
auctions where the published prices cannot decrease in time in 
this work we try to systematically analyze what do the 
differences between various models of ascending auctions mean 
we try to answer the following questions i which models 
of ascending auctions can find the optimal allocation and for 
which classes of valuations ii in cases where the optimal 
allocation cannot be determined by ascending auctions how 
well can such auctions approximate the social welfare iii 
how do the different models for ascending auctions compare 
are some models computationally stronger than others 
ascending auctions have been extensively studied in the 
literature see the recent survey by parkes most of this 
work presented upper bounds i e proposed mechanisms 
with ascending prices and analyzed their properties a 
result which is closer in spirit to ours is by gul and stacchetti 
 who showed that no item-price ascending auction can 
always determine the vcg prices even for substitutes 
valuations 
our framework is more general than the traditional 
line of research that concentrates on the final allocation and 
 
we do observe however that some weak incentive 
property comes for free in demand-query auctions since myopic 
players will answer all demand queries truthfully we also 
note that in some cases but not always the incentives 
issues can be handled orthogonally to the preference 
elicitation issues e g by using vickrey-clarke-groves vcg 
prices e g 
 
we further discuss this result in section 
iterative auctions 
demand auctions 
item-price 
auctions 
anonymous 
price auctions 
ascending 
auctions 
 
 
 
 
 
 
 
 
 
figure the diagram classifies the following auctions 
according to their properties 
 the adaptation for kelso crawford s 
auction 
 the proxy auction by ausubel milgrom 
 ibundle by parkes ungar 
 ibundle by parkes ungar 
 our descending adaptation for the -approximation 
for submodular valuations by see subsection 
 ausubel s auction for substitutes valuations 
 the adaptation by nisan segal of the o 
√ 
m 
approximation by 
 the duplicate-item auction by 
 auction for read-once formulae by 
 the akba auction by wurman wellman 
payments and in particular on reaching walrasian 
equilibria or competitive equilibria a walrasian equilibrium 
is known to exist in the case of substitutes valuations and 
is known to be impossible for any wider class of valuations 
 this does not rule out other allocations by ascending 
auctions in this paper we view the auctions as a 
computational process where the outcome - both the allocation 
and the payments - can be determined according to all the 
data elicited throughout the auction this general 
framework strengthens our negative results 
we find the study of ascending auctions appealing for 
various reasons first ascending auctions are widely used in 
many real-life settings from the fcc spectrum auctions 
to almost any e-commerce website e g actually 
this is maybe the most straightforward way to sell items ask 
the bidders what would they like to buy under certain prices 
and increase the prices of over-demanded goods ascending 
auctions are also considered more intuitive for many bidders 
and are believed to increase the trust of the bidders in the 
auctioneer as they see the result gradually emerging from 
the bidders responses ascending auctions also have other 
desired economic properties e g they incur smaller 
information revelation consider for example english auctions 
vs second-price sealed bid auctions 
 extant work 
many iterative combinatorial auction mechanisms rely on 
demand queries see the survey in figure 
summa 
a walrasian equilibrium is vector of item prices for which 
all the items are sold when each bidder receives a bundle in 
his demand set 
 
in few recent auction designs e g the payments 
are not necessarily the final prices of the auctions 
 
valuation family upper bound reference lower bound reference 
general min n o 
√ 
m section min n m − 
 
substitutes 
submodular 
 m 
 - 
e 
 
subadditive o logm 
k-duplicates o m k 
 o m k 
 
procurement ln m log m 
figure the best approximation factors currently achievable by computationally-efficient combinatorial auctions 
for several classes of valuations all lower bounds in the table apply to all iterative auctions except the one marked 
by all upper bounds in the table are achieved with item-price demand queries 
rizes the basic classes of auctions implied by combinations 
of the above properties and classifies some of the auctions 
proposed in the literature according to this classification 
for our purposes two families of these auctions serve as 
the main motivating starting points the first is the 
ascending item-price auctions of that with computational 
efficiency find an optimal allocation among gross 
substitutes valuations and the second is the ascending 
bundleprice auctions of that find an optimal allocation 
among general valuations - but not necessarily with 
computational efficiency the main lower bound in this area 
due to states that indeed due to inherent 
communication requirements it is not possible for any iterative auction 
to find the optimal allocation among general valuations with 
sub-exponentially many queries a similar exponential lower 
bound was shown in also for even approximating the 
optimal allocation to within a factor of m − 
 several lower 
bounds and upper bounds for approximation are known for 
some natural classes of valuations - these are summarized 
in figure 
in the universal generality of demand queries is also 
shown any non-deterministic communication protocol for 
finding an allocation that optimizes the social welfare can 
be converted into one that only uses demand queries with 
bundle prices in this was generalized also to 
nondeterministic protocols for finding allocations that satisfy 
other natural types of economic requirements e g 
approximate social efficiency envy-freeness however in it was 
demonstrated that this completeness of demand queries 
holds only in the nondeterministic setting while in the usual 
deterministic setting demand queries even with bundle 
prices may be exponentially weaker than general 
communication 
bundle-price auctions are a generalization of the more 
natural and intuitive item-price auctions it is known that 
indeed item-price auctions may be exponentially weaker a 
nice example is the case of valuations that are a xor of k 
bundles 
 where k is small say polynomial lahaie and 
parkes show an economically-efficient bundle-price 
auction that uses a polynomial number of queries whenever k is 
polynomial in contrast show that there exist valuations 
that are xors of k 
√ 
m bundles such that any item-price 
auction that finds an optimal allocation between them 
requires exponentially many queries these results are part 
of a recent line of research that study the 
preference elicitation problem in combinatorial auctions 
and its relation to the full elicitation problem i e 
learn 
these are valuations where bidders have values for k 
specific packages and the value of each bundle is the maximal 
value of one of these packages that it contains 
ing the exact valuations of the bidders these papers adapt 
methods from machine-learning theory to the 
combinatorialauction setting the preference elicitation problem and the 
full elicitation problem relate to a well studied problem in 
microeconomics known as the integrability problem see e g 
 this problem studies if and when one can derive the 
utility function of a consumer from her demand function 
paper organization due to the relatively large 
number of results we present we start with a survey of our new 
results in section after describing our formal model in 
section we present our results concerning the power of 
demand queries in section then we describe the power of 
item-price ascending auctions section and bundle-price 
ascending auctions section readers who are mainly 
interested in the self-contained discussion of ascending 
auctions can skip section 
missing proofs from section can be found in part i of 
the full paper missing proofs from sections and 
can be found in part ii of the full paper 
 a survey of our results 
our systematic analysis is composed of the combination 
of a rather large number of results characterizing the power 
and limitations of various classes of auctions in this section 
we will present an exposition describing our new results we 
first discuss the power of demand-query iterative auctions 
and then we turn our attention to ascending auctions 
figure summarizes some of our main results 
 demand queries 
comparison of query types 
we first ask what other natural types of queries could 
we imagine iterative auctions using here is a list of such 
queries that are either natural have been used in the 
literature or that we found useful 
 value query the auctioneer presents a bundle s the 
bidder reports his value v s for this bundle 
 marginal-value query the auctioneer presents a 
bundle a and an item j the bidder reports how much he 
is willing to pay for j given that he already owns a 
i e v j a v a ∪ {j} − v a 
 demand query with item prices the auctioneer 
presents a vector of item prices p pm the bidder reports 
his demand under these prices i e some set s that 
maximizes v s − 
p 
i∈s pi 
 
a tie breaking rule should be specified all of our results 
 
communication constraint 
can find an 
optimal allocation 
upper bound for 
welfare approx 
lower bound for 
welfare approx 
item-price demand queries yes 
poly communication no min n o m 
 min n m − 
 
poly item-price demand queries no min n o m 
 min n m − 
 
poly value queries no o m√ 
log m 
 o m 
log m 
 
anonymous item-price aa no - min o n o m − 
 
non-anonymous item-price aa no 
-anonymous bundle-price aa no - min o n o m − 
 
non-anonymous bundle-price aa yes 
poly number of item-price aa no min n o m 
 
 figure this paper studies the economic efficiency of auctions that follow certain communication constraints for 
each class of auctions the table shows whether the optimal allocation can be achieved or else how well can it be 
approximated both upper bounds and lower bounds new results are highlighted 
abbreviations poly polynomial number size aa ascending auctions - means that nothing is currently 
known except trivial solutions 
 indirect-utility query the auctioneer presents a set of 
item prices p pm and the bidder responds with his 
indirect-utility under these prices that is the 
highest utility he can achieve from a bundle under these 
prices maxs⊆m v s − 
p 
i∈s pi 
 relative-demand query the auctioneer presents a set 
of non-zero prices p pm and the bidder reports the 
bundle that maximizes his value per unit of money 
i e some set that maximizes v s p 
i∈s pi 
 
theorem each of these queries can be efficiently i e in 
time polynomial in n m and the number of bits of precision 
t needed to represent a single value vi s simulated by a 
sequence of demand queries with item prices 
in particular this shows that demand queries can elicit all 
information about a valuation by simulating all m 
− value 
queries we also observe that value queries and 
marginalvalue queries can simulate each other in polynomial time 
and that demand queries and indirect-utility queries can also 
simulate each other in polynomial time we prove that 
exponentially many value queries may be needed in order to 
simulate a single demand query it is interesting to note 
that for the restricted class of substitutes valuations 
demand queries may be simulated by polynomial number of 
value queries 
welfare approximation 
the next question that we ask is how well can a 
computationally-efficient auction that uses only demand queries 
approximate the optimal allocation two separate 
obstacles are known in a lower bound of min n m − 
 
for any fixed was shown for the approximation factor 
apply for any fixed tie breaking rule 
 
this is exactly the utility achieved by the bundle which 
would be returned in a demand query with the same prices 
this notion relates to the indirect-utility function studied 
in the microeconomic literature see e g 
 
note that when all the prices are the bidder actually 
reports the bundle with the highest per-item price we found 
this type of query useful for example in the design of the 
approximation algorithm described in figure in section 
 
obtained using any polynomial amount of communication 
a computational bound with the same value applies even for 
the case of single-minded bidders but under the assumption 
of np zpp as noted in the 
computationallyefficient greedy algorithm of can be adapted to become 
a polynomial-time iterative auction that achieves a nearly 
matching approximation factor of min n o 
√ 
m this 
iterative auction may be implemented with bundle-price 
demand queries but as far as we see not as one with item 
prices since in a single bundle-price demand query an 
exponential number of prices can be presented this algorithm 
can have an exponential communication cost in section 
 we describe a different item-price auction that achieves 
the same approximation factor with a polynomial number 
of queries and thus with a polynomial communication 
theorem there exists a computationally-efficient 
iterative auction with item-price demand queries that finds an 
allocation that approximates the optimal welfare between 
arbitrary valuations to within a factor of min n o 
√ 
m 
one may then attempt obtaining such an approximation 
factor using iterative auctions that use only the weaker value 
queries however we show that this is impossible 
theorem any iterative auction that uses a polynomial 
 in n and m number of value queries can not achieve an 
approximation factor that is better than o m 
log m 
 
note however that auctions with only value queries are not 
completely trivial in power the bundling auctions of 
holzman et al can easily be implemented by a polynomial 
number of value queries and can achieve an approximation 
factor of o m√ 
log m 
 by using o log m equi-sized bundles 
we do not know how to close the tiny gap between this 
upper bound and our lower bound 
representing bundle-prices 
we then deal with a critical issue with bundle-price 
auctions that was side-stepped by our model as well as by all 
previous works that used bundle-price auctions how are 
 
this was also proven independently by shahar dobzinski 
and michael schapira 
 
the bundle prices represented for item-price auctions this 
is not an issue since a query needs only to specify a small 
number m of prices in bundle-price auctions that 
situation is more difficult since there are exponentially many 
bundles that require pricing our basic model like all 
previous work that used bundle prices e g ignores 
this issue and only requires that the prices be determined 
somehow by the protocol a finer model would fix a 
specific language for denoting bundle prices force the protocol 
to represent the bundle-prices in this language and require 
that the representations of the bundle-prices also be 
polynomial 
what could such a language for denoting prices for all 
bundles look like first note that specifying a price for 
each bundle is equivalent to specifying a valuation second 
as noted in most of the proposed bidding languages 
are really just languages for representing valuations i e a 
syntactic representation of valuations - thus we could use 
any of them this point of view opens up the general issue 
of which language should be used in bundle-price auctions 
and what are the implications of this choice 
here we initiate this line of investigation we consider 
bundle-price auctions where the prices must be given as a 
xor-bid i e the protocol must explicitly indicate the price 
of every bundle whose value is different than that of all of 
its proper subsets note that all bundle-price auctions that 
do not explicitly specify a bidding language must implicitly 
use this language or a weaker one since without a specific 
language one would need to list prices for all bundles 
perhaps except for trivial ones those with value or more 
generally those with a value that is determined by one of 
their proper subsets we show that once the 
representation length of bundle prices is taken into account using the 
xor-language bundle-price auctions are no more strictly 
stronger than item-price auctions define the cost of an 
iterative auction as the total length of the queries and answers 
used throughout the auction in the worst case 
theorem for some class of valuations bundle price 
auctions that use the xor-language require an exponential cost 
for finding the optimal allocation in contrast item-price 
auctions can find the optimal allocation for this class within 
polynomial cost 
this put doubts on the applicability of bundle-price 
auctions like and it may justify the use of hybrid 
pricing methods such as ausubel cramton and milgrom s 
clock-proxy auction 
demand queries and linear programs 
the winner determination problem in combinatorial 
auctions may be formulated as an integer program in many 
cases solving the linear-program relaxation of this integer 
program is useful for some restricted classes of valuations 
it finds the optimum of the integer program e g substitute 
valuations or helps approximating the optimum 
 e g by randomized rounding however the linear 
program has an exponential number of variables nisan and 
segal observed the surprising fact that despite the 
ex 
our proof relies on the sophisticated known lower bounds 
for constant depth circuits we were not able to find an 
elementary proof 
ponential number of variables this linear program may be 
solved within polynomial communication the basic idea is 
to solve the dual program using the ellipsoid method see 
e g the dual program has a polynomial number of 
variables but an exponential number of constraints the 
ellipsoid algorithm runs in polynomial time even on such 
programs provided that a separation oracle is given for 
the set of constraints surprisingly such a separation oracle 
can be implemented using a single demand query with item 
prices to each of the bidders 
the treatment of was somewhat ad-hoc to the 
problem at hand the case of substitute valuations here we 
give a somewhat more general form of this important 
observation let us call the following class of linear programs 
generalized-winner-determination-relaxation gwdr 
lps 
maximize 
x 
i∈n s⊆m 
wi xi s vi s 
s t 
x 
i∈n s j∈s 
xi s ≤ qj ∀j ∈ m 
x 
s⊆m 
xi s ≤ di ∀i ∈ n 
xi s ≥ ∀i ∈ n s ⊆ m 
the case where wi di qj for every i j 
is the usual linear relaxation of the winner determination 
problem more generally wi may be viewed as the weight 
given to bidder i s welfare qj may be viewed as the quantity 
of units of good j and di may be viewed as duplicity of the 
number of bidders of type i 
theorem any gwdr linear program may be solved in 
polynomial time in n m and the number of bits of precision 
t using only demand queries with item prices 
 ascending auctions 
ascending item-price auctions 
it is well known that the item-price ascending auctions 
of kelso and crawford and its variants find 
the optimal allocation as long as all players valuations have 
the substitutes property the obvious question is whether 
the optimal allocation can be found for a larger class of 
valuations 
our main result here is a strong negative result 
theorem there is a -item -player problem where no 
ascending item-price auction can find the optimal allocation 
this is in contrast to both the power of bundle-price 
ascending auctions and to the power of general item-price 
demand queries see above both of which can always find the 
optimal allocation and in fact even provide full preference 
elicitation the same proof proves a similar impossibility 
result for other types of auctions e g descending auctions 
non-anonymous auctions more extension of this result 
 eliciting some classes of valuations requires an 
exponential number of ascending item-price trajectories 
 
the produced optimal solution will have polynomial 
support and thus can be listed fully 
 
 at least k − ascending item-price trajectories are 
needed to elicit xor formulae with k terms this 
result is in some sense tight since we show that any 
k-term xor formula can be fully elicited by k− 
nondeterministic i e when some exogenous teacher 
instructs the auctioneer on how to increase the prices 
ascending auctions 
we also show that item-price ascending auctions and 
iterative auctions that are limited to a polynomial number 
of queries of any kind not necessarily ascending are 
incomparable in their power ascending auctions with small 
enough increments can elicit the preferences in cases where 
any polynomial number of queries cannot 
motivated by several recent papers that studied the 
relation between eliciting and fully-eliciting the preferences in 
combinatorial auctions e g we explore the 
difference between these problems in the context of ascending 
auctions we show that although a single ascending auction can 
determine the optimal allocation among any number of 
bidders with substitutes valuations it cannot fully-elicit such a 
valuation even for a single bidder while it was shown in 
that the set of substitutes valuations has measure zero in the 
space of general valuations its dimension is not known and 
in particular it is still open whether a polynomial amount 
of information suffices to describe a substitutes valuation 
while our result may be a small step in that direction a 
polynomial full elicitation may still be possible with other 
communication protocols we note that our impossibility 
result also holds for valuations in the class oxs defined by 
 valuations that we are able to show have a compact 
representation 
we also give several results separating the power of 
different models for ascending combinatorial auctions that use 
item-prices we prove not surprisingly that adaptive 
ascending auctions are more powerful than oblivious 
ascending auctions and that non-deterministic ascending auctions 
are more powerful than deterministic ascending auctions 
we also compare different kinds of non-anonymous auctions 
 e g simultaneous or sequential and observe that 
anonymous bundle-price auctions and non-anonymous item-price 
auctions are incomparable in their power finally 
motivated by dutch auctions we consider descending auctions 
and how they compare to ascending ones we show classes of 
valuations that can be elicited by ascending item-price 
auctions but not by descending item-price auctions and vice 
versa 
ascending bundle-price auctions 
all known ascending bundle-price auctions that are able 
to find the optimal allocation between general valuations 
 with free disposal use non-anonymous prices 
anonymous ascending-price auctions e g are only 
known to be able to find the optimal allocation among 
superadditive valuations or few other simple classes we 
show that this is no mistake 
theorem no ascending auction with anonymous prices 
can find the optimal allocation between general valuations 
 
non-deterministic computation is widely used in cs and 
also in economics e g a walrasian equilibrium or in 
some settings deterministic and non-deterministic models 
have equal power e g computation with finite automata 
this bound is regardless of the running time and it also 
holds for descending auctions and non-deterministic 
auctions 
we strengthen this result significantly by showing that 
anonymous ascending auctions cannot produce a better than 
o 
√ 
m approximation - the approximation ratio that can 
be achieved with a polynomial number of queries 
and as mentioned with a polynomial number of item-price 
demand queries the same lower bound clearly holds for 
anonymous item-price ascending auctions since such 
auctions can be simulated by anonymous bundle-price 
ascending auctions we currently do not have any lower bound on 
the approximation achievable by non-anonymous item-price 
ascending auctions 
finally we study the performance of the existing 
computationally-efficient ascending auctions these protocols 
 require exponential time in the worst case and this is 
unavoidable as shown by however we also observe that 
these auctions as well as the whole class of similar 
ascending bundle-price auctions require an exponential time even 
for simple additive valuations this is avoidable and indeed 
the ascending item-price auctions of can find the 
optimal allocation for these simple valuations with polynomial 
communication 
 the model 
 discrete auctions for continuous values 
our model aims to capture iterative auctions that operate 
on real-valued valuations there is a slight technical 
difficulty here in bridging the gap between the discrete nature 
of an iterative auction and the continuous nature of the 
valuations this is exactly the same problem as in modeling 
a simple english auction there are three standard formal 
ways to model it 
 model the auction as a continuous process and study 
its trajectory in time for example the so-called japanese 
auction is basically a continuous model of an english 
model 
 model the auction as discrete and the valuations as 
continuously valued in this case we introduce a 
parameter and usually require the auction to produce 
results that are -close to optimal 
 model the valuations as discrete in this case we will 
assume that all valuations are integer multiples of some 
small fixed quantity δ e g penny all 
communication in this case is then naturally finite 
in this paper we use the latter formulation and assume 
that all values are multiples of some δ thus in some parts 
of the paper we assume without loss of generality that δ 
hence all valuations are integral almost all if not all of 
our results can be translated to the other two models with 
little effort 
 valuations 
a single auctioneer is selling m indivisible 
non-homogeneous items in a single auction and let m be the set of these 
 
another similar model is the moving knives model in the 
cake-cutting literature 
 
items and n be the set of bidders each one of the n 
bidders in the auction has a valuation function vi m 
→ 
{ δ δ l} where for every bundle of items s ⊆ m 
vi s denotes the value of bidder i for the bundle s and is 
a multiple of δ in the range l we will sometimes 
denote the number of bits needed to represent such values in 
the range l by t log l we assume free disposal i e 
s ⊆ t implies vi s ≤ vi t and that vi ∅ for all 
bidders 
we will mention the following classes of valuations 
 a valuation is called sub-modular if for all sets of items 
a and b we have that v a ∪ b v a ∩ b ≤ v a 
v b 
 a valuation is called super-additive if for all disjoint 
sets of items a and b we have that v a∪b ≥ v a 
v b 
 a valuation is called a k-bundle xor if it can be 
represented as a xor combination of at most k atomic 
bids i e if there are at most k bundles si and 
prices pi such that for all s v s maxi s⊇si 
pi such 
valuations will be denoted by v s p ⊕ s 
p ⊕ ⊕ sk pk 
 iterative auctions 
the auctioneer sets up a protocol equivalently an 
algorithm where at each stage of the protocol some 
information q - termed the query - is sent to some bidder i 
and then bidder i replies with some reply that depends on 
the query as well as on his own valuation in this paper 
we assume that we have complete control over the bidders 
behavior and thus the protocol also defines a reply function 
ri q vi that specifies bidder i s reply to query q the 
protocol may be adaptive the query value as well as the queried 
bidder may depend on the replies received for past queries 
at the end of the protocol an allocation s sn must be 
declared where si ∩ sj ∅ for i j 
we say that the auction finds an optimal allocation if 
it finds the allocation that maximizes the social welfarep 
i vi si we say that it finds a c-approximation if 
p 
i vi si 
≥ 
p 
i vi ti c where t tn is an optimal allocation the 
running time of the auction on a given instance of the 
bidders valuations is the total number of queries made on this 
instance the running time of a protocol is the worst case 
cost over all instances note that we impose no 
computational limitations on the protocol or on the players 
this 
of course only strengthens our hardness results yet our 
positive results will not use this power and will be efficient 
also in the usual computational sense 
our goal will be to design computationally-efficient 
protocols we will deem a protocol computationally-efficient if 
its cost is polynomial in the relevant parameters the 
number of bidders n the number of items m and t log l 
where l is the largest possible value of a bundle however 
when we discuss ascending-price auctions and their variants 
a computationally-efficient protocol will be required to be 
 
for example v abcd ⊕ ab ⊕ c denotes the 
xor valuation with the terms abcd ab c and prices 
respectively for this valuation v abcd v abd 
v abc 
 
the running time really measures communication costs and 
not computational running time 
pseudo-polynomial i e it should ask a number of queries 
which is polynomial in m n and l 
δ 
 this is because that 
ascending auctions can usually not achieve such running times 
 consider even the english auction on a single item 
note 
that all of our results give concrete bounds where the 
dependence on the parameters is given explicitly we use the 
standard big-oh notation just as a shorthand 
we say than an auction elicits some class v of valuations 
if it determines the optimal allocation for any profile of 
valuations drawn from v we say that an auction fully elicits 
some class of valuations v if it can fully learn any single 
valuation v ∈ v i e learn v s for every s 
 demand queries and ascending auctions 
most of the paper will be concerned with a common 
special case of iterative auctions that we term demand 
auctions in such auctions the queries that are sent to bidders 
are demand queries the query specifies a price p s ∈ 
for each bundle s the reply of bidder i is simply the set 
most desired - demanded - under these prices formally 
a set s that maximizes vi s − p s it may happen that 
more than one set s maximizes this value in which case 
ties are broken according to some fixed tie-breaking rule 
e g the lexicographically first such set is returned all of 
our results hold for any fixed tie-breaking rule 
ascending auctions are iterative auctions with 
non-decreasing prices 
definition in an ascending auction the prices in the 
queries to the same bidder can only increase in time 
formally let p be a query made for bidder i and q be a query 
made for bidder i at a later stage in the protocol then for 
all sets s q s ≥ p s a similar variant which we also 
study and that is also common in real life is descending 
auctions in which prices can only decrease in time 
note that the term ascending auction refers to an 
auction with a single ascending trajectory of prices it may 
be useful to define multi-trajectory ascending auctions in 
which the prices maybe reset to zero a number of times see 
e g 
we consider two main restrictions on the types of allowed 
demand queries 
definition item prices the prices in each query 
are given by prices pj for each item j the price of a set s 
is additive p s 
p 
j∈s pj 
definition anonymous prices the prices seen by 
the bidders at any stage in the auction are the same i e 
whenever a query is made to some bidder the same query is 
also made to all other bidders with the prices unchanged 
in auctions with non-anonymous discriminatory prices 
each bidder i has personalized prices denoted by pi 
 s 
in this paper all auctions are anonymous unless otherwise 
specified 
note that even though in our model valuations are integral 
 or multiples of some δ we allow the demand query to 
 
most of the auctions we present may be adapted to run in 
time polynomial in log l using a binary-search-like 
procedure losing their ascending nature 
 
note that a non-anonymous auction can clearly be 
simulated by n parallel anonymous auctions 
 
use arbitrary real numbers in that is we assume that 
the increment we use in the ascending auctions may be 
significantly smaller than δ all our hardness results hold 
for any even for continuous price increments a practical 
issue here is how will the query be specified in the general 
case an exponential number of prices needs to be sent in a 
single query formally this is not a problem as the model 
does not limit the length of queries in any way - the protocol 
must simply define what the prices are in terms of the replies 
received for previous queries we look into this issue further 
in section 
 the power of demand queries 
in this section we study the power of iterative auctions 
that use demand queries not necessarily ascending we 
start by comapring demand queries to other types of queries 
then we discuss how well can one approximate the optimal 
welfare using a polynomial number of demand queries we 
also initiate the study of the representation of bundle-price 
demand queries and finally we show how demand queries 
help solving the linear-programming relaxation of 
combinatorial auctions in polynomial time 
 the power of different types of queries 
in this section we compare the power of the various types 
of queries defined in section we will present 
computationally -efficient simulations of these query types using 
item-price demand queries in section we show that 
these simulations can also be done using item-price 
ascending auctions 
lemma a value query can be simulated by m 
marginalvalue queries a marginal-value query can be simulated by 
two value queries 
lemma a value query can be simulated by mt 
demand queries where t log l is the number of bits needed 
to represent a single bundle value 
as a direct corollary we get that demand auctions can 
always fully elicit the bidders valuations by simulating all 
possible m 
− queries and thus elicit enough information 
for determining the optimal allocation note however that 
this elicitation may be computationally inefficient 
the next lemma shows that demand queries can be 
exponentially more powerful than value queries 
lemma an exponential number of value queries may 
be required for simulating a single demand query 
indirect utility queries are however equivalent in power 
to demand queries 
lemma an indirect-utility query can be simulated by 
mt demand queries a demand query can be simulated 
by m indirect-utility queries 
demand queries can also simulate relative-demand queries 
 
note that t bundle-price demand queries can easily 
simulate a value query by setting the prices of all the bundles 
except s the bundle with the unknown value to be l and 
performing a binary search on the price of s 
 
note although in our model values are integral our 
multiples of δ we allow the query prices to be arbitrary real 
numv mv d iu rd 
v exp exp exp 
mv m exp exp exp 
d mt poly mt poly 
iu m poly 
rd - - - - 
figure each entry in the table specifies how many 
queries of this row are needed to simulate a query from 
the relevant column 
abbreviations v value query mv marginal-value 
query d demand query iu indirect-utility query 
rd relative demand query 
lemma relative-demand queries can be simulated by 
a polynomial number of demand queries 
according to our definition of relative-demand queries 
they clearly cannot simulate even value queries figure 
summarizes the relations between these query types 
 approximating the social welfare with 
value and demand queries 
we know from that iterative combinatorial auctions 
that only use a polynomial number of queries can not find 
an optimal allocation among general valuations and in fact 
can not even approximate it to within a factor better than 
min{n m − 
} in this section we ask how well can this 
approximation be done using demand queries with item prices 
or using the weaker value queries we show that using 
demand queries the lower bound can be matched while value 
queries can only do much worse 
figure describes a polynomial-time algorithm that achieves 
a min n o 
√ 
m approximation ratio this algorithm 
greedily picks the bundles that maximize the bidders per-item 
value using relative-demand queries see section as 
a final step it allocates all the items to a single bidder if 
it improves the social welfare this can be checked using 
value queries since both value queries and relative-demand 
queries can be simulated by a polynomial number of 
demand queries with item prices lemmas and this 
algorithm can be implemented by a polynomial number of 
demand queries with item prices 
theorem the auction described in figure can be 
implemented by a polynomial number of demand queries and 
achieves a min{n 
√ 
m}-approximation for the social 
welfare 
we now ask how well can the optimal welfare be 
approximated by a polynomial number of value queries first we 
note that value queries are not completely powerless in 
 it is shown that if the m items are split into k fixed 
bundles of size m k each and these fixed bundles are 
auctioned as though each was indivisible then the social welfare 
bers thus we may have bundles with arbitrarily close 
relative demands in this sense the simulation above is only up 
to any given and the number of queries is o log l log 
 
when the relative-demand query prices are given as rational 
numbers exact simulation is implied when log is linear in 
the input length 
 
in the full paper we observe that this algorithm can be 
implemented by two descending item-price auctions where 
we allow removing items along the auction 
 
generated by such an auction is at least m√ 
k 
-approximation 
of that possible in the original auction notice that such 
an auction can be implemented by k 
− value queries to 
each bidder - querying the value of each bundle of the fixed 
bundles thus if we choose k log m bundles we get an 
m√ 
log m 
-approximation while still using a polynomial number 
of queries 
the following lemma shows that not much more is possible 
using value queries 
lemma any iterative auction that uses only value 
queries and distinguishes between k-tuples of valuations 
where the optimal allocation has value and those where the 
optimal allocation has value k requires at least 
m 
k queries 
proof consider the following family of valuations for 
every s such that s m v s and there exists a 
single set t such that for s ≤ m v s iff t ⊆ s 
and v s otherwise now look at the behavior of the 
protocol when all valuations vi have t { m} clearly 
in this case the value of the best allocation is since no set 
of size m 
 
or lower has non-zero value for any player fix the 
sequence of queries and answers received on this k-tuple of 
valuations 
now consider the k-tuple of valuations chosen at random 
as follows a partition of the m items into k sets t tk 
each of size m 
k 
each is chosen uniformly at random among 
all such partitions now consider the k-tuple of valuations 
from our family that correspond to this partition - clearly 
ti can be allocated to i for each i getting a total value of k 
now look at the protocol when running on these valuations 
and compare its behavior to the original case note that the 
answer to a query s to player i differs between the case of 
ti and the original case of t { m} only if s ≤ m 
 
and 
ti ⊆ s since ti is distributed uniformly among all sets of 
size exactly m 
k 
 we have that for any fixed query s to player 
i where s ≤ m 
 
 
pr ti ⊆ s ≤ 
„ 
 s 
m 
 ti 
≤ − m 
k 
using the union-bound if the original sequence of queries 
was of length less than 
m 
k then with positive probability 
none of the queries in the sequence would receive a different 
answer than for the original input tuple this is forbidden 
since the protocol must distinguish between this case and 
the original case - which cannot happen if all queries receive 
the same answer hence there must have been at least 
m 
k 
queries for the original tuple of valuations 
we conclude that a polynomial time protocol that uses 
only value queries cannot obtain a better than o m 
log m 
 
approximation of the welfare 
theorem an iterative auction that uses a 
polynomial number of value queries cannot achieve better than 
o m 
log m 
 -approximation for the social welfare 
proof immediate from lemma achieving any 
approximation ratio k which is asymptotically greater than 
m 
log m 
needs an exponential number of value queries 
an approximation algorithm 
initialization let t ← m be the current items for sale 
let k ← n be the currently participating bidders 
let s∗ 
 ← ∅ s∗ 
n ← ∅ be the provisional allocation 
repeat until t ∅ or k ∅ 
ask each bidder i in k for the bundle si that maximizes 
her per-item value i e si ∈ argmaxs⊆t 
vi s 
 s 
 
let i be the bidder with the maximal per-item value 
i e i ∈ argmaxi∈k 
vi si 
 si 
 
set s∗ 
i si k k \ i m m \ si 
finally ask the bidders for their values vi m for the 
grand bundle if allocating all the items to some bidder i 
improves the social welfare achieved so far 
 i e ∃i ∈ n such that vi m 
p 
i∈n vi s∗ 
i 
then allocate all items to this bidder i 
figure this algorithm achieves a min{n 
√ 
 
m}approximation for the social welfare which is 
asymptotically the best worst-case approximation possible with 
polynomial communication this algorithm can be 
implemented with a polynomial number of demand queries 
 the representation of bundle prices 
in this section we explicitly fix the language in which 
bundle prices are presented to the bidders in bundle-price 
auctions this language requires the algorithm to explicitly list 
the price of every bundle with a non-trivial price 
trivial in this context is a price that is equal to that of one of 
its proper subsets which was listed explicitly this 
representation is equivalent to the xor-language for expressing 
valuations formally each query q is given by an 
expression q s p ⊕ s p ⊕ ⊕ sl pl in this 
representation the price demanded for every set s is simply 
p s max{k l sk⊆s}pk 
definition the length of the query q s p ⊕ 
 s p ⊕ ⊕ sl pl is l the cost of an algorithm is the 
sum of the lengths of the queries asked during the operation 
of the algorithm on the worst case input 
note that under this definition bundle-price auctions are 
not necessarily stronger than item-price auctions an 
itemprice query that prices each item for is translated to an 
exponentially long bundle-price query that needs to specify 
the price s for each bundle s but perhaps bundle-price 
auctions can still find optimal allocations whenever 
itemprice auction can without directly simulating such queries 
we show that this is not the case indeed when the 
representation length is taken into account bundle price auctions 
are sometimes seriously inferior to item price auctions 
consider the following family of valuations each item is 
valued at except that for some single set s its value is a 
bit more s b where b ∈ { } note that an item 
price auction can easily find the optimal allocation between 
any two such valuations set the prices of each item to 
if the demand sets of the two players are both empty then 
b for both valuations and an arbitrary allocation is fine 
if one of them is empty and the other non-empty allocate 
the non-empty demand set to its bidder and the rest to the 
other if both demand sets are non-empty then unless they 
form an exact partition we need to see which b is larger 
which we can do by increasing the price of a single item in 
each demand set 
 
we will show that any bundle-price auction that uses only 
the xor-language to describe bundle prices requires an 
exponential cost which includes the sum of all description 
lengths of prices to find an optimal allocation between any 
two such valuations 
lemma every bundle-price auction that uses 
xorexpressions to denote bundle prices requires ω 
√ 
m 
cost in 
order to find the optimal allocation among two valuations 
from the above family 
the complication in the proof stems from the fact that 
using xor-expressions the length of the price description 
depends on the number of bundles whose price is strictly 
larger than each of their subsets - this may be significantly 
smaller than the number of bundles that have a non-zero 
price the proof becomes easy if we require the protocol 
to explicitly name every bundle with non-zero price we 
do not know of any elementary proof for this lemma 
 although we believe that one can be found instead we 
reduce the problem to a well known lower bound in boolean 
circuit complexity stating that boolean circuits of depth 
 that compute the majority function on m variables require 
 ω 
√ 
m 
size 
 demand queries and linear programming 
consider the following linear-programming relaxation for 
the generalized winner-determination problem in 
combinatorial auctions the primal program 
maximize 
x 
i∈n s⊆m 
wi xi s vi s 
s t 
x 
i∈n s j∈s 
xi s ≤ qj ∀j ∈ m 
x 
s⊆m 
xi s ≤ di ∀i ∈ n 
xi s ≥ ∀i ∈ n s ⊆ m 
note that the primal program has an exponential number 
of variables yet we will be able to solve it in polynomial 
time using demand queries to the bidders the solution will 
have a polynomial size support non-zero values for xi s 
and thus we will be able to describe it in polynomial time 
here is its dual 
minimize 
x 
j∈m 
qjpj 
x 
i∈n 
diui 
s t ui 
x 
j∈s 
pj ≥ wivi s ∀i ∈ n s ⊆ m 
pi ≥ uj ≥ ∀i ∈ m j ∈ n 
notice that the dual problem has exactly n m variables 
but an exponential number of constraints thus the dual 
can be solved using the ellipsoid method in polynomial time 
- if a separation oracle can be implemented in polynomial 
time recall that a separation oracle when given a possible 
solution either confirms that it is a feasible solution or 
responds with a constraint that is violated by the possible 
solution 
we construct a separation oracle for solving the dual 
program using a single demand query to each of the bidders 
consider a possible solution u p for the dual program we 
can re-write the constraints of the dual program as 
ui wi ≥ vi s − 
x 
j∈s 
pj wi 
now a demand query to bidder i with prices pj wi reveals 
exactly the set s that maximizes the rhs of the previous 
inequality thus in order to check whether u p is 
feasible it suffices to query each bidder i for his demand 
di under the prices pj wi check only the n constraints 
ui 
p 
j∈di 
pj ≥ wivi di where vi di can be simulated 
using a polynomial sequence of demand queries as shown in 
lemma if none of these is violated then we are assured 
that u p is feasible otherwise we get a violated constraint 
what is left to be shown is how the primal program can 
be solved recall that the primal program has an 
exponential number of variables since the ellipsoid algorithm 
runs in polynomial time it encounters only a polynomial 
number of constraints during its operation clearly if all 
other constraints were removed from the dual program it 
would still have the same solution adding constraints can 
only decrease the space of feasible solutions now take the 
reduced dual where only the constraints encountered 
exist and look at its dual it will have the same solution as the 
original dual and hence of the original primal however look 
at the form of this dual of the reduced dual it is just a 
version of the primal program with a polynomial number of 
variables - those corresponding to constraints that remained 
in the reduced dual thus it can be solved in polynomial 
time and this solution clearly solves the original primal 
program setting all other variables to zero 
 item-price ascending auctions 
in this section we characterize the power of ascending 
item-price auctions we first show that this power is not 
trivial such auctions can in general elicit an exponential 
amount of information on the other hand we show that 
the optimal allocation cannot always be determined by a 
single ascending auction and in some cases nor by an 
exponential number of ascending-price trajectories finally 
we separate the power of different models of ascending 
auctions 
 the power of item-price ascending 
auctions 
we first show that if small enough increments are allowed 
a single ascending trajectory of item-prices can elicit 
preferences that cannot be elicited with polynomial 
communication as mentioned all our hardness results hold for any 
increment even infinitesimal 
theorem some classes of valuations can be elicited 
by item-price ascending auctions but cannot be elicited by a 
polynomial number of queries of any kind 
proof sketch consider two bidders with v s if 
 s n 
 
 v s if s n 
 
and every s such that s n 
 
has an unknown value from { } due to determining 
the optimal allocation here requires exponential 
communication in the worst case nevertheless we show see that 
an item-price ascending auction can do it as long as it can 
use exponentially small increments 
we now describe another positive result for the power of 
item-price ascending auctions in section we showed 
 
v ab v a v b 
bidder α ∈ β ∈ 
bidder 
figure no item-price ascending auctions can 
determine the optimal allocation for this class of valuations 
that a value query can be simulated with a truly 
polynomial number of item-price demand queries here we show 
that every value query can be simulated by a pseudo 
polynomial number of ascending item-price demand queries in 
the next subsection we show that we cannot always 
simulate even a pair of value queries using a single item-price 
ascending auction in the full paper part ii we show 
that we can simulate other types of queries using item-price 
ascending auctions 
proposition a value query can be simulated by an 
item-price ascending auction this simulation requires a 
polynomial number of queries 
actually the proof for proposition proves a stronger 
useful result regarding the information elicited by iterative 
auctions it says that in any iterative auction in which the 
changes of prices are small enough in each stage 
 pseudocontinuous auctions the value of all bundles demanded 
during the auction can be computed the basic idea is that 
when the bidder moves from demanding some bundle ti to 
demanding another bundle ti there is a point in which she 
is indifferent between these two bundles thus knowing the 
value of some demanded bundle e g the empty set enables 
computing the values of all other demanded bundles 
we say that an auction is pseudo-continuous if it only 
uses demand queries and in each step the price of at most 
one item is changed by for some ∈ δ with respect 
to the previous query 
proposition consider any pseudo-continuous 
auction not necessarily ascending in which bidder i demands 
the empty set at least once along the auction then the 
value of every bundle demanded by bidder i throughout the 
auction can be calculated at the end of the auction 
 limitations of item-price ascending 
auctions 
although we observed that demand queries can solve any 
combinatorial auction problem when the queries are 
restricted to be ascending some classes of valuations cannot 
be elicited nor fully-elicited an example for such class of 
valuations is given in figure 
theorem there are classes of valuations that 
cannot be elicited nor fully elicited by any item-price ascending 
auction 
proof let bidder have the valuation described in the 
first row of figure where α and β are unknown values in 
 first we prove that this class cannot be fully elicited 
by a single ascending auction specifically an ascending 
auction cannot reveal the values of both α and β 
as long as pa and pb are both below the bidder will 
always demand the whole bundle ab her utility from ab is 
strictly greater than the utility from either a or b separately 
for example we show that u ab u a 
u ab − pa pb − pa − pb 
 va a − pa − pb u a 
thus in order to gain any information about α or β the 
price of one of the items should become at least w l o g 
pa ≥ but then the bundle a will not be demanded by 
bidder throughout the auction thus no information at all 
will be gained about α 
now assume that bidder is known to have the valuation 
described in the second row of figure the optimal 
allocation depends on whether α is greater than β in bidder s 
valuation and we proved that an ascending auction cannot 
determine this 
the proof of the theorem above shows that for an 
unknown value to be revealed the price of one item should be 
greater than and the other price should be smaller than 
 therefore in a price-monotonic trajectory of prices only 
one of these values can be revealed an immediate 
conclusion is that this impossibility result also holds for item-price 
descending auctions since no such trajectory exists then 
the same conclusion even holds for non-deterministic 
itemprice auctions in which exogenous data tells us how to 
increase the prices also note that since the hardness stems 
from the impossibility to fully-elicit a valuation of a single 
bidder this result also holds for non-anonymous ascending 
item-price auctions 
 limitations of multi-trajectory 
ascending auctions 
according to theorem no ascending item-price 
auction can always elicit the preferences we prove a similar 
result for bundle prices in section but can two 
ascending trajectories do the job or a polynomial number of 
ascending trajectories we give negative answers for such 
suggestions 
we define a k-trajectory ascending auction as a 
demandquery iterative auction in which the demand queries can be 
partitioned to k sets of queries where the prices published 
in each set only increase in time note that we use a general 
definition it allows the trajectories to run in parallel or 
sequentially and to use information elicited in some 
trajectories for determining the future queries in other trajectories 
the power of multiple-trajectory auctions can be 
demonstrated by the negative result of gul and stacchetti who 
showed that even for an auction among substitutes 
valuations an anonymous ascending item-price auction cannot 
compute vcg prices for all players 
ausubel overcame 
this impossibility result and designed auctions that do 
compute vcg prices by organizing the auction as a sequence of 
n ascending auctions here we prove that one cannot 
elicit xor valuations with k terms by less than k − 
ascending trajectories on the other hand we show that an 
xor formula can be fully elicited by k− non-deterministic 
ascending auctions or by k− deterministic ascending 
auctions if the auctioneer knows the atomic bundles 
 
a recent unpublished paper by mishra and parkes 
extends this result and shows that non-anonymous prices with 
bundle-prices are necessary in order that an ascending 
auction will end up with a universal-competitive-equilibrium 
 that leads to vcg payments 
 
this result actually separates the power of deterministic 
 
proposition xor valuations with k terms cannot 
be elicited or fully elicited by any k- -trajectory 
itemprice ascending auction even when the atomic bundles are 
known to the elicitor however these valuations can be 
elicited and fully elicited by k- -trajectory 
non-deterministic non-anonymous item-price ascending auctions 
moreover an exponential number of trajectories is 
required for eliciting some classes of valuations 
proposition elicitation and full-elicitation of some 
classes of valuations cannot be done by any k-trajectory 
itemprice ascending auction where k o m 
 
proof sketch consider the following class of 
valuations for s m 
 
 v s and for s m 
 
 v s 
every bundle s of size m 
 
has some unknown value in 
we show that a single item-price ascending auction 
can reveal the value of at most one bundle of size n 
 
 and 
therefore an exponential number of ascending trajectories is 
needed in order to elicit such valuations 
we observe that the algorithm we presented in section 
 can be implemented by a polynomial number of 
ascending auctions each item-price demand query can be 
considered as a separate ascending auction and therefore a 
min n 
√ 
m -approximation can be achieved by a 
polynomial number of ascending auctions we do not currently 
have a better upper bound or any lower bound 
 separating the various models of 
ascending auctions 
various models for ascending auctions have been suggested 
in the literature in this section we compare the power of 
the different models as mentioned all auctions are 
considered anonymous and deterministic unless specified 
otherwise 
ascending vs descending auctions we begin the 
discussion of the relation between ascending auctions and 
descending auctions with an example the algorithm by 
lehmann lehmann and nisan can be implemented by 
a simple item-price descending auction see the full paper for 
details this algorithm guarantees at least half of the 
optimal efficiency for submodular valuations however we are 
not familiar with any ascending auction that guarantees a 
similar fraction of the efficiency this raises a more general 
question can ascending auctions solve any 
combinatorialauction problem that is solvable using a descending auction 
 and vice versa we give negative answers to these 
questions the idea behind the proofs is that the information 
that the auctioneer can get for free at the beginning of 
each type of auction is different 
and non-deterministic iterative auctions our proof shows 
that a non-deterministic iterative auction can elicit the 
kterm xor valuations with a polynomial number of demand 
queries and show that this elicitation must take an 
exponential number of demand queries 
 
in ascending auctions the auctioneer can reveal the most 
valuable bundle besides m before she starts raising the 
prices thus she can use this information for adaptively 
choose the subsequent queries in descending auctions one 
can easily find the bundle with the highest average per-item 
price keeping all other bundles with non-positive utilities 
and use this information in the adaptive price change 
proposition there are classes that cannot be 
elicited fully elicited by ascending item-price auctions but can 
be elicited resp fully elicited with a descending item-price 
auction 
proposition there are classes that cannot be 
elicited fully elicited by item-price descending auctions but can 
be elicited resp fully elicited by item-price ascending 
auctions 
deterministic vs non-deterministic auctions 
nondeterministic ascending auctions can be viewed as auctions 
where some benevolent teacher that has complete 
information guides the auctioneer on how she should raise the prices 
that is preference elicitation can be done by a 
non-deterministic ascending auction if there is some ascending trajectory 
that elicits enough information for determining the optimal 
allocation and verifying that it is indeed optimal we show 
that non-deterministic ascending auctions are more powerful 
than deterministic ascending auctions 
proposition some classes can be elicited fully 
elicited by an item-price non-deterministic ascending 
auction but cannot be elicited resp fully elicited by item-price 
deterministic ascending auctions 
anonymous vs non-anonymous auctions as will 
be shown in section the power of anonymous and 
nonanonymous bundle-price ascending auctions differs 
significantly here we show that a difference also exists for 
itemprice ascending auctions 
proposition some classes cannot be elicited by 
anonymous item-price ascending auctions but can be elicited 
by a non-anonymous item-price ascending auction 
sequential vs simultaneous auctions a 
non-anonymous auction is called simultaneous if at each stage the price 
of some item is raised by for every bidder the auctioneer 
can use the information gathered until each stage in all the 
personalized trajectories to determine the next queries 
a non-anonymous auction is called sequential if the 
auctioneer performs an auction for each bidder separately in 
sequential order the auctioneer can determine the next 
query based on the information gathered in the trajectories 
completed so far and on the history of the current trajectory 
proposition there are classes that cannot be 
elicited by simultaneous non-anonymous item-price ascending 
auctions but can be elicited by a sequential non-anonymous 
item-price ascending auction 
adaptive vs oblivious auctions if the auctioneer 
determines the queries regardless of the bidders responses i e 
the queries are predefined we say that the auction is 
oblivious otherwise the auction is adaptive we prove that an 
adaptive behaviour of the auctioneer may be beneficial 
proposition there are classes that cannot be 
elicited fully elicited using oblivious item-price ascending 
auctions but can be elicited resp fully elicited by an adaptive 
item-price ascending auction 
 
 preference elicitation vs full elicitation 
preference elicitation and full elicitation are closely 
related problems if full elicitation is easy e g in 
polynomial time then clearly elicitation is also easy by a 
nonanonymous auction simply by learning all the valuations 
separately 
 on the other hand there are examples where 
preference elicitation is considered easy but learning is 
hard typically elicitation requires smaller amount of 
information some examples can be found in 
the tatonnement algorithms by end up with 
the optimal allocation for substitutes valuations 
we prove 
that we cannot fully elicit substitutes valuations or even 
their sub-class of oxs valuations defined in even for a 
single bidder by an item-price ascending auction although 
the optimal allocation can be found by an ascending auction 
for any number of bidders 
theorem substitute valuations cannot be fully 
elicited by ascending item-price auctions moreover they 
cannot be fully elicited by any m 
 
ascending trajectories m 
whether substitutes valuations have a compact 
representation i e polynomial in the number of goods is an 
important open question as a step in this direction we show 
that its sub-class of oxs valuations does have a compact 
representation every oxs valuation can be represented by 
at most m 
values 
lemma any oxs valuation can be represented by 
no more than m 
values 
 bundle-price ascending 
auctions 
all the ascending auctions in the literature that are proved 
to find the optimal allocation for unrestricted valuations are 
non-anonymous bundle-price auctions ibundle by parkes 
and ungar and the proxy auction by ausubel and 
milgrom yet several anonymous ascending auctions 
have been suggested e g akba and ibundle 
 in this section we prove that anonymous bundle-price 
ascending auctions achieve poor results in the worst-case 
we also show that the family of non-anonymous 
bundleprice ascending auctions can run exponentially slower than 
simple item-price ascending auctions 
 limitations of anonymous bundle-price 
ascending auctions 
we present a class of valuations that cannot be elicited 
by anonymous bundle-price ascending auctions these 
valuations are described in figure the basic idea for 
determining some unknown value of one bidder we must raise 
 
note that an anonymous ascending auction cannot 
necessarily elicit a class that can be fully elicited by an ascending 
auction 
 
substitute valuations are defined e g in roughly 
speaking a bidder with a substitute valuation will continue 
demand a certain item after the price of some other items 
was increased for completeness we present in the full paper 
 a proof for the efficiency of such auctions for substitutes 
valuations 
 
a unit-demand valuation is an xor valuation in which 
all the atomic bundles are singletons oxs valuations can 
be interpreted as an aggregation or of any number of 
unit-demand bidders 
bid v ac v bd v cd α ∈ 
bid v ab v cd v bd β ∈ 
figure anonymous ascending bundle-price auctions 
cannot determine the optimal allocation for this class of 
valuations 
a price of a bundle that should be demanded by the other 
bidder in the future 
theorem some classes of valuations cannot be 
elicited by anonymous bundle-price ascending auctions 
proof consider a pair of xor valuations as described 
in figure for finding the optimal allocation we must know 
which value is greater between α and β 
however we 
cannot learn the value of both α and β by a single ascending 
trajectory assume w l o g that bidder demands cd before 
bidder demands bd no information will be elicited if none 
of these happens in this case the price for bd must be 
greater than otherwise bidder prefers bd to cd thus 
bidder will never demand the bundle bd and no 
information will be elicited about β 
the valuations described in the proof of theorem can 
be easily elicited by a non-anonymous item-price ascending 
auction on the other hand the valuations in figure can 
be easily elicited by an anonymous bundle-price ascending 
auction we conclude that the power of these two families 
of ascending auctions is incomparable 
we strengthen the impossibility result above by showing 
that anonymous bundle-price auctions cannot even achieve 
better than a min{o n o 
√ 
m }-approximation for the 
social welfare this approximation ratio can be achieved with 
polynomial communication and specifically with a 
polynomial number of item-price demand queries 
theorem an anonymous bundle-price ascending 
auction cannot guarantee better than a min{ n 
 
 
√ 
m 
 
} 
approximation for the optimal welfare 
proof sketch assume we have n bidders and n 
items 
for sale and that n is prime we construct n 
distinct 
bundles with the following properties for each bidder we define 
a partition si 
 si 
 si 
n of the n 
items to n bundles 
such that any two bundles from different partitions intersect 
in the full paper part ii we show an explicit construction 
using the properties of linear functions over finite fields the 
rest of the proof is independent of the specific construction 
using these n 
bundles we construct a hard-to-elicit 
class every bidder has an atomic bid in his xor valuation 
for each of these n 
bundles a bidder i has a value of for 
any bundle si 
j in his partition for all bundles in the other 
partitions he has a value of either or of − δ and these 
values are unknown to the auctioneer since every pair of 
bundles from different partitions intersect only one bidder 
can receive a bundle with a value of 
 
if α β the optimal allocation will allocate cd to bidder 
 and ab to bidder otherwise we give bd to bidder and 
ac to bidder note that both bidders cannot gain a value 
of in the same allocation due to the intersections of the 
high-valued bundles 
 
note that bundle-price queries may use exponential 
communication thus the lower bound of does not hold 
 
non-anonymous bundle-price economically-efficient 
ascending auctions 
initialization all prices are initialized to zero 
 non-anonymous bundle prices 
repeat - each bidder submits a bundle that maximizes his 
utility under his current personalized prices 
- the auctioneer calculates a provisional allocation that 
maximizes his revenue under the current prices 
- the prices of bundles that were demanded by losing 
bidders are increased by 
finally terminate when the provisional allocation assigns 
to each bidder the bundle he demanded 
figure auctions from this family denoted by nbea 
auctions are known to achieve the optimal welfare 
no bidder will demand a low-valued bundle as long as the 
price of one of his high-valued bundles is below and thus 
gain him a utility greater than therefore for eliciting 
any information about the low-valued bundles the 
auctioneer should first arbitrarily choose a bidder w l o g bidder 
 and raise the prices of all the bundles s 
 s 
n to be 
greater than since the prices cannot decrease the other 
bidders will clearly never demand these bundles in future 
stages an adversary may choose the values such that the 
low values of all the bidders for the bundles not in bidder s 
partition are zero i e vi s 
j for every i and every 
j however allocating each bidder a different bundle from 
bidder s partition might achieve a welfare of n − n− δ 
 bidder s valuation is and − δ for all other bidders 
if these bundles were wrongly allocated only a welfare of 
 might be achieved for bidder s high-valued bundle 
for all other bidders at this point the auctioneer cannot 
have any information about the identity of the bundles with 
the non-zero values therefore an adversary can choose the 
values of the bundles received by bidders n in the final 
allocation to be zero we conclude that anonymous 
bundleprice auctions cannot guarantee a welfare greater than for 
this class where the optimal welfare can be arbitrarily close 
to n 
 bundle prices vs item prices 
the core of the auctions in is the scheme described 
in figure in the spirit of for auctions with 
nonanonymous bundle prices auctions from this scheme end 
up with the optimal allocation for any class of valuations 
we denote this family of ascending auctions as nbea 
auctions 
 
nbea auctions can elicit k-term xor valuations by a 
polynomial in k number of steps although the elicitation 
of such valuations may require an exponential number of 
item-price queries and item-price ascending auctions 
cannot do it at all theorem nevertheless we show 
that nbea auctions and in particular ibundle and the 
proxy auction are sometimes inferior to simple item-price 
demand auctions this may justify the use of hybrid 
auctions that use both linear and non-linear prices e g the 
clock-proxy auction we show that auctions from this 
 
non-anonymous bundle-price economically efficient 
ascending auctions for completeness we give in the full 
paper a simple proof for the efficiency up to an of 
auctions of this scheme 
family may use an exponential number of queries even for 
determining the optimal allocation among two bidders with 
additive valuations 
 where such valuations can be elicited 
by a simple item-price ascending auction we actually prove 
this property for a wider class of auctions we call 
conservative auctions we also observe that in conservative auctions 
allowing the bidders to submit all the bundles in their 
demand sets ensures that the auction runs a polynomial 
number of steps - if l is not too high but with exponential 
communication of course 
an ascending auction is called conservative if it is 
nonanonymous uses bundle prices initialized to zero and at 
every stage the auctioneer can only raise prices of bundles 
demanded by the bidders until this stage in addition each 
bidder can only receive bundles he demanded during the 
auction note that nbea auctions are by definition 
conservative 
proposition if every bidder demands a single 
bundle in each step of the auction conservative auctions may 
run for an exponential number of steps even for additive 
valuations if the bidders are allowed to submit all the bundles 
in their demand sets in each step then conservative auctions 
can run in a polynomial number of steps for any profile of 
valuations as long as the maximal valuation l is polynomial 
in m n and 
δ 
 
acknowledgments 
the authors thank moshe babaioff shahar dobzinski ron 
lavi daniel lehmann ahuva mu alem david parkes michael 
schapira and ilya segal for helpful discussions supported 
by grants from the israeli academy of sciences and the 
usaisrael binational science foundation 
 references 
 amazon web page http www amazon com 
 ebay web page http www ebay com 
 l m ausubel and p r milgrom ascending auctions 
with package bidding frontiers of theoretical 
economics - 
 lawrence ausubel an efficient dynamic auction for 
heterogeneous commodities working paper 
university of maryland 
 yair bartal rica gonen and noam nisan incentive 
compatible multi unit combinatorial auctions in 
tark 
 alejandro bertelsen substitutes valuations and 
m -concavity m sc thesis the hebrew university 
of jerusalem 
 avrim blum jeffrey c jackson tuomas sandholm 
and martin a zinkevich preference elicitation and 
query learning journal of machine learning 
research - 
 liad blumrosen and noam nisan on the 
computational power of iterative auctions i demand 
queries working paper the hebrew university of 
 
valuations are called additive if for any disjoint bundles a 
and b v a ∪ b v a v b additive valuations are 
both sub-additive and super-additive and are determined by 
the m values assigned for the singletons 
 
jerusalem available from 
http www cs huji ac il ˜noam mkts html 
 liad blumrosen and noam nisan on the 
computational power of iterative auctions ii 
ascending auctions working paper the hebrew 
university of jerusalem available from 
http www cs huji ac il ˜noam mkts html 
 p cramton l m ausubel and p r milgrom in p 
cramton and y shoham and r steinberg editors 
combinatorial auctions chapter the clock-proxy 
auction a practical combinatorial auction design 
mit press forthcoming 
 p cramton y shoham and r steinberg editors 
combinatorial auctions mit press forthcoming 
 
 g demange d gale and m sotomayor multi-item 
auctions journal of political economy - 
 
 shahar dobzinski noam nisan and michael schapira 
approximation algorithms for cas with 
complement-free bidders in the th acm 
symposium on theory of computing stoc 
 shahar dobzinski and michael schapira optimal 
upper and lower approximation bounds for 
k-duplicates combinatorial auctions working paper 
the hebrew university 
 combinatorial bidding conference web page 
http wireless fcc gov auctions conferences combin 
 faruk gul and ennio stacchetti walrasian 
equilibrium with gross substitutes journal of 
economic theory - 
 faruk gul and ennio stacchetti the english auction 
with differentiated commodities journal of economic 
theory - 
 j hastad almost optimal lower bounds for small 
depth circuits in th stoc pages - 
 ron holzman noa kfir-dahav dov monderer and 
moshe tennenholtz bundling equilibrium in 
combinatrial auctions games and economic 
behavior - 
 h karloff linear programming birkh¨auser verlag 
 
 frank kelly and richard steinberg a combinatorial 
auction with multiple winners for universal service 
management science - 
 a s kelso and v p crawford job matching 
coalition formation and gross substitute 
econometrica - 
 subhash khot richard j lipton evangelos 
markakis and aranyak mehta inapproximability 
results for combinatorial auctions with submodular 
utility functions in working paper 
 sebastien lahaie and david c parkes applying 
learning algorithms to preference elicitation in ec 
 benny lehmann daniel lehmann and noam nisan 
combinatorial auctions with decreasing marginal 
utilities in acm conference on electronic commerce 
to appear games and economic behaviour 
 d lehmann l o callaghan and y shoham truth 
revelation in approximately efficient combinatorial 
auctions jacm - sept 
 a mas-collel w whinston and j green 
microeconomic theory oxford university press 
 debasis mishra and david parkes ascending price 
vickrey auctions using primal-dual algorithms 
working paper harvard university 
 noam nisan the communication complexity of 
approximate set packing and covering in icalp 
 noam nisan bidding and allocation in combinatorial 
auctions in acm conference on electronic 
commerce 
 noam nisan in p cramton and y shoham and r 
steinberg editors combinatorial auctions chapter 
 bidding languages mit press forthcoming 
 noam nisan and ilya segal the communication 
requirements of efficient allocations and supporting 
prices working paper available from 
http www cs huji ac il ˜noam mkts html 
forthcoming in the journal of economic theory 
 noam nisan and ilya segal exponential 
communication inefficiency of demand queries 
working paper available from 
http www stanford edu isegal queries pdf 
 d c parkes and l h ungar an ascending-price 
generalized vickrey auction tech rep harvard 
university 
 david parkes in p cramton and y shoham and r 
steinberg editors combinatorial auctions chapter 
 iterative combinatorial auctions mit press 
forthcoming 
 david c parkes iterative combinatorial auctions 
achieving economic and computational efficiency 
ph d thesis department of computer and 
information science university of pennsylvania 
 
 david c parkes and lyle h ungar iterative 
combinatorial auctions theory and practice in 
aaai iaai pages - 
 ariel rubinstein why are certain properties of binary 
relations relatively more common in natural 
languages econometrica - 
 tuomas sandholm algorithm for optimal winner 
determination in combinatorial auctions in artificial 
intelligence volume pages - 
 p santi v conitzer and t sandholm towards a 
characterization of polynomial preference elicitation 
with value queries in combinatorial auctions in the 
 th annual conference on learning theory 
 ilya segal the communication requirements of social 
choice rules and supporting budget sets 
working paper available from 
http www stanford edu isegal rules pdf 
 p r wurman and m p wellman akba a 
progressive anonymous-price combinatorial auction 
in second acm conference on electronic commerce 
 
 martin a zinkevich avrim blum and tuomas 
sandholm on polynomial-time preference elicitation 
with value queries in acm conference on electronic 
commerce 
 
expressive negotiation over donations to charities∗ 
vincent conitzer 
carnegie mellon university 
 forbes avenue 
pittsburgh pa usa 
conitzer cs cmu edu 
tuomas sandholm 
carnegie mellon university 
 forbes avenue 
pittsburgh pa usa 
sandholm cs cmu edu 
abstract 
when donating money to a say charitable cause it is 
possible to use the contemplated donation as negotiating 
material to induce other parties interested in the charity to 
donate more such negotiation is usually done in terms of 
matching offers where one party promises to pay a certain 
amount if others pay a certain amount however in their 
current form matching offers allow for only limited 
negotiation for one it is not immediately clear how multiple 
parties can make matching offers at the same time without 
creating circular dependencies also it is not immediately 
clear how to make a donation conditional on other 
donations to multiple charities when the donator has different 
levels of appreciation for the different charities in both 
these cases the limited expressiveness of matching offers 
causes economic loss it may happen that an arrangement 
that would have made all parties donators as well as 
charities better off cannot be expressed in terms of matching 
offers and will therefore not occur 
in this paper we introduce a bidding language for 
expressing very general types of matching offers over multiple 
charities we formulate the corresponding clearing 
problem deciding how much each bidder pays and how much 
each charity receives and show that it is np-complete to 
approximate to any ratio even in very restricted settings 
we give a mixed-integer program formulation of the 
clearing problem and show that for concave bids the program 
reduces to a linear program we then show that the clearing 
problem for a subclass of concave bids is at least as hard as 
the decision variant of linear programming subsequently 
we show that the clearing problem is much easier when bids 
are quasilinear-for surplus the problem decomposes across 
charities and for payment maximization a greedy approach 
is optimal if the bids are concave although this latter 
problem is weakly np-complete when the bids are not concave 
for the quasilinear setting we study the mechanism design 
question we show that an ex-post efficient mechanism is 
∗ 
supported by nsf under career award iri- 
grant iis- itr iis- and itr iis- 
impossible even with only one charity and a very restricted 
class of bids we also show that there may be benefits to 
linking the charities from a mechanism design standpoint 
categories and subject descriptors 
f theory of computation analysis of algorithms 
and problem complexity j computer applications 
social and behavioral sciences-economics 
general terms 
algorithms economics theory 
 introduction 
when money is donated to a charitable or other cause 
 hereafter referred to as charity often the donating party 
gives unconditionally a fixed amount is transferred from 
the donator to the charity and none of this transfer is 
contingent on other events-in particular it is not contingent 
on the amount given by other parties indeed this is 
currently often the only way to make a donation especially for 
small donating parties such as private individuals however 
when multiple parties support the same charity each of them 
would prefer to see the others give more rather than less to 
this charity in such scenarios it is sensible for a party to 
use its contemplated donation as negotiating material to 
induce the others to give more this is done by making the 
donation conditional on the others donations the 
following example will illustrate this and show that the donating 
parties as well as the charitable cause may simultaneously 
benefit from the potential for such negotiation 
suppose we have two parties and who are both 
supporters of charity a to either of them it would be worth 
 if a received it follows neither of them will be 
willing to give unconditionally because however if 
the two parties draw up a contract that says that they will 
each give both the parties have an incentive to accept 
this contract rather than have no contract at all with the 
contract the charity will receive rather than 
without a contract which is worth to each party which 
is greater than the that that party will have to give 
effectively each party has made its donation conditional on 
the other party s donation leading to larger donations and 
greater happiness to all parties involved 
 
one method that is often used to effect this is to make 
a matching offer examples of matching offers are i will 
give x dollars for every dollar donated or i will give x 
dollars if the total collected from other parties exceeds y 
in our example above one of the parties can make the offer 
i will donate if the other party also donates at least 
that much and the other party will have an incentive to 
indeed donate so that the total amount given to the 
charity increases by thus this matching offer implements 
the contract suggested above as a real-world example the 
united states government has authorized a donation of up to 
 billion to the global fund to fight aids tb and malaria 
under the condition that the american contribution does not 
exceed one third of the total-to encourage other countries 
to give more 
however there are several severe limitations to the simple 
approach of matching offers as just described 
 it is not clear how two parties can make matching 
offers where each party s offer is stated in terms of the 
amount that the other pays for example it is not 
clear what the outcome should be when both parties 
offer to match the other s donation thus matching 
offers can only be based on payments made by 
parties that are giving unconditionally not in terms of a 
matching offer -or at least there can be no circular 
dependencies 
 given the current infrastructure for making matching 
offers it is impractical to make a matching offer 
depend on the amounts given to multiple charities for 
instance a party may wish to specify that it will pay 
 given that charity a receives a total of 
but that it will also count donations made to charity 
b at half the rate thus a total payment of to 
charity a combined with a total payment of to 
charity b would be just enough for the party s offer to 
take effect 
in contrast in this paper we propose a new approach 
where each party can express its relative preferences for 
different charities and make its offer conditional on its own 
appreciation for the vector of donations made to the 
different charities moreover the amount the party offers to 
donate at different levels of appreciation is allowed to vary 
arbitrarily it does need to be a dollar-for-dollar or 
n-dollarfor-dollar matching arrangement or an arrangement where 
the party offers a fixed amount provided a given strike 
total has been exceeded finally there is a clear 
interpretation of what it means when multiple parties are making 
conditional offers that are stated in terms of each other 
given each combination of conditional offers there is a 
 usually unique solution which determines how much each 
party pays and how much each charity is paid 
however as we will show finding this solution the 
clearing problem requires solving a potentially difficult 
optimization problem a large part of this paper is devoted to 
studying how difficult this problem is under different assumptions 
on the structure of the offers and providing algorithms for 
solving it 
 
typically larger organizations match offers of private 
individuals for example the american red cross liberty 
disaster fund maintains a list of businesses that match their 
customers donations 
towards the end of the paper we also study the 
mechanism design problem of motivating the bidders to bid 
truthfully 
in short expressive negotiation over donations to charities 
is a new way in which electronic commerce can help the 
world a web-based implementation of the ideas described 
in this paper can facilitate voluntary reallocation of wealth 
on a global scale aditionally optimally solving the clearing 
problem and thereby generating the maximum economic 
welfare requires the application of sophisticated algorithms 
 comparison to combinatorial 
auctions and exchanges 
this section discusses the relationship between expressive 
charity donation and combinatorial auctions and exchanges 
it can be skipped but may be of interest to the reader with 
a background in combinatorial auctions and exchanges 
in a combinatorial auction there are m items for sale 
and bidders can place bids on bundles of one or more items 
the auctioneer subsequently labels each bid as winning or 
losing under the constraint that no item can be in more 
than one winning bid to maximize the sum of the values of 
the winning bids this is known as the clearing problem 
variants include combinatorial reverse auctions where the 
auctioneer is seeking to procure a set of items and 
combinatorial exchanges where bidders can both buy and and sell 
items even within the same bid other extensions include 
allowing for side constraints as well as the specification of 
attributes of the items in bids combinatorial auctions and 
exchanges have recently become a popular research topic 
 
the problems of clearing expressive charity donation 
markets and clearing combinatorial auctions or exchanges are 
very different in formulation nevertheless there are 
interesting parallels one of the main reasons for the interest 
in combinatorial auctions and exchanges is that it allows 
for expressive bidding a bidder can express exactly how 
much each different allocation is worth to her and thus the 
globally optimal allocation may be chosen by the 
auctioneer compare this to a bidder having to bid on two different 
items in two different one-item auctions without any way 
of expressing that for instance one item is worthless if the 
other item is not won in this scenario the bidder may win 
the first item but not the second because there was another 
high bid on the second item that she did not anticipate 
leading to economic inefficiency 
expressive bidding is also one of the main benefits of the 
expressive charity donation market here bidders can 
express exactly how much they are willing to donate for every 
vector of amounts donated to charities this may allow 
bidders to negotiate a complex arrangement of who gives 
how much to which charity which is beneficial to all 
parties involved whereas no such arrangement may have been 
possible if the bidders had been restricted to using simple 
matching offers on individual charities again expressive 
bidding is necessary to achieve economic efficiency 
another parallel is the computational complexity of the 
clearing problem in order to achieve the full economic 
efficiency allowed by the market s expressiveness or even come 
close to it hard computational problems must be solved 
in combinatorial auctions and exchanges as well as in the 
charity donation market as we will see 
 
 definitions 
throughout this paper we will refer to the offers that the 
donating parties make as bids and to the donating parties 
as bidders in our bidding framework a bid will specify 
for each vector of total payments made to the charities how 
much that bidder is willing to contribute the contribution 
of this bidder is also counted in the vector of 
paymentsso the vector of total payments to the charities represents 
the amount given by all donating parties not just the ones 
other than this bidder the bidding language is expressive 
enough that no bidder should have to make more than one 
bid the following definition makes the general form of a 
bid in our framework precise 
definition in a setting with m charities c c cm 
a bid by bidder bj is a function vj rm 
→ r the 
interpretation is that if charity ci receives a total amount of πci then 
bidder j is willing to donate up to vj πc πc πcm 
we now define possible outcomes in our model and which 
outcomes are valid given the bids that were made 
definition an outcome is a vector of payments made 
by the bidders πb πb πbn and a vector of payments 
received by the charities πc πc πcm a valid 
outcome is an outcome where 
 
n 
j 
πbj ≥ 
m 
i 
πci at least as much money is collected 
as is given away 
 for all ≤ j ≤ n πbj ≤ vj πc πc πcm no 
bidder gives more than she is willing to 
of course in the end only one of the valid outcomes can 
be chosen we choose the valid outcome that maximizes the 
objective that we have for the donation process 
definition an objective is a function from the set of 
all outcomes to r 
after all bids have been collected a valid 
outcome will be chosen that maximizes this objective 
one example of an objective is surplus given by 
n 
j 
πbj − 
m 
i 
πci the surplus could be the profits of a company 
managing the expressive donation marketplace but 
alternatively the surplus could be returned to the bidders or 
given to the charities another objective is total amount 
donated given by 
m 
i 
πci here different weights could also 
be placed on the different charities 
finding the valid outcome that maximizes the objective 
is a nontrivial computational problem we will refer to it 
as the clearing problem the formal definition follows 
definition donation-clearing we are 
given a set of n bids over charities c c cm 
additionally we are given an objective function we are asked to 
find an objective-maximizing valid outcome 
how difficult the donation-clearing problem is 
depends on the types of bids used and the language in which 
they are expressed this is the topic of the next section 
 
in general the objective function may also depend on the 
bids but the objective functions under consideration in this 
paper do not depend on the bids the techniques presented 
in this paper will typically generalize to objectives that take 
the bids into account directly 
 a simplified bidding language 
specifying a general bid in our framework as defined 
above requires being able to specify an arbitrary real-valued 
function over rm 
 even if we restricted the possible total 
payment made to each charity to the set { s} this 
would still require a bidder to specify s m 
values thus 
we need a bidding language that will allow the bidders to 
at least specify some bids more concisely we will specify a 
bidding language that only represents a subset of all possible 
bids which can be described concisely 
to introduce our bidding language we will first describe 
the bidding function as a composition of two functions then 
we will outline our assumptions on each of these functions 
first there is a utility function uj rm 
→ r specifying how 
much bidder j appreciates a given vector of total donations 
to the charities note that the way we define a bidder s 
utility function it does not take the payments the bidder 
makes into account then there is a donation willingness 
function wj r → r which specifies how much bidder j is 
willing to pay given her utility for the vector of donations 
to the charities we emphasize that this function does not 
need to be linear so that utilities should not be thought of as 
expressible in dollar amounts indeed when an individual 
is donating to a large charity the reason that the individual 
donates only a bounded amount is typically not decreasing 
marginal value of the money given to the charity but rather 
that the marginal value of a dollar to the bidder herself 
becomes larger as her budget becomes smaller so we have 
wj uj πc πc πcm vj πc πc πcm and we 
let the bidder describe her functions uj and wj separately 
 she will submit these functions as her bid 
our first restriction is that the utility that a bidder 
derives from money donated to one charity is independent of 
the amount donated to another charity thus 
uj πc πc πcm 
m 
i 
ui 
j πci we observe that this 
does not imply that the bid function vj decomposes 
similarly because of the nonlinearity of wj furthermore each 
ui 
j must be piecewise linear an interesting special case 
which we will study is when each ui 
j is a line ui 
j πci 
ai 
jπci this special case is justified in settings where the 
scale of the donations by the bidders is small relative to the 
amounts the charities receive from other sources so that the 
marginal use of a dollar to the charity is not affected by the 
amount given by the bidders 
the only restriction that we place on the payment 
willingness functions wj is that they are piecewise linear one 
interesting special case is a threshold bid where wj is a step 
function the bidder will provide t dollars if her utility 
exceeds s and otherwise another interesting case is when 
such a bid is partially acceptable the bidder will provide t 
dollars if her utility exceeds s but if her utility is u s she 
is still willing to provide ut 
s 
dollars 
one might wonder why if we are given the bidders utility 
functions we do not simply maximize the sum of the 
utilities rather than surplus or total donated there are several 
reasons first because affine transformations do not affect 
utility functions in a fundamental way it would be 
possi 
of course our bidding language can be trivially extended 
to allow for fully expressive bids by also allowing bids from 
a fully expressive bidding language in addition to the bids 
in our bidding language 
 
ble for a bidder to inflate her utility by changing its units 
thereby making her bid more important for utility 
maximization purposes second a bidder could simply give a 
payment willingness function that is everywhere and have 
her utility be taken into account in deciding on the outcome 
in spite of her not contributing anything 
 avoiding indirect payments 
in an initial implementation the approach of having 
donations made out to a center and having a center forward 
these payments to charities may not be desirable rather it 
may be preferable to have a partially decentralized solution 
where the donating parties write out checks to the charities 
directly according to a solution prescribed by the center in 
this scenario the center merely has to verify that parties are 
giving the prescribed amounts advantages of this include 
that the center can keep its legal status minimal as well 
as that we do not require the donating parties to trust the 
center to transfer their donations to the charities or require 
some complicated verification protocol it is also a step 
towards a fully decentralized solution if this is desirable 
to bring this about we can still use the approach 
described earlier after we clear the market in the manner 
described before we know the amount that each donator is 
supposed to give and the amount that each charity is 
supposed to receive then it is straightforward to give some 
specification of who should give how much to which charity 
that is consistent with that clearing any greedy algorithm 
that increases the cash flow from any bidder who has not 
yet paid enough to any charity that has not yet received 
enough until either the bidder has paid enough or the 
charity has received enough will provide such a specification 
 all of this is assuming that 
bj 
πbj 
ci 
πci in the case 
where there is nonzero surplus that is 
bj 
πbj 
ci 
πci we 
can distribute this surplus across the bidders by not 
requiring them to pay the full amount or across the charities by 
giving them more than the solution specifies 
nevertheless with this approach a bidder may have to 
write out a check to a charity that she does not care for at 
all for example an environmental activist who was using 
the system to increase donations to a wildlife preservation 
fund may be required to write a check to a group 
supporting a right-wing political party this is likely to lead to 
complaints and noncompliance with the clearing we can 
address this issue by letting each bidder specify explicitly 
 before the clearing which charities she would be willing 
to make a check out to these additional constraints of 
course may change the optimal solution in general 
checking whether a given centralized solution with zero surplus 
can be accomplished through decentralized payments when 
there are such constraints can be modeled as a max-flow 
problem in the max-flow instance there is an edge from 
the source node s to each bidder bj with a capacity of πbj 
 as specified in the centralized solution an edge from each 
bidder bj to each charity ci that the bidder is willing to 
donate money to with a capacity of ∞ and an edge from each 
charity ci to the target node t with capacity πci as specified 
in the centralized solution 
in the remainder of this paper all our hardness results 
apply even to the setting where there is no constraint on which 
bidders can pay to which charity that is even the problem 
as it was specified before this section is hard we also 
generalize our clearing algorithms to the partially decentralized 
case with constraints 
 hardness of clearing the 
market 
in this section we will show that the clearing problem is 
completely inapproximable even when every bidder s utility 
function is linear with slope or in each charity s 
payments each bidder cares either about at most two charities 
or about all charities equally and each bidder s payment 
willingness function is a step function we will reduce from 
max sat given a formula in conjunctive normal form 
 where each clause has two literals and a target number of 
satisfied clauses t does there exist an assignment of truth 
values to the variables that makes at least t clauses true 
which is np-complete 
theorem there exists a reduction from max sat 
instances to donation-clearing instances such that 
 if the max sat instance has no solution then the 
only valid outcome is the zero outcome no bidder pays 
anything and no charity receives anything otherwise there 
exists a solution with positive surplus additionally the 
donation-clearing instances that we reduce to have 
the following properties every ui 
j is a line that is the 
utility that each bidder derives from any charity is linear 
all the ui 
j have slope either or every bidder either 
has at most charities that affect her utility with slope 
or all charities affect her utility with slope every bid 
is a threshold bid that is every bidder s payment willingness 
function wj is a step function 
proof the problem is in np because we can 
nondeterministically choose the payments to be made and received 
and check the validity and objective value of this outcome 
in the following we will represent bids as follows 
 { ck ak } s t indicates that uk 
j πck akπck this 
function is for ck not mentioned in the bid and wj uj t 
for uj ≥ s wj uj otherwise 
to show np-hardness we reduce an arbitrary max sat 
instance given by a set of clauses k {k} { l 
k l 
k } 
over a variable set v together with a target number of 
satisfied clauses t to the following donation-clearing 
instance let the set of charities be as follows for every 
literal l ∈ l there is a charity cl then let the set of 
bids be as follows for every variable v there is a bid bv 
 { c v c−v } − 
 v 
 for every literal l there is 
a bid bl { cl } for every clause k {l 
k l 
k} ∈ k 
there is a bid bk { cl 
k 
 cl 
k 
 } 
 v k 
 finally 
there is a single bid that values all charities equally b 
 { c c cm } v t 
 v k 
 
 
 
 v k 
 we 
show the two instances are equivalent 
first suppose there exists a solution to the max sat 
instance if in this solution l is true then let πcl 
t 
 v k 
 otherwise πcl also the only bids that are 
not accepted meaning the threshold is not met are the bl 
where l is false and the bk such that both of l 
k l 
k are false 
first we show that no bidder whose bid is accepted pays 
more than she is willing to for each bv either c v or c−v 
receives at least so this bidder s threshold has been met 
 
for each bl either l is false and the bid is not accepted or l 
is true cl receives at least and the threshold has been met 
for each bk either both of l 
k l 
k are false and the bid is not 
accepted or at least one of them say li 
k is true that is k 
is satisfied and cli 
k 
receives at least and the threshold has 
been met finally because the total amount received by the 
charities is v t 
 v k 
 b s threshold has also been met 
the total amount that can be extracted from the accepted 
bids is at least v − 
 v 
 v t 
 v k 
 
 
 
 v k 
 
 v t 
 v k 
 
 v k 
 v t 
 v k 
 so there is positive 
surplus so there exists a solution with positive surplus to 
the donation-clearing instance 
now suppose there exists a nonzero outcome in the 
donation-clearing instance first we show that it 
is not possible for any v ∈ v that both b v and b−v are 
accepted for this would require that πc v πc−v ≥ 
the bids bv b v b−v cannot contribute more than so 
we need another at least it is easily seen that for any 
other v accepting any subset of {bv b v b−v } would 
require that at least as much is given to c v and c−v as 
can be extracted from these bids so this cannot help 
finally all the other bids combined can contribute at most 
 k 
 v k 
 
 
 
 v k 
 it follows that we can 
interpret the outcome in the donation-clearing instance 
as a partial assignment of truth values to variables v is set 
to true if b v is accepted and to false if b−v is accepted all 
that is left to show is that this partial assignment satisfies 
at least t clauses 
first we show that if a clause bid bk is accepted then 
either bl 
k 
or bl 
k 
is accepted and thus either l 
k or l 
k is set 
to true hence k is satisfied if bk is accepted at least one 
of cl 
k 
and cl 
k 
must be receiving at least without loss of 
generality say it is cl 
k 
 and say l 
k corresponds to variable 
v 
k that is it is v 
k or −v 
k if cl 
k 
does not receive at 
least bl 
k 
is not accepted and it is easy to check that 
the bids bv 
k 
 b v 
k 
 b−v 
k 
contribute at least less than is 
paid to c v 
k 
and c v 
k 
 but this is the same situation that 
we analyzed before and we know it is impossible all that 
remains to show is that at least t clause bids are accepted 
we now show that b is accepted suppose it is not then 
one of the bv must be accepted the solution is nonzero by 
assumption if only some bk are accepted the total payment 
from these bids is at most k 
 v k 
 which is not 
enough for any bid to be accepted and if one of the bl is 
accepted then the threshold for the corresponding bv is also 
reached for this v bv 
k 
 b v 
k 
 b−v 
k 
contribute at least 
 
 v 
less than the total payments to c v and c−v again 
the other bv and bl cannot by themselves help to close this 
gap and the bk can contribute at most k 
 v k 
 
 v 
 
it follows that b is accepted 
now in order for b to be accepted a total of v t 
 v k 
must be donated because is not possible for any v ∈ v 
that both b v and b−v are accepted it follows that the total 
payment by the bv and the bl can be at most v − 
 
 
adding b s payment of 
 
 
 v k 
to this we still need 
t − 
 
 v k 
from the bk but each one of them contributes at 
most 
 v k 
 so at least t of them must be accepted 
corollary unless p np there is no polynomial-time 
algorithm for approximating donation-clearing with 
either the surplus or the total amount donated as the 
objective within any ratio f n where f is a nonzero function of 
the size of the instance this holds even if the 
donationclearing structures satisfy all the properties given in 
theorem 
proof suppose we had such a polynomial time 
algorithm and applied it to the donation-clearing 
instances that were reduced from max sat instances in 
theorem it would return a nonzero solution when the 
max sat instance has a solution and a zero solution 
otherwise so we can decide whether arbitrary max sat 
instances are satisfiable this way and it would follow that 
p np 
 solving the problem to optimality is np-complete in many 
other noncomparable or even more restricted settings as 
well-we omit such results because of space constraint 
this should not be interpreted to mean that our approach is 
infeasible first as we will show there are very expressive 
families of bids for which the problem is solvable in 
polynomial time second np-completeness is often overcome in 
practice especially when the stakes are high for instance 
even though the problem of clearing combinatorial auctions 
is np-complete even to approximate they are 
typically solved to optimality in practice 
 mixed integer programming 
formulation 
in this section we give a mixed integer programming 
 mip formulation for the general problem we also discuss 
in which special cases this formulation reduces to a linear 
programming lp formulation in such cases the problem 
is solvable in polynomial time because linear programs can 
be solved in polynomial time 
the variables of the mip defining the final outcome are 
the payments made to the charities denoted by πci and 
the payments extracted from the bidders πbj in the case 
where we try to avoid direct payments and let the bidders 
pay the charities directly we add variables πci bj indicating 
how much bj pays to ci with the constraints that for each 
ci πci ≤ 
bj 
πci bj and for each bj πbj ≥ 
ci 
πci bj 
additionally there is a constraint πci bj whenever bidder bj 
is unwilling to pay charity ci the rest of the mip can be 
phrased in terms of the πci and πbj 
the objectives we have discussed earlier are both linear 
surplus is given by 
n 
j 
πbj − 
m 
i 
πci and total amount 
donated is given by 
m 
i 
πci coefficients can be added to 
represent different weights on the different charities in the 
objective 
the constraint that the outcome should be valid no deficit 
is given simply by 
n 
j 
πbj ≥ 
m 
i 
πci 
for every bidder for every charity we define an additional 
utility variable ui 
j indicating the utility that this bidder 
derives from the payment to this charity the bidder s total 
 
utility is given by another variable uj with the constraint 
that uj 
m 
i 
ui 
j 
each ui 
j is given as a function of πci by the piecewise 
linear function provided by the bidder in order to 
represent this function in the mip formulation we will merely 
place upper bounding constraints on ui 
j so that it cannot 
exceed the given functions the mip solver can then push 
the ui 
j variables all the way up to the constraint in order 
to extract as much payment from this bidder as possible 
in the case where the ui 
j are concave this is easy if sl tl 
and sl tl are endpoints of a finite linear segment in the 
function we add the constraint that ui 
j ≤ tl 
πci 
−sl 
sl −sl 
 tl − 
tl if the final infinite segment starts at sk tk and has 
slope d we add the constraint that ui 
j ≤ tk d πci − sk 
using the fact that the function is concave for each value of 
πci the tightest upper bound on ui 
j is the one corresponding 
to the segment above that value of πci and therefore these 
constraints are sufficient to force the correct value of ui 
j 
when the function is not concave we require for the first 
time some binary variables first we define another point 
on the function sk tk sk m tk dm where 
d is the slope of the infinite segment and m is any upper 
bound on the πcj this has the effect that we will never be 
on the infinite segment again now let xi j 
l be an indicator 
variable that should be if πci is below the lth segment of 
the function and otherwise to effect this first add a 
constraint 
k 
l 
xi j 
l now we aim to represent πci as a 
weighted average of its two neighboring si j 
l for ≤ l ≤ 
k let λi j 
l be the weight on si j 
l we add the constraint 
k 
l 
λi j 
l also for ≤ l ≤ k we add the constraint 
λi j 
l ≤ xl− xl where x− and xk are defined to be zero 
so that indeed only the two neighboring si j 
l have nonzero 
weight now we add the constraint πci 
k 
l 
si j 
l λi j 
l and 
now the λi j 
l must be set correctly then we can set ui 
j 
k 
l 
ti j 
l λi j 
l this is a standard mip technique 
finally each πbj is bounded by a function of uj by the 
 piecewise linear function provided by the bidder wj 
representing this function is entirely analogous to how we 
represented ui 
j as a function of πci again we will need binary 
variables only if the function is not concave 
because we only use binary variables when either a 
utility function ui 
j or a payment willingness function wj is not 
concave it follows that if all of these are concave our mip 
formulation is simply a linear program-which can be solved 
in polynomial time thus 
theorem if all functions ui 
j and wj are concave and 
piecewise linear the donation-clearing problem can 
be solved in polynomial time using linear programming 
even if some of these functions are not concave we can 
simply replace each such function by the smallest upper 
bounding concave function and use the linear programming 
formulation to obtain an upper bound on the 
objectivewhich may be useful in a search formulation of the general 
problem 
 why one cannot do much 
better than linear 
programming 
one may wonder if for the special cases of the 
donationclearing problem that can be solved in polynomial time 
with linear programming there exist special purpose 
algorithms that are much faster than linear programming 
algorithms in this section we show that this is not the case 
we give a reduction from the decision variant of the 
general linear programming problem to the decision variant 
of a special case of the donation-clearing problem 
 which can be solved in polynomial time using linear 
programming the decision variant of an optimization 
problem asks the binary question can the objective value 
exceed o thus any special-purpose algorithm for solving 
the decision variant of this special case of the 
donationclearing problem could be used to solve a decision 
question about an arbitrary linear program just as fast and 
thus if we are willing to call the algorithm a logarithmic 
number of times we can solve the optimization version of 
the linear program 
we first observe that for linear programming a decision 
question about the objective can simply be phrased as 
another constraint in the lp forcing the objective to exceed 
the given value then the original decision question 
coincides with asking whether the resulting linear program has 
a feasible solution 
theorem the question of whether an lp given by a 
set of linear constraints 
 has a feasible solution can be 
modeled as a donation-clearing instance with payment 
maximization as the objective with v charities and v c 
bids where v is the number of variables in the lp and c is 
the number of constraints in this model each bid bj has 
only linear ui 
j functions and is a partially acceptable 
threshold bid wj u tj for u ≥ sj otherwise wj u 
utj 
sj 
 the 
v bids corresponding to the variables mention only two 
charities each the c bids corresponding to the constraints mention 
only two times the number of variables in the corresponding 
constraint 
proof for every variable xi in the lp let there be two 
charities c xi and c−xi let h be some number such that 
if there is a feasible solution to the lp there is one in which 
every variable has absolute value at most h 
in the following we will represent bids as follows 
 { ck ak } s t indicates that uk 
j πck akπck this 
function is for ck not mentioned in the bid and wj uj t 
for uj ≥ s wj uj 
uj t 
s 
otherwise 
for every variable xi in the lp let there be a bid bxi 
 { c xi c−xi } h h − c 
v 
 for every constraint 
i 
rj 
i xi ≤ sj in the linear program let there be a bid bj 
 { c−xi rj 
i }i r 
j 
i 
∪ { c xi −rj 
i }i r 
j 
i 
 
i 
 rj 
i h − sj 
let the target total amount donated be vh 
suppose there is a feasible solution x∗ 
 x∗ 
 x∗ 
v to the 
lp without loss of generality we can suppose that x∗ 
i ≤ h 
for all i then in the donation-clearing instance 
 
these constraints must include bounds on the variables 
 including nonnegativity bounds if any 
 
for every i let πc xi 
 h x∗ 
i and let πc−xi 
 h − x∗ 
i 
 for a total payment of h to these two charities this 
allows us to extract the maximum payment from the bids 
bxi -a total payment of vh − c additionally the utility 
of bidder bj is now 
i r 
j 
i 
rj 
i h − x∗ 
i 
i r 
j 
i 
−rj 
i h x∗ 
i 
 
i 
 rj 
i h − 
i 
rj 
i x∗ 
i ≥ 
i 
 rj 
i h − sj where the last 
inequality stems from the fact that constraint j must be 
satisfied in the lp solution so it follows we can extract the 
maximum payment from all the bidders bj for a total 
payment of c it follows that we can extract the required vh 
payment from the bidders and there exists a solution to 
the donation-clearing instance with a total amount 
donated of at least vh 
now suppose there is a solution to the 
donationclearing instance with a total amount donated of at 
least vh then the maximum payment must be extracted 
from each bidder from the fact that the maximum payment 
must be extracted from each bidder bxi it follows that for 
each i πc xi 
 πc−xi 
≥ h because the maximum 
extractable total payment is vh it follows that for each i 
πc xi 
 πc−xi 
 h let x∗ 
i πc xi 
− h h − πc−xi 
 
then from the fact that the maximum payment must be 
extracted from each bidder bj it follows that 
i 
 rj 
i h − 
sj ≤ 
i r 
j 
i 
rj 
i πc−xi 
 
i r 
j 
i 
−rj 
i πc xi 
 
i r 
j 
i 
rj 
i h − x∗ 
i 
i r 
j 
i 
−rj 
i h x∗ 
i 
i 
 rj 
i h − 
i 
rj 
i x∗ 
i equivalently 
i 
rj 
i x∗ 
i ≤ sj it follows that the x∗ 
i constitute a feasible 
solution to the lp 
 quasilinear bids 
another class of bids of interest is the class of quasilinear 
bids in a quasilinear bid the bidder s payment willingness 
function is linear in utility that is wj uj because the 
units of utility are arbitrary we may as well let them 
correspond exactly to units of money-so we do not need a 
constant multiplier in most cases quasilinearity is an 
unreasonable assumption for example usually bidders have a 
limited budget for donations so that the payment 
willingness will stop increasing in utility after some point or at 
least increase slower in the case of a softer budget 
constraint nevertheless quasilinearity may be a reasonable 
assumption in the case where the bidders are large 
organizations with large budgets and the charities are a few small 
projects requiring relatively little money in this setting 
once a certain small amount has been donated to a charity 
a bidder will derive no more utility from more money 
being donated from that charity thus the bidders will never 
reach a high enough utility for their budget constraint even 
when it is soft to take effect and thus a linear 
approximation of their payment willingness function is reasonable 
another reason for studying the quasilinear setting is that 
it is the easiest setting for mechanism design which we will 
discuss shortly in this section we will see that the clearing 
problem is much easier in the case of quasilinear bids 
first we address the case where we are trying to maximize 
surplus which is the most natural setting for mechanism 
design the key observation here is that when bids are 
quasilinear the clearing problem decomposes across charities 
lemma suppose all bids are quasilinear and surplus 
is the objective then we can clear the market optimally by 
clearing the market for each charity individually that is 
for each bidder bj let πbj 
ci 
πbi 
j 
 then for each charity 
ci maximize 
bj 
πbi 
j 
 − πci under the constraint that for 
every bidder bj πbi 
j 
≤ ui 
j πci 
proof the resulting solution is certainly valid first of 
all at least as much money is collected as is given away 
because 
bj 
πbj − 
ci 
πci 
bj ci 
πbi 
j 
− 
ci 
πci 
ci 
 
bj 
πbi 
j 
 − 
πci -and the terms of this summation are the objectives of 
the individual optimization problems each of which can be 
set at least to by setting all the variables are set to 
so it follows that the expression is nonnegative second no 
bidder bj pays more than she is willing to because uj −πbj 
ci 
ui 
j πci − 
ci 
πbi 
j 
 
ci 
 ui 
j πci −πbi 
j 
 -and the terms of this 
summation are nonnegative by the constraints we imposed 
on the individual optimization problems 
all that remains to show is that the solution is 
optimal because in an optimal solution we will extract as 
much payment from the bidders as possible given the πci 
all we need to show is that the πci are set optimally by 
this approach let π∗ 
ci 
be the amount paid to charity πci 
in some optimal solution if we change this amount to πci 
and leave everything else unchanged this will only affect 
the payment that we can extract from the bidders because 
of this particular charity and the difference in surplus will 
be 
bj 
ui 
j πci 
 − ui 
j π∗ 
ci 
 − πci 
 π∗ 
ci 
 this expression is of 
course if πci 
 π∗ 
ci 
 but now notice that this expression 
is maximized as a function of πci 
by the decomposed 
solution for this charity the terms without πci 
in them do not 
matter and of course in the decomposed solution we always 
set πbi 
j 
 ui 
j πci it follows that if we change πci to the 
decomposed solution the change in surplus will be at least 
 and the solution will still be valid thus we can change 
the πci one by one to the decomposed solution without ever 
losing any surplus 
theorem when all bids are quasilinear and surplus 
is the objective donation-clearing can be done in 
linear time 
proof by lemma we can solve the problem 
separately for each charity for charity ci this amounts to 
maximizing 
bj 
ui 
j πci − πci as a function of πci because all 
its terms are piecewise linear functions this whole function 
is piecewise linear and must be maximized at one of the 
points where it is nondifferentiable it follows that we need 
only check all the points at which one of the terms is 
nondifferentiable 
unfortunately the decomposing lemma does not hold for 
payment maximization 
proposition when the objective is payment 
maximization even when bids are quasilinear the solution obtained 
by decomposing the problem across charities is in general not 
optimal even with concave bids 
 
proof consider a single bidder b placing the following 
quasilinear bid over two charities c and c u 
 πc πci 
for ≤ πci ≤ u 
 πc 
πci 
− 
 
otherwise u 
 πc 
πci 
 
 the decomposed solution is πc 
 
 πc for a 
total donation of 
 
 but the solution πc πc is 
also valid for a total donation of 
 
 
in fact when payment maximization is the objective 
donation-clearing remains weakly np-complete in 
general in the remainder of the paper proofs are omitted 
because of space constraint 
theorem donation-clearing is weakly 
npcomplete when payment maximization is the objective even 
when every bid is concerns only one charity and has a 
stepfunction utility function for this charity and is quasilinear 
however when the bids are also concave a simple greedy 
clearing algorithm is optimal 
theorem given a donation-clearing instance 
with payment maximization as the objective where all bids 
are quasilinear and concave consider the following 
algorithm start with πci for all charities then letting 
γci 
d 
bj 
ui 
j πci 
 
dπci 
 at nondifferentiable points these 
derivatives should be taken from the right increase πc∗ 
i 
 where 
c∗ 
i ∈ arg maxci γci until either γc∗ 
i 
is no longer the highest 
 in which case recompute c∗ 
i and start increasing the 
corresponding payment or 
bj 
uj 
ci 
πci and γc∗ 
i 
 finally 
let πbj uj 
 a similar greedy algorithm works when the objective is 
surplus and the bids are quasilinear and concave with as 
only difference that we stop increasing the payments as soon 
as γc∗ 
i 
 
 incentive compatibility 
up to this point we have not discussed the bidders 
incentives for bidding any particular way specifically the bids 
may not truthfully reflect the bidders preferences over 
charities because a bidder may bid strategically misrepresenting 
her preferences in order to obtain a result that is better to 
herself this means the mechanism is not strategy-proof 
 we will show some concrete examples of this shortly this 
is not too surprising because the mechanism described so 
far is in a sense a first-price mechanism where the 
mechanism will extract as much payment from a bidder as her bid 
allows such mechanisms for example first-price auctions 
where winners pay the value of their bids are typically not 
strategy-proof if a bidder reports her true valuation for an 
outcome then if this outcome occurs the payment the 
bidder will have to make will offset her gains from the outcome 
completely of course we could try to change the rules of 
the game-which outcome payment vector to charities do 
we select for which bid vector and which bidder pays how 
much-in order to make bidding truthfully beneficial and 
to make the outcome better with regard to the bidders true 
preferences this is the field of mechanism design in this 
section we will briefly discuss the options that mechanism 
design provides for the expressive charity donation problem 
 strategic bids under the first-price 
mechanism 
we first point out some reasons for bidders to misreport 
their preferences under the first-price mechanism described 
in the paper up to this point first of all even when there is 
only one charity it may make sense to underbid one s true 
valuation for the charity for example suppose a bidder 
would like a charity to receive a certain amount x but does 
not care if the charity receives more than that additionally 
suppose that the other bids guarantee that the charity will 
receive at least x no matter what bid the bidder submits 
 and the bidder knows this then the bidder is best off not 
bidding at all or submitting a utility for the charity of 
to avoid having to make any payment this is known in 
economics as the free rider problem 
with multiple charities another kind of manipulation may 
occur where the bidder attempts to steer others payments 
towards her preferred charity suppose that there are two 
charities and three bidders the first bidder bids u 
 πc 
 if πc ≥ u 
 πc otherwise u 
 πc if πc ≥ 
u 
 πc otherwise and w u u if u ≤ w u 
 
 
 u − otherwise the second bidder bids u 
 πc 
 if πc ≥ u 
 πc otherwise u 
 πc always 
w u 
 
u if u ≤ w u 
 
 
 
 u − otherwise 
now the third bidder s true preferences are accurately 
represented 
by the bid u 
 πc if πc ≥ u 
 πc 
otherwise u 
 πc if πc ≥ u 
 πc otherwise 
and w u 
 
u if u ≤ w u 
 
 
 
 u − 
otherwise now it is straightforward to check that if the third 
bidder bids truthfully regardless of whether the objective is 
surplus maximization or total donated charity will receive 
at least and charity will receive less than the same is 
true if bidder does not place a bid at all as in the previous 
type of manipulation hence bidder s utility will be in 
this case but now if bidder reports u 
 πc 
everywhere u 
 πc if πc ≥ u 
 πc otherwise this 
part of the bid is truthful and w u 
 
u if u ≤ 
w u 
 
otherwise then charity will receive at least 
 and bidder will have to pay at most 
 
 because up to 
this amount of payment one unit of money corresponds to 
three units of utility to bidder it follows his utility is now 
at least − we observe that in this case the 
strategic bidder is not only affecting how much the bidders 
pay but also how much the charities receive 
 mechanism design in the quasilinear 
setting 
there are four reasons why the mechanism design 
approach is likely to be most successful in the setting of 
quasilinear preferences first historically mechanism design has 
been been most successful when the quasilinear assumption 
could be made second because of this success some very 
general mechanisms have been discovered for the 
quasilinear setting for instance the vcg mechanisms 
or the dagva mechanism which we could apply 
directly to the expressive charity donation problem third as 
we saw in section the clearing problem is much easier in 
 
formally this means that if the bidder is forced to pay 
the full amount that his bid allows for a particular vector of 
payments to charities the bidder is indifferent between this 
and not participating in the mechanism at all compare 
this to bidding truthfully in a first-price auction 
 
this setting and thus we are less likely to run into 
computational trouble for the mechanism design problem fourth as 
we will show shortly the quasilinearity assumption in some 
cases allows for decomposing the mechanism design problem 
over the charities as it did for the simple clearing problem 
moreover in the quasilinear setting unlike in the general 
setting it makes sense to pursue social welfare the sum 
of the utilities as the objective because now units of 
utility correspond directly to units of money so that we do 
not have the problem of the bidders arbitrarily scaling their 
utilities and it is no longer possible to give a payment 
willingness function of while still affecting the donations 
through a utility function 
before presenting the decomposition result we introduce 
some terms from game theory a type is a preference profile 
that a bidder can have and can report thus a type report 
is a bid incentive compatibility ic means that bidders 
are best off reporting their preferences truthfully either 
regardless of the others types in dominant strategies or in 
expectation over them in bayes-nash equilibrium 
individual rationality ir means agents are at least as well off 
participating in the mechanism as not participating either 
regardless of the others types ex-post or in expectation 
over them ex-interim a mechanism is budget balanced 
if there is no flow of money into or out of the system-in 
general ex-post or in expectation over the type reports 
 ex-ante a mechanism is efficient if it always produces 
the efficient allocation of wealth to charities 
theorem suppose all agents preferences are 
quasilinear furthermore suppose that there exists a single-charity 
mechanism m that for a certain subclass p of quasilinear 
preferences under a given solution concept s 
 implementation in dominant strategies or bayes-nash equilibrium and 
a given notion of individual rationality r ex post ex 
interim or none satisfies a certain notion of budget balance 
 ex post ex ante or none and is ex-post efficient then 
there exists such a mechanism for any number of charities 
two mechanisms that satisfy efficiency and can in fact be 
applied directly to the multiple-charity problem without use 
of the previous theorem are the vcg which is incentive 
compatible in dominant strategies and dagva which is 
incentive compatible only in bayes-nash equilibrium 
mechanisms each of them however has a drawback that would 
probably make it impractical in the setting of donations to 
charities the vcg mechanism is not budget balanced the 
dagva mechanism does not satisfy ex-post individual 
rationality in the next subsection we will investigate if we 
can do better in the setting of donations to charities 
 impossibility of efficiency 
in this subsection we show that even in a very restricted 
setting and with minimal requirements on ic and ir 
constraints it is impossible to create a mechanism that is 
efficient 
theorem there is no mechanism which is ex-post 
budget balanced ex-post efficient and ex-interim individually 
rational with bayes-nash equilibrium as the solution concept 
 even with only one charity only two quasilinear bidders 
with identical type distributions uniform over two types 
with either both utility functions being step functions or both 
utility functions being concave piecewise linear functions 
the case of step-functions in this theorem corresponds 
exactly to the case of a single fixed-size nonexcludable public 
good the public good being that the charity receives the 
desired amount -for which such an impossibility result is 
already known many similar results are known 
probably the most famous of which is the myerson-satterthwaite 
impossibility result which proves the impossibility of 
efficient bilateral trade under the same requirements 
theorem indicates that there is no reason to decide on 
donations to multiple charities under a single mechanism 
 rather than a separate one for each charity when an 
efficient mechanism with the desired properties exists for the 
single-charity case however because under the 
requirements of theorem no such mechanism exists there may 
be a benefit to bringing the charities under the same 
umbrella the next proposition shows that this is indeed the 
case 
proposition there exist settings with two charities 
where there exists no ex-post budget balanced ex-post 
efficient and ex-interim individually rational mechanism with 
bayes-nash equilibrium as the solution concept for either 
charity alone but there exists an ex-post budget balanced 
ex-post efficient and ex-post individually rational 
mechanism with dominant strategies as the solution concept for 
both charities together even when the conditions are the 
same as in theorem apart from the fact that there are 
now two charities 
 conclusion 
we introduced a bidding language for expressing very 
general types of matching offers over multiple charities we 
formulated the corresponding clearing problem deciding how 
much each bidder pays and how much each charity receives 
and showed that it is np-complete to approximate to any 
ratio even in very restricted settings we gave a mixed-integer 
program formulation of the clearing problem and showed 
that for concave bids where utility functions and payment 
willingness function are concave the program reduces to a 
linear program and can hence be solved in polynomial time 
we then showed that the clearing problem for a subclass of 
concave bids is at least as hard as the decision variant of 
linear programming suggesting that we cannot do much better 
than a linear programming implementation for such bids 
subsequently we showed that the clearing problem is much 
easier when bids are quasilinear where payment willingness 
functions are linear -for surplus the problem decomposes 
across charities and for payment maximization a greedy 
approach is optimal if the bids are concave although this 
latter problem is weakly np-complete when the bids are not 
concave for the quasilinear setting we studied the 
mechanism design question of making the bidders report their 
preferences truthfully rather than strategically we showed 
that an ex-post efficient mechanism is impossible even with 
only one charity and a very restricted class of bids we 
also showed that even though the clearing problem 
decomposes over charities in the quasilinear setting there may be 
benefits to linking the charities from a mechanism design 
standpoint 
there are many directions for future research one is to 
build a web-based implementation of the first-price 
mechanism proposed in this paper another is to study the 
computational scalability of our mip lp approach it is also 
 
important to identify other classes of bids besides concave 
ones for which the clearing problem is tractable much 
crucial work remains to be done on the mechanism design 
problem finally are there good iterative mechanisms for 
charity donation 
 references 
 k arrow the property rights doctrine and demand 
revelation under incomplete information in 
m boskin editor economics and human welfare 
new york academic press 
 l m ausubel and p milgrom ascending auctions 
with package bidding frontiers of theoretical 
economics no article 
 y bartal r gonen and n nisan incentive 
compatible multi-unit combinatorial auctions in 
theoretical aspects of rationality and knowledge 
 tark ix bloomington indiana usa 
 e h clarke multipart pricing of public goods public 
choice - 
 v conitzer and t sandholm complexity of 
mechanism design in proceedings of the th annual 
conference on uncertainty in artificial intelligence 
 uai- pages - edmonton canada 
 c d aspremont and l a g´erard-varet incentives 
and incomplete information journal of public 
economics - 
 m r garey d s johnson and l stockmeyer some 
simplified np-complete graph problems theoretical 
computer science - 
 d goldburg and s mcelligott red cross statement 
on official donation locations press release 
http www redcross org press disaster ds pr 
 legitdonors html 
 r gonen and d lehmann optimal solutions for 
multi-unit combinatorial auctions branch and bound 
heuristics in proceedings of the acm conference on 
electronic commerce acm-ec pages - 
minneapolis mn oct 
 t groves incentives in teams econometrica 
 - 
 l khachiyan a polynomial algorithm in linear 
programming soviet math doklady - 
 
 r lavi a mu alem and n nisan towards a 
characterization of truthful combinatorial auctions in 
proceedings of the annual symposium on foundations 
of computer science focs 
 d lehmann l i o callaghan and y shoham 
truth revelation in rapid approximately efficient 
combinatorial auctions journal of the acm 
 - early version appeared in 
acmec- 
 
compare for example iterative mechanisms in the 
combinatorial auction setting 
 a mas-colell m whinston and j r green 
microeconomic theory oxford university press 
 r myerson and m satterthwaite efficient 
mechanisms for bilateral trading journal of economic 
theory - 
 g l nemhauser and l a wolsey integer and 
combinatorial optimization john wiley sons 
 section page 
 n nisan bidding and allocation in combinatorial 
auctions in proceedings of the acm conference on 
electronic commerce acm-ec pages - 
minneapolis mn 
 n nisan and a ronen computationally feasible 
vcg mechanisms in proceedings of the acm 
conference on electronic commerce acm-ec 
pages - minneapolis mn 
 d c parkes ibundle an efficient ascending price 
bundle auction in proceedings of the acm conference 
on electronic commerce acm-ec pages - 
denver co nov 
 m h rothkopf a pekeˇc and r m harstad 
computationally manageable combinatorial auctions 
management science - 
 t sandholm algorithm for optimal winner 
determination in combinatorial auctions artificial 
intelligence - jan conference version 
appeared at the international joint conference on 
artificial intelligence ijcai pp - 
stockholm sweden 
 t sandholm s suri a gilpin and d levine 
cabob a fast optimal algorithm for combinatorial 
auctions in proceedings of the seventeenth 
international joint conference on artificial 
intelligence ijcai pages - seattle wa 
 
 j tagliabue global aids funds is given attention 
but not money the new york times june 
reprinted on 
http www healthgap org press releases a 
 nyt hgap g fund html 
 w vickrey counterspeculation auctions and 
competitive sealed tenders journal of finance 
 - 
 p r wurman and m p wellman akba a 
progressive anonymous-price combinatorial auction 
in proceedings of the acm conference on electronic 
commerce acm-ec pages - minneapolis 
mn oct 
 m yokoo the characterization of strategy false-name 
proof combinatorial auction protocols price-oriented 
rationing-free protocol in proceedings of the 
eighteenth international joint conference on artificial 
intelligence ijcai acapulco mexico aug 
 
applying learning algorithms to preference elicitation 
sebastien m lahaie 
division of engineering and applied sciences 
harvard university 
cambridge ma 
slahaie eecs harvard edu 
david c parkes 
division of engineering and applied sciences 
harvard university 
cambridge ma 
parkes eecs harvard edu 
abstract 
we consider the parallels between the preference elicitation 
problem in combinatorial auctions and the problem of 
learning an unknown function from learning theory we show 
that learning algorithms can be used as a basis for 
preference elicitation algorithms the resulting elicitation 
algorithms perform a polynomial number of queries we also 
give conditions under which the resulting algorithms have 
polynomial communication our conversion procedure 
allows us to generate combinatorial auction protocols from 
learning algorithms for polynomials monotone dnf and 
linear-threshold functions in particular we obtain an 
algorithm that elicits xor bids with polynomial 
communication 
categories and subject descriptors 
f analysis of algorithms and problem 
complexity general j social and behavioral sciences 
economics i artificial intelligence learning 
general terms 
algorithms economics theory 
 introduction 
in a combinatorial auction agents may bid on bundles of 
goods rather than individual goods alone since there are 
an exponential number of bundles in the number of goods 
communicating values over these bundles can be 
problematic communicating valuations in a one-shot fashion can 
be prohibitively expensive if the number of goods is only 
moderately large furthermore it might even be hard for 
agents to determine their valuations for single bundles 
it is in the interest of such agents to have auction protocols 
which require them to bid on as few bundles as possible 
even if agents can efficiently compute their valuations they 
might still be reluctant to reveal them entirely in the course 
of an auction because such information may be valuable to 
their competitors these considerations motivate the need 
for auction protocols that minimize the communication and 
information revelation required to determine an optimal 
allocation of goods 
there has been recent work exploring the links between 
the preference elicitation problem in combinatorial auctions 
and the problem of learning an unknown function from 
computational learning theory in learning theory the 
goal is to learn a function via various types of queries such 
as what is the function s value on these inputs in 
preference elicitation the goal is to elicit enough partial 
information about preferences to be able to compute an optimal 
allocation though the goals of learning and preference 
elicitation differ somewhat it is clear that these problems share 
similar structure and it should come as no surprise that 
techniques from one field should be relevant to the other 
we show that any exact learning algorithm with 
membership and equivalence queries can be converted into a 
preference elicitation algorithm with value and demand queries 
the resulting elicitation algorithm guarantees elicitation in 
a polynomial number of value and demand queries here 
we mean polynomial in the number of goods agents and 
the sizes of the agents valuation functions in a given 
encoding scheme preference elicitation schemes have not 
traditionally considered this last parameter we argue that 
complexity guarantees for elicitation schemes should allow 
dependence on this parameter introducing this parameter 
also allows us to guarantee polynomial worst-case 
communication which usually cannot be achieved in the number 
of goods and agents alone finally we use our conversion 
procedure to generate combinatorial auction protocols from 
learning algorithms for polynomials monotone dnf and 
linear-threshold functions 
of course a one-shot combinatorial auction where agents 
provide their entire valuation functions at once would also 
have polynomial communication in the size of the agents 
valuations and only require one query the advantage of 
our scheme is that agents can be viewed as black-boxes 
that provide incremental information about their valuations 
there is no burden on the agents to formulate their 
valuations in an encoding scheme of the auctioneer s choosing 
we expect this to be an important consideration in practice 
also with our scheme entire revelation only happens in the 
worst-case 
 
for now we leave the issue of incentives aside when 
deriving elicitation algorithms our focus is on the time and 
communication complexity of preference elicitation 
regardless of incentive constraints and on the relationship between 
the complexities of learning and preference elicitation 
related work zinkevich et al consider the problem 
of learning restricted classes of valuation functions which can 
be represented using read-once formulas and toolbox dnf 
read-once formulas can represent certain substitutabilities 
but no complementarities whereas the opposite holds for 
toolbox dnf since their work is also grounded in learning 
theory they allow dependence on the size of the target 
valuation as we do though read-once valuations can always be 
succinctly represented anyway their work only makes use 
of value queries which are quite limited in power because 
we allow ourselves demand queries we are able to derive an 
elicitation scheme for general valuation functions 
blum et al provide results relating the complexities 
of query learning and preference elicitation they consider 
models with membership and equivalence queries in query 
learning and value and demand queries in preference 
elicitation they show that certain classes of functions can be 
efficiently learned yet not efficiently elicited and vice-versa 
in contrast our work shows that given a more general yet 
still quite standard version of demand query than the type 
they consider the complexity of preference elicitation is no 
greater than the complexity of learning we will show that 
demand queries can simulate equivalence queries until we 
have enough information about valuations to imply a 
solution to the elicitation problem 
nisan and segal study the communication 
complexity of preference elicitation they show that for many rich 
classes of valuations the worst-case communication 
complexity of computing an optimal allocation is exponential 
their results apply to the black-box model of 
computational complexity in this model algorithms are allowed to 
ask questions about agent valuations and receive honest 
responses without any insight into how the agents internally 
compute their valuations this is in fact the basic 
framework of learning theory our work also addresses the issue 
of communication complexity and we are able to derive 
algorithms that provide significant communication guarantees 
despite nisan and segal s negative results their work 
motivates the need to rely on the sizes of agents valuation 
functions in stating worst-case results 
 the models 
 query learning 
the query learning model we consider here is called exact 
learning from membership and equivalence queries 
introduced by angluin in this model the learning 
algorithm s objective is to exactly identify an unknown target 
function f x → y via queries to an oracle the target 
function is drawn from a function class c that is known to 
the algorithm typically the domain x is some subset of 
{ }m 
 and the range y is either { } or some subset 
of the real numbers ê as the algorithm progresses it 
constructs a manifest hypothesis ˜f which is its current estimate 
of the target function upon termination the manifest 
hypothesis of a correct learning algorithm satisfies ˜f x f x 
for all x ∈ x 
it is important to specify the representation that will be 
used to encode functions from c for example consider the 
following function from { }m 
to ê f x if x 
consists of m s and f x otherwise this function may 
simply be represented as a list of m 
values or it may be 
encoded as the polynomial x · · · xm which is much more 
succinct the choice of encoding may thus have a significant 
impact on the time and space requirements of the learning 
algorithm let size f be the size of the encoding of f with 
respect to the given representation class most 
representation classes have a natural measure of encoding size the 
size of a polynomial can be defined as the number of non-zero 
coefficients in the polynomial for example we will usually 
only refer to representation classes the corresponding 
function classes will be implied for example the representation 
class of monotone dnf formulae implies the function class 
of monotone boolean functions 
two types of queries are commonly used for exact 
learning membership and equivalence queries on a membership 
query the learner presents some x ∈ x and the oracle replies 
with f x on an equivalence query the learner presents 
its manifest hypothesis ˜f the oracle either replies  yes if 
˜f f or returns a counterexample x such that ˜f x f x 
an equivalence query is proper if size ˜f ≤ size f at the 
time the manifest hypothesis is presented 
we are interested in efficient learning algorithms the 
following definitions are adapted from kearns and vazirani 
definition the representation class c is 
polynomialquery exactly learnable from membership and 
equivalence queries if there is a fixed polynomial p · · and an 
algorithm l with access to membership and equivalence 
queries of an oracle such that for any target function f ∈ c l 
outputs after at most p size f m queries a function ˜f ∈ c 
such that ˜f x f x for all instances x 
similarly the representation class c is efficiently 
exactly learnable from membership and equivalence 
queries if the algorithm l outputs a correct hypothesis in 
time p size f m for some fixed polynomial p · · 
here m is the dimension of the domain since the target 
function must be reconstructed we also necessarily allow 
polynomial dependence on size f 
 preference elicitation 
in a combinatorial auction a set of goods m is to be 
allocated among a set of agents n so as to maximize the sum of 
the agents valuations such an allocation is called efficient 
in the economics literature but we will refer to it as optimal 
and reserve the term efficient to refer to computational 
efficiency we let n n and m m an allocation 
is a partition of the objects into bundles s sn such 
that si ∩ sj ∅ for all distinct i j ∈ n let γ be the set 
of possible allocations each agent i ∈ n has a valuation 
function vi m 
→ ê over the space of possible bundles 
each valuation vi is drawn from a known class of valuations 
vi the valuation classes do not need to coincide 
we will assume that all the valuations considered are 
normalized meaning v ∅ and that there are no 
externalities meaning vi s sn vi si for all agents i ∈ n 
for any allocation s sn ∈ γ that is an agent cares 
only about the bundle allocated to her valuations 
satisfying these conditions are called general valuations 
we 
 
often general valuations are made to satisfy the additional 
 
also assume that agents have quasi-linear utility functions 
meaning that agents utilities can be divided into monetary 
and non-monetary components if an agent i is allocated 
bundle s at price p it derives utility ui s p vi s − p 
a valuation function may be viewed as a vector of m 
− 
non-negative real-values of course there may also be more 
succinct representations for certain valuation classes and 
there has been much research into concise bidding languages 
for various types of valuations a classic example which 
we will refer to again later is the xor bidding language 
in this language the agent provides a list of atomic bids 
which consist of a bundle together with its value to 
determine the value of a bundle s given these bids one searches 
for the bundle s of highest value listed in the atomic bids 
such that s ⊆ s it is then the case that v s v s 
as in the learning theory setting we will usually only refer 
to bidding languages rather than valuation classes because 
the corresponding valuation classes will then be implied for 
example the xor bidding language implies the class of 
valuations satisfying free-disposal which is the condition that 
a ⊆ b ⇒ v a ≤ v b 
we let size v vn 
èn 
i size vi that is the size 
of a vector of valuations is the size of the concatenation of 
the valuations representations in their respective encoding 
schemes bidding languages 
to make an analogy to computational learning theory we 
assume that all representation classes considered are 
polynomially interpretable meaning that the value of a bundle 
may be computed in polynomial time given the valuation 
function s representation more formally a representation 
class bidding language c is polynomially interpretable if 
there exists an algorithm that given as input some v ∈ c 
and an instance x ∈ x computes the value v x in time 
q size v m for some fixed polynomial q · · 
in the intermediate rounds of an iterative auction the 
auctioneer will have elicited information about the agents 
valuation functions via various types of queries she will 
thus have constructed a set of manifest valuations denoted 
˜v ˜vn 
the values of these functions may correspond 
exactly to the true agent values or they may for example 
be upper or lower bounds on the true values depending 
on the types of queries made they may also simply be 
default or random values if no information has been acquired 
about certain bundles the goal in the preference elicitation 
problem is to construct a set of manifest valuations such 
that 
arg max 
 s sn ∈γ 
i∈n 
˜vi si ⊆ arg max 
 s sn ∈γ 
i∈n 
vi si 
that is the manifest valuations provide enough information 
to compute an allocation that is optimal with respect to the 
true valuations note that we only require one such optimal 
allocation 
condition of free-disposal monotonicity but we do not 
need it at this point 
 
this excludes or∗ 
 assuming p np because 
interpreting bids from this language is np-hard by reduction from 
weighted set-packing and there is no well-studied 
representation class in learning theory that is clearly analogous to 
or∗ 
 
 
this view of iterative auctions is meant to parallel the 
learning setting in many combinatorial auctions manifest 
valuations are not explicitly maintained but rather simply 
implied by the history of bids 
two typical queries used in preference elicitation are value 
and demand queries on a value query the auctioneer 
presents a bundle s ⊆ m and the agent responds with 
her exact value for the bundle v s on a demand 
query the auctioneer presents a vector of non-negative prices 
p ∈ ê m 
over the bundles together with a bundle s the 
agent responds  yes if it is the case that 
s ∈ arg max 
s ⊆m 
 
v s − p s 
 
or otherwise presents a bundle s such that 
v s − p s v s − p s 
that is the agent either confirms that the presented bundle 
is most preferred at the quoted prices or indicates a 
better one 
note that we include ∅ as a bundle so the 
agent will only respond  yes if its utility for the proposed 
bundle is non-negative note also that communicating 
nonlinear prices does not necessarily entail quoting a price for 
every possible bundle there may be more succinct ways of 
communicating this vector as we show in section 
we make the following definitions to parallel the query 
learning setting and to simplify the statements of later 
results 
definition the representation classes v vn can 
be polynomial-query elicited from value and demand 
queries if there is a fixed polynomial p · · and an 
algorithm l with access to value and demand queries of the 
agents such that for any v vn ∈ v × × vn l 
outputs after at most p size v vn m queries an 
allocation s sn ∈ arg max s sn ∈γ 
è 
vi si 
similarly the representation class c can be efficiently 
elicited from value and demand queries if the 
algorithm l outputs an optimal allocation with communication 
p size v vn m for some fixed polynomial p · · 
there are some key differences here with the query 
learning definition we have dropped the term exactly since 
the valuation functions need not be determined exactly in 
order to compute an optimal allocation also an efficient 
elicitation algorithm is polynomial communication rather 
than polynomial time this reflects the fact that 
communication rather than runtime is the bottleneck in elicitation 
computing an optimal allocation of goods even when given 
the true valuations is np-hard for a wide range of 
valuation classes it is thus unreasonable to require polynomial 
time in the definition of an efficient preference elicitation 
algorithm we are happy to focus on the communication 
complexity of elicitation because this problem is widely 
believed to be more significant in practice than that of winner 
determination 
 
this differs slightly from the definition provided by blum 
et al their demand queries are restricted to linear prices 
over the goods where the price of a bundle is the sum of 
the prices of its underlying goods in contrast our demand 
queries allow for nonlinear prices i e a distinct price for 
every possible bundle this is why the lower bound in their 
theorem does not contradict our result that follows 
 
though the winner determination problem is np-hard for 
general valuations there exist many algorithms that solve 
it efficiently in practice these range from special 
purpose algorithms to approaches using off-the-shelf ip 
solvers 
 
since the valuations need not be elicited exactly it is 
initially less clear whether the polynomial dependence on 
size v vn is justified in this setting intuitively this 
parameter is justified because we must learn valuations 
exactly when performing elicitation in the worst-case we 
address this in the next section 
 parallelsbetween equivalence 
and demand queries 
we have described the query learning and preference 
elicitation settings in a manner that highlights their similarities 
value and membership queries are clear analogs slightly 
less obvious is the fact that equivalence and demand queries 
are also analogs to see this we need the concept of lindahl 
prices lindahl prices are nonlinear and non-anonymous 
prices over the bundles they are nonlinear in the sense 
that each bundle is assigned a price and this price is not 
necessarily the sum of prices over its underlying goods they 
are non-anonymous in the sense that two agents may face 
different prices for the same bundle of goods thus lindahl 
prices are of the form pi s for all s ⊆ m for all i ∈ n 
lindahl prices are presented to the agents in demand 
queries 
when agents have normalized quasi-linear utility 
functions bikhchandani and ostroy show that there always 
exist lindahl prices such that s sn is an optimal 
allocation if and only if 
si ∈ arg max 
si 
 
vi si − pi si 
 
∀i ∈ n 
 s sn ∈ arg max 
 s sn ∈γ 
i∈n 
pi si 
condition states that each agent is allocated a bundle 
that maximizes its utility at the given prices condition 
states that the allocation maximizes the auctioneer s 
revenue at the given prices the scenario in which these 
conditions hold is called a lindahl equilibrium or often a 
competitive equilibrium we say that the lindahl prices support 
the optimal allocation it is therefore sufficient to announce 
supporting lindahl prices to verify an optimal allocation 
once we have found an allocation with supporting lindahl 
prices the elicitation problem is solved 
the problem of finding an optimal allocation with respect 
to the manifest valuations can be formulated as a linear 
program whose solutions are guaranteed to be integral 
the dual variables to this linear program are supporting 
lindahl prices for the resulting allocation the objective 
function to the dual program is 
min 
pi s 
πs 
 
i∈n 
πi 
with πi max 
s⊆m 
 ˜vi s − pi s 
πs 
 max 
 s sn ∈γ 
i∈n 
pi si 
the optimal values of πi and πs 
correspond to the maximal 
utility to agent i with respect to its manifest valuation and 
the maximal revenue to the seller 
there is usually a range of possible lindahl prices 
supporting a given optimal allocation the agent s manifest 
valuations are in fact valid lindahl prices and we refer to 
them as maximal lindahl prices out of all possible 
vectors of lindahl prices maximal lindahl prices maximize the 
utility of the auctioneer in fact giving her the entire 
social welfare conversely prices that maximize the 
è 
i∈n πi 
component of the objective the sum of the agents utilities 
are minimal lindahl prices any lindahl prices will do for 
our results but some may have better elicitation 
properties than others note that a demand query with maximal 
lindahl prices is almost identical to an equivalence query 
since in both cases we communicate the manifest valuation 
to the agent we leave for future work the question of which 
lindahl prices to choose to minimize preference elicitation 
considering now why demand and equivalence queries are 
direct analogs first note that given the πi in some lindahl 
equilibrium setting 
pi s max{ ˜vi s − πi} 
for all i ∈ n and s ⊆ m yields valid lindahl prices these 
prices leave every agent indifferent across all bundles with 
positive price and satisfy condition thus demand 
queries can also implicitly communicate manifest valuations 
since lindahl prices will typically be an additive constant 
away from these by equality in the following lemma we 
show how to obtain counterexamples to equivalence queries 
through demand queries 
lemma suppose an agent replies with a preferred 
bundle s when proposed a bundle s and supporting lindahl 
prices p s supporting with respect to the the agent s 
manifest valuation then either ˜v s v s or ˜v s v s 
proof we have the following inequalities 
˜v s − p s ≥ ˜v s − p s 
⇒ ˜v s − ˜v s ≤ p s − p s 
v s − p s v s − p s 
⇒ v s − v s p s − p s 
inequality holds because the prices support the proposed 
allocation with respect to the manifest valuation inequality 
 holds because the agent in fact prefers s to s given the 
prices according to its response to the demand query if it 
were the case that ˜v s v s and ˜v s v s these 
inequalities would represent a contradiction thus at least 
one of s and s is a counterexample to the agent s manifest 
valuation 
finally we justify dependence on size v vn in 
elicitation problems nisan and segal proposition 
and parkes theorem show that supporting lindahl 
prices must necessarily be revealed in the course of any 
preference elicitation protocol which terminates with an optimal 
allocation furthermore nisan and segal lemma 
state that in the worst-case agents prices must coincide with 
their valuations up to a constant when the valuation class 
is rich enough to contain dual valuations as will be the 
case with most interesting classes since revealing lindahl 
prices is a necessary condition for establishing an optimal 
allocation and since lindahl prices contain the same 
information as valuation functions in the worst-case allowing 
for dependence on size v vn in elicitation problems is 
entirely natural 
 
 from learning to preference 
elicitation 
the key to converting a learning algorithm to an 
elicitation algorithm is to simulate equivalence queries with 
demand and value queries until an optimal allocation is found 
because of our lindahl price construction when all agents 
reply  yes to a demand query we have found an optimal 
allocation analogous to the case where an agent replies  yes 
to an equivalence query when the target function has been 
exactly learned otherwise we can obtain a 
counterexample to an equivalence query given an agent s response to a 
demand query 
theorem the representation classes v vn can 
be polynomial-query elicited from value and demand queries 
if they can each be polynomial-query exactly learned from 
membership and equivalence queries 
proof consider the elicitation algorithm in figure 
each membership query in step is simulated with a value 
query since these are in fact identical consider step if all 
agents reply  yes condition holds condition holds 
because the computed allocation is revenue-maximizing for 
the auctioneer regardless of the agents true valuations 
thus an optimal allocation has been found otherwise at 
least one of si or si is a counterexample to ˜vi by lemma 
we identify a counterexample by performing value queries 
on both these bundles and provide it to ai as a response to 
its equivalence query 
this procedure will halt since in the worst-case all agent 
valuations will be learned exactly in which case the 
optimal allocation and lindahl prices will be accepted by all 
agents the procedure performs a polynomial number of 
queries since a an are all polynomial-query learning 
algorithms 
note that the conversion procedure results in a 
preference elicitation algorithm not a learning algorithm that 
is the resulting algorithm does not simply learn the 
valuations exactly then compute an optimal allocation rather 
it elicits partial information about the valuations through 
value queries and periodically tests whether enough 
information has been gathered by proposing an allocation to the 
agents through demand queries it is possible to generate a 
lindahl equilibrium for valuations v vn using an 
allocation and prices derived using manifest valuations ˜v ˜vn 
and finding an optimal allocation does not imply that the 
agents valuations have been exactly learned the use of 
demand queries to simulate equivalence queries enables this 
early halting we would not obtain this property with 
equivalence queries based on manifest valuations 
 communication complexity 
in this section we turn to the issue of the 
communication complexity of elicitation nisan and segal show 
that for a variety of rich valuation spaces such as general 
and submodular valuations the worst-case communication 
burden of determining lindahl prices is exponential in the 
number of goods m the communication burden is 
measured in terms of the number of bits transmitted between 
agents and auctioneer in the case of discrete communication 
or in terms of the number of real numbers transmitted in the 
case of continuous communication 
converting efficient learning algorithms to an elicitation 
algorithm produces an algorithm whose queries have sizes 
polynomial in the parameters m and size v vn 
theorem the representation classes v vn can 
be efficiently elicited from value and demand queries if they 
can each be efficiently exactly learned from membership and 
equivalence queries 
proof the size of any value query is o m the 
message consists solely of the queried bundle to communicate 
lindahl prices to agent i it is sufficient to communicate 
the agent s manifest valuation function and the value πi by 
equality note that an efficient learning algorithm never 
builds up a manifest hypothesis of superpolynomial size 
because the algorithm s runtime would then also be 
superpolynomial contradicting efficiency thus communicating the 
manifest valuation requires size at most p size vi m for 
some polynomial p that upper-bounds the runtime of the 
efficient learning algorithm representing the surplus πi to 
agent i cannot require space greater than q size ˜vi m for 
some fixed polynomial q because we assume that the chosen 
representation is polynomially interpretable and thus any 
value generated will be of polynomial size we must also 
communicate to i its allocated bundle so the total 
message size for a demand query is at most p size vi m 
q p size vi m m o m clearly an agent s response to 
a value or demand query has size at most q size vi m 
o m thus the value and demand queries and the 
responses to these queries are always of polynomial size an 
efficient learning algorithm performs a polynomial number of 
queries so the total communication of the resulting 
elicitation algorithm is polynomial in the relevant parameters 
there will often be explicit bounds on the number of 
membership and equivalence queries performed by a learning 
algorithm with constants that are not masked by big-o 
notation these bounds can be translated to explicit bounds 
on the number of value and demand queries made by the 
resulting elicitation algorithm we upper-bounded the size 
of the manifest hypothesis with the runtime of the learning 
algorithm in theorem we are likely to be able to do 
much better than this in practice recall that an 
equivalence query is proper if size ˜f ≤ size f at the time the 
query is made if the learning algorithm s equivalence 
queries are all proper it may then also be possible to provide 
tight bounds on the communication requirements of the 
resulting elicitation algorithm 
theorem show that elicitation algorithms that depend 
on the size v vn parameter sidestep nisan and 
segal s negative results on the worst-case communication 
complexity of efficient allocation problems they provide 
guarantees with respect to the sizes of the instances of 
valuation functions faced at any run of the algorithm these 
algorithms will fare well if the chosen representation class 
provides succinct representations for the simplest and most 
common of valuations and thus the focus moves back to one 
of compact yet expressive bidding languages we consider 
these issues below 
 applications 
in this section we demonstrate the application of our 
methods to particular representation classes for 
combinatorial valuations we have shown that the preference 
elicitation problem for valuation classes v vn can be reduced 
 
given exact learning algorithms a an for valuations classes v vn respectively 
loop until there is a signal to halt 
 run a an in parallel on their respective agents until each requires a response to an 
equivalence query or has halted with the agent s exact valuation 
 compute an optimal allocation s sn and corresponding lindahl prices with respect to 
the manifest valuations ˜v ˜vn determined so far 
 present the allocation and prices to the agents in the form of a demand query 
 if they all reply  yes output the allocation and halt otherwise there is some agent i that 
has replied with some preferred bundle si perform value queries on si and si to find a 
counterexample to ˜vi and provide it to ai 
figure converting learning algorithms to an elicitation algorithm 
to the problem of finding an efficient learning algorithm for 
each of these classes separately this is significant because 
there already exist learning algorithms for a wealth of 
function classes and because it may often be simpler to solve 
each learning subproblem separately than to attack the 
preference elicitation problem directly we can develop an 
elicitation algorithm that is tailored to each agent s valuation 
with the underlying learning algorithms linked together at 
the demand query stages in an algorithm-independent way 
we show that existing learning algorithms for 
polynomials monotone dnf formulae and linear-threshold functions 
can be converted into preference elicitation algorithms for 
general valuations valuations with free-disposal and 
valuations with substitutabilities respectively we focus on 
representations that are polynomially interpretable because 
the computational learning theory literature places a heavy 
emphasis on computational tractability 
in interpreting the methods we emphasize the 
expressiveness and succinctness of each representation class the 
representation class which in combinatorial auction terms 
defines a bidding language must necessarily be expressive 
enough to represent all possible valuations of interest and 
should also succinctly represent the simplest and most 
common functions in the class 
 polynomial representations 
schapire and sellie give a learning algorithm for sparse 
multivariate polynomials that can be used as the basis for 
a combinatorial auction protocol the equivalence queries 
made by this algorithm are all proper specifically their 
algorithm learns the representation class of t-sparse 
multivariate polynomials over the real numbers where the variables 
may take on values either or a t-sparse polynomial 
has at most t terms where a term is a product of variables 
e g x x x a polynomial over the real numbers has 
coefficients drawn from the real numbers polynomials are 
expressive every valuation function v m 
→ ê can be 
uniquely written as a polynomial 
to get an idea of the succinctness of polynomials as a 
bidding language consider the additive and single-item 
valuations presented by nisan in the additive valuation 
the value of a bundle is the number of goods the bundle 
contains in the single-item valuation all bundles have value 
 except ∅ which has value i e the agent is satisfied as 
soon as it has acquired a single item it is not hard to show 
that the single-item valuation requires polynomials of size 
 m 
− while polynomials of size m suffice for the additive 
valuation polynomials are thus appropriate for valuations 
that are mostly additive with a few substitutabilities and 
complementarities that can be introduced by adjusting 
coefficients 
the learning algorithm for polynomials makes at most 
mti equivalence queries and at most mti t 
i ti 
membership queries to an agent i where ti is the sparcity of 
the polynomial representing vi we therefore obtain an 
algorithm that elicits general valuations with a polynomial 
number of queries and polynomial communication 
 xor representations 
the xor bidding language is standard in the 
combinatorial auctions literature recall that an xor bid is 
characterized by a set of bundles b ⊆ m 
and a value function 
w b → ê defined on those bundles which induces the 
valuation function 
v b max 
{b ∈b b ⊆b} 
w b 
xor bids can represent valuations that satisfy free-disposal 
 and only such valuations which again is the property that 
a ⊆ b ⇒ v a ≤ v b 
the xor bidding language is slightly less expressive than 
polynomials because polynomials can represent valuations 
that do not satisfy free-disposal however xor is as 
expressive as required in most economic settings nisan notes 
that xor bids can represent the single-item valuation with 
m atomic bids but m 
− atomic bids are needed to 
represent the additive valuation since the opposite holds for 
polynomials these two languages are incomparable in 
succinctness and somewhat complementary for practical use 
blum et al note that monotone dnf formulae are the 
analogs of xor bids in the learning theory literature a 
monotone dnf formula is a disjunction of conjunctions in 
which the variables appear unnegated for example x x ∨ 
x ∨ x x x note that such formulae can be represented 
as xor bids where each atomic bid has value thus xor 
bids generalize monotone dnf formulae from boolean to 
real-valued functions these insights allow us to generalize 
a classic learning algorithm for monotone dnf theorem 
 
note that theorem applies even if valuations do not 
satisfy free-disposal 
 
 theorem b to a learning algorithm for xor bids 
lemma an xor bid containing t atomic bids can be 
exactly learned with t equivalence queries and at most 
tm membership queries 
proof the algorithm will identify each atomic bid in 
the target xor bid in turn initialize the manifest valuation 
˜v to the bid that is identically zero on all bundles this is an 
xor bid containing atomic bids present ˜v as an 
equivalence query if the response is  yes we are done otherwise 
we obtain a bundle s for which v s ˜v s create a 
bundle t as follows first initialize t s for each item i in t 
check via a membership query whether v t v t − {i} 
if so set t t − {i} otherwise leave t as is and proceed 
to the next item 
we claim that t v t is an atomic bid of the target 
xor bid for each item i in t we have v t v t − {i} 
to see this note that at some point when generating t we 
had a ¯t such that t ⊆ ¯t ⊆ s and v ¯t v ¯t − {i} so that 
i was kept in ¯t note that v s v ¯t v t because the 
value of the bundle s is maintained throughout the process 
of deleting items now assume v t v t − {i} then 
v ¯t v t v t − {i} v ¯t − {i} 
which contradicts free-disposal since t − {i} ⊆ ¯t − {i} 
thus v t v t − {i} for all items i in t this implies 
that t v t is an atomic bid of v if this were not the case 
t would take on the maximum value of its strict subsets by 
the definition of an xor bid and we would have 
v t max 
i∈t 
{ max 
t ⊆t −{i} 
v t } max 
i∈t 
{v t − {i} } v t 
which is a contradiction 
we now show that v t ˜v t which will imply that 
 t v t is not an atomic bid of our manifest hypothesis by 
induction assume that every atomic bid r ˜v r 
identified so far is indeed an atomic bid of v meaning r is indeed 
listed in an atomic bid of v as having value v r ˜v r 
this assumption holds vacuously when the manifest 
valuation is initialized using the notation from let ˜b ˜w be 
our hypothesis and b w be the target function we have 
˜b ⊆ b and ˜w b w b for b ∈ ˜b by assumption thus 
˜v s max 
{b∈ ˜b b⊆s} 
˜w b 
 max 
{b∈ ˜b b⊆s} 
w b 
≤ max 
{b∈b b⊆s} 
w b 
 v s 
now assume v t ˜v t then 
˜v t v t v s ˜v s 
the second equality follows from the fact that the value 
remains constant when we derive t from s the last 
inequality holds because s is a counterexample to the 
manifest valuation from equation and free-disposal we 
 
the cited algorithm was also used as the basis for zinkevich 
et al s elicitation algorithm for toolbox dnf recall 
that toolbox dnf are polynomials with non-negative 
coefficients for these representations an equivalence query can 
be simulated with a value query on the bundle containing 
all goods 
have ˜v t ˜v s then again from equation it 
follows that v s ˜v s this contradicts so we in fact 
have v t ˜v t thus t v t is not currently in our 
hypothesis as an atomic bid or we would correctly have 
˜v t v t by the induction hypothesis we add t v t 
to our hypothesis and repeat the process above 
performing additional equivalence queries until all atomic bids have 
been identified 
after each equivalence query an atomic bid is identified 
with at most m membership queries each counterexample 
leads to the discovery of a new atomic bid thus we make at 
most tm membership queries and exactly t equivalence 
queries 
the number of time steps required by this algorithm is 
essentially the same as the number of queries performed so 
the algorithm is efficient applying theorem we therefore 
obtain the following corollary 
theorem the representation class of xor bids can 
be efficiently elicited from value and demand queries 
this contrasts with blum et al s negative results 
theorem stating that monotone dnf and hence xor bids 
cannot be efficiently elicited when the demand queries are 
restricted to linear and anonymous prices over the goods 
 linear-threshold representations 
polynomials xor bids and all languages based on the 
or bidding language such as xor-of-or or-of-xor 
and or∗ 
 fail to succinctly represent the majority 
valuation in this valuation bundles have value if they 
contain at least m items and value otherwise more 
generally consider the r-of-s family of valuations where bundles 
have value if they contain at least r items from a specified 
set of items s ⊆ m and value otherwise the 
majority valuation is a special case of the r-of-s valuation with 
r m and s m these valuations are appropriate for 
representing substitutabilities once a required set of items 
has been obtained no other items can add value 
letting k s such valuations are succinctly represented 
by r-of-k threshold functions these functions take the form 
of linear inequalities 
xi xik ≥ r 
where the function has value if the inequality holds and 
 otherwise here i ik are the items in s littlestone s 
winnow algorithm can learn such functions using 
equivalence queries only using at most r 
 k kr ln m 
queries to provide this guarantee r must be known to 
the algorithm but s and k are unknown the elicitation 
algorithm that results from winnow uses demand 
queries only value queries are not necessary here because the 
values of counterexamples are implied when there are only 
two possible values 
note that r-of-k threshold functions can always be 
succinctly represented in o m space thus we obtain an 
algorithm that can elicit such functions with a polynomial 
number of queries and polynomial communication in the 
parameters n and m alone 
 
 conclusions and future work 
we have shown that exact learning algorithms with 
membership and equivalence queries can be used as a basis for 
preference elicitation algorithms with value and demand 
queries at the heart of this result is the fact that demand 
queries may be viewed as modified equivalence queries 
specialized to the problem of preference elicitation our result 
allows us to apply the wealth of available learning algorithms 
to the problem of preference elicitation 
a learning approach to elicitation also motivates a 
different approach to designing elicitation algorithms that 
decomposes neatly across agent types if the designer knowns 
beforehand what types of preferences each agent is likely to 
exhibit mostly additive many substitutes etc she can 
design learning algorithms tailored to each agents 
valuations and integrate them into an elicitation scheme the 
resulting elicitation algorithm makes a polynomial number 
of queries and makes polynomial communication if the 
original learning algorithms are efficient 
we do not require that agent valuations can be learned 
with value and demand queries equivalence queries can 
only be and need only be simulated up to the point where 
an optimal allocation has been computed this is the 
preference elicitation problem theorem implies that elicitation 
with value and demand queries is no harder than learning 
with membership and equivalence queries but it does not 
provide any asymptotic improvements over the learning 
algorithms complexity it would be interesting to find 
examples of valuation classes for which elicitation is easier than 
learning blum et al provide such an example when 
considering membership value queries only theorem 
in future work we plan to address the issue of 
incentives when converting learning algorithms to elicitation 
algorithms in the learning setting we usually assume that 
oracles will provide honest responses to queries in the 
elicitation setting agents are usually selfish and will provide 
possibly dishonest responses so as to maximize their utility 
we also plan to implement the algorithms for learning 
polynomials and xor bids as elicitation algorithms and test 
their performance against other established combinatorial 
auction protocols an interesting question here is 
which lindahl prices in the maximal to minimal range are 
best to quote in order to minimize information revelation 
we conjecture that information revelation is reduced when 
moving from maximal to minimal lindahl prices namely as 
we move demand queries further away from equivalence 
queries finally it would be useful to determine whether the 
or∗ 
bidding language can be efficiently learned and 
hence elicited given this language s expressiveness and 
succinctness for a wide variety of valuation classes 
acknowledgements 
we would like to thank debasis mishra for helpful 
discussions this work is supported in part by nsf grant 
iis 
 references 
 a andersson m tenhunen and f ygge integer 
programming for combinatorial auction winner 
determination in proceedings of the fourth 
international conference on multiagent systems 
 icmas- 
 d angluin learning regular sets from queries and 
counterexamples information and computation 
 - november 
 d angluin queries and concept learning machine 
learning - 
 s bikhchandani and j ostroy the package 
assignment model journal of economic theory 
 december 
 a blum j jackson t sandholm and m zinkevich 
preference elicitation and query learning in proc 
 th annual conference on computational learning 
theory colt washington dc 
 w conen and t sandholm partial-revelation vcg 
mechanism for combinatorial auctions in proc the 
 th national conference on artificial intelligence 
 aaai 
 y fujishima k leyton-brown and y shoham 
taming the computational complexity of 
combinatorial auctions optimal and approximate 
approaches in proc the th international joint 
conference on artificial intelligence ijcai pages 
 - 
 b hudson and t sandholm using value queries in 
combinatorial auctions in proc th acm conference 
on electronic commerce acm-ec san diego ca 
june 
 m j kearns and u v vazirani an introduction to 
computational learning theory mit press 
 n littlestone learning quickly when irrelevant 
attributes abound a new linear-threshold algorithm 
machine learning - 
 n nisan bidding and allocation in combinatorial 
auctions in proc the acm conference on electronic 
commerce pages - 
 n nisan and i segal the communication 
requirements of efficient allocations and supporting 
lindahl prices working paper hebrew university 
 
 d c parkes price-based information certificates for 
minimal-revelation combinatorial auctions in 
padget et al editor agent-mediated electronic 
commerce iv lnai pages - 
springer-verlag 
 d c parkes auction design with costly preference 
elicitation in special issues of annals of mathematics 
and ai on the foundations of electronic commerce 
forthcoming 
 d c parkes and l h ungar iterative combinatorial 
auctions theory and practice in proc th national 
conference on artificial intelligence aaai- pages 
 - 
 t sandholm s suri a gilpin and d levine 
cabob a fast optimal algorithm for combinatorial 
auctions in proc the th international joint 
conference on artificial intelligence ijcai pages 
 - 
 r schapire and l sellie learning sparse multivariate 
polynomials over a field with queries and 
counterexamples in proceedings of the sixth annual 
acm workshop on computational learning theory 
pages - acm press 
 
 l valiant a theory of the learnable commun acm 
 - nov 
 m zinkevich a blum and t sandholm on 
polynomial-time preference elicitation with 
value-queries in proc th acm conference on 
electronic commerce acm-ec san diego ca 
june 
 
towards truthful mechanisms for binary demand games 
a general framework 
ming-yang kao 
∗ 
dept of computer science 
northwestern university 
evanston il usa 
kao cs northwestern edu 
xiang-yang li 
† 
dept of computer science 
illinois institute of technology 
chicago il usa 
xli cs iit edu 
weizhao wang 
dept of computer science 
illinois institute of technology 
chicago il usa 
wangwei  iit edu 
abstract 
the family of vickrey-clarke-groves vcg mechanisms is 
arguably the most celebrated achievement in truthful mechanism 
design however vcg mechanisms have their limitations they only 
apply to optimization problems with a utilitarian or affine 
objective function and their output should optimize the objective 
function for many optimization problems finding the optimal output 
is computationally intractable if we apply vcg mechanisms to 
polynomial-time algorithms that approximate the optimal solution 
the resulting mechanisms may no longer be truthful 
in light of these limitations it is useful to study whether we can 
design a truthful non-vcg payment scheme that is computationally 
tractable for a given allocation rule o in this paper we focus our 
attention on binary demand games in which the agents only 
available actions are to take part in the a game or not to for these 
problems we prove that a truthful mechanism m o p exists with 
a proper payment method p iff the allocation rule o satisfies a 
certain monotonicity property we provide a general framework to 
design such p we further propose several general composition-based 
techniques to compute p efficiently for various types of output 
in particular we show how p can be computed through or and 
combinations round-based combinations and some more complex 
combinations of the outputs from subgames 
categories and subject descriptors 
f analysis of algorithms and problem complexity 
general j social and behavioral sciences economics k 
 computer and society electronic commerce 
general terms 
algorithms economics theory 
 introduction 
in recent years with the rapid development of the internet many 
protocols and algorithms have been proposed to make the internet 
more efficient and reliable the internet is a complex distributed 
system where a multitude of heterogeneous agents cooperate to 
achieve some common goals and the existing protocols and 
algorithms often assume that all agents will follow the prescribed rules 
without deviation however in some settings where the agents are 
selfish instead of altruistic it is more reasonable to assume these 
agents are rational - maximize their own profits - according to the 
neoclassic economics and new models are needed to cope with the 
selfish behavior of such agents 
towards this end nisan and ronen proposed the framework 
of algorithmic mechanism design and applied vcg mechanisms to 
some fundamental problems in computer science including 
shortest paths minimum spanning trees and scheduling on unrelated 
machines the vcg mechanisms are applicable to 
mechanism design problems whose outputs optimize the 
utilitarian objective function which is simply the sum of all agents 
valuations unfortunately some objective functions are not utilitarian 
even for those problems with a utilitarian objective function 
sometimes it is impossible to find the optimal output in polynomial time 
unless p np some mechanisms other than vcg mechanism are 
needed to address these issues 
archer and tardos studied a scheduling problem where it 
is np-hard to find the optimal output they pointed out that a 
certain monotonicity property of the output work load is a 
necessary and sufficient condition for the existence of a truthful 
mechanism for their scheduling problem auletta et al studied a 
similar scheduling problem they provided a family of 
deterministic truthful -approximation mechanisms for any fixed 
number of machines and several -truthful mechanisms for some 
np-hard restrictions of their scheduling problem lehmann et al 
 studied the single-minded combinatorial auction and gave a√ 
m-approximation truthful mechanism where m is the number of 
goods they also pointed out that a certain monotonicity in the 
allocation rule can lead to a truthful mechanism the work of mu alem 
and nisan is the closest in spirit to our work they 
characterized all truthful mechanisms based on a certain monotonicity 
property in a single-minded auction setting they also showed how to 
used max and if-then-else to combine outputs from 
subproblems as shown in this paper the max and if-then-else 
combinations are special cases of the composition-based techniques 
that we present in this paper for computing the payments in 
polynomial time under mild assumptions 
more generally we study how to design truthful mechanisms for 
binary demand games where the allocation of an agent is either 
selected or not selected we also assume that the valuations 
 
of agents are uncorrelated i e the valuation of an agent only 
depends on its own allocation and type recall that a mechanism 
m o p consists of two parts an allocation rule o and a 
payment scheme p previously it is often assumed that there is an 
objective function g and an allocation rule o that either optimizes 
g exactly or approximately in contrast to the vcg mechanisms 
we do not require that the allocation should optimize the objective 
function in fact we do not even require the existence of an 
objective function given any allocation rule o for a binary demand 
game we showed that a truthful mechanism m o p exists 
for the game if and only if o satisfies a certain monotonicity 
property the monotonicity property only guarantees the existence of a 
payment scheme p such that o p is truthful we complement 
this existence theorem with a general framework to design such a 
payment scheme p furthermore we present general techniques 
to compute the payment when the output is a composition of the 
outputs of subgames through the operators or and and through 
round-based combinations or through intermediate results which 
may be themselves computed from other subproblems 
the remainder of the paper is organized as follows in section 
 we discuss preliminaries and previous works define binary 
demand games and discuss the basic assumptions about binary 
demand games in section we show that o satisfying a certain 
monotonicity property is a necessary and sufficient condition for 
the existence of a truthful mechanism m o p a framework 
is then proposed in section to compute the payment p in 
polynomial time for several types of allocation rules o in section we 
provide several examples to demonstrate the effectiveness of our 
general framework we conclude our paper in section with some 
possible future directions 
 preliminaries 
 mechanism design 
as usually done in the literatures about the designing of 
algorithms or protocols with inputs from individual agents we adopt 
the assumption in neoclassic economics that all agents are rational 
i e they respond to well-defined incentives and will deviate from 
the protocol only if the deviation improves their gain 
a standard model for mechanism design is as follows there 
are n agents n and each agent i has some private 
information ti called its type only known to itself for example the type 
ti can be the cost that agent i incurs for forwarding a packet in 
a network or can be a payment that the agent is willing to pay 
for a good in an auction the agents types define the type 
vector t t t tn each agent i has a set of strategies ai 
from which it can choose for each input vector a a an 
where agent i plays strategy ai ∈ ai the mechanism m o p 
computes an output o o a and a payment vector p a 
 p a pn a here the payment pi · is the money given to 
agent i and depends on the strategies used by the agents a game 
is defined as g s m where s is the setting for the game 
g here s consists the parameters of the game that are set before 
the game starts and do not depend on the players strategies for 
example in a unicast routing game the setting consists of the 
topology of the network the source node and the destination node 
throughout this paper unless explicitly mentioned otherwise the 
setting s of the game is fixed and we are only interested in how to 
design p for a given allocation rule o 
a valuation function v ti o assigns a monetary amount to agent 
i for each possible output o everything about a game s m 
including the setting s the allocation rule o and the payment scheme 
p is public knowledge except the agent i s actual type ti which 
is private information to agent i let ui ti o denote the utility of 
agent i at the outcome of the game o given its preferences ti here 
following a common assumption in the literature we assume the 
utility for agent i is quasi-linear i e ui ti o v ti o pi a 
let a i 
ai a · · · ai− ai ai · · · an i e each agent 
j i plays an action aj except that the agent i plays ai let a−i 
 a · · · ai− ai · · · an denote the actions of all agents 
except i sometimes we write a−i bi as a i 
bi an action ai is 
called dominant for i if it weakly maximizes the utility of i for all 
possible strategies b−i of other agents i e ui ti o b−i ai ≥ 
ui ti o b−i ai for all ai ai and b−i 
a direct-revelation mechanism is a mechanism in which the only 
actions available to each agent are to report its private type either 
truthfully or falsely to the mechanism an incentive compatible 
 ic mechanism is a direct-revelation mechanism in which if an 
agent reports its type ti truthfully then it will maximize its 
utility then in a direct-revelation mechanism satisfying ic the 
payment scheme should satisfy the property that for each agent i 
v ti o t pi t ≥ v ti o t i 
ti pi t i 
ti another 
common requirement in the literature for mechanism design is so called 
individual rationality or voluntary participation the agent s utility 
of participating in the output of the mechanism is not less than the 
utility of the agent of not participating a direct-revelation 
mechanism is strategproof if it satisfies both ic and ir properties 
arguably the most important positive result in mechanism 
design is the generalized vickrey-clarke-groves vcg mechanism 
by vickrey clarke and groves the vcg 
mechanism applies to affine maximization problems where the 
objective function is utilitarian g o t 
p 
i v ti o i e the sum of 
all agents valuations and the set of possible outputs is assumed 
to be finite a direct revelation mechanism m o t p t 
belongs to the vcg family if the allocation o t maximizesp 
i v ti o and the payment to agent i is pi t 
p 
j i vj tj o t 
hi 
 t−i where hi 
 is an arbitrary function of t−i under mild 
assumptions vcg mechanisms are the only truthful implementations 
for utilitarian problems 
the allocation rule of a vcg mechanism is required to 
maximize the objective function in the range of the allocation function 
this makes the mechanism computationally intractable in many 
cases furthermore replacing an optimal algorithm for 
computing the output with an approximation algorithm usually leads to 
untruthful mechanisms if a vcg payment scheme is used in this 
paper we study how to design a truthful mechanism that does not 
optimize a utilitarian objective function 
 binary demand games 
a binary demand game is a game g s m where m 
 o p and the range of o is { }n 
 in other words the 
output is a n-tuple vector o t o t o t on t where 
oi t respectively means that agent i is respectively is 
not selected examples of binary demand games include unicast 
 and multicast generally subgraph 
construction by selecting some links nodes to satisfy some property 
facility location and a certain auction 
hereafter we make the following further assumptions 
 the valuation of the agents are not correlated i e v ti o is 
a function of v ti oi only is denoted as v ti oi 
 the valuation v ti oi is a publicly known value and is 
normalized to this assumption is needed to guarantee the ir 
property 
thus throughout his paper we only consider these direct-revelation 
mechanisms in which every agent only needs to reveal its valuation 
vi v ti 
 
notice that in applications where agents providing service and 
receiving payment e g unicast and job scheduling the valuation 
vi of an agent i is usually negative for the convenience of 
presentation we define the cost of agent as ci −v ti i e it costs 
agent i ci to provide the service throughout this paper we will 
use ci instead of vi in our analysis all our results can apply to 
the case where the agents receive the service rather than provide by 
setting ci to negative as in auction 
in a binary demand game if we want to optimize an 
objective function g o t then we call it a binary optimization demand 
game the main differences between the binary demand games and 
those problems that can be solved by vcg mechanisms are 
 the objective function is utilitarian or affine maximization 
problem for a problem solvable by vcg while there is no 
restriction on the objective function for a binary demand game 
 the allocation rule o studied here does not necessarily 
optimize an objective function while a vcg mechanism only 
uses the output that optimizes the objective function we 
even do not require the existence of an objective function 
 we assume that the agents valuations are not correlated in 
a binary demand game while the agents valuations may be 
correlated in a vcg mechanism 
in this paper we assume for technical convenience that the 
objective function g o c if exists is continuous with respect to the 
cost ci but most of our results are directly applicable to the discrete 
case without any modification 
 previous work 
lehmann et al studied how to design an efficient truthful 
mechanism for single-minded combinatorial auction in a 
singleminded combinatorial auction each agent i ≤ i ≤ n only wants 
to buy a subset si ⊆ s with private price ci a single-minded 
bidder i declares a bid bi si ai with si ⊆ s and ai ∈ r 
 
in it is assumed that the set of goods allocated to an agent 
i is either si or ∅ which is known as exactness lehmann et al 
gave a greedy round-based allocation algorithm based on the rank 
ai 
 si that has an approximation ratio 
√ 
m where m is the 
number of goods in s based on the approximation algorithm they 
gave a truthful payment scheme for an allocation rule satisfying 
 exactness the set of goods allocated to an agent i is either si or 
∅ monotonicity proposing more money for fewer goods 
cannot cause a bidder to lose its bid they proposed a truthful payment 
scheme as follows charge a winning bidder a certain amount 
that does not depend on its own bidding charge a losing 
bidder notice the assumption of exactness reveals that the single 
minded auction is indeed a binary demand game their payment 
scheme inspired our payment scheme for binary demand game 
in archer et al studied the combinatorial auctions where 
multiple copies of many different items are on sale and each 
bidder i desires only one subset si they devised a randomized 
rounding method that is incentive compatible and gave a truthful 
mechanism for combinatorial auctions with single parameter agents that 
approximately maximizes the social value of the auction as they 
pointed out their method is strongly truthful in sense that it is 
truthful with high probability − where is an error probability on 
the contrary in this paper we study how to design a deterministic 
mechanism that is truthful based on some given allocation rules 
in archer and tardos showed how to design truthful 
mechanisms for several combinatorial problems where each agent s 
private information is naturally expressed by a single positive real 
number which will always be the cost incurred per unit load the 
mechanism s output could be arbitrary real number but their 
valuation is a quasi-linear function t · w where t is the private per 
unit cost and w is the work load archer and tardos characterized 
that all truthful mechanism should have decreasing work curves 
w and that the truthful payment should be pi bi pi 
biwi bi − 
r bi 
 
wi u du using this model archer and tardos 
designed truthful mechanisms for several scheduling related 
problems including minimizing the span maximizing flow and 
minimizing the weighted sum of completion time problems notice 
when the load of the problems is w { } it is indeed a binary 
demand game if we apply their characterization of the truthful 
mechanism their decreasing work curves w implies exactly the 
monotonicity property of the output but notice that their proof 
is heavily based on the assumption that the output is a continuous 
function of the cost thus their conclusion can t directly apply to 
binary demand games 
the paper of ahuva mu alem and noam nisan is closest 
in spirit to our work they clearly stated that we only discussed a 
limited class of bidders single minded bidders that was introduced 
by they proved that all truthful mechanisms should have 
a monotonicity output and their payment scheme is based on the 
cut value with a simple generalization we get our conclusion for 
general binary demand game they proposed several combination 
methods including max if-then-else construction to perform 
partial search all of their methods required the welfare function 
associated with the output satisfying bitonic property 
distinction between our contributions and previous results 
it has been shown in that for the single minded 
combinatorial auction there exists a payment scheme which 
results in a truthful mechanism if the allocation rule satisfies a certain 
monotonicity property theorem also depends on the 
monotonicity property but it is applicable to a broader setting than the 
single minded combinatorial auction in addition the binary demand 
game studied here is different from the traditional packing ip s we 
only require that the allocation to each agent is binary and the 
allocation rule satisfies a certain monotonicity property we do not put 
any restrictions on the objective function furthermore the main 
focus of this paper is to design some general techniques to find the 
truthful payment scheme for a given allocation rule o satisfying a 
certain monotonicity property 
 general approaches 
 properties of strategyproof mechanisms 
we discuss several properties that mechanisms need to satisfy in 
order to be truthful 
theorem if a mechanism m o p satisfies ic then 
∀i if oi t i 
ti oi t i 
ti then pi t i 
ti pi t i 
ti 
corollary for any strategy-proof mechanism for a binary 
demand game g with setting s if we fix the cost c−i of all agents 
other than i the payment to agent i is a constant p 
i if oi c 
and it is another constant p 
i if oi c 
theorem fixed the setting s for a binary demand game 
if mechanism m o p satisfies ic then mechanism m 
 o p with the same output method o and pi c pi c − 
δi c−i for any function δi c−i also satisfies ic 
the proofs of above theorems are straightforward and thus 
omitted due to space limit this theorem implies that for the binary 
demand games we can always normalize the payment to an agent 
i such that the payment to the agent is when it is not selected 
hereafter we will only consider normalized payment schemes 
 
 existence of strategyproof mechanisms 
notice given the setting s a mechanism design problem is 
composed of two parts the allocation rule o and a payment scheme p 
in this paper given an allocation rule o we focus our attention 
on how to design a truthful payment scheme based on o given 
an allocation rule o for a binary demand game we first present 
a sufficient and necessary condition for the existence of a truthful 
payment scheme p 
definition monotone non-increasing property mp 
an output method o is said to satisfy the monotone non-increasing 
property if for every agent i and two of its possible costs ci ci 
oi c i 
ci ≤ oi c i 
ci 
this definition is not restricted only to binary demand games 
for binary demand games this definition implies that if oi c i 
ci 
 then oi c i 
ci 
theorem fix the setting s c−i in a binary demand game 
g with the allocation rule o the following three conditions are 
equivalent 
 there exists a value κi o c−i which we will call a cut value 
such that oi c if ci κi o c−i and oi c if 
ci κi o c−i when ci κi o c−i oi c can be 
either or depending on the tie-breaker of the allocation rule 
o hereafter we will not consider the tie-breaker scenario 
in our proofs 
 the allocation rule o satisfies mp 
 there exists a truthful payment scheme p for this binary 
demand game 
proof the proof that condition implies condition is 
straightforward and is omitted here 
we then show condition implies condition the proof of 
this is similar to a proof in to prove this direction we assume 
there exists an agent i and two valuation vectors c i 
ci and c i 
ci 
where ci ci oi c i 
ci and oi c i 
ci from 
corollary we know that pi c i 
ci p 
i and pi c i 
ci p 
i 
now fix c−i the utility for i when ci ci is ui ci p 
i 
when agent i lies its valuation to ci its utility is p 
i − ci since 
m o p is truthful we have p 
i p 
i − ci 
now consider the scenario when the actual valuation of agent i 
is ci ci its utility is p 
i − ci when it reports its true valuation 
similarly if it lies its valuation to ci its utility is p 
i since m 
 o p is truthful we have p 
i p 
i − ci 
consequently we have p 
i −ci p 
i p 
i −ci this inequality 
implies that ci ci which is a contradiction 
we then show condition implies condition we prove this 
by constructing a payment scheme and proving that this payment 
scheme is truthful the payment scheme is if oi c then 
agent i gets payment pi c κi o c−i else it gets payment 
pi c 
from condition if oi c then ci κi o c−i thus 
its utility is κi o c−i − ci which implies that the payment 
scheme satisfies the ir in the following we prove that this payment 
scheme also satisfies ic property there are two cases here 
case ci κ o c−i in this case when i declares its true 
cost ci its utility is κi o c−i − ci now consider the 
situation when i declares a cost di ci if di κi o c−i then 
i gets the same payment and utility since it is still selected if 
di κi o c−i then its utility becomes since it is not selected 
anymore thus it has no incentive to lie in this case 
case ci ≥ κ o c−i in this case when i reveals its true 
valuation its payment is and the utility is now consider the 
situation when i declares a valuation di ci if di κi o c−i 
then i gets the same payment and utility since it is still not selected 
if di ≤ κi o c−i then its utility becomes κi o c−i − ci ≤ 
since it is selected now thus it has no incentive to lie 
the equivalence of the monotonicity property of the allocation 
rule o and the existence of a truthful mechanism using o can be 
extended to games beyond binary demand games the details are 
omitted here due to space limit we now summarize the process to 
design a truthful payment scheme for a binary demand game based 
on an output method o 
general framework truthful mechanism design for a binary 
demand game 
stage check whether the allocation rule o satisfies mp if it 
does not then there is no payment scheme p such that mechanism 
m o p is truthful otherwise define the payment scheme p 
as follows 
stage based on the allocation rule o find the cut value 
κi o c−i for agent i such that oi c i 
di when di 
κi o c−i and oi c i 
di when di κi o c−i 
stage the payment for agent i is if oi c the payment is 
κi o c−i if oi c 
theorem the payment defined by our general framework 
is minimum among all truthful payment schemes using o as output 
 computing cut value functions 
to find the truthful payment scheme by using general 
framework the most difficult stage seems to be the stage notice 
that binary search does not work generally since the valuations of 
agents may be continuous we give some general techniques that 
can help with finding the cut value function under certain 
circumstances our basic approach is as follows first we decompose the 
allocation rule into several allocation rules next find the cut value 
function for each of these new allocation rules then we compute 
the original cut value function by combining these cut value 
functions of the new allocation rules 
 simple combinations 
in this subsection we introduce techniques to compute the cut 
value function by combining multiple allocation rules with 
conjunctions or disconjunctions for simplicity given an allocation 
rule o we will use κ o c to denote a n-tuple vector 
 κ o c− κ o c− κn o c−n 
here κi o c−i is the cut value for agent i when the allocation 
rule is o and the costs c−i of all other agents are fixed 
theorem with a fixed setting s of a binary demand game 
assume that there are m allocation rules o 
 o 
 · · · om 
 
satisfying the monotonicity property and κ oi 
 c is the cut value vector 
for oi 
 then the allocation rule o c 
wm 
i oi 
 c satisfies the 
monotonicity property moreover the cut value for o is κ o c 
maxm 
i {κ oi 
 c } here κ o c maxm 
i {κ oi 
 c } means 
∀j ∈ n κj o c−j maxm 
i {κj oi 
 c−j } and o c wm 
i oi 
 c means ∀j ∈ n oj c o 
j c ∨ o 
j c ∨ · · · ∨ 
om 
j c 
proof assume that ci ci and oi c without loss 
of generality we assume that ok 
i c for some k ≤ k ≤ 
m from the assumption that ok 
i c satisfies mp we obtain that 
 
ok 
i c i 
ci thus oi c i 
ci 
wm 
j oj 
 c this proves 
that o c satisfies mp the correctness of the cut value function 
follows directly from theorem 
many algorithms indeed fall into this category to demonstrate 
the usefulness of theorem we discuss a concrete example here 
in a network sometimes we want to deliver a packet to a set of 
nodes instead of one this problem is known as multicast the 
most commonly used structure in multicast routing is so called 
shortest path tree spt consider a network g v e c where 
v is the set of nodes and vector c is the actual cost of the nodes 
forwarding the data assume that the source node is s and the 
receivers are q ⊂ v for each receiver qi ∈ q we compute the 
shortest path least cost path denoted by lcp s qi d from the 
source s to qi under the reported cost profile d the union of all 
such shortest paths forms the shortest path tree we then use 
general framework to design the truthful payment scheme p when 
the spt structure is used as the output for multicast i e we 
design a mechanism m spt p notice that vcg mechanisms 
cannot be applied here since spt is not an affine maximization 
we define lcp s qi 
as the allocation corresponds to the path 
lcp s qi d i e lcp 
 s qi 
k d if and only if node vk is in 
lcp s qi d then the output spt is defined as 
w 
qi∈q lcp s qi 
 
in other words sptk d if and only if qk is selected in 
some lcp s qi d the shortest path allocation rule is a 
utilitarian and satisfies mp thus from theorem spt also satisfies 
mp and the cut value function vector for spt can be calculated as 
κ spt c maxqi∈q κ lcp s qi 
 c where κ lcp s qi 
 c 
is the cut value function vector for the shortest path lcp s qi c 
consequently the payment scheme above is truthful and the 
minimum among all truthful payment schemes when the allocation rule 
is spt 
theorem fixed the setting s of a binary demand game 
assume that there are m output methods o 
 o 
 · · · om 
 
satisfying mp and κ oi 
 c are the cut value functions respectively for 
oi 
where i · · · m then the allocation rule o c vm 
i oi 
 c satisfies mp moreover the cut value function for o 
is κ o c minm 
i {κ oi 
 c } 
we show that our simple combination generalizes the 
if-thenelse function defined in for an agent i assume that there 
are two allocation rules o 
and o 
satisfying mp let κi o 
 c−i 
κi o 
 c−i be the cut value functions for o 
 o 
respectively 
then the if-then-else function oi c is actually oi c ci ≤ 
κi o 
 c−i δ c−i ∧ o 
 c−i ci ∨ ci κi o 
 c−i − 
δ c−i where δ c−i and δ c−i are two positive functions by 
applying theorems and we know that the allocation rule o 
satisfies mp and consequently κi o c−i max{min κi o 
 c−i 
δ c−i κi o 
 c−i κi o 
 c−i − δ c−i } 
 round-based allocations 
some approximation algorithms are round-based where each 
round of an algorithm selects some agents and updates the setting 
and the cost profile if necessary for example several 
approximation algorithms for minimum weight vertex cover maximum 
weight independent set minimum weight set cover and 
minimum weight steiner tree fall into this category 
as an example we discuss the minimum weighted vertex cover 
problem mwvc to show how to compute the cut value 
for a round-based output given a graph g v e where the 
nodes v v vn are the agents and each agent vi has a weight 
ci we want to find a node set v ⊆ v such that for every edge 
 u v ∈ e at least one of u and v is in v such v is called a 
vertex cover of g the valuation of a node i is −ci if it is selected 
otherwise its valuation is for a subset of nodes v ∈ v we 
define its weight as c v 
p 
i∈v ci 
we want to find a vertex cover with the minimum weight hence 
the objective function to be implemented is utilitarian to use the 
vcg mechanism we need to find the vertex cover with the 
minimum weight which is np-hard since we are interested in 
mechanisms that can be computed in polynomial time we must 
use polynomial-time computable allocation rules many algorithms 
have been proposed in the literature to approximate the optimal 
solution in this paper we use a -approximation algorithm given 
in for the sake of completeness we briefly review this 
algorithm here the algorithm is round-based each round selects 
some vertices and discards some vertices for each node i w i 
is initialized to its weight ci and when w i drops to i is 
included in the vertex cover to make the presentation clear we say 
an edge i j is lexicographically smaller than edge i j if 
 min i j min i j or min i j min i j 
and max i j max i j 
algorithm approximate minimum weighted vertex cover 
input a node weighted graph g v e c 
output a vertex cover v 
 set v ∅ for each i ∈ v set w i ci 
 while v is not a vertex cover do 
 pick an uncovered edge i j with the least lexicographic 
order among all uncovered edges 
 let m min w i w j 
 update w i to w i − m and w j to w j − m 
 if w i add i to v if w j add j to v 
notice selecting an edge using the lexicographic order is 
crutial to guarantee the monotonicity property algorithm outputs 
a vertex cover v whose weight is within times of the optimum 
for convenience we use vc c to denote the vertex cover 
computed by algorithm when the cost vector of vertices is c below 
we generalize algorithm to a more general scenario typically a 
round-based output can be characterized as follows algorithm 
definition an updating rule ur 
is said to be 
crossingindependent if for any agent i not selected in round r sr 
and cr 
−i do not depend on cr 
j for fixed cr 
−i cr 
i 
≤ cr 
i 
implies 
that cr 
i 
≤ cr 
i 
 
we have the following theorem about the existence of a truthful 
payment using a round based allocation rule a 
theorem a round-based output a with the framework 
defined in algorithm satisfies mp if the output methods or 
satisfy 
mp and all updating rules ur 
are crossing-independent 
proof consider an agent i and fixed c−i we prove that when 
an agent i is selected with cost ci then it is also selected with cost 
di ci assume that i is selected in round r with cost ci then 
under cost di if agent i is selected in a round before r our claim 
holds otherwise consider in round r clearly the setting sr 
and 
the costs of all other agents are the same as what if agent i had cost 
ci since i is not selected in the previous rounds due to the 
crossindependent property since i is selected in round r with cost ci i 
is also selected in round r with di ci due to the reason that or 
satisfies mp this finishes the proof 
 
algorithm a general round-based allocation rule a 
 set r c 
 c and g 
 g initially 
 repeat 
 compute an output or 
using a deterministic algorithm 
or 
 sr 
× cr 
→ { }n 
 
here or 
 cr 
and sr 
are allocation rule cost vector and game 
setting in game gr 
 respectively 
remark or 
is often a simple greedy algorithm such as 
selecting the agents that minimize some utilitarian function 
for the example of vertex cover or 
will always select the 
light-weighted node on the lexicographically least 
uncovered edge i j 
 let r r update the game gr− 
to obtain a new game 
gr 
with setting sr 
and cost vector cr 
according to some rule 
ur 
 or− 
× sr− 
 cr− 
 → sr 
 cr 
 
here we updates the cost and setting of the game 
remark for the example of vertex cover the 
updating rule will decrease the weight of vertices i and j by 
min w i w j 
 until a valid output is found 
 return the union of the set of selected players of each round as 
the final output for the example of vertex cover it is the union 
of nodes selected in all rounds 
algorithm compute cut value for round-based algorithms 
input a round-based output a a game g 
 g and a updating 
function vector u 
output the cut value x for agent k 
 set r and ck ζ recall that ζ is a value that can 
guarantee ak when an agent reports the cost ζ 
 repeat 
 compute an output or 
using a deterministic algorithm based 
on setting sr 
using allocation rule or 
 sr 
×cr 
→ { }n 
 
 find the cut value for agent k based on the allocation rule 
or 
for costs cr 
−k let r κk or 
 cr 
−k be the cut value 
 set r r and obtain a new game gr from gr− 
and or 
according to the updating rule ur 
 
 let cr 
be the new cost vector for game gr 
 
 until a valid output is found 
 let gi x be the cost of ci 
k when the original cost vector is 
c k 
x 
 find the minimum value x such that 
 
 
 
g x ≥ 
g x ≥ 
 
gt− x ≥ t− 
gt x ≥ t 
here t is the total number of rounds 
 output the value x as the cut value 
if the round-based output satisfies monotonicity property the 
cut-value always exists we then show how to find the cut value 
for a selected agent k in algorithm 
the correctness of algorithm is straightforward to compute 
the cut value we assume that the cut value r for each round r 
can be computed in polynomial time we can solve the equation 
gr x r to find x in polynomial time when the cost vector c−i 
and b are given 
now we consider the vertex cover problem for each round r 
we select a vertex with the least weight and that is incident on the 
lexicographically least uncovered edge the output satisfies mp 
for agent i we update its cost to cr 
i − cr 
j iff edge i j is selected 
it is easy to verify this updating rule is crossing-independent thus 
we can apply algorithm to compute the cut value for the set cover 
game as shown in algorithm 
algorithm compute cut value for mvc 
input a node weighted graph g v e c and a node k 
selected by algorithm 
output the cut value κk v c c−k 
 for each i ∈ v set w i ci 
 set w k ∞ pk and v ∅ 
 while v is not a vertex cover do 
 pick an uncovered edge i j with the least lexicographic 
order among all uncovered edges 
 set m min w i w j 
 update w i w i − m and w j w j − m 
 if w i add i to v else add j to v 
 if i k or j k then set pk pk m 
 output pk as the cut value κk v c c−k 
 complex combinations 
in subsection we discussed how to find the cut value function 
when the output of the binary demand game is a simple 
combination of some outputs whose cut values can be computed through 
other means typically vcg however some algorithms cannot 
be decomposed in the way described in subsection 
next we present a more complex way to combine allocation 
rules and as we may expected the way to find the cut value is 
also more complicated assume that there are n agents ≤ i ≤ n 
with cost vector c and there are m binary demand games gi with 
objective functions fi o c setting si and allocation rule ψi 
where 
i · · · m there is another binary demand game with 
setting s and allocation rule o whose input is a cost vector d 
 d d · · · dm let f be the function vector f f · · · fm 
ψ be the allocation rule vector ψ 
 ψ 
 · · · ψm 
 and ∫ be the 
setting vector s s · · · sm for notation simplicity we 
define fi c fi ψi 
 c c for each ≤ i ≤ m and f c 
 f c f c · · · fm c 
let us see a concrete example of these combinations consider 
a link weighted graph g v e c and a subset of q nodes q ⊆ 
v the steiner tree problem is to find a set of links with minimum 
total cost to connect q one way to find an approximation of the 
steiner tree is as follows we build a virtual complete graph h 
using q as its vertices and the cost of each edge i j is the cost 
of lcp i j c in graph g build the minimum spanning tree 
of h denoted as mst h an edge of g is selected iff it is 
selected in some lcp i j c and edge i j of h is selected to 
mst h 
in this game we define q q − games gi j where i j ∈ 
q with objective functions fi j o c being the minimum cost of 
 
connecting i and j in graph g setting si being the original graph 
g and allocation rule is lcp i j c the game g corresponds to 
the mst game on graph h the cost of the pair-wise q q − 
shortest paths defines the input vector d d d · · · dm for 
game mst more details will be given in section 
definition given an allocation rule o and setting s an 
objective function vector f an allocation rule vector ψ and setting 
vector ∫ we define a compound binary demand game with setting 
s and output o ◦ f as o ◦ f i c 
wm 
j oj f c ∧ ψj 
i c 
the allocation rule of the above definition can be interpreted as 
follows an agent i is selected if and only if there is a j such that 
 i is selected in ψj 
 c and the allocation rule o will select 
index j under cost profile f c for simplicity we will use o ◦ f 
to denote the output of this compound binary demand game 
notice that a truthful payment scheme using o ◦ f as output 
exists if and only if it satisfies the monotonicity property to study 
when o ◦f satisfies mp several necessary definitions are in order 
definition function monotonicity property fmp given 
an objective function g and an allocation rule o a function h c 
g o c c is said to satisfy the function monotonicity property if 
given fixed c−i it satisfies 
 when oi c h c does not increase over ci 
 when oi c h c does not decrease over ci 
definition strong monotonicity property smp an 
allocation rule o is said to satisfy the strong monotonicity property 
if o satisfies mp and for any agent i with oi c and agent 
j i oi c j 
cj if cj ≥ cj or oj c j 
cj 
lemma for a given allocation rule o satisfying smp and 
cost vectors c c with ci ci if oi c and oi c then 
there must exist j i such that cj cj and oj c 
from the definition of the strong monotonicity property we have 
lemma directly we now can give a sufficient condition when 
o ◦ f satisfies the monotonicity property 
theorem if ∀i ∈ m fi satisfies fmp ψi 
satisfies mp 
and the output o satisfies smp then o ◦ f satisfies mp 
proof assuming for cost vector c we have o ◦ f i c 
 we should prove for any cost vector c c i 
ci with ci ci 
 o ◦ f i c noticing that o ◦ f i c without loss 
of generality we assume that ok f c and ψk 
i c for 
some index ≤ k ≤ m 
now consider the output o with the cost vector f c k 
fk c 
there are two scenarios which will be studied one by one as 
follows 
one scenario is that index k is not chosen by the output function 
o from lemma there must exist j k such that 
fj c fj c 
oj f c k 
fk c 
we then prove that agent i will be selected in the output ψj 
 c 
i e ψj 
i c if it is not since ψj 
 c satisfies mp we have 
ψj 
i c ψj 
i c from ci ci since fj satisfies fmp we 
know fj c ≥ fj c which is a contradiction to the inequality 
 consequently we have ψj 
i c from equation the 
fact that index k is not selected by allocation rule o and the 
definition of smp we have oj f c thus agent i is selected by 
o ◦ f because of oj f c and ψj 
i c 
the other scenario is that index k is chosen by the output 
function o first agent i is chosen in ψk 
 c since the output ψk 
 c 
satisfies the monotonicity property and ci ci and ψk 
i c 
secondly since the function fk satisfies fmp we know that fk c ≤ 
fk c remember that output o satisfies the smp thus we can 
obtain ok f c from the fact that ok f c k 
fk c 
and fk c ≤ fk c consequently agent i will also be selected 
in the final output o ◦ f this finishes our proof 
this theorem implies that there is a cut value for the compound 
output o ◦ f we then discuss how to find the cut value for this 
output below we will give an algorithm to calculate κi o ◦ f 
when o satisfies smp ψj 
satisfies mp and for fixed c−i 
fj c is a constant say hj when ψj 
i c and fj c increases 
when ψj 
i c notice that here hj can be easily computed by 
setting ci ∞ since ψj 
satisfies the monotonicity property when 
given i and fixed c−i we define fi 
j − 
 y as the smallest x such 
that fj c i 
x y for simplicity we denote fi 
j − 
as f− 
j if 
no confusion is caused when i is a fixed agent in this paper we 
assume that given any y we can find such x in polynomial time 
algorithm find cut value for compound method o ◦ f 
input allocation rule o objective function vector f and inverse 
function vector f− 
 {f− 
 · · · f− 
m } allocation rule vector 
ψ and fixed c−i 
output cut value for agent i based on o ◦ f 
 for ≤ j ≤ m do 
 compute the outputs ψj 
 ci 
 compute hj fj c i 
∞ 
 use h h h · · · hm as the input for the output 
function o denote τj κj o h−j as the cut value function of 
output o based on input h 
 for ≤ j ≤ m do 
 set κi j f− 
j min{τj hj} 
 the cut value for i is κi o ◦ f c−i maxm 
j κi j 
theorem algorithm computes the correct cut value for 
agent i based on the allocation rule o ◦ f 
proof in order to prove the correctness of the cut value 
function calculated by algorithm we prove the following two cases 
for our convenience we will use κi to represent κi o ◦ f c−i if 
no confusion caused 
first if di κi then o ◦ f i c i 
di without loss of 
generality we assume that κi κi j for some j since function fj 
satisfies fmp and ψj 
i c i 
di we have fj c i 
di fj κi 
notice di κi j from the definition of κi j f− 
j min{τj hj} 
we have ψj 
i c i 
di fj c i 
di τj due to the fact that 
fj x is a non-decreasing function when j is selected thus from 
the monotonicity property of o and τj is the cut value for output 
o we have 
oj h j 
fj c i 
di 
if oj f c i 
di then o◦f i c i 
di otherwise since 
o satisfies smp lemma and equation imply that there exists 
at least one index k such that ok f c i 
di and fk c i 
di 
hk note fk c i 
di hk implies that i is selected in ψk 
 c i 
di 
since hk fk ci i 
∞ in other words agent i is selected in o◦f 
 
second if di ≥ κi o ◦ f c−i then o ◦ f i c i 
di 
assume for the sake of contradiction that o ◦ f i c i 
di then 
there exists an index ≤ j ≤ m such that oj f c i 
di and 
ψj 
i c i 
di remember that hk ≥ fk c i 
di for any k thus 
from the fact that o satisfies smp when changing the cost vector 
from f c i 
di to h j 
fj c i 
di we still have oj h j 
fj c i 
di 
 this implies that 
fj c i 
di τj 
combining the above inequality and the fact that fj c i 
c i 
di 
hj we have fj c i 
di min{hj τj} this implies 
di f− 
j min{hj τj} κi j κi o ◦ f c−i 
which is a contradiction this finishes our proof 
in most applications the allocation rule ψj 
implements the 
objective function fj and fj is utilitarian thus we can compute 
the inverse of f− 
j efficiently another issue is that it seems the 
conditions when we can apply algorithm are restrictive 
however lots of games in practice satisfy these properties and here we 
show how to deduct the max combination in assume a 
and a are two allocation rules for single minded combinatorial 
auction then the combination max a a returns the 
allocation with the larger welfare if algorithm a and a satisfy mp and 
fmp the operation max x y which returns the larger element of 
x and y satisfies smp from theorem we obtain that 
combination max a a also satisfies mp further the cut value of the 
max combination can be found by algorithm as we will show 
in section the complex combination can apply to some more 
complicated problems 
 concrete examples 
 set cover 
in the set cover problem there is a set u of m elements needed 
to be covered and each agent ≤ i ≤ n can cover a subset of 
elements si with a cost ci let s {s s · · · sn} and c 
 c c · · · cn we want to find a subset of agents d such that 
u ⊆ 
s 
i∈d si the selected subsets is called the set cover for 
u the social efficiency of the output d is defined as 
p 
i∈d ci 
which is the objective function to be minimized clearly this is 
a utilitarian and thus vcg mechanism can be applied if we can 
find the subset of s that covers u with the minimum cost it is 
well-known that finding the optimal solution is np-hard in an 
algorithm of approximation ratio of hm has been proposed and it 
has been proved that this is the best ratio possible for the set cover 
problem for the completeness of presentation we review their 
method here 
algorithm greedy set cover gsc 
input agent i s subset si covered and cost ci ≤ i ≤ n 
output a set of agents that can cover all elements 
 initialize r t ∅ and r ∅ 
 while r u do 
 find the set sj with the minimum density 
cj 
 sj −tr 
 
 set tr tr 
s 
sj and r r 
s 
j 
 r r 
 output r 
let gsc s be the sets selected by the algorithm notice that 
the output set is a function of s and c some works assume that 
the type of an agent could be ci i e si is assumed to be a 
public knowledge here we consider a more general case in which 
the type of an agent is si ci in other words we assume that 
every agent i can not only lie about its cost ci but also can lie about 
the set si this problem now looks similar to the combinatorial 
auction with single minded bidder studied in but with the 
following differences in the set cover problem we want to cover all 
the elements and the sets chosen can have some overlap while in 
combinatorial auction the chosen sets are disjoint 
we can show that the mechanism m gsc pv cg 
 using 
algorithm to find a set cover and apply vcg mechanism to 
compute the payment to the selected agents is not truthful obviously 
the set cover problem is a binary demand game for the moment 
we assume that agent i won t be able to lie about si we will drop 
this assumption later we show how to design a truthful mechanism 
by applying our general framework 
 check the monotonicity property the output of 
algorithm is a round-based output thus for an agent i we 
first focus on the output of one round r in round r if i is 
selected by algorithm then it has the minimum ratio ci 
 si−tr 
among all remaining agents now consider the case when i 
lies its cost to ci ci obviously 
ci 
 si−tr 
is still minimum 
among all remaining agents consequently agent i is still 
selected in round r which means the output of round r satisfies 
mp now we look into the updating rules for every round 
we only update the tr tr 
s 
sj and r r 
s 
j which 
is obviously cross-independent thus by applying theorem 
 we know the output by algorithm satisfies mp 
 find the cut value to calculate the cut value for agent i 
with fixed cost vector c−i we follow the steps in algorithm 
 first we set ci ∞ and apply algorithm let ir be the 
agent selected in round r and t−i 
r be the corresponding set 
then the cut value of round r is 
r 
cir 
 sir − t−i 
r 
· si − t−i 
r 
remember the updating rule only updates the game setting 
but not the cost of the agent thus we have gr x x ≥ r 
for ≤ r ≤ t therefore the final cut value for agent i is 
κi gsc c−i max 
r 
{ 
cir 
 sir − t−i 
r 
· si − t−i 
r } 
the payment to an agent i is κi if i is selected otherwise its 
payment is 
we now consider the scenario when agent i can lie about si 
assume that agent i cannot lie upward i e it can only report a set 
si ⊆ si we argue that agent i will not lie about its elements si 
notice that the cut value computed for round r is r 
 
cir 
 sir −t −i 
r 
· 
 si − t−i 
r obviously si − t−i 
r ≤ si − t−i 
r for any si ⊆ si 
thus lying its set as si will not increase the cut value for each 
round thus lying about si will not improve agent i s utility 
 link weighted steiner trees 
consider any link weighted network g v e c where e 
{e e · · · em} are the set of links and ci is the weight of the link 
ei the link weighted steiner tree problem is to find a tree rooted at 
source node s spanning a given set of nodes q {q q · · · qk} ⊂ 
v for simplicity we assume that qi vi for ≤ i ≤ k here 
the links are agents the total cost of links in a graph h ⊆ g is 
called the weight of h denoted as ω h it is np-hard to find the 
minimum cost multicast tree when given an arbitrary link weighted 
 
graph g the currently best polynomial time method has 
approximation ratio ln 
 
 here we review and discuss the 
first approximation method by takahashi and matsuyama 
algorithm find linkweighted steinertree lst 
input network g v e c where c is the cost vector for link 
set e source node s and receiver set q 
output a tree lst rooted at s and spanned all receivers 
 set r g g q 
 q and s 
 s 
 repeat 
 in graph gr find the receiver say qi that is closest to the 
source s i e lcp s qi c has the least cost among the 
shortest paths from s to all receivers in qr 
 
 select all links on lcp s qi c as relay links and set their 
cost to the new graph is denoted as gr 
 set tr as qi and pr lcp s qi c 
 set qr 
 qr 
\qi and r r 
 until all receivers are spanned 
hereafter let lst g be the final tree constructed using the 
above method it is shown in that mechanism m lst pv cg 
 
is not truthful where pv cg 
is the payment calculated based on 
vcg mechanism 
we then show how to design a truthful payment scheme 
using our general framework observe that the output pr for any 
round r satisfies mp and the update rule for every round 
satisfies crossing-independence thus from theorem the 
roundbased output lst satisfies mp in round r the cut value for a 
link ei can be obtained by using the vcg mechanism now we 
set ci ∞ and execute algorithm let w−i 
r ci be the cost of 
the path pr ci selected in the rth round and πi 
r ci be the 
shortest path selected in round r if the cost of ci is temporarily set to 
−∞ then the cut value for round r is r wi 
r c−i − πi 
r c−i 
where πi 
r c−i is the cost of the path πi 
r c−i excluding node 
vi using algorithm we obtain the final cut value for agent i 
κi lst c−i maxr{ r} thus the payment to a link ei is 
κi lst c−i if its reported cost is di κi lst d−i 
otherwise its payment is 
 virtual minimal spanning trees 
to connect the given set of receivers to the source node besides 
the steiner tree constructed by the algorithms described before a 
virtual minimum spanning tree is also often used assume that q is 
the set of receivers including the sender assume that the nodes in 
a node-weighted graph are all agents the virtual minimum 
spanning tree is constructed as follows 
algorithm construct vmst 
 for all pairs of receivers qi qj ∈ q do 
 calculate the least cost path lcp qi qj d 
 construct a virtual complete link weighted graph k d 
using q as its node set where the link qiqj corresponds to the 
least cost path lcp qi qj d and its weight is w qiqj 
 lcp qi qj d 
 build the minimum spanning tree on k d denoted as 
v mst d 
 for every virtual link qiqj in v mst d do 
 find the corresponding least cost path lcp qi qj d in the 
original network 
 mark the agents on lcp qi qj d selected 
the mechanism m v mst pv cg 
 is not truthful 
where the payment pv cg 
to a node is based on the vcg 
mechanism we then show how to design a truthful mechanism based on 
the framework we described 
 check the monotonicity property remember that in the 
complete graph k d the weight of a link qiqj is lcp qi qj d 
in other words we implicitly defined q q − 
functions fi j for all i j and qi ∈ q and qj ∈ q with 
fi j d lcp qi qj d we can show that the function 
fi j d lcp qi qj d satisfies fmp lcp satisfies mp 
and the output mst satisfies smp from theorem the 
allocation rule vmst satisfies the monotonicity property 
 find the cut value notice vmst is the combination of 
mst and function fi j so cut value for vmst can be 
computed based on algorithm as follows 
 a given a link weighted complete graph k d on q we 
should find the cut value function for edge ek qi qj 
based on mst given a spanning tree t and a pair of 
terminals p and q clearly there is a unique path 
connecting them on t we denote this path as πt p q 
and the edge with the maximum length on this path as 
le p q t thus the cut value can be represented as 
κk mst d le qi qj mst d k 
∞ 
 b we find the value-cost function for lcp assume vk ∈ 
lcp qi qj d then the value-cost function is xk 
yk − lcpvk qi qj d k 
 here lcpvk qi qj d is 
the least cost path between qi and qj with node vk on 
this path 
 c remove vk and calculate the value k d k 
∞ set h i j 
 lcp qi qj d ∞ 
 for every pair of node i j and 
let h {h i j } be the vector then it is easy to 
show that τ i j le qi qj mst h i j 
∞ is 
the cut value for output vmst it easy to verify that 
min{h i j τ i j } le qi qj mst h thus 
we know κ 
 i j 
k v mst d is le qi qj mst h − 
 lcpvk qi qj d k 
 the cut value for agent k is 
κk v mst d−k max ≤i j≤r κij 
k v mst d−k 
 we pay agent k κk v mst d−k if and only if k is selected 
in v mst d else we pay it 
 combinatorial auctions 
lehmann et al studied how to design an efficient truthful 
mechanism for single-minded combinatorial auction in a 
singleminded combinatorial auction there is a set of items s to be sold 
and there is a set of agents ≤ i ≤ n who wants to buy some of 
the items agent i wants to buy a subset si ⊆ s with maximum 
price mi a single-minded bidder i declares a bid bi si ai 
with si ⊆ s and ai ∈ r 
 two bids si ai and sj aj conflict 
if si ∩ sj ∅ given the bids b b · · · bn they gave a greedy 
round-based algorithm as follows first the bids are sorted by some 
criterion ai 
 si is used in in an increasing order and let l be 
the list of sorted bids the first bid is granted then the algorithm 
exams each bid of l in order and grants the bid if it does not conflict 
with any of the bids previously granted if it does it is denied they 
proved that this greedy allocation scheme using criterion ai 
 si 
approximates the optimal allocation within a factor of 
√ 
m where 
m is the number of goods in s 
in the auction settings we have ci −ai it is easy to verify the 
output of the greedy algorithm is a round-based output 
remember after bidder j is selected for round r every bidder has conflict 
 
with j will not be selected in the rounds after this equals to 
update the cost of every bidder having conflict with j to which 
satisfies crossing-independence in addition in any round if 
bidder i is selected with ai then it will still be selected when it 
declares ai ai thus for every round it satisfies mp and the 
cut value is si 
· 
ajr 
 sjr where jr is the bidder selected in 
round r if we did not consider the agent i at all notice 
ajr 
 sjr 
does not increase when round r increases so the final cut value 
is si 
· 
aj 
 sj where bj is the first bid that has been denied 
but would have been selected were it not only for the presence 
of bidder i thus the payment by agent i is si 
· 
aj 
 sj if 
ai ≥ si 
· 
aj 
 sj and otherwise this payment scheme is 
exactly the same as the payment scheme in 
 conclusions 
in this paper we have studied how to design a truthful 
mechanism m o p for a given allocation rule o for a binary 
demand game we first showed that the allocation rule o 
satisfying the mp is a necessary and sufficient condition for a truthful 
mechanism m to exist we then formulate a general framework 
for designing payment p such that the mechanism m o p is 
truthful and computable in polynomial time we further presented 
several general composition-based techniques to compute p 
efficiently for various allocation rules o several concrete examples 
were discussed to demonstrate our general framework for 
designing p and for composition-based techniques of computing p in 
polynomial time 
in this paper we have concentrated on how to compute p in 
polynomial time our algorithms do not necessarily have the 
optimal running time for computing p given o it would be of interest 
to design algorithms to compute p in optimal time we have made 
some progress in this research direction in by providing an 
algorithm to compute the payments for unicast in a node weighted 
graph in optimal o n log n m time 
another research direction is to design an approximation 
allocation rule o satisfying mp with a good approximation ratio for a 
given binary demand game many works in the mechanism 
design literature are in this direction we point out here that the 
goal of this paper is not to design a better allocation rule for a 
problem but to design an algorithm to compute the payments efficiently 
when o is given it would be of significance to design allocation 
rules with good approximation ratios such that a given binary 
demand game has a computationally efficient payment scheme 
in this paper we have studied mechanism design for binary 
demand games however some problems cannot be directly 
formulated as binary demand games the job scheduling problem in 
is such an example for this problem a truthful payment scheme p 
exists for an allocation rule o if and only if the workload assigned 
by o is monotonic in a certain manner it wound be of interest to 
generalize our framework for designing a truthful payment scheme 
for a binary demand game to non-binary demand games towards 
this research direction theorem can be extended to a general 
allocation rule o whose range is r 
 the remaining difficulty is 
then how to compute the payment p under mild assumptions about 
the valuations if a truthful mechanism m o p does exist 
acknowledgements 
we would like to thank rakesh vohra tuomas sandholm and 
anonymous reviewers for helpful comments and discussions 
 references 
 a archer c papadimitriou k t and tardos e an 
approximate truthful mechanism for combinatorial auctions with 
single parameter agents in acm-siam soda pp - 
 archer a and tardos e truthful mechanisms for 
one-parameter agents in proceedings of the nd ieee focs 
 ieee computer society p 
 auletta v prisco r d penna p and persiano p 
deterministic truthful approximation schemes for scheduling related 
machines 
 chvatal v a greedy heuristic for the set covering problem 
mathematics of operations research - 
 clarke e h multipart pricing of public goods public choice 
 - 
 r muller and r v vohra on dominant strategy mechanisms 
working paper 
 devanur n r mihail m and vazirani v v 
strategyproof cost-sharing mechanisms for set cover and facility 
location games in acm electronic commerce ec 
 feigenbaum j krishnamurthy a sami r and 
shenker s approximation and collusion in multicast cost sharing 
 abstract in acm economic conference 
 feigenbaum j papadimitriou c sami r and shenker 
s a bgp-based mechanism for lowest-cost routing in proceedings 
of the acm symposium on principles of distributed 
computing pp - 
 green j and laffont j j characterization of satisfactory 
mechanisms for the revelation of preferences for public goods 
econometrica - 
 groves t incentives in teams econometrica - 
 lehmann d ocallaghan l i and shoham y truth 
revelation in approximately efficient combinatorial auctions journal 
of acm - 
 mu alem a and nisan n truthful approximation 
mechanisms for restricted combinatorial auctions extended abstract 
in th national conference on artificial intelligence 
american association for artificial intelligence pp - 
 nisan n and ronen a algorithmic mechanism design in 
proc st annual acm stoc pp - 
 e halperin improved approximation algorithms for the vertex cover 
problem in graphs and hypergraphs in proceedings of the th 
annual acm-siam symposium on discrete algorithms pages 
 - 
 r bar-yehuda and s even a local ratio theorem for approximating 
the weighted vertex cover problem annals of discrete mathematics 
volume analysis and design of algorithms for combinatorial 
problems pages - editor g ausiello and m lucertini 
 robins g and zelikovsky a improved steiner tree 
approximation in graphs in proceedings of the th annual 
acm-siam soda pp - 
 a zelikovsky an -approximation algorithm for the network 
steiner problem algorithmica - 
 d s hochbaum efficient bounds for the stable set vertex cover and 
set packing problems discrete applied mathematics - 
 
 takahashi h and matsuyama a an approximate solution 
for the steiner problem in graphs math japonica 
 - 
 vickrey w counterspeculation auctions and competitive sealed 
tenders journal of finance - 
 wang w and li x -y truthful low-cost unicast in selfish 
wireless networks in th ieee transactions on mobile computing 
 to appear 
 wang w li x -y and sun z design multicast protocols for 
non-cooperative networks ieee infocom to appear 
 wang w li x -y and wang y truthful multicast in selfish 
wireless networks acm mobicom 
 
playing games in many possible worlds 
matt lepinski∗ 
 david liben-nowell† 
 seth gilbert∗ 
 and april rasala lehman‡ 
 ∗ 
 computer science and artificial intelligence laboratory mit cambridge ma 
 † 
 department of computer science carleton college northfield mn 
 ‡ 
 google inc mountain view ca 
lepinski sethg theory lcs mit edu dlibenno carleton edu alehman google com 
abstract 
in traditional game theory players are typically endowed 
with exogenously given knowledge of the structure of the 
game-either full omniscient knowledge or partial but fixed 
information in real life however people are often unaware 
of the utility of taking a particular action until they perform 
research into its consequences in this paper we model this 
phenomenon we imagine a player engaged in a 
questionand-answer session asking questions both about his or her 
own preferences and about the state of reality thus we call 
this setting socratic game theory in a socratic game 
players begin with an a priori probability distribution over 
many possible worlds with a different utility function for 
each world players can make queries at some cost to learn 
partial information about which of the possible worlds is the 
actual world before choosing an action we consider two 
query models an unobservable-query model in which 
players learn only the response to their own queries and 
 an observable-query model in which players also learn 
which queries their opponents made 
the results in this paper consider cases in which the 
underlying worlds of a two-player socratic game are either 
constant-sum games or strategically zero-sum games a class 
that generalizes constant-sum games to include all games in 
which the sum of payoffs depends linearly on the interaction 
between the players when the underlying worlds are 
constant sum we give polynomial-time algorithms to find nash 
equilibria in both the observable- and unobservable-query 
models when the worlds are strategically zero sum we give 
efficient algorithms to find nash equilibria in 
unobservablequery socratic games and correlated equilibria in 
observablequery socratic games 
categories and subject descriptors 
f theory of computation analysis of algorithms 
and problem complexity j social and behavioral 
sciences economics 
general terms 
algorithms economics theory 
 introduction 
late october a smoky room democratic party 
strategists huddle around a map how should the kennedy 
campaign allocate its remaining advertising budget should 
it focus on say california or new york the nixon 
campaign faces the same dilemma of course neither campaign 
knows the effectiveness of its advertising in each state 
perhaps californians are susceptible to nixon s advertising but 
are unresponsive to kennedy s in light of this uncertainty 
the kennedy campaign may conduct a survey at some cost 
to estimate the effectiveness of its advertising moreover the 
larger-and more expensive-the survey the more accurate 
it will be is the cost of a survey worth the information that 
it provides how should one balance the cost of acquiring 
more information against the risk of playing a game with 
higher uncertainty 
in this paper we model situations of this type as socratic 
games as in traditional game theory the players in a 
socratic game choose actions to maximize their payoffs but we 
model players with incomplete information who can make 
costly queries to reduce their uncertainty about the state of 
the world before they choose their actions this approach 
contrasts with traditional game theory in which players are 
usually modeled as having fixed exogenously given 
information about the structure of the game and its payoffs in 
traditional games of incomplete and imperfect information 
there is information that the players do not have in socratic 
games unlike in these games the players have a chance to 
acquire the missing information at some cost a number of 
related models have been explored by economists and 
computer scientists motivated by similar situations often with 
a focus on mechanism design and auctions a sampling of 
this research includes the work of larson and sandholm 
 parkes fong compte and jehiel 
rezende persico and matthews cr´emer and 
khalil rasmusen and bergemann and v¨alim¨aki 
 the model of bergemann and v¨alim¨aki is similar in 
many regards to the one that we explore here see section 
for some discussion 
a socratic game proceeds as follows a real world is 
cho 
sen randomly from a set of possible worlds according to a 
common prior distribution each player then selects an 
arbitrary query from a set of available costly queries and 
receives a corresponding piece of information about the real 
world finally each player selects an action and receives a 
payoff-a function of the players selected actions and the 
identity of the real world-less the cost of the query that 
he or she made compared to traditional game theory the 
distinguishing feature of our model is the introduction of 
explicit costs to the players for learning arbitrary partial 
information about which of the many possible worlds is the 
real world 
our research was initially inspired by recent results in 
psychology on decision making but it soon became clear that 
socratic game theory is also a general tool for understanding 
the exploitation versus exploration tradeoff well studied 
in machine learning in a strategic multiplayer environment 
this tension between the risk arising from uncertainty and 
the cost of acquiring information is ubiquitous in economics 
political science and beyond 
our results we consider socratic games under two 
models an unobservable-query model where players learn only 
the response to their own queries and an observable-query 
model where players also learn which queries their opponents 
made we give efficient algorithms to find nash 
equilibriai e tuples of strategies from which no player has unilateral 
incentive to deviate-in broad classes of two-player socratic 
games in both models our first result is an efficient 
algorithm to find nash equilibria in unobservable-query 
socratic games with constant-sum worlds in which the sum 
of the players payoffs is independent of their actions our 
techniques also yield nash equilibria in unobservable-query 
socratic games with strategically zero-sum worlds 
strategically zero-sum games generalize constant-sum games by 
allowing the sum of the players payoffs to depend on 
individual players choices of strategy but not on any interaction 
of their choices our second result is an efficient algorithm 
to find nash equilibria in observable-query socratic games 
with constant-sum worlds finally we give an efficient 
algorithm to find correlated equilibria-a weaker but 
increasingly well-studied solution concept for games 
 -in observable-query socratic games with strategically 
zero-sum worlds 
like all games socratic games can be viewed as a 
special case of extensive-form games which represent games 
by trees in which internal nodes represent choices made by 
chance or by the players and the leaves represent outcomes 
that correspond to a vector of payoffs to the players 
algorithmically the generality of extensive-form games makes 
them difficult to solve efficiently and the special cases that 
are known to be efficiently solvable do not include even 
simple socratic games every complete-information classical 
game is a trivial socratic game with a single possible world 
and a single trivial query and efficiently finding nash 
equilibria in classical games has been shown to be hard 
 therefore we would not expect to 
find a straightforward polynomial-time algorithm to 
compute nash equilibria in general socratic games however it 
is well known that nash equilibria can be found efficiently 
via an lp for two-player constant-sum games and 
strategically zero-sum games a socratic game is itself 
a classical game so one might hope that these results can 
be applied to socratic games with constant-sum or 
strategically zero-sum worlds 
we face two major obstacles in extending these 
classical results to socratic games first a socratic game with 
constant-sum worlds is not itself a constant-sum classical 
game-rather the resulting classical game is only 
strategically zero sum worse yet a socratic game with 
strategically zero-sum worlds is not itself classically strategically 
zero sum-indeed there are no known efficient 
algorithmic techniques to compute nash equilibria in the resulting 
class of classical games exponential-time algorithms like 
lemke howson of course can be used thus even 
when it is easy to find nash equilibria in each of the worlds 
of a socratic game we require new techniques to solve the 
socratic game itself second even when the socratic game 
itself is strategically zero sum the number of possible 
strategies available to each player is exponential in the natural 
representation of the game as a result the standard linear 
programs for computing equilibria have an exponential 
number of variables and an exponential number of constraints 
for unobservable-query socratic games with strategically 
zero-sum worlds we address these obstacles by 
formulating a new lp that uses only polynomially many variables 
 though still an exponential number of constraints and then 
use ellipsoid-based techniques to solve it for 
observablequery socratic games we handle the exponentiality by 
decomposing the game into stages solving the stages 
separately and showing how to reassemble the solutions 
efficiently to solve the stages it is necessary to find nash 
equilibria in bayesian strategically zero-sum games and we 
give an explicit polynomial-time algorithm to do so 
 games and socratic games 
in this section we review background on game theory and 
formally introduce socratic games we present these 
models in the context of two-player games but the multiplayer 
case is a natural extension throughout the paper 
boldface variables will be used to denote a pair of variables e g 
a ai aii let pr x ← π denote the probability that a 
particular value x is drawn from the distribution π and let 
ex∼π g x denote the expectation of g x when x is drawn 
from π 
 background on game theory 
consider two players player i and player ii each of whom 
is attempting to maximize his or her utility or payoff a 
 two-player game is a pair a u where for i ∈ {i ii} 
 ai is the set of pure strategies for player i and a 
ai aii and 
 ui a → r is the utility function for player i and 
u ui uii 
we require that a and u be common knowledge if each 
player i chooses strategy ai ∈ ai then the payoffs to 
players i and ii are ui a and uii a respectively a game is 
constant sum if for all a ∈ a we have that ui a uii a c 
for some fixed c independent of a 
player i can also play a mixed strategy αi ∈ ai where ai 
denotes the space of probability measures over the set ai 
payoff functions are generalized as ui α ui αi αii 
ea∼α ui a 
p 
a∈a α a ui a where the quantity α a 
 
αi ai · αii aii denotes the joint probability of the 
independent events that each player i chooses action ai from the 
distribution αi this generalization to mixed strategies is 
known as von neumann morgenstern utility in which 
players are indifferent between a guaranteed payoff x and an 
expected payoff of x 
a nash equilibrium is a pair α of mixed strategies so that 
neither player has an incentive to change his or her strategy 
unilaterally formally the strategy pair α is a nash 
equilibrium if and only if both ui αi αii maxαi∈ai ui αi αii 
and uii αi αii maxαii∈aii uii αi αii that is the 
strategies αi and αii are mutual best responses 
a correlated equilibrium is a distribution ψ over a that 
obeys the following if a ∈ a is drawn randomly according 
to ψ and player i learns ai then no player i has incentive to 
deviate unilaterally from playing ai a nash equilibrium is 
a correlated equilibrium in which ψ a αi ai · αii aii is a 
product distribution formally in a correlated equilibrium 
for every a ∈ a we must have that ai is a best response to 
a randomly chosen ˆaii ∈ aii drawn according to ψ ai ˆaii 
and the analogous condition must hold for player ii 
 socratic games 
in this section we formally define socratic games a 
socratic game is a -tuple a w u s q p δ where for 
i ∈ {i ii} 
 ai is as before the set of pure strategies for player i 
 w is a set of possible worlds one of which is the real 
world wreal 
 ui {uw 
i a → r w ∈ w} is a set of payoff functions 
for player i one for each possible world 
 s is a set of signals 
 qi is a set of available queries for player i when 
player i makes query qi w → s he or she receives the 
signal qi wreal when player i receives signal qi wreal 
in response to query qi he or she can infer that wreal ∈ 
{w qi w qi wreal } i e the set of possible worlds 
from which query qi cannot distinguish wreal 
 p w → is a probability distribution over the 
possible worlds 
 δi qi → r≥ 
gives the query cost for each available 
query for player i 
initially the world wreal is chosen according to the 
probability distribution p but the identity of wreal remains 
unknown to the players that is it is as if the players are 
playing the game a uwreal but do not know wreal the 
players make queries q ∈ q and player i receives the signal 
qi wreal we consider both observable queries and 
unobservable queries when queries are observable each player 
learns which query was made by the other player and the 
results of his or her own query-that is each player i learns 
qi qii and qi wreal for unobservable queries player i learns 
only qi and qi wreal after learning the results of the queries 
the players select strategies a ∈ a and receive as payoffs 
u 
wreal 
i a − δi qi 
in the socratic game a pure strategy for player i consists 
of a query qi ∈ qi and a response function mapping any 
result of the query qi to a strategy ai ∈ ai to play a player s 
state of knowledge after a query is a point in r q × s 
or ri qi × s for observable or unobservable queries 
respectively thus player i s response function maps r or 
ri to ai note that the number of pure strategies is 
exponential as there are exponentially many response 
functions a mixed strategy involves both randomly choosing 
a query qi ∈ qi and randomly choosing an action ai ∈ ai 
in response to the results of the query formally we will 
consider a mixed-strategy-function profile f fquery 
 fresp 
to have two parts 
 a function fquery 
i qi → where fquery 
i qi is the 
probability that player i makes query qi 
 a function fresp 
i that maps r or ri to a 
probability distribution over actions player i chooses an 
action ai ∈ ai according to the probability distribution 
fresp 
i q qi w for observable queries and according to 
fresp 
i qi qi w for unobservable queries with 
unobservable queries for example the probability that 
player i plays action ai conditioned on making query 
qi in world w is given by pr ai ← fresp 
i qi qi w 
mixed strategies are typically defined as probability 
distributions over the pure strategies but here we represent a 
mixed strategy by a pair fquery 
 fresp 
 which is commonly 
referred to as a behavioral strategy in the game-theory 
literature as in any game with perfect recall one can 
easily map a mixture of pure strategies to a behavioral strategy 
f fquery 
 fresp 
that induces the same probability of 
making a particular query qi or playing a particular action after 
making a query qi in a particular world thus it suffices to 
consider only this representation of mixed strategies 
for a strategy-function profile f for observable queries the 
 expected payoff to player i is given by 
x 
q∈q w∈w a∈a 
 
 
 
 
fquery 
i qi · fquery 
ii qii · p w 
· pr ai ← fresp 
i q qi w 
· pr aii ← fresp 
ii q qii w 
· uw 
i a − δi qi 
 
 
 
 
the payoffs for unobservable queries are analogous with 
fresp 
j qj qj w in place of fresp 
j q qj w 
 strategically zero-sum games 
we can view a socratic game g with constant-sum worlds 
as an exponentially large classical game with pure 
strategies make query qi and respond according to fi 
however this classical game is not constant sum the sum of 
the players payoffs varies depending upon their strategies 
because different queries incur different costs however this 
game still has significant structure the sum of payoffs varies 
only because of varying query costs thus the sum of 
payoffs does depend on players choice of strategies but not on 
the interaction of their choices-i e for fixed functions gi 
and gii we have ui q f uii q f gi qi fi gii qii fii 
for all strategies q f such games are called strategically 
zero sum and were introduced by moulin and vial who 
describe a notion of strategic equivalence and define 
strategically zero-sum games as those strategically equivalent to 
zero-sum games it is interesting to note that two socratic 
games with the same queries and strategically equivalent 
worlds are not necessarily strategically equivalent 
a game a u is strategically zero sum if there exist labels 
 i ai for every player i and every pure strategy ai ∈ ai 
 
such that for all mixed-strategy profiles α we have that the 
sum of the utilities satisfies 
ui α uii α 
x 
ai∈ai 
αi ai · i ai 
x 
aii∈aii 
αii aii · ii aii 
note that any constant-sum game is strategically zero sum 
as well 
it is not immediately obvious that one can efficiently 
decide if a given game is strategically zero sum for 
completeness we give a characterization of classical strategically 
zero-sum games in terms of the rank of a simple matrix 
derived from the game s payoffs allowing us to efficiently 
decide if a given game is strategically zero sum and if it is to 
compute the labels i ai 
theorem consider a game g a u with ai 
{a 
i ani 
i } let mg 
be the ni-by-nii matrix whose i j th 
entry mg 
 i j satisfies log mg 
 i j ui ai 
i aj 
ii uii ai 
i aj 
ii 
then the following are equivalent 
 i g is strategically zero sum 
 ii there exist labels i ai for every player i ∈ {i ii} and 
every pure strategy ai ∈ ai such that for all pure 
strategies a ∈ a we have ui a uii a i ai 
 ii aii and 
 iii rank mg 
 
proof sketch i ⇒ ii is immediate every pure 
strategy is a trivially mixed strategy for ii ⇒ iii let ci be the 
n-element column vector with jth component i a 
j 
i 
 then 
ci · cii 
t 
 mg 
 for iii ⇒ i if rank mg 
 then mg 
 
u · vt 
 we can prove that g is strategically zero sum by 
choosing labels i aj 
i log uj and ii aj 
ii log vj 
 socratic games with 
unobservable queries 
we begin with socratic games with unobservable queries 
where a player s choice of query is not revealed to her 
opponent we give an efficient algorithm to solve 
unobservablequery socratic games with strategically zero-sum worlds 
our algorithm is based upon the lp shown in figure 
whose feasible points are nash equilibria for the game the 
lp has polynomially many variables but exponentially many 
constraints we give an efficient separation oracle for the lp 
implying that the ellipsoid method yields an efficient 
algorithm this approach extends the techniques of koller 
and megiddo see also to solve constant-sum games 
represented in extensive form recall that their result does 
not directly apply in our case even a socratic game with 
constant-sum worlds is not a constant-sum classical game 
lemma let g a w u s q p δ be an arbitrary 
unobservable-query socratic game with strategically zero-sum 
worlds any feasible point for the lp in figure can be 
efficiently mapped to a nash equilibrium for g and any nash 
equilibrium for g can be mapped to a feasible point for the 
program 
proof sketch we begin with a description of the 
correspondence between feasible points for the lp and nash 
equilibria for g first suppose that strategy profile f 
fquery 
 fresp 
forms a nash equilibrium for g then the 
following setting for the lp variables is feasible 
yi 
qi 
 fquery 
i qi 
xi 
ai qi w pr ai ← fresp 
i qi qi w · yi 
qi 
ρi 
p 
w q∈q a∈a 
p w · xi 
ai qi w · xii 
aii qii w · uw 
i a − δi qi 
 we omit the straightforward calculations that verify 
feasibility next suppose xi 
ai qi w yi 
qi 
 ρi is feasible for the lp 
let f be the strategy-function profile defined as 
fquery 
i qi → yi 
qi 
fresp 
i qi qi w ai → xi 
ai qi w yi 
qi 
 
verifying that this strategy profile is a nash equilibrium 
requires checking that fresp 
i qi qi w is a well-defined 
function from constraint vi that fquery 
i and fresp 
i qi qi w are 
probability distributions from constraints iii and iv and 
that each player is playing a best response to his or her 
opponent s strategy from constraints i and ii finally from 
constraints i and ii the expected payoff to player i is at most 
ρi because the right-hand side of constraint vii is equal 
to the expected sum of the payoffs from f and is at most 
ρi ρii the payoffs are correct and imply the lemma 
we now give an efficient separation oracle for the lp in 
figure thus allowing the ellipsoid method to solve the 
lp in polynomial time recall that a separation oracle is 
a function that given a setting for the variables in the lp 
either returns feasible or returns a particular constraint 
of the lp that is violated by that setting of the variables 
an efficient correct separation oracle allows us to solve the 
lp efficiently via the ellipsoid method 
lemma there exists a separation oracle for the lp 
in figure that is correct and runs in polynomial time 
proof here is a description of the separation oracle sp 
on input xi 
ai qi w yi 
qi 
 ρi 
 check each of the constraints iii iv v vi 
and vii if any one of these constraints is violated 
then return it 
 define the strategy profile f as follows 
fquery 
i qi → yi 
qi 
fresp 
i qi qi w ai → xi 
ai qi w yi 
qi 
for each query qi we will compute a pure best-response 
function ˆf 
qi 
i for player i to strategy fii after making 
query qi 
more specifically given fii and the result qi wreal of the 
query qi it is straightforward to compute the 
probability that conditioned on the fact that the result of 
query qi is qi w the world is w and player ii will play 
action aii ∈ aii therefore for each query qi and 
response qi w player i can compute the expected utility 
of each pure response ai to the induced mixed strategy 
over aii for player ii player i can then select the ai 
maximizing this expected payoff 
let ˆfi be the response function such that ˆfi qi qi w 
ˆf 
qi 
i qi w for every qi ∈ qi similarly compute ˆfii 
 
player i does not prefer  make query qi then play according to the function fi 
∀qi ∈ qi fi ri → ai ρi ≥ 
p 
w∈w aii∈aii qii∈qii ai fi qi qi w 
` 
p w · xii 
aii qii w · uw 
i a − δi qi 
´ 
 i 
∀qii ∈ qii fii rii → aii ρii ≥ 
p 
w∈w ai∈ai qi∈qi aii fii qii qii w 
` 
p w · xi 
ai qi w · uw 
ii a − δii qii 
´ 
 ii 
every player s choices form a probability distribution in every world 
∀i ∈ {i ii} w ∈ w 
p 
ai∈ai qi∈qi 
xi 
ai qi w iii 
∀i ∈ {i ii} w ∈ w ≤ xi 
ai qi w iv 
queries are independent of the world and actions depend only on query output 
∀i ∈ {i ii} qi ∈ qi w ∈ w w ∈ w such that qi w qi w 
yi 
qi 
 
p 
ai∈ai 
xi 
ai qi w v 
xi 
ai qi w xi 
ai qi w vi 
the payoffs are consistent with the labels i ai w 
ρi ρii 
p 
i∈{i ii} 
p 
w∈w qi∈qi ai∈ai 
` 
p w · xi 
ai qi w · i ai w − δi qi 
´ 
 vii 
figure an lp to find nash equilibria in unobservable-query socratic games with strategically zero-sum 
worlds the input is a socratic game a w u s q p δ so that world w is strategically zero sum with labels 
 i ai w player i makes query qi ∈ qi with probability yi 
qi 
and when the actual world is w ∈ w makes query 
qi and plays action ai with probability xi 
ai qi w the expected payoff to player i is given by ρi 
 let ˆρ 
qi 
i be the expected payoff to player i using the 
strategy make query qi and play response function 
ˆfi if player ii plays according to fii 
let ˆρi maxqi∈qq ˆρ 
qi 
i and let ˆqi arg maxqi∈qq ˆρ 
qi 
i 
similarly define ˆρ 
qii 
ii ˆρii and ˆqii 
 for the ˆfi and ˆqi defined in step return constraint 
 i-ˆqi- ˆfi or ii-ˆqii- ˆfii if either is violated if both are 
satisfied then return feasible 
we first note that the separation oracle runs in polynomial 
time and then prove its correctness steps and are clearly 
polynomial for step we have described how to compute 
the relevant response functions by examining every action of 
player i every world every query and every action of player 
ii there are only polynomially many queries worlds query 
results and pure actions so the running time of steps and 
 is thus polynomial 
we now sketch the proof that the separation oracle works 
correctly the main challenge is to show that if any 
constraint i-qi-fi is violated then i-ˆqi- ˆfi is violated in step 
first we observe that by construction the function ˆfi 
computed in step must be a best response to player ii playing 
fii no matter what query player i makes therefore the 
strategy make query ˆqi then play response function ˆfi 
must be a best response to player ii playing fii by definition 
of ˆqi the right-hand side of each constraint i-qi-fi is equal 
to the expected payoff that player i receives when playing 
the pure strategy make query qi and then play response 
function fi against player ii s strategy of fii therefore 
because the pure strategy make query ˆqi and then play 
response function ˆfi is a best response to player ii 
playing fii the right-hand side of constraint i-ˆqi- ˆfi is at least 
as large as the right hand side of any constraint i-ˆqi-fi 
therefore if any constraint i-qi-fi is violated constraint 
 i-ˆqi- ˆfi is also violated an analogous argument holds for 
player ii 
these lemmas and the well-known fact that nash 
equilibria always exist imply the following theorem 
theorem nash equilibria can be found in 
polynomial time for any two-player unobservable-query socratic 
game with strategically zero-sum worlds 
 socratic games with 
observable queries 
in this section we give efficient algorithms to find a 
nash equilibrium for observable-query socratic games with 
constant-sum worlds and a correlated equilibrium in the 
broader class of socratic games with strategically zero-sum 
worlds recall that a socratic game g a w u s q p δ 
with observable queries proceeds in two stages 
stage the players simultaneously choose queries q ∈ q 
player i receives as output qi qii and qi wreal 
stage the players simultaneously choose strategies a ∈ 
a the payoff to player i is u 
wreal 
i a − δi qi 
using backward induction we first solve stage and then 
proceed to the stage- game 
for a query q ∈ q we would like to analyze the stage- 
game ˆgq resulting from the players making queries q in 
stage technically however ˆgq is not actually a game 
because at the beginning of stage the players have different 
information about the world player i knows qi wreal and 
 
player ii knows qii wreal fortunately the situation in which 
players have asymmetric private knowledge has been well 
studied in the game-theory literature a bayesian game is 
a quadruple a t r u where 
 ai is the set of pure strategies for player i 
 ti is the set of types for player i 
 r is a probability distribution over t r t denotes the 
probability that player i has type ti for all i 
 ui a × t → r is the payoff function for player i 
if the players have types t and play pure strategies a 
then ui a t denotes the payoff for player i 
initially a type t is drawn randomly from t according to the 
distribution r player i learns his type ti but does not learn 
any other player s type player i then plays a mixed strategy 
αi ∈ ai-that is a probability distribution over ai-and 
receives payoff ui α t a strategy function is a function 
hi ti → ai player i plays the mixed strategy hi ti ∈ ai 
when her type is ti a strategy-function profile h is a 
bayesian nash equilibrium if and only if no player i has 
unilateral incentive to deviate from hi if the other players play 
according to h for a two-player bayesian game if α h t 
then the profile h is a bayesian nash equilibrium exactly 
when the following condition and its analogue for player ii 
hold et∼r ui α t maxhi 
et∼r ui hi ti αii t these 
conditions hold if and only if for all ti ∈ ti occurring with 
positive probability player i s expected utility conditioned 
on his type being ti is maximized by hi ti a bayesian 
game is constant sum if for all a ∈ a and all t ∈ t we 
have ui a t uii a t ct for some constant ct 
independent of a a bayesian game is strategically zero sum if the 
classical game a u · t is strategically zero sum for every 
t ∈ t whether a bayesian game is strategically zero sum 
can be determined as in theorem for further 
discussion of bayesian games see 
we now formally define the stage- game as a bayesian 
game given a socratic game g a w u s q p δ and 
a query profile q ∈ q we define the stage- bayesian game 
gstage q a tq 
 pstage q 
 ustage q 
 where 
 ai the set of pure strategies for player i is the same 
as in the original socratic game 
 tq 
i {qi w w ∈ w} the set of types for player i is 
the set of signals that can result from query qi 
 pstage q 
 t pr q w t w ← p and 
 u 
stage q 
i a t 
p 
w∈w pr w ← p q w t · uw 
i a 
we now define the stage- game in terms of the payoffs 
for the stage- games fix any algorithm alg that finds a 
bayesian nash equilibrium hq alg 
 alg gstage q for each 
stage- game define valuealg 
i gstage q to be the expected 
payoff received by player i in the bayesian game gstage q 
if each player plays according to hq alg 
 that is 
valuealg 
i gstage q 
 
p 
w∈w p w · u 
stage q 
i hq alg 
 q w q w 
define the game galg 
stage astage 
 ustage alg 
 where 
 astage 
 q the set of available queries in the socratic 
game and 
 u 
stage alg 
i q valuealg 
i gstage q − δi qi 
i e players choose queries q and receive payoffs 
corresponding to valuealg 
 gstage q less query costs 
lemma consider an observable-query socratic game 
g a w u s q p δ let gstage q be the stage- games 
for all q ∈ q let alg be an algorithm finding a bayesian 
nash equilibrium in each gstage q and let galg 
stage be the 
stage- game let α be a nash equilibrium for galg 
stage and 
let hq alg 
 alg gstage q be a bayesian nash equilibrium 
for each gstage q then the following strategy profile is a 
nash equilibrium for g 
 in stage player i makes query qi with probability 
αi qi that is set fquery 
 q α q 
 in stage if q is the query in stage and qi wreal 
denotes the response to player i s query then player i 
chooses action ai with probability hq alg 
i qi wreal in 
other words set fresp 
i q qi w hq alg 
i qi w 
we now find equilibria in the stage games for socratic games 
with constant- or strategically zero-sum worlds we first 
show that the stage games are well structured in this setting 
lemma consider an observable-query socratic game 
g a w u s q p δ with constant-sum worlds then 
the stage- game galg 
stage is strategically zero sum for every 
algorithm alg and every stage- game gstage q is bayesian 
constant sum if the worlds of g are strategically zero sum 
then every gstage q is bayesian strategically zero sum 
we now show that we can efficiently compute equilibria for 
these well-structured stage games 
theorem there exists a polynomial-time algorithm 
bne finding bayesian nash equilibria in strategically 
zerosum bayesian and thus classical strategically zero-sum or 
bayesian constant-sum two-player games 
proof sketch let g a t r u be a strategically 
zero-sum bayesian game define an unobservable-query 
socratic game g∗ 
with one possible world for each t ∈ t one 
available zero-cost query qi for each player i so that qi 
reveals ti and all else as in g bayesian nash equilibria in g 
correspond directly to nash equilibria in g∗ 
 and the worlds 
of g∗ 
are strategically zero sum thus by theorem we 
can compute nash equilibria for g∗ 
 and thus we can 
compute bayesian nash equilibria for g 
 lp s for zero-sum two-player bayesian games have been 
previously developed and studied 
theorem we can compute a nash equilibrium for 
an arbitrary two-player observable-query socratic game g 
a w u s q p δ with constant-sum worlds in polynomial 
time 
proof because each world of g is constant sum lemma 
 implies that the induced stage- games gstage q are 
all bayesian constant sum thus we can use algorithm 
bne to compute a bayesian nash equilibrium hq bne 
 
bne gstage q for each q ∈ q by theorem 
furthermore again by lemma the induced stage- game 
gbne 
stage is classical strategically zero sum therefore we can 
again use algorithm bne to compute a nash equilibrium 
α bne gbne 
stage again by theorem therefore by 
lemma we can assemble α and the hq bne 
 s into a nash 
equilibrium for the socratic game g 
 
we would like to extend our results on observable-query 
socratic games to socratic games with strategically 
zerosum worlds while we can still find nash equilibria in the 
stage- games the resulting stage- game is not in 
general strategically zero sum thus finding nash equilibria 
in observable-query socratic games with strategically 
zerosum worlds seems to require substantially new techniques 
however our techniques for decomposing observable-query 
socratic games do allow us to find correlated equilibria in 
this case 
lemma consider an observable-query socratic game 
g a w u s q p δ let alg be an arbitrary algorithm 
that finds a bayesian nash equilibrium in each of the derived 
stage- games gstage q and let galg 
stage be the derived 
stage game let φ be a correlated equilibrium for galg 
stage and let 
hq alg 
 alg gstage q be a bayesian nash equilibrium for 
each gstage q then the following distribution over pure 
strategies is a correlated equilibrium for g 
ψ q f φ q 
y 
i∈{i ii} 
y 
s∈s 
pr 
h 
fi q s ← hq alg 
i s 
i 
 
thus to find a correlated equilibrium in an observable-query 
socratic game with strategically zero-sum worlds we need 
only algorithm bne from theorem along with an 
efficient algorithm for finding a correlated equilibrium in a 
general game such an algorithm exists the definition of 
correlated equilibria can be directly translated into an lp 
and therefore we have the following theorem 
theorem we can provide both efficient oracle 
access and efficient sampling access to a correlated 
equilibrium for any observable-query two-player socratic game with 
strategically zero-sum worlds 
because the support of the correlated equilibrium may be 
exponentially large providing oracle and sampling access is 
the natural way to represent the correlated equilibrium 
by lemma we can also compute correlated equilibria 
in any observable-query socratic game for which nash 
equilibria are computable in the induced gstage q games e g 
when gstage q is of constant size 
another potentially interesting model of queries in 
socratic games is what one might call public queries in which 
both the choice and outcome of a player s query is 
observable by all players in the game this model might be most 
appropriate in the presence of corporate espionage or media 
leaks or in a setting in which the queries-and thus their 
results-are done in plain view the techniques that we 
have developed in this section also yield exactly the same 
results as for observable queries the proof is actually 
simpler with public queries the players payoffs are common 
knowledge when stage begins and thus stage really is a 
complete-information game there may still be uncertainty 
about the real world but all players use the observed 
signals to infer exactly the same set of possible worlds in which 
wreal may lie thus they are playing a complete-information 
game against each other thus we have the same results 
as in theorems and more simply by solving stage 
using a non-bayesian nash-equilibrium finder and solving 
stage as before 
our results for observable queries are weaker than for 
unobservable in socratic games with worlds that are 
strategically zero sum but not constant sum we find only a 
correlated equilibrium in the observable case whereas we find a 
nash equilibrium in the unobservable case we might hope 
to extend our unobservable-query techniques to observable 
queries but there is no obvious way to do so the 
fundamental obstacle is that the lp s payoff constraint becomes 
nonlinear if there is any dependence on the probability that 
the other player made a particular query this dependence 
arises with observable queries suggesting that observable 
socratic games with strategically zero-sum worlds may be 
harder to solve 
 related work 
our work was initially motivated by research in the social 
sciences indicating that real people seem irrationally 
paralyzed when they are presented with additional options in 
this section we briefly review some of these social-science 
experiments and then discuss technical approaches related 
to socratic game theory 
prima facie a rational agent s happiness given an added 
option can only increase however recent research has found 
that more choices tend to decrease happiness for 
example students choosing among extra-credit options are more 
likely to do extra credit if given a small subset of the choices 
and moreover produce higher-quality work see also 
 the psychology literature explores a number of 
explanations people may miscalculate their opportunity cost by 
comparing their choice to a component-wise maximum of 
all other options instead of the single best alternative 
a new option may draw undue attention to aspects of the 
other options and so on the present work explores 
an economic explanation of this phenomenon information 
is not free when there are more options a decision-maker 
must spend more time to achieve a satisfactory outcome 
see e g the work of skyrms for a philosophical 
perspective on the role of deliberation in strategic situations 
finally we note the connection between socratic games and 
modal logic a formalism for the logic of possibility and 
necessity 
the observation that human players typically do not play 
rational strategies has inspired some attempts to model 
partially rational players the typical model of this 
socalled bounded rationality is to postulate bounds 
on computational power in computing the consequences of 
a strategy the work on bounded rationality 
differs from the models that we consider here in that instead 
of putting hard limitations on the computational power of 
the agents we instead restrict their a priori knowledge of 
the state of the world requiring them to spend time and 
therefore money utility to learn about it 
partially observable stochastic games posgs are a 
general framework used in ai to model situations of multi-agent 
planning in an evolving unknown environment but the 
generality of posgs seems to make them very difficult 
recent work has been done in developing algorithms for 
restricted classes of posgs most notably classes of 
cooperative posgs-e g -which are very different from 
the competitive strategically zero-sum games we address in 
this paper 
the fundamental question in socratic games is deciding 
on the comparative value of making a more costly but more 
informative query or concluding the data-gathering phase 
and picking the best option given current information this 
tradeoff has been explored in a variety of other contexts 
a sampling of these contexts includes aggregating results 
 
from delay-prone information sources doing approximate 
reasoning in intelligent systems deciding when to take 
the current best guess of disease diagnosis from a 
beliefpropagation network and when to let it continue inference 
 among many others 
this issue can also be viewed as another perspective on 
the general question of exploration versus exploitation that 
arises often in ai when is it better to actively seek 
additional information instead of exploiting the knowledge one 
already has see e g most of this work differs 
significantly from our own in that it considers single-agent 
planning as opposed to the game-theoretic setting a 
notable exception is the work of larson and sandholm 
 on mechanism design for interacting agents whose 
computation is costly and limited they present a model 
in which players must solve a computationally intractable 
valuation problem using costly computation to learn some 
hidden parameters and results for auctions and bargaining 
games in this model 
 future directions 
efficiently finding nash equilibria in socratic games with 
non-strategically zero-sum worlds is probably difficult 
because the existence of such an algorithm for classical games 
has been shown to be unlikely 
 there has however been some algorithmic success in 
finding nash equilibria in restricted classical settings e g 
 we might hope to extend our results to 
analogous socratic games 
an efficient algorithm to find correlated equilibria in 
general socratic games seems more attainable suppose the 
players receive recommended queries and responses the 
difficulty is that when a player considers a deviation from 
his recommended query he already knows his recommended 
response in each of the stage- games in a correlated 
equilibrium a player s expected payoff generally depends on his 
recommended strategy and thus a player may deviate in 
stage so as to land in a stage- game where he has been 
given a better than average recommended response 
 socratic games are succinct games of superpolynomial type 
so papadimitriou s results do not imply correlated 
equilibria for them 
socratic games can be extended to allow players to make 
adaptive queries choosing subsequent queries based on 
previous results our techniques carry over to o rounds of 
unobservable queries but it would be interesting to 
compute equilibria in socratic games with adaptive observable 
queries or with ω rounds of unobservable queries 
special cases of adaptive socratic games are closely related to 
single-agent problems like minimum latency 
determining strategies for using priced information 
and an online version of minimum test cover 
although there are important technical distinctions between 
adaptive socratic games and these problems approximation 
techniques from this literature may apply to socratic games 
the question of approximation raises interesting questions 
even in non-adaptive socratic games an -approximate 
nash equilibrium is a strategy profile α so that no player 
can increase her payoff by an additive by deviating from α 
finding approximate nash equilibria in both adaptive and 
non-adaptive socratic games is an interesting direction to 
pursue 
another natural extension is the model where query 
results are stochastic in this paper we model a query as 
deterministically partitioning the possible worlds into 
subsets that the query cannot distinguish however one could 
instead model a query as probabilistically mapping the set 
of possible worlds into the set of signals with this 
modification our unobservable-query model becomes equivalent 
to the model of bergemann and v¨alim¨aki in which 
the result of a query is a posterior distribution over the 
worlds our techniques allow us to compute equilibria in 
such a stochastic-query model provided that each query is 
represented as a table that for each world signal pair lists 
the probability that the query outputs that signal in that 
world it is also interesting to consider settings in which the 
game s queries are specified by a compact representation of 
the relevant probability distributions for example one 
might consider a setting in which the algorithm has only a 
sampling oracle for the posterior distributions envisioned by 
bergemann and v¨alim¨aki efficiently finding equilibria in 
such settings remains an open problem 
another interesting setting for socratic games is when the 
set q of available queries is given by q p γ -i e each 
player chooses to make a set q ∈ p γ of queries from a 
specified groundset γ of queries here we take the query 
cost to be a linear function so that δ q 
p 
γ∈q δ {γ} 
natural groundsets include comparison queries if my 
opponent is playing strategy aii would i prefer to play ai or 
ˆai strategy queries what is my vector of payoffs if i 
play strategy ai and world-identity queries is the world 
w ∈ w the real world when one can infer a polynomial 
bound on the number of queries made by a rational player 
then our results yield efficient solutions for example we 
can efficiently solve games in which every groundset element 
γ ∈ γ has δ {γ} ω m − m where m and m denote 
the maximum and minimum payoffs to any player in any 
world conversely it is np-hard to compute a nash 
equilibrium for such a game when every δ {γ} ≤ w 
 even 
when the worlds are constant sum and player ii has only 
a single available strategy thus even computing a best 
response for player i is hard this proof proceeds by 
reduction from set cover intuitively for sufficiently low query 
costs player i must fully identify the actual world through 
his queries selecting a minimum-sized set of these queries 
is hard computing player i s best response can be viewed 
as maximizing a submodular function and thus a best 
response can be − e ≈ approximated greedily 
an interesting open question is whether this approximate 
best-response calculation can be leveraged to find an 
approximate nash equilibrium 
 acknowledgements 
part of this work was done while all authors were at mit 
csail we thank erik demaine natalia hernandez 
gardiol claire monteleoni jason rennie madhu sudan and 
katherine white for helpful comments and discussions 
 references 
 aaron archer and david p williamson faster 
approximation algorithms for the minimum latency 
problem in proceedings of the symposium on discrete 
algorithms pages - 
 r j aumann subjectivity and correlation in 
randomized strategies j mathematical economics 
 - 
 
 robert j aumann correlated equilibrium as an 
expression of bayesian rationality econometrica 
 - january 
 dick bergemann and juuso v¨alim¨aki information 
acquisition and efficient mechanism design 
econometrica - may 
 dick bergemann and juuso v¨alim¨aki information in 
mechanism design technical report cowles 
foundation for research in economics 
 daniel s bernstein shlomo zilberstein and neil 
immerman the complexity of decentralized control of 
markov decision processes mathematics of 
operations research pages - 
 avrim blum prasad chalasani don coppersmith 
bill pulleyblank prabhakar raghavan and madhu 
sudan the minimum latency problem in proceedings 
of the symposium on the theory of computing pages 
 - 
 andrei z broder and michael mitzenmacher optimal 
plans for aggregation in proceedings of the principles 
of distributed computing pages - 
 moses charikar ronald fagin venkatesan 
guruswami jon kleinberg prabhakar raghavan and 
amit sahai query strategies for priced information 
j computer and system sciences - 
june 
 xi chen and xiaotie deng -nash is 
ppad-complete in electronic colloquium on 
computational complexity 
 xi chen and xiaotie deng settling the complexity of 
 -player nash-equilibrium in electronic colloquium 
on computational complexity 
 olivier compte and philippe jehiel auctions and 
information acquisition sealed-bid or dynamic 
formats technical report centre d enseignement et 
de recherche en analyse socio-´economique 
 vincent conitzer and tuomas sandholm complexity 
results about nash equilibria in proceedings of the 
international joint conference on artificial 
intelligence pages - 
 gerard cornuejols marshall l fisher and george l 
nemhauser location of bank accounts to optimize 
float an analytic study of exact and approximate 
algorithms management science april 
 jacques cr´emer and fahad khalil gathering 
information before signing a contract american 
economic review - 
 constantinos daskalakis paul w goldberg and 
christos h papadimitriou the complexity of 
computing a nash equilbrium in electronic 
colloquium on computational complexity 
 konstantinos daskalakis and christos h 
papadimitriou three-player games are hard in 
electronic colloquium on computational complexity 
 
 k m j de bontridder b v halld´orsson m m 
halld´orsson c a j hurkens j k lenstra r ravi 
and l stougie approximation algorithms for the test 
cover problem mathematical programming 
 - - september 
 ap dijksterhuis maarten w bos loran f nordgren 
and rick b van baaren on making the right choice 
the deliberation-without-attention effect science 
 - february 
 rosemary emery-montemerlo geoff gordon jeff 
schneider and sebastian thrun approximate 
solutions for partially observable stochastic games 
with common payoffs in autonomous agents and 
multi-agent systems 
 alex fabrikant christos papadimitriou and kunal 
talwar the complexity of pure nash equilibria in 
proceedings of the symposium on the theory of 
computing 
 kyna fong multi-stage information acquisition in 
auction design senior thesis harvard college 
 lance fortnow and duke whang optimality and 
domination in repeated games with bounded players 
in proceedings of the symposium on the theory of 
computing pages - 
 yoav freund michael kearns yishay mansour dana 
ron ronitt rubinfeld and robert e schapire 
efficient algorithms for learning to play repeated 
games against computationally bounded adversaries 
in proceedings of the foundations of computer 
science pages - 
 drew fudenberg and jean tirole game theory mit 
 
 michel x goemans and jon kleinberg an improved 
approximation ratio for the minimum latency problem 
mathematical programming - 
 paul w goldberg and christos h papadimitriou 
reducibility among equilibrium problems in 
electronic colloquium on computational complexity 
 
 m grotschel l lovasz and a schrijver the 
ellipsoid method and its consequences in combinatorial 
optimization combinatorica - 
 anupam gupta and amit kumar sorting and 
selection with structured costs in proceedings of the 
foundations of computer science pages - 
 
 eric a hansen daniel s bernstein and shlomo 
zilberstein dynamic programming for partially 
observable stochastic games in national conference 
on artificial intelligence aaai 
 john c harsanyi games with incomplete information 
played by bayesian players management science 
 - 
 sergiu hart and david schmeidler existence of 
correlated equilibria mathematics of operations 
research - 
 eric horvitz and geoffrey rutledge time-dependent 
utility and action under uncertainty in uncertainty in 
artificial intelligence pages - 
 g e hughes and m j cresswell a new 
introduction to modal logic routledge 
 sheena s iyengar and mark r lepper when choice 
is demotivating can one desire too much of a good 
thing j personality and social psychology 
 - 
 ehud kalai bounded rationality and strategic 
complexity in repeated games game theory and 
applications pages - 
 
 sampath kannan and sanjeev khanna selection with 
monotone comparison costs in proceedings of the 
symposium on discrete algorithms pages - 
 l g khachiyan a polynomial algorithm in linear 
programming dokklady akademiia nauk sssr 
 
 daphne koller and nimrod megiddo the complexity 
of two-person zero-sum games in extensive form 
games and economic behavior - 
 daphne koller nimrod megiddo and bernhard von 
stengel efficient computation of equilibria for 
extensive two-person games games and economic 
behavior - 
 kate larson mechanism design for computationally 
limited agents phd thesis cmu 
 kate larson and tuomas sandholm bargaining with 
limited computation deliberation equilibrium 
artificial intelligence - 
 kate larson and tuomas sandholm costly valuation 
computation in auctions in proceedings of the 
theoretical aspects of rationality and knowledge 
july 
 kate larson and tuomas sandholm strategic 
deliberation and truthful revelation an impossibility 
result in proceedings of the acm conference on 
electronic commerce may 
 c e lemke and j t howson jr equilibrium points 
of bimatrix games j society for industrial and 
applied mathematics 
 richard j lipton evangelos markakis and aranyak 
mehta playing large games using simple strategies in 
proceedings of the acm conference on electronic 
commerce pages - 
 michael l littman michael kearns and satinder 
singh an efficient exact algorithm for singly 
connected graphical games in proceedings of neural 
information processing systems 
 steven a matthews and nicola persico information 
acquisition and the excess refund puzzle technical 
report - department of economics university 
of pennsylvania march 
 richard d mckelvey and andrew mclennan 
computation of equilibria in finite games in 
h amman d a kendrick and j rust editors 
handbook of compututational economics volume 
pages - elsevier 
 b m e moret and h d shapiro on minimizing a set 
of tests siam j scientific statistical computing 
 - 
 h moulin and j -p vial strategically zero-sum 
games the class of games whose completely mixed 
equilibria cannot be improved upon international j 
game theory 
 john f nash jr equilibrium points in n-person 
games proceedings of the national academy of 
sciences - 
 abraham neyman finitely repeated games with finite 
automata mathematics of operations research 
 - august 
 christos papadimitriou on the complexity of the 
parity argument and other inefficient proofs of 
existence j computer and system sciences 
 - 
 christos papadimitriou algorithms games and the 
internet in proceedings of the symposium on the 
theory of computing pages - 
 christos h papadimitriou computing correlated 
equilibria in multi-player games in proceedings of the 
symposium on the theory of computing 
 christos h papadimitriou and tim roughgarden 
computing equilibria in multiplayer games in 
proceedings of the symposium on discrete algorithms 
 
 christos h papadimitriou and mihalis yannakakis 
on bounded rationality and computational 
complexity in proceedings of the symposium on the 
theory of computing pages - 
 david c parkes auction design with costly 
preference elicitation annals of mathematics and 
artificial intelligence - 
 nicola persico information acquisition in auctions 
econometrica - 
 jean-pierre ponssard and sylvain sorin the lp 
formulation of finite zero-sum games with incomplete 
information international j game theory 
 - 
 eric rasmussen strategic implications of uncertainty 
over one s own private value in auctions technical 
report indiana university 
 leonardo rezende mid-auction information 
acquisition technical report university of illinois 
 
 ariel rubinstein modeling bounded rationality mit 
 
 barry schwartz the paradox of choice why more is 
less ecco 
 herbert simon models of bounded rationality mit 
 
 i simonson and a tversky choice in context 
tradeoff contrast and extremeness aversion j 
marketing research - 
 brian skyrms dynamic models of deliberation and 
the theory of games in proceedings of the theoretical 
aspects of rationality and knowledge pages - 
 
 richard sutton and andrew barto reinforcement 
learning an introduction mit 
 john von neumann and oskar morgenstern theory of 
games and economic behavior princeton 
 bernhard von stengel computing equilibria for 
two-person games in r j aumann and s hart 
editors handbook of game theory with econonic 
applications volume pages - elsevier 
 
 s zilberstein and s russell approximate reasoning 
using anytime algorithms in s natarajan editor 
imprecise and approximate computation kluwer 
 
 
on decentralized incentive compatible mechanisms 
for partially informed environments 
∗ 
ahuva mu alem 
school of engineering and computer science 
the hebrew university of jerusalem 
ahumu cs huji ac il 
abstract 
algorithmic mechanism design focuses on dominant 
strategy implementations the main positive results are the 
celebrated vickrey-clarke-groves vcg mechanisms and 
computationally efficient mechanisms for severely restricted 
players single-parameter domains as it turns out many 
natural social goals cannot be implemented using the 
dominant strategy concept this suggests that the 
standard requirements must be relaxed in order to construct 
general-purpose mechanisms 
we observe that in many common distributed 
environments computational entities can take advantage of the 
network structure to collect and distribute information we 
thus suggest a notion of partially informed environments 
even if the information is recorded with some probability 
this enables us to implement a wider range of social goals 
using the concept of iterative elimination of weakly dominated 
strategies as a result cooperation is achieved independent 
of agents belief as a case study we apply our methods to 
derive peer-to-peer network mechanism for file sharing 
categories and subject descriptors 
j social and behavioral sciences economics 
general terms 
design algorithms 
 introduction 
recently global networks have attracted widespread study 
the emergence of popular scalable shared networks with 
self-interested entities - such as peer-to-peer systems over 
the internet and mobile wireless communication ad-hoc 
networks - poses fundamental challenges 
naturally the study of such giant decentralized systems 
involves aspects of game theory in particular the 
subfield of mechanism design deals with the construction 
of mechanisms for a given social goal the challenge is to 
design rules for interaction such that selfish behavior of the 
agents will result in the desired social goal 
algorithmic mechanism design amd focuses on 
efficiently computable constructions distributed 
algorithmic mechanism design damd studies mechanism design 
in inherently decentralized settings the standard 
model assumes rational agents with quasi-linear utilities and 
private information playing dominant strategies 
the solution concept of dominant strategies - in which 
each player has a best response strategy regardless of the 
strategy played by any other player - is well suited to the 
assumption of private information in which each player is 
not assumed to have knowledge or beliefs regarding the other 
players the appropriateness of this set-up stems from the 
strength of the solution concept which complements the 
weak information assumption many mechanisms have been 
constructed using this set-up e g most 
of these apply to severely-restricted cases e g single-item 
auctions with no externalities in which a player s 
preference is described by only one parameter single-parameter 
domains 
to date vickrey-clarke-groves vcg mechanisms are 
the only known general method for designing dominant 
strategy mechanisms for general domains of preferences 
however in distributed settings without available subsidies from 
outside sources vcg mechanisms cannot be accepted as 
valid solutions due to a serious lack of budget balance 
additionally for some domains of preferences vcg mechanisms 
and weighted vcg mechanisms are faced with 
computational hardness further limitations of the set-up 
are discussed in subsection 
in most distributed environments players can take 
advantage of the network structure to collect and distribute 
information about other players this paper thus studies 
the effects of relaxing the private information assumption 
 
one model that has been extensively studied recently is 
the peer-to-peer p p network a p p network is a 
distributed network with no centralized authority in which the 
participants share their individual resources e g 
processing power storage capacity bandwidth and content the 
aggregation of such resources provides inexpensive 
computational platforms the most popular p p networks are 
those for sharing media files such as napster gnutella and 
kazaa recent work on p p incentives include 
micropayment methods and reputation-based methods 
the following description of a p p network scenario 
illustrates the relevance of our relaxed informational assumption 
example consider a peer-to-peer network for file 
sharing whenever agent b uploads a file from agent a all peers 
along the routing path know that b has loaded the file they 
can record this information about agent b in addition they 
can distribute this information 
however it is impossible to record all the information 
everywhere first such duplication induces huge costs 
second as agents dynamically enter and exit from the network 
the information might not be always available and so it is 
seems natural to consider environments in which the 
information is locally recorded that is the information is recorded 
in the closest neighborhood with some probability p 
in this paper we shall see that if the information is 
available with some probability then this enables us to 
implement a wider range of social goals as a result cooperation 
is achieved independent of agents belief this demonstrates 
that in some computational contexts our approach is far less 
demanding than the bayesian approach that assumes that 
players types are drawn according to some identified 
probability density function 
 implementations in complete information 
set-ups 
in complete information environments each agent is 
informed about everyone else that is each agent observes his 
own preference and the preferences of all other agents 
however no outsider can observe this information specifically 
neither the mechanism designer nor the court many 
positive results were shown for such arguably realistic settings 
for recent surveys see 
moore and repullo implement a large class of social goals 
using sequential mechanisms with a small number of rounds 
 the concept they used is subgame-perfect 
implementations spe 
the spe-implementability concept seems natural for the 
following reasons the designed mechanisms usually have 
non-artificial constructs and a small strategy space as 
a result it is straightforward for a player to compute his 
strategy 
second sequential mechanisms avoid 
simultaneous moves and thus can be considered for distributed 
networks third the constructed mechanisms are often 
decentralized i e lacking a centralized authority or designer 
 
interestingly in real life players do not always use their 
subgame perfect strategies one such widely studied case is the 
ultimatum bargaining -person game in this simple game 
the proposer first makes an offer of how to divide a certain 
known sum of money and the responder either agrees or 
refuses in the latter case both players earn zero somewhat 
surprisingly experiments show that the responder often 
rejects the suggested offer even if it is bounded away from 
zero and the game is played only once see e g 
and budget-balanced i e transfers always sum up to zero 
this happens essentially if there are at least three players 
and a direct network link between any two agents finally 
moore and repullo observed that they actually use a 
relaxed complete information assumption it is only required 
that for every player there exists only one other player who 
is informed about him 
 implementations in partially informed 
set-ups and our results 
the complete information assumption is realistic for small 
groups of players but not in general in this paper we 
consider players that are informed about each other with 
some probability more formally we say that agent b is 
p-informed about agent a if b knows the type of a with 
probability p 
for such partially-informed environments we show how to 
use the solution concept of iterative elimination of weakly 
dominated strategies we demonstrate this concept through 
some motivating examples that i seem natural in distributed 
settings and ii cannot be implemented in dominant 
strategies even if there is an authorized center with a direct 
connection to every agent or even if players have single-parameter 
domains 
 we first show how the subgame perfect techniques of 
moore and repullo can be applied to p-informed 
environments and further adjusted to the concept of 
iterative elimination of weakly dominated strategies 
 for large enough p 
 we then suggest a certificate based challenging method 
that is more natural in computerized p-informed 
environments and different from the one introduced by 
moore and repullo for p ∈ 
 we consider implementations in various network 
structures 
as a case study we apply our methods to derive 
simplified peer-to-peer network for file sharing with no 
payments in equilibrium our approach is agent file -specific 
 web-cache budget-balanced and economically efficient 
mechanism 
our mechanisms use reasonable punishments that inversely 
depend on p and so if the fines are large then small p is 
enough to induce cooperation essentially large p implies a 
large amount of recorded information 
 malicious agents 
decentralized mechanisms often utilize punishing outcomes 
as a result malicious players might cause severe harm to 
others we suggest a quantified notion of malicious player 
who benefits from his own gained surplus and from harm 
caused to others suggests several categories to classify 
non-cooperating players our approach is similar to and 
the references therein who considered independently such 
players in different context we show a simple 
decentralized mechanism in which q-malicious players cooperate and 
in particular do not use their punishing actions in 
equilibrium 
 
 dominant strategy implementations 
in this subsection we shall refer to some recent results 
demonstrating that the set-up of private information with 
the concept of dominant strategies is restrictive in general 
first roberts classical impossibility result shows that if 
players preferences are not restricted and there are at least 
 different outcomes then every dominant-strategy 
mechanism must be weighted vcg with the social goal that 
maximizes the weighted welfare for slightly-restricted 
preference domains it is not known how to turn efficiently 
computable algorithms into dominant strategy mechanisms 
this was observed and analyzed in recently 
extends roberts result to some leading examples they 
showed that under mild assumptions any dominant 
strategy mechanism for variety of combinatorial auctions over 
multi-dimensional domains must be almost weighted vcg 
additionally it turns out that the dominant strategy 
requirement implies that the social goal must be monotone 
 this condition is very restrictive as 
many desired natural goals are non-monotone 
 
several recent papers consider relaxations of the dominant 
strategy concept however most 
of these positive results either apply to severely restricted 
cases e g single-parameter players or amount to vcg 
or almost vcg mechanisms e g recently 
considered implementations for generalized single-parameter 
players 
organization of this paper in section we illustrate 
the concepts of subgame perfect and iterative elimination 
of weakly dominated strategies in completely-informed and 
partially-informed environments in section we show a 
mechanism for peer-to-peer file sharing networks in section 
 we apply our methods to derive a web cache mechanism 
future work is briefly discussed in section 
 motivating examples 
in this section we examine the concepts of subgame perfect 
and iterative elimination of weakly dominated strategies for 
completely informed and p-informed environments we also 
present the notion of q-maliciousness and some other related 
considerations through two illustrative examples 
 the fair assignment problem 
our first example is an adjustment to computerized 
context of an ancient procedure to ensure that the wealthiest 
man in athens would sponsor a theatrical production known 
as the choregia in the fair assignment problem alice 
and bob are two workers and there is a new task to be 
performed their goal is to assign the task to the least loaded 
worker without any monetary transfers the informational 
assumption is that alice and bob know both loads and the 
duration of the new task 
 
e g minimizing the makespan within a factor of and 
rawls rule over some multi-dimensional domains 
 
in first glance one might ask why the completely informed 
agents could not simply sign a contract specifying the 
desired goal such a contract is sometimes infeasible due to 
fact that the true state cannot be observed by outsiders 
especially not the court 
claim the fair assignment goal cannot be implemented 
in dominant strategies 
 basic mechanism 
the following simple mechanism implements this goal in 
subgame perfect equilibrium 
 stage alice either agrees to perform the new task 
or refuses 
 stage if she refuses bob has to choose between 
- a performing the task himself 
- b exchanging his load with alice and 
performing the new task as well 
let lt 
a lt 
b be the true loads of alice and bob and let 
t be the load of the new task assume that load 
exchanging takes zero time and cost we shall see that the 
basic mechanism achieves the goal in a subgame perfect 
equilibrium intuitively this means that in equilibrium each 
player will choose his best action at each point he might 
reach assuming similar behavior of others and thus every 
spe is a nash equilibrium 
claim the task is assigned to the least loaded 
worker in subgame perfect equilibrium 
proof by backward induction argument look forward 
and reason backward consider the following cases 
 lt 
b ≤ lt 
a if stage is reached then bob will not 
exchange 
 lt 
a lt 
b lt 
a t if stage is reached bob will 
exchange and this is what alice prefers 
 lt 
a t ≤ lt 
b if stage is reached then bob would 
exchange as a result it is strictly preferable by alice 
to perform the task 
note that the basic mechanism does not use monetary 
transfers at all and is decentralized in the sense that no third 
party is needed to run the procedure the goal is achieved in 
equilibrium ties are broken in favor of alice however in 
the second case exchange do occur in an equilibrium point 
recall the unrealistic assumption that load exchange takes 
zero time and cost introducing fines the next mechanism 
overcomes this drawback 
 elicitation mechanism 
in this subsection we shall see a centralized mechanism for 
the fair assignment goal without load exchange in 
equilibrium the additional assumptions are as follows the cost 
performing a load of duration d is exactly d we assume 
that the duration t of the new task is t the payoffs 
 
proof assume that there exists a mechanism that 
implements this goal in dominant strategies then by the 
revelation principle there exists a mechanism that implements 
this goal for which the dominant strategy of each player is 
to report his true load clearly truthfully reporting cannot 
be a dominant strategy for this goal if monetary transfers 
are not available as players would prefer to report higher 
loads 
 
of the utility maximizers agents are quasilinear the 
following mechanism is an adaptation of moore and repullo s 
elicitation mechanism 
 
 stage elicitation of alice s load 
alice announces la 
bob announces la ≤ la 
if la la bob agrees goto the next stage 
otherwise bob challenges alice is assigned the 
task 
she then has to choose between 
- a transferring her original load to bob and 
paying him la − · min{ la − la} 
alice pays to the mechanism 
bob pays the fine of t to the mechanism 
- b no load transfer alice pays to bob stop 
 stage the elicitation of bob s load is similar to 
stage switching the roles of alice and bob 
 stage if la lb alice is assigned the task 
otherwise bob stop 
observe that alice is assigned the task and fined with 
whenever bob challenges we shall see that the bonus of 
is paid to a challenging player only in out of equilibria cases 
claim if the mechanism stops at stage then the 
payoff of each agent is at least −t and at most 
proposition it is a subgame perfect equilibrium of the 
elicitation mechanism to report the true load and to 
challenge with the true load only if the other agent overreports 
proof assume w l o g that the elicitation of alice s load 
is done after bob s and that stage is reached if alice 
truly reports la lt 
a bob strictly prefers to agree 
otherwise if bob challenges alice would always strictly prefer 
to transfer as in this case bob would perform her load for 
smaller cost as a result bob would pay t to the 
mechanism this punishing outcome is less preferable than the 
normal outcome of stage achieved had he agreed 
if alice misreports la lt 
a then bob can ensure himself 
the bonus which is always strictly preferable than reaching 
stage by challenging with la lt 
a and so whenever 
bob gets the bonus alice gains the worst of all payoffs 
reporting a lower load la lt 
a is not beneficial for alice 
in this case bob would strictly prefer to agree and not to 
announce la la as he limited to challenge with a smaller 
load than what she announces thus such misreporting can 
only increase the possibility that she is assigned the task 
and so there is no incentive for alice to do so 
all together alice would prefer to report the truth in this 
stage and so stage would not abnormally end by stop 
and similarly stage 
observe that the elicitation mechanism is almost balanced 
in all outcomes no money comes in or out except for the 
non-equilibrium outcome a in which both players pay to 
the mechanism 
 
in if an agent misreport his type then it is always 
beneficial to the other agent to challenge in particular 
even if the agent reports a lower load 
 elicitation mechanism for partially informed 
agents 
in this subsection we consider partially informed agents 
formally 
definition an agent a is p-informed about agent b 
if a knows the type of b with probability p independently 
of what b knows 
it turns out that a version of the elicitation mechanism 
works for this relaxed information assumption if we use the 
concept of iterative elimination of weakly dominated 
strategies 
 we replace the fixed fine of in the elicitation 
mechanism with the fine 
βp max{l 
 − p 
 p − 
t} 
and assume the bounds lt 
a lt 
b ≤ l 
proposition if all agents are p-informed p 
the elicitation mechanism βp implements the fair 
assignment goal with the concept of iterative elimination of weakly 
dominated strategies the strategy of each player is to report 
the true load and to challenge with the true load if the other 
agent overreport 
proof assume w l o g that the elicitation of alice s load 
is done after bob s and that stage is reached first 
observe that underreporting the true value is a dominated 
strategy whether bob is not informed and mistakenly 
challenges with a lower load as βp ≥ l or not or even 
if t is very small now we shall see that overreporting her 
value is a dominated strategy as well 
alice s expected payoff gained by misreporting ≤ 
p payoff if she lies and bob is informed − p max 
payoff if bob is not informed ≤ 
p −t − βp p −t − p −t − βp ≤ 
p min payoff of true report if bob is informed − p 
 min payoff if bob is not informed ≤ 
alice s expected payoff if she truly reports 
the term −t−βp in the left hand side is due to the fact 
that if bob is informed he will always prefer to challenge 
in the right hand side if he is informed then challenging is 
a dominated strategy and if he is not informed the worst 
harm he can make is to challenge thus in stage alice will 
report her true load this implies that challenging without 
being informed is a dominated strategy for bob 
this argument can be reasoned also for the first stage 
when bob reports his value bob knows the maximum payoff 
he can gain is at most zero since he cannot expect to get the 
bonus in the next stage 
 extensions 
the elicitation mechanism for partially informed agents is 
rather general as in we need the capability to judge 
between two distinct declarations in the elicitation rounds 
 
a strategy si of player i is weakly dominated if there exists 
si such that i the payoff gained by si is at least as high as 
the payoff gained by si for all strategies of the other players 
and all preferences and ii there exist a preference and a 
combination of strategies for the other players such that the 
payoff gained by si is strictly higher than the payoff gained 
by si 
 
and upper and lower bounds based on the possible payoffs 
derived from the last stage in addition for p-informed 
environments some structure is needed to ensure that 
underbidding is a dominated strategy 
the choregia-type mechanisms can be applied to more 
than players with the same number of stages the player in 
the first stage can simply points out the name of the 
wealthiest agent similarly the elicitation mechanisms can be 
extended in a straightforward manner these mechanisms can 
be budget-balanced as some player might replace the role 
of the designer and collect the fines as observed in 
open problem design a decentralized budget balanced 
mechanism with reasonable fines for independently p-informed 
n players where p ≤ − 
 
n− 
 seller and buyer scenario 
a player might cause severe harm to others by choosing a 
non-equilibrium outcome in the mechanism for the fair 
assignment goal an agent might maliciously challenge even 
if the other agent truly reports his load in this subsection 
we consider such malicious scenarios for the ease of 
exposition we present a second example we demonstrate that 
equilibria remain unchanged even if players are malicious 
in the seller-buyer example there is one item to be traded 
and two possible future states the goal is to sell the item for 
the average low price pl ls lb 
 
in state l and the higher 
price ph hs hb 
 
in the other state h where ls is seller s 
cost and lb is buyer s value in state l and similarly hs hb in 
h the players fix the prices without knowing what will be 
the future state assume that ls hs lb hb and that 
trade can occur in both prices that is pl ph ∈ hs lb 
only the players can observe the realization of the true 
state the payoffs are of the form ub xv−tb us ts −xvs 
where the binary variable x indicates if trade occurred and 
tb ts are the transfers consider the following decentralized 
trade mechanism 
 stage if seller reports h goto stage otherwise 
trade at the low price pl stop 
 stage the buyer has to choose between 
- a trade at the high price ph 
- b no trade and seller pays ∆ to the buyer 
claim let ∆ lb−ph the unique subgame perfect 
equilibrium of the trade mechanism is to report the true state 
in stage and trading if stage is reached 
note that the outcome b is never chosen in equilibrium 
 trade mechanism for malicious agents 
the buyer might maliciously punish the seller by 
choosing the outcome b when the true state is h the following 
notion quantifies the consideration that a player is not 
indifferent to the private surpluses of others 
definition a player is q-malicious if his payoff equals 
 − q his private surplus − q summation of others 
surpluses q ∈ 
this definition appeared independently in in different 
context we shall see that the traders would avoid such bad 
behavior if they are q-malicious where q that is if 
their non-indifference impact is bounded by 
equilibria outcomes remain unchanged and so cooperation is 
achieved as in the original case of non-malicious players 
consider the trade mechanism with pl − q hs q lb 
ph q hs − q lb ∆ − q hb − lb − note that 
pl ph for q 
claim if q then the unique subgame perfect 
equilibrium for q-malicious players remains unchanged 
proof by backward induction we consider two cases 
in state h the q-malicious buyer would prefer to trade if 
 − q hb − ph q hs − ph − q ∆ q ∆ indeed 
 − q hb qhs ∆ ph trivially the seller prefers to 
trade at the higher price − q pl − hs q pl − hb 
 − q ph − hs q ph − hb 
in state l the buyer prefers the no trade outcome as 
 −q lb −ph q ls −ph ∆ the seller prefers to trade 
at a low price as − q pl − ls q pl − lb −∆ 
 discussion 
no mechanism can nash-implement this trading goal if 
the only possible outcomes are trade at pl and trade at ph 
to see this it is enough to consider normal forms as any 
extensive form mechanism can be presented as a normal one 
consider a matrix representation where the seller is the row 
player and the buyer is the column player in which every 
entry includes an outcome suppose there is equilibrium entry 
for the state l the associate column must be all pl 
otherwise the seller would have an incentive to deviate similarly 
the associate row of the h equilibrium entry must be all ph 
 otherwise the buyer would deviate a contradiction 
the buyer prefers pl and seller ph and so the preferences 
are identical in both states hence reporting preferences 
over outcomes is not enough - players must supply 
additional information this is captured by outcome b in 
the trade mechanism 
intuitively if a goal is not nash-implementable we need 
to add more outcomes the drawback is that some new 
additional equilibria must be ruled out e g additional 
nash equilibrium for the trade mechanism is trade at pl 
 b that is the seller chooses to trade at low price at either 
states and the buyer always chooses the no trade option that 
fines the seller if the second stage is reached such buyer s 
threat is not credible because if the mechanism is played 
only once and stage is reached in state h the buyer would 
strictly decrease his payoff if he chooses b clearly this is 
not a subgame perfect equilibrium although each extensive 
game-form is strategically equivalent to a normal form one 
the extensive form representation places more structure and 
so it seems plausible that the subgame perfect equilibrium 
will be played 
 
formally this goal is not maskin monotonic a necessary 
condition for nash-implementability 
 
a similar argument applies for the fair assignment 
problem 
 
interestingly it is a straight forward to construct a 
sequential mechanism with unique spe and additional ne with a 
strictly larger payoff for every player 
 
 peer-to-peer networks 
in this section we describe a simplified peer-to-peer 
network for file sharing without payments in equilibrium using 
a certificate-based challenging method in this challenging 
method - as opposed to - an agent that challenges cannot 
harm other agents unless he provides a valid certificate 
in general if agent b copied a file f from agent a then 
agent a knows that agent b holds a copy of the file we 
denote such information as a certificate b f we shall omit 
cryptographic details such a certificate can be recorded 
and distributed along the network and so we can treat each 
agent holding the certificate as an informed agent 
assumptions we assume an homogeneous system with 
files of equal size the benefit each agent gains by holding 
a copy of any file is v the only cost each agent has is 
the uploading cost c induced while transferring a file to 
an immediate neighbor all other costs are negligible e g 
storing the certificates forwarding messages providing 
acknowledgements digital signatures etc let upa downa 
be the numbers of agent a uploads and downloads if he 
always cooperates we assume that each agent a enters the 
system if upa · c downa · v 
each agent has a quasilinear utility and only cares about 
his current bandwidth usage in particular he ignores future 
scenarios e g whether forwarding or dropping of a packet 
might affect future demand 
 basic mechanism 
we start with a mechanism for a network with p-informed 
agents b a a we assume that b is directly connected 
to a and a 
if b has the certificate a f then he can apply directly 
to a and request the file if he refuses then b can go to 
court the following basic sequential mechanism is 
applicable whenever agent b is not informed and still would like to 
download the file if it exists in the network note that this 
goal cannot be implemented in dominant strategies 
without payments similar to claim when the type of each 
agent here is the set of files he holds define ta b to be the 
monetary amount that agent a should transfer to b 
 stage agent b requests the file f from a 
- if a replies yes then b downloads the file from 
a stop 
- otherwise agent b sends a s no reply to agent 
a 
∗ if a declares agree then goto the next stage 
∗ else a sends a certificate a f to agent b 
· if the certificate is correct then ta a 
βp stop 
· else ta a c stop 
stage agent b requests the file f from a switch 
the roles of the agents a a 
claim the basic mechanism is budget-balanced 
 transfers always sum to zero and decentralized 
theorem let βp c 
p 
 p ∈ a strategy that 
survives iterative elimination of weakly dominated strategies 
is to reply yes if ai holds the file and to challenge only 
with a valid certificate as a result b downloads the file if 
some agent holds it in equilibrium there are no payments 
or transfers in equilibrium 
proof clearly if the mechanism ends without 
challenging −c ≤ u ai ≤ and so challenging with an invalid 
certificate is always a dominated strategy now when stage 
 is reached a is the last to report if he has the file if a 
has the file it is a weakly undominated strategy to misreport 
whether a is informed or not 
a s expected payoff gained by misreporting no ≤ 
p · −βp − p · −c ≤ a s payoff if she reports 
yes 
this argument can be reasoned also for stage when 
a reports whether he has the file a knows that a will 
report yes if and only if she has the file in the next stage 
and so the maximum payoff he can gain is at most zero since 
he cannot expect to get a bonus 
 chain networks 
in a chain network agent b is directly connected to a 
and ai is directly connected to agent ai assume that we 
have an acknowledgment protocol to confirm the receipt of 
a particular message to avoid message dropping we add 
the fine βp to be paid by an agent who hasn t properly 
forwarded a message the chain mechanism follows 
 stage i agent b forwards a request for the file f to 
ai through {ak}k≤i 
 if ai reports yes then b downloads f from ai 
stop 
 otherwise ai reports no 
if aj sends a certificate ak f to b j k ≤ i then 
- if certificate ak f is correct then t ak aj 
βp stop 
- else t aj ak c stop 
if ai reports that he has no copy of the file then any agent 
in between might challenge using digital signatures and 
acknowledgements observe that every agent must forward 
each message even if it contains a certificate showing that 
he himself has misreported 
we use the same fine βp as in the basic mechanism 
because the protocol might end at stage clearly the former 
analysis still applies since the actual p increases with the 
number of players 
 network mechanism 
in this subsection we consider general network structures 
we need the assumption that there is a ping protocol that 
checks whether a neighbor agent is on-line or not that is 
an on-line agent cannot hide himself to limit the amount 
of information to be recorded we assume that an agent is 
committed to keep any downloaded file to at least one hour 
and so certificates are valid for a limited amount of time we 
assume that each agent has a digitally signed listing of his 
current immediate neighbors as in real p p file sharing 
applications we restrict each request for a file to be forwarded 
at most r times that is downloads are possible only inside 
a neighborhood of radius r 
 
the network mechanism utilizes the chain mechanism in 
the following way when agent b requests a file from agent 
a at most r − far then a sends to b the list of his 
neighbors and the output of the ping protocol to all of these 
neighbors as a result b can explore the network 
remark in this mechanism we assumed that the 
environment is p-informed an important design issue that it 
is not addressed here is the incentives for the information 
propagation phase 
 web cache 
web caches are widely used tool to improve overall 
system efficiency by allowing fast local access they were listed 
in as a challenging application of distributed 
algorithmic mechanism design 
nisan considered a single cache shared by strategic 
agents in this problem agent i gains the value vt 
i if a 
particular item is loaded to the local shared cache the 
efficient goal is to load the item if and only if σvt 
i ≥ c 
where c is the loading cost this goal reduces to the public 
project problem analyzed by clarke however it is well 
known that this mechanism is not budget-balanced e g if 
the valuation of each player is c then everyone pays zero 
in this section we suggest informational and 
environmental assumptions for which we describe a decentralized 
budgetbalanced efficient mechanism we consider environments 
for which future demand of each agent depends on past 
demand the underlying informational and environmental 
requirements are as follows 
 an agent can read the content of a message only if he 
is the target node even if he has to forward the 
message as an intermediate node of some routing path 
an agent cannot initiate a message on behalf of other 
agents 
 an acknowledgement protocol is available so that 
every agent can provide a certificate indicating that he 
handled a certain message properly 
 negligible costs we assume p-informed agents where 
p is such that the agent s induced cost for keeping 
records of information is negligible we also assume 
that the cost incurred by sending and forwarding 
messages is negligible 
 let qi t denotes the number of loading requests agent 
i initiated for the item during the time slot t we 
assume that vt 
i t the value for caching the item in 
the beginning of slot t depends only on most recent 
slot formally vt 
i t max{vi qi t − c} where 
vi · is a non-decreasing real function in addition 
vi · is a common knowledge among the players 
 the network is homogeneous in the sense that if 
agent j happens to handle k requests initiated by agent 
i during the time slot t then qi t kα where α 
depends on the routing protocol and the environment 
 α might be smaller than if each request is flooded 
several times we assume that the only way agent i 
can affect the true qi t is by superficially increasing 
his demand for the cached item but not the other way 
 that is agent s loss incurred by giving up a necessary 
request for the item is not negligible 
the first requirement is to avoid free riding and also to 
avoid the case that an agent superficially increases the 
demand of others and as a result decreases his own demand 
the second requirement is to avoid the case that an agent 
who gets a routing request for the item records it and then 
drops it the third is to ensure that the environment stays 
well informed in addition if the forwarding cost is 
negligible each agent cooperates and forwards messages as he would 
not like to decrease the future demand that monotonically 
depends on the current time slot as assumed in the forth 
requirement of some other agent given that the payments 
are increasing with the declared values the forth and fifth 
requirements ensure that the agent would not increase his 
demand superficially and so qi t is the true demand 
the following web-cache mechanism implements the 
efficient goal that shares the cost proportionally for simplicity 
it is described for two players and w l o g vt 
i t equals the 
number of requests initiated by i and observed by any 
informed j that is α and vi qi t − qi t − 
 stage elicitation of vt 
a t 
alice announces va 
bob announces va ≥ va if va va goto the next 
stage otherwise bob challenges 
- if bob provides va valid records then alice pays c 
to finance the loading of the item into the cache 
she also pays βp to bob stop 
- otherwise bob finances the loading of the item 
into the cache stop 
 stage the elicitation of vt 
b t is done analogously 
 stage if va vb c then stop 
otherwise load the item to the cache alice pays pa 
va 
va vb 
· c and bob pays pb vb 
va vb 
· c 
claim it is a dominated strategy to overreport the true 
value 
proof let vt 
a va there are two cases to consider 
 if vt 
a vb c and va vb ≥ c 
we need to show that if the mechanism stops normally 
alice would pay more than vt 
a that is va 
va vb 
·c vt 
a 
indeed va c va vt 
a vb vt 
a va vb 
 if vt 
a vb ≥ c then clearly va 
va vb 
 
vt 
a 
vt 
a 
 vb 
 
theorem let βp max{ − p 
p 
· c} p ∈ 
 a strategy that survives iterative elimination of weakly 
dominated strategies is to report the truth and to challenge 
only when the agent is informed the mechanism is efficient 
budget-balanced exhibits consumer sovereignty no positive 
transfer and individual rationality 
 
proof challenging without being informed that is 
without providing enough valid records is always dominated 
strategy in this mechanism now assume w l o g alice is 
 
see or for exact definitions 
 
the last to report her value alice s expected payoff gained 
by underreporting ≤ 
p · −c − βp − p · c p · − p · ≤ alice s 
expected payoff if she honestly reports 
the right hand side equals zero as the participation costs 
are negligible reasoning back bob cannot expect to get 
the bonus and so misreporting is dominated strategy for 
him 
 concluding remarks 
in this paper we have seen a new partial informational 
assumption and we have demonstrated its suitability to 
networks in which computational agents can easily collect 
and distribute information we then described some 
mechanisms using the concept of iterative elimination of weakly 
dominated strategies some issues for future work include 
 as we have seen the implementation issue in p-informed 
environments is straightforward - it is easy to construct 
incentive compatible mechanisms even for 
non-singleparameter cases the challenge is to find more realistic 
scenarios in which the partial informational 
assumption is applicable 
 mechanisms for information propagation and 
maintenance in our examples we choose p such that the 
maintenance cost over time is negligible however 
the dynamics of the general case is delicate an agent 
can use the recorded information to eliminate data 
that is not likely to be needed in order to decrease 
his maintenance costs as a result the probability 
that the environment is informed decreases and 
selfish agents would not cooperate incentives for 
information propagation should be considered as well e g 
for p p networks for file sharing 
 it seems that some social choice goals cannot be 
implemented if each player is at least n-malicious where 
n is the number of players it would be interesting to 
identify these cases 
acknowledgements 
we thank meitav ackerman moshe babaioff liad 
blumrozen michal feldman daniel lehmann noam nisan motty 
perry and eyal winter for helpful discussions 
 references 
 a archer and e tardos truthful mechanisms for 
one-parameter agents in ieee symposium on 
foundations of computer science pages - 
 
 aaron archer christos papadimitriou kunal talwar 
and eva tardos an approximate truthful mechanism 
for combinatorial auctions with single parameter 
agent in soda 
 moshe babaioff ron lavi and elan pavlov 
single-parameter domains and implementation in 
undominated strategies working paper 
 yair bartal rica gonen and noam nisan incentive 
compatible multi-unit combinatorial auctions 
tark- 
 sushil bikhchandani shurojit chatterji and arunava 
sen incentive compatibility in multi-unit auctions 
 working paper 
 liad blumrosen noam nisan and ilya segal 
auctions with severely bounded communication 
working paper 
 f brandt t sandholm and y shoham spiteful 
bidding in sealed-bid auctions 
 patrick briest piotr krysta and berthold voecking 
approximation techniques for utilitarian mechanism 
design in stoc 
 chiranjeeb buragohain divy agrawal and subhash 
suri a game-theoretic framework for incentives in 
p p systems in ieee p p 
 e h clarke multipart pricing of public goods public 
choice - 
 joan feigenbaum christos papadimitrios and scott 
shenkar sharing the cost of multicast transmissions 
computer and system sciences 
 joan feigenbaum and scott shenker distributed 
algorithmic mechanism design recent results and 
future directions in proceedings of the th 
international workshop on discrete algorithms and 
methods for mobile computing and communications 
pages - acm press new york 
 m feldman k lai i stoica and j chuang robust 
incentive techniques for peer-to-peer networks in ec 
 
 a goldberg j hartline a karlin and a wright 
competitive auctions working paper 
 philippe golle kevin leyton-brown ilya mironov 
and mark lillibridge incentives for sharing in 
peer-to-peer networks in ec 
 ron holzman noa kfir-dahav dov monderer and 
moshe tennenholtz bundling equilibrium in 
combinatorial auctions games and economic 
behavior - 
 ron holzman and dov monderer characterization of 
ex post equilibrium in the vcg combinatorial auctions 
games and economic behavior - 
 matthew o jackson a crash course in 
implementation theory mimeo california 
institute of technology 
 a kothari d parkes and s suri 
approximately-strategyproof and tractable multi-unit 
auctions in ec 
 ron lavi ahuva mu alem and noam nisan towards 
a characterization of truthful combinatorial auctions 
in focs 
 ron lavi and noam nisan online ascending auctions 
for gradually expiring goods in soda 
 daniel lehmann liadan o callaghan and yoav 
shoham truth revelation in approximately efficient 
combinatorial auctions journal of the acm 
 - 
 a mas-collel w whinston and j green 
microeconomic theory oxford university press 
 eric maskin nash equilibrium and welfare optimality 
review of economic studies - 
 eric maskin and tomas sj¨ostr¨om implementation 
theory 
 
 aranyak mehta and vijay vazirani randomized 
truthful auctions of digital goods are randomizations 
over truthful auctions in ec 
 john moore implementation contract and 
renegotiation in environments with complete 
information 
 john moore and rafael repullo subgame perfect 
implementation econometrica - 
 h moulin and s shenker strategyproof sharing of 
submodular costs budget balance versus efficiency 
economic theory - 
 noam nisan algorithms for selfish agents in stacs 
 
 noam nisan and amir ronen computationally 
feasable vcg mechanisms in ec 
 noam nisan and amir ronen algorithmic mechanism 
design games and economic behavior - 
 
 m j osborne and a rubinstein a course in game 
theory mit press 
 christos h papadimitriou algorithms games and 
the internet in stoc 
 kevin roberts the characterization of implementable 
choice rules in jean-jacques laffont editor 
aggregation and revelation of preferences papers 
presented at the st european summer workshop of 
the econometric society pages - 
north-holland 
 irit rozenshtrom dominant strategy implementation 
with quasi-linear preferences master s thesis 
dept of economics the hebrew university 
jerusalem israel 
 rakesh vohra and rudolf muller on dominant 
strategy mechanisms working paper 
 shmuel zamir rationality and emotions in ultimatum 
bargaining annales d economie et de statistique 
 
 
on cheating in sealed-bid auctions 
ryan porter 
rwporter stanford edu 
yoav shoham 
shoham stanford edu 
computer science department 
stanford university 
stanford ca 
abstract 
motivated by the rise of online auctions and their relative 
lack of security this paper analyzes two forms of cheating in 
sealed-bid auctions the first type of cheating we consider 
occurs when the seller spies on the bids of a second-price 
auction and then inserts a fake bid in order to increase the 
payment of the winning bidder in the second type a bidder 
cheats in a first-price auction by examining the competing 
bids before deciding on his own bid in both cases we derive 
equilibrium strategies when bidders are aware of the 
possibility of cheating these results provide insights into 
sealedbid auctions even in the absence of cheating including some 
counterintuitive results on the effects of overbidding in a 
first-price auction 
categories and subject descriptors 
j computer applications social and behavioral 
sciences-economics 
general terms 
economics security 
 introduction 
among the types of auctions commonly used in practice 
sealed-bid auctions are a good practical choice because they 
require little communication and can be completed almost 
instantly each bidder simply submits a bid and the winner 
is immediately determined however sealed-bid auctions 
do require that the bids be kept private until the auction 
clears the increasing popularity of online auctions only 
makes this disadvantage more troublesome at an auction 
house with all participants present it is difficult to examine 
a bid that another bidder gave directly to the auctioneer 
however in an online auction the auctioneer is often little 
more than a server with questionable security and since all 
participants are in different locations one can anonymously 
attempt to break into the server in this paper we present a 
game theoretic analysis of how bidders should behave when 
they are aware of the possibility of cheating that is based on 
knowledge of the bids 
we investigate this type of cheating along two dimensions 
whether it is the auctioneer or a bidder who cheats and 
which variant either first or second-price of the sealed-bid 
auction is used note that two of these cases are trivial 
in our setting there is no incentive for the seller to submit 
a shill bid in a first price auction because doing so would 
either cancel the auction or not affect the payment of the 
winning bidder in a second-price auction knowing the 
competing bids does not help a bidder because it is dominant 
strategy to bid truthfully this leaves us with two cases that 
we examine in detail 
a seller can profitably cheat in a second-price auction by 
looking at the bids before the auction clears and submitting 
an extra bid this possibility was pointed out as early as 
the seminal paper that introduced this type of auction 
for example if the bidders in an ebay auction each use a 
proxy bidder essentially creating a second-price auction 
then the seller may be able to break into ebay s server 
observe the maximum price that a bidder is willing to pay and 
then extract this price by submitting a shill bid just below 
it using a false identity we assume that there is no chance 
that the seller will be caught when it cheats however not 
all sellers are willing to use this power or not all sellers can 
successfully cheat we assume that each bidder knows the 
probability with which the seller will cheat possible 
motivation for this knowledge could be a recently published expos´e 
on seller cheating in ebay auctions in this setting we derive 
an equilibrium bidding strategy for the case in which each 
bidder s value for the good is independently drawn from a 
common distribution with no further assumptions except 
for continuity and differentiability this result shows how 
first and second-price auctions can be viewed as the 
endpoints of a spectrum of auctions 
but why should the seller have all the fun in a first-price 
auction a bidder must bid below his value for the good also 
called shaving his bid in order to have positive utility if he 
 
wins to decide how much to shave his bid he must trade off 
the probability of winning the auction against how much he 
will pay if he does win of course if he could simply examine 
the other bids before submitting his own then his problem 
is solved bid the minimum necessary to win the auction 
in this setting our goal is to derive an equilibrium bidding 
strategy for a non-cheating bidder who is aware of the 
possibility that he is competing against cheating bidders when 
bidder values are drawn from the commonly-analyzed 
uniform distribution we show the counterintuitive result that 
the possibility of other bidders cheating has no effect on 
the equilibrium strategy of an honest bidder this result 
is then extended to show the robustness of the equilibrium 
of a first-price auction without the possibility of cheating 
we conclude this section by exploring other distributions 
including some in which the presence of cheating bidders 
actually induces an honest bidder to lower its bid 
the rest of the paper is structured as follows in section 
 we formalize the setting and present our results for the 
case of a seller cheating in a second price auction section 
 covers the case of bidders cheating in a first-price auction 
in section we quantify the effects that the possibility of 
cheating has on an honest seller in the two settings we 
discuss related work including other forms of cheating in 
auctions in section before concluding with section all 
proofs and derivations are found in the appendix 
 second-price auction 
cheating seller 
in this section we consider a second-price auction in which 
the seller may cheat by inserting a shill bid after 
observing all of the bids the formulation for this section will be 
largely reused in the following section on bidders cheating 
in a first-price auction while no prior knowledge of game 
theory or auction theory is assumed good introductions can 
be found in and respectively 
 formulation 
the setting consists of n bidders or agents indexed by 
i · · · n and a seller each agent has a type θi ∈ 
 drawn from a continuous range which represents the 
agent s value for the good being auctioned 
each agent s 
type is independently drawn from a cumulative distribution 
function cdf f over where f and f 
we assume that f · is strictly increasing and differentiable 
over the interval call the probability density function 
 pdf f θi f θi which is the derivative of the cdf 
each agent knows its own type θi but only the 
distribution over the possible types of the other agents a bidding 
strategy for an agent bi → maps its type to its 
bid 
let θ θ · · · θn be the vector of types for all agents 
and θ−i θ · · · θi− θi · · · θn be the vector of all 
types except for that of agent i we can then combine the 
vectors so that θ θi θ−i we also define the vector of 
bids as b θ b θ bn θn and this vector without 
 
we can restrict the types to the range without loss 
of generality because any distribution over a different range 
can be normalized to this range 
 
we thus limit agents to deterministic bidding strategies 
but because of our continuity assumption there always 
exists a pure strategy equilibrium 
the bid of agent i as b−i θ−i let b θ be the value of the 
highest bid of the vector b θ with a corresponding 
definition for b θ−i 
an agent obviously wins the auction if its bid is greater 
than all other bids but ties complicate the formulation 
fortunately we can ignore the case of ties in this paper because 
our continuity assumption will make them a zero probability 
event in equilibrium we assume that the seller does not set 
a reserve price 
if the seller does not cheat then the winning agent pays 
the highest bid by another agent on the other hand if 
the seller does cheat then the winning agent will pay its 
bid since we assume that a cheating seller would take full 
advantage of its power let the indicator variable µc 
be 
if the seller cheats and otherwise the probability that 
the seller cheats pc 
 is known by all agents 
we can then 
write the payment of the winning agent as follows 
pi b θ µc 
 µc 
· bi θi − − µc 
 · b θ−i 
let µ · be an indicator function that takes an inequality 
as an argument and returns is if it holds and otherwise 
the utility for agent i is zero if it does not win the auction 
and the difference between its valuation and its price if it 
does 
ui b θ µc 
 θi µ bi θi b θ−i · θi − pi b θ µc 
 
 
we will be concerned with the expected utility of an agent 
with the expectation taken over the types of the other agents 
and over whether or not the seller cheats by pushing the 
expectation inward so that it is only over the price 
 conditioned on the agent winning the auction we can write the 
expected utility as 
eθ−i µc ui b θ µc 
 θi prob bi θi b θ−i · 
θi − eθ−i µc pi b θ µc 
 bi θi b θ−i 
we assume that all agents are rational expected utility 
maximizers because of the uncertainty over the types of 
the other agents we will be looking for a bayes-nash 
equilibrium a vector of bidding strategies b∗ 
is a bayes-nash 
equilibrium if for each agent i and each possible type θi 
agent i cannot increase its expected utility by using an 
alternate bidding strategy bi holding the bidding strategies 
for all other agents fixed formally b∗ 
is a bayes-nash 
equilibrium if 
∀i θi bi eθ−i µc ui b∗ 
i θi b∗ 
−i θ−i µc 
 θi ≥ 
eθ−i µc ui bi θi b∗ 
−i θ−i µc 
 θi 
 equilibrium 
we first present the bayes-nash equilibrium for an 
arbitrary distribution f · 
 
this simplifies the analysis but all of our results can be 
applied to the case in which the seller announces a reserve 
price before the auction begins 
 
note that common knowledge is not necessary for the 
existence of an equilibrium 
 
theorem in a second-price auction in which the seller 
cheats with probability pc 
 it is a bayes-nash equilibrium for 
each agent to bid according to the following strategy 
bi θi θi − 
θi 
 
f n− 
p c 
 x dx 
f n− 
p c 
 θi 
 
it is useful to consider the extreme points of pc 
 setting 
pc 
 yields the correct result for a first-price auction see 
e g in the case of pc 
 this solution is not defined 
however in the limit bi θi approaches θi as pc 
approaches 
 which is what we expect as the auction approaches a 
standard second-price auction 
the position of pc 
is perhaps surprising for example the 
linear combination bi θi θi − pc 
· 
θi 
 f n− 
 x dx 
f n− θi 
of the 
equilibrium bidding strategies of first and second-price 
auctions would have also given us the correct bidding strategies 
for the cases of pc 
 and pc 
 
 continuum of auctions 
an alternative perspective on the setting is as a 
continuum between first and second-price auctions consider a 
probabilistic sealed-bid auction in which the seller is 
honest but the price paid by the winning agent is determined 
by a weighted coin flip with probability pc 
it is his bid 
and with probability − pc 
it is the second-highest bid 
by adjusting pc 
 we can smoothly move between a first 
and second-price auction furthermore the fact that this 
probabilistic auction satisfies the properties required for the 
revenue equivalence theorem see e g provides a way 
to verify that the bidding strategy in equation is the 
symmetric equilibrium of this auction see the alternative proof 
of theorem in the appendix 
 special case uniform distribution 
another way to try to gain insight into equation is by 
instantiating the distribution of types we now consider the 
often-studied uniform distribution f θi θi 
corollary in a second-price auction in which the 
seller cheats with probability pc 
 and f θi θi it is a 
bayes-nash equilibrium for each agent to bid according to 
the following strategy 
bi θi 
n − 
n − pc 
θi 
this equilibrium bidding strategy parameterized by pc 
 
can be viewed as an interpolation between two well-known 
results when pc 
 the bidding strategy is now 
welldefined each agent bids its true type while when pc 
 
we get the correct result for a first-price auction each agent 
bids according to the strategy bi θi n− 
n 
θi 
 first-price auction 
cheating agents 
we now consider the case in which the seller is honest 
but there is a chance that agents will cheat and examine the 
other bids before submitting their own or alternatively 
they will revise their bid before the auction clears since 
this type of cheating is pointless in a second-price auction 
we only analyze the case of a first-price auction after 
revising the formulation from the previous section we present a 
fixed point equation for the equilibrium strategy for an 
arbitrary distribution f · this equation will be useful for the 
analysis the uniform distribution in which we show that the 
possibility of cheating agents does not change the 
equilibrium strategy of honest agents this result has implications 
for the robustness of the symmetric equilibrium to 
overbidding in a standard first-price auction furthermore we find 
that for other distributions overbidding actually induces a 
competing agent to shave more off of its bid 
 formulation 
it is clear that if a single agent is cheating he will bid 
 up to his valuation the minimum amount necessary to win 
the auction it is less obvious though what will happen if 
multiple agents cheat one could imagine a scenario similar 
to an english auction in which all cheating agents keep 
revising their bids until all but one cheater wants the good 
at the current winning bid however we are only concerned 
with how an honest agent should bid given that it is aware 
of the possibility of cheating thus it suffices for an honest 
agent to know that it will win the auction if and only if 
its bid exceeds every other honest agent s bid and every 
cheating agent s type 
this intuition can be formalized as the following 
discriminatory auction in the first stage each agent s payment 
rule is determined with probability pa 
 the agent will pay 
the second highest bid if it wins the auction essentially 
he is a cheater and otherwise it will have to pay its bid 
these selections are recorded by a vector of indicator 
variables µa 
 µa 
 µan 
 where µai 
 denotes that agent 
i pays the second highest bid each agent knows the 
probability pa 
 but does not know the payment rule for all other 
agents otherwise this auction is a standard sealed-bid 
auction it is thus a dominant strategy for a cheater to bid 
its true type making this formulation strategically 
equivalent to the setting outlined in the previous paragraph the 
expression for the utility of an honest agent in this 
discriminatory auction is as follows 
ui b θ µa 
 θi θi − bi θ · 
j i 
µaj 
· µ bi θi θj − µaj 
 · µ bi θi bj θj 
 
 equilibrium 
our goal is to find the equilibrium in which all cheating 
agents use their dominant strategy of bidding truthfully and 
honest agents bid according to a symmetric bidding strategy 
since we have left f · unspecified we cannot present a 
closed form solution for the honest agent s bidding strategy 
and instead give a fixed point equation for it 
theorem in a first-price auction in which each agent 
cheats with probability pa 
 it is a bayes-nash equilibrium 
for each non-cheating agent i to bid according to the strategy 
that is a fixed point of the following equation 
bi θi θi − 
θi 
 pa · f bi x − pa · f x 
 n− 
dx 
pa · f bi θi − pa · f θi 
 n− 
 
 
 special case uniform distribution 
since we could not solve equation in the general case 
we can only see how the possibility of cheating affects the 
equilibrium bidding strategy for particular instances of f · 
a natural place to start is uniform distribution f θi θi 
recall the logic behind the symmetric equilibrium strategy 
in a first-price auction without cheating bi θi n− 
n 
θi is 
the optimal tradeoff between increasing the probability of 
winning and decreasing the price paid upon winning given 
that the other agents are bidding according to the same 
strategy since in the current setting the cheating agents 
do not shave their bid at all and thus decrease an honest 
agent s probability of winning while obviously not affecting 
the price that an honest agent pays if he wins it is 
natural to expect that an honest agent should compensate by 
increasing his bid the idea is that sacrificing some 
potential profit in order to regain some of the lost probability of 
winning would bring the two sides of the tradeoff back into 
balance however it turns out that the equilibrium bidding 
strategy is unchanged 
corollary in a first-price auction in which each agent 
cheats with probability pa 
 and f θi θi it is a 
bayesnash equilibrium for each non-cheating agent to bid 
according to the strategy bi θi n− 
n 
θi 
this result suggests that the equilibrium of a first-price 
auction is particularly robust when types are drawn from the 
uniform distribution since the best response is unaffected 
by deviations of the other agents to the strategy of always 
bidding their type in fact as long as all other agents shave 
their bid by a fraction which can differ across the agents no 
greater than 
n 
 it is still a best response for the remaining 
agent to bid according to the equilibrium strategy note 
that this result holds even if other agents are shaving their 
bid by a negative fraction and are thus irrationally bidding 
above their type 
theorem in a first-price auction where f θi θi if 
each agent j i bids according a strategy bj θj 
n− αj 
n 
θj 
where αj ≥ then it is a best response for the remaining 
agent i to bid according to the strategy bi θi n− 
n 
θi 
obviously these strategy profiles are not equilibria unless 
each αj because each agent j has an incentive to set 
αj the point of this theorem is that a wide range of 
possible beliefs that an agent can hold about the strategies 
of the other agents will all lead him to play the equilibrium 
strategy this is important because a common and valid 
criticism of equilibrium concepts such as nash and 
bayesnash is that they are silent on how the agents converge 
on a strategy profile from which no one wants to deviate 
however if the equilibrium strategy is a best response to a 
large set of strategy profiles that are out of equilibrium then 
it seems much more plausible that the agents will indeed 
converge on this equilibrium 
it is important to note though that while this equilibrium 
is robust against arbitrary deviations to strategies that shave 
less it is not robust to even a single agent shaving more off 
of its bid in fact if we take any strategy profile consistent 
with the conditions of theorem and change a single agent 
j s strategy so that its corresponding αj is negative then 
agent i s best response is to shave more than 
n 
off of its 
bid 
 effects of overbidding for 
other distributions 
a natural question is whether the best response bidding 
strategy is similarly robust to overbidding by competing 
agents for other distributions it turns out that theorem 
 holds for all distributions of the form f θi θi k 
 where 
k is some positive integer however taking a simple linear 
combination of two such distributions to produce f θi 
θ 
i θi 
 
yields a distribution in which an agent should 
actually shave its bid more when other agents shave their bids 
less in the example we present for this distribution with 
the details in the appendix there are only two players and 
the deviation by one agent is to bid his type however it 
can be generalized to a higher number of agents and to other 
deviations 
example in a first-price auction where f θi 
θ 
i θi 
 
and n if agent always bids its type b θ θ 
then for all θ agent s best response bidding strategy 
is strictly less than the bidding strategy of the symmetric 
equilibrium 
we also note that the same result holds for the normalized 
exponential distribution f θi eθi − 
e− 
 
it is certainly the case that distributions can be found 
that support the intuition given above that agents should 
shave their bid less when other agents are doing likewise 
examples include f θi − 
 
θ 
i 
 
θi the solution to the 
system of equations f θi − f and f 
and f θi e−e −θi 
e− 
 
it would be useful to relate the direction of the change 
in the best response bidding strategy to a general 
condition on f · unfortunately we were not able to find such 
a condition in part because the integral in the symmetric 
bidding strategy of a first-price auction cannot be solved 
without knowing f · or at least some restrictions on it 
we do note however that the sign of the second 
derivative of f θi f θi is an accurate predictor for all of the 
distributions that we considered 
 revenue loss for an 
honest seller 
in both of the settings we covered an honest seller suffers 
a loss in expected revenue due to the possibility of cheating 
the equilibrium bidding strategies that we derived allow us 
to quantify this loss although this is as far as we will take 
the analysis it could be applied to more general settings 
in which the seller could for example choose the market 
in which he sells his good or pay a trusted third party to 
oversee the auction 
in a second-price auction in which the seller may cheat an 
honest seller suffers due the fact that the agents will shave 
their bids for the case in which agent types are drawn from 
the uniform distribution every agent will shave its bid by 
p c 
n− p c which is thus also the fraction by which an honest 
seller s revenue decreases due to the possibility of cheating 
analysis of the case of a first-price auction in which agents 
may cheat is not so straightforward if pa 
 each agent 
cheats with certainty then we simply have a second-price 
auction and the seller s expected revenue will be unchanged 
again considering the uniform distribution for agent types 
it is not surprising that pa 
 
 
causes the seller to lose 
 
the most revenue however even in this worst case the 
percentage of expected revenue lost is significantly less than 
it is for the second-price auction in which pc 
 
 
 as shown 
in table 
it turns out that setting pc 
 would make 
the expected loss of these two settings comparable while 
this comparison between the settings is unlikely to be useful 
for a seller it is interesting to note that agent suspicions of 
possible cheating by the seller are in some sense worse than 
agents actually cheating themselves 
percentage of revenue lost for an honest seller 
agents second-price auction first-price auction 
 pc 
 pa 
 
 
 
 
 
 
 
 
table the percentage of expected revenue lost 
by an honest seller due to the possibility of cheating 
in the two settings considered in this paper agent 
valuations are drawn from the uniform distribution 
 related work 
existing work covers another dimension along which we 
could analyze cheating altering the perceived value of n 
in this paper we have assumed that n is known by all 
of the bidders however in an online setting this 
assumption is rather tenuous for example a bidder s only source 
of information about n could be a counter that the seller 
places on the auction webpage or a statement by the seller 
about the number of potential bidders who have indicated 
that they will participate in these cases the seller could 
arbitrarily manipulate the perceived n in a first-price 
auction the seller obviously has an incentive to increase the 
perceived value of n in order to induce agents to bid closer 
to their true valuation however if agents are aware that 
the seller has this power then any communication about 
n to the agents is cheap talk and furthermore is not 
credible thus in equilibrium the agents would ignore the 
declared value of n and bid according to their own prior 
beliefs about the number of agents if we make the natural 
assumption of a common prior then the setting reduces to 
the one tackled by which derived the equilibrium 
bidding strategies of a first-price auction when the number of 
bidders is drawn from a known distribution but not revealed 
to any of the bidders of course instead of assuming that 
the seller can always exploit this power we could assume 
that it can only do so with some probability that is known 
by the agents the analysis would then proceed in a similar 
manner as that of our cheating seller model 
the other interesting case of this form of cheating is by 
bidders in a first-price auction bidders would obviously 
want to decrease the perceived number of agents in order 
to induce their competition to lower their bids while it is 
 
note that we have not considered the costs of the seller 
thus the expected loss in profit could be much greater than 
the numbers that appear here 
unreasonable for bidders to be able to alter the perceived 
n arbitrarily collusion provides an opportunity to decrease 
the perceived n by having only one of a group of colluding 
agents participate in the auction while the non-colluding 
agents would account for this possibility as long as they are 
not certain of the collusion they will still be induced to shave 
more off of their bids than they would if the collusion did 
not take place this issue is tackled in 
other types of collusion are of course related to the 
general topic of cheating in auctions results on collusion in 
first and second-price auctions can be found in and 
respectively 
the work most closely related to our first setting is 
which also presents a model in which the seller may cheat in 
a second-price auction in their setting the seller is a 
participant in the bayesian game who decides between running 
a first-price auction where profitable cheating is never 
possible or second-price auction the seller makes this choice 
after observing his type which is his probability of having 
the opportunity and willingness to cheat in a second-price 
auction the bidders who know the distribution from which 
the seller s type is drawn then place their bid it is shown 
that in equilibrium only a seller with the maximum 
probability of cheating would ever choose to run a second-price 
auction our work differs in that we focus on the agents 
strategies in a second-price auction for a given probability 
of cheating by the seller an explicit derivation of the 
equilibrium strategies then allows us relate first and second-price 
auctions 
an area of related work that can be seen as 
complementary to ours is that of secure auctions which takes the point 
of view of an auction designer the goals often extend 
well beyond simply preventing cheating including 
properties such as anonymity of the bidders and nonrepudiation of 
bids cryptographic methods are the standard weapon of 
choice here see 
 conclusion 
in this paper we presented the equilibria of sealed-bid 
auctions in which cheating is possible in addition to providing 
strategy profiles that are stable against deviations these 
results give us with insights into both first and second-price 
auctions the results for the case of a cheating seller in a 
second-price auction allow us to relate the two auctions as 
endpoints along a continuum the case of agents cheating in 
a first-price auction showed the robustness of the first-price 
auction equilibrium when agent types are drawn from the 
uniform distribution we also explored the effect of 
overbidding on the best response bidding strategy for other 
distributions and showed that even for relatively simple 
distributions it can be positive negative or neutral finally 
results from both of our settings allowed us to quantify the 
expected loss in revenue for a seller due to the possibility of 
cheating 
 references 
 m franklin and m reiter the design and 
implementation of a secure auction service in proc 
ieee symp on security and privacy 
 d fudenberg and j tirole game theory mit 
press 
 
 d graham and r marshall collusive bidder behavior 
at single-object second-price and english auctions 
journal of political economy - 
 m harkavy j d tygar and h kikuchi electronic 
auctions with private bids in proceedings of the rd 
usenix workshop on electronic commerce 
 r harstad j kagel and d levin equilibrium bid 
functions for auctions with an uncertain number of 
bidders economic letters - 
 p klemperer auction theory a guide to the 
literature journal of economic surveys 
 - 
 k leyton-brown y shoham and m tennenholtz 
bidding clubs in first-price auctions in aaai- 
 r mcafee and j mcmillan bidding rings the 
american economic review - 
 m naor b pinkas and r sumner privacy 
preserving auctions and mechanism design in ec- 
 j riley and w samuelson optimal auctions 
american economic review - 
 m rothkopf and r harstad two models of bid-taker 
cheating in vickrey auctions the journal of business 
 - 
 w vickrey counterspeculations auctions and 
competitive sealed tenders journal of finance 
 - 
appendix 
theorem in a second-price auction in which the seller 
cheats with probability pc 
 it is a bayes-nash equilibrium for 
each agent to bid according to the following strategy 
bi θi θi − 
θi 
 
f n− 
p c 
 x dx 
f n− 
p c 
 θi 
 
proof to find an equilibrium we start by guessing that 
there exists an equilibrium in which all agents bid 
according to the same function bi θi because the game is 
symmetric further we guess that bi θi is strictly increasing 
and differentiable over the range we can also assume 
that bi because negative bids are not allowed and 
a positive bid is not rational when the agent s valuation is 
 note that these are not assumptions on the setting- they 
are merely limitations that we impose on our search 
let φi bi → be the inverse function of bi θi 
that is it takes a bid for agent i as input and returns the 
type θi that induced this bid recall equation 
eθ−i µc ui b θ µc 
 θi prob bi θi b θ−i · 
θi − eθ−i µc pi b θ µc 
 bi θi b θ−i 
the probability that a single other bid is below that of 
agent i is equal to the cdf at the type that would induce 
a bid equal to that of agent i which is formally written as 
f φi bi θi since all agents are independent the 
probability that all other bids are below agent i s is simply this 
term raised the n − -th power 
thus we can re-write the expected utility as 
eθ−i µc ui b θ µc 
 θi fn− 
 φi bi θi · 
θi − eθ−i µc pi b θ µc 
 bi θi b θ−i 
we now solve for the expected payment plugging 
equation which gives the price for the winning agent into the 
term for the expected price in equation and then 
simplifying the expectation yields 
eθ−i µc pi b θ µc 
 bi θi b θ−i 
 eθ−i µc µc 
· bi θi − µc 
 · b θ−i 
bi θi b θ−i 
 pc 
· bi θi − pc 
 · eθ−i b θ−i 
bi θi b θ−i 
 pc 
· bi θi − pc 
 · 
bi θi 
 
b θ−i · 
pdf b θ−i bi θi b θ−i db θ−i 
note that the integral on the last line is taken up to 
bi θi because we are conditioning on the fact that bi θi 
b θ−i to derive the pdf of b θ−i given this condition 
we start with the cdf for a given value b θ−i the 
probability that any one agent s bid is less than this value is equal 
to f φi b θ−i we then condition on the agent s bid 
being below bi θi by dividing by f φi bi θi the cdf for 
the n − agents is then this value raised to the n − -th 
power 
cdf b θ−i bi θi b θ−i 
fn− 
 φi b θ−i 
fn− φi bi θi 
the pdf is then the derivative of the cdf with respect to 
b θ−i 
pdf b θ−i bi θi b θ−i 
 
n − 
fn− φi bi θi 
· fn− 
 φi b θ−i · 
f φi b θ−i · φi b θ−i 
substituting the pdf into equation and pulling terms 
out of the integral that do not depend on b θ−i yields 
eθ−i µc pi b θ µc 
 bi θi b θ−i pc 
· bi θi 
 − pc 
 · n − 
fn− φi bi θi 
· 
bi θi 
 
b θ−i · fn− 
 φi b θ−i · 
f φi b θ−i · φi b θ−i db θ−i 
 
plugging the expected price back into the expected utility 
equation and distributing fn− 
 φi bi θi yields 
eθ−i µc ui b θ µc 
 θi fn− 
 φi bi θi · θi− 
fn− 
 φi bi θi · pc 
· bi θi − 
 − pc 
 · n − · 
bi θi 
 
b θ−i · fn− 
 φi b θ−i · 
f φi b θ−i · φi b θ−i db θ−i 
we are now ready to optimize the expected utility by 
taking the derivative with respect to bi θi and setting it to 
note that we do not need to solve the integral because it 
will disappear when the derivative is taken by application 
of the fundamental theorem of calculus 
 n− ·fn− 
 φi bi θi ·f φi bi θi ·φi bi θi ·θi− 
fn− 
 φi bi θi · pc 
− 
pc 
· n− ·fn− 
 φi bi θi ·f φi bi θi ·φi bi θi ·bi θi − 
 −pc 
 · n− · bi θi ·fn− 
 φi bi θi ·f φi bi θi ·φi bi θi 
dividing through by fn− 
 φi bi θi and combining like 
terms yields 
 θi − pc 
· bi θi − − pc 
 · bi θi · n − 
· f φi bi θi · φi bi θi − pc 
· f φi bi θi 
simplifying the expression and rearranging terms produces 
bi θi θi − 
pc 
· f φi bi θi 
 n − · f φi bi θi · φi bi θi 
to further simplify we use the formula f x 
g f x 
 
where g x is the inverse function of f x plugging in 
function from our setting gives us φi bi θi 
bi θi 
 applying 
both this equation and φi bi θi θi yields 
bi θi θi − 
pc 
· f θi · bi θi 
 n − · f θi 
 
attempts at a derivation of the solution from this point 
proved fruitless but we are at a point now where a guessed 
solution can be quickly verified we used the solution for 
the first-price auction see e g as our starting point 
to find the answer 
bi θi θi − 
θi 
 
f n− 
p c 
 x dx 
f n− 
p c 
 θi 
 
to verify the solution we first take its derivative 
bi θi − 
f · n− 
p c 
 θi − n− 
p c · f n− 
p c − 
 θi · f θi · 
θi 
 
f n− 
p c 
 x dx 
f · n− 
p c 
 θi 
this simplifies to 
bi θi 
n− 
p c · f θi · 
θi 
 
f n− 
p c 
 x dx 
f n− 
p c 
 θi 
we then plug this derivative into the equation we derived 
 
bi θi θi − 
pc 
· f θi · n− 
p c · f θi · 
θi 
 
f n− 
p c 
 x dx 
 n − · f θi · f n− 
p c 
 θi 
cancelling terms yields equation verifying that our 
guessed solution is correct 
alternative proof of theorem 
the following proof uses the revenue equivalence 
theorem ret and the probabilistic auction given as an 
interpretation of our cheating seller setting 
in a first-price auction without the possibility of cheating 
the expected payment for an agent with type θi is simply 
the product of its bid and the probability that this bid is 
the highest for the symmetric equilibrium this is equal to 
f n− 
 θi · θi − 
θi 
 
f n− 
 x dx 
f n− θi 
for our probabilistic auction the expected payment of 
the winning agent is a weighted average of its bid and the 
second highest bid for the bi · we found in the original 
interpretation of the setting it can be written as follows 
f n− 
 θi · pc 
θi − 
θi 
 
f n− 
p c 
 x dx 
f n− 
p c 
 θi 
 
 − pc 
 
 
f n− θi 
· 
θi 
 
x− 
x 
 
f n− 
p c 
 y dy 
f n− 
p c 
 x 
· n − ·f n− 
 x ·f x dx 
by the ret the expected payments will be the same 
in the two auctions thus we can verify our equilibrium 
bidding strategy by showing that the expected payment in 
the two auctions is equal since the expected payment is 
zero at θi for both functions it suffices to verify that the 
derivatives of the expected payment functions with respect 
to θi are equal for an arbitrary value θi thus we need to 
verify the following equation 
f n− 
 θi n − · f n− 
 θi · f θi · θi − f n− 
 θi 
 
pc 
· f n− 
 θi · − − 
 n− 
p c · f n− 
p c − 
 θi · f θi · 
θi 
 
f n− 
p c 
 x dx 
f n− 
p c 
 θi 
 
 n − · f n− 
 θi · f θi · θi − 
θi 
 
f n− 
p c 
 x dx 
f n− 
p c 
 θi 
 
 −pc 
 θi− 
θi 
 
f n− 
p c 
 y dy 
f n− 
p c 
 θi 
· n− ·f n− 
 θi ·f θi 
 
this simplifies to 
 pc 
· 
 n− 
p c · f n− 
 θi · f θi · 
θi 
 
f n− 
p c 
 x dx 
f n− 
p c 
 θi 
 
 n − · f n− 
 θi · f θi · − 
θi 
 
f n− 
p c 
 x dx 
f n− 
p c 
 θi 
 
 −pc 
 − 
θi 
 
f n− 
p c 
 y dy 
f n− 
p c 
 θi 
· n− ·f n− 
 θi ·f θi 
after distributing pc 
 the right-hand side of this equation 
cancels out and we have verified our equilibrium bidding 
strategy 
corollary in a second-price auction in which the 
seller cheats with probability pc 
 and f θi θi it is a 
bayes-nash equilibrium for each agent to bid according to 
the following strategy 
bi θi 
n − 
n − pc 
θi 
proof plugging f θi θi into equation repeated 
as we get 
bi θi θi − 
θi 
 
x n− 
p c 
dx 
θi 
 n− 
p c 
 θi − 
p c 
n− p c θi 
 n− p c 
p c 
θi 
 n− 
p c 
 θi − 
pc 
n − pc 
· θi 
n − 
n − pc 
· θi 
theorem in a first-price auction in which each agent 
cheats with probability pa 
 it is a bayes-nash equilibrium 
for each non-cheating agent i to bid according to the strategy 
that is a fixed point of the following equation 
bi θi θi − 
θi 
 pa · f bi x − pa · f x 
 n− 
dx 
pa · f bi θi − pa · f θi 
 n− 
 
proof we make the same guesses about the equilibrium 
strategy to aid our search as we did in the proof of theorem 
 when simplifying the expectation of this setting s 
utility equation we use the fact that the probability that 
agent i will have a higher bid than another honest agent is 
still f φi bi θi while the probability is f bi θi if the 
other agent cheats the probability that agent i beats a 
single other agent is then a weighted average of these two 
probabilities thus we can write agent i s expected utility 
as 
eθ−i µa ui b θ µa 
 θi θi − bi θi · 
pa 
· f bi θi − pa 
 · f φi bi θi 
n− 
as before to find the equilibrium bi θi we take the 
derivative and set it to zero 
 θi − bi θi · n − · 
pa 
· f bi θi − pa 
 · f φi bi θi 
n− 
· 
pa 
· f bi θi − pa 
 · f φi bi θi · φi bi θi − 
pa 
· f bi θi − pa 
 · f φi bi θi 
n− 
applying the equations φi bi θi 
bi θi 
and φi bi θi 
θi and dividing through produces 
 θi − bi θi · n − · 
pa 
· f bi θi − pa 
 · f θi · 
 
bi θi 
− 
pa 
· f bi θi − pa 
 · f θi 
rearranging terms yields 
bi θi θi − 
pa · f bi θi − pa · f θi · bi θi 
 n − · pa · f bi θi · bi θi − pa · f θi 
 
in this setting because we leave f · unspecified we 
cannot present a closed form solution however we can simplify 
the expression by removing its dependence on bi θi 
bi θi θi − 
θi 
 pa · f bi x − pa · f x 
 n− 
dx 
pa · f bi θi − pa · f θi 
 n− 
 
to verify equation first take its derivative 
bi θi − − 
 n − · pa 
· f bi θi − pa 
 · f θi 
 n− 
· 
pa 
· f bi θi · bi θi − pa 
 · f θi · 
θi 
 
pa 
· f bi x − pa 
 · f x 
 n− 
dx 
pa · f bi θi − pa · f θi 
 n− 
this equation simplifies to 
bi θi n − · pa 
·f bi θi ·bi θi −pa 
 ·f θi · 
θi 
 
pa 
· f bi x − pa 
 · f x 
 n− 
dx 
pa · f bi θi − pa · f θi 
n 
plugging this equation into the bi θi in the numerator of 
equation yields equation verifying the solution 
 
corollary in a first-price auction in which each agent 
cheats with probability pa 
 and f θi θi it is a 
bayesnash equilibrium for each non-cheating agent to bid 
according to the strategy bi θi n− 
n 
θi 
proof instantiating the fixed point equation and 
repeated as with f θi θi yields 
bi θi θi − 
θi 
 
pa 
· bi x − pa 
 · x 
 n− 
dx 
pa · bi θi − pa · θi 
 n− 
we can plug the strategy bi θi n− 
n 
θi into this 
equation in order to verify that it is a fixed point 
bi θi θi − 
θi 
 
pa 
· n− 
n 
x − pa 
 · x 
 n− 
dx 
pa · n− 
n 
θi − pa · θi 
 n− 
 θi − 
θi 
 
x n− 
dx 
θ 
 n− 
i 
 θi − 
 
n 
θn 
i 
θ 
 n− 
i 
 
n − 
n 
θi 
theorem in a first-price auction where f θi θi if 
each agent j i bids according a strategy bj θj 
n− αj 
n 
θj 
where αj ≥ then it is a best response for the remaining 
agent i to bid according to the strategy bi θi n− 
n 
θi 
proof we again use φj bj → as the inverse 
of bj θj for all j i in this setting φj x n 
n− αj 
x 
the probability that agent i has a higher bid than a single 
agent j is f φj bi θi n 
n− αj 
bi θi note however 
that since φj · is only defined over the range bj it 
must be the case that bi ≤ bj which is why αj ≥ 
 is necessary in addition to being sufficient assuming 
that bi θi n− 
n 
θi then indeed φj bi θi is always 
welldefined we will now show that this assumption is correct 
the expected utility for agent i can then be written as 
eθ−i ui b θ θi πj i 
n 
n − αj 
bi θi · θi −bi θ 
 πj i 
n 
n − αj 
· θi· bi θi 
 n− 
− bi θi 
n 
taking the derivative with respect to bi θi setting it to 
zero and dividing out πj i 
n 
n− αj 
yields 
 θi · n − · bi θi 
 n− 
− n · bi θi 
 n− 
this simplifies to the solution bi θi n− 
n 
θi 
full version of example 
in a first-price auction where f θi 
θ 
i θi 
 
and n 
if agent always bids its type b θ θ then for all 
θ agent s best response bidding strategy is strictly 
less than the bidding strategy of the symmetric equilibrium 
after calculating the symmetric equilibrium in which both 
agents shave their bid by the same amount we find the best 
response to an agent who instead does not shave its bid 
we then show that this best response is strictly less than 
the equilibrium strategy to find the symmetric equilibrium 
bidding strategy we instantiate n in the general 
formula the equation found in plug in f θi 
θ 
i θi 
 
 and 
simplify 
bi θi θi − 
θi 
 
f x dx 
f θi 
 θi− 
 
 
· 
θi 
 
 x 
 x dx 
 
 
· θ 
i θi 
 θi− 
 
 
θ 
i 
 
θ 
i 
θ 
i θi 
 
 
 
θ 
i 
 
θi 
θi 
we now derive the best response for agent to the 
strategy b θ θ denoting the best response strategy b∗ 
 θ 
to distinguish it from the symmetric case the 
probability of agent winning is f b∗ 
 θ which is the probability 
that agent s type is less than agent s bid thus agent 
 s expected utility is 
eθ ui b∗ 
 θ b θ θ f b∗ 
 θ · θ − b∗ 
 θ 
 
 b∗ 
 θ 
 b∗ 
 θ 
 
· θ − b∗ 
 θ 
taking the derivative with respect to b∗ 
 θ setting it to 
zero and then rearranging terms gives us 
 
 
 
· · b∗ 
 θ · θ − · b∗ 
 θ 
 θ − · b∗ 
 θ 
 · b∗ 
 θ 
 − · θ · b∗ 
 θ − θ 
of the two solutions of this equation one always produces 
a negative bid the other is 
b∗ 
 θ 
θ − θ 
 θ 
 
we now need to show that b θ b∗ 
 θ holds for all 
θi substituting in for both terms and then simplifying 
the inequality gives us 
 
 
θ 
 
 
θ 
θ 
 
θ − θ 
 θ 
 
θ 
 
 
 
θ θ θ 
 θ 
since θ ≥ we can square both sides of the inequality 
which then allows us to verify the inequality for all θ 
θ 
 θ 
 
 
 
θ 
 θ θ 
 θ 
 θ 
 θ 
 
 
θ 
 
 
from optimal limited to unlimited supply auctions 
jason d hartline 
microsoft research 
 la avenida 
mountain view ca 
hartline microsoft com 
robert mcgrew 
∗ 
computer science department 
stanford university 
stanford ca 
bmcgrew stanford edu 
abstract 
we investigate the class of single-round sealed-bid auctions for a 
set of identical items to bidders who each desire one unit we adopt 
the worst-case competitive framework defined by that 
compares the profit of an auction to that of an optimal single-price sale 
of least two items in this paper we first derive an optimal auction 
for three items answering an open question from second we 
show that the form of this auction is independent of the 
competitive framework used third we propose a schema for converting 
a given limited-supply auction into an unlimited supply auction 
applying this technique to our optimal auction for three items we 
achieve an auction with a competitive ratio of which improves 
upon the previously best-known competitive ratio of from 
finally we generalize a result from and extend our 
understanding of the nature of the optimal competitive auction by showing that 
the optimal competitive auction occasionally offers prices that are 
higher than all bid values 
categories and subject descriptors 
f analysis of algorithms and problem complexity 
nonnumerical algorithms and problems j social and behavioral 
sciences economics 
general terms 
algorithms economics theory 
 introduction 
the research area of optimal mechanism design looks at 
designing a mechanism to produce the most desirable outcome for the 
entity running the mechanism this problem is well studied for the 
auction design problem where the optimal mechanism is the one 
that brings the seller the most profit here the classical approach 
is to design such a mechanism given the prior distribution from 
which the bidders preferences are drawn see e g 
recently goldberg et al introduced the use of worst-case 
competitive analysis see e g to analyze the performance of auctions 
that have no knowledge of the prior distribution the goal of such 
work is to design an auction that achieves a large constant fraction 
of the profit attainable by an optimal mechanism that knows the 
prior distribution in advance positive results in this direction are 
fueled by the observation that in auctions for a number of identical 
units much of the distribution from which the bidders are drawn 
can be deduced on the fly by the auction as it is being run 
 
the performance of an auction in such a worst-case competitive 
analysis is measured by its competitive ratio the ratio between a 
benchmark performance and the auction s performance on the input 
distribution that maximizes this ratio the holy grail of the 
worstcase competitive analysis of auctions is the auction that achieves 
the optimal competitive ratio as small as possible since this 
search has led to improved understanding of the nature of the 
optimal auction the techniques for on-the-fly pricing in these 
scenarios and the competitive ratio of the optimal auction in this 
paper we continue this line of research by improving in all of these 
directions furthermore we give evidence corroborating the 
conjecture that the form of the optimal auction is independent of the 
benchmark used in the auction s competitive analysis this result 
further validates the use of competitive analysis in gauging auction 
performance 
we consider the single item multi-unit unit-demand auction 
problem in such an auction there are many units of a single item 
available for sale to bidders who each desire only one unit each bidder 
has a valuation representing how much the item is worth to him 
the auction is performed by soliciting a sealed bid from each of 
the bidders and deciding on the allocation of units to bidders and 
the prices to be paid by the bidders the bidders are assumed to bid 
so as to maximize their personal utility the difference between their 
valuation and the price they pay to handle the problem of 
designing and analyzing auctions where bidders may falsely declare their 
valuations to get a better deal we will adopt the solution concept 
of truthful mechanism design see e g in a truthful 
auction revealing one s true valuation as one s bid is an optimal 
strategy for each bidder regardless of the bids of the other bidders 
in this paper we will restrict our attention to truthful a k a 
incentive compatible or strategyproof auctions 
a particularly interesting special case of the auction problem is 
the unlimited supply case in this case the number of units for sale is 
at least the number of bidders in the auction this is natural for the 
sale of digital goods where there is negligible cost for duplicating 
 
and distributing the good pay-per-view television and 
downloadable audio files are examples of such goods 
the competitive framework introduced in and further refined 
in uses the profit of the optimal omniscient single priced 
mechanism that sells at least two units as the benchmark for competitive 
analysis the assumption that two or more units are sold is 
necessary because in the worst case it is impossible to obtain a constant 
fraction of the profit of the optimal mechanism when it sells only 
one unit in this framework for competitive analysis an auction 
is said to be β-competitive if it achieves a profit that is within a 
factor of β ≥ of the benchmark profit on every input the optimal 
auction is the one which is β-competitive with the minimum value 
of β 
previous to this work the best known auction for the unlimited 
supply case had a competitive ratio of and the best lower 
bound known was for the limited supply case auctions 
can achieve substantially better competitive ratios when there are 
only two units for sale the optimal auction gives a competitive ratio 
of which matches the lower bound for two units when there 
are three units for sale the best previously known auction had a 
competitive ratio of compared with a lower bound of ≈ 
 
the results of this paper are as follows 
 we give the auction for three units that is optimally 
competitive against the profit of the omniscient single priced 
mechanism that sells at least two units this auction achieves a 
competitive ratio of matching the lower bound from 
 section 
 we show that the form of the optimal auction is 
independent of the benchmark used in competitive analysis in doing 
so we give an optimal three bidder auction for generalized 
benchmarks section 
 we give a general technique for converting a limited supply 
auction into an unlimited supply auction where it is possible 
to use the competitive ratio of the limited supply auction to 
obtain a bound on the competitive ratio of the unlimited 
supply auction we refer to auctions derived from this 
framework as aggregation auctions section 
 we improve on the best known competitive ratio by 
proving that the aggregation auction constructed from our optimal 
three-unit auction is -competitive section 
 assuming that the conjecture that the optimal -unit auction 
has a competitive ratio that matches the lower bound proved 
in we show that this optimal auction for ≥ on some 
inputs will occasionally offer prices that are higher than any 
bid in that input section for the three-unit case where we 
have shown that the lower bound of is tight this 
observation led to our construction of the optimal three-unit auction 
 definitions and background 
we consider single-round sealed-bid auctions for a set of 
identical units of an item to bidders who each desire one unit as 
mentioned in the introduction we adopt the game-theoretic solution 
concept of truthful mechanism design a useful simplification of 
the problem of designing truthful auctions is obtained through the 
following algorithmic characterization related formulations to 
this one have appeared in numerous places in recent literature e g 
 
definition given a bid vector of n bids b b bn 
let b-i denote the vector of with bi replaced with a   i e 
b-i b bi− bi bn 
definition let f be a function from bid vectors with a 
  to prices non-negative real numbers the deterministic 
bidindependent auction defined by f bif works as follows for each 
bidder i 
 set ti f b-i 
 if ti bi bidder i wins at price ti 
 if ti bi bidder i loses 
 otherwise ti bi the auction can either accept the bid at 
price ti or reject it 
a randomized bid-independent auction is a distribution over 
deterministic bid-independent auctions 
the proof of the following theorem can be found for example 
in 
theorem an auction is truthful if and only if it is equivalent 
to a bid-independent auction 
given this equivalence we will use the the terminology 
bidindependent and truthful interchangeably 
for a randomized bid-independent auction f b-i is a random 
variable we denote the probability density of f b-i at z by ρb-i z 
we denote the profit of a truthful auction a on input b as a b 
the expected profit of the auction e a b is the sum of the 
expected payments made by each bidder which we denote by pi b 
for bidder i clearly the expected payment of each bid satisfies 
pi b 
bi 
 
xρb-i x dx 
 competitive framework 
we now review the competitive framework from in order 
to evaluate the performance of auctions with respect to the goal of 
profit maximization we introduce the optimal single price 
omniscient auction f and the related omniscient auction f 
 
definition give a vector b b bn let b i 
represent the i-th largest value in b 
the optimal single price omniscient auction f is defined as 
follows auction f on input b determines the value k such that 
kb k is maximized all bidders with bi ≥ b k win at price b k all 
remaining bidders lose the profit of f on input b is thus f b 
max ≤k≤n kb k 
in the competitive framework of and subsequent papers the 
performance of a truthful auction is gauged in comparison to f 
 
the optimal singled priced auction that sells at least two units the 
profit of f 
is max ≤k≤n kb k there are a number of reasons 
to choose this benchmark for comparison interested readers should 
see or for a more detailed discussion 
let a be a truthful auction we say that a is β-competitive 
against f 
 or just β-competitive if for all bid vectors b the 
expected profit of a on b satisfies 
e a b ≥ 
f 
 b 
β 
 
in section we generalize this framework to other profit 
benchmarks 
 
 scale invariant and symmetric auctions 
a symmetric auction is one where the auction outcome is 
unchanged when the input bids arrive in a different permutation 
goldberg et al show that a symmetric auction achieves the optimal 
competitive ratio this is natural as the profit benchmark we 
consider is symmetric and it allows us to consider only symmetric 
auctions when looking for the one with the optimal competitive 
ratio 
an auction defined by bid-independent function f is scale 
invariant if for all i and all z pr f b-i ≥ z pr f cb-i ≥ cz 
it is conjectured that the assumption of scale invariance is without 
loss of generality thus we are motivated to consider 
symmetric scale-invariant auctions when specifying a symmetric 
scaleinvariant auction we can assume that f is only a function of the 
relative magnitudes of the n − bids in b-i and that one of the 
bids bj it will be convenient to specify such auctions via the 
density function of f b-i ρb-i z it is enough to specify such a 
density function of the form ρ z zn− z with ≤ zi ≤ zi 
 limited supply versus unlimited supply 
following throughout the remainder of this paper we will 
be making the assumption that n i e the number of bidders 
is equal to the number of units for sale this is without loss of 
generality as a any lower bound that applies to the n case 
also extends to the case where n ≥ and b there is a 
reduction from the unlimited supply auction problem to the limited 
supply auction problem that takes an unlimited supply auction that 
is β-competitive with f 
and constructs a limited supply auction 
parameterized by that is β-competitive with f 
 the optimal 
omniscient auction that sells between and units 
henceforth we will assume that we are in the unlimited supply 
case and we will examine lower bounds for limited supply 
problems by placing a restriction on the number of bidders in the 
auction 
 lower bounds and optimal auctions 
frequently in this paper we will refer to the best known lower 
bound on the competitive ratio of truthful auctions 
theorem the competitive ratio of any auction on n 
bidders is at least 
 − 
n 
i 
− 
n 
i− 
i 
i − 
n − 
i − 
 
definition let υn denote the n-bidder auction that achieves 
the optimal competitive ratio 
this bound is derived by analyzing the performance of any 
auction on the following distribution b in each random bid 
vector b each bid bi is drawn i i d from the distribution such that 
pr bi ≥ s ≤ s for all s ∈ s 
in the two-bidder case this lower bound is this is achieved by 
υ which is the -unit vickrey auction 
in the three-bidder case 
this lower bound is in the next section we define the 
auction υ which matches this lower bound in the four-bidder case 
this lower bound is in the limit as the number of bidders 
grows this lower bound approaches a number which is 
approximately 
it is conjectured that this lower bound is tight for any number of 
bidders and the optimal auction υn matches it 
 
the -unit vickrey auction sells to the highest bidder at the second 
highest bid value 
 profit extraction 
in this section we review the truthful profit extraction mechanism 
profitextractr this mechanism is a special case of a general 
cost-sharing schema due to moulin and shenker 
the goal of profit extraction is given bids b to extract a target 
value r of profit from some subset of the bidders 
profitextractr given bids b find the largest k such 
that the highest k bidders can equally share the cost 
r charge each of these bidders r k if no subset 
of bidders can cover the cost the mechanism has no 
winners 
important properties of this auction are as follows 
 profitextractr is truthful 
 if r ≤ f b profitextractr b r otherwise it has no 
winners and no revenue 
we will use this profit extraction mechanism in section with 
the following intuition such a profit extractor makes it possible to 
treat this subset of bidders as a single bid with value f s note 
that given a single bid b a truthful mechanism might offer it price 
t and if t ≤ b then the bidder wins and pays t otherwise the 
bidder pays nothing and loses likewise a mechanism can offer 
the set of bidders s a target revenue r if r ≤ f 
 s then 
profitextractr raises r from s otherwise the it raises no 
revenue from s 
 an optimal auction for three 
bidders 
in this section we define the optimal auction for three bidders 
υ and prove that it indeed matches the known lower bound of 
 we follow the definition and proof with a discussion of how 
this auction was derived 
definition υ is scale-invariant and symmetric and given 
by the bid-independent function with density function 
ρ x z 
⎧ 
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨ 
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ 
for x ≤ 
 with probability 
z with probability density g z for z 
for x ⎧ 
⎪⎨ 
⎪⎩ 
 with probability − 
x 
 
zg z dz 
x with probability 
x 
 
 z g z dz 
z with probability density g z for z x 
where g x 
 x− 
theorem the υ auction has a competitive ratio of ≈ 
 which is optimal furthermore the auction raises exactly 
 
 
f 
on every input with non-identical bids 
proof consider the bids x y with x y there are 
three cases 
case x y ≤ f 
 the auction must raise 
expected revenue of at least on these bids the bidder with 
valuation x will pay with and the bidder with valuation y 
will pay with probability therefore υ raises on 
these bids 
case x ≤ y f 
 the auction must raise 
expected revenue of at least on these bids the bidder with 
 
valuation x will pay − 
y 
 
zg z dz in expectation the 
bidder with valuation y will pay 
y 
 
zg z dz in expectation 
therefore υ raises on these bids 
case x ≤ y f 
 x the auction must raise 
expected revenue of at least x on these bids consider the 
revenue raised from all three bidders 
e υ b p x y p x y p y x 
 − 
y 
 
zg z dz − 
x 
 
zg z dz 
 x 
x 
 
 z g z dz 
y 
x 
zg z dz 
 x − 
x 
 
zg z dz x 
x 
 
g z dz 
 x 
the final equation comes from substituting in g x 
 x− and 
expanding the integrals note that the fraction of f 
raised on 
every input is identical if any of the inequalities ≤ x ≤ y 
are not strict the same proof applies giving a lower bound on the 
auction s profit however this bound may no longer be tight 
motivation for υ 
in this section we will conjecture that a particular input distribution 
is worst-case and show as a consequence that all inputs are 
worstcase in the optimal auction by applying this consequence we will 
derive an optimal auction for three bidders 
a truthful randomized auction on n bidders can be represented 
by a randomized function f rn− 
× n → r that maps masked 
bid vectors to prices in r by normalization we can assume that 
the lowest possible bid is recall that ρb-i z pr f b-i z 
the optimal auction for the finite auction problem can be found 
by the following optimization problem in which the variables are 
ρb-i z 
maximize r 
subject to 
n 
i 
bi 
z 
zρb-i z dz ≥ rf 
 b 
∞ 
z 
ρb-i z dz 
ρb-i z ≥ 
this set of integral inequalities is difficult to maximize over 
however by guessing which constraints are tight and which are 
slack at the optimum we will be able to derive a set of differential 
equations for which any feasible solution is an optimal auction 
as we discuss in section in the authors define a 
distribution and use it to find a lower bound on the competitive ratio of 
the optimal auction for two bidders this bid distribution is the 
worst-case input distribution we guess and later verify that this 
distribution is the worst-case input distribution for three bidders as 
well since this distribution has full support over the set of all bid 
vectors and a worst-case distribution puts positive probability only 
on worst-case inputs we can therefore assume that all but a 
measure zero set of inputs is worst-case for the optimal auction in the 
optimal two-bidder auction all inputs with non-identical bids are 
worst-case so we will assume the same for three bidders 
the guess that these constraints are tight allows us to transform 
the optimization problem into a feasibility problem constrained by 
differential equations if the solution to these equations has value 
matching the lower bound obtained from the worst-case 
distribution then this solution is the optimal auction and that our 
conjectured choice of worst-case distribution is correct 
in section we show that the optimal auction must sometimes 
place probability mass on sale prices above the highest bid this 
motivates considering symmetric scale-invariant auctions for three 
bidders with probability density function ρ x z of the following 
form 
ρ x z 
⎧ 
⎪⎨ 
⎪⎩ 
 with discrete probability a x 
x with discrete probability b x 
z with probability density g z for z x 
in this auction the sale price for the first bidder is either one 
of the latter two bids or higher than either bid with a probability 
density which is independent of the input 
the feasibility problem which arises from the linear optimization 
problem by assuming the constraints are tight is as follows 
a y a x xb x 
y 
x 
zg z dz r max x ∀x y 
a x b x 
∞ 
x 
g z dz 
a x ≥ 
b x ≥ 
g z ≥ 
solving this feasibility problem gives the auction υ proposed 
above the proof of its optimality validates its proposed form 
finding a simple restriction on the form of n-bidder auctions for 
n under which the optimal auction can be found analytically 
as above remains an open problem 
 generalized profit benchmarks 
in this section we widen our focus beyond auctions that compete 
with f 
to consider other benchmarks for an auction s profit we 
will show that for three bidders the form of the optimal auction 
is essentially independent of the benchmark profit used this 
results strongly corroborates the worst-case competitive analysis of 
auctions by showing that our techniques allow us to derive auctions 
which are competitive against a broad variety of reasonable 
benchmarks rather than simply against f 
 
previous work in competitive analysis of auctions has focused on 
the question of designing the auction with the best competitive 
ratio against f 
 the profit of the optimal omniscient single-priced 
mechanism that sells at least two items however it is reasonable to 
consider other benchmarks for instance one might wish to 
compete against v∗ 
 the profit of the k-vickrey auction with 
optimal-inhindsight choice of k 
alternatively if an auction is being used as 
a subroutine in a larger mechanism one might wish to choose the 
auction which is optimally competitive with a benchmark specific 
to that purpose 
recall that f 
 b max ≥k≥n kb k we can generalize this 
definition to gs parameterized by s s sn and defined as 
gs b max 
 ≤k≤n 
skb k 
when considering gs we assume without loss of generality that 
si si as otherwise the constraint imposed by si is irrelevant 
note that f 
is the special case of gs with si i and that v∗ 
 
gs with si i − 
 
recall that the k-vickrey auction sells a unit to each of the 
highest k bidders at a price equal to the k st highest bid b k 
achieving a profit of kb k 
 
competing with gs 
we will now design a three-bidder auction υs t 
 that achieves the 
optimal competitive ratio against gs t as before we will first find 
a lower bound on the competitive ratio and then design an auction 
to meet that bound 
we can lower bound the competitive ratio of υs t 
 using the same 
worst-case distribution from that we used against f 
 
evaluating the performance of any auction competing against gs t on 
this distribution will yield the following theorem we denote the 
optimal auction for three bidders against gs t as υs t 
 
theorem the optimal three-bidder auction υs t 
 
competing against gs t b max sb tb has a competitive ratio of 
at least s 
 t 
 t 
 
the proof can be found in the appendix 
similarly we can find the optimal auction against gs t using the 
same technique we used to solve for the three bidder auction with 
the best competitive ratio against f 
 
definition υs t 
 is scale-invariant and symmetric and given 
by the bid-independent function with density function 
ρ x z 
⎧ 
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨ 
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ 
for x ≤ t 
s 
 with probability t 
s t 
z with probability density g z for z t 
s 
for x t 
s⎧ 
⎪⎪⎨ 
⎪⎪⎩ 
 with probability t 
s t − 
x 
t 
s 
zg z dz 
x with probability 
x 
t 
s 
 z g z dz 
z with probability density g z for z x 
where g x t−s 
 s 
 t 
 
 x− 
theorem υs t 
 is s 
 t 
 t 
-competitive with gs t 
this auction like υ can be derived by reducing the optimization 
problem to a feasibility problem guessing that the optimal solution 
has the same form as υs t 
 and solving the auction is optimal 
because it matches the lower bound found above note that the form 
of υs t 
 is essentially the same as for υ but that the probability of 
each price is scaled depending on the values of s and t 
that our auction for three bidders matches the lower bound 
computed by the input distribution used in is strong evidence that 
this input distribution is the worst-case input distribution for any 
number of bidders and any generalized profit benchmark 
furthermore we strongly suspect that for any number of bidders the form 
of the optimal auction will be independent of the benchmark used 
 aggregation auctions 
we have seen that optimal auctions for small cases of the 
limitedsupply model can be found analytically in this section we will 
construct a schema for turning limited supply auctions into 
unlimited supply auctions with a good competitive ratio 
as discussed in section the existence of a profit 
extractor profitextractr allows an auction to treat a set of bids s 
as a single bid with value f s given n bidders and an 
auction am for m n bidders we can convert the m-bidder 
auction into an n-bidder auction by randomly partitioning the bidders 
into m subsets and then treating each subset as a single bidder via 
profitextractr and running the m-bidder auction 
definition given a truthful m-bidder auction am the 
m-aggregation auction for am aggam 
 works as follows 
 cast each bid uniformly at random into one of m bins 
resulting in bid vectors b 
 b m 
 
 for each bin j compute the aggregate bid bj f b j 
 
let b be the vector of aggregate bids and b−j be the 
aggregate bids for all bins other than j 
 compute the aggregate price tj f b−j where f is the 
bid-independent function for am 
 for each bin j run profitextracttj on b j 
 
since am and profitextractr are truthful tj is computed 
independently of any bid in bin j and thus the price offered any bidder 
in b j 
is independent of his bid therefore 
theorem if am is truthful the m-aggregation auction for 
am aggam 
 is truthful 
note that this schema yields a new way of understanding the 
random sampling profit extraction rspe auction as the 
simplest case of an aggregation auction it is the -aggregation auction 
for υ the -unit vickrey auction 
to analyze aggam 
 consider throwing k balls into m labeled 
bins let k represent a configuration of balls in bins so that ki is 
equal to the number of balls in bin i and k i is equal to the number 
of balls in the ith largest bin let km k represent the set of all 
possible configurations of k balls in m bins we write the multinomial 
coefficient of k as k 
k 
 the probability that a particular 
configuration k arises by throwing balls into bins uniformly at random is 
k 
k 
m−k 
 
theorem let am be an auction with competitive ratio β 
then the m-aggregation auction for am aggam 
 raises the 
following fraction of the optimal revenue f 
 b 
e aggam 
 b 
f 
≥ min 
k≥ 
k∈km k 
f 
 k k 
k 
βkmk 
proof by definition f 
sells to k ≥ bidders at a single 
price p let kj be the number of such bidders in b j 
 clearly 
f b j 
 ≥ pkj therefore 
f 
 f b 
 f b n 
 
f b 
≥ 
f 
 pk pkn 
pk 
 
f 
 k kn 
k 
the inequality follows from the monotonicity of f 
 and the 
equality from the homogeneity of f 
 
profitextracttj will raise tj if tj ≤ bj and no profit 
otherwise thus e aggam 
 b ≥ e f 
 b β the theorem 
follows by rewriting this expectation as a sum over all k in km k 
 a competitive auction 
we apply the aggregation auction schema to υ our optimal 
auction for three bidders to achieve an auction with competitive 
ratio this improves on the previously best known auction 
which is -competitive 
theorem the aggregation auction for υ has competitive 
ratio 
 
proof by theorem 
e aggυ 
 b 
f b 
≥ min 
k≥ 
k 
i 
k−i 
j 
f 
 i j k − i − j k 
i j k−i−j 
βk k 
for k and k e aggυ 
 b 
 
k β as k increases 
e aggυ 
 b f 
increases as well since we do not expect 
to find a closed-form formula for the revenue we lower bound 
f 
 b by b using large deviation bounds one can show 
that this lower bound is greater than 
 
k β for large-enough k and 
the remainder can be shown by explicit calculation 
plugging in β the competitive ratio is as k 
increases the competitive ratio approaches 
note that the above bound on the competitive ratio of aggυ 
is tight to see this consider the bid vector with two very large 
and non-identical bids of h and h with the remaining bids 
given that the competitive ratio of υ is tight on this example 
the expected revenue of this auction on this input will be exactly 
 
 a gs t-based aggregation auction 
in this section we show that υ is not the optimal auction to 
use in an aggregation auction one can do better by choosing the 
auction that is optimally competitive against a specially tailored 
benchmark 
to see why this might be the case notice table that the 
fraction of f 
 b raised for when there are k and k 
winning bidders in f 
 b is substantially smaller than the fraction of 
f 
 b raised when there are more winners this occurs because 
the expected ratio between f 
 b and f 
 b is lower in this 
case while the competitive ratio of υ is constant if we chose a 
three bidder auction that performed better when f 
has smaller 
numbers of winners our aggregation auction would perform better 
in the worst case 
one approach is to compete against a different benchmark that 
puts more weight than f 
on solutions with a small number of 
winners recall that f 
is the instance of gs t with s and 
t by using the auction that competes optimally against gs t 
with s while holding t we will raise a higher 
fraction of revenue on smaller numbers of winning bidders and a lower 
fraction of revenue on large numbers of winning bidders we can 
numerically optimize the values of s and t in gs t b in order to 
achieve the best competitive ratio for the aggregation auction in 
fact this will allow us to improve our competitive ratio slightly 
theorem for an optimal choice of s and t the aggregation 
auction for υs t 
 is -competitive 
the proof follows the outline of theorem and with the 
optimal choice of s while t is held constant at 
 further reducing the competitive ratio 
there are a number of ways we might attempt to use this 
aggregation auction schema to continue to push the competitive ratio 
down in this section we give a brief discussion of several attempts 
 aggυm 
for m 
if the aggregation auction for υ has a competitive ratio of 
and the aggregation auction for υ has a competitive ratio of 
can we improve the competitive ratio by aggregating υ or υm 
for larger m we conjecture in the negative for m the 
aggregation auction for υm has a larger competitive ratio than the 
aggregation auction for υ the primary difficulty in proving this 
k m m m m m m 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
table e a b f 
 b for aggυm 
as a function of k the 
optimal number of winners in f 
 b the lowest value for 
each column is printed in bold 
conjecture lies in the difficulty of finding a closed-form solution 
for the formula of theorem we can however evaluate this 
formula numerically for different values of m and k assuming that the 
competitive ratio for υm matches the lower bound for m given by 
theorem table shows for each value of m and k the fraction 
of f 
raised by the aggregation auction for aggυm 
when there 
are k winning bidders assuming the lower bound of theorem is 
tight 
 convex combinations of aggυm 
as can be seen in table when m the worst-case value 
of k is no longer and but instead an increasing function of 
m an aggregation auction for υm outperforms the aggregation 
auction for υ when there are two or three winning bidders while 
the aggregation auction for υ outperforms the other when there 
are at least six winning bidders thus for instance an auction 
which randomizes between aggregation auctions for υ and υ 
will have a worst-case which is better than that of either auction 
alone larger combinations of auctions will allow more room to 
optimize the worst-case however we suspect that no convex 
combination of aggregation auctions will have a competitive ratio lower 
than furthermore note that we cannot yet claim the existence of 
a good auction via this technique as the optimal auction υn for 
n is not known and it is only conjectured that the bound given 
by theorem and represented in table is correct for υn 
 a lower bound for conservative 
auctions 
in this section we define a class of auctions that never offer a 
sale price which is higher than any bid in the input and prove a 
lower bound on the competitive ratio of these auctions as this 
 
lower bound is stronger than the lower bound of theorem for 
n ≥ it shows that the optimal auction must occasionally charge 
a sales price higher than any bid in the input specifically this result 
partially explains the form of the optimal three bidder auction 
definition we say an auction bif is conservative if its 
bidindependent function f satisfies f b-i ≤ max b-i 
we can now state our lower bound for conservative auctions 
theorem let a be a conservative auction for n bidders 
then the competitive ratio of a is at least n− 
n 
 
corollary the competitive ratio of any conservative 
auction for an arbitrary number of bidders is at least three 
for a two-bidder auction this restriction does not prevent 
optimality υ the -unit vickrey auction is conservative for larger 
numbers of bidders however the restriction to conservative 
auctions does affect the competitive ratio for the three-bidder case 
υ has competitive ratio while the best conservative auction 
is no better than -competitive 
the k-vickrey auction and the random sampling optimal price 
auction are conservative auctions the random sampling profit 
extraction auction and the core auction on the other hand 
use the profitextractr mechanism as a subroutine and thus 
sometimes offer a sale price which is higher than the highest input bid 
value 
in the authors define a restricted auction as one on which 
for any input the sale prices are drawn from the set of input bid 
values the class of conservative auctions can be viewed as a 
generalization of the class of restricted auctions and therefore our result 
below gives lower bounds on the performance of a broader class of 
auctions 
we will prove theorem with the aid of the following lemma 
lemma let a be a conservative auction with competitive 
ratio r for n bidders let h n let h and hk kh 
otherwise then for all k and h ≥ kh pr f h ≤ hk ≥ 
nr− 
n− 
 k nr− r−n 
n− 
 
proof the lemma is proved by strong induction on k first 
some notation that will be convenient for any particular k and 
h we will be considering the bid vector b hk h 
and placing bounds on ρb-i z since we can assume without loss 
of generality that the auction is symmetric we will notate b- as 
b with any one of the -valued bids masked similarly we notate 
b-hk 
 resp b-h as b with the hk-valued bid resp h-valued bid 
masked we will also let p b phk 
 b and ph b represent the 
expected payment of a -valued hk-valued and h-valued bidder 
in a on b respectively note by symmetry the expected payment 
for all -valued bidders is the same 
base case k hk a must raise revenue of at least rn 
on b h 
rn ≤ ph b n − p b 
 n − 
 
 
xρb- x dx 
≤ n − 
 
 
ρb- x dx 
the second inequality follows from the conservatism of the 
underlying auction the base case follows trivially from the final 
inequality 
inductive case k hk kh let b hk h 
first we will find an upper bound on ph b 
ph b 
 
 
xρb-h x dx 
k 
i 
hi 
hi− 
xρb-h x dx 
≤ 
k 
i 
hi 
hi 
hi− 
ρb-h x dx 
≤ 
 nr − r − n 
n − 
k− 
i 
ih 
 kh − 
nr − 
n − 
− k − 
 nr − r − n 
n − 
 
 kh 
n − r 
n − 
 
 k − 
 
 nr − r − n 
n − 
 
equation follows from the conservatism of a and is from 
invoking the strong inductive hypothesis with h kh and the 
observation that the maximum possible revenue will be found by 
placing exactly enough probability at each multiple of h to satisfy 
the constraints of the inductive hypothesis and placing the 
remaining probability at kh next we will find a lower bound on phk 
 b 
by considering the revenue raised by the bids b recall that a must 
obtain a profit of at least rf 
 b rkh given upper-bounds 
on the profit from the h-valued equation bid and the -valued 
bids the profit from the hk-valued bid must be at least 
phk 
 b ≥ rkh − n − p b − ph b 
≥ kh r − 
n − r 
n − 
 
 k − 
 
 nr − r − n 
n − 
− o n 
 
in order to lower bound pr f b-hk 
 ≤ kh consider the auction 
that minimizes it and is consistent with the lower bounds obtained 
by the strong inductive hypothesis on pr f b-hk 
 ≤ ih to 
minimize the constraints implied by the strong inductive hypothesis we 
place the minimal amount of probability mass required each price 
level this gives ρhk 
 b with nr− 
n− 
probability at and exactly 
 nr− r−n 
n− 
at each hi for ≤ i k thus the profit from offering 
prices at most hk− is nr− 
n− 
−kh k− nr− r−n 
n− 
 in order to 
satisfy our lower bound on phk 
 b it must put at least nr− r−n 
n− 
on hk 
therefore the probability that the sale price will be no more than 
kh on masked bid vector on bid vector b kh h must 
be at least nr− 
n− 
 k nr− r−n 
n− 
 
given lemma theorem is simple to prove 
proof let a be a conservative auction suppose nr− r−n 
n− 
 
 let k h ≥ kh and h n by lemma 
pr f kh h ≤ hk ≥ nr− 
n− 
 k but this is a 
contradiction so nr− r−n 
n− 
≤ thus r ≤ n 
 n− 
 the theorem 
follows 
 conclusions and future work 
we have found the optimal auction for the three-unit 
limitedsupply case and shown that its structure is essentially independent 
of the benchmark used in its competitive analysis we have then 
used this auction to derive the best known auction for the unlimited 
supply case 
our work leaves many interesting open questions we found that 
the lower bound of is matched by an auction for three bidders 
 
even when competing against generalized benchmarks the most 
interesting open question from our work is whether the lower bound 
from theorem can be matched by an auction for more than three 
bidders we conjecture that it can 
second we consider whether our techniques can be extended 
to find optimal auctions for greater numbers of bidders the use 
of our analytic solution method requires knowledge of a restricted 
class of auctions which is large enough to contain an optimal 
auction but small enough that the optimal auction in this class can be 
found explicitly through analytic methods no class of auctions 
which meets these criteria is known even for the four bidder case 
also when the number of bidders is greater than three it might 
be the case that the optimal auction is not expressible in terms of 
elementary functions 
another interesting set of open questions concerns aggregation 
auctions as we show the aggregation auction for υ outperforms 
the aggregation auction for υ and it appears that the aggregation 
auction for υ is better than υm for m we leave 
verification of this conjecture for future work we also show that υ is 
not the best three-bidder auction for use in an aggregation auction 
but the auction that beats it is able to reduce the competitive 
ratio of the overall auction only a little bit it would be interesting 
to know whether for any m there is an m-aggregation auction that 
substantially improves on the competitive ratio of aggυm 
 
finally we remark that very little is known about the structure 
of the optimal competitive auction in our auction υ the sales 
price for a given bidder is restricted either to be one of the other bid 
values or to be higher than all other bid values the optimal 
auction for two bidders the -unit vickrey auction also falls within 
this class of auctions as its sales prices are restricted to bid values 
we conjecture that an optimal auction for any number of bidders 
lies within this class our paper provides partial evidence for this 
conjecture the lower bound of section on conservative auctions 
shows that the optimal auction must offer sales prices higher than 
any bid value if the lower bound of theorem is tight as is 
conjectured it remains to show that optimal auctions otherwise only 
offer sales prices at bid values 
 acknowledgements 
the authors wish to thank yoav shoham and noga alon for 
helpful discussions 
 references 
 a archer and e tardos truthful mechanisms for 
one-parameter agents in proc of the nd ieee symposium 
on foundations of computer science 
 s baliga and r vohra market research and market design 
advances in theoretical economics 
 a borodin and r el-yaniv online computation and 
competitive analysis cambridge university press 
 j bulow and j roberts the simple economics of optimal 
auctions the journal of political economy - 
 
 a fiat a v goldberg j d hartline and a r karlin 
competitive generalized auctions in proc th acm 
symposium on the theory of computing pages - 
acm 
 a goldberg j hartline a karlin m saks and a wright 
competitive auctions and digital goods games and 
economic behavior submitted for publication an 
earlier version available as intertrust technical report at 
url http www star-lab com tr tr- - html 
 a v goldberg and j d hartline competitiveness via 
consensus in proc th symposium on discrete algorithms 
pages - acm siam 
 a v goldberg j d hartline a r karlin and m e saks 
a lower bound on the competitive ratio of truthful auctions 
in proc st symposium on theoretical aspects of 
computer science pages - springer 
 a v goldberg j d hartline and a wright competitive 
auctions and digital goods in proc th symposium on 
discrete algorithms pages - acm siam 
 d lehmann l i o callaghan and y shoham truth 
revelation in approximately efficient combinatorial 
auctions in proc of st acm conf on e-commerce pages 
 - acm press new york 
 h moulin and s shenker strategyproof sharing of 
submodular costs budget balance versus efficiency 
economic theory - 
 r myerson optimal auction design mathematics of 
operations research - 
 n nisan and a ronen algorithmic mechanism design in 
proc of st symp on theory of computing pages 
 - acm press new york 
 i segal optimal pricing mechanisms with unknown 
demand american economic review 
 w vickrey counterspeculation auctions and competitive 
sealed tenders j of finance - 
appendix 
a proof of theorem 
we wish to prove that υs t 
 the optimal auction for three bidders 
against gs t has competitive ratio at least s 
 t 
 t 
 our proof 
follows the outline of the proof of lemma and theorem from 
however our case is simpler because we only looking for a bound 
when n define the random bid vector b b b b 
with pr bi z z we compute eb gs t b by integrating 
pr gs t b z then we use the fact that no auction can have 
expected profit greater than on b to find a lower bound on the 
competitive ratio against gs t for any auction 
for the input distribution b defined above let b i be the ith 
largest bid define the disjoint events h b ≥ z s ∧ b 
z t and h b ≥ z t intuitively h corresponds to the 
event that all three bidders win in gs t while h corresponds to 
the event that only the top two bidders win gs t b will be greater 
than z if either event occurs 
pr gs t b z pr h pr h 
 
s 
z 
 
 − 
t 
z 
 
t 
z 
 
 
using the identity defined for non-negative continuous random 
variables that e x 
∞ 
 
pr x x dx we have 
eb gs t b t 
∞ 
t 
 
s 
z 
 
 − 
t 
z 
 
t 
z 
 
dz 
 
s 
 t 
 t 
 
given that for any auction a eb ea a b ≤ it is clear 
that 
eb gs t b 
eb ea a b 
≥ s 
 t 
 t 
 therefore there exists some input b 
for each auction a such that 
gs t b 
ea a b 
≥ s t 
 t 
 
 
an analysis of alternative slot auction designs for 
sponsored search 
s´ebastien lahaie 
∗ 
division of engineering and applied sciences 
harvard university cambridge ma 
slahaie eecs harvard edu 
abstract 
billions of dollars are spent each year on sponsored search 
a form of advertising where merchants pay for placement 
alongside web search results slots for ad listings are 
allocated via an auction-style mechanism where the higher a 
merchant bids the more likely his ad is to appear above 
other ads on the page in this paper we analyze the 
incentive efficiency and revenue properties of two slot 
auction designs rank by bid rbb and rank by revenue 
 rbr which correspond to stylized versions of the 
mechanisms currently used by yahoo and google respectively 
we also consider first- and second-price payment rules 
together with each of these allocation rules as both have been 
used historically we consider both the short-run 
incomplete information setting and the long-run complete 
information setting with incomplete information neither rbb 
nor rbr are truthful with either first or second pricing we 
find that the informational requirements of rbb are much 
weaker than those of rbr but that rbr is efficient whereas 
rbb is not we also show that no revenue ranking of rbb 
and rbr is possible given an arbitrary distribution over 
bidder values and relevance with complete information 
we find that no equilibrium exists with first pricing using 
either rbb or rbr we show that there typically exists a 
multitude of equilibria with second pricing and we bound 
the divergence of economic value in such equilibria from 
the value obtained assuming all merchants bid truthfully 
categories and subject descriptors 
j computer applications social and behavioral 
sciences-economics 
general terms 
economics theory 
 introduction 
today internet giants google and yahoo boast a 
combined market capitalization of over billion largely on 
the strength of sponsored search the fastest growing 
component of a resurgent online advertising industry 
pricewaterhousecoopers estimates that industry-wide sponsored 
search revenues were billion or of total 
internet advertising revenues 
industry watchers expect 
revenues to reach or exceed billion 
roughly of 
google s estimated billion in revenue and roughly 
 of yahoo s estimated billion in revenue will 
likely be attributable to sponsored search 
a number of 
other companies-including looksmart findwhat 
interactivecorp ask jeeves and ebay shopping com -earn 
hundreds of millions of dollars of sponsored search revenue 
annually 
sponsored search is a form of advertising where merchants 
pay to appear alongside web search results for example 
when a user searches for used honda accord san diego in 
a web search engine a variety of commercial entities san 
diego car dealers honda corp automobile information 
portals classified ad aggregators ebay etc may bid to to 
have their listings featured alongside the standard 
algorithmic search listings advertisers bid for placement on the 
page in an auction-style format where the higher they bid 
the more likely their listing will appear above other ads on 
the page by convention sponsored search advertisers 
generally pay per click meaning that they pay only when a user 
clicks on their ad and do not pay if their ad is displayed but 
not clicked though many people claim to systematically 
ignore sponsored search ads majestic research reports that 
 
www iab net resources adrevenue pdf iab pwc full pdf 
 
battellemedia com archives php 
 
these are rough back of the envelope estimates google 
and yahoo revenue estimates were obtained from 
yahoo finance we assumed billion in industry-wide 
sponsored search revenues we used nielsen netratings 
estimates of search engine market share in the us the most 
monetized market 
wired-vig wired com news technology html 
using comscore s international search engine market share 
estimates would yield different estimates 
www comscore com press release asp press 
 
as many as of google searches result in a paid click and 
that google earns roughly nine cents on average for every 
search query they process 
usually sponsored search results appear in a separate 
section of the page designated as sponsored above or to the 
right of the algorithmic results sponsored search results 
are displayed in a format similar to algorithmic results as 
a list of items each containing a title a text description 
and a hyperlink to a corresponding web page we call each 
position in the list a slot generally advertisements that 
appear in a higher ranked slot higher on the page garner 
more attention and more clicks from users thus all else 
being equal merchants generally prefer higher ranked slots 
to lower ranked slots 
merchants bid for placement next to particular search 
queries for example orbitz and travelocity may bid for 
las vegas hotel while dell and hp bid for laptop 
computer as mentioned bids are expressed as a maximum 
willingness to pay per click for example a forty-cent bid 
by hostrocket for web hosting means hostrocket is 
willing to pay up to forty cents every time a user clicks on their 
ad 
the auctioneer the search engine 
 evaluates the bids 
and allocates slots to advertisers in principle the 
allocation decision can be altered with each new incoming search 
query so in effect new auctions clear continuously over time 
as search queries arrive 
many allocation rules are plausible in this paper we 
investigate two allocation rules roughly corresponding to the 
two allocation rules used by yahoo and google the rank 
by bid rbb allocation assigns slots in order of bids with 
higher ranked slots going to higher bidders the rank by 
revenue rbr allocation assigns slots in order of the 
product of bid times expected relevance where relevance is the 
proportion of users that click on the merchant s ad after 
viewing it in our model we assume that an ad s expected 
relevance is known to the auctioneer and the advertiser but 
not necessarily to other advertisers and that clickthrough 
rate decays monotonically with lower ranked slots in 
practice the expected clickthrough rate depends on a number 
of factors including the position on the page the ad text 
 which in turn depends on the identity of the bidder the 
nature and intent of the user and the context of other ads 
and algorithmic results on the page and must be learned 
over time by both the auctioneer and the bidder as of 
this writing to a rough first-order approximation yahoo 
employs a rbb allocation and google employs a rbr 
allocation though numerous caveats apply in both cases when 
it comes to the vagaries of real-world implementations 
even when examining a one-shot version of a slot 
auction the mechanism differs from a standard multi-item 
auc 
battellemedia com archives php 
 
usually advertisers also set daily or monthly budget caps 
in this paper we do not model budget constraints 
 
in the sponsored search industry the auctioneer and search 
engine are not always the same entity for example google 
runs the sponsored search ads for aol web search with 
revenue being shared similarly yahoo currently runs the 
sponsored search ads for msn web search though microsoft 
will begin independent operations soon 
 
here are two among many exceptions to the yahoo 
rbb and google rbr assertion yahoo excludes 
ads deemed insufficiently relevant either by a human editor 
or due to poor historical click rate google sets differing 
reserve prices depending on google s estimate of ad quality 
tion in subtle ways first a single bid per merchant is used 
to allocate multiple non-identical slots second the bid is 
communicated not as a direct preference over slots but as 
a preference for clicks that depend stochastically on slot 
allocation 
we investigate a number of economic properties of rbb 
and rbr slot auctions we consider the short-run 
incomplete information case in section adapting and 
extending standard analyses of single-item auctions in section 
we turn to the long-run complete information case our 
characterization results here draw on techniques from 
linear programming throughout important observations are 
highlighted as claims supported by examples our 
contributions are as follows 
 we show that with multiple slots bidders do not reveal 
their true values with either rbb or rbr and with 
either first- or second-pricing 
 with incomplete information we find that the 
informational requirements of playing the equilibrium bid 
are much weaker for rbb than for rbr because 
bidders need not know any information about each others 
relevance or even their own with rbb 
 with incomplete information we prove that rbr is 
efficient but that rbb is not 
 we show via a simple example that no general revenue 
ranking of rbb and rbr is possible 
 we prove that in a complete-information setting 
firstprice slot auctions have no pure strategy nash 
equilibrium but that there always exists a pure-strategy 
equilibrium with second pricing 
 we provide a constant-factor bound on the deviation 
from efficiency that can occur in the equilibrium of a 
second-price slot auction 
in section we specify our model of bidders and the 
various slot auction formats 
in section we study the incentive properties of each 
format asking in which cases agents would bid truthfully 
there is possible confusion here because the second-price 
design for slot auctions is reminiscent of the vickrey auction 
for a single item we note that for slot auctions the vickrey 
mechanism is in fact very different from the second-price 
mechanism and so they have different incentive properties 
in section we derive the bayes-nash equilibrium bids 
for the various auction formats this is useful for the 
efficiency and revenue results in later sections it should 
become clear in this section that slot auctions in our model 
are a straightforward generalization of single-item auctions 
sections and address questions of efficiency and 
revenue under incomplete information respectively 
in section we determine whether pure-strategy 
equilibria exist for the various auction formats under complete 
information in section we derive bounds on the 
deviation from efficiency in the pure-strategy equilibria of 
secondprice slot auctions 
our approach is positive rather than normative we aim 
to clarify the incentive efficiency and revenue properties of 
two slot auction designs currently in use under settings of 
 
other authors have also made this observation 
 
incomplete and complete information we do not attempt 
to derive the optimal mechanism for a slot auction 
related work feng et al compare the revenue 
performance of various ranking mechanisms for slot auctions 
in a model with incomplete information much as we do in 
section but they obtain their results via simulations 
whereas we perform an equilibrium analysis 
liu and chen study properties of slot auctions under 
incomplete information their setting is essentially the same 
as ours except they restrict their attention to a model with 
a single slot and a binary type for bidder relevance high or 
low they find that rbr is efficient but that no general 
revenue ranking of rbb and rbr is possible which agrees 
with our results they also take a design approach and 
show how the auctioneer should assign relevance scores to 
optimize its revenue 
edelman et al model the slot auction problem both as 
a static game of complete information and a dynamic game 
of incomplete information they study the locally 
envyfree equilibria of the static game of complete information 
this is a solution concept motivated by certain bidding 
behaviors that arise due to the presence of budget constraints 
they do not view slot auctions as static games of 
incomplete information as we do but do study them as dynamic 
games of incomplete information and derive results on the 
uniqueness and revenue properties of the resulting 
equilibria they also provide a nice description of the evolution of 
the market for sponsored search 
varian also studies slot auctions under a setting of 
complete information he focuses on symmetric 
equilibria which are a refinement of nash equilibria appropriate for 
slot auctions he provides bounds on the revenue obtained 
in equilibrium he also gives bounds that can be used to 
infer bidder values given their bids and performs some 
empirical analysis using these results in contrast we focus 
instead on efficiency and provide bounds on the deviation 
from efficiency in complete-information equilibria 
 preliminaries 
we focus on a slot auction for a single keyword in a 
setting of incomplete information a bidder knows only 
distributions over others private information value per click 
and relevance with complete information a bidder knows 
others private information and so does not need to rely on 
distributions to strategize we first describe the model for 
the case with incomplete information and drop the 
distributional information from the model when we come to the 
complete-information case in section 
 the model 
there is a fixed number k of slots to be allocated among 
n bidders we assume without loss of generality that k ≤ 
n since superfluous slots can remain blank bidder i assigns 
a value of xi to each click received on its advertisement 
regardless of this advertisement s rank 
the probability 
that i s advertisement will be clicked if viewed is ai ∈ 
we refer to ai as bidder i s relevance we refer to ri 
aixi as bidder i s revenue the xi ai and ri are random 
 
indeed kitts et al find that in their sample of actual 
click data the correlation between rank and conversion rate 
is not statistically significant however for the purposes 
of our model it is also important that bidders believe that 
conversion rate does not vary with rank 
variables and we denote their realizations by xi αi and ri 
respectively the probability that an advertisement will be 
viewed if placed in slot j is γj ∈ we assume γ 
γ γk hence bidder i s advertisement will have a 
clickthrough rate of γjαi if placed in slot j of course an 
advertisement does not receive any clicks if it is not allocated 
a slot 
each bidder s value and relevance pair xi ai is 
independently and identically distributed on ¯x × according 
to a continuous density function f that has full support on 
its domain the density f and slot probabilities γ γk 
are common knowledge only bidder i knows the 
realization xi of its value per click xi both bidder i and the seller 
know the realization αi of ai but this realization remains 
unobservable to the other bidders 
we assume that bidders have quasi-linear utility 
functions that is the expected utility to bidder i of obtaining 
the slot of rank j at a price of b per click is 
ui j b γjαi xi − b 
if the advertising firms bidding in the slot auction are 
riskneutral and have ample liquidity quasi-linearity is a 
reasonable assumption 
the assumptions of independence symmetry and 
riskneutrality made above are all quite standard in single-item 
auction theory the assumption that clickthrough 
rate decays monotonically with lower slots-by the same 
factors for each agent-is unique to the slot auction 
problem we view it as a main contribution of our work to 
show that this assumption allows for tractable analysis of 
the slot auction problem using standard tools from 
singleitem auction theory it also allows for interesting results in 
the complete information case a common model of 
decaying clickthrough rate is the exponential decay model where 
γk 
δk− with decay δ feng et al state that 
their actual clickthrough data is fitted extremely well by an 
exponential decay model with δ 
our model lacks budget constraints which are an 
important feature of real slot auctions with budget constraints 
keyword auctions cannot be considered independently of one 
another because the budget must be allocated across 
multiple keywords-a single advertiser typically bids on multiple 
keywords relevant to his business introducing this element 
into the model is an important next step for future work 
 auction formats 
in a slot auction a bidder provides to the seller a declared 
value per click ˜xi xi αi which depends on his true value 
and relevance we often denote this declared value bid 
by ˜xi for short since a bidder s relevance αi is observable 
to the seller the bidder cannot misrepresent it we denote 
the kth 
highest of the n declared values by ˜x k 
 and the 
kth 
highest of the n declared revenues by ˜r k 
 where the 
declared revenue of bidder i is ˜ri αi ˜xi we consider two 
types of allocation rules rank by bid rbb and rank 
by revenue rbr 
 
models with budget constraints have begun to appear in 
this research area abrams and borgs et al design 
multi-unit auctions for budget-constrained bidders which 
can be interpreted as slot auctions with a focus on revenue 
optimization and truthfulness mehta et al address 
the problem of matching user queries to budget-constrained 
advertisers so as to maximize revenue 
 
rbb slot k goes to bidder i if and only if ˜xi ˜x k 
 
rbr slot k goes to bidder i if and only if ˜ri ˜r k 
 
we will commonly represent an allocation by a one-to-one 
function σ k → n where n is the set of integers 
{ n} hence slot k goes to bidder σ k 
we also consider two different types of payment rules 
note that no matter what the payment rule a bidder that 
is not allocated a slot will pay since his listing cannot 
receive any clicks 
first-price the bidder allocated slot k namely σ k pays 
˜xσ k per click under both the rbb and rbr 
allocation rules 
second-price if k n bidder σ k pays ˜xσ k per click 
under the rbb rule and pays ˜rσ k ασ k per click 
under the rbr rule if k n bidder σ k pays per 
click 
intuitively a second-price payment rule sets a bidder s 
payment to the lowest bid it could have declared while 
maintaining the same ranking given the allocation rule used 
overture introduced the first slot auction design in 
using a first-price rbb scheme google then followed in 
 with a second-price rbr scheme in overture 
 at this point acquired by yahoo then switched to second 
pricing but still allocates using rbb one possible reason 
for the switch is given in section 
we assume that ties are broken as follows in the event 
that two agents make the exact same bid or declare the 
same revenue there is a permutation of the agents κ 
 n → n that is fixed beforehand if the bids of agents i 
and j are tied then agent i obtains a higher slot if and only 
if κ i κ j this is consistent with the practice in real 
slot auctions where ties are broken by the bidders order of 
arrival 
 incomplete information 
 incentives 
it should be clear that with a first-price payment rule 
truthful bidding is neither a dominant strategy nor an ex 
post nash equilibrium using either rbb or rbr because 
this guarantees a payoff of there is always an incentive 
to shade true values with first pricing 
the second-price payment rule is reminiscent of the 
secondprice vickrey auction used for selling a single item and in 
a vickrey auction it is a dominant strategy for a bidder to 
reveal his true value for the item however using a 
second-price rule in a slot auction together with either 
allocation rule above does not yield an incentive-compatible 
mechanism either in dominant strategies or ex post nash 
equilibrium 
with a second-price rule there is no 
incentive for a bidder to bid higher than his true value per click 
using either rbb or rbr this either leads to no change 
 
we are effectively assuming a reserve price of zero but in 
practice search engines charge a non-zero reserve price per 
click 
 
unless of course there is only a single slot available since 
this is the single-item case with a single slot both rbb 
and rbr with a second-price payment rule are 
dominantstrategy incentive-compatible 
in the outcome or a situation in which he will have to pay 
more than his value per click for each click received 
resulting in a negative payoff 
however with either allocation 
rule there may be an incentive to shade true values with 
second pricing 
claim with second pricing and k ≥ truthful 
bidding is not a dominant strategy nor an ex post nash 
equilibrium for either rbb or rbr 
example there are two agents and two slots the 
agents have relevance α α whereas γ and 
γ agent has a value of x per click and agent 
 has a value of x per click let us first consider the 
rbb rule suppose agent bids truthfully if agent also 
bids truthfully he wins the first slot and obtains a payoff of 
 however if he shades his bid down below he obtains 
the second slot at a cost of per click yielding a payoff of 
 since the agents have equal relevance the exact same 
situation holds with the rbr rule hence truthful bidding 
is not a dominant strategy in either format and neither is 
it an ex post nash equilibrium 
to find payments that make rbb and rbr 
dominantstrategy incentive-compatible we can apply holmstrom s 
lemma see also chapter in milgrom under the 
restriction that a bidder with value per click does not pay 
anything even if he obtains a slot which can occur if there 
are as many slots as bidders this lemma implies that there 
is a unique payment rule that achieves dominant-strategy 
incentive compatibility for either allocation rule for rbb 
the bidder allocated slot k is charged per click 
kx 
i k 
 γi− − γi ˜x i 
 γk ˜x k 
 
note that if k n ˜x k 
 since there is no k th 
bidder for rbr the bidder allocated slot k is charged per 
click 
 
ασ k 
kx 
i k 
 γi− − γi ˜r i 
 γk ˜r k 
 
 
using payment rule and rbr the auctioneer is aware 
of the true revenues of the bidders since they reveal their 
values truthfully and hence ranks them according to their 
true revenues we show in section that this allocation 
is in fact efficient since the vcg mechanism is the unique 
mechanism that is efficient truthful and ensures bidders 
with value pay nothing by the green-laffont theorem 
the rbr rule and payment scheme constitute exactly the 
vcg mechanism 
in the vcg mechanism an agent pays the externality he 
imposes on others to understand payment in this sense 
note that the first term is the added utility due to an 
increased clickthrough rate agents in slots k to k would 
receive if they were all to move up a slot the last term is the 
utility that the agent with the k st 
revenue would receive 
by obtaining the last slot as opposed to nothing the 
leading coefficient simply reduces the agent s expected payment 
to a payment per click 
 
in a dynamic setting with second pricing there may be an 
incentive to bid higher than one s true value in order to 
exhaust competitors budgets this phenomenon is commonly 
called bid jamming or antisocial bidding 
 
 equilibrium analysis 
to understand the efficiency and revenue properties of 
the various auction formats we must first understand which 
rankings of the bidders occur in equilibrium with different 
allocation and payment rule combinations the following 
lemma essentially follows from the monotonic selection 
theorem by milgrom and shannon 
lemma in a rbb rbr auction with either a 
firstor second-price payment rule the symmetric bayes-nash 
equilibrium bid is strictly increasing with value revenue 
as a consequence of this lemma we find that rbb and 
rbr auctions allocate the slots greedily by the true values 
and revenues of the agents respectively whether using 
firstor second-price payment rules this will be relevant in 
section below for a first-price payment rule we can 
explicitly derive the symmetric bayes-nash equilibrium bid 
functions for rbb and rbr auctions the purpose of this 
exercise is to lend qualitative insights into the parameters 
that influence an agent s bidding and to derive formulae for 
the expected revenue in rbb and rbr auctions in order 
to make a revenue ranking of these two allocation rules in 
section 
let g y be the expected resulting clickthrough rate in 
a symmetric equilibrium of the rbb auction with either 
payment rule to a bidder with value y and relevance α 
 let h y be the analogous quantity for a bidder with 
revenue y and relevance in a rbr auction by lemma 
a bidder with value y will obtain slot k in a rbb auction 
if y is the kth 
highest of the true realized values the same 
applies in a rbr auction when y is the kth 
highest of the true 
realized revenues let fx y be the distribution function for 
value and let fr y be the distribution function for revenue 
the probability that y is the kth 
highest out of n values is 
n − 
k − 
 
 − fx y k− 
fx y n−k 
whereas the probability that y is the kth 
highest out of n 
revenues is the same formula with fr replacing fx hence 
we have 
g y 
kx 
k 
γk 
n − 
k − 
 
 − fx y k− 
fx y n−k 
the h function is analogous to g with fr replacing fx 
in the two propositions that follow g and h are the 
derivatives of g and h respectively we omit the proof of the 
next proposition because it is almost identical to the 
derivation of the equilibrium bid in the single-item case see 
krishna proposition 
proposition the symmetric bayes-nash equilibrium 
strategies in a first-price rbb auction are given by 
˜xb 
 x α 
 
g x 
z x 
 
y g y dy 
the first-price equilibrium above closely parallels the 
firstprice equilibrium in the single-item model with a single 
item g is the density of the second highest value among all 
n agent values whereas in a slot auction it is a weighted 
combination of the densities for the second third etc 
highest values 
note that the symmetric bayes-nash equilibrium bid in 
a first-price rbb auction does not depend on a bidder s 
relevance α to see clearly why note that a bidder chooses 
a bid b so as to maximize the objective 
αg ˜x− 
 b x − b 
and here α is just a leading constant factor so dropping 
it does not change the set of optimal solutions hence the 
equilibrium bid depends only on the value x and function 
g and g in turn depends only on the marginal cumulative 
distribution of value fx so really only the latter needs to 
be common knowledge to the bidders on the other hand 
we will now see that information about relevance is needed 
for bidders to play the equilibrium in the first-price rbr 
auction so the informational requirements for a first-price 
rbb auction are much weaker than for a first-price rbr 
auction in the rbb auction a bidder need not know his own 
relevance and need not know any distributional information 
over others relevance in order to play the equilibrium 
again we omit the next proposition s proof since it is so 
similar to the one above 
proposition the symmetric bayes-nash equilibrium 
strategies in a first-price rbr auction are given by 
˜xr 
 x α 
 
αh αx 
z αx 
 
y h y dy 
here it can be seen that the equilibrium bid is increasing 
with x but not necessarily with α this should not be much 
of a concern to the auctioneer however because in any case 
the declared revenue in equilibrium is always increasing in 
the true revenue 
it would be interesting to obtain the equilibrium bids 
when using a second-price payment rule but it appears that 
the resulting differential equations for this case do not have a 
neat analytical solution nonetheless the same conclusions 
about the informational requirements of the rbb and rbr 
rules still hold as can be seen simply by inspecting the 
objective function associated with an agent s bidding problem 
for the second-price case 
 efficiency 
a slot auction is efficient if in equilibrium the sum of the 
bidders revenues from their allocated slots is maximized 
using symmetry as our equilibrium selection criterion we 
find that the rbb auction is not efficient with either 
payment rule 
claim the rbb auction is not efficient with either 
first or second pricing 
example there are two agents and one slot with γ 
 agent has a value of x per click and relevance 
α agent has a value of x per click and 
relevance α by lemma agents are ranked greedily 
by value hence agent obtains the lone slot for a total 
revenue of to the agents however it is most efficient to 
allocate the slot to agent for a total revenue of 
examples with more agents or more slots are simple to 
construct along the same lines on the other hand under 
our assumptions on how clickthrough rate decreases with 
lower rank the rbr auction is efficient with either payment 
rule 
 
theorem the rbr auction is efficient with either 
first- or second-price payments rules 
proof since by lemma the agents equilibrium bids 
are increasing functions of their revenues in the rbr 
auction slots are allocated greedily according to true revenues 
let σ be a non-greedy allocation then there are slots s t 
with s t and rσ s rσ t we can switch the agents in 
slots s and t to obtain a new allocation and the difference 
between the total revenue in this new allocation and the 
original allocation s total revenue is 
` 
γtrσ s γsrσ t 
´ 
− 
` 
γsrσ s γtrσ t 
´ 
 γs − γt 
` 
rσ t − rσ s 
´ 
both parenthesized terms above are positive hence the 
switch has increased the total revenue to the bidders if we 
continue to perform such switches we will eventually reach 
a greedy allocation of greater revenue than the initial 
allocation since the initial allocation was arbitrary it follows 
that a greedy allocation is always efficient and hence the 
rbr auction s allocation is efficient 
note that the assumption that clickthrough rate decays 
montonically by the same factors γ γk for all agents is 
crucial to this result a greedy allocation scheme does not 
necessarily find an efficient solution if the clickthrough rates 
are monotonically decreasing in an independent fashion for 
each agent 
 revenue 
to obtain possible revenue rankings for the different 
auction formats we first note that when the allocation rule is 
fixed to rbb then using either a first-price second-price or 
truthful payment rule leads to the same expected revenue in 
a symmetric increasing bayes-nash equilibrium because 
a rbb auction ranks agents by their true values in 
equilibrium for any of these payment rules by lemma it 
follows that expected revenue is the same for all these 
payment rules following arguments that are virtually identical 
to those used to establish revenue equivalence in the 
singleitem case see e g proposition in krishna the 
same holds for rbr auctions however the revenue ranking 
of the rbb and rbr allocation rules is still unclear 
because of this revenue equivalence principle we can choose 
whichever payment rule is most convenient for the purpose 
of making revenue comparisons 
using propositions and it is a simple matter to 
derive formulae for the expected revenue under both allocation 
rules the payment of an agent in a rbb auction is 
mb 
 x α αg x ˜xv 
 x α 
the expected revenue is then n · e 
ˆ 
mv 
 x a 
˜ 
 where the 
expectation is taken with respect to the joint density of value 
and relevance the expected revenue formula for rbr 
auctions is entirely analogous using ˜xr 
 x α and the h 
function with these in hand we can obtain revenue rankings 
for specific numbers of bidders and slots and specific 
distributions over values and relevance 
claim for fixed k n and fixed γ γk no 
revenue ranking of rbb and rbr is possible for an arbitrary 
density f 
example assume there are bidders slots and that 
γ γ assume that value-relevance pairs are 
uniformly distributed over × for such a 
distribution with a closed-form formula it is most convenient to use 
the revenue formulae just derived rbb dominates rbr in 
terms of revenue for these parameters the formula for the 
expected revenue in a rbb auction yields whereas for 
rbr auctions we have 
assume instead that with probability an agent s 
valuerelevance pair is and that with probability it 
is in this scenario it is more convenient to appeal 
to formulae and in a truthful auction the second 
agent will always pay according to in a truthful 
rbb auction the first agent makes an expected payment of 
e 
ˆ 
 γ − γ aσ xσ 
˜ 
 
 
 
e 
ˆ 
aσ 
˜ 
e 
ˆ 
xσ 
˜ 
where we have used the fact that value and relevance are 
independently distributed for different agents the expected 
relevance of the agent with the highest value is e 
ˆ 
aσ 
˜ 
 
 the expected second highest value is also e 
ˆ 
xσ 
˜ 
 
 the expected revenue for a rbb auction here is then 
 according to in a truthful rbr auction the 
first agent makes an expected payment of 
e 
ˆ 
 γ − γ rσ 
˜ 
 
 
 
e 
ˆ 
rσ 
˜ 
in expectation the second highest revenue is e 
ˆ 
rσ 
˜ 
 
 so the expected revenue for a rbr auction is 
hence in this case the rbr auction yields higher expected 
revenue 
this example suggests the following conjecture when 
value and relevance are either uncorrelated or positively 
correlated rbb dominates rbr in terms of revenue when 
value and relevance are negatively correlated rbr 
dominates 
 complete information 
in typical slot auctions such as those run by yahoo and 
google bidders can adjust their bids up or down at any 
time as b¨orgers et al and edelman et al have 
noted this can be viewed as a continuous-time process in 
which bidders learn each other s bids if the process 
stabilizes the result can then be modeled as a nash equilibrium 
in pure strategies of the static one-shot game of complete 
information since each bidder will be playing a best-response 
to the others bids 
this argument seems especially 
appropriate for yahoo s slot auction design where all bids are 
 
to be entirely rigorous and consistent with our initial 
assumptions we should have constructed a continuous 
probability density with full support over an appropriate domain 
taking the domain to be e g × and a continuous 
density with full support that is sufficiently concentrated 
around and with roughly equal mass around 
both would yield the same conclusion 
 
claim should serve as a word of caution because feng 
et al find through their simulations that with a 
bivariate normal distribution over value-relevance pairs and with 
 slots bidders and δ rbr dominates rbb in 
terms of revenue for any level of correlation between value 
and relevance however they assume that bidding behavior 
in a second-price slot auction can be well approximated by 
truthful bidding 
 
we do not claim that bidders will actually learn each 
others private information value and relevance just that for 
a stable set of bids there is a corresponding equilibrium of 
the complete information game 
 
made public google keeps bids private but 
experimentation can allow one to discover other bids especially since 
second pricing automatically reveals to an agent the bid of 
the agent ranked directly below him 
 equilibrium analysis 
in this section we ask whether a pure-strategy nash 
equilibrium exists in a rbb or rbr slot auction with either 
first or second pricing 
before dealing with the first-price case there is a technical 
issue involving ties in our model we allow bids to be 
nonnegative real numbers for mathematical convenience but 
this can become problematic because there is then no bid 
that is just higher than another we brush over such 
issues by assuming that an agent can bid infinitesimally 
higher than another this is imprecise but allows us to 
focus on the intuition behind the result that follows see 
reny for a full treatment of such issues 
for the remainder of the paper we assume that there are 
as many slots as bidders the following result shows that 
there can be no pure-strategy nash equilibrium with first 
pricing 
note that the argument holds for both rbb and 
rbr allocation rules for rbb bids should be interpreted 
as declared values and for rbr as declared revenues 
theorem there exists no complete information nash 
equilibrium in pure strategies in the first-price slot auction 
for any possible values of the agents whether using a rbb 
or rbr allocation rule 
proof let σ k → n be the allocation of slots to 
the agents resulting from their bids let ri and bi be the 
revenue and bid of the agent ranked ith 
 respectively note 
that we cannot have bi bi or else the agent in slot 
i can make a profitable deviation by instead bidding bi − 
 bi for small enough this does not change 
its allocation but increases its profit hence we must have 
bi bi i e with one bidder bidding infinitesimally higher 
than the other since this holds for any two consecutive 
bidders it follows that in a nash equilibrium all bidders 
must be bidding since the bidder ranked last matches the 
bid directly below him which is by default because there 
is no such bid but this is impossible consider the bidder 
ranked last the identity of this bidder is always clear given 
the deterministic tie-breaking rule this bidder can obtain 
the top spot and increase his revenue by γ −γk rk by 
bidding some and for small enough this is necessarily 
a profitable deviation hence there is no nash equilibrium 
in pure strategies 
on the other hand we find that in a second-price slot 
auction there can be a multitude of pure strategy nash 
equilibria the next two lemmas give conditions that characterize 
the allocations that can occur as a result of an equilibrium 
profile of bids given fixed agent values and revenues then 
if we can exhibit an allocation that satisfies these conditions 
there must exist at least one equilibrium we first consider 
the rbr case 
 
b¨orgers et al have proven this result in a model with 
three bidders and three slots and we generalize their 
argument edelman et al also point out this non-existence 
phenomenon they only illustrate the fact with an example 
because the result is quite immediate 
lemma given an allocation σ there exists a nash 
equilibrium profile of bids b leading to σ in a second-price rbr 
slot auction if and only if 
„ 
 − 
γi 
γj 
 
rσ i ≤ rσ j 
for ≤ j ≤ n − and i ≥ j 
proof there exists a desired vector b which constitutes 
a nash equilibrium if and only if the following set of 
inequalities can be satisfied the variables are the πi and bj 
πi ≥ γj rσ i − bj ∀i ∀j i 
πi ≥ γj rσ i − bj ∀i ∀j i 
πi γi rσ i − bi ∀i 
bi ≥ bi ≤ i ≤ n − 
πi ≥ bi ≥ ∀i 
here rσ i is the revenue of the agent allocated slot i and 
πi and bi may be interpreted as this agent s surplus and 
declared revenue respectively we first argue that 
constraints can be removed because the inequalities above 
can be satisfied if and only if the inequalities without can 
be satisfied the necessary direction is immediate assume 
we have a vector π b which satisfies all inequalities above 
except then there is some i for which bi bi 
construct a new vector π b identical to the original except 
with bi bi we now have bi bi an agent in slot 
k i sees the price of slot i decrease from bi to bi bi 
but this does not make i more preferred than k to this agent 
because we have πk ≥ γi− rσ k − bi ≥ γi rσ k − bi 
γi rσ k −bi i e because the agent in slot k did not 
originally prefer slot i − at price bi he will not prefer slot i 
at price bi a similar argument applies for agents in slots 
k i the agent in slot i sees the price of this slot 
go down which only makes it more preferred finally the 
agent in slot i sees no change in the price of any slot so 
his slot remains most preferred hence inequalities - 
remain valid at π b we first make this change to the bi 
where bi bi and index i is smallest we then recursively 
apply the change until we eventually obtain a vector that 
satisfies all inequalities 
we safely ignore inequalities from now on by the 
farkas lemma the remaining inequalities can be satisfied if 
and only if there is no vector z such that 
x 
i j 
 γj rσ i zσ i j 
x 
i j 
γjzσ i j 
x 
i j 
γj− zσ i j− ≤ ∀j 
x 
j 
zσ i j ≤ ∀i 
zσ i j ≥ ∀i ∀j i 
zσ i i free ∀i 
note that a variable of the form zσ i i appears at most once 
in a constraint of type so such a variable can never be 
positive also zσ i for all i by constraint 
since such variables never appear with another of the form 
zσ i i 
now if we wish to raise zσ i j above by one unit for j i 
we must lower zσ i i by one unit because of the constraint 
of type because γjrσ i ≤ γirσ i for i j raising 
 
zσ i j with i j while adjusting other variables to maintain 
feasibility cannot make the objective 
p 
i j γjrσ i zσ i j 
positive if this objective is positive then this is due to some 
component zσ i j with i j being positive 
now for the constraints of type if i j then zσ i j 
appears with zσ j− j− for j n so to raise the 
former variable γ− 
j units and maintain feasibility we must 
 i lower zσ i i by γ− 
j units and ii lower zσ j− j− by 
γ− 
j− units hence if the following inequalities hold 
rσ i ≤ 
„ 
γi 
γj 
 
rσ i rσ j− 
for ≤ j ≤ n − and i j raising some zσ i j with 
i j cannot make the objective positive and there is no 
z that satisfies all inequalities above conversely if some 
inequality does not hold the objective can be made 
positive by raising the corresponding zσ i j and adjusting other 
variables so that feasibility is just maintained by a slight 
reindexing inequalities yield the statement of the 
theorem 
the rbb case is entirely analogous 
lemma given an allocation σ there exists a nash 
equilibrium profile of bids b leading to σ in a second-price rbb 
slot auction if and only if 
„ 
 − 
γi 
γj 
 
xσ i ≤ xσ j 
for ≤ j ≤ n − and i ≥ j 
proof sketch the proof technique is the same as in the 
previous lemma the desired nash equilibrium exists if and 
only if a related set of inequalities can be satisfied by the 
farkas lemma this occurs if and only if an alternate set of 
inequalities cannot be satisfied the conditions that 
determine whether the latter holds are given in the statement of 
the lemma 
the two lemmas above immediately lead to the following 
result 
theorem there always exists a complete information 
nash equilibrium in pure strategies in the second-price rbb 
slot auction there always exists an efficient complete 
information nash equilibrium in pure strategies in the 
secondprice rbr slot auction 
proof first consider rbb suppose agents are ranked 
according to their true values since xσ i ≤ xσ j for i j 
the system of inequalities in lemma is satisfied and the 
allocation is the result of some nash equilibrium bid profile 
by the same type of argument but appealing to lemma 
for rbr there exists a nash equilibrium bid profile such 
that bidders are ranked according to their true revenues 
by theorem this latter allocation is efficient 
this theorem establishes existence but not uniqueness 
indeed we expect that in many cases there will be multiple 
allocations and hence equilibria which satisfy the 
conditions of lemmas and in particular not all equilibria of 
a second-price rbr auction will be efficient for instance 
according to lemma with two agents and two slots any 
allocation can arise in a rbr equilibrium because no 
constraints apply 
theorems and taken together provide a possible 
explanation for yahoo s switch from first to second pricing 
we saw in section that this does not induce truthfulness 
from bidders with first pricing there will always be some 
bidder that feels compelled to adjust his bid second pricing 
is more convenient because an equilibrium can be reached 
and this reduces the cost of bid management 
 efficiency 
for a given allocation rule we call the allocation that 
would result if the bidders reported their values truthfully 
the standard allocation hence in the standard rbb 
allocation bidders are ranked by true values and in the standard 
rbr allocation they are ranked by true revenues 
according to lemmas and a ranking that results from a nash 
equilibrium profile can only deviate from the standard 
allocation by having agents with relatively similar values or 
revenues switch places that is if ri rj then with rbr 
agent j can be ranked higher than i only if the ratio rj ri is 
sufficiently large similarly for rbb this suggests that the 
value of an equilibrium allocation cannot differ too much 
from the value obtained in the standard allocation and the 
following theorems confirms this 
for an allocation σ of slots to agents we denote its 
total value by f σ 
pn 
i γirσ i we denote by g σ 
pn 
i γixσ i allocation σ s value when assuming all agents 
have identical relevance normalized to let 
l min 
i n− 
min 
 
γi 
γi 
 − 
γi 
γi 
ff 
 where by default γn let ηx and ηr be the standard 
allocations when using rbb and rbr respectively 
theorem for an allocation σ that results from a 
purestrategy nash equilibrium of a second-price rbr slot 
auction we have f σ ≥ lf ηr 
proof we number the agents so that agent i has the 
ith 
highest revenue so r ≥ r ≥ ≥ rn hence the 
standard allocation has value f ηr 
pn 
i γiri to prove 
the theorem we will make repeated use of the fact thatp 
k akp 
k bk 
≥ mink 
ak 
bk 
when the ak and bk are positive note 
that according to lemma if agent i lies at least two slots 
below slot j then rσ j ≥ ri 
 
 − 
γj 
γj 
 
 
it may be the case that for some slot i we have σ i i 
and for slots k i we have σ k i we then say that 
slot i is inverted let s be the set of agents with indices at 
least i there are n − i of these if slot i is inverted it is 
occupied by some agent from s also all slots strictly lower 
than i must be occupied by the remaining agents from 
s since σ k i for k ≥ i the agent in slot i must 
then have an index σ i ≤ i note this means slot i 
cannot be inverted now there are two cases in the first 
case we have σ i i then 
γirσ i γi rσ i 
γiri γi ri 
≥ 
γi ri γiri 
γiri γi ri 
≥ min 
 
γi 
γi 
 
γi 
γi 
ff 
 
γi 
γi 
in the second case we have σ i i then since all agents 
in s except the one in slot i lie strictly below slot i and 
 
the agent in slot i is not agent i it must be that agent 
i is in a slot strictly below slot i this means that it is 
at least two slots below the agent that actually occupies slot 
i and by lemma we then have rσ i ≥ ri 
 
 − 
γi 
γi 
 
 
thus 
γirσ i γi rσ i 
γiri γi ri 
≥ 
γi ri γirσ i 
γiri γi ri 
≥ min 
 
γi 
γi 
 − 
γi 
γi 
ff 
if slot i is not inverted then on one hand we may have 
σ i ≤ i in which case rσ i ri ≥ on the other hand we 
may have σ i i but there is some agent with index j ≤ i 
that lies at least two slots below slot i then by lemma 
rσ i ≥ rj 
 
 − 
γi 
γi 
 
≥ ri 
 
 − 
γi 
γi 
 
 
we write i ∈ i if slot i is inverted and i ∈ i if neither i nor 
i − are inverted by our arguments above two consecutive 
slots cannot be inverted so we can write 
f σ 
f γr 
 
p 
i∈i 
` 
γirσ i γi rσ i 
´ 
 
p 
i∈i γirσ i 
p 
i∈i γiri γi ri 
p 
i∈i γiri 
≥ min 
 
min 
i∈i 
 
γirσ i γi rσ i 
γiri γi ri 
ff 
 min 
i∈i 
 
γirσ i 
γiri 
ffff 
≥ l 
and this completes the proof 
note that for rbr the standard value is also the 
efficient value by theorem also note that for an exponential 
decay model l min 
˘ 
δ 
 − 
δ 
¯ 
 with δ see 
section the factor is l ≈ so the total value in a 
pure-strategy nash equilibrium of a second-price rbr slot 
auction is always within a factor of of the efficient value 
with such a discount 
again for rbb we have an analogous result 
theorem for an allocation σ that results from a 
purestrategy nash equilibrium of a second-price rbb slot 
auction we have g σ ≥ lg ηx 
proof sketch simply substitute bidder values for bidder 
revenues in the proof of theorem and appeal to lemma 
 conclusions 
this paper analyzed stylized versions of the slot auction 
designs currently used by yahoo and google namely rank 
by bid rbb and rank by revenue rbr respectively 
we also considered first and second pricing rules together 
with each of these allocation rules since both have been used 
historically we first studied the short-run setting with 
incomplete information corresponding to the case where 
agents have just approached the mechanism our 
equilibrium analysis revealed that rbb has much weaker 
informational requirements than rbr because bidders need not 
know any information about relevance even their own to 
play the bayes-nash equilibrium however rbr leads to 
an efficient allocation in equilibrium whereas rbb does not 
we showed that for an arbitrary distribution over value and 
relevance no revenue ranking of rbb and rbr is possible 
we hope that the tools we used to establish these results 
 revenue equivalence the form of first-price equilibria the 
truthful payments rules will help others wanting to pursue 
further analyses of slot auctions 
we also studied the long-run case where agents have 
experimented with their bids and each settled on one they 
find optimal we argued that a stable set of bids in this 
setting can be modeled as a pure-strategy nash equilibrium 
of the static game of complete information we showed that 
no pure-strategy equilibrium exists with either rbb or rbr 
using first pricing but that with second pricing there always 
exists such an equilibrium in the case of rbr an efficient 
equilibrium in general second pricing allows for multiple 
pure-strategy equilibria but we showed that the value of 
such equilibria diverges by only a constant factor from the 
value obtained if all agents bid truthfully which in the case 
of rbr is the efficient value 
 future work 
introducing budget constraints into the model is a 
natural next step for future work the complication here lies 
in the fact that budgets are often set for entire campaigns 
rather than single keywords assuming that the optimal 
choice of budget can be made independent of the choice 
of bid for a specific keyword it can be shown that it is a 
dominant-strategy to report this optimal budget with one s 
bid the problem is then to ascertain that bids and budgets 
can indeed be optimized separately or to find a plausible 
model where deriving equilibrium bids and budgets together 
is tractable 
identifying a condition on the distribution over value and 
relevance that actually does yield a revenue ranking of rbb 
and rbr such as correlation between value and relevance 
perhaps would yield a more satisfactory characterization 
of their relative revenue properties placing bounds on the 
revenue obtained in a complete information equilibrium is 
also a relevant question 
because the incomplete information case is such a close 
generalization of the most basic single-item auction model 
it would be interesting to see which standard results from 
single-item auction theory e g results with risk-averse 
bidders an endogenous number of bidders asymmetries etc 
automatically generalize and which do not to fully 
understand the structural differences between single-item and slot 
auctions 
acknowledgements 
david pennock provided valuable guidance throughout this 
project i would also like to thank david parkes for helpful 
comments 
 references 
 z abrams revenue maximization when bidders have 
budgets in proc the acm-siam symposium on 
discrete algorithms 
 t b¨orgers i cox and m pesendorfer personal 
communication 
 c borgs j chayes n immorlica m mahdian and 
a saberi multi-unit auctions with 
budget-constrained bidders in proc the sixth acm 
conference on electronic commerce vancouver bc 
 
 f brandt and g weiß antisocial agents and vickrey 
auctions in j -j c meyer and m tambe editors 
 
intelligent agents viii volume of lecture notes 
in artificial intelligence springer verlag 
 b edelman and m ostrovsky strategic bidder 
behavior in sponsored search auctions in workshop 
on sponsored search auctions acm electronic 
commerce 
 b edelman m ostrovsky and m schwarz internet 
advertising and the generalized second price auction 
selling billions of dollars worth of keywords nber 
working paper november 
 j feng h k bhargava and d m pennock 
implementing sponsored search in web search engines 
computational evaluation of alternative mechanisms 
informs journal on computing forthcoming 
 j green and j -j laffont characterization of 
satisfactory mechanisms for the revelation of 
preferences for public goods econometrica 
 - 
 b holmstrom groves schemes on restricted domains 
econometrica - 
 b kitts p laxminarayan b leblanc and 
r meech a formal analysis of search auctions 
including predictions on click fraud and bidding 
tactics in workshop on sponsored search auctions 
acm electronic commerce 
 v krishna auction theory academic press 
 d liu and j chen designing online auctions with 
past performance information decision support 
systems forthcoming 
 c meek d m chickering and d b wilson 
stochastic and contingent payment auctions in 
workshop on sponsored search auctions acm 
electronic commerce 
 a mehta a saberi u vazirani and v vazirani 
adwords and generalized on-line matching in proc 
 th ieee symposium on foundations of computer 
science 
 p milgrom putting auction theory to work 
cambridge university press 
 p milgrom and c shannon monotone comparative 
statics econometrica - 
 p j reny on the existence of pure and mixed 
strategy nash equilibria in discontinuous games 
econometrica - 
 h r varian position auctions working paper 
february 
 w vickrey counterspeculation auctions and 
competitive sealed tenders journal of finance 
 - 
 
hidden-action in multi-hop routing 
michal feldman 
mfeldman sims berkeley edu 
john chuang 
chuang sims berkeley edu 
ion stoica 
istoica cs berkeley edu 
scott shenker 
shenker icir org 
 
school of information 
management and systems 
u c berkeley 
 
computer science division 
u c berkeley 
abstract 
in multi-hop networks the actions taken by individual intermediate nodes 
are typically hidden from the communicating endpoints all the endpoints 
can observe is whether or not the end-to-end transmission was successful 
therefore in the absence of incentives to the contrary rational i e 
selfish intermediate nodes may choose to forward packets at a low priority 
or simply not forward packets at all using a principal-agent model we 
show how the hidden-action problem can be overcome through 
appropriate design of contracts in both the direct the endpoints contract with each 
individual router and recursive each router contracts with the next 
downstream router cases we further demonstrate that per-hop monitoring does 
not necessarily improve the utility of the principal or the social welfare in 
the system in addition we generalize existing mechanisms that deal with 
hidden-information to handle scenarios involving both hidden-information 
and hidden-action 
categories and subject descriptors 
c computer-communication networks distributed 
systems j social and behavioral sciences economics 
general terms 
design economics 
 introduction 
endpoints wishing to communicate over a multi-hop network 
rely on intermediate nodes to forward packets from the sender to 
the receiver in settings where the intermediate nodes are 
independent agents such as individual nodes in ad-hoc and 
peer-topeer networks or autonomous systems on the internet this poses 
an incentive problem the intermediate nodes may incur significant 
communication and computation costs in the forwarding of packets 
without deriving any direct benefit from doing so consequently a 
rational i e utility maximizing intermediate node may choose to 
forward packets at a low priority or not forward the packets at all 
this rational behavior may lead to suboptimal system performance 
the endpoints can provide incentives e g in the form of 
payments to encourage the intermediate nodes to forward their 
packets however the actions of the intermediate nodes are often hidden 
from the endpoints in many cases the endpoints can only observe 
whether or not the packet has reached the destination and cannot 
attribute failure to a specific node on the path even if some form 
of monitoring mechanism allows them to pinpoint the location of 
the failure they may still be unable to attribute the cause of failure 
to either the deliberate action of the intermediate node or to some 
external factors beyond the control of the intermediate node such 
as network congestion channel interference or data corruption 
the problem of hidden action is hardly unique to networks also 
known as moral hazard this problem has long been of interest in 
the economics literature concerning information asymmetry 
incentive and contract theory and agency theory we follow this 
literature by formalizing the problem as a principal-agent model where 
multiple agents making sequential hidden actions 
our results are threefold first we show that it is possible to 
design contracts to induce cooperation when intermediate nodes 
can choose to forward or drop packets as well as when the nodes 
can choose to forward packets with different levels of quality of 
service if the path and transit costs are known prior to 
transmission the principal achieves first best solution and can implement 
the contracts either directly with each intermediate node or 
recursively through the network each node making a contract with the 
following node without any loss in utility second we find that 
introducing per-hop monitoring has no impact on the principal s 
expected utility in equilibrium for a principal who wishes to induce 
an equilibrium in which all intermediate nodes cooperate its 
expected total payment is the same with or without monitoring 
however monitoring provides a dominant strategy equilibrium which 
is a stronger solution concept than the nash equilibrium achievable 
in the absence of monitoring third we show that in the absence 
of a priori information about transit costs on the packet forwarding 
path it is possible to generalize existing mechanisms to overcome 
scenarios that involve both hidden-information and hidden-action 
in these scenarios the principal pays a premium compared to 
scenarios with known transit costs 
 baseline model 
we consider a principal-agent model where the principal is a 
pair of communication endpoints who wish to communicate over 
a multi-hop network and the agents are the intermediate nodes 
capable of forwarding packets between the endpoints the 
principal who in practice can be either the sender the receiver or 
 
both makes individual take-it-or-leave-it offers contracts to the 
agents if the contracts are accepted the agents choose their 
actions sequentially to maximize their expected payoffs based on the 
payment schedule of the contract when necessary agents can in 
turn make subsequent take-it-or-leave-it offers to their downstream 
agents 
we assume that all participants are risk neutral and that standard 
assumptions about the global observability of the final outcome and 
the enforceability of payments by guaranteeing parties hold 
for simplicity we assume that each agent has only two possible 
actions one involving significant effort and one involving little 
effort we denote the action choice of agent i by ai ∈ { } where 
ai and ai stand for the low-effort and high-effort actions 
respectively each action is associated with a cost to the agent 
c ai and we assume 
c ai c ai 
at this stage we assume that all nodes have the same c ai for 
presentation clarity but we relax this assumption later without loss 
of generality we normalize the c ai to be zero and denote 
the high-effort cost by c so c ai and c ai c 
the utility of agent i denoted by ui is a function of the payment 
it receives from the principal si the action it takes ai and the 
cost it incurs ci as follows 
ui si ci ai si − aici 
the outcome is denoted by x ∈ {xg 
 xb 
} where xg 
stands 
for the good outcome in which the packet reaches the 
destination and xb 
stands for the bad outcome in which the packet 
is dropped before it reaches the destination the outcome is a 
function of the vector of actions taken by the agents on the path 
a a an ∈ { }n 
 and the loss rate on the channels k 
the benefit of the sender from the outcome is denoted by w x 
where 
w xg 
 wg 
 and w xb 
 wb 
 
the utility of the sender is consequently 
u x s w x − s 
where s 
pn 
i si 
a sender who wishes to induce an equilibrium in which all nodes 
engage in the high-effort action needs to satisfy two constraints for 
each agent i 
 ir individual rationality participation constraint 
 the 
expected utility from participation should weakly exceed its 
reservation utility which we normalize to 
 ic incentive compatibility the expected utility from exerting 
high-effort should weakly exceed its expected utility from 
exerting low-effort 
in some network scenarios the topology and costs are common 
knowledge that is the sender knows in advance the path that its 
packet will take and the costs on that path in other routing 
scenarios the sender does not have this a priori information we show 
that our model can be applied to both scenarios with known and 
unknown topologies and costs and highlight the implications of each 
scenario in the context of contracts we also distinguish between 
direct contracts where the principal signs an individual contract 
 we use the notion of ex ante individual rationality in which the agents 
choose to participate before they know the state of the system 
s dn 
source destination 
n intermediate nodes 
figure multi-hop path from sender to destination 
figure structure of the multihop routing game under known 
topology and transit costs 
with each node and recursive contracts where each node enters a 
contractual relationship with its downstream node 
the remainder of this paper is organized as follows in section 
we consider agents who decide whether to drop or forward 
packets with and without monitoring when the transit costs are common 
knowledge in section we extend the model to scenarios with 
unknown transit costs in section we distinguish between recursive 
and direct contracts and discuss their relationship in section we 
show that the model applies to scenarios in which agents choose 
between different levels of quality of service we consider internet 
routing as a case study in section in section we present related 
work and section concludes the paper 
 known transit costs 
in this section we analyze scenarios in which the principal knows 
in advance the nodes on the path to the destination and their costs 
as shown in figure we consider agents who decide whether to 
drop or forward packets and distinguish between scenarios with 
and without monitoring 
 drop versus forward without monitoring 
in this scenario the agents decide whether to drop a or 
forward a packets the principal uses no monitoring to 
observe per-hop outcomes consequently the principal makes the 
payment schedule to each agent contingent on the final outcome x 
as follows 
si x sb 
i sg 
i 
where 
sb 
i si x xb 
 
sg 
i si x xg 
 
the timeline of this scenario is shown in figure given a 
perhop loss rate of k we can express the probability that a packet is 
successfully delivered from node i to its successor i as 
pr xg 
i→i ai − k ai 
where xg 
i→j denotes a successful transmission from node i to j 
proposition under the optimal contract that induces 
high-effort behavior from all intermediate nodes in the nash 
equi 
librium 
 the expected payment to each node is the same as its 
expected cost with the following payment schedule 
sb 
i si x xb 
 
sg 
i si x xg 
 
c 
 − k n−i 
 
proof the principal needs to satisfy the ic and ir 
constraints for each agent i which can be expressed as follows 
 ic pr xg 
 aj≥i sg 
i pr xb 
 aj≥i sb 
i − c ≥ 
pr xg 
 ai aj i sg 
i pr xb 
 ai aj i sb 
i 
 
this constraint says that the expected utility from forwarding is 
greater than or equal to its expected utility from dropping if all 
subsequent nodes forward as well 
 ir pr xg 
s→i aj i pr xg 
 aj≥i sg 
i 
pr xb 
 aj≥i sb 
i − c pr xb 
s→i aj i sb 
i ≥ 
 
this constraint says that the expected utility from participating is 
greater than or equal to zero reservation utility if all other nodes 
forward 
the above constraints can be expressed as follows based on 
eq 
 ic − k n−i 
sg 
i − − k n−i 
 sb 
i − c ≥ sb 
i 
 ir −k i 
 −k n−i 
sg 
i − −k n−i 
 sb 
i −c 
 − − k i 
 sb 
i ≥ 
it is a standard result that both constraints bind at the optimal 
contract see solving the two equations we obtain the 
solution that is presented in eqs and 
we next prove that the expected payment to a node equals its 
expected cost in equilibrium the expected cost of node i is its 
transit cost multiplied by the probability that it faces this cost i e 
the probability that the packet reaches node i which is − k i 
c 
the expected payment that node i receives is 
pr xg 
 sg 
i pr xb 
 sb 
i − k n c 
 − k n−i 
 − k i 
c 
note that the expected payment to a node decreases as the node 
gets closer to the destination due to the asymmetric distribution 
of risk the closer the node is to the destination the lower the 
probability that a packet will fail to reach the destination resulting 
in the low payment being made to the node 
the expected payment by the principal is 
e s − k n 
nx 
i 
sg 
i − − k n 
 
nx 
i 
sb 
i 
 − k n 
nx 
i 
ci 
 − k n−i 
 
the expected payment made by the principal depends not only 
on the total cost but also the number of nodes on the path 
proposition given two paths with respective lengths of 
n and n hops per-hop transit costs of c and c and per-hop 
loss rates of k and k such that 
 since transit nodes perform actions sequentially this is really a 
subgameperfect equilibrium spe but we will refer to it as nash equilibrium in the 
remainder of the paper 
figure two paths of equal total costs but different lengths and 
individual costs 
 c n c n equal total cost 
 − k n 
 − k n 
 equal expected benefit 
 n n path is shorter than path 
the expected total payment made by the principal is lower on the 
shorter path 
proof the expected payment in path j is 
e s j 
nj 
x 
i 
cj − kj i 
 cj − kj 
 − − kj nj 
kj 
so we have to show that 
c − k 
 − − k n 
k 
 c − k 
 − − k n 
k 
let m c n c n and n − k n 
 − k n 
 
we have to show that 
mn 
 
n − n 
n 
n 
n − n 
 
n 
 
mn 
 
n − n 
n 
n 
n − n 
 
n 
 
let 
f 
n 
 
n − n 
n 
n 
n − n 
 
n 
then it is enough to show that f is monotonically increasing in n 
∂f 
∂n 
 
g n n 
h n n 
where 
g n n − ln n n − n 
 n 
 
n 
− n 
n 
n − n 
 n n 
 
n 
and 
h n n n 
n 
 − n 
 
n 
but h n n ∀n n therefore it is enough to show that 
g n n because n ∈ i ln n and ii 
n 
 
n n 
n 
n therefore g n n ∀n n 
this means that ceteris paribus shorter paths should always be 
preferred over longer ones 
for example consider the two topologies presented in figure 
while the paths are of equal total cost the total expected payment 
by the principal is different based on eqs and the expected 
total payment for the top path is 
e s pr xg 
 sg 
a sg 
b 
 
„ 
c 
 − k 
 
c 
 − k 
 
 − k 
 
 
while the expect total payment for the bottom path is 
e s pr xg 
 sg 
a sg 
b sg 
c 
 
c 
 − k 
 
c 
 − k 
 
c 
 − k 
 − k 
for n c k n c k 
we have equal total cost and equal expected benefit but e s 
 and e s 
 drop versus forward with monitoring 
suppose the principal obtains per-hop monitoring information 
per-hop information broadens the set of mechanisms the principal 
can use for example the principal can make the payment schedule 
contingent on arrival to the next hop instead of arrival to the final 
destination can such information be of use to a principal wishing 
to induce an equilibrium in which all intermediate nodes forward 
the packet 
proposition in the drop versus forward model the 
principal derives the same expected utility whether it obtains per-hop 
monitoring information or not 
proof the proof to this proposition is already implied in the 
findings of the previous section we found that in the absence of 
per-hop information the expected cost of each intermediate node 
equals its expected payment in order to satisfy the ir constraint it 
is essential to pay each intermediate node an expected amount of at 
least its expected cost otherwise the node would be better-off not 
participating therefore no other payment scheme can reduce the 
expected payment from the principal to the intermediate nodes in 
addition if all nodes are incentivized to forward packets the 
probability that the packet reaches the destination is the same in both 
scenarios thus the expected benefit of the principal is the same 
indeed we have found that even in the absence of per-hop monitoring 
information the principal achieves first-best solution 
to convince the reader that this is indeed the case we provide an 
example of a mechanism that conditions payments on arrival to the 
next hop this is possible only if per-hop monitoring information 
is provided in the new mechanism the principal makes the 
payment schedule contingent on whether the packet has reached the 
next hop or not that is the payment to node i is sg 
i if the packet 
has reached node i and sb 
i otherwise we assume costless 
monitoring giving us the best case scenario for the use of 
monitoring as before we consider a principal who wishes to induce an 
equilibrium in which all intermediate nodes forward the packet 
the expected utility of the principal is the difference between its 
expected benefit and its expected payment because the expected 
benefit when all nodes forward is the same under both scenarios 
we only need to show that the expected total payment is identical as 
well under the monitoring mechanism the principal has to satisfy 
the following constraints 
 ic pr xg 
i→i ai sg 
 pr xb 
i→i ai sb 
− c ≥ 
pr xg 
i→i ai sg 
 pr xb 
i→i ai sb 
 
 ir pr xg 
s→i aj i pr xg 
i→i ai sg 
 pr xb 
i→i ai sb 
− c ≥ 
 
 for a recent proposal of an accountability framework that provides such 
monitoring information see 
these constraints can be expressed as follows 
 ic − k sg 
 ksb 
− c ≥ s 
 ir − k i 
 − k sg 
 ksb 
− c ≥ 
the two constraints bind at the optimal contract as before and 
we get the following payment schedule 
sb 
 
sg 
 
c 
 − k 
the expected total payment under this scenario is 
e s 
nx 
i 
 − k i 
 sb 
 i − sg 
 − k n 
nsg 
 − k n 
nx 
i 
ci 
 − k n−i 
as in the scenario without monitoring see equation 
while the expected total payment is the same with or without 
monitoring there are some differences between the two scenarios 
first the payment structure is different if no per-hop monitoring 
is used the payment to each node depends on its location i in 
contrast monitoring provides us with n identical contracts 
second the solution concept used is different if no monitoring 
is used the strategy profile of ai ∀i is a nash equilibrium 
which means that no agent has an incentive to deviate unilaterally 
from the strategy profile in contrast with the use of monitoring 
the action chosen by node i is independent of the other agents 
forwarding behavior therefore monitoring provides us with 
dominant strategy equilibrium which is a stronger solution concept than 
nash equilibrium discuss the appropriateness of 
different solution concepts in the context of online environments 
 unknown transit costs 
in certain network settings the transit costs of nodes along the 
forwarding path may not be common knowledge i e there exists 
the problem of hidden information in this section we address the 
following questions 
 is it possible to design contracts that induce cooperative 
behavior in the presence of both hidden-action and 
hiddeninformation 
 what is the principal s loss due to the lack of knowledge of 
the transit costs 
in hidden-information problems the principal employs 
mechanisms to induce truthful revelation of private information from the 
agents in the routing game the principal wishes to extract transit 
cost information from the network routers in order to determine the 
lowest cost path lcp for a given source-destination pair the 
network routers act strategically and declare transit costs to maximize 
their profit mechanisms that have been proposed in the literature 
for the routing game assume that once the transit costs 
have been obtained and the lcp has been determined the nodes 
on the lcp obediently forward all packets and that there is no loss 
in the network i e k in this section we consider both hidden 
information and hidden action and generalize these mechanisms 
to induce both truth revelation and high-effort action in 
equilibrium where nodes transmit over a lossy communication channel 
i e k ≥ 
 v cg mechanism 
in their seminal paper nisan and ronen present a vcg 
mechanism that induces truthful revelation of transit costs by edges 
 
figure game structure for f p ss where only hidden-information 
is considered 
figure game structure for f p ss where both 
hiddeninformation and hidden-action are considered 
in a biconnected network such that lowest cost paths can be 
chosen like all vcg mechanisms it is a strategyproof mechanism 
meaning that it induces truthful revelation in a dominant strategy 
equilibrium in fpss feigenbaum et al slightly modify 
the model to have the routers as the selfish agents instead of the 
edges and present a distributed algorithm that computes the vcg 
payments the timeline of the fpss game is presented in 
figure under fpss transit nodes keep track of the amount of 
traffic routed through them via counters and payments are 
periodically transferred from the principals to the transit nodes based on 
the counter values fpss assumes that transit nodes are 
obedient in packet forwarding behavior and will not update the counters 
without exerting high effort in packet forwarding 
in this section we present fpss which generalizes fpss to 
operate in an environment with lossy communication channels i e 
k ≥ and strategic behavior in terms of packet forwarding we 
will show that fpss induces an equilibrium in which all nodes 
truthfully reveal their transit costs and forward packets if they are 
on the lcp figure presents the timeline of fpss in the first 
stage the sender declares two payment functions sg 
i sb 
i that 
will be paid upon success or failure of packet delivery given these 
payments nodes have incentive to reveal their costs truthfully and 
later to forward packets payments are transferred based on the 
final outcome 
in fpss each node i submits a bid bi which is its reported 
transit cost node i is said to be truthful if bi ci we write b for 
the vector b bn of bids submitted by all transit nodes let 
ii b be the indicator function for the lcp given the bid vector b 
such that 
ii b 
 
 if i is on the lcp 
 otherwise 
following fpss the payment received by node i at 
equilibrium is 
pi biii b 
x 
r 
ir b i 
∞ br − 
x 
r 
ir b br 
 
x 
r 
ir b i 
∞ br − 
x 
r i 
ir b br 
 
where the expression b i 
x means that b i 
x j cj for all j i 
and b i 
x i x 
in fpss we compute sb 
i and sg 
i as a function of pi k and 
n first we recognize that sb 
i must be less than or equal to zero 
in order for the true lcp to be chosen otherwise strategic nodes 
may have an incentive to report extremely small costs to mislead 
the principal into believing that they are on the lcp then these 
nodes can drop any packets they receive incur zero transit cost 
collect a payment of sb 
i and earn positive profit 
proposition let the payments of fpss be 
sb 
i 
sg 
i 
pi 
 − k n−i 
then fpss has a nash equilibrium in which all nodes truthfully 
reveal their transit costs and all nodes on the lcp forward packets 
proof in order to prove the proposition above we have to 
show that nodes have no incentive to engage in the following 
misbehaviors 
 truthfully reveal cost but drop packet 
 lie about cost and forward packet 
 lie about cost and drop packet 
if all nodes truthfully reveal their costs and forward packets the 
expected utility of node i on the lcp is 
e u i pr xg 
s→i e si − ci pr xb 
s→i sb 
i 
 − k i 
 
 − k n−i 
sg 
i − − k n−i 
 sb 
i − ci 
 
 − − k i 
 sb 
i 
 − k i 
 − k n−i pi 
 − k n−i 
− − k i 
ci 
 − k i 
 pi − ci 
≥ 
 
the last inequality is derived from the fact that fpss is a truthful 
mechanism thus pi ≥ ci the expected utility of a node not on the 
lcp is 
a node that drops a packet receives sb 
i which is smaller 
than or equal to e u i for i ∈ lcp and equals e u i for i ∈ lcp 
therefore nodes cannot gain utility from misbehaviors or 
we next show that nodes cannot gain utility from misbehavior 
 if i ∈ lcp e u i 
 a if it reports bi ci 
i if bi 
p 
r ir b i 
∞ br − 
p 
r i ir b br it is still 
on the lcp and since the payment is independent 
of bi its utility does not change 
ii if bi 
p 
r ir b i 
∞ br − 
p 
r i ir b br it will 
not be on the lcp and obtain e u i which is 
less than its expected utility if truthfully revealing 
its cost 
 
 b if it reports bi ci it is still on the lcp and since 
the payment is independent of bi its utility does not 
change 
 if i ∈ lcp e u i 
 a if it reports bi ci it remains out of the lcp so its 
utility does not change 
 b if it reports bi ci 
i if bi 
p 
r ir b i 
∞ br − 
p 
r i ir b br it joins 
the lcp and gains an expected utility of 
e u i − k i 
 pi − ct 
however if i ∈ lcp it means that 
ci 
x 
r 
ir c i 
∞ cr − 
x 
r i 
ir c cr 
but if all nodes truthfully reveal their costs 
pi 
x 
r 
ir c i 
∞ cr − 
x 
r i 
ir c cr ci 
therefore e u i 
ii if bi 
p 
r ir b i 
∞ br − 
p 
r i ir b br it 
remains out of the lcp so its utility does not change 
therefore there exists an equilibrium in which all nodes truthfully 
reveal their transit costs and forward the received packets 
we note that in the hidden information only context fpss 
induces truthful revelation as a dominant strategy equilibrium in 
the current setting with both hidden information and hidden action 
fpss achieves a nash equilibrium in the absence of per-hop 
monitoring and a dominant strategy equilibrium in the presence of 
per-hop monitoring consistent with the results in section where 
there is hidden action only in particular with per-hop monitoring 
the principal declares the payments sb 
i and sg 
i to each node upon 
failure or success of delivery to the next node given the payments 
sb 
i and sg 
i pi − k it is a dominant strategy for the 
nodes to reveal costs truthfully and forward packets 
 discussion 
more generally for any mechanism m that induces a bid vector b 
in equilibrium by making a payment of pi b to node i on the lcp 
there exists a mechanism m that induces an equilibrium with the 
same bid vector and packet forwarding by making a payment of 
sb 
i 
sg 
i 
pi b 
 − k n−i 
 
a sketch of the proof would be as follows 
 im 
i b im 
i b ∀i since m uses the same choice metric 
 the expected utility of a lcp node is e u i − 
k i 
 pi b − ci ≥ if it forwards and if it drops and 
the expected utility of a non-lcp node is 
 from and we get that if a node i can increase its expected 
utility by deviating from bi under m it can also increase its 
utility by deviating from bi in m but this is in contradiction 
to bi being an equilibrium in m 
 nodes have no incentive to drop packets since they derive an 
expected utility of if they do 
in addition to the generalization of fpss into fpss we 
can also consider the generalization of the first-price auction fpa 
mechanism where the principal determines the lcp and pays each 
node on the lcp its bid pi b bi first-price auctions achieve 
nash equilibrium as opposed to dominant strategy equilibrium 
therefore we should expect the generalization of fpa to achieve 
nash equilibrium with or without monitoring 
we make two additional comments concerning this class of 
mechanisms first we find that the expected total payment made 
by the principal under the proposed mechanisms is 
e s 
nx 
i 
 − k i 
pi b 
and the expected benefit realized by the principal is 
e w − k n 
wg 
where 
pn 
i pi and wg 
are the expected payment and expected 
benefit respectively when only the hidden-information problem is 
considered when hidden action is also taken into consideration 
the generalized mechanism handles strategic forwarding behavior 
by conditioning payments upon the final outcome and accounts for 
lossy communication channels by designing payments that reflect 
the distribution of risk the difference between expected payment 
and benefit is not due to strategic forwarding behavior but to lossy 
communications therefore in a lossless network we should not 
see any gap between expected benefits and payments independent 
of strategic or non-strategic forwarding behavior 
second the loss to the principal due to unknown transit costs is 
also known as the price of frugality and is an active field of 
research this price greatly depends on the network topology 
and on the mechanism employed while it is simple to characterize 
the principal s loss in some special cases it is not a trivial problem 
in general for example in topologies with parallel disjoint paths 
from source to destination we can prove that under first-price 
auctions the loss to the principal is the difference between the cost of 
the shortest path and the second-shortest path and the loss is higher 
under the fpss mechanism 
 recursive contracts 
in this section we distinguish between direct and recursive 
contracts in direct contracts the principal contracts directly with each 
node on the path and pays it directly in recursive payment the 
principal contracts with the first node on the path which in turn 
contracts with the second and so on such that each node contracts 
with its downstream node and makes the payment based on the final 
result as demonstrated in figure 
with direct payments the principal needs to know the identity 
and cost of each node on the path and to have some communication 
channel with the node with recursive payments every node needs 
to communicate only with its downstream node several questions 
arise in this context 
 what knowledge should the principal have in order to induce 
cooperative behavior through recursive contracts 
 what should be the structure of recursive contracts that 
induce cooperative behavior 
 what is the relation between the total expected payment 
under direct and recursive contracts 
 is it possible to design recursive contracts in scenarios of 
unknown transit costs 
 
figure structure of the multihop routing game under known 
topology and recursive contracts 
in order to answer the questions outlined above we look at the 
ir and ic constraints that the principal needs to satisfy when 
contracting with the first node on the path when the principal designs 
a contract with the first node he should take into account the 
incentives that the first node should provide to the second node and 
so on all the way to the destination 
for example consider the topology given in figure a when 
the principal comes to design a contract with node a he needs to 
consider the subsequent contract that a should sign with b which 
should satisfy the following constrints 
 ir pr xg 
a→b aa e s ab − c 
pr xb 
a→b aa sb 
a→b ≥ 
 ic e s ab − c ≥ e s ab 
where 
e s ab pr xg 
b→d ab sg 
a→b 
 pr xb 
b→d ab sb 
a→b 
and 
e s ab pr xg 
b→d ab sg 
a→b 
 pr xb 
b→d ab sb 
a→b 
these binding constraints yield the values of sb 
a→b and sg 
a→b 
sb 
a→b 
sg 
a→b c − k 
based on these values s can express the constraints it should 
satisfy in a contract with a 
 ir pr xg 
s→a as e ss→a − sa→b ai ∀i − c 
 pr xb 
s→a as sb 
s→a ≥ 
 ic e ss→a − sa→b ai ∀i − c 
≥ e ss→a − sa→b aa ab 
where 
e ss→a − sa→b ai ∀i 
pr xg 
a→d ai ∀i sg 
s→a − sg 
a→b 
 pr xb 
a→d ai ∀i sb 
s→a − sb 
a→b 
and 
e ss→a − sa→b aa ab 
pr xg 
a→d aa ab sg 
s→a − sg 
a→b 
 pr xb 
a→d aa ab sb 
s→a − sb 
a→b 
solving for sb 
s→a and sg 
s→a we get 
sb 
s→a 
sg 
s→a 
c − k 
 − k k 
the expected total payment is 
e s sg 
s→apr xg 
s→d sb 
s→apr xb 
s→d 
 c − k − k 
 
which is equal to the expected total payment under direct contracts 
 see eq 
proposition the expected total payments by the 
principal under direct and recursive contracts are equal 
proof in order to calculate the expected total payment we 
have to find the payment to the first node on the path that will 
induce appropriate behavior because sb 
i in the drop forward 
model both constraints can be reduced to 
pr xg 
i→r aj ∀j sg 
i − sg 
i − ci 
⇔ − k n−i 
 sg 
i − sg 
i − ci 
which yields for all ≤ i ≤ n 
sg 
i 
ci 
 − k n−i 
 sg 
i 
thus 
sg 
n 
cn 
 − k 
sg 
n− 
cn− 
 − k 
 sg 
n 
cn− 
 − k 
 
cn 
 − k 
· · · 
sg 
 
c 
 − k n 
 sg 
 
nx 
i 
ci 
 − k i 
and the expected total payment is 
e s − k n 
sg 
 − k n 
nx 
i 
ci 
 − k n−i 
which equals the total expected payment in direct payments as 
expressed in eq 
because the payment is contingent on the final outcome and the 
expected payment to a node equals its expected cost nodes have 
no incentive to offer their downstream nodes lower payment than 
necessary since if they do their downstream nodes will not forward 
the packet 
what information should the principal posess in order to 
implement recursive contracts like in direct payments the expected 
payment is not affected solely by the total payment on the path but 
also by the topology therefore while the principal only needs to 
communicate with the first node on the forwarding path and does 
not have to know the identities of the other nodes it still needs to 
know the number of nodes on the path and their individual transit 
costs 
finally is it possible to design recursive contracts under 
unknown transit costs and if so what should be the structure of such 
contracts suppose the principal has implemented the distributed 
algorithm that calculates the necessary payments pi for truthful 
 
revelation would the following payment schedule to the first node 
induce cooperative behavior 
sb 
 
sg 
 
nx 
i 
pi 
 − k i 
the answer is not clear unlike contracts in known transit costs 
the expected payment to a node usually exceeds its expected cost 
therefore transit nodes may not have the appropriate incentive to 
follow the principal s guarantee during the payment phase for 
example in fpss the principal guarantees to pay each node an 
expected payment of pi ci we assume that payments are 
enforceable if made by the same entity that pledge to pay however in the 
case of recursive contracts the entity that pledges to pay in the cost 
discovery stage the principal is not the same as the entity that 
defines and executes the payments in the forwarding stage the transit 
nodes transit nodes who design the contracts in the second stage 
know that their downstream nodes will forward the packet as long 
as the expected payment exceeds the expected cost even if it is less 
than the promised amount thus every node has incentive to offer 
lower payments than promised and keep the profit transit nodes 
who know this is a plausible scenario may no longer truthfully 
reveal their cost therefore while recursive contracts under known 
transit costs are strategically equivalent to direct contracts it is not 
clear whether this is the case under unknown transit costs 
 high-quality versus 
low-quality forwarding 
so far we have considered the agents strategy space to be 
limited to the drop a and forward a actions in this 
section we consider a variation of the model where the agents choose 
between providing a low-quality service a and a high-quality 
service a 
this may correspond to a service-differentiated service model 
where packets are forwarded on a best-effort or a priority basis 
in contrast to drop versus forward a packet may still reach the next 
hop albeit with a lower probability even if the low-effort action is 
taken 
as a second example consider the practice of hot-potato routing 
in inter-domain routing of today s internet individual autonomous 
systems as s can either adopt hot-potato routing or early exit 
routing a where a packet is handed off to the downstream 
as at the first possible exit or late exit routing a where an 
as carries the packet longer than it needs to handing off the packet 
at an exit closer to the destination in the absence of explicit 
incentives it is not surprising that as s choose hot-potato routing to 
minimize their costs even though it leads to suboptimal routes 
 
in both examples in the absence of contracts a rational node 
would exert low-effort resulting in lower performance 
nevertheless this behavior can be avoided with an appropriate design of 
contracts 
formally the probability that a packet successfully gets from 
node i to node i is 
pr xg 
i→i ai − k − qai 
where q ∈ and k ∈ q 
in the drop versus forward model a low-effort action by any node 
results in a delivery failure in contrast a node in the high low 
scenario may exert low-effort and hope to free-ride on the 
higheffort level exerted by the other agents 
proposition in the high-quality versus low-quality 
forwarding model where transit costs are common knowledge the 
principal derives the same expected utility whether it obtains 
perhop monitoring information or not 
proof the ic and ir constraints are the same as specified 
in the proof of proposition but their values change based on 
eq to reflect the different model 
 ic −k q n−i 
sg 
i − −k q n−i 
 sb 
i −c ≥ 
 − k − k q n−i 
sg 
i − − k − k q n−i 
 sb 
i 
 ir − k q i 
 − k q n−i 
sg 
i 
 − − k q n−i 
 sb 
i − c − − k q i 
 sb 
i ≥ 
for this set of constraints we obtain the following solution 
sb 
i 
 − k q i 
c k − 
q 
 
sg 
i 
 − k q i 
c k − − k q −n 
 
q 
 
we observe that in this version both the high and the low payments 
depend on i if monitoring is used we obtain the following 
constraints 
 ic − k q sg 
i k − q sb 
i − c ≥ − k sg 
i k sb 
i 
 ir − k q i 
 − k q sg 
i k − q sb 
i − c ≥ 
and we get the solution 
sb 
i 
c k − 
q 
sg 
i 
ck 
q 
the expected payment by the principal with or without forwarding 
is the same and equals 
e s 
c − k q − − k q n 
 
k − q 
 
and this concludes the proof 
the payment structure in the high-quality versus low-quality 
forwarding model is different from that in the drop versus forward 
model in particular at the optimal contract the low-outcome 
payment sb 
i is now less than zero a negative payment means that 
the agent must pay the principal in the event that the packet fails 
to reach the destination in some settings it may be necessary to 
impose a limited liability constraint i e si ≥ this prevents the 
first-best solution from being achieved 
proposition in the high-quality versus low-quality 
forwarding model if negative payments are disallowed the expected 
payment to each node exceeds its expected cost under the optimal 
contract 
proof the proof is a direct outcome of the following 
statements which are proved above 
 the optimal contract is the contract specified in equations 
and 
 under the optimal contract e si equals node i s expected 
cost 
 under the optimal contract sb 
i −k q i 
c k− 
q 
 
therefore under any other contract the sender will have to 
compensate each node with an expected payment that is higher than its 
expected transit cost 
 
there is an additional difference between the two models in 
drop versus forward a principal either signs a contract with all n 
nodes along the path or with none this is because a single node 
dropping the packet determines a failure in contrast in high versus 
low-quality forwarding a success may occur under the low effort 
actions as well and payments are used to increase the probability 
of success therefore it may be possible for the principal to 
maximize its utility by contracting with only m of the n nodes along the 
path while the expected outcome depends on m it is independent 
of which specific m nodes are induced at the same time the 
individual expected payments decrease in i see eq therefore 
a principal who wishes to sign a contract with only m out of the n 
nodes should do so with the nodes that are closest to the destination 
namely nodes n − m n − n 
solving for the high-quality versus low-quality forwarding 
model with unknown transit costs is left for future work 
 case study internet routing 
we can map different deployed and proposed internet routing 
schemes to the various models we have considered in this work 
border gateway protocol bgp the current inter-domain 
routing protocol in the internet computes routes based on path vectors 
since the protocol reveals only the autonomous systems as s 
along a route but not the cost associated to them the current bgp 
routing is best characterized by lack of a priori information about 
transit costs in this case the principal e g a multi-homed site 
or a tier- as can implement one of the mechanisms proposed in 
section by contracting with individual nodes on the path such 
contracts involve paying some premium over the real cost and it 
is not clear whether recursive contacts can be implemented in this 
scenario in addition the current protocol does not have the 
infrastructure to support implementation of direct contracts between 
endpoints and the network 
recently several new architectures have been proposed in the 
context of the internet to provide the principal not only with a set 
of paths from which it can chose like bgp does but also with 
the performance along those paths and the network topology one 
approach to obtain such information is through end-to-end 
probing another approach is to have the edge networks perform 
measurements and discover the network topology yet another 
approach is to delegate the task of obtaining topology and 
performance information to a third-party like in the routing-as-a-service 
proposal these proposals are quite different in nature but 
they are common in their attempt to provide more visibility and 
transparency into the network if information about topology and 
transit costs is obtained the scenario is mapped to the known 
transit costs model section in this case first-best contracts can be 
achieved through individual contracts with nodes along the path 
however as we have shown in section as long as each agent can 
chose the next hop the principal can gain full benefit by 
contracting with only the first hop through the implementation of recursive 
contracts 
however the various proposals for acquiring network topology 
and performance information do not deal with strategic behavior 
by the intermediate nodes with the realization that the 
information collected may be used by the principal in subsequent 
contractual relationships the intermediate nodes may behave strategically 
misrepresenting their true costs to the entities that collect and 
aggregate such information one recent approach that can alleviate 
this problem is to provide packet obituaries by having each packet 
to confirm its delivery or report its last successful as hop 
another approach is to have third parties like keynote independently 
monitor the network performance 
 related work 
the study of non-cooperative behavior in communication 
networks and the design of incentives has received significant 
attention in the context of wireless ad-hoc routing considers the 
problem of malicious behavior where nodes respond positively to 
route requests but then fail to forward the actual packets it 
proposes to mitigate it by detection and report mechanisms that will 
essentially help to route around the malicious nodes however 
rather than penalizing nodes that do not forward traffic it bypasses 
the misbehaving nodes thereby relieving their burden therefore 
such a mechanism is not effective against selfish behavior 
in order to mitigate selfish behavior some approaches 
require reputation exchange between nodes or simply first-hand 
observations other approaches propose payment schemes 
 to encourage cooperation is the closest to our work in 
that it designs payment schemes in which the sender pays the 
intermediate nodes in order to prevent several types of selfish behavior 
in their approach nodes are supposed to send receipts to a 
thirdparty entity we show that this type of per-hop monitoring may not 
be needed 
in the context of internet routing proposes an accountability 
framework that provide end hosts and service providers 
after-thefact audits on the fate of their packets this proposal is part of a 
broader approach to provide end hosts with greater control over the 
path of their packets if senders have transit cost 
information and can fully control the path of their packets they can design 
contracts that yield them with first-best utility the accountability 
framework proposed in can serve two main goals informing 
nodes of network conditions to help them make informed decision 
and helping entities to establish whether individual ass have 
performed their duties adequately while such a framework can be 
used for the first task we propose a different approach to the 
second problem without the need of per-hop auditing information 
research in distributed algorithmic mechanism design damd 
has been applied to bgp routing these works propose 
mechanisms to tackle the hidden-information problem but ignore 
the problem of forwarding enforcement inducing desired behavior 
is also the objective in which attempts to respond to the 
challenge of distributed amd raised in if the same agents that 
seek to manipulate the system also run the mechanism what 
prevents them from deviating from the mechanism s proposed rules to 
maximize their own welfare they start with the proposed 
mechanism presented in and use mostly auditing mechanisms to 
prevent deviation from the algorithm 
the focus of this work is the design of a payment scheme that 
provides the appropriate incentives within the context of multi-hop 
routing like other works in this field we assume that all the 
accounting services are done using out-of-band mechanisms 
security issues within this context such as node authentication or 
message encryption are orthogonal to the problem presented in this 
paper and can be found for example in 
the problem of information asymmetry and hidden-action also 
known as moral hazard is well studied in the economics 
literature identifies the problem of moral hazard in 
production teams and shows that it is impossible to design a 
sharing rule which is efficient and budget-balanced shows that this 
task is made possible when production takes place sequentially 
 conclusions and future 
directions 
in this paper we show that in a multi-hop routing setting where 
the actions of the intermediate nodes are hidden from the source 
 
and or destination it is possible to design payment schemes to 
induce cooperative behavior from the intermediate nodes we 
conclude that monitoring per-hop outcomes may not improve the 
utility of the participants or the network performace in addition in 
scenarios of unknown transit costs it is also possible to design 
mechanisms that induce cooperative behavior in equilibrium but 
the sender pays a premium for extracting information from the 
transit nodes 
our model and results suggest several natural and intriguing 
research avenues 
 consider manipulative or collusive behaviors which may 
arise under the proposed payment schemes 
 analyze the feasibility of recursive contracts under 
hiddeninformation of transit costs 
 while the proposed payment schemes sustain cooperation in 
equilibrium it is not a unique equilibrium we plan to study 
under what mechanisms this strategy profile may emerge as 
a unique equilibrium e g penalty by successor nodes 
 consider the effect of congestion and capacity constraints on 
the proposed mechanisms our preliminary results show that 
when several senders compete for a single transit node s 
capacity the sender with the highest demand pays a premium 
even if transit costs are common knowledge the premium 
can be expressed as a function of the second-highest demand 
in addition if congestion affects the probability of 
successful delivery a sender with a lower cost alternate path may 
end up with a lower utility level than his rival with a higher 
cost alternate path 
 fully characterize the full-information nash equilibrium in 
first price auctions and use this characterization to derive its 
overcharging compared to truthful mechaisms 
 acknowledgements 
we thank hal varian for his useful comments this work is 
supported in part by the national science foundation under itr 
awards ani- and ani- and career award 
ani 
 references 
 andersen d g balakrishnan h kaashoek m f and 
morris r resilient overlay networks in th acm sosp 
 archer a and tardos e frugal path mechanisms 
 argyraki k and cheriton d loose source routing as a 
mechanism for traffic policies in proceedings of sigcomm fdna 
 august 
 argyraki k maniatis p cheriton d and shenker s 
providing packet obituaries in third workshop on hot topics in 
networks hotnets november 
 bansal s and baker m observation-based cooperation 
enforcement in ad-hoc networks technical report stanford 
university 
 blake s black d carlson m davies e wang z 
and weiss w an architecture for differentiated service 
rfc 
 buchegger s and boudec j -y l performance analysis of 
the confidant protocol cooperation of nodes - fairness in 
dynamic ad-hoc networks in ieee acm symposium on mobile ad 
hoc networking and computing mobihoc 
 buchegger s and boudec j -y l coping with false 
accusations in misbehavior reputation systems for mobile ad-hoc 
networks in epfl technical report 
 buchegger s and boudec j -y l the effect of rumor 
spreading in reputation systems for mobile ad-hoc networks in 
wiopt modeling and optimization in mobile ad-hoc and 
wireless networks 
 buttyan l and hubaux j stimulating cooperation in 
self-organizing mobile ad-hoc networks acm kluwer journal on 
mobile networks and applications monet 
 caillaud b and hermalin b hidden action and incentives 
teaching notes u c berkeley 
 elkind e sahai a and steiglitz k frugality in path 
auctions 
 feigenbaum j papadimitriou c sami r and shenker 
s a bgp-based mechanism for lowest-cost routing in 
proceedings of the acm symposium on principles of distributed 
computing 
 feigenbaum j sami r and shenker s mechanism design 
for policy routing in yale university technical report 
 feigenbaum j and shenker s distributed algorithmic 
mechanism design recent results and future directions in 
proceedings of the international workshop on discrete algorithms 
and methods for mobile computing and communications 
 friedman e and shenker s learning and implementation on 
the internet in manuscript new brunswick rutgers university 
department of economics 
 holmstrom b moral hazard in teams bell journal of 
economics - 
 hu y perrig a and johnson d ariadne a secure 
on-demand routing protocol for ad-hoc networks in eighth 
annual international conference on mobile computing and 
networking mobicom pp - 
 hu y perrig a and johnson d sead secure efficient 
distance vector routing for mobile ad-hoc networks in th ieee 
workshop on mobile computing systems and applications wmcsa 
 
 jakobsson m hubaux j -p and buttyan l a 
micro-payment scheme encouraging collaboration in multi-hop 
cellular networks in financial cryptography 
 lakshminarayanan k stoica i and shenker s 
routing as a service in ucb technical report no 
ucb csd- - january 
 marti s giuli t j lai k and baker m mitigating 
routing misbehavior in mobile ad-hoc networks in proceedings of 
mobicom pp - 
 mass-colell a whinston m and green j 
microeconomic theory oxford university press 
 nisan n and ronen a algorithmic mechanism design in 
proceedings of the st symposium on theory of computing 
 sanzgiri k dahill b levine b shields c and 
belding-royer e a secure routing protocol for ad-hoc 
networks in international conference on network protocols icnp 
 
 shneidman j and parkes d c overcoming rational 
manipulation in mechanism implementation 
 strausz r moral hazard in sequential teams departmental 
working paper free university of berlin 
 teixeira r griffin t shaikh a and voelker g 
network sensitivity to hot-potato disruptions in proceedings of acm 
sigcomm september 
 teixeira r shaikh a griffin t and rexford 
j dynamics of hot-potato routing in ip networks in proceedings of 
acm sigmetrics june 
 yang x nira a new internet routing architecture in 
proceedings of sigcomm fdna august 
 zhong s chen j and yang y r sprite a simple 
cheat-proof credit-based system for mobile ad-hoc networks in 
 nd annual joint conference of the ieee computer and 
communications societies 
 zhu d gritter m and cheriton d feedback-based 
routing in proc hotnets-i 
 
competitive algorithms for vwap 
and limit order trading 
sham m kakade 
computer and information science 
university of pennsylvania 
kakade linc cis upenn edu 
michael kearns 
computer and information science 
university of pennsylvania 
mkearns cis upenn edu 
yishay mansour 
computer science 
tel aviv university 
mansour post tau ac il 
luis e ortiz 
computer and information science 
university of pennsylvania 
leortiz linc cis upenn edu 
abstract 
we introduce new online models for two important aspects 
of modern financial markets volume weighted average price 
trading and limit order books we provide an extensive 
study of competitive algorithms in these models and relate 
them to earlier online algorithms for stock trading 
categories and subject descriptors 
f analysis of algorithms and problem 
complexity miscellaneous j social and behavioral sciences 
economics 
general terms 
algorithms economics 
 introduction 
while popular images of wall street often depict 
swashbuckling traders boldly making large gambles on just their 
market intuitions the vast majority of trading is actually 
considerably more technical and constrained the constraints 
often derive from a complex combination of business 
regulatory and institutional issues and result in certain kinds 
of standard trading strategies or criteria that invite 
algorithmic analysis 
one of the most common activities in modern financial 
markets is known as volume weighted average price or 
vwap trading informally the vwap of a stock over a 
specified market period is simply the average price paid per 
share during that period so the price of each transaction in 
the market is weighted by its volume in vwap trading 
one attempts to buy or sell a fixed number of shares at a 
price that closely tracks the vwap 
very large institutional trades constitute one of the main 
motivations behind vwap activity a typical scenario goes 
as follows suppose a very large mutual fund holds of 
the outstanding shares of a large publicly traded company 
- a huge fraction of the shares - and that this fund s 
manager decides he would like to reduce this holding to over 
a -month period such a decision might be forced by the 
fund s own regulations or other considerations typically 
such a fund manager would be unqualified to sell such a large 
number of shares in the open market - it requires a 
professional broker to intelligently break the trade up over time 
and possibly over multiple exchanges in order to minimize 
the market impact of such a sizable transaction thus the 
fund manager would approach brokerages for help in selling 
the 
the brokerage will typically alleviate the fund manager s 
problem immediately by simply buying the shares directly 
from the fund manager and then selling them off 
laterbut what price should the brokerage pay the fund manager 
paying the price on the day of the sale is too risky for the 
brokerage as they need to sell the shares themselves over an 
extended period and events beyond their control such as 
wars could cause the price to fall dramatically the usual 
answer is that the brokerage offers to buy the shares from 
the fund manager at a per-share price tied to the vwap 
over some future period - in our example the brokerage 
might offer to buy the at a per-share price of the 
coming month s vwap minus cent the brokerage now has a 
very clean challenge by selling the shares themselves over 
the next month in a way that exactly matches the vwap 
a penny per share is earned in profits if they can beat the 
vwap by a penny they make two cents per share such 
small-margin high-volume profits can be extremely 
lucrative for a large brokerage the importance of the vwap 
has led to many automated vwap trading algorithms - 
indeed every major brokerage has at least one vwap box 
 
price volume model order book model macroscopic distribution model 
owt θ log r from o log r log n e pbins 
maxprice 
 e pbins 
maxprice for -approx of pbins 
maxprice 
θ log q same as above plus 
vwap θ log r o log r log n from above e pbins 
vol 
ω q fixed schedule o log q for large n e pbins 
vol for -approx of pbins 
vol 
 for volume in n qn 
figure the table summarizes the results presented in this paper the rows represent results for either the owt 
or vwap criterion the columns represent which model we are working in the entry in the table is the competitive 
ratio between our algorithm and an optimal algorithm and the closer the ratio is to the better the parameter r 
represents a bound on the maximum to the minimum price fluctuation and the parameter q represents a bound on the 
maximum to minimum volume fluctuation in the respective model see section for a description of the macroscopic 
distribution model all the results for the owt trading criterion which is a stronger criterion directly translate 
to the vwap criterion however in the vwap setting considering a restriction on the maximum to the minimum 
volume fluctuation q leads to an additional class of results which depends on q 
and some small companies focus exclusively on proprietary 
vwap trading technology 
in this paper we provide the first study of vwap trading 
algorithms in an online competitive ratio setting we first 
formalize the vwap trading problem in a basic online model 
we call the price-volume model which can be viewed as a 
generalization of previous theoretical online trading models 
incorporating market volume information in this model we 
provide vwap algorithms and competitive ratios and 
compare this setting with the one-way trading owt problem 
studied in 
our most interesting results however examine the vwap 
trading problem in a new online trading model capturing the 
important recent phenomenon of limit order books in 
financial markets briefly a limit buy or sell order specifies both 
the number of shares and the desired price and will only 
be executed if there is a matching party on the opposing 
side according to a well-defined matching procedure used 
by all the major exchanges while limit order books the 
list of limit orders awaiting possible future execution have 
existed since the dawn of equity exchanges only very 
recently have these books become visible to traders in real 
time thus opening the way to trading algorithms of all 
varieties that attempt to exploit this rich market microstructure 
data such data and algorithms are a topic of great current 
interest on wall street 
we thus introduce a new online trading model 
incorporating limit order books and examine both the one-way and 
vwap trading problems in it our results are summarized 
in figure see the caption for a summary 
 theprice-volumetradingmodel 
we now present a trading model which includes both price 
and volume information about the sequence of trades while 
this model is a generalization of previous formalisms for 
online trading it makes an infinite liquidity assumption which 
fails to model the negative market impact that trading a 
large number of shares typically has this will be addressed 
in the order book model studied in the next section 
a note on terminology throughout the paper unless 
otherwise specified we shall use the term market to describe 
all activity or orders other than those of the algorithm 
under consideration the setting we consider can be viewed as 
a game between our algorithm and the market 
 the model 
in the price-volume trading model we assume that the 
intraday trading activity in a given stock is summarized by 
a discrete sequence of price and volume pairs pt vt for 
t t here t corresponds to the day s 
market open and t t to the close while there is nothing 
technically special about the time horizon of a single day it 
is particularly consistent with limit order book trading on 
wall street the pair pt vt represents the fact that a total 
of vt shares were traded at an average price per share pt 
in the market between time t − and t realistically we 
should imagine the number of intervals t being reasonably 
large so that it is sensible to assign a common approximate 
price to all shares traded within an interval 
in the price-volume model we shall make an infinite 
liquidity assumption for our trading algorithms more 
precisely in this online model we see the price-volume sequence 
one pair at a time following the observation of pt vt 
we are permitted to sell any possibly fractional number 
of shares nt at the price pt let us assume that our goal 
is to sell n shares over the course of the day hence at 
each time we must select a possibly fractional number of 
shares nt to sell at price pt subject to the global constraint 
t 
t nt n it is thus assumed that if we have left over 
shares to sell after time t − we are forced to sell them at 
the closing price of the market - that is nt n − t − 
t nt 
is sold at pt in this way we are certain to sell exactly n 
shares over the course of the day the only thing an 
algorithm must do is determine the schedule of selling based on 
the incoming market price-volume stream 
any algorithm which sells fractional volumes can be 
converted to a randomized algorithm which only sells integral 
volumes with the same expected number of shares sold if 
we keep the hard constraint of selling exactly n shares we 
might incur an additional slight loss in the conversion note 
that we only allow fractional volumes in the price-volume 
model where liquidity is not an issue in the order book 
model to follow we do not allow fractional volumes 
in vwap trading the goal of an online algorithm a which 
sells exactly n shares is not to maximize profits per se but 
to track the market vwap the market vwap for an 
intraday trading sequence s p v pt vt is simply the 
average price paid per share over the course of the trading 
 
day ie 
vwapm s 
t 
t 
ptvt v 
where v is the total daily volume i e v t 
t vt if on 
the sequence s the algorithm a sells its n stocks using the 
volume sequence n nt then we analogously define the 
vwap of a on market sequence s by 
vwapa s 
t 
t 
ptnt n 
note that the market vwap does not include the shares 
that the algorithm sells 
the vwap competitive ratio of a with respect to a set 
of sequences σ is then 
rvwap a max 
s∈σ 
{vwapm s vwapa s } 
in the case that a is randomized we generalize the definition 
above by taking an expectation over vwapa s inside the 
max we note that unlike on wall street our definition of 
vwapm does not take our own trading into account it is 
easy to see that this makes it a more challenging criterion 
to track 
in contrast to the vwap another common measure of 
the performance of an online selling algorithm would be its 
one-way trading owt competitive ratio with respect 
to a set of sequences σ 
rowt a max 
s∈σ 
max 
 ≤t≤t 
{pt vwapa s } 
where the algorithms performance is compared to the largest 
individual price appearing in the sequence s 
in both vwap and owt we are comparing the average 
price per share received by a selling algorithm to some 
measure of market performance in the case of owt we 
compare to the rather ambitious benchmark of the high price of 
the day ignoring volumes entirely in vwap trading we 
have the more modest goal of comparing favorably to the 
overall market average of the day as we shall see there are 
some important commonalities and differences to these two 
approaches for now we note one simple fact on any specific 
sequence s vwapa s may be larger that vwapm s 
however rvwap a cannot be smaller than since on any 
sequence s in which all price pt are identical it is impossible 
to get a better average share per price thus for all 
algorithms a both rvwap a and rowt a are larger than 
 and the closer to they are the better a is tracking its 
respective performance measure 
 vwap results in the price-volume model 
as in previous work on online trading it is generally not 
possible to obtain finite bounds on competitive ratios with 
absolutely no assumptions on the set of sequences 
σbounds on the maximum variation in price or volume are 
required depending on the exact setting we thus introduce 
the following two assumptions 
 volume variability assumption 
let vmin ≤ vmax be known positive constants and 
define q vmax vmin for all intraday trading sequences 
s ∈ σ the total daily volume v ∈ vmin vmax 
 price variability assumption 
let pmin ≤ pmax be known positive constants and 
define r pmax pmin for all intraday trading sequences s ∈ 
σ the prices satisfy pt ∈ pmin pmax for all t t 
competitive ratios are generally taken over all sets σ 
consistent with at least one of these assumptions to gain some 
intuition consider the two trivial cases of r and q 
in the case of r where there is no fluctuation in price 
any schedule is optimal in the case of q where the 
total volume v over the trading period is known we can 
gain a competitive ratio of by selling vt 
v 
n shares after each 
time period 
for the owt problem in the price-volume model 
volumes are irrelevant for the performance criterion but for 
the vwap criterion they are central for the owt problem 
under the price variability assumption the results of 
established that the optimal competitive ratio was θ log r 
our first result establishes that the optimal competitive 
ratio for vwap under the volume variability assumption is 
θ log q and is achieved by an algorithm that ignores the 
price data 
theorem in the price-volume model under the volume 
variability assumption there exists an online algorithm a 
for selling n shares achieving competitive ratio rvwap a ≤ 
 log q in addition if only the volume variability and not 
the price variability assumption holds any online algorithm 
a for selling n shares has rvwap a ω log q 
proof sketch for the upper bound the idea is 
similar to the price reservation algorithm of for the owt 
problem and similar in spirit to the general technique of 
classify and select consider algorithms which use a 
parameter ˆv which is interpreted as an estimate for the total 
volume for the day then at each time t if the market 
price and volume is pt vt the algorithm sells a fraction 
vt ˆv of its shares we consider a family of log q such 
algorithms where algorithm ai uses ˆv vmin i− 
 clearly 
one of the ai has a competitive ratio of we can derive an 
o log q vwap competitive ratio by running these 
algorithms in parallel and letting each algorithm sell n log q 
shares alternatively we can randomly select one ai and 
guarantee the same expected competitive ratio 
we now sketch the proof of the lower bound which 
relates performance in the vwap and owt problems any 
algorithm that is c-competitive in the vwap setting 
 under fixed q is c-competitive in the owt setting with 
r q to show this we take any sequence s of prices 
for the owt problem and convert it into a price-volume 
sequence for the vwap problem the prices in the vwap 
sequence are the same as in s to construct the volumes in the 
vwap sequence we segment the prices in s into log r 
intervals i− 
pmin i 
pmin suppose pt ∈ i− 
pmin i 
pmin 
and this is the first time in s that a price has fallen in this 
interval then in the vwap sequence we set the volume 
vt i− 
 if this is not the first visit to the interval 
containing pt we set vt assume that the maximum price 
in s is pmax the vwap of our sequence is at least pmax 
since we had a c competitive algorithm its average sell is at 
least pmax c the lower bound now follows using the lower 
bound in 
an alternative approach to vwap is to ignore the 
volumes in favor of prices and apply an algorithm for the owt 
problem note that the lower bound in this theorem unlike 
in the previous one only assumes a price variation bound 
 
theorem in the price-volume model under the price 
variability assumption there exists an online algorithm a 
for selling n shares achieving competitive ratio rvwap a 
o log r in addition if only the price variability and not 
the volume variability assumption holds any online a for 
selling n shares has rvwap a ω log r 
proof sketch follows immediately from the results of 
 for owt the upper bound from the simple fact that for 
any sequence s vwapa s is less than max ≤t≤t {pt} and 
the lower bound from a reduction to owt 
theorems and demonstrate that one can achieve 
logarithmic vwap competitive ratios under the assumption of 
either bounded variability of total volume or bounded 
variability of maximum price if both assumptions hold it is 
possible to give an algorithm accomplishing the minimum 
of log q and log r this flexibility of approach derives 
from the fact that the vwap is a quantity in which both 
prices and volumes matter as opposed to owt 
 relatedresultsintheprice-volumemodel 
all of the vwap algorithms we have discussed so far 
make some use of the daily data pt vt as it unfolds 
using either the price or volume information in contrast a 
fixed schedule vwap algorithm has a predetermined 
distribution {f f ft } and simply sells ftn shares at time t 
independent of pt vt fixed schedule vwap algorithms 
or slight variants of them are surprisingly common on wall 
street and the schedule is usually derived from historical 
intraday volume data our next result demonstrates that 
such algorithms can perform considerably worse than 
dynamically adaptive algorithms in terms of the worst case 
competitive ratio 
theorem in the price-volume model under both the 
volume and price variability assumptions any fixed schedule 
vwap algorithm a for selling n shares has sell vwap 
competitive ratio rvwap a ω min t r 
the proofs of all the results in this subsection are in the 
appendix 
so far our emphasis has been on vwap algorithms that 
must sell exactly n shares in many realistic circumstances 
however there is actually some flexibility in the precise 
number of shares to be sold for instance this is true at 
large brokerages where many separate vwap trades may 
be pooled and executed by a common algorithm and the 
firm would be quite willing to carry a small position of 
unsold shares overnight if it resulted in better execution prices 
the following theorem which interestingly has no analogue 
for the owt problem demonstrates that this trade-off in 
shares sold and performance can be realized dramatically in 
our model it states that if we are willing to let the number 
of shares sold vary with q we can in fact achieve a vwap 
competitive ratio of 
theorem in the price-volume model under the volume 
variability assumption there exists an algorithm a that 
always sells between n and qn shares and that the average 
price per sold share is exactly vwapm s 
in many online problems there is a clear distinction 
between benefit problems and cost problems in the 
vwap setting selling shares is a benefit problem and 
buying shares is a cost problem the definitions of the 
competitive ratios rbuy 
vwap a and rbuy 
owt a for algorithms which 
figure sample island order books for msft 
buy exactly n shares are maxs∈σ{vwapa s vwapm s } 
and maxs∈σ maxt{vwapa s pt} respectively eventhough 
theorem also holds for buying in general the competitive 
ratio of the buy cost problem is much higher as stated in 
the following theorem 
theorem in the price-volume model under the volume 
and price variability assumptions there exists an online 
algorithm a for buying n shares achieving buy vwap 
competitive ratio rbuy 
vwap a o min{q 
√ 
r} in addition 
any online algorithm a for buying n shares has buy vwap 
competitive ratio rbuy 
vwap a ω min{q 
√ 
r} 
 a limit order book trading 
model 
before we can define our online trading model based on 
limit order books we give some necessary background on 
the detailed mechanics of financial markets which are 
sometimes referred to as market microstructure we then provide 
results and algorithms for both the owt and vwap 
problems 
 
 background on limit order books and 
market microstructure 
a fundamental distinction in stock trading is that between 
a limit order and a market order suppose we wish to 
purchase shares of microsoft msft stock in a limit 
order we specify not only the desired volume shares 
but also the desired price suppose that msft is currently 
trading at roughly a share see figure which shows 
an actual snapshot of a recent msft order book on 
island www island com a well-known electronic exchange 
for nasdaq stocks but we are only willing to buy the 
 shares at a share or lower we can choose to 
submit a limit order with this specification and our order 
will be placed in a queue called the buy order book which 
is ordered by price with the highest offered unexecuted buy 
price at the top often referred to as the bid if there are 
multiple limit orders at the same price they are ordered by 
time of arrival with older orders higher in the book in the 
example provided by figure our order would be placed 
immediately after the extant order for shares at 
though we offer the same price this order has arrived before 
ours similarly a sell order book for sell limit orders for 
instance we might want to sell shares of msft at 
or higher is maintained this time with the lowest sell price 
offered often referred to as the ask 
thus the order books are sorted from the most 
competitive limit orders at the top high buy prices and low sell 
prices down to less competitive limit orders the bid and 
ask prices which again are simply the prices in the limit 
orders at the top of the buy and sell books respectively 
together are sometimes referred to as the inside market and 
the difference between them as the spread by definition 
the order books always consist exclusively of unexecuted 
orders - they are queues of orders hopefully waiting for the 
price to move in their direction 
how then do orders get executed there are two 
methods first any time a market order arrives it is immediately 
matched with the most competitive limit orders on the 
opposing book thus a market order to buy shares is 
matched with enough volume on the sell order book to fill 
the shares for instance in the example of figure 
such an order would be filled by the two limit sell orders 
for shares at the shares at the 
shares at and then of the shares at 
the remaining shares of this last limit order would 
remain as the new top of the sell limit order book second 
if a buy sell respectively limit order comes in above the 
ask below the bid respectively price then the order is 
matched with orders on the opposing books it is important 
to note that the prices of executions are the prices specified 
in the limit orders already in the books not the prices of the 
incoming order that is immediately executed 
every market or limit order arrives atomically and 
instantaneously - there is a strict temporal sequence in which 
orders arrive and two orders can never arrive simultaneously 
this gives rise to the definition of the last price of the 
exchange which is simply the last price at which the exchange 
executed an order it is this quantity that is usually meant 
when people casually refer to the ticker price of a stock 
note that a limit buy sell respectively order with a 
price of infinity respectively is effectively a market 
order we shall thus assume without loss of generality that 
all orders are placed as limit order although limit orders 
which are unexecuted may be removed by the party which 
placed them for simplicity we assume that limit orders are 
never removed from the books 
we refer the reader to for further discussion of modern 
electronic exchanges and market microstructure 
 the model 
the online order book trading model is intended to capture 
the realistic details of market microstructure just discussed 
in a competitive ratio setting in this refined model a day s 
market activity is described by a sequence of limit orders 
 pt vt bt here bt is a bit indicating whether the order is 
a buy or sell order while pt is the limit order price and vt 
the number of shares desired following the arrival of each 
such limit order an online trading algorithm is permitted 
to place its own limit order these two interleaved sources 
 market and algorithm of limit orders are then simply 
operated on according to the matching process described in 
section any limit order that is not immediately 
executable according to this process is placed in the appropriate 
 buy or sell book for possible future execution arriving 
orders that can be partially or fully executed are so executed 
with any residual shares remaining on the respective book 
the goal of a vwap or owt selling algorithm is 
essentially the same as in the price-volume model but the 
context has changed in the following two fundamental ways 
first the assumption of infinite liquidity in the price-volume 
model is eliminated entirely the number of shares available 
at any given price is restricted to the total volume of limit 
orders offering that price second all incoming orders and 
therefore the complete limit order books are assumed to 
be visible to the algorithm this is consistent with modern 
electronic financial exchanges and indeed is the source of 
much current interest on wall street 
in general the definition of competitive ratios in the order 
book model is complicated by the fact that now our 
algorithm s activity influences the sequence of executed prices 
and volumes we thus first define the execution sequence 
determined by a limit order sequence placed by the 
market and our algorithm let s p v b pt vt bt 
be a limit order sequence placed by the market and let 
s p v b pt vt bt be a limit order sequence 
placed by our algorithm unless otherwise specified all bt are 
of the sell type let merge s s be the merged sequence 
 p v b p v b pt vt bt pt vt bt which is 
the time sequence of orders placed by the market and 
algorithm note that the algorithm has the option of not placing 
an order which we can view as a zero volume order 
if we conducted the order book maintenance and order 
execution process described in section on the sequence 
merge s s at irregular intervals a trade occurs for some 
number of shares and some price in each executed trade 
the selling party is either the market or the algorithm let 
execm s s q w qt wt be the sequence of 
executions where the market that is a party other than 
the algorithm was the selling party where the qt are the 
execution prices and wt the execution volumes similarly 
we define execa s s r x rt xt to be the 
sequence of executions in which the algorithm was the selling 
party thus execa s s ∪ execm s s is the set of all 
executions we generally expect t to be possibly much 
smaller than t 
the revenue of the algorithm and the market are defined 
 
as 
revm s s ≡ 
t 
t 
qtwt reva s s ≡ 
t 
t 
rtxt 
note that both these quantities are solely determined by the 
execution sequences execm s s and execa s s 
respectively 
for an algorithm a which is constrained to sell exactly n 
shares we define the owt competitive ratio of a rowt a 
as the maximum ratio under any s ∈ σ of the revenue 
obtained by a as compared to the revenue obtained by an 
optimal oﬄine algorithm a∗ 
 more formally for a∗ 
which 
is constrained to sell exactly n shares we define 
rowt a max 
s∈σ 
max 
a∗ 
reva∗ s s∗ 
 
reva s s 
where s∗ 
is the limit order sequence placed by a∗ 
on s if 
the algorithm a is randomized then we take the appropriate 
expectation with respect to s ∼ a 
we define the vwap competitive ratio rvwap a as 
the maximum ratio under any s ∈ σ between the market 
and algorithm vwaps more formally define vwapm s s 
as revm s s t 
t wt where the denominator is just 
the total executed volume of orders placed by the 
market similarly we define vwapa s s as reva s s n 
since we assume the algorithm sells no more than n shares 
 this definition implicitly assumes that a gets a price for 
unsold shares the vwap competitive ratio of a is then 
rvwap a max 
s∈σ 
{vwapm s s vwapa s s } 
where s is the online sequence of limit orders generated by 
a in response to the sequence s 
 owt results in the order book model 
for the owt problem in the order book model we 
introduce a more subtle version of the price variability 
assumption this is due to the fact that our algorithm s trading 
can impact the high and low prices of the day for the 
assumption below note that execm s ∅ is the sequence of 
executions without the interaction of our algorithm 
 order book price variability assumption 
let pmin ≤ pmax be known positive constants and 
define r pmax pmin for all intraday trading sequences 
s ∈ σ the prices pt in the sequence execm s ∅ satisfy 
pt ∈ pmin pmax for all t t 
note that this assumption does not imply that the ratios 
of high to low prices under the sequences execm s s or 
execa s s are bounded by r in fact the ratio in the 
sequence execa s s could be infinite if the algorithm ends 
up selling some stocks at a price 
theorem in the order book model under the order 
book price variability assumption there exists an online 
algorithm a for selling n shares achieving sell owt competitive 
ratio rowt a log r log n 
proof the algorithm a works by guessing a price p in 
the set {pmin i 
 ≤ i ≤ log r } and placing a sell limit 
order for all n shares at the price p at the beginning of 
the day alternatively algorithm a can place log r sell 
limit orders where the i-th one has price i 
pmin and volume 
n log r by placing an order at the beginning of the day 
the algorithm undercuts all sell orders that will be placed 
during the day for a price of p or higher meaning the 
algorithm s n shares must be filled first at this price hence 
if there were k shares that would have been sold at price p 
or higher without our activity then a would sell at least kp 
shares 
we define {pj} to be the multiset of prices of 
individual shares that are either executed or are buy limit order 
shares that remained unexecuted excluding the activity of 
our algorithm that is assuming our algorithm places no 
orders assume without loss of generality that p ≥ p ≥ 
consider guessing the kth highest such price pk if an 
order for n shares is placed at the day s start at price pk 
then we are guaranteed to obtain a return of kpk let 
k∗ 
 argmaxk{kpk} we can view our algorithm as 
attempting to guess pk∗ and succeeding if the guess p 
satisfies p ∈ pk∗ pk∗ hence we are log r competitive 
with the quantity max ≤k≤n kpk note that 
ρ ≡ 
n 
i 
pi 
 
n 
i 
 
i 
ipi 
≤ max 
 ≤k≤n 
kpk 
n 
i 
 
i 
≤ log n max 
 ≤k≤n 
kpk 
where ρ is defined as the sum of the top n prices pi without 
a s involvement 
similarly let {pj} be the multiset of prices of 
individual executed shares or the prices of unexecuted buy order 
shares but now including the orders placed by some selling 
algorithm a we now wish to show that for all algorithms 
a which sell n shares reva ≤ n 
i pi ≤ ρ 
essentially this inequality states the intuitive idea that a selling 
algorithm can only lower executed or unmatched buy 
order share prices to prove this we use induction to show 
that the removal of the activity of a selling algorithm causes 
these prices to increase first remove the last share in the 
last sell order placed by either a or the market on an 
arbitrary sequence merge s s - by this we mean take the 
last sell order placed by a or the market and decrease its 
volume by one share after this modification the top n 
prices p pn will not decrease this is because either this 
sell order share was not executed in which case the claim 
is trivially true or if it was executed the removal of this 
sell order share leaves an additional unexecuted buy order 
share of equal or higher price for induction assume that if 
we remove a share from any sell order that was placed by 
a or the market at or after time t then the top n prices 
do not decrease we now show that if we remove a share 
from the last sell order that was placed by a or the market 
before time t then the top n prices do not decrease if this 
sell order share was not executed then the claim is trivially 
true else if the sell order share was executed then claim 
is true because by removing this executed share from the 
sell order either i the corresponding buy order share of 
equal or higher value is unmatched on the remainder of the 
sequence in which case the claim is true or ii this buy 
 
order matches some sell order share at an equal or higher 
price which has the effect of removing a share from a sell 
order on the remainder of the sequence and by the 
inductive assumption this can only increase prices hence we 
have proven that for all a which sell n shares reva ≤ ρ 
we have now established that our revenue satisfies 
 log r es ∼a reva s s ≥ max 
 ≤k≤n 
{kpk} 
≥ ρ log n 
≥ max 
a 
{reva } log n 
where a performs an arbitrary sequence of n sell limit 
orders 
 vwap results in the order book model 
the owt algorithm from theorem can be applied to 
obtain the following vwap result 
corollary in the order book model under the order 
book price variability assumption there exists an online 
algorithm a for selling n shares achieving sell vwap 
competitive ratio rvwap a o log r log n 
we now make a rather different assumption on the 
sequences s 
 bounded order volume and max price 
assumption 
the set of sequences σ satisfies the following two 
properties first we assume that each order placed by the market 
is of volume less than γ which we view as a mild assumption 
since typically single orders on the market are not of high 
volume due to liquidity issues this assumption allows our 
algorithm to place at least one limit order at a time 
interleaved with approximately γ market executions second we 
assume that there is large volume in the sell order books 
below the price pmax which means that no orders placed by 
the market will be executed above the price pmax the 
simplest way to instantiate this latter assumption in the order 
book model is to assume that each sequence s ∈ σ starts 
by placing a huge number of sell orders more than vmax 
at price pmax 
although this assumption has a maximum price 
parameter it does not imply that the price ratio r is finite since 
it does not imply any lower bound on the prices of buy or 
executed shares aside from the trivial one of 
theorem consider the order book model under the 
bounded order volume and max price assumption there 
exists an algorithm a in which after exactly γn market 
executions have occurred then a has sold at most n shares 
and 
reva s s 
n 
 vwapa s s 
≥ − vwapm s s − 
pmax 
n 
where s is a sequence of n sell limit orders generated by a 
when observing s 
proof the algorithm divides the trading day into 
volume intervals whose real-time duration may vary for each 
period i in which γ shares have been executed in the 
market the algorithm computes the market vwap of only those 
shares traded in period i let us denote this by vwapi 
following this ith volume interval the algorithm places a limit 
order to sell exactly one share at a price close to vwapi 
more precisely the algorithm only places orders at the 
discrete prices − pmax − 
pmax following volume 
interval i the algorithm places a limit order to sell one share 
at the discretized price that is closest to vwapi but which 
is strictly smaller 
for the analysis we begin by noting that if all of the 
algorithm s limit orders are executed during the day the 
total revenue received by the algorithm would be at least 
 − vwapm s s n to see this it suffices to note that 
vwapm s s is a uniform mixture of the vwapi since 
by definition they each cover the same amount of market 
volume and if all the algorithm s limit orders were 
executed they each received more than − vwapi dollars 
for the interval i they followed 
we now count the potential lost revenue of the 
algorithm due to unexecuted limit orders by the assumption 
that individual orders are placed with volume less than γ 
then our algorithm is able to place a limit order during every 
block of γ shares have been traded hence after γn market 
orders have been executed a has placed n orders in the 
market 
note that there can be at most one limit order and thus 
at most one share left unexecuted at each level of the 
discretized price ladder defined above this is because 
following interval i the algorithm places its limit order strictly 
below vwapi so if vwapj ≥ vwapi for j i this limit 
order must have been executed thus unexecuted limit 
orders bound the vwaps of the remainder of the day 
resulting in at most one unexecuted order per price level 
a bound on the lost revenue is thus the sum of the 
discretized prices ∞ 
i − i 
pmax ≤ pmax clearly our 
algorithm has sold at most n shares 
note that as n becomes large vwapa approaches − 
times the market vwap if we knew that the final total 
volume of the market executions is v then we can set γ 
v n assuming that γ if we have only an upper and 
lower bound on v we should be able to guess and incur a 
logarithmic loss the following assumption tries to capture 
the market volume variability 
 order book volume variability assumption 
we now assume that the total volume which includes 
the shares executed by both our algorithm and the market 
is variable within some known region and that the market 
volume will be greater than our algorithms volume more 
formally for all s ∈ σ assume that the total volume v 
of shares traded in execm s s for any sequence s of n 
sell limit orders satisfies n ≤ vmin ≤ v ≤ vmax let 
q vmax vmin 
the following corollary is derived using a constant 
and observing that if we set γ such that v ≤ γn ≤ v then 
our algorithm will place between n and n limit orders 
corollary in the order book model if the bounded 
order volume and max price assumption and the order book 
volume variability assumption hold there exists an online 
algorithm a for selling at most n shares such that 
vwapa s s ≥ 
 
 log q 
vwapm s s − 
 pmax 
n 
 
 
x 
 
 
 
 
 
 
 
qqq log q e 
 
x 
 
 
 
 
 
 
jnpr log q e 
 
x 
 
 
 
 
 
 
 
 
 
mchp log q e 
 
x 
 
 
 
 
 
 
 
chkp log q e 
figure here we present bounds from section based on the empirical volume distributions for four real stocks 
qqq mchp jnpr and chkp the plots show histograms for the total daily volumes transacted on island for 
these stocks in the last year and a half along with the corresponding values of log q and e pbins 
vol denoted by e 
we assume that the minimum and maximum daily volumes in the data correspond to vmin and vmax respectively 
the worst-case competitive ratio bounds which are twice log q of our algorithm for those stocks are 
and respectively the corresponding bounds on the competitive ratio performance of our algorithm under the 
volume distribution model which are twice e pbins 
vol are better and respectively a − relative 
improvement using a finer volume binning along with a slightly more refined bound on the competitive ratio we can 
construct algorithms that using the empirical volume distribution given as correct guarantee even better competitive 
ratios of and respectively for those stocks details omitted 
 macroscopic distribution 
models 
we conclude our results with a return to the price-volume 
model where we shall introduce some refined methods of 
analysis for online trading algorithms we leave the 
generalization of these methods to the order book model for 
future work 
the competitive ratios defined so far measure performance 
relative to some baseline criterion in the worst case over all 
market sequences s ∈ σ it has been observed in many 
online settings that such worst-case metrics can yield 
pessimistic results and various relaxations have been 
considered such as permitting a probability distribution over the 
input sequence 
we now consider distributional models that are 
considerably weaker than assuming a distribution over complete 
market sequences s ∈ σ in the volume distribution model 
we assume only that there exists a distribution pvol over the 
total volume v traded in the market for the day and then 
examine the worst-case competitive ratio over sequences 
consistent with the randomly chosen volume more precisely 
we define 
rvwap a pvol ev ∼pvol max 
s∈seq v 
vwapm s 
vwapa s 
 
here v ∼ pvol denotes that v is chosen with respect to 
distribution pvol and seq v ⊂ σ is the set of all market 
sequences p v pt vt satisfying t 
t vt v 
similarly for owt we can define 
rowt a pmaxprice ep∼pmaxprice max 
s∈seq p 
p 
vwapa s 
 
here pmaxprice is a distribution over just the maximum price 
of the day and we then examine worst-case sequences 
consistent with this price seq p ⊂ σ is the set of all market 
sequences satisfying max ≤t≤t pt p analogous buy-side 
definitions can be given 
we emphasize that in these models only the distribution 
of maximum volume and price is known to the algorithm 
we also note that our probabilistic assumptions on s are 
considerably weaker than typical statistical finance 
models which would posit a detailed stochastic model for the 
step-by-step evolution of pt vt here we instead permit 
only a distribution over crude macroscopic measures of the 
entire day s market activity such as the total volume and 
high price and analyze the worst-case performance 
consistent with these crude measures for this reason we refer to 
such settings as the macroscopic distribution model 
the work of el-yaniv et al examines distributional 
assumptions similar to ours but they emphasize the 
worst 
case choices for the distributions as well and show that this 
leads to results no better than the original worst-case 
analysis over all sequences in contrast we feel that the analysis 
of specific distributions pvol and pmaxprice is natural in many 
financial contexts and our preliminary experimental results 
show significant improvements when this rather crude 
distributional information is taken into account see figure 
our results in the vwap setting examine the cases where 
these distributions are known exactly or only approximately 
similar results can be obtained for macroscopic distributions 
of maximum daily price for the one-way trading setting 
 results in the macroscopic distribution 
model 
we begin by noting that the algorithms examined so far 
work by binning total volumes or maximum prices into bins 
of exponentially increasing size and then guessing the 
index of the bin in which the actual quantity falls it is 
thus natural that the macroscopic distribution model 
performance of such algorithms which are common in 
competitive analysis might depend on the distribution of the true 
bin index 
in the remaining we assume that q is a power of and 
the base of the logarithm is let pvol denote the 
distribution of total daily market volume we define the 
related distribution pbins 
vol over bin indices i as follows for 
all i log q − pbins 
vol i is equal to the 
probability under pvol that the daily volume falls in the interval 
 vmin i− 
 vmin i 
 and pbins 
vol log q is for the last interval 
 vmax vmax 
we define e as 
e pbins 
vol ≡ ei∼p bins 
vol 
 pbins 
vol i 
 
 
 
 
log q 
i 
pbins 
vol i 
 
 
 
 
since the support of pbins 
vol has only log q elements e pbins 
vol 
can vary from for distributions pvol that place all of 
their weight in only one of the log q intervals between 
vmin vmin vmin vmax to log q for distributions 
pvol in which the total daily volume is equally likely to fall 
in any one of these intervals note that distributions pvol 
of this latter type are far from uniform over the entire range 
 vmin vmax 
theorem in the volume distribution model under the 
volume variability assumption there exists an online 
algorithm a for selling n shares that using only knowledge of 
the total volume distribution pvol achieves rvwap a pvol ≤ 
 e pbins 
vol 
all proofs in this section are provided in the appendix 
as a concrete example consider the case in which pvol 
is the uniform distribution over vmin vmax in that case 
pbins 
vol is exponentially increasing and peaks at the last bin 
which having the largest width also has the largest weight 
in this case e pbins 
vol is a constant i e independent of q 
leading to a constant competitive ratio on the other hand 
if pvol is exponential then pbins 
vol is uniform leading to an 
o log q competitive ratio just as in the more adversarial 
price-volume setting discussed earlier in figure we 
provide additional specific bounds obtained for empirical total 
daily volume distributions computed for some real stocks 
we now examine the setting in which pvol is unknown 
but an approximation ˜pvol is available let us define 
c pbins 
vol ˜pbins 
vol log q 
j 
˜pbins 
vol j log q 
i 
p bins 
vol i 
√ ˜p bins 
vol 
 i 
 
c is minimized at c pbins 
vol pbins 
vol e pbins 
vol and c may 
be infinite if ˜pbins 
vol i is when pbins 
vol i 
theorem in the volume distribution model under the 
volume variability assumption there exists an online 
algorithm a for selling n shares that using only knowledge of 
an approximation ˜pvol of pvol achieves rvwap a pvol ≤ 
 c pbins 
vol ˜pbins 
vol 
as an example of this result suppose our approximation 
obeys α pbins 
vol i ≤ ˜pbins 
vol i ≤ αpbins 
vol i for all i for 
some α thus our estimated bin index probabilities 
are all within a factor of α of the truth then it is easy 
to show that c pbins 
vol ˜pbins 
vol ≤ αe pbins 
vol so according to 
theorems and our penalty for using the approximate 
distribution is a factor of α in competitive ratio 
 references 
 b awerbuch y bartal a fiat and a ros´en 
competitive non-preemptive call control in proc th 
acm-siam symp on discrete algorithms pages 
 - 
 a borodin and r el-yaniv online computation and 
competitive analysis cambridge university press 
 
 r el-yaniv a fiat r m karp and g turpin 
optimal search and one-way trading online algorithms 
algorithmica - 
 m kearns and l ortiz the penn-lehman automated 
trading project ieee intelligent systems to 
appear 
 appendix 
 proofs from subsection 
proof sketch of theorem w l o g assume that q 
 and the total volume is v consider the time t where the 
fixed schedule f sells the least then ft ≤ n t consider the 
sequences where at time t we have pt pmax vt v and 
for times t t we have pt pmin and vt the vwap 
is pmax and the fixed schedule average is n t pmax n − 
n t pmin 
proof sketch of theorem the algorithm simply sells 
ut vt vmin n shares at time t the total number of 
shares sold u is clearly more than n and 
u 
t 
ut 
t 
 vt vmin n v vmin n ≤ qn 
the average price is 
v wapa s 
t 
ptut u 
t 
pt vt v v wapm s 
where we used the fact that ut u vt v 
 
proof of theorem we start with the proof of the 
lower bound consider the following scenario for the first 
t time units we have a price of 
√ 
rpmin and a total volume 
of vmin we observe how many shares the online algorithm 
has bought if it has bought more than half of the shares 
the remaining time steps have price pmin and volume vmax − 
vmin otherwise the remaining time steps have price pmax 
and negligible volume 
in the first case the online has paid at least 
√ 
rpmin 
while the vwap is at most 
√ 
rpmin q pmin therefore 
in this case the competitive ratio is ω q in the second case 
the online has to buy at least half of the shares at pmax so 
its average cost is at least pmax the market vwap is√ 
rpmin pmax 
√ 
r hence the competitive ratio is ω 
√ 
r 
for the upper bound we can get a 
√ 
r competitive ratio 
by buying all the shares once the price drops below 
√ 
rpmin 
the q upper bound is derive by running an algorithm that 
assumes the volume is vmin the online pays a cost of p 
while the vwap will be at least p q 
 proofs from section 
proof sketch of theorem we use the idea of 
guessing the total volume from theorem but now allow for the 
possibility of an arbitrary but known distribution over the 
total volume in particular consider constructing a 
distribution gbins 
vol over a set of volume values using pvol and use 
it to guess the total volume v let the algorithm guess 
ˆv vmin i 
with probability gbins 
vol i then note that 
for any price-volume sequence s if v ∈ vmin i− 
 vmin i 
 
vwapa s ≥ gbins 
vol i vwapm s this implies an 
upper bound on rvwap a pvol in terms of gbins 
vol we then 
get that gbins 
vol i ∝ pbins 
vol i minimizes the upper bound 
which leads to the upper bound stated in the theorem 
proof sketch of theorem replace pvol with ˜pvol 
in the expression for gbins 
vol in the proof sketch for the last 
result 
 
mechanism design for online real-time scheduling 
ryan porter∗ 
computer science department 
stanford university 
stanford ca 
rwporter stanford edu 
abstract 
for the problem of online real-time scheduling of jobs on a 
single processor previous work presents matching upper and 
lower bounds on the competitive ratio that can be achieved 
by a deterministic algorithm however these results only 
apply to the non-strategic setting in which the jobs are 
released directly to the algorithm motivated by emerging 
areas such as grid computing we instead consider this 
problem in an economic setting in which each job is released to 
a separate self-interested agent the agent can then delay 
releasing the job to the algorithm inflate its length and 
declare an arbitrary value and deadline for the job while the 
center determines not only the schedule but the payment 
of each agent for the resulting mechanism design problem 
 in which we also slightly strengthen an assumption from 
the non-strategic setting we present a mechanism that 
addresses each incentive issue while only increasing the 
competitive ratio by one we then show a matching lower bound 
for deterministic mechanisms that never pay the agents 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence-multiagent systems j social and 
behavioral sciences economics f computation by 
abstract devices modes of computation-online 
computation 
general terms 
algorithms economics design theory 
 introduction 
we consider the problem of online scheduling of jobs on 
a single processor each job is characterized by a release 
time a deadline a processing time and a value for successful 
completion by its deadline the objective is to maximize the 
sum of the values of the jobs completed by their respective 
deadlines the key challenge in this online setting is that 
the schedule must be constructed in real-time even though 
nothing is known about a job until its release time 
competitive analysis with its roots in is a 
well-studied approach for analyzing online algorithms by 
comparing them against the optimal oﬄine algorithm which 
has full knowledge of the input at the beginning of its 
execution one interpretation of this approach is as a game 
between the designer of the online algorithm and an adversary 
first the designer selects the online algorithm then the 
adversary observes the algorithm and selects the sequence of 
jobs that maximizes the competitive ratio the ratio of the 
value of the jobs completed by an optimal oﬄine algorithm 
to the value of those completed by the online algorithm 
two papers paint a complete picture in terms of 
competitive analysis for this setting in which the algorithm is 
assumed to know k the maximum ratio between the value 
densities value divided by processing time of any two jobs 
for k presents a -competitive algorithm and proves 
that this is a lower bound on the competitive ratio for 
deterministic algorithms the same paper also generalizes the 
lower bound to 
√ 
k 
for any k ≥ and then 
presents a matching 
√ 
k 
-competitive algorithm 
the setting addressed by these papers is completely 
nonstrategic and the algorithm is assumed to always know the 
true characteristics of each job upon its release however 
in domains such as grid computing see for example 
 this assumption is invalid because buyers of processor 
time choose when and how to submit their jobs 
furthermore sellers not only schedule jobs but also determine 
the amount that they charge buyers an issue not addressed 
in the non-strategic setting 
thus we consider an extension of the setting in which 
each job is owned by a separate self-interested agent 
instead of being released to the algorithm each job is now 
released only to its owning agent each agent now has four 
different ways in which it can manipulate the algorithm it 
decides when to submit the job to the algorithm after the 
true release time it can artificially inflate the length of the 
job and it can declare an arbitrary value and deadline for 
the job because the agents are self-interested they will 
choose to manipulate the algorithm if doing so will cause 
 
their job to be completed and indeed one can find 
examples in which agents have incentive to manipulate the 
algorithms presented in and 
the addition of self-interested agents moves the problem 
from the area of algorithm design to that of mechanism 
design the science of crafting protocols for self-interested 
agents recent years have seen much activity at the 
interface of computer science and mechanism design see e g 
 in general a mechanism defines a protocol for 
interaction between the agents and the center that 
culminates with the selection of an outcome in our setting a 
mechanism will take as input a job from each agent and 
return a schedule for the jobs and a payment to be made by 
each agent to the center a basic solution concept of 
mechanism design is incentive compatibility which in our setting 
requires that it is always in each agent s best interests to 
immediately submit its job upon release and to truthfully 
declare its value length and deadline 
in order to evaluate a mechanism using competitive 
analysis the adversary model must be updated in the new 
model the adversary still determines the sequence of jobs 
but it is the self-interested agents who determine the 
observed input of the mechanism thus in order to achieve a 
competitive ratio of c an online mechanism must both be 
incentive compatible and always achieve at least 
c 
of the 
value that the optimal oﬄine mechanism achieves on the 
same sequence of jobs 
the rest of the paper is structured as follows in 
section we formally define and review results from the 
original non-strategic setting after introducing the incentive 
issues through an example we formalize the mechanism 
design setting in section in section we present our first 
main result a 
√ 
k 
 -competitive mechanism and 
formally prove incentive compatibility and the competitive 
ratio we also show how we can simplify this mechanism for 
the special case in which k and each agent cannot alter 
the length of its job returning the general setting we show 
in section that this competitive ratio is a lower bound for 
deterministic mechanisms that do not pay agents finally 
in section we discuss related work other than the directly 
relevant and before concluding with section 
 non-strategic setting 
in this section we formally define the original non-strategic 
setting and recap previous results 
 formulation 
there exists a single processor on which jobs can execute 
and n jobs although this number is not known beforehand 
each job i is characterized by a tuple θi ri di li vi 
which denotes the release time deadline length of 
processing time required and value respectively the space θi of 
possible tuples is the same for each job and consists of all 
θi such that ri di li vi ∈ thus the model of time is 
continuous each job is released at time ri at which point 
its three other characteristics are known nothing is known 
about the job before its arrival each deadline is firm or 
hard which means that no value is obtained for a job that 
is completed after its deadline preemption of jobs is 
allowed and it takes no time to switch between jobs thus 
job i is completed if and only if the total time it executes 
on the processor before di is at least li 
let θ θ θn denote the vector of tuples for all 
jobs and let θ−i θ θi− θi θn denote the 
same vector without the tuple for job i thus θi θ−i 
denotes a complete vector of tuples 
define the value density ρi vi 
li 
of job i to be the ratio of 
its value to its length for an input θ denote the maximum 
and minimum value densities as ρmin mini ρi and ρmax 
maxi ρi the importance ratio is then defined to be ρmax 
ρmin 
 
the maximal ratio of value densities between two jobs the 
algorithm is assumed to always know an upper bound k on 
the importance ratio for simplicity we normalize the range 
of possible value densities so that ρmin 
an online algorithm is a function f θ × × θn → 
o that maps the vector of tuples for any number n to 
an outcome o an outcome o ∈ o is simply a schedule of 
jobs on the processor recorded by the function s → 
{ n} which maps each point in time to the active 
job or to if the processor is idle 
to denote the total elapsed time that a job has spent on 
the processor at time t we will use the function ei t 
t 
 
µ s x i dx where µ · is an indicator function that 
returns if the argument is true and zero otherwise a 
job s laxity at time t is defined to be di − t − li ei t 
the amount of time that it can remain inactive and still be 
completed by its deadline a job is abandoned if it cannot 
be completed by its deadline formally if di −t ei t li 
also overload s · and ei · so that they can also take a 
vector θ as an argument for example s θ t is shorthand 
for the s t of the outcome f θ and it denotes the active 
job at time t when the input is θ 
since a job cannot be executed before its release time the 
space of possible outcomes is restricted in that s θ t i 
implies ri ≤ t also because the online algorithm must 
produce the schedule over time without knowledge of future 
inputs it must make the same decision at time t for inputs 
that are indistinguishable at this time formally let θ t 
denote the subset of the tuples in θ that satisfy ri ≤ t the 
constraint is then that θ t θ t implies s θ t s θ t 
the objective function is the sum of the values of the jobs 
that are completed by their respective deadlines w o θ 
i vi · µ ei θ di ≥ li let w∗ 
 θ maxo∈o w o θ 
denote the maximum possible total value for the profile θ 
in competitive analysis an online algorithm is evaluated 
by comparing it against an optimal oﬄine algorithm 
because the oﬄine algorithm knows the entire input θ at time 
 but still cannot start each job i until time ri it 
always achieves w∗ 
 θ an online algorithm f · is strictly 
c-competitive if there does not exist an input θ such that 
c · w f θ θ w∗ 
 θ an algorithm that is c-competitive 
is also said to achieve a competitive ratio of c 
we assume that there does not exist an overload period 
of infinite duration a period of time ts 
 tf 
 is overloaded 
if the sum of the lengths of the jobs whose release time and 
deadline both fall within the time period exceeds the 
duration of the interval formally if tf 
−ts 
≤ i ts≤ri di≤tf li 
without such an assumption it is not possible to achieve a 
finite competitive ratio 
 previous results 
in the non-strategic setting presents a -competitive 
algorithm called td version for the case of k while 
 presents a 
√ 
k 
-competitive algorithm called dover 
for the general case of k ≥ matching lower bounds for 
deterministic algorithms for both of these cases were shown 
 
in in this section we provide a high-level description of 
td version using an example 
td version divides the schedule into intervals each 
of which begins when the processor transitions from idle to 
busy call this time tb 
 and ends with the completion of 
a job the first active job of an interval may have laxity 
however for the remainder of the interval preemption of the 
active job is only considered when some other job has zero 
laxity for example when the input is the set of jobs listed 
in table the first interval is the complete execution of 
job over the range no preemption is considered 
during this interval because job has laxity until time 
then a new interval starts at tb 
 when job becomes 
active before job can finish preemption is considered at 
time when job is released with zero laxity 
in order to decide whether to preempt the active job td 
 version uses two more variables te 
and p loss the 
former records the latest deadline of a job that would be 
abandoned if the active job executes to completion or if 
no such job exists the time that the active job will finish 
if it is not preempted in this case te 
 the value 
te 
−tb 
represents the an upper bound on the amount of 
possible execution time lost to the optimal oﬄine algorithm 
due to the completion of the active job the other variable 
p loss is equal to the length of the first active job of the 
current interval because in general this job could have 
laxity the oﬄine algorithm may be able to complete it outside 
of the range tb 
 te 
 
if the algorithm completes the active 
job and this job s length is at least te 
−tb 
 p loss 
 
 then the 
algorithm is guaranteed to be -competitive for this 
interval note that k implies that all jobs have the same 
value density and thus that lengths can used to compute 
the competitive ratio because this is not case at time 
 since te 
−tb 
 p loss 
 
 − 
 
 l the algorithm 
preempts job for job which then executes to completion 
job ri di li vi 
 
 
 
 
 
 
 
 
 
 
table input used to recap td version 
the up and down arrows represent ri and di 
respectively while the length of the box equals li 
 mechanism design setting 
however false information about job would cause td 
 version to complete this job for example if job s 
deadline were declared as ˆd then it would have zero laxity 
at time at this time the algorithm would preempt job 
 for job because te 
−tb 
 p loss 
 
 − 
 
 l 
job would then complete before the arrival of job 
 
while it would be easy to alter the algorithm to recognize 
that this is not possible for the jobs in table our example 
does not depend on the use of p loss 
 
while we will not describe the significantly more complex 
in order to address incentive issues such as this one we 
need to formalize the setting as a mechanism design 
problem in this section we first present the mechanism design 
formulation and then define our goals for the mechanism 
 formulation 
there exists a center who controls the processor and 
n agents where the value of n is unknown by the center 
beforehand each job i is owned by a separate agent i the 
characteristics of the job define the agent s type θi ∈ θi 
at time ri agent i privately observes its type θi and has 
no information about job i before ri thus jobs are still 
released over time but now each job is revealed only to the 
owning agent 
agents interact with the center through a direct 
mechanism γ θ θn g · in which each agent declares a 
job denoted by ˆθi ˆri ˆdi ˆli ˆvi and g θ × ×θn → o 
maps the declared types to an outcome o ∈ o an outcome 
o s · p pn consists of a schedule and a payment 
from each agent to the mechanism 
in a standard mechanism design setting the outcome is 
enforced at the end of the mechanism however since the 
end is not well-defined in this online setting we choose to 
model returning the job if it is completed and collecting a 
payment from each agent i as occurring at ˆdi which 
according to the agent s declaration is the latest relevant point of 
time for that agent that is even if job i is completed before 
ˆdi the center does not return the job to agent i until that 
time this modelling decision could instead be viewed as a 
decision by the mechanism designer from a larger space of 
possible mechanisms indeed as we will discuss later this 
decision of when to return a completed job is crucial to our 
mechanism 
each agent s utility ui g ˆθ θi vi · µ ei ˆθ di ≥ li · 
µ ˆdi ≤ di − pi ˆθ is a quasi-linear function of its value for 
its job if completed and returned by its true deadline and 
the payment it makes to the center we assume that each 
agent is a rational expected utility maximizer 
agent declarations are restricted in that an agent cannot 
declare a length shorter than the true length since the center 
would be able to detect such a lie if the job were completed 
on the other hand in the general formulation we will allow 
agents to declare longer lengths since in some settings it 
may be possible add unnecessary work to a job however 
we will also consider a restricted formulation in which this 
type of lie is not possible the declared release time ˆri 
is the time that the agent chooses to submit job i to the 
center and it cannot precede the time ri at which the job 
is revealed to the agent the agent can declare an arbitrary 
deadline or value to summarize agent i can declare any 
type ˆθi ˆri ˆdi ˆli ˆvi such that ˆli ≥ li and ˆri ≥ ri 
while in the non-strategic setting it was sufficient for the 
algorithm to know the upper bound k on the ratio ρmax 
ρmin 
 
in the mechanism design setting we will strengthen this 
assumption so that the mechanism also knows ρmin or 
equivalently the range ρmin ρmax of possible value densities 
dover 
 we note that it is similar in its use of intervals and 
its preference for the active job also we note that the 
lower bound we will show in section implies that false 
information can also benefit a job in dover 
 
 
note that we could then force agent declarations to satisfy 
ρmin ≤ ˆvi 
ˆli 
≤ ρmax however this restriction would not 
 
while we feel that it is unlikely that a center would know k 
without knowing this range we later present a mechanism 
that does not depend on this extra knowledge in a restricted 
setting 
the restriction on the schedule is now that s ˆθ t i 
implies ˆri ≤ t to capture the fact that a job cannot be 
scheduled on the processor before it is declared to the mechanism 
as before preemption of jobs is allowed and job switching 
takes no time 
the constraints due to the online mechanism s lack of 
knowledge of the future are that ˆθ t ˆθ t implies s ˆθ t 
s ˆθ t and ˆθ ˆdi ˆθ ˆdi implies pi ˆθ pi ˆθ for each 
agent i the setting can then be summarized as follows 
 overview of the setting 
for all t do 
the center instantiates s ˆθ t ← i for some i s t ˆri ≤ t 
if ∃i ri t then 
θi is revealed to agent i 
if ∃i t ≥ ri and agent i has not declared a job then 
agent i can declare any job ˆθi s t ˆri t and ˆli ≥ li 
if ∃i ˆdi t ∧ ei ˆθ t ≥ li then 
completed job i is returned to agent i 
if ∃i ˆdi t then 
center sets and collects payment pi ˆθ from agent i 
 mechanism goals 
our aim as mechanism designer is to maximize the value 
of completed jobs subject to the constraints of incentive 
compatibility and individual rationality 
the condition for dominant strategy incentive 
compatibility is that for each agent i regardless of its true type 
and of the declared types of all other agents agent i cannot 
increase its utility by unilaterally changing its declaration 
definition a direct mechanism γ satisfies incentive 
compatibility ic if ∀i θi θi ˆθ−i 
ui g θi ˆθ−i θi ≥ ui g θi ˆθ−i θi 
from an agent perspective dominant strategies are 
desirable because the agent does not have to reason about either 
the strategies of the other agents or the distribution from 
the which other agent s types are drawn from a 
mechanism designer perspective dominant strategies are 
important because we can reasonably assume that an agent who 
has a dominant strategy will play according to it for these 
reasons in this paper we require dominant strategies as 
opposed to a weaker equilibrium concept such as bayes-nash 
under which we could improve upon our positive results 
decrease the lower bound on the competitive ratio 
 
a possible argument against the need for incentive 
compatibility is that an agent s lie may actually improve the 
schedule in fact this was the case in the example we showed 
for the false declaration ˆd however if an agent lies 
due to incorrect beliefs over the future input then the lie 
could instead make the schedule the worse for example if 
job were never released then job would have been 
unnecessarily abandoned furthermore if we do not know the 
beliefs of the agents and thus cannot predict how they will 
lie then we can no longer provide a competitive guarantee 
for our mechanism 
while restricting ourselves to incentive compatible direct 
mechanisms may seem limiting at first the revelation 
principle for dominant strategies see e g tells us that if 
our goal is dominant strategy implementation then we can 
make this restriction without loss of generality 
the second goal for our mechanism individual rationality 
requires that agents who truthfully reveal their type never 
have negative utility the rationale behind this goal is that 
participation in the mechanism is assumed to be voluntary 
definition a direct mechanism γ satisfies individual 
rationality ir if ∀i θi ˆθ−i ui g θi ˆθ−i θi ≥ 
finally the social welfare function that we aim to 
maximize is the same as the objective function of the non-strategic 
setting w o θ i vi · µ ei θ di ≥ li as in the 
nonstrategic setting we will evaluate an online mechanism using 
competitive analysis to compare it against an optimal oﬄine 
mechanism which we will denote by γoffline an oﬄine 
mechanism knows all of the types at time and thus can 
always achieve w∗ 
 θ 
definition an online mechanism γ is strictly 
ccompetitive if it satisfies ic and ir and if there does not 
exist a profile of agent types θ such that c·w g θ θ w∗ 
 θ 
 results 
in this section we first present our main positive result a 
 
√ 
k 
 -competitive mechanism γ after providing 
some intuition as to why γ satisfies individual rationality 
and incentive compatibility we formally prove first these two 
properties and then the competitive ratio we then consider 
a special case in which k and agents cannot lie about the 
length of their job which allows us to alter this mechanism 
so that it no longer requires either knowledge of ρmin or the 
collection of payments from agents 
unlike td version and dover 
 γ gives no 
preference to the active job instead it always executes the 
available job with the highest priority ˆvi 
√ 
k · ei ˆθ t · ρmin 
each agent whose job is completed is then charged the 
lowest value that it could have declared such that its job still 
would have been completed holding constant the rest of its 
declaration 
by the use of a payment rule similar to that of a 
secondprice auction γ satisfies both ic with respect to values 
and ir we now argue why it satisfies ic with respect to 
the other three characteristics declaring an improved job 
 i e declaring an earlier release time a shorter length or 
a later deadline could possibly decrease the payment of an 
agent however the first two lies are not possible in our 
setting while the third would cause the job if it is completed 
to be returned to the agent after the true deadline this is 
the reason why it is important to always return a completed 
job at its declared deadline instead of at the point at which 
it is completed 
 
another possibility is to allow only the agents to know 
their types at time and to force γoffline to be incentive 
compatible so that agents will truthfully declare their types 
at time however this would not affect our results since 
executing a vcg mechanism see e g at time both 
satisfies incentive compatibility and always maximizes social 
welfare 
 
mechanism γ 
execute s ˆθ · according to algorithm 
for all i do 
if ei ˆθ ˆdi ≥ ˆli {agent i s job is completed} then 
pi ˆθ ← arg minvi≥ ei ˆri ˆdi ˆli vi ˆθ−i ˆdi ≥ ˆli 
else 
pi ˆθ ← 
algorithm 
for all t do 
avail ← {i t ≥ ˆri ∧ ei ˆθ t ˆli ∧ ei ˆθ t ˆdi−t ≥ ˆli } 
{set of all released non-completed non-abandoned jobs} 
if avail ∅ then 
s ˆθ t ← arg maxi∈avail ˆvi 
√ 
k · ei ˆθ t · ρmin 
{break ties in favor of lower ˆri} 
else 
s ˆθ t ← 
it remains to argue why an agent does not have incentive 
to worsen its job the only possible effects of an inflated 
length are delaying the completion of the job and causing it 
to be abandoned and the only possible effects of an earlier 
declared deadline are causing to be abandoned and causing 
it to be returned earlier which has no effect on the agent s 
utility in our setting on the other hand it is less obvious 
why agents do not have incentive to declare a later release 
time consider a mechanism γ that differs from γ in that 
it does not preempt the active job i unless there exists 
another job j such that ˆvi 
√ 
k·li ˆθ t ·ρmin ˆvj note that 
as an active job approaches completion in γ its condition 
for preemption approaches that of γ 
however the types in table for the case of k show 
why an agent may have incentive to delay the arrival of its 
job under γ job becomes active at time and job 
is abandoned upon its release at time because 
v l v then at time job is preempted by job 
 because v l v job then executes 
to completion forcing job to be abandoned however job 
 had more weight than job and would have prevented 
job from being executed if it had been the active job at 
time since v l v thus if agent 
 had falsely declared ˆr then job would have been 
abandoned at time and job would have completed over 
the range 
job ri di li vi 
 
 
 
 
 
 
 
 
 
 
table jobs used to show why a slightly altered 
version of γ would not be incentive compatible with 
respect to release times 
intuitively γ avoids this problem because of two 
properties first when a job becomes active it must have a greater 
priority than all other available jobs second because a job s 
priority can only increase through the increase of its elapsed 
time ei ˆθ t the rate of increase of a job s priority is 
independent of its characteristics these two properties together 
imply that while a job is active there cannot exist a time 
at which its priority is less than the priority that one of 
these other jobs would have achieved by executing on the 
processor instead 
 proof of individual rationality and 
incentive compatibility 
after presenting the trivial proof of ir we break the 
proof of ic into lemmas 
theorem mechanism γ satisfies individual 
rationality 
proof for arbitrary i θi ˆθ−i if job i is not completed 
then agent i pays nothing and thus has a utility of zero 
that is pi θi ˆθ−i and ui g θi ˆθ−i θi on the 
other hand if job i is completed then its value must 
exceed agent i s payment formally ui g θi ˆθ−i θi vi − 
arg minvi≥ ei ri di li vi ˆθ−i di ≥ li ≥ must hold 
since vi vi satisfies the condition 
to prove ic we need to show that for an arbitrary agent 
i and an arbitrary profile ˆθ−i of declarations of the other 
agents agent i can never gain by making a false declaration 
ˆθi θi subject to the constraints that ˆri ≥ ri and ˆli ≥ li 
we start by showing that regardless of ˆvi if truthful 
declarations of ri di and li do not cause job i to be completed 
then worse declarations of these variables that is 
declarations that satisfy ˆri ≥ ri ˆli ≥ li and ˆdi ≤ di can never 
cause the job to be completed we break this part of the 
proof into two lemmas first showing that it holds for the 
release time regardless of the declarations of the other 
variables and then for length and deadline 
lemma in mechanism γ the following condition holds 
for all i θi ˆθ−i ∀ ˆvi ˆli ≥ li ˆdi ≤ di ˆri ≥ ri 
ei ˆri ˆdi ˆli ˆvi ˆθ−i ˆdi ≥ ˆli ⇒ 
ei ri ˆdi ˆli ˆvi ˆθ−i ˆdi ≥ ˆli 
proof assume by contradiction that this condition does 
not hold- that is job i is not completed when ri is truthfully 
declared but is completed for some false declaration ˆri ≥ 
ri we first analyze the case in which the release time is 
truthfully declared and then we show that job i cannot be 
completed when agent i delays submitting it to the center 
case i agent i declares ˆθi ri ˆdi ˆli ˆvi 
first define the following three points in the execution of 
job i 
 let ts 
 arg mint s ˆθi ˆθ−i t i be the time that 
job i first starts execution 
 let tp 
 arg mint ts s ˆθi ˆθ−i t i be the time 
that job i is first preempted 
 let ta 
 arg mint ei ˆθi ˆθ−i t ˆdi − t ˆli be the 
time that job i is abandoned 
 
if ts 
and tp 
are undefined because job i never becomes 
active then let ts 
 tp 
 ta 
 
also partition the jobs declared by other agents before ta 
into the following three sets 
 x {j ˆrj tp 
 ∧ j i } consists of the jobs other 
than i that arrive before job i is first preempted 
 y {j tp 
≤ ˆrj ≤ ta 
 ∧ ˆvj ˆvi 
√ 
k·ei ˆθi ˆθ−i ˆrj } 
consists of the jobs that arrive in the range tp 
 ta 
 and 
that when they arrive have higher priority than job i 
 note that we are make use of the normalization 
 z {j tp 
≤ ˆrj ≤ ta 
 ∧ ˆvj ≤ ˆvi 
√ 
k ·ei ˆθi ˆθ−i ˆrj } 
consists of the jobs that arrive in the range tp 
 ta 
 and 
that when they arrive have lower priority than job i 
we now show that all active jobs during the range tp 
 ta 
 
must be either i or in the set y unless tp 
 ta 
 in which case 
this property trivially holds it must be the case that job i 
has a higher priority than an arbitrary job x ∈ x at time tp 
 
since at the time just preceding tp 
job x was available and job 
i was active formally ˆvx 
√ 
k · ex ˆθi ˆθ−i tp 
 ˆvi 
√ 
k · 
ei ˆθi ˆθ−i tp 
 must hold 
we can then show that over the 
range tp 
 ta 
 no job x ∈ x runs on the processor assume 
by contradiction that this is not true let tf 
∈ tp 
 ta 
 be 
the earliest time in this range that some job x ∈ x is active 
which implies that ex ˆθi ˆθ−i tf 
 ex ˆθi ˆθ−i tp 
 we 
can then show that job i has a higher priority at time tf 
as 
follows ˆvx 
√ 
k·ex ˆθi ˆθ−i tf 
 ˆvx 
√ 
k·ex ˆθi ˆθ−i tp 
 
ˆvi 
√ 
k · ei ˆθi ˆθ−i tp 
 ≤ ˆvi 
√ 
k · ei ˆθi ˆθ−i tf 
 
contradicting the fact that job x is active at time tf 
 
a similar argument applies to an arbitrary job z ∈ z 
starting at it release time ˆrz tp 
 since by definition job i 
has a higher priority at that time the only remaining jobs 
that can be active over the range tp 
 ta 
 are i and those in 
the set y 
case ii agent i declares ˆθi ˆri ˆdi ˆli ˆvi where ˆri ri 
we now show that job i cannot be completed in this case 
given that it was not completed in case i first we can 
restrict the range of ˆri that we need to consider as follows 
declaring ˆri ∈ ri ts 
 would not affect the schedule since 
ts 
would still be the first time that job i executes also 
declaring ˆri ta 
could not cause the job to be completed 
since di − ta 
 ˆli holds which implies that job i would be 
abandoned at its release thus we can restrict consideration 
to ˆri ∈ ts 
 ta 
 
in order for declaring ˆθi to cause job i to be completed a 
necessary condition is that the execution of some job yc 
∈ y 
must change during the range tp 
 ta 
 since the only jobs 
other than i that are active during that range are in y 
let tc 
 arg mint∈ tp ta ∃yc 
∈ y s ˆθi ˆθ−i t yc 
 ∧ 
 s ˆθi ˆθ−i t yc 
 be the first time that such a change 
occurs we will now show that for any ˆri ∈ ts 
 ta 
 there 
cannot exist a job with higher priority than yc 
at time tc 
 
contradicting s ˆθi ˆθ−i t yc 
 
first note that job i cannot have a higher priority since 
there would have to exist a t ∈ tp 
 tc 
 such that ∃y ∈ 
 
for simplicity when we give the formal condition for a job x 
to have a higher priority than another job y we will assume 
that job x s priority is strictly greater than job y s because 
in the case of a tie that favors x future ties would also be 
broken in favor of job x 
y s ˆθi ˆθ−i t y ∧ s ˆθi ˆθ−i t i contradicting 
the definition of tc 
 
now consider an arbitrary y ∈ y such that y yc 
 in case 
i we know that job y has lower priority than yc 
at time tc 
 
that is ˆvy 
√ 
k·ey ˆθi ˆθ−i tc 
 ˆvyc 
√ 
k·eyc ˆθi ˆθ−i tc 
 
thus moving to case ii job y must replace some other job 
before tc 
 since ˆry ≥ tp 
 the condition is that there must 
exist some t ∈ tp 
 tc 
 such that ∃w ∈ y ∪{i} s ˆθi ˆθ−i t 
w ∧ s ˆθi ˆθ−i t y since w ∈ y would contradict the 
definition of tc 
 we know that w i that is the job that y 
replaces must be i by definition of the set y we know that 
ˆvy ˆvi 
√ 
k · ei ˆθi ˆθ−i ˆry thus if ˆry ≤ t then job i 
could not have executed instead of y in case i on the other 
hand if ˆry t then job y obviously could not execute at 
time t contradicting the existence of such a time t 
now consider an arbitrary job x ∈ x we know that 
in case i job i has a higher priority than job x at time 
ts 
 or formally that ˆvx 
√ 
k · ex ˆθi ˆθ−i ts 
 ˆvi 
√ 
k · 
ei ˆθi ˆθ−i ts 
 we also know that ˆvi 
√ 
k·ei ˆθi ˆθ−i tc 
 
ˆvyc 
√ 
k · eyc ˆθi ˆθ−i tc 
 since delaying i s arrival will 
not affect the execution up to time ts 
 and since job x 
cannot execute instead of a job y ∈ y at any time t ∈ 
 tp 
 tc 
 by definition of tc 
 the only way for job x s 
priority to increase before tc 
as we move from case i to ii 
is to replace job i over the range ts 
 tc 
 thus an 
upper bound on job x s priority when agent i declares ˆθi is 
ˆvx 
√ 
k· ex ˆθi ˆθ−i ts 
 ei ˆθi ˆθ−i tc 
 −ei ˆθi ˆθ−i ts 
 
ˆvi 
√ 
k· ei ˆθi ˆθ−i ts 
 ei ˆθi ˆθ−i tc 
 −ei ˆθi ˆθ−i ts 
 
ˆvi 
√ 
k · ei ˆθi ˆθ−i tc 
 ˆvyc 
√ 
k · eyc ˆθi ˆθ−i tc 
 
thus even at this upper bound job yc 
would execute 
instead of job x at time tc 
 a similar argument applies to 
an arbitrary job z ∈ z starting at it release time ˆrz since 
the sets {i} x y z partition the set of jobs released before 
ta 
 we have shown that no job could execute instead of job 
yc 
 contradicting the existence of tc 
 and completing the 
proof 
lemma in mechanism γ the following condition holds 
for all i θi ˆθ−i ∀ ˆvi ˆli ≥ li ˆdi ≤ di 
ei ri ˆdi ˆli ˆvi ˆθ−i ˆdi ≥ ˆli ⇒ 
ei ri di li ˆvi ˆθ−i ˆdi ≥ li 
proof assume by contradiction there exists some 
instantiation of the above variables such that job i is not 
completed when li and di are truthfully declared but is 
completed for some pair of false declarations ˆli ≥ li and 
ˆdi ≤ di 
note that the only effect that ˆdi and ˆli have on the 
execution of the algorithm is on whether or not i ∈ avail 
specifically they affect the two conditions ei ˆθ t ˆli 
and ei ˆθ t ˆdi − t ≥ ˆli because job i is completed when 
ˆli and ˆdi are declared the former condition for 
completion must become false before the latter since truthfully 
declaring li ≤ ˆli and di ≥ ˆdi will only make the former 
condition become false earlier and the latter condition become 
false later the execution of the algorithm will not be 
affected when moving to truthful declarations and job i will 
be completed a contradiction 
we now use these two lemmas to show that the payment 
for a completed job can only increase by falsely declaring 
worse ˆli ˆdi and ˆri 
 
lemma in mechanism γ the following condition holds 
for all i θi ˆθ−i ∀ ˆli ≥ li ˆdi ≤ di ˆri ≥ ri 
arg min 
vi≥ 
ei ˆri ˆdi ˆli vi ˆθ−i ˆdi ≥ ˆli ≥ 
arg min 
vi≥ 
ei ri di li vi ˆθ−i di ≥ li 
proof assume by contradiction that this condition does 
not hold this implies that there exists some value vi such 
that the condition ei ˆri ˆdi ˆli vi ˆθ−i ˆdi ≥ ˆli holds 
but ei ri di li vi ˆθ−i di ≥ li does not applying 
lemmas and ei ˆri ˆdi ˆli vi ˆθ−i ˆdi ≥ ˆli ⇒ 
 ei ri ˆdi ˆli vi ˆθ−i ˆdi ≥ ˆli ⇒ 
 ei ri di li vi ˆθ−i di ≥ li a contradiction 
finally the following lemma tells us that the completion 
of a job is monotonic in its declared value 
lemma in mechanism γ the following condition holds 
for all i ˆθi ˆθ−i ∀ ˆvi ≥ ˆvi 
ei ˆri ˆdi ˆli ˆvi ˆθ−i ˆdi ≥ ˆli ⇒ 
ei ˆri ˆdi ˆli ˆvi ˆθ−i ˆdi ≥ ˆli 
the proof by contradiction of this lemma is omitted 
because it is essentially identical to that of lemma for ˆri in 
case i agent i declares ˆri ˆdi ˆli ˆvi and the job is not 
completed while in case ii he declares ˆri ˆdi ˆli ˆvi and the job 
is completed the analysis of the two cases then proceeds as 
before- the execution will not change up to time ts 
because 
the initial priority of job i decreases as we move from case 
i to ii and as a result there cannot be a change in the 
execution of a job other than i over the range tp 
 ta 
 
we can now combine the lemmas to show that no 
profitable deviation is possible 
theorem mechanism γ satisfies incentive 
compatibility 
proof for an arbitrary agent i we know that ˆri ≥ ri 
and ˆli ≥ li hold by assumption we also know that agent 
i has no incentive to declare ˆdi di because job i would 
never be returned before its true deadline then because 
the payment function is non-negative agent i s utility could 
not exceed zero by ir this is the minimum utility it would 
achieve if it truthfully declared θi thus we can restrict 
consideration to ˆθi that satisfy ˆri ≥ ri ˆli ≥ li and ˆdi ≤ di 
again using ir we can further restrict consideration to ˆθi 
that cause job i to be completed since any other ˆθi yields a 
utility of zero 
if truthful declaration of θi causes job i to be completed 
then by lemma any such false declaration ˆθi could not 
decrease the payment of agent i on the other hand if 
truthful declaration does not cause job i to be completed 
then declaring such a ˆθi will cause agent i to have negative 
utility since vi arg minvi≥ ei ri di li vi ˆθ−i ˆdi ≥ 
li ≤ arg minvi≥ ei ˆri ˆdi ˆli vi ˆθ−i ˆdi ≥ ˆli holds by 
lemmas and respectively 
 proof of competitive ratio 
the proof of the competitive ratio which makes use of 
techniques adapted from those used in is also broken 
into lemmas having shown ic we can assume truthful 
declaration ˆθ θ since we have also shown ir in order 
to prove the competitive ratio it remains to bound the loss 
of social welfare against γoffline 
denote by f the sequence of jobs completed by 
γ divide time into intervals if topen 
f tclose 
f one for 
each job f in this sequence set tclose 
f to be the time at 
which job f is completed and set topen 
f tclose 
f− for f ≥ 
and topen 
 for f also let tbegin 
f be the first time 
that the processor is not idle in interval if 
lemma for any interval if the following inequality 
holds tclose 
f − tbegin 
f ≤ √ 
k 
 · vf 
proof interval if begins with a possibly zero length 
period of time in which the processor is idle because there is 
no available job then it continuously executes a sequence 
of jobs c where each job i in this sequence is 
preempted by job i except for job c which is completed 
 thus job c in this sequence is the same as job f is the global 
sequence of completed jobs let ts 
i be the time that job i 
begins execution note that ts 
 tbegin 
f 
over the range tbegin 
f tclose 
f the priority vi 
√ 
k·ei θ t 
of the active job is monotonically increasing with time 
because this function linearly increases while a job is active 
and can only increase at a point in time when preemption 
occurs thus each job i in this sequence begins 
execution at its release time that is ts 
i ri because its priority 
does not increase while it is not active 
we now show that the value of the completed job c 
exceeds the product of 
√ 
k and the time spent in the interval 
on jobs through c− or more formally that the following 
condition holds vc ≥ 
√ 
k c− 
h eh θ ts 
h − eh θ ts 
h to 
show this we will prove by induction that the stronger 
condition vi ≥ 
√ 
k i− 
h eh θ ts 
h holds for all jobs i in the 
sequence 
base case for i v ≥ 
√ 
k 
h eh θ ts 
h 
since the sum is over zero elements 
inductive step for an arbitrary ≤ i c we assume 
that vi ≥ 
√ 
k i− 
h eh θ ts 
h holds at time ts 
i we 
know that vi ≥ vi 
√ 
k · ei θ ts 
i holds because ts 
i 
ri these two inequalities together imply that vi ≥√ 
k i 
h eh θ ts 
h completing the inductive step 
we also know that tclose 
f − ts 
c ≤ lc ≤ vc must hold by the 
simplifying normalization of ρmin and the fact that job 
c s execution time cannot exceed its length we can thus 
bound the total execution time of if by tclose 
f − tbegin 
f 
 tclose 
f −ts 
c c− 
h eh θ ts 
h −eh θ ts 
h ≤ √ 
k 
 vf 
we now consider the possible execution of uncompleted 
jobs by γoffline associate each job i that is not completed 
by γ with the interval during which it was abandoned all 
jobs are now associated with an interval since there are no 
gaps between the intervals and since no job i can be 
abandoned after the close of the last interval at tclose 
f because 
the processor is idle after tclose 
f any such job i would 
become active at some time t ≥ tclose 
f which would lead to the 
completion of some job creating a new interval and 
contradicting the fact that if is the last one 
 
the following lemma is equivalent to lemma of 
but the proof is different for our mechanism 
lemma for any interval if and any job i abandoned 
in if the following inequality holds vi ≤ 
√ 
k vf 
proof assume by contradiction that there exists a job 
i abandoned in if such that vi 
√ 
k vf at tclose 
f 
the priority of job f is vf 
√ 
k · lf 
√ 
k vf because 
the priority of the active job monotonically increases over 
the range tbegin 
f tclose 
f job i would have a higher priority 
than the active job and thus begin execution at some time 
t ∈ tbegin 
f tclose 
f again applying monotonicity this would 
imply that the priority of the active job at tclose 
f exceeds 
 
√ 
k vf contradicting the fact that it is 
√ 
k vf 
as in for each interval if we give γoffline the 
following gift k times the amount of time in the range 
 tbegin 
f tclose 
f that it does not schedule a job additionally 
we give the adversary vf since the adversary may be able 
to complete this job at some future time due to the fact 
that γ ignores deadlines the following lemma is lemma 
 in and its proof now applies directly 
lemma with the above gifts the total net gain 
obtained by the clairvoyant algorithm from scheduling the jobs 
abandoned during if is not greater than 
√ 
k · vf 
the intuition behind this lemma is that the best that 
the adversary can do is to take almost all of the gift of 
k · tclose 
f −tbegin 
f intuitively this is equivalent to executing 
jobs with the maximum possible value density over the time 
that γ is active and then begin execution of a job 
abandoned by γ right before tclose 
f by lemma the value of 
this job is bounded by 
√ 
k · vf we can now combine 
the results of these lemmas to prove the competitive ratio 
theorem mechanism γ is 
√ 
k -competitive 
proof using the fact that the way in which jobs are 
associated with the intervals partitions the entire set of jobs 
we can show the competitive ratio by showing that γ is 
 
√ 
k 
 -competitive for each interval in the sequence 
 f over an arbitrary interval if the oﬄine 
algorithm can achieve at most tclose 
f −tbegin 
f ·k vf 
√ 
k vf 
from the two gifts and the net gain bounded by lemma 
 applying lemma this quantity is then bounded from 
above by √ 
k 
 ·vf ·k vf 
√ 
k vf 
√ 
k 
 ·vf 
since γ achieves vf the competitive ratio holds 
 special case unalterable length and k 
while so far we have allowed each agent to lie about all 
four characteristics of its job lying about the length of the 
job is not possible in some settings for example a user 
may not know how to alter a computational problem in a 
way that both lengthens the job and allows the solution of 
the original problem to be extracted from the solution to 
the altered problem another restriction that is natural in 
some settings is uniform value densities k which was 
the case considered by if the setting satisfies these two 
conditions then by using mechanism γ we can achieve a 
competitive ratio of which is the same competitive ratio 
as γ for the case of k without knowledge of ρmin and 
without the use of payments the latter property may be 
necessary in settings that are more local than grid 
computing e g within a department but in which the users are 
still self-interested 
mechanism γ 
execute s ˆθ · according to algorithm 
for all i do 
pi ˆθ ← 
algorithm 
for all t do 
avail ← {i t ≥ ˆri ∧ ei ˆθ t li ∧ ei ˆθ t ˆdi−t ≥ li } 
if avail ∅ then 
s ˆθ t ← arg maxi∈avail li ei ˆθ t 
{break ties in favor of lower ˆri} 
else 
s ˆθ t ← 
theorem when k and each agent i cannot 
falsely declare li mechanism γ satisfies individual 
rationality and incentive compatibility 
theorem when k and each agent i cannot 
falsely declare li mechanism γ is -competitive 
since this mechanism is essentially a simplification of γ 
we omit proofs of these theorems basically the fact that 
k and ˆli li both hold allows γ to substitute the 
priority li ei ˆθ t for the priority used in γ and since ˆvi 
is ignored payments are no longer needed to ensure incentive 
compatibility 
 competitive lower bound 
we now show that the competitive ratio of 
√ 
k 
 
 achieved by γ is a lower bound for deterministic 
online mechanisms to do so we will appeal to third 
requirement on a mechanism non-negative payments nnp 
which requires that the center never pays an agent formally 
∀i ˆθ pi ˆθi ≥ unlike ic and ir this requirement is not 
standard in mechanism design we note however that both 
γ and γ satisfy it trivially and that in the following proof 
zero only serves as a baseline utility for an agent and could 
be replaced by any non-positive function of ˆθ−i 
the proof of the lower bound uses an adversary argument 
similar to that used in to show a lower bound of √ 
k 
in the non-strategic setting with the main novelty 
lying in the perturbation of the job sequence and the related 
incentive compatibility arguments we first present a lemma 
relating to the recurrence used for this argument with the 
proof omitted due to space constraints 
lemma for any k ≥ for the recurrence defined by 
li λ · li − k · i 
h lh and l where 
√ 
k 
− 
λ 
√ 
k 
 there exists an integer m ≥ such that 
lm k· m− 
h 
lh 
lm 
 λ 
 
while payments are not required in this setting γ can be 
changed to collect a payments without affecting incentive 
compatibility by charging some fixed fraction of li for each 
job i that is completed 
 
theorem there does not exist a deterministic online 
mechanism that satisfies nnp and that achieves a 
competitive ratio less than 
√ 
k 
 
proof assume by contradiction that there exists a 
deterministic online mechanism γ that satisfies nnp and that 
achieves a competitive ratio of c 
√ 
k 
 − for 
some and by implication satisfies ic and ir as 
well since a competitive ratio of c implies a competitive 
ratio of c x for any x we assume without loss of 
generality that first we will construct a profile of 
agent types θ using an adversary argument after possibly 
slightly perturbing θ to assure that a strictness property is 
satisfied we will then use a more significant perturbation of 
θ to reach a contradiction 
we now construct the original profile θ pick an α such 
that α and define δ α 
ck k 
 the adversary uses 
two sequences of jobs minor and major minor jobs i are 
characterized by li δ vi k · δ and zero laxity the first 
minor job is released at time and ri di− for all i 
the sequence stops whenever γ completes any job 
major jobs also have zero laxity but they have the 
smallest possible value ratio that is vi li the lengths of the 
major jobs that may be released starting with i are 
determined by the following recurrence relation 
li c − α · li − k · 
i 
h 
lh 
l 
the bounds on α imply that 
√ 
k 
− c − α 
 
√ 
k 
 which allows us to apply lemma let m be the 
smallest positive number such that 
lm k· m− 
h 
lh 
lm 
 c− α 
the first major job has a release time of and each major 
job i has a release time of ri di− − δ just before 
the deadline of the previous job the adversary releases 
major job i ≤ m if and only if each major job j i was 
executed continuously over the range ri ri no major 
job is released after job m 
in order to achieve the desired competitive ratio γ must 
complete some major job f because γoffline can always at 
least complete major job for a value of and γ can 
complete at most one minor job for a value of α 
c 
 
c 
 
also in order for this job f to be released the processor 
time preceding rf can only be spent executing major jobs 
that are later abandoned if f m then major job f 
will be released and it will be the final major job γ cannot 
complete job f because rf lf df rf therefore 
θ consists of major jobs through f or f if f m 
plus minor jobs from time through time df 
we now possibly perturb θ slightly by ir we know 
that vf ≥ pf θ since we will later need this inequality 
to be strict if vf pf θ then change θf to θf where 
rf rf but vf lf and df are all incremented by δ over 
their respective values in θf by ic job f must still be 
completed by γ for the profile θf θ−f if not then by 
ir and nnp we know that pf θf θ−f and thus that 
uf g θf θ−f θf however agent f could then increase 
its utility by falsely declaring the original type of θf 
receiving a utility of uf g θf θ−f θf vf − pf θ δ 
violating ic furthermore agent f must be charged the same 
amount that is pf θf θ−f pf θ due to a similar 
incentive compatibility argument thus for the remainder of 
the proof assume that vf pf θ 
we now use a more substantial perturbation of θ to 
complete the proof if f m then define θf to be identical 
to θf except that df df lf allowing job f to be 
completely executed after job f is completed if f m 
then instead set df df lf ic requires that for the profile 
 θf θ−f γ still executes job f continuously over the range 
 rf rf lf thus preventing job f from being completed 
assume by contradiction that this were not true then at 
the original deadline of df job f is not completed consider 
the possible profile θf θ−f θx which differs from the new 
profile only in the addition of a job x which has zero laxity 
rx df and vx lx max df − df c · lf lf 
because this new profile is indistinguishable from θf θ−f 
to γ before time df it must schedule jobs in the same way 
until df then in order to achieve the desired competitive 
ratio it must execute job x continuously until its deadline 
which is by construction at least as late as the new deadline 
df of job f thus job f will not be completed and by 
ir and nnp it must be the case that pf θf θ−f θx 
 and uf g θf θ−f θx θf using the fact that θ is 
indistinguishable from θf θ−f θx up to time df if agent 
f falsely declared his type to be the original θf then its job 
would be completed by df and it would be charged pf θ 
its utility would then increase to uf g θf θ−f θx θf 
vf − pf θ contradicting ic 
while γ s execution must be identical for both θf θ−f 
and θf θ−f γoffline can take advantage of the change if 
f m then γ achieves a value of at most lf δ the value of 
job f if it were perturbed while γoffline achieves a value of 
at least k· f 
h lh − δ lf lf by executing minor jobs 
until rf followed by job f and then job f we subtract 
two δ s instead of one because the last minor job before rf 
may have to be abandoned substituting in for lf the 
competitive ratio is then at least 
k· 
f 
h 
lh− δ lf lf 
lf δ 
 
k· 
f 
h 
lh − k·δ c− α ·lf −k· 
f 
h 
lh lf 
lf δ 
 
c·lf α·lf − k·δ 
lf δ 
≥ 
c·lf ck k δ− k·δ 
lf δ 
 c 
if instead f m then γ achieves a value of at most lm δ 
while γoffline achieves a value of at least k · m 
h lh − 
 δ lm by completing minor jobs until dm rm lm 
and then completing job m the competitive ratio is then 
at least 
k· m 
h lh− δ lm 
lm δ 
 
k· m− 
h 
lh − k·δ klm lm 
lm δ 
 
 c− α ·lm− k·δ klm 
lm δ 
 c k− ·lm αlm− k·δ 
lm δ 
 c 
 related work 
in this section we describe related work other than the 
two papers and on which this paper is based 
recent work related to this scheduling domain has focused on 
competitive analysis in which the online algorithm uses a 
faster processor than the oﬄine algorithm see e g 
 mechanism design was also applied to a scheduling 
problem in in their model the center owns the jobs 
in an oﬄine setting and it is the agents who can execute 
them the private information of an agent is the time it will 
require to execute each job several incentive compatible 
mechanisms are presented that are based on approximation 
algorithms for the computationally infeasible optimization 
problem this paper also launched the area of 
algorithmic mechanism design in which the mechanism must 
sat 
isfy computational requirements in addition to the standard 
incentive requirements a growing sub-field in this area is 
multicast cost-sharing mechanism design see e g in 
which the mechanism must efficiently determine for each 
agent in a multicast tree whether the agent receives the 
transmission and the price it must pay for a survey of 
this and other topics in distributed algorithmic mechanism 
design see 
online execution presents a different type of algorithmic 
challenge and several other papers study online algorithms 
or mechanisms in economic settings for example 
considers an online market clearing setting in which the 
auctioneer matches buy and sells bids which are assumed to be 
exogenous that arrive and expire over time in a 
general method is presented for converting an online algorithm 
into an online mechanism that is incentive compatible with 
respect to values truthful declaration of values is also 
considered in and which both consider multi-unit online 
auctions the main difference between the two is that the 
former considers the case of a digital good which thus has 
unlimited supply it is pointed out in that their results 
continue to hold when the setting is extended so that bidders 
can delay their arrival 
the only other paper we are aware of that addresses the 
issue of incentive compatibility in a real-time system is 
which considers several variants of a model in which the 
center allocates bandwidth to agents who declare both their 
value and their arrival time a dominant strategy ic 
mechanism is presented for the variant in which every point in time 
is essentially independent while a bayes-nash ic 
mechanism is presented for the variant in which the center s 
current decision affects the cost of future actions 
 conclusion 
in this paper we considered an online scheduling domain 
for which algorithms with the best possible competitive 
ratio had been found but for which new solutions were 
required when the setting is extended to include self-interested 
agents we presented a mechanism that is incentive 
compatible with respect to release time deadline length and value 
and that only increases the competitive ratio by one we 
also showed how this mechanism could be simplified when 
k and each agent cannot lie about the length of its job 
we then showed a matching lower bound on the 
competitive ratio that can be achieved by a deterministic mechanism 
that never pays the agents 
several open problems remain in this setting one is to 
determine whether the lower bound can be strengthened by 
removing the restriction of non-negative payments also 
while we feel that it is reasonable to strengthen the 
assumption of knowing the maximum possible ratio of value 
densities k to knowing the actual range of possible value 
densities it would be interesting to determine whether there 
exists a 
√ 
k 
 -competitive mechanism under the 
original assumption finally randomized mechanisms 
provide an unexplored area for future work 
 references 
 a archer j feigenbaum a krishnamurthy 
r sami and s shenker approximation and collusion 
in multicast cost sharing games and economic 
behavior to appear 
 b awerbuch y azar and a meyerson reducing 
truth-telling online mechanisms to online 
optimization proceedings on the th symposium on 
the theory of computing 
 z bar-yossef k hildrum and f wu 
incentive-compatible online auctions for digital goods 
proceedings of the th annual acm-siam 
symposium on discrete algorithms 
 s baruah g koren d mao b mishra 
a raghunathan l rosier d shasha and f wang 
on the competitiveness of on-line real-time task 
scheduling journal of real-time systems 
no - 
 a blum t sandholm and m zinkevich online 
algorithms for market clearing proceedings of the 
 th annual acm-siam symposium on discrete 
algorithms 
 a borodin and r el-yaniv online computation and 
competitive analysis cambridge university press 
 
 r buyya d abramson j giddy and h stockinger 
economic models for resource management and 
scheduling in grid computing the journal of 
concurrency and computation practice and 
experience - 
 n camiel s london n nisan and o regev the 
popcorn project distributed computation over the 
internet in java th international world wide web 
conference 
 j feigenbaum and s shenker distributed algorithmic 
mechanism design recent results and future 
directions proceedings of the th international 
workshop on discrete algorithms and methods for 
mobile computing and communications 
pp - 
 a fiat and g woeginger editors online 
algorithms the state of the art springer verlag 
 e friedman and d parkes pricing wifi at 
starbucksissues in online mechanism design ec 
 r l graham bounds for certain multiprocessor 
anomalies bell system technical journal 
 - 
 b kalyanasundaram and k pruhs speed is as 
powerful as clairvoyance journal of the acm 
 - 
 c koo t lam t ngan and k to on-line 
scheduling with tight deadlines theoretical computer 
science - 
 g koren and d shasha d-over an optimal on-line 
scheduling algorithm for overloaded real-time systems 
siam journal of computing no 
 - 
 r lavi and n nisan competitive analysis of online 
auctions ec 
 a mas-colell m whinston and j green 
microeconomic theory oxford university press 
 n nisan and a ronen algorithmic mechanism 
design games and economic behavior 
 - 
 c papadimitriou algorithms games and the 
internet stoc pp - 
 
finding equilibria in large sequential games of imperfect 
information∗ 
andrew gilpin 
computer science department 
carnegie mellon university 
pittsburgh pa usa 
gilpin cs cmu edu 
tuomas sandholm 
computer science department 
carnegie mellon university 
pittsburgh pa usa 
sandholm cs cmu edu 
abstract 
finding an equilibrium of an extensive form game of 
imperfect information is a fundamental problem in computational 
game theory but current techniques do not scale to large 
games to address this we introduce the ordered game 
isomorphism and the related ordered game isomorphic 
abstraction transformation for a multi-player sequential game of 
imperfect information with observable actions and an 
ordered signal space we prove that any nash equilibrium in 
an abstracted smaller game obtained by one or more 
applications of the transformation can be easily converted into 
a nash equilibrium in the original game we present an 
algorithm gameshrink for abstracting the game using our 
isomorphism exhaustively its complexity is ˜o n 
 where n 
is the number of nodes in a structure we call the signal tree 
it is no larger than the game tree and on nontrivial games 
it is drastically smaller so gameshrink has time and space 
complexity sublinear in the size of the game tree using 
gameshrink we find an equilibrium to a poker game with 
 billion nodes-over four orders of magnitude more than 
in the largest poker game solved previously we discuss 
several electronic commerce applications for gameshrink 
to address even larger games we introduce approximation 
methods that do not preserve equilibrium but nevertheless 
yield ex post provably close-to-optimal strategies 
categories and subject descriptors i artificial 
intelligence f theory of computation j social and 
behavioral sciences economics 
general terms algorithms economics theory 
 introduction 
in environments with more than one agent an agent s 
outcome is generally affected by the actions of the other 
agent s consequently the optimal action of one agent can 
depend on the others game theory provides a normative 
framework for analyzing such strategic situations in 
particular it provides solution concepts that define what rational 
behavior is in such settings the most famous and 
important solution concept is that of nash equilibrium it is 
a strategy profile one strategy for each agent in which no 
agent has incentive to deviate to a different strategy 
however for the concept to be operational we need algorithmic 
techniques for finding an equilibrium 
games can be classified as either games of perfect 
information or imperfect information chess and go are examples 
of the former and until recently most game playing work 
has been on games of this type to compute an optimal 
strategy in a perfect information game an agent traverses 
the game tree and evaluates individual nodes if the agent 
is able to traverse the entire game tree she simply computes 
an optimal strategy from the bottom-up using the principle 
of backward induction 
in computer science terms this is 
done using minimax search often in conjunction with 
αβ-pruning to reduce the search tree size and thus enhance 
speed minimax search runs in linear time in the size of the 
game tree 
the differentiating feature of games of imperfect 
information such as poker is that they are not fully observable 
when it is an agent s turn to move she does not have access 
to all of the information about the world in such games 
the decision of what to do at a point in time cannot 
generally be optimally made without considering decisions at all 
other points in time including ones on other paths of play 
because those other decisions affect the probabilities of 
being at different states at the current point in time thus 
the algorithms for perfect information games do not solve 
games of imperfect information 
for sequential games with imperfect information one could 
try to find an equilibrium using the normal matrix form 
where every contingency plan of the agent is a pure strategy 
for the agent 
unfortunately even if equivalent strategies 
 
this actually yields a solution that satisfies not only the 
nash equilibrium solution concept but a stronger solution 
concept called subgame perfect nash equilibrium 
 
this type of algorithm still does not scale to huge 
trees such as in chess or go but effective game-playing 
agents can be developed even then by evaluating 
intermediate nodes using a heuristic evaluation and then treating 
those nodes as leaves 
 
an -equilibrium in a normal form game with any 
 
are replaced by a single strategy this representation is 
generally exponential in the size of the game tree 
by observing that one needs to consider only sequences of 
moves rather than pure strategies one arrives 
at a more compact representation the sequence form which 
is linear in the size of the game tree 
for -player games 
there is a polynomial-sized in the size of the game tree 
linear programming formulation linear complementarity in 
the non-zero-sum case based on the sequence form such that 
strategies for players and correspond to primal and dual 
variables thus the equilibria of reasonable-sized -player 
games can be computed using this method 
however this approach still yields enormous unsolvable 
optimization problems for many real-world games such as 
poker 
 our approach 
in this paper we take a different approach to tackling 
the difficult problem of equilibrium computation instead 
of developing an equilibrium-finding method per se we 
instead develop a methodology for automatically abstracting 
games in such a way that any equilibrium in the smaller 
 abstracted game corresponds directly to an equilibrium in 
the original game thus by computing an equilibrium in 
the smaller game using any available equilibrium-finding 
algorithm we are able to construct an equilibrium in the 
original game the motivation is that an equilibrium for 
the smaller game can be computed drastically faster than 
for the original game 
to this end we introduce games with ordered signals 
 section a broad class of games that has enough structure 
which we can exploit for abstraction purposes instead of 
operating directly on the game tree something we found 
to be technically challenging we instead introduce the use 
of information filters section which coarsen the 
information each player receives they are used in our analysis 
and abstraction algorithm by operating only in the space 
of filters we are able to keep the strategic structure of the 
game intact while abstracting out details of the game in a 
way that is lossless from the perspective of equilibrium 
finding we introduce the ordered game isomorphism to describe 
strategically symmetric situations and the ordered game 
isomorphic abstraction transformation to take advantange of 
such symmetries section as our main equilibrium 
result we have the following 
constant number of agents can be constructed in 
quasipolynomial time but finding an exact equilibrium is 
ppad-complete even in a -player game the most 
prevalent algorithm for finding an equilibrium in a -agent 
game is lemke-howson but it takes exponentially many 
steps in the worst case for a survey of equilibrium 
computation in -player games see recently 
equilibriumfinding algorithms that enumerate supports i e sets of 
pure strategies that are played with positive probability 
have been shown efficient on many games and efficient 
mixed integer programming algorithms that search in the 
space of supports have been developed for more than 
two players many algorithms have been proposed but they 
currently only scale to very small games 
 
there were also early techniques that capitalized in 
different ways on the fact that in many games the vast majority 
of pure strategies are not played in equilibrium 
 
recently this approach was extended to handle 
computing sequential equilibria as well 
theorem let γ be a game with ordered 
signals and let f be an information filter for γ let 
f be an information filter constructed from f by 
one application of the ordered game isomorphic 
abstraction transformation and let σ be a nash 
equilibrium strategy profile of the induced game 
γf i e the game γ using the filter f if σ is 
constructed by using the corresponding strategies 
of σ then σ is a nash equilibrium of γf 
the proof of the theorem uses an equivalent 
characterization of nash equilibria σ is a nash equilibrium if and 
only if there exist beliefs μ players beliefs about unknown 
information at all points of the game reachable by σ such 
that σ is sequentially rational i e a best response given μ 
where μ is updated using bayes rule we can then use the 
fact that σ is a nash equilibrium to show that σ is a nash 
equilibrium considering only local properties of the game 
we also give an algorithm gameshrink for abstracting 
the game using our isomorphism exhaustively section 
its complexity is ˜o n 
 where n is the number of nodes in 
a structure we call the signal tree it is no larger than the 
game tree and on nontrivial games it is drastically smaller 
so gameshrink has time and space complexity sublinear in 
the size of the game tree we present several algorithmic and 
data structure related speed improvements section 
and we demonstrate how a simple modification to our 
algorithm yields an approximation algorithm section 
 electronic commerce applications 
sequential games of imperfect information are ubiquitous 
for example in negotiation and in auctions often aspects 
of a player s knowledge are not pertinent for deciding what 
action the player should take at a given point in the game 
on the trivial end some aspects of a player s knowledge are 
never pertinent e g whether it is raining or not has no 
bearing on the bidding strategy in an art auction and such 
aspects can be completely left out of the model specification 
however some aspects can be pertinent in certain states 
of the game while they are not pertinent in other states 
and thus cannot be left out of the model completely 
furthermore it may be highly non-obvious which aspects are 
pertinent in which states of the game our algorithm 
automatically discovers which aspects are irrelevant in different 
states and eliminates those aspects of the game resulting 
in a more compact equivalent game representation 
one broad application area that has this property is 
sequential negotiation potentially over multiple issues 
another broad application area is sequential auctions 
 potentially over multiple goods for example in those states of 
a -object auction where bidder a can infer that his 
valuation is greater than that of bidder b bidder a can ignore all 
his other information about b s signals although that 
information would be relevant for inferring b s exact valuation 
furthermore in some states of the auction a bidder might 
not care which exact other bidders have which valuations 
but cares about which valuations are held by the other 
bidders in aggregate ignoring their identities many open-cry 
sequential auction and negotiation mechanisms fall within 
the game model studied in this paper specified in detail 
later as do certain other games in electronic commerce 
such as sequences of take-it-or-leave-it offers 
our techniques are in no way specific to an application 
the main experiment that we present in this paper is on 
 
a recreational game we chose a particular poker game as 
the benchmark problem because it yields an extremely 
complicated and enormous game tree it is a game of imperfect 
information it is fully specified as a game and the data is 
available and it has been posted as a challenge problem 
by others to our knowledge no such challenge 
problem instances have been proposed for electronic commerce 
applications that require solving sequential games 
 rhode island hold em poker 
poker is an enormously popular card game played around 
the world the world series of poker had over 
million dollars in total prize money including million 
for the main event increasingly poker players compete 
in online casinos and television stations regularly 
broadcast poker tournaments poker has been identified as an 
important research area in ai due to the uncertainty 
stemming from opponents cards opponents future actions and 
chance moves among other reasons 
almost since the field s founding game theory has been 
used to analyze different aspects of poker pp 
 - however this work was limited to tiny games that 
could be solved by hand more recently ai researchers have 
been applying the computational power of modern hardware 
to computing game theory-based strategies for larger games 
koller and pfeffer determined solutions to poker games with 
up to nodes using the sequence form and linear 
programming large-scale approximations have been 
developed but those methods do not provide any 
guarantees about the performance of the computed strategies 
furthermore the approximations were designed manually 
by a human expert our approach yields an automated 
abstraction mechanism along with theoretical guarantees on 
the strategies performance 
rhode island hold em was invented as a testbed for 
computational game playing it was designed so that it was 
similar in style to texas hold em yet not so large that 
devising reasonably intelligent strategies would be impossible 
 the rules of rhode island hold em as well as a discussion 
of how rhode island hold em can be modeled as a game 
with ordered signals that is it fits in our model is available 
in an extended version of this paper we applied the 
techniques developed in this paper to find an exact 
 minimax solution to rhode island hold em which has a game 
tree exceeding billion nodes 
applying the sequence form to rhode island hold em 
directly without abstraction yields a linear program with 
 rows and the same number of columns this is 
much too large for current linear programming algorithms 
to handle we used our gameshrink algorithm to reduce 
this with lossless abstraction and it yielded a linear program 
with rows and columns-with non-zero 
coefficients we then applied iterated elimination of 
dominated strategies which further reduced this to 
rows and columns applying iterated 
elimination of dominated strategies without gameshrink yielded 
 rows and columns which still would 
have been prohibitively large to solve gameshrink 
required less than one second to perform the shrinking i e 
to compute all of the ordered game isomorphic abstraction 
transformations using a ghz ibm eserver p with 
 gigabytes of ram the linear program solver actually 
needed gigabytes we solved it in days and hours 
using the interior-point barrier method of cplex version 
 we recently demonstrated our optimal rhode island 
hold em poker player at the aaai- conference and 
it is available for play on-line at http www cs cmu edu 
 gilpin gsi html 
while others have worked on computer programs for 
playing rhode island hold em no optimal strategy has been 
found before this is the largest poker game solved to date 
by over four orders of magnitude 
 games with ordered signals 
we work with a slightly restricted class of games as 
compared to the full generality of the extensive form this class 
which we call games with ordered signals is highly 
structured but still general enough to capture a wide range of 
strategic situations a game with ordered signals consists 
of a finite number of rounds within a round the players 
play a game on a directed tree the tree can be different in 
different rounds the only uncertainty players face stems 
from private signals the other players have received and from 
the unknown future signals in other words players observe 
each others actions but potentially not nature s actions 
in each round there can be public signals announced to all 
players and private signals confidentially communicated 
to individual players for simplicity we assume-as is the 
case in most recreational games-that within each round 
the number of private signals received is the same across 
players this could quite likely be relaxed we also assume 
that the legal actions that a player has are independent of 
the signals received for example in poker the legal 
betting actions are independent of the cards received finally 
the strongest assumption is that there is a partial ordering 
over sets of signals and the payoffs are increasing not 
necessarily strictly in these signals for example in poker this 
partial ordering corresponds exactly to the ranking of card 
hands 
definition a game with ordered signals is a tuple 
γ i g l θ κ γ p ω u where 
 i { n} is a finite set of players 
 g g 
 gr 
 gj 
 
` 
v j 
 ej 
´ 
 is a finite collection 
of finite directed trees with nodes v j 
and edges ej 
 let 
zj 
denote the leaf nodes of gj 
and let nj 
 v denote 
the outgoing neighbors of v ∈ v j 
 gj 
is the stage game 
for round j 
 l l 
 lr 
 lj 
 v j 
\ zj 
→ i indicates which 
player acts chooses an outgoing edge at each internal 
node in round j 
 θ is a finite set of signals 
 κ κ 
 κr 
and γ γ 
 γr 
are vectors of 
nonnegative integers where κj 
and γj 
denote the 
number of public and private signals per player 
respectively revealed in round j each signal θ ∈ θ may 
only be revealed once and in each round every player 
receives the same number of private signals so we 
require 
pr 
j κj 
 nγj 
≤ θ the public information 
revealed in round j is αj 
∈ θκj 
and the public 
information revealed in all rounds up through round j 
is ˜αj 
 
` 
α 
 αj 
´ 
 the private information 
revealed to player i ∈ i in round j is βj 
i ∈ θγj 
and 
the private information revaled to player i ∈ i in all 
rounds up through round j is ˜βj 
i 
` 
β 
i βj 
i 
´ 
 we 
 
also write ˜βj 
 
 
˜βj 
 ˜βj 
n 
 
to represent all private 
information up through round j and 
 
˜β 
j 
i ˜βj 
−i 
 
 
 
˜βj 
 ˜βj 
i− ˜β 
j 
i ˜βj 
i ˜βj 
n 
 
is ˜βj 
with ˜βj 
i replaced 
with ˜β 
j 
i the total information revealed up through 
round j 
 
˜αj 
 ˜βj 
 
 is said to be legal if no signals are 
repeated 
 p is a probability distribution over θ with p θ 
for all θ ∈ θ signals are drawn from θ according 
to p without replacement so if x is the set of signals 
already revealed then 
p x x 
 
p x p 
y ∈x p y 
if x ∈ x 
 if x ∈ x 
 is a partial ordering of subsets of θ and is defined 
for at least those pairs required by u 
 ω 
rs 
j 
zj 
→ {over continue} is a mapping of 
terminal nodes within a stage game to one of two 
values over in which case the game ends or continue 
in which case the game continues to the next round 
clearly we require ω z over for all z ∈ zr 
 note 
that ω is independent of the signals let ωj 
over {z ∈ 
zj 
 ω z over} and ωj 
cont {z ∈ zj 
 ω z 
continue} 
 u u 
 ur 
 uj 
 
j− 
k 
ωk 
cont × ωj 
over × 
j 
k 
θκk 
× 
n 
i 
j 
k 
θγk 
→ rn 
is a utility function such that for 
every j ≤ j ≤ r for every i ∈ i and for every 
˜z ∈ 
j− 
k 
ωk 
cont × ωj 
over at least one of the following two 
conditions holds 
 a utility is signal independent uj 
i ˜z ϑ uj 
i ˜z ϑ 
for all legal ϑ ϑ ∈ 
j 
k 
θκk 
× 
n 
i 
j 
k 
θγk 
 
 b is defined for all legal signals ˜αj 
 ˜βj 
i ˜αj 
 ˜β j 
i 
through round j and a player s utility is increasing 
in her private signals everything else equal 
 
˜αj 
 ˜βj 
i 
 
˜αj 
 ˜β j 
i 
 
 ⇒ 
ui 
 
˜z ˜αj 
 
 
˜βj 
i ˜βj 
−i 
 
≥ ui 
 
˜z ˜αj 
 
 
˜β 
j 
i ˜βj 
−i 
 
 
we will use the term game with ordered signals and the 
term ordered game interchangeably 
 information filters 
in this subsection we define an information filter for 
ordered games instead of completely revealing a signal either 
public or private to a player the signal first passes through 
this filter which outputs a coarsened signal to the player by 
varying the filter applied to a game we are able to obtain 
a wide variety of games while keeping the underlying action 
space of the game intact we will use this when designing 
our abstraction techniques formally an information filter 
is as follows 
definition let γ i g l θ κ γ p ω u be an 
ordered game let sj 
⊆ 
j 
k 
θκk 
× 
j 
k 
θγk 
be the set of 
legal signals i e no repeated signals for one player through 
round j an information filter for γ is a collection f 
f 
 fr 
where each fj 
is a function fj 
 sj 
→ sj 
such that each of the following conditions hold 
 truthfulness ˜αj 
 ˜βj 
i ∈ fj 
 ˜αj 
 ˜βj 
i for all legal ˜αj 
 ˜βj 
i 
 independence the range of fj 
is a partition of sj 
 
 information preservation if two values of a signal are 
distinguishable in round k then they are 
distinguishable fpr each round j k let mj 
 
pj 
l κl 
 γl 
 we 
require that for all legal θ θmk θmj ⊆ θ and 
 θ θmk θmj ⊆ θ 
 θ θmk ∈ fk 
 θ θmk ⇒ 
 θ θmk θmj ∈ fj 
 θ θmk θmj 
a game with ordered signals γ and an information filter 
f for γ defines a new game γf we refer to such games as 
filtered ordered games we are left with the original game 
if we use the identity filter fj 
 
˜αj 
 ˜βj 
i 
 
 
n 
˜αj 
 ˜βj 
i 
o 
 we 
have the following simple but important result 
proposition a filtered ordered game is an extensive 
form game satisfying perfect recall 
a simple proof proceeds by constructing an extensive form 
game directly from the ordered game and showing that it 
satisfies perfect recall in determining the payoffs in a game 
with filtered signals we take the average over all real signals 
in the filtered class weighted by the probability of each real 
signal occurring 
 strategies and nash equilibrium 
we are now ready to define behavior strategies in the 
context of filtered ordered games 
definition a behavior strategy for player i in round 
j of γ i g l θ κ γ p ω u with information filter 
f is a probability distribution over possible actions and is 
defined for each player i each round j and each v ∈ v j 
\zj 
for lj 
 v i 
σj 
i v 
j− 
k 
ωk 
cont×range 
 
fj 
 
→ δ 
n 
w ∈ v j 
 v w ∈ ej 
o 
 
 δ x is the set of probability distributions over a finite set 
x a behavior strategy for player i in round j is σj 
i 
 σj 
i v 
 σj 
i vm 
 for each vk ∈ v j 
\ zj 
where lj 
 vk i 
a behavior strategy for player i in γ is σi 
` 
σ 
i σr 
i 
´ 
 
a strategy profile is σ σ σn a strategy profile with 
σi replaced by σi is σi σ−i σ σi− σi σi σn 
by an abuse of notation we will say player i receives an 
expected payoff of ui σ when all players are playing the 
strategy profile σ strategy σi is said to be player i s best 
response to σ−i if for all other strategies σi for player i we 
have ui σi σ−i ≥ ui σi σ−i σ is a nash equilibrium if 
for every player i σi is a best response for σ−i a nash 
equilibrium always exists in finite extensive form games 
and one exists in behavior strategies for games with perfect 
recall using these observations we have the following 
corollary to proposition 
 
corollary for any filtered ordered game a nash 
equilibrium exists in behavior strateges 
 equilibrium-preserving 
abstractions 
in this section we present our main technique for reducing 
the size of games we begin by defining a filtered signal tree 
which represents all of the chance moves in the game the 
bold edges i e the first two levels of the tree in the game 
trees in figure correspond to the filtered signal trees in 
each game 
definition associated with every ordered game γ 
i g l θ κ γ p ω u and information filter f is a 
filtered signal tree a directed tree in which each node 
corresponds to some revealed filtered signals and edges 
correspond to revealing specific filtered signals the nodes in the 
filtered signal tree represent the set of all possible revealed 
filtered signals public and private at some point in time the 
filtered public signals revealed in round j correspond to the 
nodes in the κj 
levels beginning at level 
pj− 
k 
` 
κk 
 nγk 
´ 
and the private signals revealed in round j correspond to 
the nodes in the nγj 
levels beginning at level 
pj 
k κk 
 
pj− 
k nγk 
 we denote children of a node x as n x in 
addition we associate weights with the edges corresponding 
to the probability of the particular edge being chosen given 
that its parent was reached 
in many games there are certain situations in the game 
that can be thought of as being strategically equivalent to 
other situations in the game by melding these situations 
together it is possible to arrive at a strategically 
equivalent smaller game the next two definitions formalize this 
notion via the introduction of the ordered game isomorphic 
relation and the ordered game isomorphic abstraction 
transformation 
definition two subtrees beginning at internal nodes 
x and y of a filtered signal tree are ordered game 
isomorphic if x and y have the same parent and there is a 
bijection f n x → n y such that for w ∈ n x and 
v ∈ n y v f w implies the weights on the edges x w 
and y v are the same and the subtrees beginning at w and 
v are ordered game isomorphic two leaves 
 corresponding to filtered signals ϑ and ϑ up through round r are 
ordered game isomorphic if for all ˜z ∈ 
r− 
j 
ωj 
cont × ωr 
over 
ur 
 ˜z ϑ ur 
 ˜z ϑ 
definition let γ i g l θ κ γ p ω u be an 
ordered game and let f be an information filter for γ let 
ϑ and ϑ be two nodes where the subtrees in the induced 
filtered signal tree corresponding to the nodes ϑ and ϑ are 
ordered game isomorphic and ϑ and ϑ are at either levelpj− 
k 
` 
κk 
 nγk 
´ 
or 
pj 
k κk 
 
pj− 
k nγk 
for some round 
j the ordered game isomorphic abstraction transformation 
is given by creating a new information filter f 
f j 
 
˜αj 
 ˜βj 
i 
 
 
 
 
 
fj 
 
˜αj 
 ˜βj 
i 
 
if 
 
˜αj 
 ˜βj 
i 
 
 ∈ ϑ ∪ ϑ 
ϑ ∪ ϑ if 
 
˜αj 
 ˜βj 
i 
 
∈ ϑ ∪ ϑ 
figure shows the ordered game isomorphic abstraction 
transformation applied twice to a tiny poker game 
theorem our main equilibrium result shows how the ordered 
game isomorphic abstraction transformation can be used to 
compute equilibria faster 
theorem let γ i g l θ κ γ p ω u be an 
ordered game and f be an information filter for γ let f be 
an information filter constructed from f by one application 
of the ordered game isomorphic abstraction transformation 
let σ be a nash equilibrium of the induced game γf if 
we take σj 
i v 
 
˜z fj 
 
˜αj 
 ˜βj 
i 
 
 σ j 
i v 
 
˜z f j 
 
˜αj 
 ˜βj 
i 
 
 σ is 
a nash equilibrium of γf 
proof for an extensive form game a belief system μ 
assigns a probability to every decision node x such thatp 
x∈h μ x for every information set h a strategy 
profile σ is sequentially rational at h given belief system μ if 
ui σi σ−i h μ ≥ ui τi σ−i h μ 
for all other strategies τi where i is the player who controls 
h a basic result proposition c characterizing nash 
equilibria dictates that σ is a nash equilibrium if and only 
if there is a belief system μ such that for every information 
set h with pr h σ the following two conditions hold 
 c σ is sequentially rational at h given μ and c μ x 
pr x σ 
pr h σ 
for all x ∈ h since σ is a nash equilibrium of γ 
there exists such a belief system μ for γf using μ we will 
construct a belief system μ for γ and show that conditions 
c and c hold thus supporting σ as a nash equilibrium 
fix some player i ∈ i each of i s information sets in some 
round j corresponds to filtered signals fj 
 
˜α∗j 
 ˜β∗j 
i 
 
 history 
in the first j − rounds z zj− ∈ 
j− 
k 
ωk 
cont and 
history so far in round j v ∈ v j 
\ zj 
 let ˜z z zj− v 
represent all of the player actions leading to this 
information set thus we can uniquely specify this information set 
using the information 
 
fj 
 
˜α∗j 
 ˜β∗j 
i 
 
 ˜z 
 
 
each node in an information set corresponds to the 
possible private signals the other players have received denote 
by ˜β some legal 
 fj 
 ˜αj 
 ˜βj 
 fj 
 ˜αj 
 ˜βj 
i− fj 
 ˜αj 
 ˜βj 
i fj 
 ˜αj 
 ˜βj 
n 
in other words there exists ˜αj 
 ˜βj 
 ˜βj 
n such that 
 ˜αj 
 ˜βj 
i ∈ fj 
 ˜α∗j 
 ˜β∗j 
i ˜αj 
 ˜βj 
k ∈ fj 
 ˜αj 
 ˜βj 
k 
for k i and no signals are repeated using such a set 
of signals ˜αj 
 ˜βj 
 ˜βj 
n let ˆβ denote f j 
 ˜αj 
 ˜βj 
 
f j 
 ˜αj 
 ˜βj 
i− f j 
 ˜αj 
 ˜βj 
i f j 
 ˜αj 
 ˜βj 
n we will abuse 
notation and write f j 
−i 
 
ˆβ 
 
 ˆβ we can now compute μ 
directly from μ 
μ 
 
ˆβ fj 
 
˜αj 
 ˜βj 
i 
 
 ˜z 
 
 
 
 
 
μ 
 
ˆβ f j 
 
˜αj 
 ˜βj 
i 
 
 ˜z 
 
if fj 
 
˜αj 
 ˜βj 
i 
 
 f j 
 
˜αj 
 ˜βj 
i 
 
or ˆβ ˆβ 
p∗ 
μ 
 
ˆβ f j 
 
˜αj 
 ˜βj 
i 
 
 ˜z 
 
if fj 
 
˜αj 
 ˜βj 
i 
 
 f j 
 
˜αj 
 ˜βj 
i 
 
and ˆβ ˆβ 
 
j 
j 
j k 
k 
k 
k 
c b 
c b f b 
f b 
c b 
c b f b 
f b 
c b 
c 
f b 
b bf 
c b 
c 
f b 
b bf 
c b 
c b f b 
f b 
c b 
c bf b 
f b 
c b 
c 
f b 
b bf 
c b 
c 
f b 
b bf 
c b 
c 
f b 
b bf 
c b 
c 
f b 
b bf 
c b 
c b f b 
f b 
c b 
c b f b 
f b 
 
 - 
- 
- 
- 
- 
- 
- 
- - 
- - 
- 
- 
- 
- 
- 
- 
- - 
- - 
- 
- 
- 
 
 
 
 
 
- 
- 
- - 
- 
- - 
- 
- - 
- 
- 
 
 
 
 
 
 
 
 
j k k j j k j j k 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
{{j } {j } {k } {k }} 
{{j j } {k } {k }} 
c b 
c bf b 
f b 
c b 
c 
f b 
b bf 
c b 
c b f b 
f b 
j j k k 
 
 
c b 
c 
f b 
b bf 
c b 
c bf b 
f b 
c b 
c bf b 
f b 
c b 
c b f b 
f b 
j j 
k 
k 
 
 
 
 
j j k j j k 
 
 - 
- 
- 
- - 
- 
- 
- 
- - 
- 
- 
 
 
 
 
 
 
- 
- - 
- 
 
 
 
 
 
- 
- - 
- 
 
 
 
 
 
c b 
c b f b 
f b 
- 
- 
 
c b 
b f b 
f b 
- 
- - 
- 
- 
c b 
c bf b 
f b 
 
 - 
- 
c b 
c bf b 
f b 
j j 
j j j j k k 
k k 
k k 
- 
- 
 
 
 
 
 
 
 
{{j j } {k k }} 
 
 
 
 
 
 
 
 
 
 
 
figure gameshrink applied to a tiny two-person four-card two jacks and two kings poker game next 
to each game tree is the range of the information filter f dotted lines denote information sets which are 
labeled by the controlling player open circles are chance nodes with the indicated transition probabilities 
the root node is the chance node for player s card and the next level is for player s card the payment 
from player to player is given below each leaf in this example the algorithm reduces the game tree from 
 nodes to nodes 
where p∗ 
 
pr ˆβ f j 
 ˜αj 
 ˜β 
j 
i 
pr ˆβ f j ˜αj ˜β 
j 
i 
 the following three claims 
show that μ as calculated above supports σ as a nash 
equilibrium 
claim μ is a valid belief system for γf 
claim for all information sets h with pr h σ 
μ x pr x σ 
pr h σ 
for all x ∈ h 
claim for all information sets h with pr h σ 
σ is sequentially rational at h given μ 
the proofs of claims - are in an extended version of this 
paper by claims and we know that condition c 
holds by claim we know that condition c holds thus 
σ is a nash equilibrium 
 nontriviality of generalizing beyond this 
model 
our model does not capture general sequential games of 
imperfect information because it is restricted in two ways 
 as discussed above there is a special structure 
connecting the player actions and the chance actions for one 
the players are assumed to observe each others actions but 
nature s actions might not be publicly observable and 
there is a common ordering of signals in this subsection we 
show that removing either of these conditions can make our 
technique invalid 
first we demonstrate a failure when removing the first 
assumption consider the game in figure 
nodes a and 
b are in the same information set have the same parent 
 chance node have isomorphic subtrees with the same 
payoffs and nodes c and d also have similar structural 
properties by merging the subtrees beginning at a and b we get 
the game on the right in figure in this game player 
 s only nash equilibrium strategy is to play left but in 
the original game player knows that node c will never be 
reached and so should play right in that information set 
 
 
 
 
 
 
 
- 
 
 
 
 
a b 
 - 
c 
d 
figure example illustrating difficulty in 
developing a theory of equilibrium-preserving abstractions 
for general extensive form games 
removing the second assumption that the utility 
functions are based on a common ordering of signals can also 
cause failure consider a simple three-card game with a 
deck containing two jacks j and j and a king k 
where player s utility function is based on the ordering 
 
we thank albert xin jiang for providing this example 
 
k j ∼ j but player s utility function is based on the 
ordering j k j it is easy to check that in the 
abstracted game where player treats j and j as being 
equivalent the nash equilibrium does not correspond to 
a nash equilibrium in the original game 
 gameshrink an efficient 
algorithm for computing ordered 
game isomorphic abstraction 
transformations 
this section presents an algorithm gameshrink for 
conducting the abstractions it only needs to analyze the signal 
tree discussed above rather than the entire game tree 
we first present a subroutine that gameshrink uses it 
is a dynamic program for computing the ordered game 
isomorphic relation again it operates on the signal tree 
algorithm orderedgameisomorphic γ ϑ ϑ 
 if ϑ and ϑ have different parents then return false 
 if ϑ and ϑ are both leaves of the signal tree 
 a if ur 
 ϑ ˜z ur 
 ϑ ˜z for all ˜z ∈ 
r− 
j 
ωj 
cont × 
ωr 
over then return true 
 b otherwise return false 
 create a bipartite graph gϑ ϑ v v e with v 
n ϑ and v n ϑ 
 for each v ∈ v and v ∈ v 
if orderedgameisomorphic γ v v 
create edge v v 
 return true if gϑ ϑ has a perfect matching otherwise 
return false 
by evaluating this dynamic program from bottom to top 
algorithm determines in time polynomial in the size of 
the signal tree whether or not any pair of equal depth 
nodes x and y are ordered game isomorphic we can 
further speed up this computation by only examining nodes 
with the same parent since we know from step that 
no nodes with different parents are ordered game 
isomorphic the test in step a can be computed in o time 
by consulting the relation from the specification of the 
game each call to orderedgameisomorphic performs at 
most one perfect matching computation on a bipartite graph 
with o θ nodes and o θ 
 edges recall that θ is the 
set of signals using the ford-fulkerson algorithm for 
finding a maximal matching this takes o θ 
 time let 
s be the maximum number of signals possibly revealed in 
the game e g in rhode island hold em s because 
each of the two players has one card in the hand plus there 
are two cards on the table the number of nodes n in 
the signal tree is o θ s 
 the dynamic program visits each 
node in the signal tree with each visit requiring o θ 
 
calls to the orderedgameisomorphic routine so it takes 
o θ s 
 θ 
 θ 
 o θ s 
 time to compute the entire 
ordered game isomorphic relation 
while this is exponential in the number of revealed 
signals we now show that it is polynomial in the size of the 
signal tree-and thus polynomial in the size of the game tree 
 
we thank an anonymous person for this example 
because the signal tree is smaller than the game tree the 
number of nodes in the signal tree is 
n 
sx 
i 
iy 
j 
 θ − j 
 each term in the summation corresponds to the number of 
nodes at a specific depth of the tree the number of leaves 
is 
sy 
j 
 θ − j 
 θ 
s 
 
s 
which is a lower bound on the number of nodes for large 
 θ we can use the relation 
`n 
k 
´ 
∼ nk 
k 
to get 
 θ 
s 
 
s ∼ 
„ 
 θ s 
s 
 
s θ s 
and thus the number of leaves in the signal tree is ω θ s 
 
thus o θ s 
 o n θ 
 which proves that we can 
indeed compute the ordered game isomorphic relation in time 
polynomial in the number of nodes n of the signal tree 
the algorithm often runs in sublinear time and space in 
the size of the game tree because the signal tree is 
significantly smaller than the game tree in most nontrivial games 
 note that the input to the algorithm is not an explicit game 
tree but a specification of the rules so the algorithm does 
not need to read in the game tree see figure in 
general if an ordered game has r rounds and each round s stage 
game has at least b nonterminal leaves then the size of the 
signal tree is at most 
br of the size of the game tree for 
example in rhode island hold em the game tree has 
billion nodes while the signal tree only has 
given the orderedgameisomorphic routine for 
determining ordered game isomorphisms in an ordered game we 
are ready to present the main algorithm gameshrink 
algorithm gameshrink γ 
 initialize f to be the identity filter for γ 
 for j from to r 
for each pair of sibling nodes ϑ ϑ at either levelpj− 
k 
` 
κk 
 nγk 
´ 
or 
pj 
k κk 
 
pj− 
k nγk 
in the 
filtered according to f signal tree 
if orderedgameisomorphic γ ϑ ϑ then 
fj 
 ϑ ← fj 
 ϑ ← fj 
 ϑ ∪ fj 
 ϑ 
 output f 
given as input an ordered game γ gameshrink applies 
the shrinking ideas presented above as aggressively as 
possible once it finishes there are no contractible nodes since it 
compares every pair of nodes at each level of the signal tree 
and it outputs the corresponding information filter f the 
correctness of gameshrink follows by a repeated application 
of theorem thus we have the following result 
theorem gameshrink finds all ordered game 
isomorphisms and applies the associated ordered game isomorphic 
abstraction transformations furthermore for any nash 
equilibrium σ of the abstracted game the strategy profile 
constructed for the original game from σ is a nash equilibrium 
the dominating factor in the run time of gameshrink is 
in the rth 
iteration of the main for-loop there are at most 
 
` θ 
s 
´ 
s nodes at this level where we again take s to be the 
maximum number of signals possibly revealed in the game 
thus the inner for-loop executes o 
„` θ 
s 
´ 
s 
 
 
times as 
discussed in the next subsection we use a union-find data 
structure to represent the information filter f each 
iteration of the inner for-loop possibly performs a union 
operation on the data structure performing m operations on 
a union-find data structure containing n elements takes 
o α m n amortized time per operation where α m n 
is the inverse ackermann s function which grows 
extremely slowly thus the total time for gameshrink is 
o 
„` θ 
s 
´ 
s 
 
α 
„` θ 
s 
´ 
s 
 
 θ s 
 
 by the inequality 
`n 
k 
´ 
≤ nk 
k 
 this is o 
` 
 θ s 
 
α 
` 
 θ s 
 
 θ s 
´´ 
 again 
although this is exponential in s it is ˜o n 
 where n is the 
number of nodes in the signal tree furthermore 
gameshrink tends to actually run in sublinear time and space in 
the size of the game tree because the signal tree is 
significantly smaller than the game tree in most nontrivial games 
as discussed above 
 efficiency enhancements 
we designed several speed enhancement techniques for 
gameshrink and all of them are incorporated into our 
implementation one technique is the use of the union-find 
data structure for storing the information filter f this 
data structure uses time almost linear in the number of 
operations initially each node in the signalling tree is 
its own set this corresponds to the identity information 
filter when two nodes are contracted they are joined into 
a new set upon termination the filtered signals for the 
abstracted game correspond exactly to the disjoint sets in 
the data structure this is an efficient method of recording 
contractions within the game tree and the memory 
requirements are only linear in the size of the signal tree 
determining whether two nodes are ordered game 
isomorphic requires us to determine if a bipartite graph has a 
perfect matching we can eliminate some of these computations 
by using easy-to-check necessary conditions for the ordered 
game isomorphic relation to hold one such condition is to 
check that the nodes have the same number of chances as 
being ranked according to higher than lower than and 
the same as the opponents we can precompute these 
frequencies for every game tree node this substantially speeds 
up gameshrink and we can leverage this database across 
multiple runs of the algorithm for example when trying 
different abstraction levels see next section the indices 
for this database depend on the private and public signals 
but not the order in which they were revealed and thus 
two nodes may have the same corresponding database 
entry this makes the database significantly more compact 
 for example in texas hold em the database is reduced by 
a factor 
` 
 
´` 
 
´` 
 
´ 
 
` 
 
´ 
 we store the histograms 
in a -dimensional database the first dimension is indexed 
by the private signals the second by the public signals the 
problem of computing the index in either one of the 
dimensions is exactly the problem of computing a bijection 
between all subsets of size r from a set of size n and 
integers in 
ˆ 
 
`n 
r 
´ 
− 
˜ 
 we efficiently compute this using 
the subsets colexicographical ordering let {c cr} 
ci ∈ { n − } denote the r signals and assume that 
ci ci we compute a unique index for this set of signals 
as follows index c cr 
pr 
i 
`ci 
i 
´ 
 
 approximation methods 
some games are too large to compute an exact 
equilibrium even after using the presented abstraction technique 
this section discusses general techniques for computing 
approximately optimal strategy profiles for a two-player game 
we can always evaluate the worst-case performance of a 
strategy thus providing some objective evaluation of the 
strength of the strategy to illustrate this suppose we know 
player s planned strategy for some game we can then fix 
the probabilities of player s actions in the game tree as 
if they were chance moves then player is faced with a 
single-agent decision problem which can be solved 
bottomup maximizing expected payoff at every node thus we can 
objectively determine the expected worst-case performance 
of player s strategy this will be most useful when we 
want to evaluate how well a given strategy performs when 
we know that it is not an equilibrium strategy a 
variation of this technique may also be applied in n-person games 
where only one player s strategies are held fixed this 
technique provides ex post guarantees about the worst-case 
performance of a strategy and can be used independently of 
the method that is used to compute the strategies 
 state-space approximations 
by slightly modifying gameshrink we can obtain an 
algorithm that yields even smaller game trees at the expense 
of losing the equilibrium guarantees of theorem instead 
of requiring the payoffs at terminal nodes to match exactly 
we can instead compute a penalty that increases as the 
difference in utility between two nodes increases 
there are many ways in which the penalty function could 
be defined and implemented one possibility is to create 
edge weights in the bipartite graphs used in algorithm 
and then instead of requiring perfect matchings in the 
unweighted graph we would instead require perfect matchings 
with low cost i e only consider two nodes to be ordered 
game isomorphic if the corresponding bipartite graph has a 
perfect matching with cost below some threshold thus 
with this threshold as a parameter we have a knob to turn 
that in one extreme threshold yields an optimal 
abstraction and in the other extreme threshold ∞ yields 
a highly abstracted game this would in effect restrict 
players to ignoring all signals but still observing actions this 
knob also begets an anytime algorithm one can solve 
increasingly less abstracted versions of the game and evaluate 
the quality of the solution at every iteration using the ex post 
method discussed above 
 algorithmic approximations 
in the case of two-player zero-sum games the 
equilibrium computation can be modeled as a linear program lp 
which can in turn be solved using the simplex method this 
approach has inherent features which we can leverage into 
desirable properties in the context of solving games 
in the lp primal solutions correspond to strategies of 
player and dual solutions correspond to strategies of player 
 there are two versions of the simplex method the primal 
simplex and the dual simplex the primal simplex maintains 
primal feasibility and proceeds by finding better and better 
primal solutions until the dual solution vector is feasible 
 
at which point optimality has been reached analogously 
the dual simplex maintains dual feasibility and proceeds by 
finding increasingly better dual solutions until the primal 
solution vector is feasible the dual simplex method can 
be thought of as running the primal simplex method on the 
dual problem thus the primal and dual simplex 
methods serve as anytime algorithms for a given abstraction 
for players and respectively at any point in time they 
can output the best strategies found so far 
also for any feasible solution to the lp we can get bounds 
on the quality of the strategies by examining the primal and 
dual solutions when using the primal simplex method 
dual solutions may be read off of the lp tableau every 
feasible solution of the dual yields an upper bound on the 
optimal value of the primal and vice versa p thus 
without requiring further computation we get lower bounds 
on the expected utility of each agent s strategy against that 
agent s worst-case opponent 
one problem with the simplex method is that it is not a 
primal-dual algorithm that is it does not maintain both 
primal and dual feasibility throughout its execution in fact 
it only obtains primal and dual feasibility at the very end of 
execution in contrast there are interior-point methods for 
linear programming that maintain primal and dual 
feasibility throughout the execution for example many 
interiorpoint path-following algorithms have this property ch 
 we observe that running such a linear programming 
method yields a method for finding -equilibria i e 
strategy profiles in which no agent can increase her expected 
utility by more than by deviating a threshold on can also 
be used as a termination criterion for using the method as 
an anytime algorithm furthermore interior-point methods 
in this class have polynomial-time worst-case run time as 
opposed to the simplex algorithm which takes exponentially 
many steps in the worst case 
 related research 
functions that transform extensive form games have been 
introduced in contrast to our work those 
approaches were not for making the game smaller and easier 
to solve the main result is that a game can be derived 
from another by a sequence of those transformations if and 
only if the games have the same pure reduced normal form 
the pure reduced normal form is the extensive form game 
represented as a game in normal form where duplicates of 
pure strategies i e ones with identical payoffs are removed 
and players essentially select equivalence classes of 
strategies an extension to that work shows a similar result 
but for slightly different transformations and mixed reduced 
normal form games modern treatments of this prior 
work on game transformations exist ch 
the recent notion of weak isomorphism in extensive form 
games is related to our notion of restricted game 
isomorphism the motivation of that work was to justify solution 
concepts by arguing that they are invariant with respect 
to isomorphic transformations indeed the author shows 
among other things that many solution concepts including 
nash perfect subgame perfect and sequential equilibrium 
are invariant with respect to weak isomorphisms however 
that definition requires that the games to be tested for weak 
isomorphism are of the same size our focus is totally 
different we find strategically equivalent smaller games also 
their paper does not provide algorithms 
abstraction techniques have been used in artificial 
intelligence research before in contrast to our work most but 
not all research involving abstraction has been for 
singleagent problems e g furthermore the use of 
abstraction typically leads to sub-optimal solutions unlike 
the techniques presented in this paper which yield 
optimal solutions a notable exception is the use of abstraction 
to compute optimal strategies for the game of sprouts 
however a significant difference to our work is that sprouts 
is a game of perfect information 
one of the first pieces of research to use abstraction in 
multi-agent settings was the development of partition search 
which is the algorithm behind gib the world s first 
expertlevel computer bridge player in contrast to other 
game tree search algorithms which store a particular game 
position at each node of the search tree partition search 
stores groups of positions that are similar typically the 
similarity of two game positions is computed by ignoring the 
less important components of each game position and then 
checking whether the abstracted positions are similar-in 
some domain-specific expert-defined sense-to each other 
partition search can lead to substantial speed improvements 
over α-β-search however it is not game theory-based it 
does not consider information sets in the game tree and 
thus does not solve for the equilibrium of a game of 
imperfect information such as poker 
another difference is that 
the abstraction is defined by an expert human while our 
abstractions are determined automatically 
there has been some research on the use of abstraction 
for imperfect information games most notably billings et 
al describe a manually constructed abstraction for texas 
hold em poker and include promising results against expert 
players however this approach has significant drawbacks 
first it is highly specialized for texas hold em second 
a large amount of expert knowledge and effort was used in 
constructing the abstraction third the abstraction does 
not preserve equilibrium even if applied to a smaller game 
it might not yield a game-theoretic equilibrium promising 
ideas for abstraction in the context of general extensive form 
games have been described in an extended abstract but 
to our knowledge have not been fully developed 
 conclusions and discussion 
we introduced the ordered game isomorphic abstraction 
transformation and gave an algorithm gameshrink for 
abstracting the game using the isomorphism exhaustively we 
proved that in games with ordered signals any nash 
equilibrium in the smaller abstracted game maps directly to a 
nash equilibrium in the original game 
the complexity of gameshrink is ˜o n 
 where n is the 
number of nodes in the signal tree it is no larger than the 
game tree and on nontrivial games it is drastically smaller 
so gameshrink has time and space complexity sublinear in 
 
bridge is also a game of imperfect information and 
partition search does not find the equilibrium for that 
game either instead partition search is used in 
conjunction with statistical sampling to simulate the uncertainty 
in bridge there are also other bridge programs that use 
search techniques for perfect information games in 
conjunction with statistical sampling and expert-defined 
abstraction such non-game-theoretic techniques are unlikely 
to be competitive in poker because of the greater importance 
of information hiding and bluffing 
 
the size of the game tree using gameshrink we found 
a minimax equilibrium to rhode island hold em a poker 
game with billion nodes in the game tree-over four 
orders of magnitude more than in the largest poker game 
solved previously 
to further improve scalability we introduced an 
approximation variant of gameshrink which can be used as an 
anytime algorithm by varying a parameter that controls 
the coarseness of abstraction we also discussed how in 
a two-player zero-sum game linear programming can be 
used in an anytime manner to generate approximately 
optimal strategies of increasing quality the method also yields 
bounds on the suboptimality of the resulting strategies we 
are currently working on using these techniques for full-scale 
 -player limit texas hold em poker a highly popular card 
game whose game tree has about 
nodes that game 
tree size has required us to use the approximation version of 
gameshrink as well as round-based abstraction 
 references 
 w ackermann zum hilbertschen aufbau der reellen zahlen 
math annalen - 
 d applegate g jacobson and d sleator computer analysis 
of sprouts technical report cmu-cs- - 
 r bellman and d blackwell some two-person games involving 
bluffing pnas - 
 d billings n burch a davidson r holte j schaeffer 
t schauenberg and d szafron approximating game-theoretic 
optimal strategies for full-scale poker in ijcai 
 d billings a davidson j schaeffer and d szafron the 
challenge of poker artificial intelligence - 
 b bollob´as combinatorics cambridge university press 
 a casajus weak isomorphism of extensive games 
mathematical social sciences - 
 x chen and x deng settling the complexity of -player nash 
equilibrium eccc report no 
 v chv´atal linear programming w h freeman co 
 b p de bruin game transformations and game equivalence 
technical note x- - university of amsterdam institute 
for logic language and computation 
 s elmes and p j reny on the strategic equivalence of 
extensive form games j of economic theory - 
 l r ford jr and d r fulkerson flows in networks 
princeton university press 
 a gilpin and t sandholm finding equilibria in large 
sequential games of imperfect information technical report 
cmu-cs- - carnegie mellon university 
 a gilpin and t sandholm optimal rhode island hold em 
poker in aaai pages - pittsburgh pa usa 
 a gilpin and t sandholm a competitive texas hold em 
poker player via automated abstraction and real-time 
equilibrium computation mimeo 
 a gilpin and t sandholm a texas hold em poker player 
based on automated abstraction and real-time equilibrium 
computation in aamas hakodate japan 
 m l ginsberg partition search in aaai pages - 
portland or 
 m l ginsberg gib steps toward an expert-level 
bridge-playing program in ijcai stockholm sweden 
 s govindan and r wilson a global newton method to 
compute nash equilibria j of econ theory - 
 c a knoblock automatically generating abstractions for 
planning artificial intelligence - 
 e kohlberg and j -f mertens on the strategic stability of 
equilibria econometrica - 
 d koller and n megiddo the complexity of two-person 
zero-sum games in extensive form games and economic 
behavior - oct 
 d koller and n megiddo finding mixed strategies with small 
supports in extensive form games international journal of 
game theory - 
 d koller n megiddo and b von stengel efficient 
computation of equilibria for extensive two-person games 
games and economic behavior - 
 d koller and a pfeffer representations and solutions for 
game-theoretic problems artificial intelligence - 
july 
 d m kreps and r wilson sequential equilibria 
econometrica - 
 h w kuhn extensive games pnas - 
 h w kuhn a simplified two-person poker in contributions 
to the theory of games volume of annals of mathematics 
studies pages - princeton university press 
 h w kuhn extensive games and the problem of information 
in contributions to the theory of games volume of annals 
of mathematics studies pages - princeton 
university press 
 c lemke and j howson equilibrium points of bimatrix 
games journal of the society for industrial and applied 
mathematics - 
 r lipton e markakis and a mehta playing large games 
using simple strategies in acm-ec pages - 
 c -l liu and m wellman on state-space abstraction for 
anytime evaluation of bayesian networks sigart bulletin 
 - 
 a mas-colell m whinston and j r green microeconomic 
theory oxford university press 
 r d mckelvey and a mclennan computation of equilibria 
in finite games in handbook of computational economics 
volume pages - elsevier 
 p b miltersen and t b sørensen computing sequential 
equilibria for two-player games in soda pages - 
 j nash equilibrium points in n-person games proc of the 
national academy of sciences - 
 j f nash and l s shapley a simple three-person poker 
game in contributions to the theory of games volume 
pages - princeton university press 
 a perea rationality in extensive form games kluwer 
academic publishers 
 a pfeffer d koller and k takusagawa state-space 
approximations for extensive form games july talk given 
at the first international congress of the game theory 
society bilbao spain 
 r porter e nudelman and y shoham simple search 
methods for finding a nash equilibrium in aaai pages 
 - san jose ca usa 
 i romanovskii reduction of a game with complete memory to 
a matrix game soviet mathematics - 
 t sandholm and a gilpin sequences of take-it-or-leave-it 
offers near-optimal auctions without full valuation revelation 
in aamas hakodate japan 
 t sandholm a gilpin and v conitzer mixed-integer 
programming methods for finding nash equilibria in aaai 
pages - pittsburgh pa usa 
 r savani and b von stengel exponentially many steps for 
finding a nash equilibrium in a bimatrix game in focs pages 
 - 
 r selten spieltheoretische behandlung eines oligopolmodells 
mit nachfragetr¨agheit zeitschrift f¨ur die gesamte 
staatswissenschaft - 
 r selten evolutionary stability in extensive two-person games 
- correction and further development mathematical social 
sciences - 
 j shi and m littman abstraction methods for game theoretic 
poker in computers and games pages - 
springer-verlag 
 s j j smith d s nau and t throop computer bridge a 
big win for ai planning ai magazine - 
 r e tarjan efficiency of a good but not linear set union 
algorithm journal of the acm - 
 f thompson equivalence of games in extensive form rand 
memo rm- the rand corporation jan 
 j von neumann and o morgenstern theory of games and 
economic behavior princeton university press 
 b von stengel efficient computation of behavior strategies 
games and economic behavior - 
 b von stengel computing equilibria for two-person games in 
handbook of game theory volume north holland 
amsterdam 
 r wilson computing equilibria of two-person games from the 
extensive form management science - 
 s j wright primal-dual interior-point methods siam 
 
 
cost sharing in a job scheduling problem 
using the shapley value 
debasis mishra 
center for operations research and 
econometrics core 
universit´e catholique de louvain 
louvain la neuve belgium 
mishra core ucl ac be 
bharath rangarajan 
center for operations research and 
econometrics core 
universit´e catholique de louvain 
louvain la neuve belgium 
rangarajan core ucl ac be 
abstract 
a set of jobs need to be served by a single server which can 
serve only one job at a time jobs have processing times 
and incur waiting costs linear in their waiting time the 
jobs share their costs through compensation using monetary 
transfers we characterize the shapley value rule for this 
model using fairness axioms our axioms include a bound 
on the cost share of jobs in a group efficiency and some 
independence properties on the the cost share of a job 
categories and subject descriptors 
j social and behaviorial sciences economics 
general terms 
economics theory 
 introduction 
a set of jobs need to be served by a server the server can 
process only one job at a time each job has a finite 
processing time and a per unit time waiting cost efficient ordering 
of this queue directs us to serve the jobs in increasing 
order of the ratio of per unit time waiting cost and processing 
time to compensate for waiting by jobs monetary 
transfers to jobs are allowed how should the jobs share the cost 
equitably amongst themselves through transfers 
the problem of fair division of costs among agents in a 
queue has many practical applications for example 
computer programs are regularly scheduled on servers data are 
scheduled to be transmitted over networks jobs are 
scheduled in shop-floor on machines and queues appear in many 
public services post offices banks study of queueing 
problems has attracted economists for a long time 
cost sharing is a fundamental problem in many settings 
on the internet internet can be seen as a common resource 
shared by many users and the cost incured by using the 
resource needs to be shared in an equitable manner the 
current surge in cost sharing literature from computer 
scientists validate this claim internet has many 
settings in which our model of job scheduling appears and 
the agents waiting in a queue incur costs jobs scheduled on 
servers queries answered from a database data scheduled 
to be transmitted over a fixed bandwidth network etc we 
hope that our analysis will give new insights on cost sharing 
problems of this nature 
recently there has been increased interest in cost 
sharing methods with submodular cost functions 
while many settings do have submodular cost functions for 
example multi-cast transmission games while the cost 
function of our game is supermodular also such literature 
typically does not assume budget-balance transfers adding 
up to zero while it is an inherent feature of our model 
a recent paper by maniquet is the closest to our model 
and is the motivation behind our work 
 maniquet 
studies a model where he assumes all processing times are 
unity for such a model he characterizes the shapley value 
rule using classical fairness axioms chun interprets the 
worth of a coalition of jobs in a different manner for the same 
model and derives a reverse rule chun characterizes this 
rule using similar fairness axioms chun also studies the 
envy properties of these rules moulin studies the 
queueing problem from a strategic point view when per unit 
waiting costs are unity moulin introduces new concepts in 
the queueing settings such as splitting and merging of jobs 
and ways to prevent them 
another stream of literature is on sequencing games 
first introduced by curiel et al for a detailed survey 
refer to curiel et al curiel et al defined sequencing 
games similar to our model but in which an initial ordering 
of jobs is given besides their notion of worth of a coalition 
is very different from the notions studied in maniquet 
and chun these are the notions used in our work too 
the particular notion of the worth of a coalition makes the 
sequencing game of curiel et al convex whereas our 
game is not convex and does not assume the presence of 
any initial order in summary the focus of this stream of 
 
the authors thank fran¸cois maniquet for several fruitful 
discussions 
 
research is how to share the savings in costs from the 
initial ordering to the optimal ordering amongst jobs also see 
hamers et al curiel et al recently klijn and 
s´anchez considered sequencing games without any 
initial ordering of jobs they take two approaches to define 
the worth of coalitions one of their approaches called the 
tail game is related to the reverse rule of chun in the 
tail game jobs in a coalition are served after the jobs not in 
the coalition are served klijn and s´anchez showed that 
the tail game is balanced further they provide expressions 
for the shapley value in tail game in terms of marginal 
vectors and reversed marginal vectors we provide a simpler 
expression of the shapley value in the tail game 
generalizing the result in chun klijn and s´anchez study the 
core of this game in detail 
strategic aspects of queueing problems have also been 
researched mitra studies the first best implementation 
in queueing models with generic cost functions first best 
implementation means that there exists an efficient 
mechanism in which jobs in the queue have a dominant strategy 
to reveal their true types and their transfers add up to zero 
suijs shows that if waiting costs of jobs are linear then 
first best implementation is possible mitra shows that 
among a more general class of queueing problems first best 
implementation is possible if and only if the cost is linear 
for another queueing model mitra shows that first best 
implementation is possible if and only if the cost function 
satisfies a combinatorial property and an independence 
property moulin studies strategic concepts such as 
splitting and merging in queueing problems with unit per unit 
waiting costs 
the general cost sharing literature is vast and has a long 
history for a good survey we refer to from the 
seminal work of shapley to recent works on cost sharing in 
multi-cast transmission and optimization problems 
this area has attracted economists computer scientists and 
operations researchers 
 our contribution 
ours is the first model which considers cost sharing when 
both processing time and per unit waiting cost of jobs are 
present we take a cooperative game theory approach and 
apply the classical shapley value rule to the problem we 
show that the shapley value rule satisfies many intuitive 
fairness axioms due to two dimensional nature of our model 
and one dimensional nature of maniquet s model his 
axioms are insufficient to characterize the shapley value in 
our setting we introduce axioms such as independece of 
preceding jobs unit waiting cost and independence of following 
jobs processing time a key axiom that we introduce gives 
us a bound on cost share of a job in a group of jobs which 
have the same ratio of unit time waiting cost and 
processing time these jobs can be ordered in any manner between 
themseleves in an efficient ordering if such a group consists 
of just one job then the axiom says that such a job should 
at least pay his own processing cost i e the cost it would 
have incurred if it was the only job in the queue if there 
are multiple jobs in such a group the probability of any two 
jobs from such a group inflicting costs on each other is same 
 
 
 in an efficient ordering depending on the ordering 
selected one job inflicts cost on the other our fairness axiom 
says that each job should at least bear such expected costs 
we characterize the shapley value rule using these fairness 
axioms we also extend the envy results in to our setting 
and discuss a class of reasonable cost sharing mechanisms 
 the model 
there are n jobs that need to be served by one server 
which can process only one job at a time the set of jobs 
are denoted as n { n} σ n → n is an ordering of 
jobs in n and σi denotes the position of job i in the ordering 
σ given an ordering σ define fi σ {j ∈ n σi σj} 
and pi σ {j ∈ n σi σj} 
every job i is identified by two parameters pi θi pi 
is the processing time and θi is the cost per unit waiting 
time of job i thus a queueing problem is defined by a list 
q n p θ ∈ q where q is the set of all possible lists we 
will denote γi θi 
pi 
 given an ordering of jobs σ the cost 
incurred by job i is given by 
ci σ piθi θi 
 
j∈pi σ 
pj 
the total cost incurred by all jobs due to an ordering σ 
can be written in two ways i by summing the cost incurred 
by every job and ii by summing the costs inflicted by a job 
on other jobs with their own processing cost 
c n σ 
 
i∈n 
ci σ 
 
i∈n 
piθi 
 
i∈n 
 θi 
 
j∈pi σ 
pj¢ 
 
 
i∈n 
piθi 
 
i∈n 
 pi 
 
j∈fi σ 
θj¢ 
an efficient ordering σ∗ 
is the one which minimizes the 
total cost incurred by all jobs so c n σ∗ 
 ≤ c n σ ∀ σ ∈ 
σ to achieve notational simplicity we will write the total 
cost in an efficient ordering of jobs from n as c n 
whenever it is not confusing sometimes we will deal with only 
a subset of jobs s ⊆ n the ordering σ will then be 
defined on jobs in s only and we will write the total cost from 
an efficient ordering of jobs in s as c s the following 
lemma shows that jobs are ordered in decreasing γ in an 
efficient ordering this is also known as the weighted shortest 
processing time rule first introduced by smith 
lemma for any s ⊆ n let σ∗ 
be an efficient ordering 
of jobs in s for every i j i j ∈ s if σ∗ 
i σ∗ 
j then 
γi ≤ γj 
proof assume for contradiction that the statment of 
the lemma is not true this means we can find two 
consecutive jobs i j ∈ s σ∗ 
i σ∗ 
j such that γi γj 
define a new ordering σ by interchanging i and j in σ∗ 
 
the costs to jobs in s \ {i j} is not changed from σ∗ 
to σ 
the difference between total costs in σ∗ 
and σ is given by 
c s σ − c s σ∗ 
 θjpi − θipj from efficiency we get 
θjpi − θipj ≥ this gives us γj ≥ γi which is a 
contradiction 
an allocation for q n p θ ∈ q has two components 
an ordering σ and a transfer ti for every job i ∈ n ti 
denotes the payment received by job i given a transfer ti 
and an ordering σ the cost share of job i is defined as 
πi ci σ − ti θi 
 
j∈n σj ≤σi 
pj − ti 
 
an allocation σ t is efficient for q n p θ whenever 
σ is an efficient ordering and £i∈n ti the set of 
efficient orderings of q is denoted as σ∗ 
 q and σ∗ 
 q will be 
used to refer to a typical element of the set the following 
straightforward lemma says that for two different efficient 
orderings the cost share in one efficient allocation is 
possible to achieve in the other by appropriately modifying the 
transfers 
lemma let σ t be an efficient allocation and π be the 
vector of cost shares of jobs from this allocation if σ∗ 
 σ 
be an efficient ordering and t∗ 
i ci σ∗ 
 − πi ∀ i ∈ n then 
 σ∗ 
 t∗ 
 is also an efficient allocation 
proof since σ t is efficient £i∈n ti this gives 
£i∈n πi c n since σ∗ 
is an efficient ordering £i∈n ci σ∗ 
 
c n this means £i∈n t∗ 
i £i∈n ci σ∗ 
 − πi so 
 σ∗ 
 t∗ 
 is an efficient allocation 
depending on the transfers the cost shares in different 
efficient allocations may differ an allocation rule ψ associates 
with every q ∈ q a non-empty subset ψ q of allocations 
 cost sharing using the shapley 
value 
in this section we define the coalitional cost of this game 
and analyze the solution proposed by the shapley value 
given a queue q ∈ q the cost of a coalition of s ⊆ n jobs 
in the queue is defined as the cost incurred by jobs in s if 
these are the only jobs served in the queue using an efficient 
ordering formally the cost of a coalition s ⊆ n is 
c s 
 
i∈s 
 
j∈s σ∗ 
j ≤σ∗ 
i 
θjpj 
where σ∗ 
 σ∗ 
 s is an efficient ordering considering jobs 
from s only the worth of a coalition of s jobs is just 
−c s maniquet observes another equivalent way to 
define the worth of a coalition is using the dual function of 
the cost function c · other interesting ways to define the 
worth of a coalition in such games is discussed by chun 
who assume that a coalition of jobs are served after the jobs 
not in the coalition are served 
the shapley value or cost share of a job i is defined as 
svi 
 
s⊆n\{i} 
 s n − s − 
 n 
 c s∪{i} −c s ¢ 
the shapley value allocation rule says that jobs are ordered 
using an efficient ordering and transfers are assigned to jobs 
such that the cost share of job i is given by equation 
lemma let σ∗ 
be an efficient ordering of jobs in set 
n for all i ∈ n the shapley value is given by 
svi piθi 
 
 
 li ri¢ 
where li θi £j∈pi σ∗ pj and ri pi £j∈fi σ∗ θj 
proof another way to write the shapley value formula 
is the following 
svi 
 
s⊆n i∈s 
∆ s 
 s 
 
where ∆ s c s if s and ∆ s c s −£t s ∆ t 
this gives ∆ {i} c {i} piθi ∀i ∈ n for any i j ∈ n 
with i j we have 
∆ {i j} c {i j} − c {i} − c {j} 
 min piθi pjθj pjθi piθi pjθj piθj 
− piθi − pjθj 
 min pjθi piθj 
we will show by induction that ∆ s if s for 
 s let s {i j k} without loss of generality assume 
θi 
pi 
≥ 
θj 
pj 
≥ θk 
pk 
 so ∆ s c s − ∆ {i j} − ∆ {j k} − 
∆ {i k} −∆ {i} −∆ {j} −∆ {k} c s −piθj −pjθk − 
piθk − piθi − pjθj − pkθk c s − c s 
now assume for t s ∆ t if t without 
loss of generality assume that σ to be the identity mapping 
now 
∆ s c s − 
 
t s 
∆ t 
 c s − 
 
i∈s 
 
j∈s j i 
∆ {i j} − 
 
i∈s 
∆ {i} 
 c s − 
 
i∈s 
 
j∈s j i 
pjθi − 
 
i∈s 
piθi 
 c s − c s 
this proves that ∆ s if s using the shapley 
value formula now 
svi 
 
s⊆n i∈s 
∆ s 
 s 
 ∆ {i} 
 
 
 
j∈n j i 
∆ {i j} 
 piθi 
 
 
 
j i 
∆ {i j} 
 
j i 
∆ {i j} ¢ 
 piθi 
 
 
 
j i 
pjθi 
 
j i 
piθj¢ piθi 
 
 
 li ri¢ 
 axiomaticcharacterizationof 
the shapley value 
in this section we will define serveral axioms on fairness 
and characterize the shapley value using them for a given 
q ∈ q we will denote ψ q as the set of allocations from 
allocation rule ψ also we will denote the cost share vector 
associated with an allocation rule σ t as π and that with 
allocation rule σ t as π etc 
 the fairness axioms 
we will define three types of fairness axioms i related 
to efficiency ii related to equity and iii related to 
independence 
efficiency axioms 
we define two types of efficiency axioms one related to 
efficiency which states that an efficient ordering should be 
selected and the transfers of jobs should add up to zero 
 budget balance 
definition an allocation rule ψ satisfies efficiency if 
for every q ∈ q and σ t ∈ ψ q σ t is an efficient 
allocation 
 
the second axiom related to efficiency says that the 
allocation rule should not discriminate between two allocations 
which are equivalent to each other in terms of cost shares of 
jobs 
definition an allocation rule ψ satisfies pareto 
indifference if for every q ∈ q σ t ∈ ψ q and σ t ∈ σ q 
we have 
 πi πi ∀ i ∈ n¢⇒ 
 σ t ∈ ψ q ¢ 
an implication of pareto indifference axiom and lemma 
 is that for every efficient ordering there is some set of 
transfers of jobs such that it is part of an efficient rule and 
the cost share of a job in all these allocations are same 
equity axioms 
how should the cost be shared between two jobs if the jobs 
have some kind of similarity between them equity axioms 
provide us with fairness properties which help us answer 
this question we provide five such axioms some of these 
axioms for example anonymity equal treatment of equals 
are standard in the literature while some are new 
we start with a well known equity axiom called anonymity 
denote ρ n → n as a permutation of elements in n let 
ρ σ t denote the allocation obtained by permuting elements 
in σ and t according to ρ similarly let ρ p θ denote the 
new list of p θ obtained by permuting elements of p and θ 
according to ρ our first equity axiom states that allocation 
rules should be immune to such permutation of data 
definition an allocation rule ψ satisfies anonymity if 
for all q ∈ q σ t ∈ ψ q and every permutation ρ we then 
ρ σ t ∈ ψ n ρ q 
the next equity axiom is classical in literature and says 
that two similar jobs should be compensated such that their 
cost shares are equal this implies that if all the jobs are of 
same type then jobs should equally share the total system 
cost 
definition an allocation rule ψ satisfies equal 
treatment of equals ete if for all q ∈ q σ t ∈ ψ q 
i j ∈ n then 
 pi pj θi θj¢⇒ 
 πi πj¢ 
ete directs us to share costs equally between jobs if they 
are of the same per unit waiting cost and processing time 
but it is silent about the cost shares of two jobs i and j 
which satisfy θi 
pi 
 
θj 
pj 
 we introduce a new axiom for this 
if an efficient rule chooses σ such that σi σj for some 
i j ∈ n then job i is inflicting a cost of piθj on job j 
and job j is inflicting zero cost on job i define for some 
γ ≥ s γ {i ∈ n γi γ} in an efficient rule the 
elements in s γ can be ordered in any manner in s γ 
ways if i j ∈ s γ then we have pjθi piθj probability 
of σi σj is 
 
and so is the probability of σi σj the 
expected cost i inflicts on j is 
 
piθj and j inflicts on i is 
 
 
pjθi our next fairness axiom says that i and j should 
each be responsible for their own processing cost and this 
expected cost they inflict on each other arguing for every 
pair of jobs i j ∈ s γ we establish a bound on the cost 
share of jobs in s γ we impose this as an equity axiom 
below 
definition an allocation rule satisfies expected cost 
bound ecb if for all q ∈ q σ t ∈ ψ q with π being the 
resulting cost share for any γ ≥ and for every i ∈ s γ 
we have 
πi ≥ piθi 
 
 
 
j∈s γ σj σi 
pjθi 
 
j∈s γ σj σi 
piθj¢ 
the central idea behind this axiom is that of expected 
cost inflicted if an allocation rule chooses multiple 
allocations we can assign equal probabilities of selecting one of 
the allocations in that case the expected cost inflicted by 
a job i on another job j in the allocation rule can be 
calculated our axiom says that the cost share of a job should 
be at least its own processing cost and the total expected 
cost it inflicts on others note that the above bound poses 
no constraints on how the costs are shared among different 
groups also observe that if s γ contains just one job ecb 
says that job should at least bear its own processing cost 
a direct consequence of ecb is the following lemma 
lemma let ψ be an efficient rule which satisfies ecb 
for a q ∈ q if s γ n then for any σ t ∈ ψ q which 
gives a cost share of π πi piθi 
 
 li ri¢∀ i ∈ n 
proof from ecb we get πi ≥ piθi 
 
 li ri¢∀ i ∈ n 
assume for contradiction that there exists j ∈ n such that 
πj pjθj 
 
 li ri¢ using efficiency and the fact 
that £i∈n li £i∈n ri we get £i∈n πi c n 
£i∈n piθi 
 
£i∈n 
 li ri¢ c n this gives us a 
contradiction 
next we introduce an axiom about sharing the transfer 
of a job between a set of jobs in particular if the last 
job quits the system then the ordering need not change 
but the transfer to the last job needs to be shared between 
the other jobs this should be done in proportion to their 
processing times because every job influenced the last job 
based on its processing time 
definition an allocation rule ψ satisfies 
proportionate responsibility of p prp if for all q ∈ q for all 
 σ t ∈ ψ q k ∈ n such that σk n q n \ 
{k} p θ ∈ q such that for all i ∈ n\{k} θi θi pi pi 
there exists σ t ∈ ψ q such that for all i ∈ n \ {k} 
σi σi and 
ti ti tk 
pi 
£j k pj 
 
an analogous fairness axiom results if we remove the job 
from the beginning of the queue since the presence of the 
first job influenced each job depending on their θ values its 
transfer needs to be shared in proportion to θ values 
definition an allocation rule ψ satisfies 
proportionate responsibility of θ prθ if for all q ∈ q for all 
 σ t ∈ ψ q k ∈ n such that σk q n \{k} p θ ∈ 
q such that for all i ∈ n \{k} θi θi pi pi there exists 
 σ t ∈ ψ q such that for all i ∈ n \ {k} σi σi and 
ti ti tk 
θi 
£j k θj 
 
the proportionate responsibility axioms are 
generalizations of equal responsibility axioms introduced by 
maniquet 
 
independence axioms 
the waiting cost of a job does not depend on the per unit 
waiting cost of its preceding jobs similarly the waiting cost 
inflicted by a job to its following jobs is independent of the 
processing times of the following jobs these independence 
properties should be carried over to the cost sharing rules 
this gives us two independence axioms 
definition an allocation rule ψ satisfies independence 
of preceding jobs θ ipjθ if for all q n p θ q 
 n p θ ∈ q σ t ∈ ψ q σ t ∈ ψ q if for all 
i ∈ n \ {k} θi θi pi pi and γk γk pk pk 
then for all j ∈ n such that σj σk πj πj where π is 
the cost share in σ t and π is the cost share in σ t 
definition an allocation rule ψ satisfies independence 
of following jobs p ifjp if for all q n p θ q 
 n p θ ∈ q σ t ∈ ψ q σ t ∈ ψ q if for all 
i ∈ n \ {k} θi θi pi pi and γk γk θk θk 
then for all j ∈ n such that σj σk πj πj where π is 
the cost share in σ t and π is the cost share in σ t 
 the characterization results 
having stated the fairness axioms we propose three 
different ways to characterize the shapley value rule using 
these axioms all our characterizations involve efficiency 
and ecb but if we have ipjθ we either need ifjp or prp 
similarly if we have ifjp we either need ipjθ or prθ 
proposition any efficient rule ψ that satisfies ecb 
ipjθ and ifjp is a rule implied by the shapley value rule 
proof define for any i j ∈ n θi 
j γipj and pi 
j 
θj 
γi 
 assume without loss of generality that σ is an efficient 
ordering with σi i ∀ i ∈ n 
consider the following q n p θ corresponding to 
job i with pj pj if j ≤ i and pj pi 
j if j i θj θi 
j if 
j i and θj θj if j ≥ i observe that all jobs have the 
same γ γi by lemma and efficiency σ t ∈ ψ q for 
some set of transfers t using lemma we get cost share of 
i from σ t as πi piθi 
 
 li ri¢ now for any j i 
if we change θj to θj without changing processing time the 
new γ of j is γj ≥ γi applying ipjθ the cost share of job i 
should not change similarly for any job j i if we change 
pj to pj without changing θj the new γ of j is γj ≤ γi 
applying ifjp the cost share of job i should not change 
applying this procedure for every j i with ipjθ and for 
every j i with ifjp we reach q n p θ and the payoff 
of i does not change from πi using this argument for every 
i ∈ n and using the expression for the shapley value in 
lemma we get the shapley value rule 
it is possible to replace one of the independence axioms 
with an equity axiom on sharing the transfer of a job this 
is shown in propositions and 
proposition any efficient rule ψ that satisfies ecb 
ipjθ and prp is a rule implied by the shapley value rule 
proof as in the proof of proposition define θi 
j 
γipj ∀ i j ∈ n assume without loss of generality that σ is 
an efficient ordering with σi i ∀ i ∈ n 
consider a queue with jobs in set k { i i } 
where i n define q k p θ where θj θi 
j ∀ j ∈ 
k define σj σj ∀ j ∈ k σ is an efficient ordering 
for q by ecb and lemma the cost share of job i 
 in any allocation rule in ψ must be πi pi θi 
 
 
 £j i pjθi ¢ now consider q k p θ such that 
θj θi 
j ∀ j ≤ i and θi θi σ remains an efficient 
ordering in q and by ipjθ the cost share of i remains 
πi in q k \ {i } p θ we can calculate the 
cost share of job i using ecb and lemma as πi piθi 
 
 
£j i pjθi so using prp we get the new cost share of job 
i in q as πi πi ti 
pi 
j i pj 
 piθi 
 
 £j i pjθi 
piθi ¢ 
now we can set k k ∪ {i } as before we can 
find cost share of i in this queue as πi pi θi 
 
 
 £j i pjθi ¢ using prp we get the new cost share 
of job i in the new queue as πi piθi 
 
 £j i pjθi 
piθi piθi ¢ this process can be repeated till we add 
job n at which point cost share of i is piθi 
 
 £j i pjθi 
£j i piθj¢ then we can adjust the θ of preceding jobs of 
i to their original value and applying ipjθ the payoffs of 
jobs i through n will not change this gives us the shapley 
values of jobs i through n setting i we get cost shares 
of all the jobs from ψ as the shapley value 
proposition any efficient rule ψ that satisfies ecb 
ifjp and prθ is a rule implied by the shapley value rule 
proof the proof mirrors the proof of proposition we 
provide a short sketch analogous to the proof of 
proposition θs are kept equal to original data and processing times 
are initialized to pi 
j this allows us to use ifjp also 
contrast to proposition we consider k {i i n} and 
repeatedly add jobs to the beginning of the queue 
maintaining the same efficient ordering so we add the cost 
components of preceding jobs to the cost share of jobs in each 
iteration and converge to the shapley value rule 
the next proposition shows that the shapley value rule 
satisfies all the fairness axioms discussed 
proposition the shapley value rule satisfies efficiency 
pareto indifference anonymity ete ecb ipjθ ifjp prp 
and prθ 
proof the shapley value rule chooses an efficient 
ordering and by definition the payments add upto zero so it 
satisfies efficiency 
the shapley value assigns same cost share to a job 
irrespective of the efficient ordering chosen so it is pareto 
indifferent 
the shapley value is anonymous because the particular 
index of a job does not effect his ordering or cost share 
for ete consider two jobs i j ∈ n such that pi pj 
and θi θj without loss of generality assume the efficient 
ordering to be i j n now the shapley value 
of job i is 
 
svi piθi 
 
 
 li ri¢ from lemma 
 pjθj 
 
 
 lj rj¢− 
 
 
 li − lj ri − rj¢ 
 svj − 
 
 
 
i k≤j 
piθk − 
 
i≤k j 
pkθi¢ 
 svj − 
 
 
 
i k≤j 
 piθk − pkθi using pi pj and θi θj 
 svj using 
θk 
pk 
 
θi 
pi 
for all i ≤ k ≤ j 
the shapley value satisfies ecb by its expression in lemma 
 
consider any job i in an efficient ordering σ if we increase 
the value of γj for some j i such that σj σi then 
the set pi preceding jobs does not change in the new 
efficient ordering if γj is changed such that pj remains the 
same then the expression £j∈pi 
θipj is unchanged if p θ 
values of no other jobs are changed then the shapley value 
is unchanged by increasing γj for some j ∈ pi while keeping 
pj unchanged thus the shapley value rule satisfies ipjθ 
an analogous argument shows that the shapley value rule 
satisfies ifjp 
for prp assume without loss of generality that jobs are 
ordered n in an efficient ordering denote the transfer 
of job i n due to the shapley value with set of jobs n and 
set of jobs n \ {n} as ti and ti respectively transfer of last 
job is tn 
 
θn £j n pj now 
ti 
 
 
 θi 
 
j i 
pj − pi 
 
j i 
θj¢ 
 
 
 
 θi 
 
j i 
pj − pi 
 
j i j n 
θj¢− 
 
 
piθn 
 ti − 
 
 
θn 
 
j n 
pj 
pi 
£j n pj 
 ti − tn 
pi 
£j n pj 
 
a similar argument shows that the shapley value rule 
satisfies prθ 
these series of propositions lead us to our main result 
theorem let ψ be an allocation rule the following 
statements are equivalent 
 for each q ∈ q ψ q selects all the allocation assigning 
jobs cost shares implied by the shapley value 
 ψ satisfies efficiency ecb ifjp and ipjθ 
 ψ satisfies efficiency ecb ifjp and prθ 
 ψ satisfies efficiency ecb prp and ipjθ 
proof the proof follows from propositions and 
 
 discussions 
 a reasonable class of cost sharing 
mechanisms 
in this section we will define a reasonable class of cost 
sharing mechanisms we will show how these reasonable 
mechanisms lead to the shapley value mechanism 
definition an allocation rule ψ is reasonable if for 
all q ∈ q and σ t ∈ ψ q we have for all i ∈ n 
ti α 
 θi 
 
j∈pi σ 
pj − pi 
 
j∈fi σ 
θj¢∀ i ∈ n 
where ≤ α ≤ 
the reasonable cost sharing mechanism says that every 
job should be paid a constant fraction of the difference 
between the waiting cost he incurs and the waiting cost he 
inflicts on other jobs if α then every job bears its 
own cost if α then every job gets compensated for its 
waiting cost but compensates others for the cost he inflicts 
on others the shapley value rule comes as a result of ete 
as shown in the following proposition 
proposition any efficient and reasonable allocation 
rule ψ that satisfies ete is a rule implied by the shapley 
value rule 
proof consider a q ∈ q in which pi pj and θi θj 
let σ t ∈ ψ q and π be the resulting cost shares from 
ete we get 
πi πj 
⇒ ci σ − ti cj σ − tj 
⇒ piθi − α li αri pjθj − α lj αrj 
 since ψ is efficient and reasonable 
⇒ − α li − lj α rj − ri 
 using pi pj θi θj 
⇒ − α α 
 using li − lj rj − ri 
⇒ α 
 
 
 
this gives us the shapley value rule by lemma 
 results on envy 
chun discusses a fariness condition called no-envy for 
the case when processing times of all jobs are unity 
definition an allocation rule satisfies no-envy if for 
all q ∈ q σ t ∈ ψ q and i j ∈ n we have πi ≤ ci σij 
 − 
tj where π is the cost share from allocation rule σ t and 
σij 
is the ordering obtaining by swapping i and j 
from the result in the shapley value rule does not 
satisfy no-envy in our model also to overcome this chun 
introduces the notion of adjusted no-envy which he shows 
is satisfied in the shapley value rule when processing times 
of all jobs are unity here we show that adjusted envy 
continues to hold in the shapley value rule in our model when 
processing times need not be unity 
as before denote σij 
be an ordering where the position 
of i and j is swapped from an ordering σ for adjusted 
noenvy if σ t is an allocation for some q ∈ q let tij 
be the 
 
transfer of job i when the transfer of i is calculated with 
respect to ordering σij 
 observe that an allocation may not 
allow for calculation of tij 
 for example if ψ is efficient 
then tij 
cannot be calculated if σij 
is also not efficient for 
simplicity we state the definition of adjusted no-envy to 
apply to all such rules 
definition an allocation rule satisfies adjusted 
noenvy if for all q ∈ q σ t ∈ ψ q and i j ∈ n we have 
πi ≤ ci σij 
 − tij 
i 
proposition the shapley value rule satisfies adjusted 
no-envy 
proof without loss of generality assume efficient 
ordering of jobs is n consider two jobs i and i k 
from lemma 
svi piθi 
 
 
 
j i 
θipj 
 
j i 
θjpi¢ 
let ˆπi be the cost share of i due to adjusted transfer tii k 
i 
in the ordering σii k 
 
ˆπi ci σii k 
 − tii k 
i 
 piθi 
 
 
 
j i 
θipj θipi k 
 
i j i k 
θipj 
 
 
j i 
θjpi − θi kpi − 
 
i j i k 
θjpi¢ 
 svi 
 
 
 
i j≤i k 
 θipj − θjpi¢ 
≥ svi using the fact that 
θi 
pi 
≥ 
θj 
pj 
for i j 
 conclusion 
we studied the problem of sharing costs for a job 
scheduling problem on a single server when jobs have processing 
times and unit time waiting costs we took a cooperative 
game theory approach and show that the famous the 
shapley value rule satisfies many nice fairness properties we 
characterized the shapley value rule using different intuitive 
fairness axioms 
in future we plan to further simplify some of the fairness 
axioms some initial simplifications already appear in 
where we provide an alternative axiom to ecb and also 
discuss the implication of transfers between jobs in stead of 
transfers from jobs to a central server we also plan to look 
at cost sharing mechanisms other than the shapley value 
investigating the strategic power of jobs in such mechanisms 
is another line of future research 
 references 
 youngsub chun a note on maniquet s 
characterization of the shapley value in queueing 
problems working paper rochester university 
 youngsub chun no-envy in queuing problems 
working paper rochester university 
 imma curiel herbert hamers and flip klijn 
sequencing games a survey in peter borm and 
hans peters editors chapter in game theory 
theory and decision library kulwer academic 
publishers 
 imma curiel giorgio pederzoli and stef tijs 
sequencing games european journal of operational 
research - 
 imma curiel jos potters rajendra prasad stef tijs 
and bart veltman sequencing and cooperation 
operations research - may-june 
 nikhil r devanur milena mihail and vijay v 
vazirani strategyproof cost-sharing mechanisms for 
set cover and facility location games in 
proceedings of fourth annual acm conferece on 
electronic commerce 
 robert j dolan incentive mechanisms for priority 
queueing problems bell journal of economics 
 - 
 joan feigenbaum christos papadimitriou and scott 
shenker sharing the cost of multicast transmissions 
in proceedings of thirty-second annual acm 
symposium on theory of computing 
 herbert hamers jeroen suijs stef tijs and peter 
borm the split core for sequencing games games 
and economic behavior - 
 john c harsanyi contributions to theory of games 
iv chapter a bargaining model for cooperative 
n-person games princeton university press 
editors a w tucker r d luce 
 kamal jain and vijay vazirani applications of 
approximate algorithms to cooperative games in 
proceedings of rd symposium on theory of 
computing stoc 
 kamal jain and vijay vazirani equitable cost 
allocations via primal-dual type algorithms in 
proceedings of th symposium on theory of 
computing stoc 
 flip klijn and estela s´anchez sequencing games 
without a completely specified initial order report 
in statistics and operations research pages - 
 report - 
 flip klijn and estela s´anchez sequencing games 
without initial order working paper universitat 
aut´onoma de barcelona july 
 franois maniquet a characterization of the shapley 
value in queueing problems journal of economic 
theory - 
 debasis mishra and bharath rangarajan cost 
sharing in a job scheduling problem working paper 
core 
 manipushpak mitra essays on first best 
implementable incentive problems ph d thesis 
indian statistical institute new delhi 
 manipushpak mitra mechanism design in queueing 
problems economic theory - 
 manipushpak mitra achieving the first best in 
sequencing problems review of economic design 
 - 
 herv´e moulin handbook of social choice and 
welfare chapter axiomatic cost and surplus sharing 
north-holland publishers arrow sen 
suzumura 
 herv´e moulin on scheduling fees to prevent 
 
merging splitting and transferring of jobs working 
paper rice university 
 herv´e moulin split-proof probabilistic scheduling 
working paper rice university 
 herv´e moulin and rakesh vohra characterization of 
additive cost sharing methods economic letters 
 - 
 martin p´al and ´eva tardos group strategyproof 
mechanisms via primal-dual algorithms in 
proceedings of the th annual ieee symposium on 
the foundations of computer science focs 
 
 lloyd s shapley contributions to the theory of 
games ii chapter a value for n-person games pages 
 - annals of mathematics studies 
ediors h w kuhn a w tucker 
 wayne e smith various optimizers for single-stage 
production naval research logistics quarterly 
 - 
 jeroen suijs on incentive compatibility and budget 
balancedness in public decision making economic 
design 
 
ice an iterative combinatorial exchange 
david c parkes∗ † 
ruggiero cavallo† 
nick elprin† 
adam juda† 
s´ebastien lahaie† 
benjamin lubin† 
loizos michael† 
jeffrey shneidman† 
hassan sultan† 
abstract 
we present the first design for an iterative combinatorial 
exchange ice the exchange incorporates a tree-based 
bidding language that is concise and expressive for ces 
bidders specify lower and upper bounds on their value for 
different trades these bounds allow price discovery and useful 
preference elicitation in early rounds and allow termination 
with an efficient trade despite partial information on bidder 
valuations all computation in the exchange is carefully 
optimized to exploit the structure of the bid-trees and to avoid 
enumerating trades a proxied interpretation of a 
revealedpreference activity rule ensures progress across rounds a 
vcg-based payment scheme that has been shown to 
mitigate opportunities for bargaining and strategic behavior is 
used to determine final payments the exchange is fully 
implemented and in a validation phase 
categories and subject descriptors i 
 artificial intelligence distributed artificial intelligence j 
 computer applications social and behavioral sciences 
-economics 
general terms algorithms economics theory 
 introduction 
combinatorial exchanges combine and generalize two 
different mechanisms double auctions and combinatorial 
auctions in a double auction da multiple buyers and sellers 
trade units of an identical good in a combinatorial 
auction ca a single seller has multiple heterogeneous items 
up for sale buyers may have complementarities or 
substitutabilities between goods and are provided with an 
expressive bidding language a common goal in both market 
designs is to determine the efficient allocation which is the 
allocation that maximizes total value 
a combinatorial exchange ce is a combinatorial 
double auction that brings together multiple buyers and 
sellers to trade multiple heterogeneous goods for example in 
an exchange for wireless spectrum a bidder may declare that 
she is willing to pay million for a trade where she obtains 
licenses for new york city boston and philadelphia and 
loses her license for washington dc thus unlike a da a 
ce allows all participants to express complex valuations via 
expressive bids unlike a ca a ce allows for fragmented 
ownership with multiple buyers and sellers and agents that 
are both buying and selling 
ces have received recent attention both in the context of 
wireless spectrum allocation and for airport takeoff and 
landing slot allocation in both of these domains there 
are incumbents with property rights and it is important 
to facilitate a complex multi-way reallocation of resources 
another potential application domain for ces is to resource 
allocation in shared distributed systems such as planetlab 
 the instantiation of our general purpose design to 
specific domains is a compelling next step in our research 
this paper presents the first design for an iterative 
combinatorial exchange ice the genesis of this project was a 
class cs r topics at the interface between economics 
and computer science taught at harvard university in 
spring 
the entire class was dedicated to the design 
and prototyping of an iterative ce 
the ice design problem is multi-faceted and quite hard 
the main innovation in our design is an expressive yet 
concise tree-based bidding language which generalizes known 
languages such as xor or and the tight coupling 
of this language with efficient algorithms for price-feedback 
to guide bidding winner-determination to determine trades 
and revealed-preference activity rules to ensure progress 
across rounds the exchange is iterative bidders express 
upper and lower valuations on trades by annotating their 
bid-tree and then tighten these bounds in response to price 
feedback in each round the threshold payment rule 
introduced by parkes et al is used to determine final 
payments 
the exchange has a number of interesting theoretical 
properties for instance when there exist linear prices we 
establish soundness and completeness for straightforward 
bidders that adjust their bounds to meet activity rules while 
keeping their true value within the bounds the exchange 
will terminate with the efficient allocation in addition the 
 
http www eecs harvard edu ∼parkes cs r ice html 
 
truth agent act rule wd acc 
fair 
balclosing rulevickreythreshold 
done 
 done 
 
 a 
 
 b 
 
buyer 
 
-a 
- 
-b 
- 
seller 
 
 a 
 
 
 b 
 
 
buyer 
 
-a 
- 
- 
-b 
- 
- 
seller 
buyer buy ab 
seller sell ab 
 pa pb 
pa pb 
pa pb 
pbuyer - - 
pseller - - - - 
pbuyer 
pseller - 
pessim 
istic 
o 
ptim 
istic 
figure ice system flow of control 
efficient allocation can often be determined without bidders 
revealing or even knowing their exact value for all trades 
this is essential in complex domains where the valuation 
problem can itself be very challenging for a participant 
while we cannot claim that straightforward bidding is an 
equilibrium of the exchange and indeed should not expect 
to by the myerson-satterthwaite impossibility theorem 
the threshold payment rule minimizes the ex post incentive 
to manipulate across all budget-balanced payment rules 
the exchange is implemented in java and is currently in 
validation in describing the exchange we will first provide 
an overview of the main components and introduce several 
working examples then we introduce the basic 
components for a simple one-shot variation in which bidders state 
their exact values for trades in a single round we then 
describe the full iterative exchange with upper and lower 
values price-feedback activity rules and termination 
conditions we state some theoretical properties of the exchange 
and end with a discussion to motivate our main design 
decisions and suggest some next steps 
 an overview of the ice design 
the design has four main components which we will 
introduce in order through the rest of the paper 
 expressive and concise tree-based bidding language 
the language describes values for trades such as my value 
for selling ab and buying c is or my value for selling 
abc is - with negative values indicating that a bidder 
must receive a payment for the trade to be acceptable the 
language allows bidders to express upper and lower bounds 
on value which can be tightened across rounds 
 winner determination winner-determination wd 
is formulated as a mixed-integer program mip with the 
structure of the bid-trees captured explicitly in the 
formulation comparing the solution at upper and lower values 
allows for a determination to be made about termination 
with progress in intermediate rounds driven by an 
intermediate valuation and the lower values adopted on termination 
 payments payments are computed using the threshold 
payment rule with the intermediate valuations adopted 
in early rounds and lower values adopted on termination 
 price feedback an approximate price is computed 
for each item in the exchange in each round in terms of 
the intermediate valuations and the provisional trade the 
prices are optimized to approximate competitive equilibrium 
prices and further optimized to best approximate the 
current threshold payments with remaining ties broken to favor 
prices that are balanced across different items in computing 
the prices we adopt the methods of constraint-generation to 
exploit the structure of the bidding language and avoid 
enumerating all feasible trades the subproblem to generate 
new constraints is a variation of the wd problem 
 activity rule a revealed-preference activity rule 
ensures progress across rounds in order to remain active a 
bidder must tighten bounds so that there is enough 
information to define a trade that maximizes surplus at the current 
prices another variation on the wd problem is formulated 
both to verify that the activity rule is met and also to 
provide feedback to a bidder to explain how to meet the rule 
an outline of the ice system flow of control is provided 
in figure we will return to this example later in the 
paper for now just observe in this two-agent example that 
the agents state lower and upper bounds that are checked in 
the activity rule and then passed to winner-determination 
 wd and then through three stages of pricing accuracy 
fairness balance on passing the closing rule in which 
parameters αeff 
and αthresh 
are checked for convergence of the 
trade and payments the exchange goes to a last-and-final 
round at the end of this round the trade and payments 
are finally determined based on the lower valuations 
 related work 
many ascending-price one-sided cas are known in the 
literature direct elicitation approaches have also 
been proposed for one-sided cas in which agents respond to 
explicit queries about their valuations a number 
of ascending cas are designed to work with simple prices 
on items the price generation methods that we use 
in ice generalize the methods in these earlier papers 
parkes et al studied sealed-bid combinatorial 
exchanges and introduced the threshold payment rule 
subsequently krych demonstrated experimentally that the 
threshold rule promotes efficient allocations we are not 
aware of any previous studies of iterative ces dominant 
strategy das are known for unit demand and also for 
single-minded agents no dominant strategy mechanisms 
are known for the general ce problem 
ice is a hybrid auction design in that it couples 
simple item prices to drive bidding in early rounds with 
combinatorial wd and payments a feature it shares with the 
clock-proxy design of ausubel et al for one-sided cas 
we adopt a variation on the clock-proxy auctions s 
revealedpreference activity rule 
the bidding language shares some structural elements 
with the lgb language of boutilier and hoos but has 
very different semantics rothkopf et al also describe a 
restricted tree-based bidding language in lgb the 
semantics are those of propositional logic with the same items 
in an allocation able to satisfy a tree in multiple places 
although this can make lgb especially concise in some 
settings the semantics that we propose appear to provide 
useful locality so that the value of one component in a tree 
can be understood independently from the rest of the tree 
the idea of capturing the structure of our bidding language 
explicitly within a mixed-integer programming formulation 
follows the developments in boutilier 
 preliminaries 
in our model we consider a set of goods indexed { 
m} and a set of bidders indexed { n} the initial 
allocation of goods is denoted x 
 x 
 x 
n with x 
i 
 x 
i x 
im and x 
ij ≥ for good j indicating the number 
 
of units of good j held by bidder i a trade λ λ λn 
denotes the change in allocation with λi λi λim 
where λij ∈ 
 
is the change in the number of units of item 
j to bidder i so the final allocation is x 
 x 
 λ 
each bidder has a value vi λi ∈ for a trade λi this 
value can be positive or negative and represents the change 
in value between the final allocation x 
i λi and the initial 
allocation x 
i utility is quasi-linear with ui λi p vi λi −p 
for trade λi and payment p ∈ price p can be negative 
indicating the bidder receives a payment for the trade we 
use the term payoff interchangeably with utility 
our goal in the ice design is to implement the efficient 
trade the efficient trade λ∗ 
 maximizes the total increase 
in value across bidders 
definition efficient trade the efficient trade 
λ∗ 
solves 
max 
 λ λn 
¢ 
i 
vi λi 
s t λij x 
ij ≥ ∀i ∀j 
¢ 
i 
λij ≤ ∀j 
λij ∈ 
 
 
constraints ensure that no agent sells more items than 
it has in its initial allocation constraints provide free 
disposal and allows feasible trades to sell more items than 
are purchased but not vice versa 
later we adopt feas x 
 to denote the set of feasible 
trades given these constraints and given an initial 
allocation x 
 x 
 x 
n 
 working examples 
in this section we provide three simple examples of 
instances that we will use to illustrate various components of 
the exchange all three examples have only one seller but 
this is purely illustrative 
example one seller and one buyer two goods {a b} 
with the seller having an initial allocation of ab changes 
in values for trades 
seller buyer 
and −a −b and a b 
- 
the and indicates that both the buyer and the seller 
are only interested in trading both goods as a bundle here 
the efficient value-maximizing trade is for the seller to sell 
ab to the buyer denoted λ∗ 
 − − 
example one seller and four buyers four goods {a b 
c d} with the seller having an initial allocation of abcd 
changes in values for trades 
seller buyer buyer buyer buyer 
or −a −b and a xor a and c xor c 
−c −d b b d d 
 
the or indicates that the seller is willing to sell any 
number of goods the xor indicates that buyers and 
 are willing to buy at most one of the two goods in which 
they are interested the efficient trade is for bundle ab 
to go to buyer and bundle cd to buyer denoted λ∗ 
 
 − − − − 
 
 
 a 
 
 b 
 
buyer 
 
-a 
- 
-b 
- 
seller 
example example 
 
 c d 
buyer 
 
 a b 
buyer 
 
-b 
seller 
-a -c -d 
example 
 
 a b 
buyer 
 
 a b 
buyer 
 
-b 
seller 
-c -d-a 
 
 c d 
buyer 
 
 c d 
 
buyer 
- 
figure example bid trees 
example one seller and two buyers four goods {a b 
c d} with the seller having an initial allocation of abcd 
changes in values for trades 
seller buyer buyer 
and −a −b −c −d and a b and c d 
- 
the efficient trade is for bundle ab to go to buyer and 
bundle cd to go to buyer denoted λ∗ 
 − − − − 
 
 a one-shot exchange design 
the description of ice is broken down into two sections 
one-shot sealed-bid and iterative in this section we 
abstract away the iterative aspect and introduce a 
specialization of the tree-based language that supports only exact 
values on nodes 
 tree-based bidding language 
the bidding language is designed to be expressive and 
concise entirely symmetric with respect to buyers and 
sellers and to extend to capture bids from mixed buyers and 
sellers ranging from simple swaps to highly complex trades 
bids are expressed as annotated bid trees and define a 
bidder s value for all possible trades 
the language defines changes in values on trades with 
leaves annotated with traded items and nodes annotated 
with changes in values either positive or negative the 
main feature is that it has a general interval-choose 
logical operator on internal nodes and that it defines careful 
semantics for propagating values within the tree we 
illustrate the language on each of examples - in figure 
the language has a tree structure with trades on items 
defined on leaves and values annotated on nodes and leaves 
the nodes have zero values where no value is indicated 
internal nodes are also labeled with interval-choose ic 
ranges given a trade the semantics of the language define 
which nodes in the tree can be satisfied or switched-on 
first if a child is on then its parent must be on second if 
a parent node is on then the number of children that are on 
must be within the ic range on the parent node finally 
leaves in which the bidder is buying items can only be on if 
the items are provided in the trade 
for instance in example we can consider the efficient 
trade and observe that in this trade all nodes in the trees of 
buyers and and also the seller but none of the nodes in 
the trees of buyers and can be on on the other hand in 
 
the trade in which a goes to buyer and d to buyer then 
the root and appropriate leaf nodes can be on for buyers 
and but no nodes can be on for buyers and given a 
trade there is often a number of ways to choose the set of 
satisfied nodes the semantics of the language require that 
the nodes that maximize the summed value across satisfied 
nodes be activated 
consider bid tree ti from bidder i this defines nodes β ∈ 
ti of which some are leaves leaf i ⊆ ti let child β ⊆ 
ti denote the children of a node β that is not itself a leaf 
all nodes except leaves are labeled with the interval-choose 
operator ic x 
i β icy 
i β every node is also labeled with a 
value viβ ∈ each leaf β is labeled with a trade qiβ ∈ 
 
m 
 i e leaves can define a bundled trade on more than one 
type of item 
given a trade λi to bidder i the interval-choose operators 
and trades on leaves define which nodes can be satisfied 
there will often be a choice ties are broken to maximize 
value let satiβ ∈ { } denote whether node β is satisfied 
solution sati is valid given tree ti and trade λi written 
sati ∈ valid ti λi if and only if 
¢ 
β∈leaf i 
qiβj · satiβ ≤ λij ∀i ∀j 
icx 
i β satiβ ≤ 
¢ 
β ∈child β 
satiβ ≤ icy 
i β satiβ ∀β ∈ leaf i 
in words a set of leaves can only be considered satisfied 
given trade λi if the total increase in quantity summed across 
all such leaves is covered by the trade for all goods eq 
this works for sellers as well as buyers for sellers a trade 
is negative and this requires that the total number of items 
indicated sold in the tree is at least the total number sold as 
defined in the trade we also need upwards-propagation 
any time a node other than the root is satisfied then its 
parent must be satisfied by 
β ∈child β satiβ ≤ icy 
i β satiβ 
in eq finally we need downwards-propagation any 
time an internal node is satisfied then the appropriate 
number of children must also be satisfied eq the total 
value of trade λi given bid-tree ti is defined as 
vi ti λi max 
sat∈valid ti λi 
¢ 
β∈t 
vβ · satβ 
the tree-based language generalizes existing languages 
for instance ic on a node with children is equivalent 
to an and operator ic on a node with children is 
equivalent to an or operator and ic on a node with 
 children is equivalent to an xor operator similarly the 
xor or bidding languages can be directly expressed as a 
bid tree in our language 
 winner determination 
this section defines the winner determination problem 
which is formulated as a mip and solved in our 
implementation with a commercial solver 
the solver uses 
branchand-bound search with dynamic cut generation and 
branching heuristics to solve large mips in economically feasible 
run times 
 
the or language is the or language with dummy items 
to provide additional structure or is known to be 
expressive and concise however it is not known whether or 
dominates xor or in terms of conciseness 
 
cplex www ilog com 
in defining the mip representation we are careful to avoid 
an xor-based enumeration of all bundles a variation on 
the wd problem is reused many times within the exchange 
e g for column generation in pricing and for checking 
revealed preference 
given bid trees t t tn and initial allocation x 
 
the mixed-integer formulation for wd is 
wd t x 
 max 
λ sat 
¢ 
i 
¢ 
β∈ti 
viβ · satiβ 
s t satiβ ∈ { } λij ∈ 
 
sati ∈ valid ti λi ∀i 
some goods may go unassigned because free disposal is 
allowed within the clearing rules of winner determination 
these items can be allocated back to agents that sold the 
items i e for which λij 
 computing threshold payments 
the threshold payment rule is based on the payments 
in the vickrey-clarke-groves vcg mechanism which 
itself is truthful and efficient but does not satisfy budget 
balance budget-balance requires that the total payments 
to the exchange are equal to the total payments made by 
the exchange in vcg the payment paid by agent i is 
pvcg i ˆv λ∗ 
i − v ∗ 
− v−i 
where λ∗ 
is the efficient trade v ∗ 
is the reported value of 
this trade and v−i is the reported value of the efficient 
trade that would be implemented without bidder i we 
call ∆vcg i v ∗ 
− v−i the vcg discount for instance 
in example pvcg seller − − − − and 
pvcg buyer − − and the exchange would 
run at a budget deficit of − − 
the threshold payment rule determines 
budgetbalanced payments to minimize the maximal error across all 
agents to the vcg outcome 
definition the threshold payment scheme implements 
the efficient trade λ∗ 
given bids and sets payments pthresh i 
ˆvi λ∗ 
i − ∆i where ∆ ∆ ∆n is set to minimize 
maxi ∆vcg i − ∆i subject to ∆i ≤ ∆vcg i and 
i ∆i ≤ v ∗ 
 this gives budget-balance 
example in example the vcg discounts are 
 to the seller and four buyers respectively vcg 
payments are − and the exchange runs at a deficit 
of - in threshold the discounts are and the 
payments are − this minimizes the worst-case 
error to vcg discounts across all budget-balanced payment 
schemes 
threshold payments are designed to minimize the 
maximal ex post incentive to manipulate krych confirmed 
that threshold promotes allocative efficiency in restricted 
and approximate bayes-nash equilibrium 
 the ice design 
we are now ready to introduce the iterative 
combinatorial exchange ice design several new components are 
introduced relative to the design for the one-shot exchange 
rather than provide precise valuations bidders can provide 
lower and upper valuations and revise this bid information 
across rounds the exchange provides price-based feedback 
 
to guide bidders in this process and terminates with an 
efficient or approximately-efficient trade with respect to 
reported valuations 
in each round t ∈ { } the current lower and upper 
bounds vt 
and vt 
 are used to define a provisional 
valuation profile vα 
 the α-valuation together with a provisional 
trade λt 
and provisional prices pt 
 pt 
 pt 
m on items 
the α-valuation is a linear combination of the current 
upper and lower valuations with αeff 
∈ chosen 
endogenously based on the closeness of the optimistic trade at 
v and the pessimistic trade at v prices pt 
are used to 
inform an activity rule and drive progress towards an efficient 
trade 
 upper and lower valuations 
the bidding language is extended to allow a bidder i to 
report a lower and upper value viβ viβ on each node these 
take the place of the exact value viβ defined in section 
based on these labels we can define the valuation functions 
vi ti λi and vi ti λi using the exact same semantics as 
in eq we say that such a bid-tree is well-formed if 
viβ ≤ viβ for all nodes the following lemma is useful 
lemma given a well-formed tree t then vi ti λi ≤ 
vi ti λi for all trades 
proof suppose there is some λi for which vi ti λi 
vi ti λi then maxsat∈valid ti λi 
β∈ti 
viβ · satβ 
maxsat∈valid ti λi 
β∈ti 
viβ · satβ but this is a 
contradiction because the trade λ that defines vi ti λi is still 
feasible with upper bounds vi and viβ ≥ viβ for all nodes 
β in a well-formed tree 
 price feedback 
in each round approximate competitive-equilibrium ce 
prices pt 
 pt 
 pt 
m are determined given these 
provisional prices the price on trade λi for bidder i is pt 
 λi 
 
j≤m pt 
j · λij 
definition ce prices prices p∗ 
are competitive 
equilibrium prices if the efficient trade λ∗ 
is supported at 
prices p∗ 
 so that for each bidder 
λ∗ 
i ∈ arg max 
λ∈feas x 
{vi λi − p∗ 
 λi } 
ce prices will not always exist and we will often need to 
compute approximate prices we extend ideas due to 
rassenti et al kwasnica et al and dunford et al 
 and select approximate prices as follows 
i accuracy first we compute prices that minimize the 
maximal error in the best-response constraints across 
all bidders 
ii fairness second we break ties to prefer prices that 
minimize the maximal deviation from threshold 
payments across all bidders 
iii balance third we break ties to prefer prices that 
minimize the maximal price across all items 
taken together these steps are designed to promote the 
informativeness of the prices in driving progress across rounds 
in computing prices we explain how to compute 
approximate or otherwise prices for structured bidding languages 
and without enumerating all possible trades for this we 
adopt constraint generation to efficient handle an 
exponential number of constraints each step is described in detail 
below 
i accuracy we adopt a definition of price accuracy that 
generalizes the notions adopted in previous papers for 
unstructured bidding languages let λt 
denote the current 
provisional trade and suppose the provisional valuation is 
vα 
 to compute accurate ce prices we consider 
min 
p δ 
δ 
s t vα 
i λ − p λ ≤ vα 
i λt 
i − p λt 
i δ ∀i ∀λ 
δ ≥ pj ≥ ∀j 
this linear program lp is designed to find prices that 
minimize the worst-case error across all agents 
from the definition of ce prices it follows that ce prices 
would have δ as a solution to at which point trade 
λt 
i would be in the best-response set of every agent with 
λt 
i ∅ i e no trade for all agents with no surplus for trade 
at the prices 
example we can illustrate the formulation on 
example assuming for simplicity that vα 
 v i e truth 
the efficient trade allocates ab to buyer and cd to buyer 
 accuracy will seek prices p a p b p c and p d to 
minimize the δ ≥ required to satisfy constraints 
p a p b p c p d ≥ seller 
p a p b ≤ δ buyer 
p a δ ≥ p b δ ≥ buyer 
p c p d ≤ buyer 
p c δ ≥ p d δ ≥ buyer 
an optimal solution requires p a p b with 
δ with p c and p d taking values such as p c 
p d 
but has an exponential number of constraints eq 
rather than solve it explicitly we use constraint 
generation and dynamically generate a sufficient subset of 
constraints let 
i denote a manageable subset of all possible 
feasible trades to bidder i then a relaxed version of 
 written acc is formulated by substituting with 
vα 
i λ − p λ ≤ vα 
i λt 
i − p λt 
i δ ∀i ∀λ ∈ 
i 
where 
i is a set of trades that are feasible for bidder i 
given the other bids fixing the prices p∗ 
 we then solve n 
subproblems one for each bidder 
max 
λ 
vα 
i λi − p∗ 
 λi r-wd i 
s t λ ∈ feas x 
 
to check whether solution p∗ 
 δ∗ 
 to acc is feasible in 
problem in r-wd i the objective is to determine a most 
preferred trade for each bidder at these prices let ˆλi denote 
the solution to r-wd i check condition 
vα 
i ˆλi − p∗ 
 ˆλ ≤ vα 
i λt 
i − p∗ 
 λt 
i δ∗ 
 
and if this condition holds for all bidders i then solution 
 p∗ 
 δ∗ 
 is optimal for problem otherwise trade ˆλi is 
added to 
i for all bidders i for which this constraint is 
 
violated and we re-solve the lp with the new set of 
constraints 
ii fairness second we break remaining ties to prefer fair 
prices choosing prices that minimize the worst-case error 
with respect to threshold payoffs i e utility to bidders with 
threshold payments but without choosing prices that are 
less accurate 
example for example accuracy in example 
 depicted in figure requires ≤ pa pb ≤ for vα 
 v 
at these valuations the threshold payoffs would be to both 
the seller and the buyer this can be exactly achieved in 
pricing with pa pb 
the fairness tie-breaking method is formulated as the 
following lp 
min 
p π 
π fair 
s t vα 
i λ − p λ ≤ vα 
i λt 
i − p λt 
i δ∗ 
i ∀i ∀λ ∈ 
i 
π ≥ πvcg i − vα 
i λt 
i − p λt 
i ∀i 
π ≥ pj ≥ ∀j 
where δ∗ 
represents the error in the optimal solution from 
acc the objective here is the same as in the threshold 
payment rule see section minimize the maximal 
error between bidder payoff at vα 
 for the provisional trade 
and the vcg payoff at vα 
 problem fair is also solved 
through constraint generation using r-wd i to add 
additional violated constraints as necessary 
iii balance third we break remaining ties to prefer 
balanced prices choosing prices that minimize the maximal 
price across all items returning again to example 
depicted in figure we see that accuracy and fairness require 
p a p b finally balance sets p a p b 
balance is justified when all else being equal items are 
more likely to have similar than dissimilar values 
the lp 
for balance is formulated as follows 
min 
p y 
y bal 
s t vα 
i λ − p λ ≤ vα 
i λt 
i − p λt 
i δ∗ 
i ∀i ∀λ ∈ 
i 
π∗ 
i ≥ πvcg i − vα 
i λt 
i − p λt 
i ∀i 
y ≥ pj ∀j 
y ≥ pj ≥ ∀j 
where δ∗ 
represents the error in the optimal solution from 
acc and π∗ 
represents the error in the optimal solution 
from fair constraint generation is also used to solve bal 
generating new trades for 
i as necessary 
 
problem r-wd i is a specialization of the wd problem 
in which the objective is to maximize the payoff of a single 
bidder rather than the total value across all bidders it is 
solved as a mip by rewriting the objective in wd t x 
 
as max{viβ · satiβ − 
j p∗ 
j · λij } for agent i thus the 
structure of the bid-tree language is exploited in generating 
new constraints because this is solved as a concise mip 
the other bidders are kept around in the mip but do not 
appear in the objective and are used to define the space of 
feasible trades 
 
the methods of dunford et al that use a nucleolus 
approach are also closely related 
 
the use of balance was advocated by kwasnica et al 
dunford et al prefer to smooth prices across rounds 
comment lexicographical refinement for all 
three sub-problems we also perform lexicographical 
refinement with respect to bidders in acc and fair and with 
respect to goods in bal for instance in acc we 
successively minimize the maximal error across all bidders given 
an initial solution we first pin down the error on all 
bidders for whom a constraint is binding for such a bidder 
i the constraint is replaced with 
vα 
i λ − p λ ≤ vα 
i λt 
i − p λt 
i δ∗ 
i ∀λ ∈ 
i 
and the error to bidder i no longer appears explicitly in 
the objective acc is then re-solved and makes progress 
by further minimizing the maximal error across all bidders 
yet to be pinned down this continues pinning down any 
new bidders for whom one of constraints is binding 
until the error is lexicographically optimized for all 
bidders 
the exact same process is repeated for fair and 
bal with bidders pinned down and constraints 
replaced with π∗ 
i ≥ πvcg i − vα 
i λt 
i − p λt 
i ∀λ ∈ 
i where 
π∗ 
i is the current objective in fair and items pinned down 
and constraints replaced with p∗ 
j ≥ pj where p∗ 
j 
represents the target for the maximal price on that item in 
bal 
comment computation all constraints in 
i are 
retained and this set grows across all stages and across all 
rounds of the exchange thus the computational effort in 
constraint generation is re-used in implementation we are 
careful to address a number of -issues that arise due to 
floating-point issues we prefer to err on the side of being 
conservative in determining whether or not to add another 
constraint in performing check this avoids later 
infeasibility issues in addition when pinning-down bidders for 
the purpose of lexicographical refinement we relax the 
associated bidder-constraints with a small on the 
righthand side 
 revealed-preference activity rules 
the role of activity rules in the auction is to ensure both 
consistency and progress across rounds consistency in 
our exchange requires that bidders tighten bounds as the 
exchange progresses activity rules ensure that bidders are 
active during early rounds and promote useful elicitation 
throughout the exchange 
we adopt a simple revealed-preference rp activity rule 
the idea is loosely based around the rp-rule in ausubel et 
al where it is used for one-sided cas the motivation 
is to require more than simply consistency we need bidders 
to provide enough information for the system to be able to 
to prove that an allocation is approximately efficient 
it is helpful to think about the bidders interacting with 
proxy agents that will act on their behalf in responding 
to provisional prices pt− 
determined at the end of round 
t − the only knowledge that such a proxy has of the 
valuation of a bidder is through the bid-tree suppose a 
proxy was queried by the exchange and asked which trade 
the bidder was most interested in at the provisional prices 
the rp rule says the following the proxy must have enough 
 
for example applying this to accuracy on example we 
solve once and find bidders and are binding for error 
δ∗ 
 we pin these down and then minimize the error 
to bidders and finally this gives p a p b 
and p c p d with accuracy to bidders and 
 and to bidders and 
 
information to be able to determine this surplus-maximizing 
trade at current prices consider the following examples 
example a bidder has xor a b and a value of 
 on the leaf a and a value range of on leaf b 
suppose prices are currently for each of a and b the rp 
rule is satisfied because the proxy knows that however the 
remaining value uncertainty on b is resolved the bidder 
will always weakly prefer b to a 
example a bidder has xor a b and value 
bounds on the root node and a value of on leaf a 
suppose prices are currently for each of a and b the rp 
rule is satisfied because the bidder will always prefer a to 
 b at equal prices whichever way the uncertain value on 
the root node is ultimately resolved 
overloading notation let vi ∈ ti denote a valuation that 
is consistent with lower and upper valuations in bid tree ti 
definition bid tree ti satisfies rp at prices pt− 
if 
and only if there exists some feasible trade l∗ 
for which 
vi l∗ 
i − pt− 
 l∗ 
i ≥ max 
λ∈feas x 
vi λi − pt− 
 λi ∀vi ∈ ti 
 
to make this determination for bidder i we solve a 
sequence of problems each of which is a variation on the wd 
problem first we construct a candidate lower-bound trade 
which is a feasible trade that solves 
max 
λ 
vi λi − pt− 
 λi rp i 
s t λ ∈ feas x 
 
the solution π∗ 
l to rp i represents the maximal payoff 
that bidder i can achieve across all feasible trades given its 
pessimistic valuation 
second we break ties to find a trade with maximal value 
uncertainty across all possible solutions to rp i 
max 
λ 
vi λi − vi λi rp i 
s t λ ∈ feas x 
 
vi λi − pt− 
 λi ≥ π∗ 
l 
we adopt solution l∗ 
i as our candidate for the trade that 
may satisfy rp to understand the importance of this 
tiebreaking rule consider example the proxy can prove b 
but not a is a best-response for all vi ∈ ti and should 
choose b as its candidate notice that b is a 
counterexample to a but not the other way round 
now we construct a modified valuation ˜vi by setting 
˜viβ 
 
viβ if β ∈ sat l∗ 
i 
viβ otherwise 
 
where sat l∗ 
i is the set of nodes that are satisfied in the 
lower-bound tree for trade l∗ 
i given this modified 
valuation we find u∗ 
to solve 
max 
λ 
˜vi λi − pt− 
 λi rp i 
s t λ ∈ feas x 
 
let π∗ 
u denote the payoff from this optimal trade at modified 
values ˜v we call trade u∗ 
i the witness trade we show in 
proposition that the rp rule is satisfied if and only if 
π∗ 
l ≥ π∗ 
u 
constructing the modified valuation as ˜vi recognizes that 
there is shared uncertainty across trades that satisfy the 
same nodes in a bid tree example helps to illustrate this 
just using vi in rp i we would find l∗ 
i is buy a with 
payoff π∗ 
l but then find u∗ 
i is buy b with π∗ 
u and 
fail rp we must recognize that however the uncertainty on 
the root node is resolved it will affect a and b in exactly 
the same way for this reason we set ˜viβ viβ on the 
root node which is exactly the same value that was adopted 
in determining π∗ 
l then rp i applied to u∗ 
i gives buy 
a and the rp test is judged to be passed 
proposition bid tree ti satisfies rp given prices pt− 
if and only if any lower-bound trade l∗ 
i that solves rp i 
and rp i satisfies 
vi ti l∗ 
i − pt− 
 l∗ 
i ≥ ˜vi ti u∗ 
i − pt− 
 u∗ 
i 
where ˜vi is the modified valuation in eq 
proof for sufficiency notice that the difference in 
payoff between trade l∗ 
i and another trade λi is unaffected by 
the way uncertainty is resolved on any node that is satisfied 
in both l∗ 
i and λi fixing the values in ˜vi on nodes satisfied 
in l∗ 
i has the effect of removing this consideration when a 
trade u∗ 
i is selected that satisfies one of these nodes on 
the other hand fixing the values on these nodes has no 
effect on trades considered in rp i that do not share a node 
with l∗ 
i for the necessary direction we first show that any 
trade that satisfies rp must solve rp i suppose 
otherwise that some λi with payoff greater than π∗ 
l satisfies rp 
but valuation vi ∈ ti together with l∗ 
i presents a 
counterexample to rp eq now suppose for 
contradiction that some λi with maximal payoff π∗ 
l but uncertainty 
less than l∗ 
i satisfies rp proceed by case analysis case 
a only one solution to rp i has uncertain value and so 
λi has certain value but this cannot satisfy rp because 
l∗ 
i with uncertain value would be a counterexample to rp 
 eq case b two or more solutions to rp i have 
uncertain value here we first argue that one of these trades 
must satisfy a weak superset of all the nodes with 
uncertain value that are satisfied by all other trades in this set 
this is by rp without this then for any choice of trade 
that solves rp i there is another trade with a disjoint set 
of uncertain but satisfied nodes that provides a 
counterexample to rp eq now consider the case that some 
trade contains a superset of all the uncertain satisfied nodes 
of the other trades clearly rp i will choose this trade 
l∗ 
i and λi must satisfy a subset of these nodes by 
assumption but we now see that λi cannot satisfy rp because 
l∗ 
i would be a counterexample to rp 
failure to meet the activity rule must have some 
consequence in the current rules the default action we choose 
is to set the upper bounds in valuations down to the 
maximal value of the provisional price on a node 
and the 
lowerbound value on that node 
such a bidder can remain active 
 
the provisional price on a node is defined as the minimal 
total price across all feasible trades for which the subtree 
rooted at the tree is satisfied 
 
this is entirely analogous to when a bidder in an ascending 
clock auction stops bidding at a price she is not permitted 
to bid at a higher price again in future rounds 
 
within the exchange but only with valuations that are 
consistent with these new bounds 
 bidder feedback 
in each round our default design provides every bidder 
with the provisional trade and also with the current 
provisional prices see for an additional discussion we also 
provide guidance to help a bidder meet the rp rule let 
sat l∗ 
i and sat u∗ 
i denote the nodes that are satisfied in 
trades l∗ 
i and u∗ 
i as computed in rp -rp 
lemma when rp fails a bidder must increase a lower 
bound on at least one node in sat l∗ 
i \ sat u∗ 
i or decrease 
an upper bound on at least one node in sat u∗ 
i \ sat l∗ 
i in 
order to meet the activity rule 
proof changing the upper- or lower- values on nodes 
that are not satisfied by either trade does not change l∗ 
i or 
u∗ 
i and does not change the payoff from these trades thus 
the rp condition will continue to fail similarly changing 
the bounds on nodes that are satisfied in both trades has 
no effect on revealed preference a change to a lower bound 
on a shared node affects both l∗ 
i and u∗ 
i identically because 
of the use of the modified valuation to determine u∗ 
i a 
change to an upper bound on a shared node has no effect in 
determining either l∗ 
i or u∗ 
i 
note that when sat u∗ 
i sat l∗ 
i then condition is 
always trivially satisfied and so the guidance in the lemma 
is always well-defined when rp fails this is an elegant 
feedback mechanism because it is adaptive once a bidder 
makes some changes on some subset of these nodes the 
bidder can query the exchange the exchange can then respond 
yes or can revise the set of nodes sat λ∗ 
l and sat λ∗ 
u as 
necessary 
 termination conditions 
once each bidder has committed its new bids and either 
met the rp rule or suffered the penalty then round t closes 
at this point the task is to determine the new α-valuation 
and in turn the provisional allocation λt 
and provisional 
prices pt 
 a termination condition is also checked to 
determine whether to move the exchange to a last-and-final 
round to define the α-valuation we compute the following 
two quantities 
pessimistic at pessimistic pp determine an efficient 
trade λ∗ 
l at pessimistic values i e to solve 
maxλ 
i vi λi and set pp 
i vi λ∗ 
li 
pessimistic at optimistic po determine an efficient 
trade λ∗ 
u at optimistic values i e to solve 
maxλ 
i vi λi and set po 
i vi λ∗ 
ui 
first note that pp ≥ po and pp ≥ by definition 
for all bid-trees although po can be negative because the 
right trade at v is not currently a useful trade at v 
recognizing this define 
γeff 
 pp po 
pp − po 
pp 
 
when pp and observe that γeff 
 pp po ≥ when 
this is defined and that γeff 
 pp po will start large and 
then trend towards as the optimistic allocation converges 
towards the pessimistic allocation in each round we define 
αeff 
∈ as 
αeff 
 
 
 when pp is 
 γeff 
otherwise 
 
which is while pp is and then trends towards once 
pp in some round this is used to define α-valuation 
vα 
i αeff 
vi − αeff 
 vi ∀i 
which is used to define the provisional allocation and 
provisional prices the effect is to endogenously define a 
schedule for moving from optimistic to pessimistic values across 
rounds based on how close the trades are to one another 
termination condition in moving to the last-and-final 
round and finally closing we also care about the 
convergence of payments in addition to the convergence towards 
an efficient trade for this we introduce another parameter 
αthresh 
∈ that trends from to as the threshold 
payments at lower and upper valuations converge consider 
the following parameter 
γthresh 
 
 pthresh v − pthresh v 
 pp nactive 
 
which is defined for pp where pthresh v denotes the 
threshold payments at valuation profile v nactive is the 
number of bidders that are actively engaged in trade in the 
pp trade and · is the l -norm note that γthresh 
is 
defined for payments and not payoffs this is appropriate 
because it is the accuracy of the outcome of the exchange 
that matters i e the trade and the payments given this 
we define 
αthresh 
 
 
 when pp is 
 γthresh 
otherwise 
 
which is while pp is and then trends towards as 
progress is made 
definition termination ice transitions to a 
lastand-final round when one of the following holds 
 αeff 
≥ cutoffeff and αthresh 
≥ cutoffthresh 
 there is no trade at the optimistic values 
where cutoffeff cutoffthresh ∈ determine the 
accuracy required for termination 
at the end of the last-and-final round vα 
 v is used to 
define the final trade and the final threshold payments 
example consider again example and consider the 
upper and lower bounds as depicted in figure first if the 
seller s bounds were − − then there is an optimistic 
trade but no pessimistic trade and po − and pp 
and αeff 
 at the bounds depicted both the optimistic 
and the pessimistic trades occur and po pp and 
αeff 
 however we can see the threshold payments are 
 − at v but − at v evaluating γthresh 
 we 
have γthresh 
 
√ 
 
 
 and αthresh 
 
for cutoffthresh the exchange would remain open 
on the other hand if the buyer s value for ab was 
between and the seller s value for −ab was between 
 − − the threshold payments are − at both 
upper and lower bounds and αthresh 
 
 
component purpose lines 
agent captures strategic behavior and information revelation decisions 
model support provides xml support to load goods and valuations into world 
world keeps track of all agent good and valuation details 
exchange driver communication controls exchange and coordinates remote agent behavior 
bidding language implements the tree-based bidding language 
activity rule engine implements the revealed preference rule with range support 
closing rule engine checks if auction termination condition reached 
wd engine provides wd-related logic 
pricing engine provides pricing-related logic 
mip builders translates logic used by engines into our general optimizer formulation 
pricing builders used by three pricing stages 
winner determination builders used by wd activity rule closing rule and pricing constraint generation 
framework support code eases modular replacement of above components 
table exchange component and code breakdown 
 systems infrastructure 
ice is approximately lines of java code broken up 
into the functional packages described in table 
the prototype is modular so that researchers may easily 
replace components for experimentation in addition to the 
core exchange discussed in this paper we have developed 
an agent component that allows a user to simulate the 
behavior and knowledge of other players in the system better 
allowing a user to formulate their strategy in advance of 
actual play a user specifies a valuation model in an 
xmlinterpretation of our bidding language which is revealed to 
the exchange via the agent s strategy 
major exchange tasks are handled by engines that 
dictate the non-optimizer specific logic these engines drive 
the appropriate mip lp builders we realized that all of 
our optimization formulations boil down to two classes of 
optimization problem the first used by winner 
determination activity rule closing rule and constraint generation 
in pricing is a mip that finds trades that maximize value 
holding prices and slacks constant the second used by the 
three pricing stages is an lp that holds trades constant 
seeking to minimize slack profit or prices we take 
advantage of the commonality of these problems by using common 
lp mip builders that differ only by a few functional hooks 
to provide the correct variables for optimization 
we have generalized our back-end optimization solver 
interface 
 we currently support cplex and the lgpl- 
licensed lpsolve and can take advantage of the load-balancing 
and parallel mip lp solving capability that this library 
provides 
 discussion 
the bidding language was defined to allow for perfect 
symmetry between buyers and sellers and provide expressiveness 
in an exchange domain for instance for mixed bidders 
interested in executing trades such as swaps this proved 
especially challenging the breakthrough came when we 
focused on changes in value for trades rather than providing 
absolute values for allocations for simplicity we require the 
same tree structure for both the upper and lower valuations 
 
code size is measured in physical source line of code 
 sloc as generated using david a wheeler s sloc 
count the total of includes for instrumentation 
 not shown in the table the jopt solver interface is 
another lines and castor automatically generates around 
 lines of code for xml file manipulation 
 
http econcs eecs harvard edu jopt 
this allows the language itself to ensure consistency with 
the upper value at least the lower value on all trades and 
enforce monotonic tightening of these bounds for all trades 
across rounds it also provides for an efficient method to 
check the rp activity rule because it makes it simple to 
reason about shared uncertainty between trades 
the decision to adopt a direct and proxied approach 
in which bidders express their upper and lower values to a 
trusted proxy agent that interacts with the exchange was 
made early in the design process in many ways this is 
the clearest and most immediate way to generalize the 
design in parkes et al and make it iterative in addition 
this removes much opportunity for strategic manipulation 
bidders are restricted to making incremental statements 
about their valuations another advantage is that it makes 
the activity rule easy to explain bidders can always meet 
the activity rule by tightening bounds such that their true 
value remains in the support 
perhaps most importantly 
having explicit information on upper and lower values 
permits progress in early rounds even while there is no efficient 
trade at pessimistic values 
upper and lower bound information also provides 
guidance about when to terminate note that taken by itself 
pp po does not imply that the current provisional trade 
is efficient with respect to all values consistent with current 
value information the difference in values between 
different trades aggregated across all bidders could be similar at 
lower and upper bounds but quite different at intermediate 
values including truth nevertheless we conjecture that 
pp po will prove an excellent indicator of efficiency in 
practical settings where the shape of the upper and lower 
valuations does convey useful information this is worthy of 
experimental investigation moreover the use of price and 
rp activity provides additional guarantees 
we adopted linear prices prices on individual items rather 
than non-linear prices with prices on a trade not equal to 
the sum of the prices on the component items early in the 
design process the conciseness of this price representation 
is very important for computational tractability within the 
exchange and also to promote simplicity and transparency 
for bidders the rp activity rule was adopted later and is 
a good choice because of its excellent theoretical properties 
when coupled with ce prices the following can be easily 
established given exact ce prices pt− 
for provisional trade 
 
this is in contrast to indirect price-based approaches such 
as clock-proxy in which bidders must be able to reason 
about the rp-constraints implied by bids in each round 
 
λt− 
at valuations vα 
 then if the upper and lower values at 
the start of round t already satisfy the rp rule and without 
the need for any tie-breaking the provisional trade is 
efficient for all valuations consistent with the current bid trees 
when linear ce prices exist this provides for a soundness 
and completeness statement if pp po linear ce prices 
exist and the rp rule is satisfied the provisional trade is 
efficient soundness if prices are exact ce prices for the 
provisional trade at vα 
 but the trade is inefficient with 
respect to some valuation profile consistent with the current 
bid trees then at least one bidder must fail rp with her 
current bid tree and progress will be made completeness 
future work must study convergence experimentally and 
extend this theory to allow for approximate prices 
some strategic aspects of our ice design deserve 
comment and further study first we do not claim that 
truthfully responding to the rp rule is an ex post equilibrium 
however the exchange is designed to mimic the threshold 
rule in its payment scheme which is known to have 
useful incentive properties we must be careful though 
for instance we do not suggest to provide αeff 
to bidders 
because as αeff 
approaches it would inform bidders that 
bid values are becoming irrelevant to determining the trade 
but merely used to determine payments and bidders would 
become increasingly reluctant to increase their lower 
valuations also no consideration has been given in this work 
to collusion by bidders this is an issue that deserves some 
attention in future work 
 conclusions 
in this work we designed and prototyped a scalable and 
highly-expressive iterative combinatorial exchange the 
design includes many interesting features including a new 
bid-tree language for exchanges a new method to construct 
approximate linear prices from expressive languages and a 
proxied elicitation method with optimistic and pessimistic 
valuations with a new method to evaluate a revealed- 
preference activity rule the exchange is fully implemented in 
java and is in a validation phase 
the next steps for our work are to allow bidders to refine 
the structure of the bid tree in addition to values on the 
tree we intend to study the elicitation properties of the 
exchange and we have put together a test suite of exchange 
problem instances in addition we are beginning to engage 
in collaborations to apply the design to airline takeoff and 
landing slot scheduling and to resource allocation in 
widearea network distributed computational systems 
acknowledgments 
we would like to dedicate this paper to all of the participants 
in cs r at harvard university in spring this work 
is supported in part by nsf grant iis- 
 references 
 l ausubel p cramton and p milgrom the clock-proxy 
auction a practical combinatorial auction design in cramton 
et al chapter 
 m babaioff n nisan and e pavlov mechanisms for a 
spatially distributed market in proc th acm conf on 
electronic commerce pages - acm press 
 
given the myerson-satterthwaite impossibility 
theorem and the method by which we determine the trade 
we should not expect this 
 m ball g donohue and k hoffman auctions for the safe 
efficient and equitable allocation of airspace system resources 
in s cramton shoham editor combinatorial auctions 
 forthcoming 
 d bertsimas and j tsitsiklis introduction to linear 
optimization athena scientific 
 s bikhchandani and j m ostroy the package assignment 
model journal of economic theory - 
 c boutilier a pomdp formulation of preference elicitation 
problems in proc th national conference on artificial 
intelligence aaai- 
 c boutilier and h hoos bidding languages for combinatorial 
auctions in proc th international joint conference on 
artificial intelligence ijcai- 
 w conen and t sandholm preference elicitation in 
combinatorial auctions in proc rd acm conf on 
electronic commerce ec- pages - acm press 
new york 
 p cramton y shoham and r steinberg editors 
combinatorial auctions mit press 
 s de vries j schummer and r v vohra on ascending 
vickrey auctions for heterogeneous objects technical report 
meds kellogg school northwestern university 
 s de vries and r v vohra combinatorial auctions a 
survey informs journal on computing - 
 m dunford k hoffman d menon r sultana and 
t wilson testing linear pricing algorithms for use in 
ascending combinatorial auctions technical report seor 
george mason university 
 y fu j chase b chun s schwab and a vahdat sharp 
an architecture for secure resource peering in proceedings of 
the nineteenth acm symposium on operating systems 
principles pages - acm press 
 b hudson and t sandholm effectiveness of query types and 
policies for preference elicitation in combinatorial auctions in 
proc rd int joint conf on autonomous agents and multi 
agent systems pages - 
 v krishna auction theory academic press 
 d krych calculation and analysis of nash equilibria of 
vickrey-based payment rules for combinatorial exchanges 
harvard college april 
 a m kwasnica j o ledyard d porter and c demartini 
a new and improved design for multi-object iterative auctions 
management science to appear 
 e kwerel and j williams a proposal for a rapid transition to 
market allocation of spectrum technical report fcc office of 
plans and policy nov 
 s m lahaie and d c parkes applying learning algorithms 
to preference elicitation in proc acm conf on electronic 
commerce pages - 
 r p mcafee a dominant strategy double auction j of 
economic theory - 
 p milgrom putting auction theory to work the simultaneous 
ascending auction j pol econ - 
 r b myerson and m a satterthwaite efficient mechanisms 
for bilateral trading journal of economic theory 
 - 
 n nisan bidding and allocation in combinatorial auctions in 
proc nd acm conf on electronic commerce ec- 
pages - 
 d c parkes j r kalagnanam and m eso achieving 
budget-balance with vickrey-based payment schemes in 
exchanges in proc th international joint conference on 
artificial intelligence ijcai- pages - 
 d c parkes and l h ungar iterative combinatorial 
auctions theory and practice in proc th national 
conference on artificial intelligence aaai- pages 
 - july 
 s j rassenti v l smith and r l bulfin a combinatorial 
mechanism for airport time slot allocation bell journal of 
economics - 
 m h rothkopf a pekeˇc and r m harstad 
computationally manageable combinatorial auctions 
management science - 
 t sandholm and c boutilier preference elicitation in 
combinatorial auctions in cramton et al chapter 
 p r wurman and m p wellman akba a progressive 
anonymous-price combinatorial auction in second acm 
conference on electronic commerce pages - 
 
networks preserving evolutionary equilibria 
and the power of randomization 
michael kearns 
mkearns cis upenn edu 
siddharth suri 
ssuri cis upenn edu 
department of computer and information science 
university of pennsylvania 
philadelphia pa 
abstract 
we study a natural extension of classical evolutionary game 
theory to a setting in which pairwise interactions are 
restricted to the edges of an undirected graph or network we 
generalize the definition of an evolutionary stable strategy 
 ess and show a pair of complementary results that 
exhibit the power of randomization in our setting subject 
to degree or edge density conditions the classical ess of 
any game are preserved when the graph is chosen randomly 
and the mutation set is chosen adversarially or when the 
graph is chosen adversarially and the mutation set is chosen 
randomly we examine natural strengthenings of our 
generalized ess definition and show that similarly strong results 
are not possible for them 
categories and subject descriptors 
j social and behavioral sciences economics 
general terms 
economics theory 
 introduction 
in this paper we introduce and examine a natural 
extension of classical evolutionary game theory egt to a setting 
in which pairwise interactions are restricted to the edges of 
an undirected graph or network this extension generalizes 
the classical setting in which all pairs of organisms in an 
infinite population are equally likely to interact the 
classical setting can be viewed as the special case in which the 
underlying network is a clique 
there are many obvious reasons why one would like to 
examine more general graphs the primary one being in that 
many scenarios considered in evolutionary game theory all 
interactions are in fact not possible for example 
geographical restrictions may limit interactions to physically 
proximate pairs of organisms more generally as evolutionary 
game theory has become a plausible model not only for 
biological interaction but also economic and other kinds of 
interaction in which certain dynamics are more imitative than 
optimizing see and chapter of the network 
constraints may come from similarly more general sources 
evolutionary game theory on networks has been considered 
before but not in the generality we will do so here see 
section 
we generalize the definition of an evolutionary stable 
strategy ess to networks and show a pair of complementary 
results that exhibit the power of randomization in our 
setting subject to degree or edge density conditions the 
classical ess of any game are preserved when the graph is 
chosen randomly and the mutation set is chosen adversarially 
or when the graph is chosen adversarially and the mutation 
set is chosen randomly we examine natural strengthenings 
of our generalized ess definition and show that similarly 
strong results are not possible for them 
the work described here is part of recent efforts 
examining the relationship between graph topology or structure 
and properties of equilibrium outcomes previous works in 
this line include studies of the relationship of topology to 
properties of correlated equilibria in graphical games 
and studies of price variation in graph-theoretic market 
exchange models more generally this work contributes 
to the line of graph-theoretic models for game theory 
investigated in both computer science and economics 
 classical egt 
the fundamental concept of evolutionary game theory is 
the evolutionarily stable strategy ess intuitively an ess 
is a strategy such that if all the members of a population 
adopt it then no mutant strategy could invade the 
population to make this more precise we describe the basic 
model of evolutionary game theory in which the notion of 
an ess resides 
the standard model of evolutionary game theory 
considers an infinite population of organisms each of which plays 
a strategy in a fixed -player symmetric game the game 
is defined by a fitness function f all pairs of members of 
the infinite population are equally likely to interact with one 
another if two organisms interact one playing strategy s 
 
and the other playing strategy t the s-player earns a fitness 
of f s t while the t-player earns a fitness of f t s 
in this infinite population of organisms suppose there is a 
 − fraction who play strategy s and call these organisms 
incumbents and suppose there is an fraction who play t 
and call these organisms mutants assume two organisms 
are chosen uniformly at random to play each other the 
strategy s is an ess if the expected fitness of an 
organism playing s is higher than that of an organism playing t 
for all t s and all sufficiently small since an 
incumbent will meet another incumbent with probability − 
and it will meet a mutant with probability we can 
calculate the expected fitness of an incumbent which is simply 
 − f s s f s t similarly the expected fitness of 
a mutant is − f t s f t t thus we come to the 
formal definition of an ess 
definition a strategy s is an evolutionarily stable 
strategy ess for the -player symmetric game given by 
fitness function f if for every strategy t s there exists 
an t such that for all t − f s s f s t 
 − f t s f t t 
a consequence of this definition is that for s to be an ess 
it must be the case that f s s ≥ f t s for all strategies 
t this inequality means that s must be a best response 
to itself and thus any ess strategy s must also be a nash 
equilibrium in general the notion of ess is more 
restrictive than nash equilibrium and not all -player symmetric 
games have an ess 
in this paper our interest is to examine what kinds of 
network structure preserve the ess strategies for those games 
that do have a standard ess first we must of course 
generalize the definition of ess to a network setting 
 egt on graphs 
in our setting we will no longer assume that two 
organisms are chosen uniformly at random to interact instead 
we assume that organisms interact only with those in their 
local neighborhood as defined by an undirected graph or 
network as in the classical setting which can be viewed 
as the special case of the complete network or clique we 
shall assume an infinite population by which we mean we 
examine limiting behavior in a family of graphs of increasing 
size 
before giving formal definitions some comments are in 
order on what to expect in moving from the classical to the 
graph-theoretic setting in the classical complete graph 
setting there exist many symmetries that may be broken in 
moving to the the network setting at both the group and 
individual level indeed such asymmetries are the primary 
interest in examining a graph-theoretic generalization 
for example at the group level in the standard ess 
definition one need not discuss any particular set of mutants of 
population fraction since all organisms are equally likely 
to interact the survival or fate of any specific mutant set is 
identical to that of any other in the network setting this 
may not be true some mutant sets may be better able to 
survive than others due to the specific topologies of their 
interactions in the network for instance foreshadowing some 
of our analysis if s is an ess but f t t is much larger than 
f s s and f s t a mutant set with a great deal of 
internal interaction that is edges between mutants may be 
able to survive whereas one without this may suffer at 
the level of individuals in the classical setting the assertion 
that one mutant dies implies that all mutants die again by 
symmetry in the network setting individual fates may 
differ within a group all playing a common strategy these 
observations imply that in examining ess on networks we 
face definitional choices that were obscured in the classical 
model 
if g is a graph representing the allowed pairwise 
interactions between organisms vertices and u is a vertex of g 
playing strategy su then the fitness of u is given by 
f u 
p 
v∈γ u f su sv 
 γ u 
 
here sv is the strategy being played by the neighbor v and 
γ u {v ∈ v u v ∈ e} one can view the fitness of 
u as the average fitness u would obtain if it played each if 
its neighbors or the expected fitness u would obtain if it 
were assigned to play one of its neighbors chosen uniformly 
at random 
classical evolutionary game theory examines an infinite 
symmetric population graphs or networks are inherently 
finite objects and we are specifically interested in their 
asymmetries as discussed above thus all of our definitions shall 
revolve around an infinite family g {gn}∞ 
n of finite 
graphs gn over n vertices but we shall examine asymptotic 
 large n properties of such families 
we first give a definition for a family of mutant vertex 
sets in such an infinite graph family to contract 
definition let g {gn}∞ 
n be an infinite family 
of graphs where gn has n vertices let m {mn}∞ 
n 
be any family of subsets of vertices of the gn such that 
 mn ≥ n for some constant suppose all the vertices 
of mn play a common mutant strategy t and suppose the 
remaining vertices in gn play a common incumbent 
strategy s we say that mn contracts if for sufficiently large n 
for all but o n of the j ∈ mn j has an incumbent neighbor 
i such that f j f i 
a reasonable alternative would be to ask that the 
condition above hold for all mutants rather than all but o n 
note also that we only require that a mutant have one 
incumbent neighbor of higher fitness in order to die one might 
considering requiring more in sections and we 
consider these stronger conditions and demonstrate that our 
results can no longer hold 
in order to properly define an ess for an infinite family of 
finite graphs in a way that recovers the classical definition 
asymptotically in the case of the family of complete graphs 
we first must give a definition that restricts attention to 
families of mutant vertices that are smaller than some invasion 
threshold n yet remain some constant fraction of the 
population this prevents invasions that survive merely by 
constituting a vanishing fraction of the population 
definition let and let g {gn}∞ 
n be 
an infinite family of graphs where gn has n vertices let 
m {mn}∞ 
n be any family of mutant vertices in gn 
we say that m is -linear if there exists an 
such that for all sufficiently large n n mn n 
we can now give our definition for a strategy to be 
evolutionarily stable when employed by organisms interacting 
with their neighborhood in a graph 
 
definition let g {gn}∞ 
n be an infinite family 
of graphs where gn has n vertices let f be any -player 
symmetric game for which s is a strategy we say that s is 
an ess with respect to f and g if for all mutant strategies 
t s there exists an t such that for any t-linear 
family of mutant vertices m {mn}∞ 
n all playing t for n 
sufficiently large mn contracts 
thus to violate the ess property for g one must witness 
a family of mutations m in which each mn is an arbitrarily 
small but nonzero constant fraction of the population of gn 
but does not contract i e every mutant set has a subset of 
linear size that survives all of its incumbent interactions 
in section a we show that the definition given coincides 
with the classical one in the case where g is the family of 
complete graphs in the limit of large n we note that even 
in the classical model small sets of mutants were allowed to 
have greater fitness than the incumbents as long as the size 
of the set was o n 
in the definition above there are three parameters the 
game f the graph family g and the mutation family m 
our main results will hold for any -player symmetric game 
f we will also study two rather general settings for g and 
m that in which g is a family of random graphs and m is 
arbitrary and that in which g is nearly arbitrary and m is 
randomly chosen in both cases we will see that subject to 
conditions on degree or edge density essentially forcing 
connectivity of g but not much more for any -player 
symmetric game the ess of the classical settings and only those 
strategies are always preserved thus a common theme of 
these results is the power of randomization as long as either 
the network itself is chosen randomly or the mutation set is 
chosen randomly classical ess are preserved 
 related work 
there has been previous work that analyzes which 
strategies are resilient to mutant invasions with respect to various 
types of graphs what sets our work apart is that the model 
we consider encompasses a significantly more general class of 
games and graph topologies we will briefly survey this 
literature and point out the differences in the previous models 
and ours 
in and the authors consider specific families 
of graphs such as cycles and lattices where players play 
specific games such as × -games or k × k-coordination 
games in these papers the authors specify a simple local 
dynamic for players to improve their payoffs by changing 
strategies and analyze what type of strategies will grow to 
dominate the population the model we propose is more 
general than both of these as it encompasses a larger class 
of graphs as well as a richer set of games 
also related to our work is that of where the authors 
propose two models the first assumes organisms interact 
according to a weighted undirected graph however the 
fitness of each organism is simply assigned and does not 
depend on the actions of each organism s neighborhood the 
second model has organisms arranged around a directed 
cycle where neighbors play a × -game with probability 
proportional to its fitness an organism is chosen to 
reproduce by placing a replica of itself in its neighbors position 
thereby killing the neighbor we consider more general 
games than the first model and more general graphs than 
the second 
finally the works most closely related to ours are 
and the authors consider -action coordination games 
played by players in a general undirected graph in these 
three works the authors specify a dynamic for a strategy to 
reproduce and analyze properties of the graph that allow a 
strategy to overrun the population here again one can see 
that our model is more general than these as it allows for 
organisms to play any -player symmetric game 
 networks preserving ess 
we now proceed to state and prove two complementary 
results in the network ess model defined in section first 
we consider a setting where the graphs are generated via the 
gn p model of erd˝os and r´enyi in this model every 
pair of vertices are joined by an edge independently and 
with probability p where p may depend on n the mutant 
set however will be constructed adversarially subject to 
the linear size constraint given by definition for these 
settings we show that for any -player symmetric game s 
is a classical ess of that game if and only if s is an ess 
for {gn p}∞ 
n where p ω nc 
 and ≤ c and any 
mutant family {mn}∞ 
n where each mn has linear size we 
note that under these settings if we let c − γ for small 
γ the expected number of edges in gn is n γ 
or larger 
- that is just superlinear in the number of vertices and 
potentially far smaller than o n 
 it is easy to convince 
oneself that once the graphs have only a linear number of 
edges we are flirting with disconnectedness and there may 
simply be large mutant sets that can survive in isolation due 
to the lack of any incumbent interactions in certain games 
thus in some sense we examine the minimum plausible edge 
density 
the second result is a kind of dual to the first considering 
a setting where the graphs are chosen arbitrarily subject to 
conditions but the mutant sets are chosen randomly it 
states that for any -player symmetric game s is a 
classical ess for that game if and only if s is an ess for any 
{gn vn en }∞ 
n in which for all v ∈ vn deg v ω nγ 
 
 for any constant γ and a family of mutant sets 
{mn}∞ 
n that is chosen randomly that is in which each 
organism is labeled a mutant with constant probability 
thus in this setting we again find that classical ess are 
preserved subject to edge density restrictions since the degree 
assumption is somewhat strong we also prove another result 
which only assumes that en ≥ n γ 
 and shows that there 
must exist at least mutant with an incumbent neighbor of 
higher fitness as opposed to showing that all but o n 
mutants have an incumbent neighbor of higher fitness as will 
be discussed this rules out stationary mutant invasions 
 random graphs adversarial mutations 
now we state and prove a theorem which shows that if s 
is a classical ess then s will be an ess for random graphs 
where a linear sized set of mutants is chosen by an adversary 
theorem let f be any -player symmetric game 
and suppose s is a classical ess of f let the infinite 
graph family {gn}∞ 
n be drawn according to gn p where 
p ω nc 
 and ≤ c then with probability s is 
an ess 
the main idea of the proof is to divide mutants into 
categories those with normal fitness and those with 
ab 
normal fitness first we show all but o n of the 
population incumbent or mutant have an incumbent neighbor of 
normal fitness this will imply that all but o n of the 
mutants of normal fitness have an incumbent neighbor of higher 
fitness the vehicle for proving this is theorem of 
which gives an upper bound on the number of vertices not 
connected to a sufficiently large set this theorem assumes 
that the size of this large set is known with equality which 
necessitates the union bound argument below secondly we 
show that there can be at most o n mutants with abnormal 
fitness since there are so few of them even if none of them 
have an incumbent neighbor of higher fitness s will still be 
an ess with respect to f and g 
proof sketch let t s be the mutant strategy since 
s is a classical ess there exists an t such that − f s s 
f s t − f t s f t t for all t let m 
be any mutant family that is t-linear thus for any fixed 
value of n that is sufficiently large there exists an such 
that mn n and t also let in vn \ mn and 
let i ⊆ in be the set of incumbents that have fitness in the 
range ± τ − f s s f s t for some constant τ 
 τ lemma below shows − n ≥ i ≥ 
 − n − log n 
τ p 
 finally let 
ti {x ∈ v \ i γ x ∩ i ∅} 
 for the sake of clarity we suppress the subscript n on the 
sets i and t the union bound gives us 
pr ti ≥ δn ≤ 
 − n 
x 
i − n− log n 
τ p 
pr ti ≥ δn and i i 
letting δ n−γ 
for some γ gives δn o n we will 
apply theorem of to the summand on the right hand 
side of equation if we let γ −c and combine this 
with the fact that ≤ c all of the requirements of this 
theorem will be satisfied details omitted now when we 
apply this theorem to equation we get 
pr ti ≥ δn ≤ 
 − n 
x 
i − n− log n 
τ p 
exp 
„ 
− 
 
 
cδn 
 
 
 o 
this is because equation has only log n 
τ p 
terms and 
theorem of gives us that c ≥ − n −c 
− log n 
τ 
thus we have shown with probability tending to as n → 
∞ at most o n individuals are not attached to an 
incumbent which has fitness in the range ± τ − f s s 
f s t this implies that the number of mutants of 
approximately normal fitness not attached to an incumbent 
of approximately normal fitness is also o n 
now those mutants of approximately normal fitness that 
are attached to an incumbent of approximately normal 
fitness have fitness in the range ±τ − f t s f t t 
the incumbents that they are attached to have fitness in the 
range ±τ − f s s f s t since s is an ess of f 
we know − f s s f s t − f t s f t t thus 
if we choose τ small enough we can ensure that all but o n 
mutants of normal fitness have a neighboring incumbent of 
higher fitness 
finally by lemma we know there are at most o n 
mutants of abnormal fitness so even if all of them are more fit 
than their respective incumbent neighbors we have shown 
all but o n of the mutants have an incumbent neighbor of 
higher fitness 
we now state and prove the lemma used in the proof 
above 
lemma for almost every graph gn p with − n 
incumbents all but log n 
δ p 
incumbents have fitness in the 
range ±δ − f s s f s t where p ω nc 
 and 
 δ and c are constants satisfying δ 
 ≤ c similarly under the same assumptions all 
but log n 
δ p 
mutants have fitness in the range ± δ − 
 f t s f t t 
proof we define the mutant degree of a vertex to be 
the number of mutant neighbors of that vertex and 
incumbent degree analogously observe that the only way for 
an incumbent to have fitness far from its expected value of 
 − f s s f s t is if it has a fraction of mutant 
neighbors either much higher or much lower than theorem 
 of gives us a bound on the number of such 
incumbents it states that the number of incumbents with mutant 
degree outside the range ± δ p m is at most log n 
δ p 
 by 
the same theorem the number of incumbents with 
incumbent degree outside the range ± δ p i is at most log n 
δ p 
 
from the linearity of fitness as a function of the fraction 
of mutant or incumbent neighbors one can show that for 
those incumbents with mutant and incumbent degree in the 
expected range their fitness is within a constant factor of 
 − f s s f s t where that constant goes to as n 
tends to infinity and δ tends to the proof for the mutant 
case is analogous 
we note that if in the statement of theorem we let 
c then p this in turn makes g {kn}∞ 
n 
where kn is a clique of n vertices then for any kn all 
of the incumbents will have identical fitness and all of the 
mutants will have identical fitness furthermore since s was 
an ess for g the incumbent fitness will be higher than the 
mutant fitness finally one can show that as n → ∞ the 
incumbent fitness converges to − f s s f s t and 
the mutant fitness converges to − f t s f t t in 
other words s must be a classical ess providing a converse 
to theorem we rigorously present this argument in 
section a 
 adversarial graphs random mutations 
we now move on to our second main result here we show 
that if the graph family rather than being chosen randomly 
is arbitrary subject to a minimum degree requirement and 
the mutation sets are randomly chosen classical ess are 
again preserved a modified notion of ess allows us to 
considerably weaken the degree requirement to a minimum 
edge density requirement 
theorem let g {gn vn en }∞ 
n be an 
infinite family of graphs in which for all v ∈ vn deg v ω nγ 
 
 for any constant γ let f be any -player symmetric 
game and suppose s is a classical ess of f let t be any 
mutant strategy and let the mutant family m {mn}∞ 
n be 
chosen randomly by labeling each vertex a mutant with 
constant probability where t then with probability 
 s is an ess with respect to f g and m 
 
proof let t s be the mutant strategy and let x be 
the event that every incumbent has fitness within the range 
 ± τ − f s s f s t for some constant τ to 
be specified later similarly let y be the event that every 
mutant has fitness within the range ± τ − f t s 
f t t since pr x ∩ y − pr ¬x ∪ ¬y we proceed 
by showing pr ¬x ∪ ¬y o 
¬x is the event that there exists an incumbent with fitness 
outside the range ±τ − f s s f s t if degm v 
denotes the number of mutant neighbors of v similarly 
degi v denotes the number of incumbent neighbors of v 
then an incumbent i has fitness 
degi i 
deg i 
f s s 
degm i 
deg i 
f s t 
since f s s and f s t are fixed quantities the only 
variation in an incumbents fitness can come from variation in the 
terms degi i 
deg i 
and degm i 
deg i 
 one can use the chernoff bound 
followed by the union bound to show that for any incumbent 
i 
pr f i ∈ ± τ − f s s f s t 
 exp 
„ 
− 
deg i τ 
 
 
 
next one can use the union bound again to bound the 
probability of the event ¬x 
pr ¬x ≤ n exp 
„ 
− 
diτ 
 
 
where di mini∈v \m deg i ≤ an analogous 
argument can be made to show pr ¬y n exp − 
dj τ 
 
 
where dj minj∈m deg j and ≤ thus by the 
union bound 
pr ¬x ∪ ¬y n exp 
„ 
− 
dτ 
 
 
where d minv∈v deg v ≤ since deg v 
ω nγ 
 for all v ∈ v and τ and γ are all constants greater 
than 
lim 
n→∞ 
 n 
exp dτ 
 
so pr ¬x∪¬y o thus we can choose τ small enough 
such that τ − f t s f t t − τ − 
 f s s f s t and then choose n large enough such that 
with probability − o every incumbent will have fitness 
in the range ±τ − f s s f s t and every mutant 
will have fitness in the range ± τ − f t s f t t 
so with high probability every incumbent will have a higher 
fitness than every mutant 
by arguments similar to those following the proof of 
theorem if we let g {kn}∞ 
n each incumbent will have 
the same fitness and each mutant will have the same fitness 
furthermore since s is an ess for g the incumbent fitness 
must be higher than the mutant fitness here again one 
has to show show that as n → ∞ the incumbent fitness 
converges to − f s s f s t and the mutant fitness 
converges to − f t s f t t observe that the exact 
fraction mutants of vn is now a random variable so to prove 
this convergence we use an argument similar to one that is 
used to prove that sequence of random variables that 
converges in probability also converges in distribution details 
omitted this in turn establishes that s must be a classical 
ess and we thus obtain a converse to theorem this 
argument is made rigorous in section a 
the assumption on the degree of each vertex of 
theorem is rather strong the following theorem relaxes 
this requirement and only necessitates that every graph have 
n γ 
edges for some constant γ in which case it shows 
there will alway be at least mutant with an incumbent 
neighbor of higher fitness a strategy that is an ess in 
this weakened sense will essentially rule out stable static 
sets of mutant invasions but not more complex invasions 
an example of more complex invasions are mutant sets that 
survive but only by perpetually migrating through the 
graph under some natural evolutionary dynamics akin to 
gliders in the well-known game of life 
theorem let f be any game and let s be a classical 
ess of f and let t s be a mutant strategy for any graph 
family g {gn vn en }∞ 
n in which en ≥ n γ 
 for 
any constant γ and any mutant family m {mn}∞ 
n 
which is determined by labeling each vertex a mutant with 
probability where t the probability that there 
exists a mutant with an incumbent neighbor of higher fitness 
approaches as n → ∞ 
proof sketch the main idea behind the proof is to 
show that with high probability over only the choice of 
mutants there will be an incumbent-mutant edge in which both 
vertices have high degree if their degree is high enough we 
can show that close to an fraction of their neighbors are 
mutants and thus their fitnesses are very close to what we 
expect them to be in the classical case since s is an ess 
the fitness of the incumbent will be higher than the mutant 
we call an edge i j ∈ en a g n -barbell if deg i ≥ g n 
and deg j ≥ g n suppose gn has at most h n edges that 
are g n -barbells this means there are at least en − h n 
edges in which at least one vertex has degree at most g n 
we call these vertices light vertices let n be the number 
of light vertices in gn observe that en −h n ≤ n g n 
this is because each light vertex is incident on at most g n 
edges this gives us that 
 en ≤ h n n g n ≤ h n ng n 
so if we choose h n and g n such that h n ng n 
o n γ 
 then en o n γ 
 this contradicts the 
assumption that en ω n γ 
 thus subject to the above 
constraint on h n and g n gn must contain at least h n 
edges that are g n -barbells 
now let hn denote the subgraph induced by the barbell 
edges of gn note that regardless of the structure of gn 
there is no reason that hn should be connected thus let 
m be the number of connected components of hn and let 
c c cm be the number of vertices in each of these 
connected components note that since hn is an edge-induced 
subgraph we have ck ≥ for all components k let us choose 
the mutant set by first flipping the vertices in hn only we 
now show that the probability with respect to the random 
mutant set that none of the components of hn have an 
incumbent-mutant edge is exponentially small in n let an 
be the event that every component of hn contains only 
mutants or only incumbents then algebraic manipulations can 
establish that 
pr an πm 
k ck 
 − ck 
 
≤ − − β 
 
 
pm 
k ck 
 
where β is a constant thus for sufficiently small the bound 
decreases exponentially with 
pm 
k ck furthermore sincepm 
k 
`ck 
 
´ 
≥ h n with equality achieved by making each 
component a clique one can show that 
pm 
k ck ≥ 
p 
h n 
thus as long as h n → ∞ with n the probability that all 
components are uniformly labeled will go to 
now assuming that there exists a non-uniformly labeled 
component by construction that component contains an 
edge i j where i is an incumbent and j is a mutant that 
is a g n -barbell we also assume that the h n vertices 
already labeled have been done so arbitrarily but that the 
remaining g n − h n vertices neighboring i and j are 
labeled mutants independently with probability then via 
a standard chernoff bound argument one can show that 
with high probability the fraction of mutants neighboring 
i and the fraction of mutants neighboring j is in the range 
 ± τ g n −h n 
g n 
 similarly one can show that the 
fraction of incumbents neighboring i and the fraction of mutants 
neighboring j is in the range − ± τ g n −h n 
g n 
 
since s is an ess there exists a ζ such that − 
 f s s f s t − f t s f t t ζ if we 
choose g n nγ 
 and h n o g n we can choose n 
large enough and τ small enough to force f i f j as 
desired 
 limitations of stronger models 
in this section we show that if one tried to strengthen 
the model described in section in two natural ways one 
would not be able to prove results as strong as theorems 
and which hold for every -player symmetric game 
 stronger contraction for the mutant set 
in section we alluded to the fact that we made certain 
design decisions in arriving at definitions and 
one such decision was to require that all but o n mutants 
have incumbent neighbors of higher fitness instead we 
could have required that all mutants have an incumbent 
neighbor of higher fitness the two theorems in this 
subsection show that if one were to strengthen our notion of 
contraction for the mutant set given by definition in 
this way it would be impossible to prove theorems analogous 
to theorems and 
recall that definition gave the notion of contraction 
for a linear sized subset of mutants in what follows we 
will say an edge i j contracts if i is an incumbent j is a 
mutant and f i f j also recall that theorem 
stated that if s is a classical ess then it is an ess for 
random graphs with adversarial mutations next we prove 
that if we instead required every incumbent-mutant edge to 
contract this need not be the case 
theorem let f be a -player symmetric game that 
has a classical ess s for which there exists a mutant 
strategy t s with f t t f s s and f t t f s t let 
g {gn}∞ 
n be an infinite family of random graphs drawn 
according to gn p where p ω nc 
 for any constant 
 ≤ c then with probability approaching as n → ∞ 
there exists a mutant family m {mn}∞ 
n where tn 
 mn n and t in which there is an edge that does 
not contract 
proof sketch with probability approaching as n → 
∞ there exists a vertex j where deg j is arbitrarily close 
to n so label j mutant label one of its neighbors 
incumbent denoted i and label the rest of j s neighborhood 
mutant also label all of i s neighbors incumbent with 
the exception of j and j s neighbors which were already 
labeled mutant in this setting one can show that f j 
will be arbitrarily close to f t t and f i will be a convex 
combination of f s s and f s t which are both strictly 
less than f t t 
theorem stated that if s is a classical ess then for 
graphs where en ≥ n γ 
 for some γ and where each 
organism is labeled a mutant with probability one edge 
must contract below we show that for certain graphs and 
certain games there will always exist one edge that will not 
contract 
theorem let f be a -player symmetric game that 
has a classical ess s such that there exists a mutant 
strategy t s where f t s f s t there exists an infinite 
family of graphs {gn vn en }∞ 
n where en θ n 
 
such that for a mutant family m {mn}∞ 
n which is 
determined by labeling each vertex a mutant with probability 
 the probability there exists an edge in en that does 
not contract approaches as n → ∞ 
proof sketch construct gn as follows pick n 
vertices u u un and add edges such that they from 
a clique then for each ui i ∈ n add edges ui vi 
 vi wi and wi xi with probability as n → ∞ there 
exists an i such that ui and wi are mutants and vi and xi 
are incumbents observe that f vi f xi f s t and 
f wi f t s 
 stronger contraction for individuals 
the model of section requires that for an edge i j to 
contract the fitness of i must be greater than the fitness of j 
one way to strengthen this notion of contraction would be 
to require that the maximum fitness incumbent in the 
neighborhood of j be more fit than the maximum fitness mutant 
in the neighborhood of j this models the idea that each 
organism is trying to take over each place in its 
neighborhood but only the most fit organism in the neighborhood 
of a vertex gets the privilege of taking it if we assume that 
we adopt this notion of contraction for individual mutants 
and require that all incumbent-mutant edges contract we 
will next show that theorems and still hold and 
thus it is still impossible to get results such as theorems 
and which hold for every -player symmetric game 
in the proof of theorem we proved that f i is strictly 
less than f j observe that maximum fitness mutant in 
the neighborhood of j must have fitness at least f j also 
observe that there is only incumbent in the neighborhood 
of j namely i so under this stronger notion of contraction 
the edge i j will not contract 
similarly in the proof of theorem observe that the 
only mutant in the neighborhood of wi is wi itself which 
has fitness f t s furthermore the only incumbents in the 
neighborhood of wi are vi and xi both of which have fitness 
f s t by assumption f t s f s t thus under this 
stronger notion of contraction neither of the 
incumbentmutant edges vi wi and xi wi will contract 
 references 
 elwyn r berlekamp john horton conway and 
richard k guy winning ways for your 
 
mathematical plays volume ak peters ltd march 
 
 jonas bj¨ornerstedt and karl h schlag on the 
evolution of imitative behavior discussion paper 
b- university of bonn 
 l e blume the statistical mechanics of strategic 
interaction games and economic behavior 
 - 
 l e blume the statistical mechanics of 
best-response strategy revision games and economic 
behavior - november 
 b bollob´as random graphs cambridge university 
press 
 michael suk-young chwe communication and 
coordination in social networks review of economic 
studies - 
 glenn ellison learning local interaction and 
coordination econometrica - sept 
 
 i eshel l samuelson and a shaked altruists 
egoists and hooligans in a local interaction model 
the american economic review 
 geoffrey r grimmett and david r stirzaker 
probability and random processes oxford university 
press rd edition 
 m jackson a survey of models of network formation 
stability and efficiency in group formation in 
economics networks clubs and coalitions 
cambridge university press 
 s kakade m kearns j langford and l ortiz 
correlated equilibria in graphical games acm 
conference on electronic commerce 
 s kakade m kearns l ortiz r pemantle and 
s suri economic properties of social networks 
neural information processing systems 
 m kearns m littman and s singh graphical 
models for game theory conference on uncertainty in 
artificial intelligence pages - 
 e lieberman c hauert and m a nowak 
evolutionary dynamics on graphs nature 
 - 
 s morris contagion review of economic studies 
 - 
 karl h schlag why imitate and if so how journal 
of economic theory - 
 j m smith evolution and the theory of games 
cambridge university press 
 william l vickery how to cheat against a simple 
mixed strategy ess journal of theoretical biology 
 - 
 j¨orgen w weibull evolutionary game theory the 
mit press 
appendix 
a graphical and classical ess 
in this section we explore the conditions under which a 
graphical ess is also a classical ess to do so we state and 
prove two theorems which provide converses to each of the 
major theorems in section 
a random graphs adversarial mutations 
theorem states that if s is a classical ess and g 
{gn p} where p ω nc 
 and ≤ c then with 
probability as n → ∞ s is an ess with respect to g here we 
show that if s is an ess with respect to g then s is a 
classical ess in order to prove this theorem we do not need the 
full generality of s being an ess for g when p ω nc 
 
where ≤ c all we need is s to be an ess for g when 
p in this case there are no more probabilistic events 
in the theorem statement also since p each graph in 
g is a clique so if one incumbent has a higher fitness than 
one mutant then all incumbents have higher fitness than all 
mutants this gives rise to the following theorem 
theorem a let f be any -player symmetric game 
and suppose s is a strategy for f and t s is a mutant 
strategy let g {kn}∞ 
n if as n → ∞ for any t-linear 
family of mutants m {mn}∞ 
n there exists an incumbent 
i and a mutant j such that f i f j then s is a classical 
ess of f 
the proof of this theorem analyzes the limiting behavior 
of the mutant population as the size of the cliques in g tends 
to infinity it also shows how the definition of ess given in 
section recovers the classical definition of ess 
proof since each graph in g is a clique every 
incumbent will have the same number of incumbent and mutant 
neighbors and every mutant will have the same number of 
incumbent and mutant neighbors thus all incumbents will 
have identical fitness and all mutants will have identical 
fitness next one can construct an t-linear mutant family 
m where the fraction of mutants converges to for any 
where t so for n large enough the number of 
mutants in kn will be arbitrarily close to n thus any 
mutant subset of size n will result in all incumbents having 
fitness − n 
n− 
 f s s n 
n− 
f s t and all mutants 
having fitness − n− 
n− 
 f t s n− 
n− 
f t t furthermore by 
assumption the incumbent fitness must be higher than the 
mutant fitness this implies 
lim 
n→∞ 
„ 
 − 
n 
n − 
 f s s 
n 
n − 
f s t 
 − 
n − 
n − 
 f t s 
n − 
n − 
f t t 
 
 
this implies − f s s f s t − f t s f t t 
for all where t 
a adversarial graphs random mutations 
theorem states that if s is a classical ess for a 
 player symmetric game f where g is chosen adversarially 
subject to the constraint that the degree of each vertex is 
ω nγ 
 for any constant γ and mutants are chosen 
with probability then s is an ess with respect to f g 
and m here we show that if s is an ess with respect to f 
g and m then s is a classical ess 
all we will need to prove this is that s is an ess with 
respect to g {kn}∞ 
n that is when each vertex has degree 
n − as in theorem a since the graphs are cliques if 
one incumbent has higher fitness than one mutant then all 
incumbents have higher fitness than all mutants thus the 
theorem below is also a converse to theorem recall 
that theorem uses a weaker notion of contraction that 
 
requires only one incumbent to have higher fitness than one 
mutant 
theorem a let f be any -player symmetric game 
and suppose s is an incumbent strategy for f and t s 
is a mutant strategy let g {kn}∞ 
n if with 
probability as n → ∞ s is an ess for g and a mutant family 
m {mn}∞ 
n which is determined by labeling each vertex 
a mutant with probability where t then s is a 
classical ess of f 
this proof also analyzes the limiting behavior of the 
mutant population as the size of the cliques in g tends to 
infinity since the mutants are chosen randomly we will use 
an argument similar to the proof that a sequence of random 
variables that converges in probability also converge in 
distribution in this case the sequence of random variables will 
be actual fraction of mutants in each kn 
proof fix any value of where n and 
construct each mn by labeling a vertex a mutant with 
probability by the same argument as in the proof of theorem a 
if the actual number of mutants in kn is denoted by nn 
any mutant subset of size nn will result in all incumbents 
having fitness − nn 
n− 
 f s s nn 
n− 
f s t and in all 
mutants having fitness − nn− 
n− 
 f t s nn− 
n− 
f t t this 
implies 
lim 
n→∞ 
pr s is an ess for gn w r t nn mutants ⇒ 
lim 
n→∞ 
pr 
„ 
 − 
nn 
n − 
 f s s 
nn 
n − 
f s t 
 − 
nn − 
n − 
 f t s 
nn − 
n − 
f t t 
 
 ⇔ 
lim 
n→∞ 
pr 
„ 
n 
f t s − f s s 
f s t − f s s − f t t f t s 
 
f s s − f t t 
n 
 
 
by two simple applications of the chernoff bound and an 
application of the union bound one can show the sequence 
of random variables { n}∞ 
n converges to in probability 
next if we let xn − n x − b −f s s f t t 
and a − f t s −f s s 
f s t −f s s −f t t f t s 
 by theorem a 
below we get that limn→∞ pr xn a b n pr x a 
combining this with equation pr −a 
the proof of the following theorem is very similar to the 
proof that a sequence of random variables that converges in 
probability also converge in distribution a good 
explanation of this can be found in which is the basis for the 
argument below 
theorem a if {xn}∞ 
n is a sequence of random 
variables that converge in probability to the random variable x 
and a and b are constants then limn→∞ pr xn a b n 
pr x a 
proof by lemma a see below we have the following 
two inequalities 
pr x a b n − τ 
≤ pr xn a b n pr x − xn τ 
pr xn a b n 
≤ pr x a b n τ pr x − xn τ 
combining these gives 
pr x a b n − τ − pr x − xn τ 
≤ pr xn a b n 
≤ pr x a b n τ pr x − xn τ 
there exists an n such that for all n n b n τ so 
the following statement holds for all n n 
pr x a − τ − pr x − xn τ 
≤ pr xn a b n 
≤ pr x a τ pr x − xn τ 
take the limn→∞ of both sides of both inequalities and 
since xn converges in probability to x 
pr x a − τ ≤ lim 
n→∞ 
pr xn a b n 
≤ pr x a τ 
recall that x is a continuous random variable representing 
the fraction of mutants in an infinite sized graph so if we 
let fx a pr x a we see that fx a is a cumulative 
distribution function of a continuous random variable and 
is therefore continuous from the right so 
lim 
τ↓ 
fx a − τ lim 
τ↓ 
fx a τ fx a 
thus if we take the limτ↓ of inequalities and we get 
pr x a lim 
n→∞ 
pr xn a b n 
the following lemma is quite useful as it expresses the 
cumulative distribution of one random variable y in terms 
of the cumulative distribution of another random variable 
x and the difference between x and y 
lemma a if x and y are random variables c ∈ 
and τ then 
pr y c ≤ pr x c τ pr y − x τ 
proof 
pr y c 
 pr y c x c τ pr y c x ≥ c τ 
≤ pr y c x c τ pr x c τ 
 pr y − x τ 
≤ pr x c τ pr y − x τ 
 
