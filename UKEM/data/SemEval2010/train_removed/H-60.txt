a frequency-based and a poisson-based definition of the 
probability of being informative 
thomas roelleke 
department of computer science 
queen mary university of london 
thor dcs qmul ac uk 
abstract 
this paper reports on theoretical investigations about the 
assumptions underlying the inverse document frequency idf 
we show that an intuitive idf -based probability function for 
the probability of a term being informative assumes disjoint 
document events by assuming documents to be 
independent rather than disjoint we arrive at a poisson-based 
probability of being informative the framework is useful for 
understanding and deciding the parameter estimation and 
combination in probabilistic retrieval models 
categories and subject descriptors 
h information search and retrieval retrieval 
models 
general terms 
theory 
 introduction and background 
the inverse document frequency idf is one of the most 
successful parameters for a relevance-based ranking of 
retrieved objects with n being the total number of 
documents and n t being the number of documents in which 
term t occurs the idf is defined as follows 
idf t − log 
n t 
n 
 idf t ∞ 
ranking based on the sum of the idf -values of the query 
terms that occur in the retrieved documents works well this 
has been shown in numerous applications also it is well 
known that the combination of a document-specific term 
weight and idf works better than idf alone this approach 
is known as tf-idf where tf t d tf t d is 
the so-called term frequency of term t in document d the 
idf reflects the discriminating power informativeness of a 
term whereas the tf reflects the occurrence of a term 
the idf alone works better than the tf alone does an 
explanation might be the problem of tf with terms that occur 
in many documents let us refer to those terms as noisy 
terms we use the notion of noisy terms rather than 
frequent terms since frequent terms leaves open whether we 
refer to the document frequency of a term in a collection or 
to the so-called term frequency also referred to as 
withindocument frequency of a term in a document we 
associate noise with the document frequency of a term in a 
collection and we associate occurrence with the 
withindocument frequency of a term the tf of a noisy term might 
be high in a document but noisy terms are not good 
candidates for representing a document therefore the removal 
of noisy terms known as stopword removal is essential 
when applying tf in a tf-idf approach the removal of 
stopwords is conceptually obsolete if stopwords are just words 
with a low idf 
from a probabilistic point of view tf is a value with a 
frequency-based probabilistic interpretation whereas idf has 
an informative rather than a probabilistic interpretation 
the missing probabilistic interpretation of idf is a problem 
in probabilistic retrieval models where we combine uncertain 
knowledge of different dimensions e g informativeness of 
terms structure of documents quality of documents age 
of documents etc such that a good estimate of the 
probability of relevance is achieved an intuitive solution is a 
normalisation of idf such that we obtain values in the 
interval for example consider a normalisation based on 
the maximal idf -value let t be the set of terms occurring 
in a collection 
pfreq t is informative 
idf t 
maxidf 
maxidf max idf t t ∈ t maxidf − log n 
minidf min idf t t ∈ t minidf 
minidf 
maxidf 
≤ pfreq t is informative ≤ 
this frequency-based probability function covers the interval 
 if the minimal idf is equal to zero which is the case 
if we have at least one term that occurs in all documents 
can we interpret pfreq the normalised idf as the probability 
that the term is informative 
when investigating the probabilistic interpretation of the 
 
normalised idf we made several observations related to 
disjointness and independence of document events these 
observations are reported in section we show in section 
that the frequency-based noise probability n t 
n 
used in the 
classic idf -definition can be explained by three assumptions 
binary term occurrence constant document containment and 
disjointness of document containment events in section 
we show that by assuming independence of documents we 
obtain − e− 
≈ − as the upper bound of the noise 
probability of a term the value e− 
is related to the 
logarithm and we investigate in section the link to 
information theory in section we link the results of the previous 
sections to probability theory we show the steps from 
possible worlds to binomial distribution and poisson distribution 
in section we emphasise that the theoretical framework 
of this paper is applicable for both idf and tf finally in 
section we base the definition of the probability of 
being informative on the results of the previous sections and 
compare frequency-based and poisson-based definitions 
 background 
the relationship between frequencies probabilities and 
information theory entropy has been the focus of many 
researchers in this background section we focus on work 
that investigates the application of the poisson distribution 
in ir since a main part of the work presented in this paper 
addresses the underlying assumptions of poisson 
 proposes a -poisson model that takes into account 
the different nature of relevant and non-relevant documents 
rare terms content words and frequent terms noisy terms 
function words stopwords shows experimentally that 
most of the terms words in a collection are distributed 
according to a low dimension n-poisson model uses a 
 -poisson model for including term frequency-based 
probabilities in the probabilistic retrieval model the non-linear 
scaling of the poisson function showed significant 
improvement compared to a linear frequency-based probability the 
poisson model was here applied to the term frequency of a 
term in a document we will generalise the discussion by 
pointing out that document frequency and term frequency 
are dual parameters in the collection space and the 
document space respectively our discussion of the poisson 
distribution focuses on the document frequency in a collection 
rather than on the term frequency in a document 
 and address the deviation of idf and poisson and 
apply poisson mixtures to achieve better poisson-based 
estimates the results proved again experimentally that a 
onedimensional poisson does not work for rare terms therefore 
poisson mixtures and additional parameters are proposed 
 section illustrates and summarises 
comprehensively the relationships between frequencies probabilities 
and poisson different definitions of idf are put into 
context and a notion of noise is defined where noise is viewed 
as the complement of idf we use in our paper a different 
notion of noise we consider a frequency-based noise that 
corresponds to the document frequency and we consider a 
term noise that is based on the independence of document 
events 
 and link frequencies and probability 
estimation to information theory establishes a framework 
in which information retrieval models are formalised based 
on probabilistic inference a key component is the use of a 
space of disjoint events where the framework mainly uses 
terms as disjoint events the probability of being 
informative defined in our paper can be viewed as the probability 
of the disjoint terms in the term space of 
 address entropy and bibliometric distributions 
entropy is maximal if all events are equiprobable and the 
frequency-based lotka law n iλ 
is the number of scientists 
that have written i publications where n and λ are 
distribution parameters zipf and the pareto distribution are 
related the pareto distribution is the continuous case of the 
lotka and lotka and zipf show equivalences the pareto 
distribution is used by for term frequency normalisation 
the pareto distribution compares to the poisson 
distribution in the sense that pareto is fat-tailed i e pareto 
assigns larger probabilities to large numbers of events than 
poisson distributions do this makes pareto interesting 
since poisson is felt to be too radical on frequent events 
we restrict in this paper to the discussion of poisson 
however our results show that indeed a smoother distribution 
than poisson promises to be a good candidate for improving 
the estimation of probabilities in information retrieval 
 establishes a theoretical link between tf-idf and 
information theory and the theoretical research on the meaning 
of tf-idf clarifies the statistical model on which the different 
measures are commonly based this motivation matches 
the motivation of our paper we investigate theoretically 
the assumptions of classical idf and poisson for a better 
understanding of parameter estimation and combination 
 from disjoint to independent 
we define and discuss in this section three probabilities 
the frequency-based noise probability definition the 
total noise probability for disjoint documents definition 
and the noise probability for independent documents 
 definition 
 binary occurrence constant containment 
and disjointness of documents 
we show in this section that the frequency-based noise 
probability n t 
n 
in the idf definition can be explained as 
a total probability with binary term occurrence constant 
document containment and disjointness of document 
containments 
we refer to a probability function as binary if for all events 
the probability is either or the occurrence 
probability p t d is binary if p t d is equal to if t ∈ d and 
p t d is equal to otherwise 
p t d is binary ⇐⇒ p t d ∨ p t d 
we refer to a probability function as constant if for all 
events the probability is equal the document containment 
probability reflect the chance that a document occurs in a 
collection this containment probability is constant if we 
have no information about the document containment or 
we ignore that documents differ in containment 
containment could be derived for example from the size quality 
age links etc of a document for a constant containment 
in a collection with n documents 
n 
is often assumed as 
the containment probability we generalise this definition 
and introduce the constant λ where ≤ λ ≤ n the 
containment of a document d depends on the collection c this 
is reflected by the notation p d c used for the containment 
 
of a document 
p d c is constant ⇐⇒ ∀d p d c 
λ 
n 
for disjoint documents that cover the whole event space 
we set λ and obtain 
èd p d c next we define 
the frequency-based noise probability and the total noise 
probability for disjoint documents we introduce the event 
notation t is noisy and t occurs for making the difference 
between the noise probability p t is noisy c in a collection 
and the occurrence probability p t occurs d in a document 
more explicit thereby keeping in mind that the noise 
probability corresponds to the occurrence probability of a term 
in a collection 
definition the frequency-based term noise 
probability 
pfreq t is noisy c 
n t 
n 
definition the total term noise probability for 
disjoint documents 
pdis t is noisy c 
d 
p t occurs d · p d c 
now we can formulate a theorem that makes assumptions 
explicit that explain the classical idf 
theorem idf assumptions if the occurrence 
probability p t d of term t over documents d is binary and 
the containment probability p d c of documents d is 
constant and document containments are disjoint events then 
the noise probability for disjoint documents is equal to the 
frequency-based noise probability 
pdis t is noisy c pfreq t is noisy c 
proof the assumptions are 
∀d p t occurs d ∨ p t occurs d ∧ 
p d c 
λ 
n 
∧ 
d 
p d c 
we obtain 
pdis t is noisy c 
d t∈d 
 
n 
 
n t 
n 
 pfreq t is noisy c 
the above result is not a surprise but it is a 
mathematical formulation of assumptions that can be used to explain 
the classical idf the assumptions make explicit that the 
different types of term occurrence in documents frequency 
of a term importance of a term position of a term 
document part where the term occurs etc and the different 
types of document containment size quality age etc are 
ignored and document containments are considered as 
disjoint events 
from the assumptions we can conclude that idf 
 frequencybased noise respectively is a relatively simple but strict 
estimate still idf works well this could be explained 
by a leverage effect that justifies the binary occurrence and 
constant containment the term occurrence for small 
documents tends to be larger than for large documents whereas 
the containment for small documents tends to be smaller 
than for large documents from that point of view idf 
means that p t ∧ d c is constant for all d in which t occurs 
and p t ∧ d c is zero otherwise the occurrence and 
containment can be term specific for example set p t∧d c 
 nd c if t occurs in d where nd c is the number of 
documents in collection c we used before just n we choose a 
document-dependent occurrence p t d nt d i e the 
occurrence probability is equal to the inverse of nt d which 
is the total number of terms in document d next we choose 
the containment p d c nt d nt c ·nt c nd c where 
nt d nt c is a document length normalisation number 
of terms in document d divided by the number of terms in 
collection c and nt c nd c is a constant factor of the 
collection number of terms in collection c divided by the 
number of documents in collection c we obtain p t∧d c 
 nd c 
in a tf-idf -retrieval function the tf -component reflects 
the occurrence probability of a term in a document this is 
a further explanation why we can estimate the idf with a 
simple p t d since the combined tf-idf contains the 
occurrence probability the containment probability corresponds 
to a document normalisation document length 
normalisation pivoted document length and is normally attached to 
the tf -component or the tf-idf -product 
the disjointness assumption is typical for frequency-based 
probabilities from a probability theory point of view we 
can consider documents as disjoint events in order to achieve 
a sound theoretical model for explaining the classical idf 
but does disjointness reflect the real world where the 
containment of a document appears to be independent of the 
containment of another document in the next section we 
replace the disjointness assumption by the independence 
assumption 
 the upper bound of the noise probability 
for independent documents 
for independent documents we compute the probability 
of a disjunction as usual namely as the complement of the 
probability of the conjunction of the negated events 
p d ∨ ∨ dn − p ¬d ∧ ∧ ¬dn 
 − 
d 
 − p d 
the noise probability can be considered as the conjunction 
of the term occurrence and the document containment 
p t is noisy c p t occurs ∧ d ∨ ∨ dn c 
for disjoint documents this view of the noise probability 
led to definition for independent documents we use now 
the conjunction of negated events 
definition the term noise probability for 
independent documents 
pin t is noisy c 
d 
 − p t occurs d · p d c 
with binary occurrence and a constant containment p d c 
λ n we obtain the term noise of a term t that occurs in n t 
documents 
pin t is noisy c − − 
λ 
n 
n t 
 
for binary occurrence and disjoint documents the 
containment probability was n now with independent 
documents we can use λ as a collection parameter that controls 
the average containment probability we show through the 
next theorem that the upper bound of the noise probability 
depends on λ 
theorem the upper bound of being noisy if the 
occurrence p t d is binary and the containment p d c 
is constant and document containments are independent 
events then − e−λ 
is the upper bound of the noise 
probability 
∀t pin t is noisy c − e−λ 
proof the upper bound of the independent noise 
probability follows from the limit limn→∞ x 
n 
 n 
 ex 
 see 
any comprehensive math book for example for the 
convergence equation of the euler function with x −λ we 
obtain 
lim 
n→∞ 
 − 
λ 
n 
n 
 e−λ 
for the term noise we have 
pin t is noisy c − − 
λ 
n 
n t 
pin t is noisy c is strictly monotonous the noise of a term 
tn is less than the noise of a term tn where tn occurs in 
n documents and tn occurs in n documents 
therefore a term with n n has the largest noise probability 
for a collection with infinite many documents the upper 
bound of the noise probability for terms tn that occur in all 
documents becomes 
lim 
n→∞ 
pin tn is noisy lim 
n→∞ 
 − − 
λ 
n 
n 
 − e−λ 
by applying an independence rather a disjointness 
assumption we obtain the probability e− 
that a term is not noisy 
even if the term does occur in all documents in the disjoint 
case the noise probability is one for a term that occurs in 
all documents 
if we view p d c λ n as the average containment 
then λ is large for a term that occurs mostly in large 
documents and λ is small for a term that occurs mostly in small 
documents thus the noise of a term t is large if t occurs in 
n t large documents and the noise is smaller if t occurs in 
small documents alternatively we can assume a constant 
containment and a term-dependent occurrence if we 
assume p d c then p t d λ n can be interpreted as 
the average probability that t represents a document the 
common assumption is that the average containment or 
occurrence probability is proportional to n t however here 
is additional potential the statistical laws see on luhn 
and zipf indicate that the average probability could follow 
a normal distribution i e small probabilities for small n t 
and large n t and larger probabilities for medium n t 
for the monotonous case we investigate here the noise of 
a term with n t is equal to − − λ n λ n and 
the noise of a term with n t n is close to − e−λ 
 in the 
next section we relate the value e−λ 
to information theory 
 the probability of a maximal informative 
signal 
the probability e− 
is special in the sense that a signal 
with that probability is a signal with maximal information as 
derived from the entropy definition consider the definition 
of the entropy contribution h t of a signal t 
h t p t · − ln p t 
we form the first derivation for computing the optimum 
∂h t 
∂p t 
 − ln p t 
− 
p t 
· p t 
 − ln p t 
for obtaining optima we use 
 − ln p t 
the entropy contribution h t is maximal for p t e− 
 
this result does not depend on the base of the logarithm as 
we see next 
∂h t 
∂p t 
 − logb p t 
− 
p t · ln b 
· p t 
 − 
 
ln b 
 logb p t − 
 ln p t 
ln b 
we summarise this result in the following theorem 
theorem the probability of a maximal 
informative signal the probability pmax e− 
≈ is the 
probability of a maximal informative signal the entropy of a 
maximal informative signal is hmax e− 
 
proof the probability and entropy follow from the 
derivation above 
the complement of the maximal noise probability is e−λ 
and we are looking now for a generalisation of the entropy 
definition such that e−λ 
is the probability of a maximal 
informative signal we can generalise the entropy definition 
by computing the integral of λ ln p t i e this derivation 
is zero for e−λ 
 we obtain a generalised entropy 
− λ ln p t d p t p t · − λ − ln p t 
the generalised entropy corresponds for λ to the 
classical entropy by moving from disjoint to independent 
documents we have established a link between the complement 
of the noise probability of a term that occurs in all 
documents and information theory next we link independent 
documents to probability theory 
 the link to probability theory 
we review for independent documents three concepts of 
probability theory possible worlds binomial distribution 
and poisson distribution 
 possible worlds 
each conjunction of document events for each document 
we consider two document events the document can be 
true or false is associated with a so-called possible world 
for example consider the eight possible worlds for three 
documents n 
 
world w conjunction 
w d ∧ d ∧ d 
w d ∧ d ∧ ¬d 
w d ∧ ¬d ∧ d 
w d ∧ ¬d ∧ ¬d 
w ¬d ∧ d ∧ d 
w ¬d ∧ d ∧ ¬d 
w ¬d ∧ ¬d ∧ d 
w ¬d ∧ ¬d ∧ ¬d 
with each world w we associate a probability µ w which 
is equal to the product of the single probabilities of the 
document events 
world w probability µ w 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
the sum over the possible worlds in which k documents are 
true and n −k documents are false is equal to the 
probability function of the binomial distribution since the binomial 
coefficient yields the number of possible worlds in which k 
documents are true 
 binomial distribution 
the binomial probability function yields the probability 
that k of n events are true where each event is true with 
the single event probability p 
p k binom n k p 
n 
k 
pk 
 − p n −k 
the single event probability is usually defined as p λ n 
i e p is inversely proportional to n the total number of 
events with this definition of p we obtain for an infinite 
number of documents the following limit for the product of 
the binomial coefficient and pk 
 
lim 
n→∞ 
n 
k 
pk 
 
 lim 
n→∞ 
n · n − · · n −k 
k 
λ 
n 
k 
 
λk 
k 
the limit is close to the actual value for k n for large 
k the actual value is smaller than the limit 
the limit of −p n −k follows from the limit limn→∞ 
x 
n 
 n 
 ex 
 
lim 
n→∞ 
 − p n−k 
 lim 
n→∞ 
 − 
λ 
n 
n −k 
 lim 
n→∞ 
e−λ 
· − 
λ 
n 
−k 
 e−λ 
again the limit is close to the actual value for k n for 
large k the actual value is larger than the limit 
 poisson distribution 
for an infinite number of events the poisson probability 
function is the limit of the binomial probability function 
lim 
n→∞ 
binom n k p 
λk 
k 
· e−λ 
p k poisson k λ 
λk 
k 
· e−λ 
the probability poisson is equal to e− 
 which is the 
probability of a maximal informative signal this shows 
the relationship of the poisson distribution and information 
theory 
after seeing the convergence of the binomial distribution 
we can choose the poisson distribution as an approximation 
of the independent term noise probability first we define 
the poisson noise probability 
definition the poisson term noise probability 
ppoi t is noisy c e−λ 
· 
n t 
k 
λk 
k 
for independent documents the poisson distribution 
approximates the probability of the disjunction for large n t 
since the independent term noise probability is equal to the 
sum over the binomial probabilities where at least one of 
n t document containment events is true 
pin t is noisy c 
n t 
k 
n t 
k 
pk 
 − p n −k 
pin t is noisy c ≈ ppoi t is noisy c 
we have defined a frequency-based and a poisson-based 
probability of being noisy where the latter is the limit of the 
independence-based probability of being noisy before we 
present in the final section the usage of the noise 
probability for defining the probability of being informative we 
emphasise in the next section that the results apply to the 
collection space as well as to the the document space 
 the collection space and the 
document space 
consider the dual definitions of retrieval parameters in 
table we associate a collection space d × t with a 
collection c where d is the set of documents and t is the set 
of terms in the collection let nd d and nt t 
be the number of documents and terms respectively we 
consider a document as a subset of t and a term as a subset 
of d let nt d t d ∈ t be the number of terms that 
occur in the document d and let nd t d t ∈ d be the 
number of documents that contain the term t 
in a dual way we associate a document space l × t with 
a document d where l is the set of locations also referred 
to as positions however we use the letters l and l and not 
p and p for avoiding confusion with probabilities and t is 
the set of terms in the document the document dimension 
in a collection space corresponds to the location position 
dimension in a document space 
the definition makes explicit that the classical notion of 
term frequency of a term in a document also referred to as 
the within-document term frequency actually corresponds 
to the location frequency of a term in a document for the 
 
space collection document 
dimensions documents and terms locations and terms 
document location 
frequency 
nd t c number of documents in which term t 
occurs in collection c 
nl t d number of locations positions at which 
term t occurs in document d 
nd c number of documents in collection c nl d number of locations positions in 
document d 
term frequency nt d c number of terms that document d 
contains in collection c 
nt l d number of terms that location l contains 
in document d 
nt c number of terms in collection c nt d number of terms in document d 
noise occurrence p t c term noise p t d term occurrence 
containment p d c document p l d location 
informativeness − ln p t c − ln p t d 
conciseness − ln p d c − ln p l d 
p informative ln p t c ln p tmin c ln p t d ln p tmin d 
p concise ln p d c ln p dmin c ln p l d ln p lmin d 
table retrieval parameters 
actual term frequency value it is common to use the 
maximal occurrence number of locations let lf be the location 
frequency 
tf t d lf t d 
pfreq t occurs d 
pfreq tmax occurs d 
 
nl t d 
nl tmax d 
a further duality is between informativeness and 
conciseness shortness of documents or locations informativeness 
is based on occurrence noise conciseness is based on 
containment 
we have highlighted in this section the duality between 
the collection space and the document space we 
concentrate in this paper on the probability of a term to be noisy 
and informative those probabilities are defined in the 
collection space however the results regarding the term noise 
and informativeness apply to their dual counterparts term 
occurrence and informativeness in a document also the 
results can be applied to containment of documents and 
locations 
 the probability of being 
informative 
we showed in the previous sections that the disjointness 
assumption leads to frequency-based probabilities and that 
the independence assumption leads to poisson probabilities 
in this section we formulate a frequency-based definition 
and a poisson-based definition of the probability of being 
informative and then we compare the two definitions 
definition the frequency-based probability of 
being informative 
pfreq t is informative c 
− ln n t 
n 
− ln 
n 
 − logn 
n t 
n 
 − logn n t − 
ln n t 
ln n 
we define the poisson-based probability of being 
informative analogously to the frequency-based probability of being 
informative see definition 
definition the poisson-based probability of 
being informative 
ppoi t is informative c 
− ln e−λ 
· 
èn t 
k 
λk 
k 
− ln e−λ · λ 
 
λ − ln 
èn t 
k 
λk 
k 
λ − ln λ 
for the sum expression the following limit holds 
lim 
n t →∞ 
n t 
k 
λk 
k 
 eλ 
− 
for λ we can alter the noise and informativeness 
poisson by starting the sum from since eλ 
 then the 
minimal poisson informativeness is poisson λ e−λ 
 we 
obtain a simplified poisson probability of being informative 
ppoi t is informative c ≈ 
λ − ln 
èn t 
k 
λk 
k 
λ 
 − 
ln 
èn t 
k 
λk 
k 
λ 
the computation of the poisson sum requires an 
optimisation for large n t the implementation for this paper 
exploits the nature of the poisson density the poisson 
density yields only values significantly greater than zero in an 
interval around λ 
consider the illustration of the noise and 
informativeness definitions in figure the probability functions 
displayed are summarised in figure where the simplified 
poisson is used in the noise and informativeness graphs the 
frequency-based noise corresponds to the linear solid curve 
in the noise figure with an independence assumption we 
obtain the curve in the lower triangle of the noise figure by 
changing the parameter p λ n of the independence 
probability we can lift or lower the independence curve the 
noise figure shows the lifting for the value λ ln n ≈ 
 the setting λ ln n is special in the sense that the 
frequency-based and the poisson-based informativeness have 
the same denominator namely ln n and the poisson sum 
converges to λ whether we can draw more conclusions from 
this setting is an open question 
we can conclude that the lifting is desirable if we know 
for a collection that terms that occur in relatively few 
doc 
 
 
 
 
 
 
 
probabilityofbeingnoisy 
n t number of documents with term t 
frequency 
independence n 
independence ln n n 
poisson 
poisson 
poisson 
 
 
 
 
 
 
 
probabilityofbeinginformative 
n t number of documents with term t 
frequency 
independence n 
independence ln n n 
poisson 
poisson 
poisson 
figure noise and informativeness 
probability function noise informativeness 
frequency pfreq def n t n ln n t n ln n 
interval n ≤ pfreq ≤ ≤ pfreq ≤ 
independence pin def − − p n t 
ln − − p n t 
 ln p 
interval p ≤ pin − e−λ 
ln p ≤ pin ≤ 
poisson ppoi def e−λ èn t 
k 
λk 
k 
 λ − ln 
èn t 
k 
λk 
k 
 λ − ln λ 
interval e−λ 
· λ ≤ ppoi − e−λ 
 λ − ln eλ 
− λ − ln λ ≤ ppoi ≤ 
poisson ppoi simplified def e−λ èn t 
k 
λk 
k 
 λ − ln 
èn t 
k 
λk 
k 
 λ 
interval e−λ 
≤ ppoi ppoi ≤ 
figure probability functions 
uments are no guarantee for finding relevant documents 
i e we assume that rare terms are still relatively noisy on 
the opposite we could lower the curve when assuming that 
frequent terms are not too noisy i e they are considered as 
being still significantly discriminative 
the poisson probabilities approximate the independence 
probabilities for large n t the approximation is better for 
larger λ for n t λ the noise is zero whereas for n t λ 
the noise is one this radical behaviour can be smoothened 
by using a multi-dimensional poisson distribution figure 
shows a poisson noise based on a two-dimensional poisson 
poisson k λ λ π · e−λ 
· 
λk 
 
k 
 − π · e−λ 
· 
λk 
 
k 
the two dimensional poisson shows a plateau between λ 
 and λ we used here π the idea 
behind this setting is that terms that occur in less than 
documents are considered to be not noisy i e they are 
informative that terms between and are half noisy 
and that terms with more than are definitely noisy 
for the informativeness we observe that the radical 
behaviour of poisson is preserved the plateau here is 
approximately at and it is important to realise that this 
plateau is not obtained with the multi-dimensional poisson 
noise using π the logarithm of the noise is 
normalised by the logarithm of a very small number namely 
 · e− 
 · e− 
 that is why the informativeness 
will be only close to one for very little noise whereas for a 
bit of noise informativeness will drop to zero this effect 
can be controlled by using small values for π such that the 
noise in the interval λ λ is still very little the setting 
π e− 
leads to noise values of approximately e− 
in the interval λ λ the logarithms lead then to for 
the informativeness 
the indepence-based and frequency-based informativeness 
functions do not differ as much as the noise functions do 
however for the indepence-based probability of being 
informative we can control the average informativeness by the 
definition p λ n whereas the control on the 
frequencybased is limited as we address next 
for the frequency-based idf the gradient is monotonously 
decreasing and we obtain for different collections the same 
distances of idf -values i e the parameter n does not affect 
the distance for an illustration consider the distance 
between the value idf tn of a term tn that occurs in n 
documents and the value idf tn of a term tn that occurs in 
n documents 
idf tn − idf tn ln 
n 
n 
the first three values of the distance function are 
idf t − idf t ln 
idf t − idf t ln 
idf t − idf t ln 
for the poisson-based informativeness the gradient decreases 
first slowly for small n t then rapidly near n t ≈ λ and 
then it grows again slowly for large n t 
in conclusion we have seen that the poisson-based 
definition provides more control and parameter possibilities than 
 
the frequency-based definition does whereas more control 
and parameter promises to be positive for the 
personalisation of retrieval systems it bears at the same time the 
danger of just too many parameters the framework presented 
in this paper raises the awareness about the probabilistic 
and information-theoretic meanings of the parameters the 
parallel definitions of the frequency-based probability and 
the poisson-based probability of being informative made 
the underlying assumptions explicit the frequency-based 
probability can be explained by binary occurrence constant 
containment and disjointness of documents independence 
of documents leads to poisson where we have to be aware 
that poisson approximates the probability of a disjunction 
for a large number of events but not for a small number 
this theoretical result explains why experimental 
investigations on poisson see show that a poisson estimation 
does work better for frequent bad noisy terms than for 
rare good informative terms 
in addition to the collection-wide parameter setting the 
framework presented here allows for document-dependent 
settings as explained for the independence probability this 
is in particular interesting for heterogeneous and structured 
collections since documents are different in nature size 
quality root document sub document and therefore 
binary occurrence and constant containment are less 
appropriate than in relatively homogeneous collections 
 summary 
the definition of the probability of being informative 
transforms the informative interpretation of the idf into a 
probabilistic interpretation and we can use the idf -based 
probability in probabilistic retrieval approaches we showed that 
the classical definition of the noise document frequency in 
the inverse document frequency can be explained by three 
assumptions the term within-document occurrence 
probability is binary the document containment probability is 
constant and the document containment events are disjoint 
by explicitly and mathematically formulating the 
assumptions we showed that the classical definition of idf does not 
take into account parameters such as the different nature 
 size quality structure etc of documents in a collection 
or the different nature of terms coverage importance 
position etc in a document we discussed that the absence 
of those parameters is compensated by a leverage effect of 
the within-document term occurrence probability and the 
document containment probability 
by applying an independence rather a disjointness 
assumption for the document containment we could 
establish a link between the noise probability term occurrence 
in a collection information theory and poisson from the 
frequency-based and the poisson-based probabilities of 
being noisy we derived the frequency-based and poisson-based 
probabilities of being informative the frequency-based 
probability is relatively smooth whereas the poisson probability 
is radical in distinguishing between noisy or not noisy and 
informative or not informative respectively we showed how 
to smoothen the radical behaviour of poisson with a 
multidimensional poisson 
the explicit and mathematical formulation of idf - and 
poisson-assumptions is the main result of this paper also 
the paper emphasises the duality of idf and tf collection 
space and document space respectively thus the result 
applies to term occurrence and document containment in a 
collection and it applies to term occurrence and position 
containment in a document this theoretical framework is 
useful for understanding and deciding the parameter 
estimation and combination in probabilistic retrieval models the 
links between indepence-based noise as document frequency 
probabilistic interpretation of idf information theory and 
poisson described in this paper may lead to variable 
probabilistic idf and tf definitions and combinations as required 
in advanced and personalised information retrieval systems 
acknowledgment i would like to thank mounia lalmas 
gabriella kazai and theodora tsikrika for their comments 
on the as they said heavy pieces my thanks also go to the 
meta-reviewer who advised me to improve the presentation 
to make it less formidable and more accessible for those 
without a theoretic bent this work was funded by a 
research fellowship from queen mary university of london 
 references 
 a aizawa an information-theoretic perspective of 
tf-idf measures information processing and 
management - january 
 g amati and c j rijsbergen term frequency 
normalization via pareto distributions in th 
bcs-irsg european colloquium on ir research 
glasgow scotland 
 r k belew finding out about cambridge university 
press 
 a bookstein and d swanson probabilistic models 
for automatic indexing journal of the american 
society for information science - 
 i n bronstein taschenbuch der mathematik harri 
deutsch thun frankfurt am main 
 k church and w gale poisson mixtures natural 
language engineering - 
 k w church and w a gale inverse document 
frequency a measure of deviations from poisson in 
third workshop on very large corpora acl 
anthology 
 t lafouge and c michel links between information 
construction and information gain entropy and 
bibliometric distribution journal of information 
science - 
 e margulis n-poisson document modelling in 
proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - 
 s e robertson and s walker some simple effective 
approximations to the -poisson model for 
probabilistic weighted retrieval in proceedings of the 
 th annual international acm sigir conference on 
research and development in information retrieval 
pages - london et al springer-verlag 
 s wong and y yao an information-theoric measure 
of term specificity journal of the american society 
for information science - 
 s wong and y yao on modeling information 
retrieval with probabilistic inference acm 
transactions on information systems - 
 
 
