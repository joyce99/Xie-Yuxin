estimating the global pagerank of web communities 
jason v davis 
dept of computer sciences 
university of texas at austin 
austin tx 
jdavis cs utexas edu 
inderjit s dhillon 
dept of computer sciences 
university of texas at austin 
austin tx 
inderjit cs utexas edu 
abstract 
localized search engines are small-scale systems that index 
a particular community on the web they offer several 
benefits over their large-scale counterparts in that they are 
relatively inexpensive to build and can provide more precise 
and complete search capability over their relevant domains 
one disadvantage such systems have over large-scale search 
engines is the lack of global pagerank values such 
information is needed to assess the value of pages in the localized 
search domain within the context of the web as a whole 
in this paper we present well-motivated algorithms to 
estimate the global pagerank values of a local domain the 
algorithms are all highly scalable in that given a local 
domain of size n they use o n resources that include 
computation time bandwidth and storage we test our methods 
across a variety of localized domains including site-specific 
domains and topic-specific domains we demonstrate that 
by crawling as few as n or n additional pages our methods 
can give excellent global pagerank estimates 
categories and subject descriptors 
h information storage and retrieval 
information search and retrieval g numerical analysis 
numerical linear algebra g probability and 
statistics markov processes 
general terms 
pagerank markov chain stochastic complementation 
 introduction 
localized search engines are small-scale search engines 
that index only a single community of the web such 
communities can be site-specific domains such as pages within 
the cs utexas edu domain or topic-related 
communitiesfor example political websites compared to the web graph 
crawled and indexed by large-scale search engines the size 
of such local communities is typically orders of magnitude 
smaller consequently the computational resources needed 
to build such a search engine are also similarly lighter by 
restricting themselves to smaller more manageable sections 
of the web localized search engines can also provide more 
precise and complete search capabilities over their respective 
domains 
one drawback of localized indexes is the lack of global 
information needed to compute link-based rankings the 
pagerank algorithm has proven to be an effective such 
measure in general the pagerank of a given page is 
dependent on pages throughout the entire web graph in the 
context of a localized search engine if the pageranks are 
computed using only the local subgraph then we would 
expect the resulting pageranks to reflect the perceived 
popularity within the local community and not of the web as a 
whole for example consider a localized search engine that 
indexes political pages with conservative views a person 
wishing to research the opinions on global warming within 
the conservative political community may encounter 
numerous such opinions across various websites if only local 
pagerank values are available then the search results will reflect 
only strongly held beliefs within the community however if 
global pageranks are also available then the results can 
additionally reflect outsiders views of the conservative 
community those documents that liberals most often access within 
the conservative community 
thus for many localized search engines incorporating 
global pageranks can improve the quality of search results 
however the number of pages a local search engine indexes 
is typically orders of magnitude smaller than the number of 
pages indexed by their large-scale counterparts localized 
search engines do not have the bandwidth storage capacity 
or computational power to crawl download and compute 
the global pageranks of the entire web in this work we 
present a method of approximating the global pageranks of 
a local domain while only using resources of the same 
order as those needed to compute the pageranks of the local 
subgraph 
our proposed method looks for a supergraph of our local 
subgraph such that the local pageranks within this 
supergraph are close to the true global pageranks we construct 
this supergraph by iteratively crawling global pages on the 
current web frontier-i e global pages with inlinks from 
pages that have already been crawled in order to provide 
 
research track paper 
a good approximation to the global pageranks care must 
be taken when choosing which pages to crawl next in this 
paper we present a well-motivated page selection algorithm 
that also performs well empirically this algorithm is 
derived from a well-defined problem objective and has a 
running time linear in the number of local nodes 
we experiment across several types of local subgraphs 
including four topic related communities and several 
sitespecific domains to evaluate performance we measure the 
difference between the current global pagerank estimate 
and the global pagerank as a function of the number of 
pages crawled we compare our algorithm against several 
heuristics and also against a baseline algorithm that chooses 
pages at random and we show that our method outperforms 
these other methods finally we empirically demonstrate 
that given a local domain of size n we can provide good 
approximations to the global pagerank values by crawling 
at most n or n additional pages 
the paper is organized as follows section gives an 
overview of localized search engines and outlines their 
advantages over global search section provides background 
on the pagerank algorithm section formally defines our 
problem and section presents our page selection criteria 
and derives our algorithms section provides 
experimental results section gives an overview of related work and 
finally conclusions are given in section 
 localized search engines 
localized search engines index a single community of the 
web typically either a site-specific community or a 
topicspecific community localized search engines enjoy three 
major advantages over their large-scale counterparts they 
are relatively inexpensive to build they can offer more 
precise search capability over their local domain and they can 
provide a more complete index 
the resources needed to build a global search engine are 
enormous a study by lyman et al found that 
the  surface web publicly available static sites consists of 
 billion pages and that the average size of these pages is 
approximately kilobytes to download a crawl of this 
size approximately terabytes of space is needed for a 
researcher who wishes to build a search engine with access 
to a couple of workstations or a small server storage of this 
magnitude is simply not available however building a 
localized search engine over a web community of a hundred 
thousand pages would only require a few gigabytes of 
storage the computational burden required to support search 
queries over a database this size is more manageable as well 
we note that for topic-specific search engines the relevant 
community can be efficiently identified and downloaded by 
using a focused crawler 
for site-specific domains the local domain is readily 
available on their own web server this obviates the need for 
crawling or spidering and a complete and up-to-date 
index of the domain can thus be guaranteed this is in 
contrast to their large-scale counterparts which suffer from 
several shortcomings first crawling dynamically generated 
pages-pages in the  hidden web -has been the subject of 
research and is a non-trivial task for an external crawler 
second site-specific domains can enable the robots 
exclusion policy this prohibits external search engines crawlers 
from downloading content from the domain and an external 
search engine must instead rely on outside links and anchor 
text to index these restricted pages 
by restricting itself to only a specific domain of the 
internet a localized search engine can provide more precise 
search results consider the canonical ambiguous search 
query  jaguar which can refer to either the car 
manufacturer or the animal a scientist trying to research the 
habitat and evolutionary history of a jaguar may have better 
success using a finely tuned zoology-specific search engine 
than querying google with multiple keyword searches and 
wading through irrelevant results a method to learn 
better ranking functions for retrieval was recently proposed by 
radlinski and joachims and has been applied to various 
local domains including cornell university s website 
 pagerank overview 
the pagerank algorithm defines the importance of web 
pages by analyzing the underlying hyperlink structure of a 
web graph the algorithm works by building a markov chain 
from the link structure of the web graph and computing its 
stationary distribution one way to compute the 
stationary distribution of a markov chain is to find the limiting 
distribution of a random walk over the chain thus the 
pagerank algorithm uses what is sometimes referred to as 
the  random surfer model in each step of the random walk 
the  surfer either follows an outlink from the current page 
 i e the current node in the chain or jumps to a random 
page on the web 
we now precisely define the pagerank problem let u 
be an m × m adjacency matrix for a given web graph such 
that uji if page i links to page j and uji otherwise 
we define the pagerank matrix pu to be 
pu αud− 
u − α vet 
 
where du is the unique diagonal matrix such that ud− 
u 
is column stochastic α is a given scalar such that ≤ α ≤ 
e is the vector of all ones and v is a non-negative 
l normalized vector sometimes called the  random surfer 
vector note that the matrix d− 
u is well-defined only if each 
column of u has at least one non-zero entry-i e each page 
in the webgraph has at least one outlink in the presence of 
such  dangling nodes that have no outlinks one commonly 
used solution proposed by brin et al is to replace each 
zero column of u by a non-negative l -normalized vector 
the pagerank vector r is the dominant eigenvector of the 
pagerank matrix r pu r we will assume without loss of 
generality that r has an l -norm of one computationally 
r can be computed using the power method this method 
first chooses a random starting vector r 
 and iteratively 
multiplies the current vector by the pagerank matrix pu 
see algorithm in general each iteration of the power 
method can take o m 
 operations when pu is a dense 
matrix however in practice the number of links in a web 
graph will be of the order of the number of pages by 
exploiting the sparsity of the pagerank matrix the work per 
iteration can be reduced to o km where k is the average 
number of links per web page it has also been shown that 
the total number of iterations needed for convergence is 
proportional to α and does not depend on the size of the web 
graph finally the total space needed is also o km 
mainly to store the matrix u 
 
research track paper 
algorithm a linear time per iteration algorithm for 
computing pagerank 
computepr u 
input u adjacency matrix 
output r pagerank vector 
choose randomly an initial non-negative vector r 
such that r 
 
i ← 
repeat 
i ← i 
ν ← αud− 
u r i− 
 α is the random surfing 
probability 
r i 
← ν − α v v is the random surfer vector 
until r i 
− r i− 
 δ δ is the convergence threshold 
r ← r i 
 problem definition 
given a local domain l let g be an n × n adjacency 
matrix for the entire connected component of the web that 
contains l such that gji if page i links to page j 
and gji otherwise without loss of generality we will 
partition g as 
g 
l gout 
lout gwithin 
 
where l is the n × n local subgraph corresponding to links 
inside the local domain lout is the subgraph that 
corresponds to links from the local domain pointing out to the 
global domain gout is the subgraph containing links from 
the global domain into the local domain and gwithin 
contains links within the global domain we assume that when 
building a localized search engine only pages inside the 
local domain are crawled and the links between these pages 
are represented by the subgraph l the links in lout are 
also known as these point from crawled pages in the local 
domain to uncrawled pages in the global domain 
as defined in equation pg is the pagerank matrix 
formed from the global graph g and we define the global 
pagerank vector of this graph to be g let the n-length 
vector p 
be the l -normalized vector corresponding to the 
global pagerank of the pages in the local domain l 
p 
 
el g 
elg 
 
where el i is the restriction matrix that selects 
the components from g corresponding to nodes in l let p 
denote the pagerank vector constructed from the local 
domain subgraph l in practice the observed local pagerank 
p and the global pagerank p 
will be quite different one 
would expect that as the size of local matrix l approaches 
the size of global matrix g the global pagerank and the 
observed local pagerank will become more similar thus one 
approach to estimating the global pagerank is to crawl the 
entire global domain compute its pagerank and extract 
the pageranks of the local domain 
typically however n n i e the number of global 
pages is much larger than the number of local pages 
therefore crawling all global pages will quickly exhaust all local 
resources computational storage and bandwidth available 
to create the local search engine we instead seek a 
supergraph ˆf of our local subgraph l with size o n our goal 
algorithm the findglobalpr algorithm 
findglobalpr l lout t k 
input l zero-one adjacency matrix for the local 
domain lout zero-one outlink matrix from l to global 
subgraph as in t number of iterations k number of 
pages to crawl per iteration 
output ˆp an improved estimate of the global 
pagerank of l 
f ← l 
fout ← lout 
f ← computepr f 
for i to t 
 determine which pages to crawl next 
pages ← selectnodes f fout f k 
crawl pages augment f and modify fout 
 update pageranks for new local domain 
f ← computepr f 
end 
 extract pageranks of original local domain normalize 
ˆp ← elf 
elf 
is to find such a supergraph ˆf with pagerank ˆf so that 
ˆf when restricted to l is close to p 
 formally we seek to 
minimize 
globaldiff ˆf 
el 
ˆf 
el 
ˆf 
− p 
 
 
we choose the l norm for measuring the error as it does 
not place excessive weight on outliers as the l norm does 
for example and also because it is the most commonly used 
distance measure in the literature for comparing pagerank 
vectors as well as for detecting convergence of the 
algorithm 
we propose a greedy framework given in algorithm 
for constructing ˆf initially f is set to the local subgraph 
l and the pagerank f of this graph is computed the 
algorithm then proceeds as follows first the selectnodes 
algorithm which we discuss in the next section is called 
and it returns a set of k nodes to crawl next from the set 
of nodes in the current crawl frontier fout these selected 
nodes are then crawled to expand the local subgraph f and 
the pageranks of this expanded graph are then recomputed 
these steps are repeated for each of t iterations finally 
the pagerank vector ˆp which is restricted to pages within 
the original local domain is returned given our 
computation bandwidth and memory restrictions we will assume 
that the algorithm will crawl at most o n pages since the 
pageranks are computed in each iteration of the algorithm 
which is an o n operation we will also assume that the 
number of iterations t is a constant of course the main 
challenge here is in selecting which set of k nodes to crawl 
next in the next section we formally define the problem 
and give efficient algorithms 
 node selection 
in this section we present node selection algorithms that 
operate within the greedy framework presented in the 
previous section we first give a well-defined criteria for the 
page selection problem and provide experimental evidence 
that this criteria can effectively identify pages that optimize 
our problem objective we then present our main 
al 
research track paper 
gorithmic contribution of the paper a method with linear 
running time that is derived from this page selection 
criteria finally we give an intuitive analysis of our algorithm in 
terms of  leaks and  flows we show that if only the  flow 
is considered then the resulting method is very similar to a 
widely used page selection heuristic 
 formulation 
for a given page j in the global domain we define the 
expanded local graph fj 
fj 
f s 
ut 
j 
 
where uj is the zero-one vector containing the outlinks from 
f into page j and s contains the inlinks from page j into 
the local domain note that we do not allow self-links in 
this framework in practice self-links are often removed as 
they only serve to inflate a given page s pagerank 
observe that the inlinks into f from node j are not known 
until after node j is crawled therefore we estimate this 
inlink vector as the expectation over inlink counts among 
the set of already crawled pages 
s 
f t 
e 
f t e 
 
in practice for any given page this estimate may not reflect 
the true inlinks from that page furthermore this 
expectation is sampled from the set of links within the crawled 
domain whereas a better estimate would also use links from 
the global domain however the latter distribution is not 
known to a localized search engine and we contend that the 
above estimate will on average be a better estimate than 
the uniform distribution for example 
let the pagerank of f be f we express the pagerank 
f 
j of the expanded local graph fj as 
f 
j 
 − xj fj 
xj 
 
where xj is the pagerank of the candidate global node j 
and fj is the l -normalized pagerank vector restricted to 
the pages in f 
since directly optimizing our problem goal requires 
knowing the global pagerank p 
 we instead propose to crawl 
those nodes that will have the greatest influence on the 
pageranks of pages in the original local domain l 
influence j 
k∈l 
 fj k − f k 
 el fj − f 
experimentally the influence score is a very good predictor 
of our problem objective for each candidate global node 
j figure a shows the objective function value global diff fj 
as a function of the influence of page j the local domain 
used here is a crawl of conservative political pages we will 
provide more details about this dataset in section we 
observed similar results in other domains the correlation 
is quite strong implying that the influence criteria can 
effectively identify pages that improve the global pagerank 
estimate as a baseline figure b compares our 
objective with an alternative criteria outlink count the outlink 
count is defined as the number of outlinks from the local 
domain to page j the correlation here is much weaker 
 
 
 
 
 
influence 
objective 
 
 
 
 
 
outlink count 
objective 
 a b 
figure a the correlation between our influence 
page selection criteria and the actual objective 
function value is quite strong b this is in 
contrast to other criteria such as outlink count which 
exhibit a much weaker correlation 
 computation 
as described for each candidate global page j the 
influence score must be computed if fj is computed 
exactly for each global page j then the pagerank 
algorithm would need to be run for each of the o n such global 
pages j we consider resulting in an o n 
 computational 
cost for the node selection method thus computing the 
exact value of fj will lead to a quadratic algorithm and we 
must instead turn to methods of approximating this vector 
the algorithm we present works by performing one power 
method iteration used by the pagerank algorithm 
 algorithm the convergence rate for the pagerank algorithm 
has been shown to equal the random surfer probability α 
 given a starting vector x 
 if k pagerank iterations 
are performed the current pagerank solution x k 
satisfies 
x k 
− x 
 o αk 
x 
− x 
 
where x 
is the desired pagerank vector therefore if only 
one iteration is performed choosing a good starting vector 
is necessary to achieve an accurate approximation 
we partition the pagerank matrix pfj corresponding to 
the × subgraph fj as 
pfj 
˜f ˜s 
˜ut 
j w 
 
where ˜f αf df diag uj − 
 − α 
e 
 
et 
 
˜s αs − α 
e 
 
 
˜uj α df diag uj − 
uj − α 
e 
 
 
w 
 − α 
 
 
and diag uj is the diagonal matrix with the i i th 
entry 
equal to one if the ith 
element of uj equals one and is zero 
otherwise we have assumed here that the random surfer 
vector is the uniform vector and that l has no  dangling 
links these assumptions are not necessary and serve only 
to simplify discussion and analysis 
a simple approach for estimating fj is the following first 
estimate the pagerank f 
j of fj by computing one 
pagerank iteration over the matrix pfj using the starting 
vector ν 
f 
 
 then estimate fj by removing the last 
 
research track paper 
component from our estimate of f 
j i e the component 
corresponding to the added node j and renormalizing 
the problem with this approach is in the starting vector 
recall from that xj is the pagerank of the added node 
j the difference between the actual pagerank f 
j of pfj 
and the starting vector ν is 
ν − f 
j xj f − − xj fj 
≥ xj f − − xj fj 
 xj xj 
 xj 
thus by after one pagerank iteration we expect our 
estimate of f 
j to still have an error of about αxj in 
particular for candidate nodes j with relatively high pagerank 
xj this method will yield more inaccurate results we will 
next present a method that eliminates this bias and runs in 
o n time 
 stochastic complementation 
since f 
j as given in is the pagerank of the matrix 
pfj we have 
fj − xj 
xj 
 
˜f ˜s 
˜ut 
j w 
fj − xj 
xj 
 
˜f fj − xj ˜sxj 
˜ut 
j fj − xj wxj 
 
solving the above system for fj can be shown to yield 
fj ˜f − w − 
˜s˜ut 
j fj 
the matrix s ˜f −w − 
˜s˜ut 
j is known as the stochastic 
complement of the column stochastic matrix pfj with 
respect to the sub matrix ˜f the theory of stochastic 
complementation is well studied and it can be shown the stochastic 
complement of an irreducible matrix such as the pagerank 
matrix is unique furthermore the stochastic complement 
is also irreducible and therefore has a unique stationary 
distribution as well for an extensive study see 
it can be easily shown that the sub-dominant eigenvalue 
of s is at most 
α where is the size of f for sufficiently 
large this value will be very close to α this is important 
as other properties of the pagerank algorithm notably the 
algorithm s sensitivity are dependent on this value 
in this method we estimate the length vector fj by 
computing one pagerank iteration over the × stochastic 
complement s starting at the vector f 
fj ≈ sf 
this is in contrast to the simple method outlined in the 
previous section which first iterates over the × 
matrix pfj to estimate f 
j and then removes the last 
component from the estimate and renormalizes to approximate 
fj the problem with the latter method is in the choice 
of the length starting vector ν consequently the 
pagerank estimate given by the simple method differs from 
the true pagerank by at least αxj where xj is the 
pagerank of page j by using the stochastic complement we 
can establish a tight lower bound of zero for this difference 
to see this consider the case in which a node k is added 
to f to form the augmented local subgraph fk and that 
the pagerank of this new graph is 
 − xk f 
xk 
 
specifically the addition of page k does not change the pageranks 
of the pages in f and thus fk f by construction of 
the stochastic complement fk sfk so the approximation 
given in equation will yield the exact solution 
next we present the computational details needed to 
efficiently compute the quantity fj −f over all known global 
pages j we begin by expanding the difference fj −f where 
the vector fj is estimated as in 
fj − f ≈ sf − f 
 αf df diag uj − 
f − α 
e 
 
et 
f 
 − w − 
 ˜ut 
j f ˜s − f 
note that the matrix df diag uj − 
is diagonal letting 
o k be the outlink count for page k in f we can express 
the kth 
diagonal element as 
 df diag uj − 
 k k 
 
o k 
if uj k 
 
o k 
if uj k 
noting that o k − 
 o k − 
− o k o k − 
and 
rewriting this in matrix form yields 
 df diag uj − 
 d− 
f −d− 
f df diag uj − 
diag uj 
 
we use the same identity to express 
e 
 
 
e 
− 
e 
 
 
recall that by definition we have pf αf d− 
f −α e 
 
substituting and in yields 
fj − f ≈ pf f − f 
−αf d− 
f df diag uj − 
diag uj f 
− − α 
e 
 
 − w − 
 ˜ut 
j f ˜s 
 x y ˜ut 
j f z 
noting that by definition f pf f and defining the vectors 
x y and z to be 
x −αf d− 
f df diag uj − 
diag uj f 
y − − α 
e 
 
 
z − w − 
˜s 
the first term x is a sparse vector and takes non-zero values 
only for local pages k that are siblings of the global page 
j we define i j ∈ f if and only if f j i 
 equivalently page i links to page j and express the value of the 
component x k as 
x k −α 
k k k ∈f uj k 
f k 
o k o k 
 
where o k as before is the number of outlinks from page k 
in the local domain note that the last two terms y and z 
are not dependent on the current global node j given the 
function hj f y ˜ut 
j f z the quantity fj − f 
 
research track paper 
can be expressed as 
fj − f 
k 
x k y k ˜ut 
j f z k 
 
k x k 
y k ˜ut 
j f z k 
 
k x k 
x k y k ˜ut 
j f z k 
 hj f − 
k x k 
y k ˜ut 
j f z k 
 
k x k 
x k y k ˜ut 
j f z k 
if we can compute the function hj in linear time then we 
can compute each value of fj − f using an additional 
amount of time that is proportional to the number of 
nonzero components in x these optimizations are carried out 
in algorithm note that computes the difference 
between all components of f and fj whereas our node 
selection criteria given in is restricted to the components 
corresponding to nodes in the original local domain l 
let us examine algorithm in more detail first the 
algorithm computes the outlink counts for each page in the 
local domain the algorithm then computes the quantity 
˜ut 
j f for each known global page j this inner product can 
be written as 
 − α 
 
 
 α 
k k j ∈fout 
f k 
o k 
 
where the second term sums over the set of local pages that 
link to page j since the total number of edges in fout was 
assumed to have size o recall that is the number of 
pages in f the running time of this step is also o 
the algorithm then computes the vectors y and z as 
given in and respectively the l normdiff 
method is called on the components of these vectors which 
correspond to the pages in l and it estimates the value of 
el y ˜ut 
j f z for each page j the estimation works 
as follows first the values of ˜ut 
j f are discretized uniformly 
into c values a ac the quantity el y aiz is 
then computed for each discretized value of ai and stored in 
a table to evaluate el y az for some a ∈ a ac 
the closest discretized value ai is determined and the 
corresponding entry in the table is used the total running time 
for this method is linear in and the discretization 
parameter c which we take to be a constant we note that if exact 
values are desired we have also developed an algorithm that 
runs in o log time that is not described here 
in the main loop we compute the vector x as defined 
in equation the nested loops iterate over the set of 
pages in f that are siblings of page j typically the size 
of this set is bounded by a constant finally for each page 
j the scores vector is updated over the set of non-zero 
components k of the vector x with k ∈ l this set has 
size equal to the number of local siblings of page j and is 
a subset of the total number of siblings of page j thus 
each iteration of the main loop takes constant time and the 
total running time of the main loop is o since we have 
assumed that the size of f will not grow larger than o n 
the total running time for the algorithm is o n 
algorithm node selection via stochastic 
complementation 
sc-select f fout f k 
input f zero-one adjacency matrix of size 
corresponding to the current local subgraph fout zero-one 
outlink matrix from f to global subgraph f 
pagerank of f k number of pages to return 
output pages set of k pages to crawl next 
 compute outlink sums for local subgraph 
foreach page j ∈ f 
o j ← k j k ∈f f j k 
end 
 compute scalar ˜ut 
j f for each global node j 
foreach page j ∈ fout 
g j ← − α 
 
foreach page k k j ∈ fout 
g j ← g j α f k 
o k 
end 
end 
 compute vectors y and z as in and 
y ← − − α e 
 
z ← − w − 
˜s 
 approximate y g j z for all values g j 
norm diffs ←l normdiffs g ely elz 
foreach page j ∈ fout 
 compute sparse vector x as in 
x ← 
foreach page k k j ∈ fout 
foreach page k k k ∈ f 
x k ← x k − f k 
o k o k 
end 
end 
x ← αx 
scores j ← norm diffs j 
foreach k x k and page k ∈ l 
scores j ← scores j − y k g j z k 
 x k y k g j z k 
end 
end 
return k pages with highest scores 
 pagerank flows 
we now present an intuitive analysis of the stochastic 
complementation method by decomposing the change in 
pagerank in terms of  leaks and  flows this analysis is 
motivated by the decomposition given in pagerank  flow is 
the increase in the local pageranks originating from global 
page j the flows are represented by the non-negative vector 
 ˜ut 
j f z equations and the scalar ˜ut 
j f can be 
thought of as the total amount of pagerank flow that page 
j has available to distribute the vector z dictates how the 
flow is allocated to the local domain the flow that local 
page k receives is proportional to within a constant factor 
due to the random surfer vector the expected number of its 
inlinks 
the pagerank  leaks represent the decrease in pagerank 
resulting from the addition of page j the leakage can 
be quantified in terms of the non-positive vectors x and 
y equations and for vector x we can see from 
equation that the amount of pagerank leaked by a 
local page is proportional to the weighted sum of the 
page 
research track paper 
ranks of its siblings thus pages that have siblings with 
higher pageranks and low outlink counts will experience 
more leakage the leakage caused by y is an artifact of the 
random surfer vector 
we will next show that if only the  flow term ˜ut 
j f z 
is considered then the resulting method is very similar to 
a heuristic proposed by cho et al that has been widely 
used for the crawling through url ordering problem 
this heuristic is computationally cheaper but as we will see 
later not as effective as the stochastic complementation 
method 
our node selection strategy chooses global nodes that 
have the largest influence equation if this influence is 
approximated using only  flows the optimal node j 
is 
j 
 argmaxj el ˜ut 
j fz 
 argmaxj ˜ut 
j f el z 
 argmaxj ˜ut 
j f 
 argmaxj α df diag uj − 
uj − α 
e 
 
 f 
 argmaxjft 
 df diag uj − 
uj 
the resulting page selection score can be expressed as a sum 
of the pageranks of each local page k that links to j where 
each pagerank value is normalized by o k interestingly 
the normalization that arises in our method differs from the 
heuristic given in which normalizes by o k the 
algorithm pf-select which is omitted due to lack of space 
first computes the quantity ft 
 df diag uj − 
uj for each 
global page j and then returns the pages with the k largest 
scores to see that the running time for this algorithm is 
o n note that the computation involved in this method is 
a subset of that needed for the sc-select method 
 algorithm which was shown to have a running time of o n 
 experiments 
in this section we provide experimental evidence to 
verify the effectiveness of our algorithms we first outline our 
experimental methodology and then provide results across 
a variety of local domains 
 methodology 
given the limited resources available at an academic 
institution crawling a section of the web that is of the same 
magnitude as that indexed by google or yahoo is clearly 
infeasible thus for a given local domain we approximate 
the global graph by crawling a local neighborhood around 
the domain that is several orders of magnitude larger than 
the local subgraph even though such a graph is still orders 
of magnitude smaller than the  true global graph we 
contend that even if there exist some highly influential pages 
that are very far away from our local domain it is 
unrealistic for any local node selection algorithm to find them such 
pages also tend to be highly unrelated to pages within the 
local domain 
when explaining our node selection strategies in section 
 we made the simplifying assumption that our local graph 
contained no dangling nodes this assumption was only 
made to ease our analysis our implementation efficiently 
handles dangling links by replacing each zero column of our 
adjacency matrix with the uniform vector we evaluate the 
algorithm using the two node selection strategies given in 
section and also against the following baseline methods 
 random nodes are chosen uniformly at random among 
the known global nodes 
 outlinkcount global nodes with the highest 
number of outlinks from the local domain are chosen 
at each iteration of the findglobalpr algorithm we 
evaluate performance by computing the difference between the 
current pagerank estimate of the local domain elf 
elf 
 and 
the global pagerank of the local domain elg 
elg 
 all 
pagerank calculations were performed using the uniform 
random surfer vector across all experiments we set the 
random surfer parameter α to be and used a convergence 
threshold of − 
 we evaluate the difference between the 
local and global pagerank vectors using three different 
metrics the l and l∞ norms and kendall s tau the l norm 
measures the sum of the absolute value of the differences 
between the two vectors and the l∞ norm measures the 
absolute value of the largest difference kendall s tau metric is 
a popular rank correlation measure used to compare 
pageranks this metric can be computed by counting 
the number of pairs of pairs that agree in ranking and 
subtracting from that the number of pairs of pairs that disagree 
in ranking the final value is then normalized by the total 
number of n 
 
such pairs resulting in a − range where 
a negative score signifies anti-correlation among rankings 
and values near one correspond to strong rank correlation 
 results 
our experiments are based on two large web crawls and 
were downloaded using the web crawler that is part of the 
nutch open source search engine project all crawls 
were restricted to only  http pages and to limit the 
number of dynamically generated pages that we crawl we 
ignored all pages with urls containing any of the characters 
      or   the first crawl which we will refer to 
as the  edu dataset was seeded by homepages of the top 
 graduate computer science departments in the usa as 
rated by the us news and world report and also by 
the home pages of their respective institutions a crawl of 
depth was performed restricted to pages within the   edu 
domain resulting in a graph with approximately million 
pages and million links the second crawl was seeded 
by the set of pages under the  politics hierarchy in the dmoz 
open directory project we crawled all pages up to four 
links away which yielded a graph with million pages and 
 million links 
within the  edu crawl we identified the five site-specific 
domains corresponding to the websites of the top five 
graduate computer science departments as ranked by the us 
news and world report this yielded local domains of 
various sizes from uiuc to berkeley for each 
of these site-specific domains with size n we performed 
iterations of the findglobalpr algorithm to crawl a total 
of n additional nodes figure a gives the l difference 
from the pagerank estimate at each iteration to the global 
pagerank for the berkeley local domain 
the performance of this dataset was representative of the 
typical performance across the five computer science 
sitespecific local domains initially the l difference between 
the global and local pageranks ranged from 
 stanford to mit for the first several iterations the 
 
research track paper 
 
 
 
 
 
 
 
 
 
 
number of iterations 
globalandlocalpagerankdifference l 
stochastic complement 
pagerank flow 
outlink count 
random 
 
 
 
 
 
 
 
 
 
number of iterations 
globalandlocalpagerankdifference l 
stochastic complement 
pagerank flow 
outlink count 
random 
 
 
 
 
 
 
 
 
 
 
 
number of iterations 
globalandlocalpagerankdifference l 
stochastic complement 
pagerank flow 
outlink count 
random 
 a www cs berkeley edu b www enterstageright com c politics 
figure l difference between the estimated and true global pageranks for a berkeley s computer science 
website b the site-specific domain www enterstageright com and c the  politics topic-specific domain the 
stochastic complement method outperforms all other methods across various domains 
three link-based methods all outperform the random 
selection heuristic after these initial iterations the random 
heuristic tended to be more competitive with or even 
outperform as in the berkeley local domain the outlink count 
and pagerank flow heuristics in all tests the stochastic 
complementation method either outperformed or was 
competitive with the other methods table gives the average 
difference between the final estimated global pageranks and 
the true global pageranks for various distance measures 
algorithm l l∞ kendall 
stoch comp 
pr flow 
outlink 
random 
table average final performance of various node 
selection strategies for the five site-specific 
computer science local domains note that kendall s 
tau measures similarity while the other metrics are 
dissimilarity measures stochastic 
complementation clearly outperforms the other methods in all 
metrics 
within the  politics dataset we also performed two 
sitespecific tests for the largest websites in the crawl 
www adamsmith org the website for the london based adam smith 
institute and www enterstageright com an online 
conservative journal as with the  edu local domains we ran our 
algorithm for iterations crawling a total of n nodes 
figure b plots the results for the www enterstageright com 
domain in contrast to the  edu local domains the random 
and outlinkcount methods were not competitive with 
either the sc-select or the pf-select methods among all 
datasets and all node selection methods the stochastic 
complementation method was most impressive in this dataset 
realizing a final estimate that differed only from the 
global pagerank a ten-fold improvement over the initial 
local pagerank difference of for the adam smith local 
domain the initial difference between the local and global 
pageranks was and the final estimates given by the 
sc-select pf-select outlinkcount and random 
methods were and respectively 
within the  politics dataset we constructed four 
topicspecific local domains the first domain consisted of all 
pages in the dmoz politics category and also all pages within 
each of these sites up to two links away this yielded a local 
domain of pages and the results are given in figure 
 c because of the larger size of the topic-specific domains 
we ran our algorithm for only iterations to crawl a total 
of n nodes 
we also created topic-specific domains from three 
political sub-topics liberalism conservatism and socialism the 
pages in these domains were identified by their 
corresponding dmoz categories for each sub-topic we set the local 
domain to be all pages within three links from the 
corresponding dmoz category pages table summarizes the 
performance of these three topic-specific domains and also 
the larger political domain 
to quantify a global page j s effect on the global 
pagerank values of pages in the local domain we define page 
j s impact to be its pagerank value g j normalized by the 
fraction of its outlinks pointing to the local domain 
impact j 
ol j 
o j 
· g j 
where ol j is the number of outlinks from page j to pages 
in the local domain l and o j is the total number of j s 
outlinks in terms of the random surfer model the impact 
of page j is the probability that the random surfer is 
currently at global page j in her random walk and takes 
an outlink to a local page given that she has already decided 
not to jump to a random page 
for the politics local domain we found that many of the 
pages with high impact were in fact political pages that 
should have been included in the dmoz politics topic but 
were not for example the two most influential global pages 
were the political search engine www askhenry com and the 
home page of the online political magazine 
www policyreview com among non-political pages the home page of 
the journal education next was most influential the 
journal is freely available online and contains articles 
regarding various aspect of k- education in america to provide 
some anecdotal evidence for the effectiveness of our page 
selection methods we note that the sc-select method chose 
 pages within the www educationnext org domain the 
pf-select method discovered such pages while the 
outlinkcount and random methods found only pages each 
for the conservative political local domain the socialist 
website www ornery org had a very high impact score this 
 
research track paper 
all politics 
algorithm l l kendall 
stoch comp 
pr flow 
outlink 
random 
conservativism 
algorithm l l kendall 
stoch comp 
pr flow 
outlink 
random 
liberalism 
algorithm l l kendall 
stoch comp 
pr flow 
outlink 
random 
socialism 
algorithm l l∞ kendall 
stoch comp 
pr flow 
outlink 
random 
table final performance among node selection 
strategies for the four political topic-specific crawls 
note that kendall s tau measures similarity while 
the other metrics are dissimilarity measures 
was largely due to a link from the front page of this site 
to an article regarding global warming published by the 
national center for public policy research a conservative 
research group in washington dc not surprisingly the 
global pagerank of this article which happens to be on the 
home page of the nccpr www nationalresearch com 
was approximately whereas the local pagerank of this 
page was only the sc-select method yielded a 
global pagerank estimate of approximately the 
pfselect method estimated a value of and the 
random and outlinkcount methods yielded values of 
and respectively 
 related work 
the node selection framework we have proposed is similar 
to the url ordering for crawling problem proposed by cho 
et al in whereas our framework seeks to minimize the 
difference between the global and local pagerank the 
objective used in is to crawl the most highly globally ranked 
pages first they propose several node selection algorithms 
including the outlink count heuristic as well as a variant of 
our pf-select algorithm which they refer to as the 
 pagerank ordering metric they found this method to be most 
effective in optimizing their objective as did a recent survey 
of these methods by baeza-yates et al boldi et al also 
experiment within a similar crawling framework in but 
quantify their results by comparing kendall s rank 
correlation between the pageranks of the current set of crawled 
pages and those of the entire global graph they found that 
node selection strategies that crawled pages with the 
highest global pagerank first actually performed worse with 
respect to kendall s tau correlation between the local and 
global pageranks than basic depth first or breadth first 
strategies however their experiments differ from our work 
in that our node selection algorithms do not use or have 
access to global pagerank values 
many algorithmic improvements for computing exact 
pagerank values have been proposed if such 
algorithms are used to compute the global pageranks of our 
local domain they would all require o n computation 
storage and bandwidth where n is the size of the global 
domain this is in contrast to our method which 
approximates the global pagerank and scales linearly with the size 
of the local domain 
wang and dewitt propose a system where the set of 
web servers that comprise the global domain communicate 
with each other to compute their respective global 
pageranks for a given web server hosting n pages the 
computational bandwidth and storage requirements are also 
linear in n one drawback of this system is that the 
number of distinct web servers that comprise the global domain 
can be very large for example our  edu dataset contains 
websites from over different universities coordinating 
such a system among a large number of sites can be very 
difficult 
gan chen and suel propose a method for estimating the 
pagerank of a single page which uses only constant 
bandwidth computation and space their approach relies on the 
availability of a remote connectivity server that can supply 
the set of inlinks to a given page an assumption not used in 
our framework they experimentally show that a reasonable 
estimate of the node s pagerank can be obtained by visiting 
at most a few hundred nodes using their algorithm for our 
problem would require that either the entire global domain 
first be downloaded or a connectivity server be used both 
of which would lead to very large web graphs 
 conclusions and future work 
the internet is growing exponentially and in order to 
navigate such a large repository as the web global search 
engines have established themselves as a necessity along with 
the ubiquity of these large-scale search engines comes an 
increase in search users expectations by providing complete 
and isolated coverage of a particular web domain localized 
search engines are an effective outlet to quickly locate 
content that could otherwise be difficult to find in this work 
we contend that the use of global pagerank in a localized 
search engine can improve performance 
to estimate the global pagerank we have proposed an 
iterative node selection framework where we select which 
pages from the global frontier to crawl next our primary 
contribution is our stochastic complementation page 
selection algorithm this method crawls nodes that will most 
significantly impact the local domain and has running time 
linear in the number of nodes in the local domain 
experimentally we validate these methods across a diverse set of 
local domains including seven site-specific domains and four 
topic-specific domains we conclude that by crawling an 
additional n or n pages our methods find an estimate of the 
global pageranks that is up to ten times better than just 
using the local pageranks furthermore we demonstrate 
that our algorithm consistently outperforms other existing 
heuristics 
 
research track paper 
often times topic-specific domains are discovered using 
a focused web crawler which considers a page s content in 
conjunction with link anchor text to decide which pages to 
crawl next although such crawlers have proven to be 
quite effective in discovering topic-related content many 
irrelevant pages are also crawled in the process typically 
these pages are deleted and not indexed by the localized 
search engine these pages can of course provide valuable 
information regarding the global pagerank of the local 
domain one way to integrate these pages into our framework 
is to start the findglobalpr algorithm with the current 
subgraph f equal to the set of pages that were crawled by 
the focused crawler 
the global pagerank estimation framework along with 
the node selection algorithms presented all require o n 
computation per iteration and bandwidth proportional to 
the number of pages crawled tk if the number of 
iterations t is relatively small compared to the number of pages 
crawled per iteration k then the bottleneck of the algorithm 
will be the crawling phase however as the number of 
iterations increases relative to k the bottleneck will reside in 
the node selection computation in this case our algorithms 
would benefit from constant factor optimizations recall 
that the findglobalpr algorithm algorithm requires 
that the pageranks of the current expanded local domain be 
recomputed in each iteration recent work by langville and 
meyer gives an algorithm to quickly recompute 
pageranks of a given webgraph if a small number of nodes are 
added this algorithm was shown to give speedup of five to 
ten times on some datasets we plan to investigate this and 
other such optimizations as future work 
in this paper we have objectively evaluated our methods 
by measuring how close our global pagerank estimates are 
to the actual global pageranks to determine the 
benefit of using global pageranks in a localized search engine 
we suggest a user study in which users are asked to rate 
the quality of search results for various search queries for 
some queries only the local pageranks are used in 
ranking and for the remaining queries local pageranks and the 
approximate global pageranks as computed by our 
algorithms are used the results of such a study can then be 
analyzed to determine the added benefit of using the global 
pageranks computed by our methods over just using the 
local pageranks 
acknowledgements this research was supported by nsf 
grant ccf- nsf career award aci- and 
a grant from sabre inc 
 references 
 r baeza-yates m marin c castillo and 
a rodriguez crawling a country better strategies 
than breadth-first for web page ordering world-wide 
web conference 
 p boldi m santini and s vigna do your worst to 
make the best paradoxical effects in pagerank 
incremental computations workshop on web graphs 
 - 
 s brin and l page the anatomy of a large-scale 
hypertextual web search engine computer networks 
and isdn systems - - 
 s chakrabarti m van den berg and b dom 
focused crawling a new approach to topic-specific 
web resource discovery world-wide web conference 
 
 y chen q gan and t suel local methods for 
estimating pagerank values conference on 
information and knowledge management 
 j cho h garcia-molina and l page efficient 
crawling through url ordering world-wide web 
conference 
 t h haveliwala and s d kamvar the second 
eigenvalue of the google matrix technical report 
stanford university 
 t joachims f radlinski l granka a cheng 
c tillekeratne and a patel learning retrieval 
functions from implicit feedback 
http www cs cornell edu people tj career 
 s d kamvar t h haveliwala c d manning and 
g h golub exploiting the block structure of the 
web for computing pagerank world-wide web 
conference 
 s d kamvar t h haveliwala c d manning and 
g h golub extrapolation methods for accelerating 
pagerank computation world-wide web conference 
 
 a n langville and c d meyer deeper inside 
pagerank internet mathematics 
 a n langville and c d meyer updating the 
stationary vector of an irreducible markov chain with 
an eye on google s pagerank siam journal on 
matrix analysis 
 p lyman h r varian k swearingen p charles 
n good l l jordan and j pal how much 
information school of information management 
and system university of california at berkely 
 f mcsherry a uniform approach to accelerated 
pagerank computation world-wide web conference 
 
 c d meyer stochastic complementation uncoupling 
markov chains and the theory of nearly reducible 
systems siam review - 
 us news and world report http www usnews com 
 dmoz open directory project http www dmoz org 
 nutch open source search engine 
http www nutch org 
 f radlinski and t joachims query chains learning 
to rank from implicit feedback acm sigkdd 
international conference on knowledge discovery and 
data mining 
 s raghavan and h garcia-molina crawling the 
hidden web in proceedings of the twenty-seventh 
international conference on very large databases 
 
 t tin tang d hawking n craswell and 
k griffiths focused crawling for both topical 
relevance and quality of medical information 
conference on information and knowledge 
management 
 y wang and d j dewitt computing pagerank in a 
distributed internet search system proceedings of the 
 th vldb conference 
 
research track paper 
