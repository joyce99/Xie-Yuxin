feature representation for effective action-item detection 
paul n bennett 
computer science department 
carnegie mellon university 
pittsburgh pa 
pbennett  cs cmu edu 
jaime carbonell 
language technologies institute 
carnegie mellon university 
pittsburgh pa 
jgc  cs cmu edu 
abstract 
e-mail users face an ever-growing challenge in managing their 
inboxes due to the growing centrality of email in the workplace for 
task assignment action requests and other roles beyond 
information dissemination whereas information retrieval and machine 
learning techniques are gaining initial acceptance in spam filtering 
and automated folder assignment this paper reports on a new task 
automated action-item detection in order to flag emails that require 
responses and to highlight the specific passage s indicating the 
request s for action unlike standard topic-driven text classification 
action-item detection requires inferring the sender s intent and as 
such responds less well to pure bag-of-words classification 
however using enriched feature sets such as n-grams up to n with 
chi-squared feature selection and contextual cues for action-item 
location improve performance by up to over unigrams using 
in both cases state of the art classifiers such as svms with 
automated model selection via embedded cross-validation 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval i artificial intelligence learning i pattern 
recognition applications 
general terms 
experimentation 
 introduction 
e-mail users are facing an increasingly difficult task of 
managing their inboxes in the face of mounting challenges that result from 
rising e-mail usage this includes prioritizing e-mails over a range 
of sources from business partners to family members filtering and 
reducing junk e-mail and quickly managing requests that demand 
from henry hutchins hhutchins innovative company com 
to sara smith joe johnson william woolings 
subject meeting with prospective customers 
sent fri am 
hi all 
i d like to remind all of you that the group from grty will be visiting us 
next friday at p m the current schedule looks like this 
 a m informal breakfast and discussion in cafeteria 
 a m company overview 
 a m individual meetings continue over lunch 
 p m tour of facilities 
 p m sales pitch 
in order to have this go off smoothly i would like to practice the 
presentation well in advance as a result i will need each of your parts by 
wednesday 
keep up the good work 
-henry 
figure an e-mail with emphasized action-item an explicit 
request that requires the recipient s attention or action 
the receiver s attention or action automated action-item detection 
targets the third of these problems by attempting to detect which 
e-mails require an action or response with information and within 
those e-mails attempting to highlight the sentence or other 
passage length that directly indicates the action request 
such a detection system can be used as one part of an e-mail 
agent which would assist a user in processing important e-mails 
quicker than would have been possible without the agent we view 
action-item detection as one necessary component of a successful 
e-mail agent which would perform spam detection action-item 
detection topic classification and priority ranking among other 
functions the utility of such a detector can manifest as a method of 
prioritizing e-mails according to task-oriented criteria other than 
the standard ones of topic and sender or as a means of ensuring that 
the email user hasn t dropped the proverbial ball by forgetting to 
address an action request 
action-item detection differs from standard text classification in 
two important ways first the user is interested both in 
detecting whether an email contains action items and in locating exactly 
where these action item requests are contained within the email 
body in contrast standard text categorization merely assigns a 
topic label to each text whether that label corresponds to an e-mail 
folder or a controlled indexing vocabulary second 
action-item detection attempts to recover the email sender s intent 
- whether she means to elicit response or action on the part of the 
receiver note that for this task classifiers using only unigrams as 
features do not perform optimally as evidenced in our results 
below instead we find that we need more information-laden features 
such as higher-order n-grams text categorization by topic on the 
other hand works very well using just individual words as features 
 in fact genre-classification which one would think 
may require more than a bag-of-words approach also works quite 
well using just unigram features topic detection and 
tracking tdt also works well with unigram feature sets we 
believe that action-item detection is one of the first clear instances 
of an ir-related task where we must move beyond bag-of-words 
to achieve high performance albeit not too far as bag-of-n-grams 
seem to suffice 
we first review related work for similar text classification 
problems such as e-mail priority ranking and speech act identification 
then we more formally define the action-item detection problem 
discuss the aspects that distinguish it from more common problems 
like topic classification and highlight the challenges in 
constructing systems that can perform well at the sentence and document 
level from there we move to a discussion of feature 
representation and selection techniques appropriate for this problem and how 
standard text classification approaches can be adapted to smoothly 
move from the sentence-level detection problem to the 
documentlevel classification problem we then conduct an empirical analysis 
that helps us determine the effectiveness of our feature extraction 
procedures as well as establish baselines for a number of 
classification algorithms on this task finally we summarize this paper s 
contributions and consider interesting directions for future work 
 related work 
several other researchers have considered very similar text 
classification tasks cohen et al describe an ontology of speech 
acts such as propose a meeting and attempt to predict when an 
e-mail contains one of these speech acts we consider action-items 
to be an important specific type of speech act that falls within their 
more general classification while they provide results for 
several classification methods their methods only make use of human 
judgments at the document-level in contrast we consider whether 
accuracy can be increased by using finer-grained human judgments 
that mark the specific sentences and phrases of interest 
corston-oliver et al consider detecting items in e-mail to 
put on a to-do list this classification task is very similar to 
ours except they do not consider simple factual questions to 
belong to this category we include questions but note that not all 
questions are action-items - some are rhetorical or simply social 
convention how are you from a learning perspective while 
they make use of judgments at the sentence-level they do not 
explicitly compare what if any benefits finer-grained judgments offer 
additionally they do not study alternative choices or approaches to 
the classification task instead they simply apply a standard svm 
at the sentence-level and focus primarily on a linguistic analysis of 
how the sentence can be logically reformulated before adding it to 
the task list in this study we examine several alternative 
classification methods compare document-level and sentence-level 
approaches and analyze the machine learning issues implicit in these 
problems 
interest in a variety of learning tasks related to e-mail has been 
rapidly growing in the recent literature for example in a forum 
dedicated to e-mail learning tasks culotta et al presented 
methods for learning social networks from e-mail in this work we do 
not focus on peer relationships however such methods could 
complement those here since peer relationships often influence word 
choice when requesting an action 
 problem definition approach 
in contrast to previous work we explicitly focus on the benefits 
that finer-grained more costly sentence-level human judgments 
offer over coarse-grained document-level judgments additionally 
we consider multiple standard text classification approaches and 
analyze both the quantitative and qualitative differences that arise 
from taking a document-level vs a sentence-level approach to 
classification finally we focus on the representation necessary to 
achieve the most competitive performance 
 problem definition 
in order to provide the most benefit to the user a system would 
not only detect the document but it would also indicate the specific 
sentences in the e-mail which contain the action-items therefore 
there are three basic problems 
 document detection classify a document as to whether or 
not it contains an action-item 
 document ranking rank the documents such that all 
documents containing action-items occur as high as possible in 
the ranking 
 sentence detection classify each sentence in a document as 
to whether or not it is an action-item 
as in most information retrieval tasks the weight the 
evaluation metric should give to precision and recall depends on the 
nature of the application in situations where a user will eventually 
read all received messages ranking e g via precision at recall of 
 may be most important since this will help encourage shorter 
delays in communications between users in contrast high-precision 
detection at low recall will be of increasing importance when the 
user is under severe time-pressure and therefore will likely not read 
all mail this can be the case for crisis managers during disaster 
management finally sentence detection plays a role in both 
timepressure situations and simply to alleviate the user s required time 
to gist the message 
 approach 
as mentioned above the labeled data can come in one of two 
forms a document-labeling provides a yes no label for each 
document as to whether it contains an action-item a phrase-labeling 
provides only a yes label for the specific items of interest we term 
the human judgments a phrase-labeling since the user s view of the 
action-item may not correspond with actual sentence boundaries or 
predicted sentence boundaries obviously it is straightforward to 
generate a document-labeling consistent with a phrase-labeling by 
labeling a document yes if and only if it contains at least one 
phrase labeled yes 
to train classifiers for this task we can take several viewpoints 
related to both the basic problems we have enumerated and the form 
of the labeled data the document-level view treats each e-mail as 
a learning instance with an associated class-label then the 
document can be converted to a feature-value vector and learning 
progresses as usual applying a document-level classifier to document 
detection and ranking is straightforward in order to apply it to 
sentence detection one must make additional steps for example 
if the classifier predicts a document contains an action-item then 
areas of the document that contain a high-concentration of words 
which the model weights heavily in favor of action-items can be 
indicated the obvious benefit of the document-level approach is 
that training set collection costs are lower since the user only has 
to specify whether or not an e-mail contains an action-item and not 
the specific sentences 
in the sentence-level view each e-mail is automatically segmented 
into sentences and each sentence is treated as a learning instance 
with an associated class-label since the phrase-labeling provided 
by the user may not coincide with the automatic segmentation we 
must determine what label to assign a partially overlapping 
sentence when converting it to a learning instance once trained 
applying the resulting classifiers to sentence detection is now 
straightforward but in order to apply the classifiers to document 
detection and document ranking the individual predictions over each 
sentence must be aggregated in order to make a document-level 
prediction this approach has the potential to benefit from 
morespecific labels that enable the learner to focus attention on the key 
sentences instead of having to learn based on data that the majority 
of the words in the e-mail provide no or little information about 
class membership 
 features 
consider some of the phrases that might constitute part of an 
action item would like to know let me know as soon as 
possible have you each of these phrases consists of common 
words that occur in many e-mails however when they occur in 
the same sentence they are far more indicative of an action-item 
additionally order can be important consider have you versus 
you have because of this we posit that n-grams play a larger 
role in this problem than is typical of problems like topic 
classification therefore we consider all n-grams up to size 
when using n-grams if we find an n-gram of size in a segment 
of text we can represent the text as just one occurrence of the 
ngram or as one occurrence of the n-gram and an occurrence of each 
smaller n-gram contained by it we choose the second of these 
alternatives since this will allow the algorithm itself to smoothly 
back-off in terms of recall methods such as na¨ıve bayes may be 
hurt by such a representation because of double-counting 
since sentence-ending punctuation can provide information we 
retain the terminating punctuation token when it is identifiable 
additionally we add a beginning-of-sentence and end-of-sentence 
token in order to capture patterns that are often indicators at the 
beginning or end of a sentence assuming proper punctuation these 
extra tokens are unnecessary but often e-mail lacks proper 
punctuation in addition for the sentence-level classifiers that use 
ngrams we additionally code for each sentence a binary encoding 
of the position of the sentence relative to the document this 
encoding has eight associated features that represent which octile the 
first eighth second eighth etc contains the sentence 
 implementation details 
in order to compare the document-level to the sentence-level 
approach we compare predictions at the document-level we do not 
address how to use a document-level classifier to make predictions 
at the sentence-level 
in order to automatically segment the text of the e-mail we use 
the rasp statistical parser since the automatically segmented 
sentences may not correspond directly with the phrase-level 
boundaries we treat any sentence that contains at least of a marked 
action-item segment as an action-item when evaluating 
sentencedetection for the sentence-level system we use these class labels 
as ground truth since we are not evaluating multiple segmentation 
approaches this does not bias any of the methods if multiple 
segmentation systems were under evaluation one would need to use a 
metric that matched predicted positive sentences to phrases labeled 
positive the metric would need to punish overly long true 
predictions as well as too short predictions our criteria for converting 
to labeled instances implicitly includes both criteria since the 
segmentation is fixed an overly long prediction would be predicting 
yes for many no instances since presumably the extra length 
corresponds to additional segmented sentences all of which do not 
contain of action-item likewise a too short prediction must 
correspond to a small sentence included in the action-item but not 
constituting all of the action-item therefore in order to consider 
the prediction to be too short there will be an additional 
preceding following sentence that is an action-item where we incorrectly 
predicted no 
once a sentence-level classifier has made a prediction for each 
sentence we must combine these predictions to make both a 
document-level prediction and a document-level score we use the 
simple policy of predicting positive when any of the sentences is 
predicted positive in order to produce a document score for 
ranking the confidence that the document contains an action-item is 
ψ d 
 
n d s∈d π s ψ s if for any s ∈ d π s 
 
n d 
maxs∈d ψ s o w 
where s is a sentence in document d π is the classifier s 
prediction ψ is the score the classifier assigns as its confidence that 
π s and n d is the greater of and the number of unigram 
tokens in the document in other words when any sentence is 
predicted positive the document score is the length normalized sum of 
the sentence scores above threshold when no sentence is predicted 
positive the document score is the maximum sentence score 
normalized by length as in other text problems we are more likely to 
emit false positives for documents with more words or sentences 
thus we include a length normalization factor 
 experimental analysis 
 the data 
our corpus consists of e-mails obtained from volunteers at an 
educational institution and cover subjects such as organizing a 
research workshop arranging for job-candidate interviews 
publishing proceedings and talk announcements the messages were 
anonymized by replacing the names of each individual and 
institution with a pseudonym 
after attempting to identify and eliminate 
duplicate e-mails the corpus contains e-mail messages 
after identity anonymization the corpora has three basic 
versions quoted material refers to the text of a previous e-mail that 
an author often leaves in an e-mail message when responding to the 
e-mail quoted material can act as noise when learning since it may 
include action-items from previous messages that are no longer 
relevant to isolate the effects of quoted material we have three 
versions of the corpora the raw form contains the basic messages 
the auto-stripped version contains the messages after quoted 
material has been automatically removed the hand-stripped version 
contains the messages after quoted material has been removed by 
a human additionally the hand-stripped version has had any xml 
content and e-mail signatures removed - leaving only the essential 
content of the message the studies reported here are performed 
with the hand-stripped version this allows us to balance the 
cognitive load in terms of number of tokens that must be read in the 
user-studies we report - including quoted material would 
complicate the user studies since some users might skip the material while 
others read it additionally ensuring all quoted material is removed 
 
we have an even more highly anonymized version of the 
corpus that can be made available for some outside experimentation 
please contact the authors for more information on obtaining this 
data 
prevents tainting the cross-validation since otherwise a test item 
could occur as quoted material in a training document 
 data labeling 
two human annotators labeled each message as to whether or 
not it contained an action-item in addition they identified each 
segment of the e-mail which contained an action-item a segment 
is a contiguous section of text selected by the human annotators 
and may span several sentences or a complete phrase contained in 
a sentence they were instructed that an action item is an explicit 
request for information that requires the recipient s attention or a 
required action and told to highlight the phrases or sentences that 
make up the request 
annotator 
no yes 
annotator 
no 
yes 
table agreement of human annotators at document level 
annotator one labeled messages as containing action items 
annotator two labeled messages as containing action items 
the agreement of the human annotators is shown in tables and 
 the annotators are said to agree at the document-level when 
both marked the same document as containing no action-items or 
both marked at least one action-item regardless of whether the text 
segments were the same at the document-level the annotators 
agreed of the time the kappa statistic is often used to 
evaluate inter-annotator agreement 
κ 
a − r 
 − r 
a is the empirical estimate of the probability of agreement r 
is the empirical estimate of the probability of random agreement 
given the empirical class priors a value close to − implies the 
annotators agree far less often than would be expected randomly 
while a value close to means they agree more often than randomly 
expected 
at the document-level the kappa statistic for inter-annotator 
agreement is this value is both strong enough to expect the 
problem to be learnable and is comparable with results for similar tasks 
 
in order to determine the sentence-level agreement we use each 
judgment to create a sentence-corpus with labels as described in 
section then consider the agreement over these sentences 
this allows us to compare agreement over no judgments we 
perform this comparison over the hand-stripped corpus since that 
eliminates spurious no judgments that would come from 
including quoted material etc both annotators were free to label the 
subject as an action-item but since neither did we omit the subject 
line of the message as well this only reduces the number of no 
agreements this leaves automatically segmented sentences 
at the sentence-level the annotators agreed of the time and 
the kappa statistic for inter-annotator agreement is 
in order to produce one single set of judgments the human 
annotators went through each annotation where there was 
disagreement and came to a consensus opinion the annotators did not 
collect statistics during this process but anecdotally reported that 
the majority of disagreements were either cases of clear annotator 
oversight or different interpretations of conditional statements for 
example if you would like to keep your job come to tomorrow s 
meeting implies a required action where if you would like to join 
annotator 
no yes 
annotator 
no 
yes 
table agreement of human annotators at sentence level 
the football betting pool come to tomorrow s meeting does not 
the first would be an action-item in most contexts while the 
second would not of course many conditional statements are not so 
clearly interpretable after reconciling the judgments there are 
e-mails with no action-items and e-mails containing 
actionitems of the e-mails containing action-items messages 
have one action-item segment messages have two action-item 
segments messages have three action-item segments two 
messages have four action-item segments and one message has six 
action-item segments computing the sentence-level agreement 
using the reconciled gold standard judgments with each of the 
annotators individual judgments gives a kappa of for annotator 
one and a kappa of for annotator two 
in terms of message characteristics there were on average 
content tokens in the body after stripping for action-item 
messages there were however by examining figure we see 
the length distributions are nearly identical as would be expected 
for e-mail it is a long-tailed distribution with about half the 
messages having more than tokens in the body this paragraph has 
 tokens 
 classifiers 
for this experiment we have selected a variety of standard text 
classification algorithms in selecting algorithms we have chosen 
algorithms that are not only known to work well but which differ 
along such lines as discriminative vs generative and lazy vs 
eager we have done this in order to provide both a competitive and 
thorough sampling of learning methods for the task at hand this 
is important since it is easy to improve a strawman classifier by 
introducing a new representation by thoroughly sampling 
alternative classifier choices we demonstrate that representation 
improvements over bag-of-words are not due to using the information in the 
bag-of-words poorly 
 knn 
we employ a standard variant of the k-nearest neighbor 
algorithm used in text classification knn with s-cut score 
thresholding we use a tfidf-weighting of the terms with a 
distanceweighted vote of the neighbors to compute the score before 
thresholding it in order to choose the value of s for thresholding we 
perform leave-one-out cross-validation over the training set the 
value of k is set to be log n where n is the number of 
training points this rule for choosing k is theoretically motivated 
by results which show such a rule converges to the optimal 
classifier as the number of training points increases in practice 
we have also found it to be a computational convenience that 
frequently leads to comparable results with numerically optimizing k 
via a cross-validation procedure 
 na¨ıve bayes 
we use a standard multinomial na¨ıve bayes classifier in 
using this classifier we smoothed word and class probabilities using a 
bayesian estimate with the word prior and a laplace m-estimate 
respectively 
 
 
 
 
 
 
 
 
 
 
numberofmessages 
number of tokens 
all messages 
action-item messages 
 
 
 
 
 
 
 
 
 
 
 
 
percentageofmessages 
number of tokens 
all messages 
action-item messages 
figure the histogram left and distribution right of message length a bin size of words was used only tokens in the body 
after hand-stripping were counted after stripping the majority of words left are usually actual message content 
classifiers document unigram document ngram sentence unigram sentence ngram 
f 
knn ± ± ± ± 
na¨ıve bayes ± ± ± ± 
svm ± ± ± ± 
voted perceptron ± ± ± ± 
accuracy 
knn ± ± ± ± 
na¨ıve bayes ± ± ± ± 
svm ± ± ± ± 
voted perceptron ± ± ± ± 
table average document-detection performance during cross-validation for each method and the sample standard deviation 
 sn− in italics the best performance for each classifier is shown in bold 
 svm 
we have used a linear svm with a tfidf feature representation 
and l -norm as implemented in the svmlight package v 
all default settings were used 
 voted perceptron 
like the svm the voted perceptron is a kernel-based 
learning method we use the same feature representation and kernel 
as we have for the svm a linear kernel with tfidf-weighting and 
an l -norm the voted perceptron is an online-learning method 
that keeps a history of past perceptrons used as well as a weight 
signifying how often that perceptron was correct with each new 
training example a correct classification increases the weight on 
the current perceptron and an incorrect classification updates the 
perceptron the output of the classifier uses the weights on the 
perceptra to make a final voted classification when used in an 
offline-manner multiple passes can be made through the training 
data both the voted perceptron and the svm give a solution from 
the same hypothesis space - in this case a linear classifier 
furthermore it is well-known that the voted perceptron increases the 
margin of the solution after each pass through the training data 
since cohen et al obtain worse results using an svm than a 
voted perceptron with one training iteration they conclude that the 
best solution for detecting speech acts may not lie in an area with 
a large margin because their tasks are highly similar to ours we 
employ both classifiers to ensure we are not overlooking a 
competitive alternative classifier to the svm for the basic bag-of-words 
representation 
 performance measures 
to compare the performance of the classification methods we 
look at two standard performance measures f and accuracy the 
f measure is the harmonic mean of precision and recall 
where precision correct positives 
predicted positives 
and recall correct positives 
actual positives 
 
 experimental methodology 
we perform standard -fold cross-validation on the set of 
documents for the sentence-level approach all sentences in a 
document are either entirely in the training set or entirely in the test set 
for each fold for significance tests we use a two-tailed t-test 
to compare the values obtained during each cross-validation fold 
with a p-value of 
feature selection was performed using the chi-squared 
statistic different levels of feature selection were considered for each 
classifier each of the following number of features was tried 
 there are 
approximately unigram tokens without feature selection in order to choose 
the number of features to use for each classifier we perform nested 
cross-validation and choose the settings that yield the optimal 
document-level f for that classifier for this study only the body of 
each e-mail message was used feature selection is always applied 
to all candidate features that is for the n-gram representation the 
n-grams and position features are also subject to removal by the 
feature selection method 
 results 
the results for document-level classification are given in table 
 the primary hypothesis we are concerned with is that n-grams 
are critical for this task if this is true we expect to see a significant 
gap in performance between the document-level classifiers that use 
n-grams denoted document ngram and those using only unigram 
features denoted document unigram examining table we 
observe that this is indeed the case for every classifier except na¨ıve 
bayes this difference in performance produced by the n-gram 
representation is statistically significant for each classifier except 
for na¨ıve bayes and the accuracy metric for knn see table 
na¨ıve bayes poor performance with the n-gram representation is 
not surprising since the bag-of-n-grams causes excessive 
doublecounting as mentioned in section however na¨ıve bayes is 
not hurt at the sentence-level because the sparse examples provide 
few chances for agglomerative effects of double counting in either 
case when a language-modeling approach is desired modeling the 
n-grams directly would be preferable to na¨ıve bayes more 
importantly for the n-gram hypothesis the n-grams lead to the best 
document-level classifier performance as well 
as would be expected the difference between the sentence-level 
n-gram representation and unigram representation is small this 
is because the window of text is so small that the unigram 
representation when done at the sentence-level implicitly picks up 
on the power of the n-grams further improvement would 
signify that the order of the words matter even when only 
considering a small sentence-size window therefore the finer-grained 
sentence-level judgments allows a unigram representation to 
succeed but only when performed in a small window - behaving as 
an n-gram representation for all practical purposes 
document winner sentence winner 
knn ngram ngram 
na¨ıve bayes unigram ngram 
svm ngram† 
ngram 
voted perceptron ngram† 
ngram 
table significance results for n-grams versus unigrams for 
document detection using document-level and sentence-level 
classifiers when the f result is statistically significant it is 
shown in bold when the accuracy result is significant it is 
shown with a † 
 
f winner accuracy winner 
knn sentence sentence 
na¨ıve bayes sentence sentence 
svm sentence sentence 
voted perceptron sentence document 
table significance results for sentence-level classifiers vs 
document-level classifiers for the document detection problem 
when the result is statistically significant it is shown in bold 
further highlighting the improvement from finer-grained 
judgments and n-grams figure graphically depicts the edge the svm 
sentence-level classifier has over the standard bag-of-words approach 
with a precision-recall curve in the high precision area of the 
graph the consistent edge of the sentence-level classifier is rather 
impressive - continuing at precision out to recall this 
would mean that a tenth of the user s action-items would be placed 
at the top of their action-item sorted inbox additionally the large 
separation at the top right of the curves corresponds to the area 
where the optimal f occurs for each classifier agreeing with the 
large improvement from to in f score considering 
the relative unexplored nature of classification at the sentence-level 
this gives great hope for further increases in performance 
accuracy f 
unigram ngram unigram ngram 
knn 
na¨ıve bayes 
svm 
voted perceptron 
table performance of the sentence-level classifiers at 
sentence detection 
although cohen et al observed that the voted perceptron 
with a single training iteration outperformed svm in a set of 
similar tasks we see no such behavior here this further strengthens the 
evidence that an alternate classifier with the bag-of-words 
representation could not reach the same level of performance the voted 
perceptron classifier does improve when the number of training 
iterations are increased but it is still lower than the svm classifier 
sentence detection results are presented in table with regard 
to the sentence detection problem we note that the f measure 
gives a better feel for the remaining room for improvement in this 
difficult problem that is unlike document detection where 
actionitem documents are fairly common action-item sentences are very 
rare thus as in other text problems the accuracy numbers are 
deceptively high sheerly because of the default accuracy attainable by 
always predicting no although the results here are significantly 
above-random it is unclear what level of performance is necessary 
for sentence detection to be useful in and of itself and not simply 
as a means to document ranking and classification 
figure users find action-items quicker when assisted by a 
classification system 
finally when considering a new type of classification task one 
of the most basic questions is whether an accurate classifier built 
for the task can have an impact on the end-user in order to 
demonstrate the impact this task can have on e-mail users we conducted 
a user study using an earlier less-accurate version of the sentence 
classifier - where instead of using just a single sentence a 
threesentence windowed-approach was used there were three distinct 
sets of e-mail in which users had to find action-items these sets 
were either presented in a random order unordered ordered by 
the classifier ordered or ordered by the classifier and with the 
 
 
 
 
 
 
 
 
precision 
recall 
action-item detection svm performance post model selection 
document unigram 
sentence ngram 
figure both n-grams and a small prediction window lead to consistent improvements over the standard approach 
center sentence in the highest confidence window highlighted 
 order help in order to perform fair comparisons between 
conditions the overall number of tokens in each message set should be 
approximately equal that is the cognitive reading load should be 
approximately the same before the classifier s reordering 
additionally users typically show practice effects by improving at the 
overall task and thus performing better at later message sets this 
is typically handled by varying the ordering of the sets across users 
so that the means are comparable while omitting further detail 
we note the sets were balanced for the total number of tokens and 
a latin square design was used to balance practice effects 
figure shows that at intervals of and minutes users 
consistently found significantly more action-items when assisted 
by the classifier but were most critically aided in the first five 
minutes although the classifier consistently aids the users we did not 
gain an additional end-user impact by highlighting as mentioned 
above this might be a result of the large room for improvement that 
still exists for sentence detection but anecdotal evidence suggests 
this might also be a result of how the information is presented to the 
user rather than the accuracy of sentence detection for example 
highlighting the wrong sentence near an actual action-item hurts 
the user s trust but if a vague indicator e g an arrow points to the 
approximate area the user is not aware of the near-miss since the 
user studies used a three sentence window we believe this played a 
role as well as sentence detection accuracy 
 discussion 
in contrast to problems where n-grams have yielded little 
difference we believe their power here stems from the fact that many of 
the meaningful n-grams for action-items consist of common words 
e g let me know therefore the document-level unigram 
approach cannot gain much leverage even when modeling their joint 
probability correctly since these words will often co-occur in the 
document but not necessarily in a phrase additionally action-item 
detection is distinct from many text classification tasks in that a 
single sentence can change the class label of the document as a 
result good classifiers cannot rely on aggregating evidence from a 
large number of weak indicators across the entire document 
even though we discarded the header information examining 
the top-ranked features at the document-level reveals that many of 
the features are names or parts of e-mail addresses that occurred in 
the body and are highly associated with e-mails that tend to 
contain many or no action-items a few examples are terms such as 
org bob and gov we note that these features will be 
sensitive to the particular distribution senders receivers and thus the 
document-level approach may produce classifiers that transfer less 
readily to alternate contexts and users at different institutions this 
points out that part of the problem of going beyond bag-of-words 
may be the methodology and investigating such properties as 
learning curves and how well a model transfers may highlight 
differences in models which appear to have similar performance when 
tested on the distributions they were trained on we are currently 
investigating whether the sentence-level classifiers do perform 
better over different test corpora without retraining 
 future work 
while applying text classifiers at the document-level is fairly 
well-understood there exists the potential for significantly 
increasing the performance of the sentence-level classifiers such methods 
include alternate ways of combining the predictions over each 
sentence weightings other than tfidf which may not be appropriate 
since sentences are small better sentence segmentation and other 
types of phrasal analysis additionally named entity tagging time 
expressions etc seem likely candidates for features that can 
further improve this task we are currently pursuing some of these 
avenues to see what additional gains these offer 
finally it would be interesting to investigate the best methods for 
combining the document-level and sentence-level classifiers since 
the simple bag-of-words representation at the document-level leads 
to a learned model that behaves somewhat like a context-specific 
prior dependent on the sender receiver and general topic a first 
choice would be to treat it as such when combining probability 
estimates with the sentence-level classifier such a model might 
serve as a general example for other problems where bag-of-words 
can establish a baseline model but richer approaches are needed to 
achieve performance beyond that baseline 
 summary and conclusions 
the effectiveness of sentence-level detection argues that 
labeling at the sentence-level provides significant value further 
experiments are needed to see how this interacts with the amount of 
training data available sentence detection that is then agglomerated to 
document-level detection works surprisingly better given low recall 
than would be expected with sentence-level items this in turn 
indicates that improved sentence segmentation methods could yield 
further improvements in classification 
in this work we examined how action-items can be effectively 
detected in e-mails our empirical analysis has demonstrated that 
n-grams are of key importance to making the most of 
documentlevel judgments when finer-grained judgments are available then 
a standard bag-of-words approach using a small sentence window 
size and automatic segmentation techniques can produce results 
almost as good as the n-gram based approaches 
acknowledgments 
this material is based upon work supported by the defense 
advanced research projects agency darpa under contract no 
nbchd any opinions findings and conclusions or 
recommendations expressed in this material are those of the author s 
and do not necessarily reflect the views of the defense advanced 
research projects agency darpa or the department of 
interiornational business center doi-nbc 
we would like to extend our sincerest thanks to jill lehman 
whose efforts in data collection were essential in constructing the 
corpus and both jill and aaron steinfeld for their direction of the 
hci experiments we would also like to thank django wexler for 
constructing and supporting the corpus labeling tools and curtis 
huttenhower s support of the text preprocessing package finally 
we gratefully acknowledge scott fahlman for his encouragement 
and useful discussions on this topic 
 references 
 j allan j carbonell g doddington j yamron and 
y yang topic detection and tracking pilot study final 
report in proceedings of the darpa broadcast news 
transcription and understanding workshop washington 
d c 
 c apte f damerau and s m weiss automated learning 
of decision rules for text categorization acm transactions 
on information systems - july 
 j carletta assessing agreement on classification tasks the 
kappa statistic computational linguistics - 
 
 j carroll high precision extraction of grammatical relations 
in proceedings of the th international conference on 
computational linguistics coling pages - 
 w w cohen v r carvalho and t m mitchell learning 
to classify email into speech acts in emnlp- 
 conference on empirical methods in natural language 
processing pages - 
 s corston-oliver e ringger m gamon and r campbell 
task-focused summarization of email in text summarization 
branches out proceedings of the acl- workshop pages 
 - 
 a culotta r bekkerman and a mccallum extracting 
social networks and contact information from email and the 
web in ceas- conference on email and anti-spam 
mountain view ca july 
 l devroye l gy¨orfi and g lugosi a probabilistic theory 
of pattern recognition springer-verlag new york ny 
 
 s t dumais j platt d heckerman and m sahami 
inductive learning algorithms and representations for text 
categorization in cikm proceedings of the th acm 
conference on information and knowledge management 
pages - 
 y freund and r schapire large margin classification using 
the perceptron algorithm machine learning - 
 
 t joachims making large-scale svm learning practical in 
b sch¨olkopf c j burges and a j smola editors 
advances in kernel methods - support vector learning 
pages - mit press 
 l s larkey a patent search and classification system in 
proceedings of the fourth acm conference on digital 
libraries pages - 
 d d lewis an evaluation of phrasal and clustered 
representations on a text categorization task in sigir 
proceedings of the th annual international acm 
conference on research and development in information 
retrieval pages - 
 y liu j carbonell and r jin a pairwise ensemble 
approach for accurate genre classification in proceedings of 
the european conference on machine learning ecml 
 
 y liu r yan r jin and j carbonell a comparison study 
of kernels for multi-label text classification using category 
association in the twenty-first international conference on 
machine learning icml 
 a mccallum and k nigam a comparison of event models 
for naive bayes text classification in working notes of aaai 
 the th national conference on artificial 
intelligence workshop on learning for text categorization 
pages - tr ws- - 
 f sebastiani machine learning in automated text 
categorization acm computing surveys - march 
 
 c j van rijsbergen information retrieval butterworths 
london 
 y yang an evaluation of statistical approaches to text 
categorization information retrieval - 
 y yang j carbonell r brown t pierce b t archibald 
and x liu learning approaches to topic detection and 
tracking ieee expert special issue on applications of 
intelligent information retrieval 
 y yang and x liu a re-examination of text categorization 
methods in sigir proceedings of the nd annual 
international acm conference on research and 
development in information retrieval pages - 
 y yang j zhang j carbonell and c jin 
topic-conditioned novelty detection in proceedings of the 
acm sigkdd international conference on knowledge 
discovery and data mining july 
