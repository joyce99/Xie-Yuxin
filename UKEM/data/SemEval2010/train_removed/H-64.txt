machine learning for information architecture in a large 
governmental website 
miles efron 
school of information library science 
cb manning hall 
university of north carolina 
chapel hill nc - 
efrom ils unc edu 
jonathan elsas 
school of information library science 
cb manning hall 
university of north carolina 
chapel hill nc - 
jelsas email unc edu 
gary marchionini 
school of information library science 
cb manning hall 
university of north carolina 
chapel hill nc - 
march ils unc edu 
junliang zhang 
school of information library science 
cb manning hall 
university of north carolina 
chapel hill nc - 
junliang email unc edu 
abstract 
this paper describes ongoing research into the application 
of machine learning techniques for improving access to 
governmental information in complex digital libraries under 
the auspices of the govstat project our goal is to identify a 
small number of semantically valid concepts that adequately 
spans the intellectual domain of a collection the goal of this 
discovery is twofold first we desire a practical aid for 
information architects second automatically derived 
documentconcept relationships are a necessary precondition for 
realworld deployment of many dynamic interfaces the current 
study compares concept learning strategies based on three 
document representations keywords titles and full-text in 
statistical and user-based studies human-created keywords 
provide significant improvements in concept learning over 
both title-only and full-text representations 
categories and subject descriptors 
h information storage and retrieval digital 
libraries-systems issues user issues h information 
storage and retrieval information search and 
retrievalclustering 
general terms 
design experimentation 
 introduction 
the govstat project is a joint effort of the university 
of north carolina interaction design lab and the 
university of maryland human-computer interaction lab 
 
citing end-user difficulty in finding governmental information 
 especially statistical data online the project seeks to 
create an integrated model of user access to us government 
statistical information that is rooted in realistic data 
models and innovative user interfaces to enable such models 
and interfaces we propose a data-driven approach based 
on data mining and machine learning techniques in 
particular our work analyzes a particular digital library-the 
website of the bureau of labor statistics 
 bls -in efforts 
to discover a small number of linguistically meaningful 
concepts or bins that collectively summarize the semantic 
domain of the site 
the project goal is to classify the site s web content 
according to these inferred concepts as an initial step towards 
data filtering via active user interfaces cf many 
digital libraries already make use of content classification 
both explicitly and implicitly they divide their resources 
manually by topical relation they organize content into 
hierarchically oriented file systems the goal of the present 
 
http www ils unc edu govstat 
 
http www bls gov 
 
research is to develop another means of browsing the content 
of these collections by analyzing the distribution of terms 
across documents our goal is to supplement the agency s 
pre-existing information structures statistical learning 
technologies are appealing in this context insofar as they stand 
to define a data-driven-as opposed to an 
agency-drivennavigational structure for a site 
our approach combines supervised and unsupervised 
learning techniques a pure document clustering approach 
to such a large diverse collection as bls led to poor results 
in early tests but strictly supervised techniques are 
inappropriate too although bls designers have defined 
high-level subject headings for their collections as we 
discuss in section this scheme is less than optimal thus we 
hope to learn an additional set of concepts by letting the 
data speak for themselves 
the remainder of this paper describes the details of our 
concept discovery efforts and subsequent evaluation in 
section we describe the previously existing human-created 
conceptual structure of the bls website this section also 
describes evidence that this structure leaves room for 
improvement next sections - we turn to a description 
of the concepts derived via content clustering under three 
document representations keyword title only and full-text 
section describes a two-part evaluation of the derived 
conceptual structures finally we conclude in section by 
outlining upcoming work on the project 
 structuring access to the bls 
website 
the bureau of labor statistics is a federal government 
agency charged with compiling and publishing statistics 
pertaining to labor and production in the us and abroad given 
this broad mandate the bls publishes a wide array of 
information intended for diverse audiences the agency s 
website acts as a clearinghouse for this process with over 
 text html documents and many more documents if 
spreadsheets and typeset reports are included providing 
access to the collection provides a steep challenge to 
information architects 
 the relation browser 
the starting point of this work is the notion that access 
to information in the bls website could be improved by 
the addition of a dynamic interface such as the relation 
browser described by marchionini and brunk the 
relation browser allows users to traverse complex data sets by 
iteratively slicing the data along several topics in figure 
 we see a prototype instantiation of the relation browser 
applied to the fedstats website 
 
the relation browser supports information seeking by 
allowing users to form queries in a stepwise fashion slicing and 
re-slicing the data as their interests dictate its motivation 
is in keeping with shneiderman s suggestion that queries 
and their results should be tightly coupled thus in 
fig 
http www fedstats gov 
figure relation browser prototype 
ure users might limit their search set to those documents 
about energy within this subset of the collection they 
might further eliminate documents published more than a 
year ago finally they might request to see only documents 
published in pdf format 
as marchionini and brunk discuss capturing the 
publication date and format of documents is trivial but successful 
implementations of the relation browser also rely on topical 
classification this presents two stumbling blocks for system 
designers 
 information architects must define the appropriate set 
of topics for their collection 
 site maintainers must classify each document into its 
appropriate categories 
these tasks parallel common problems in the metadata 
community defining appropriate elements and marking up 
documents to support metadata-aware information access 
given a collection of over documents these 
hurdles are especially daunting and automatic methods of 
approaching them are highly desirable 
 a pre-existing structure 
prior to our involvement with the project designers at 
bls created a shallow classificatory structure for the most 
important documents in their website as seen in figure 
the bls home page organizes top-level documents into 
 categories these include topics such as employment and 
unemployment productivity and inflation and spending 
 
figure the bls home page 
we hoped initially that these pre-defined categories could 
be used to train a -way document classifier thus 
automating the process of populating the relation browser altogether 
however this approach proved unsatisfactory in personal 
meetings bls officials voiced dissatisfaction with the 
existing topics their form it was argued owed as much to 
the institutional structure of bls as it did to the inherent 
topology of the website s information space in other words 
the topics reflected official divisions rather than semantic 
clusters the bls agents suggested that re-designing this 
classification structure would be desirable 
the agents misgivings were borne out in subsequent 
analysis the bls topics comprise a shallow classificatory 
structure each of the top-level categories is linked to a small 
number of related pages thus there are pages associated 
with inflation altogether the link structure of this 
classificatory system contains documents that is excluding 
navigational links there are documents linked from the 
bls home page where each hyperlink connects a document 
to a topic pages can be linked to multiple topics based on 
this hyperlink structure we defined m a symmetric Ã— 
matrix where mij counts the number of topics in which 
documents i and j are both classified on the bls home page to 
analyze the redundancy inherent in the pre-existing 
structure we derived the principal components of m cf 
figure shows the resultant scree plot 
 
because all documents belong to at least one bls topic 
 
a scree plot shows the magnitude of the kth 
eigenvalue 
versus its rank during principal component analysis scree 
plots visualize the amount of variance captured by each 
component 
m m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
m m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
eigenvalue rankmeigenvalue rankm 
eigenvalue rank 
eigenvlue magnitudemeigenvlue magnitudem 
eigenvluemagnitude 
figure scree plot of bls categories 
the rank of m is guaranteed to be less than or equal to 
 hence eigenvalues what is surprising 
about figure however is the precipitous decline in 
magnitude among the first four eigenvalues the four largest 
eigenvlaues account for of the total variance in the 
data this fact suggests a high degree of redundancy among 
the topics topical redundancy is not in itself problematic 
however the documents in this very shallow classificatory 
structure are almost all gateways to more specific 
information thus the listing of the producer price index under 
three categories could be confusing to the site s users in 
light of this potential for confusion and the agency s own 
request for redesign we undertook the task of topic discovery 
described in the following sections 
 a hybrid approach to topic 
discovery 
to aid in the discovery of a new set of high-level topics for 
the bls website we turned to unsupervised machine 
learning methods in efforts to let the data speak for themselves 
we desired a means of concept discovery that would be based 
not on the structure of the agency but on the content of the 
material to begin this process we crawled the bls 
website downloading all documents of mime type text html 
this led to a corpus of documents based on this 
corpus we hoped to derive k â‰ˆ topical categories such 
that each document di is assigned to one or more classes 
 
document clustering cf provided an obvious but 
only partial solution to the problem of automating this type 
of high-level information architecture discovery the 
problems with standard clustering are threefold 
 mutually exclusive clusters are inappropriate for 
identifying the topical content of documents since 
documents may be about many subjects 
 due to the heterogeneity of the data housed in the 
bls collection tables lists surveys etc many 
documents terms provide noisy topical information 
 for application to the relation browser we require a 
small number k â‰ˆ of topics without significant 
data reduction term-based clustering tends to deliver 
clusters at too fine a level of granularity 
in light of these problems we take a hybrid approach to 
topic discovery first we limit the clustering process to 
a sample of the entire collection described in section 
working on a focused subset of the data helps to overcome 
problems two and three listed above to address the 
problem of mutual exclusivity we combine unsupervised with 
supervised learning methods as described in section 
 focusing on content-rich 
documents 
to derive empirically evidenced topics we initially turned 
to cluster analysis let a be the nÃ—p data matrix with n 
observations in p variables thus aij shows the measurement 
for the ith 
observation on the jth 
variable as described 
in the goal of cluster analysis is to assign each of the 
n observations to one of a small number k groups each of 
which is characterized by high intra-cluster correlation and 
low inter-cluster correlation though the algorithms for 
accomplishing such an arrangement are legion our analysis 
focuses on k-means clustering 
 during which each 
observation oi is assigned to the cluster ck whose centroid is closest 
to it in terms of euclidean distance readers interested in 
the details of the algorithm are referred to for a 
thorough treatment of the subject 
clustering by k-means is well-studied in the statistical 
literature and has shown good results for text analysis cf 
 however k-means clustering requires that the 
researcher specify k the number of clusters to define when 
applying k-means to our document collection 
indicators such as the gap statistic and an analysis of 
the mean-squared distance across values of k suggested that 
k â‰ˆ was optimal this paramterization led to 
semantically intelligible clusters however clusters are far too 
many for application to an interface such as the relation 
 
we have focused on k-means as opposed to other clustering 
algorithms for several reasons chief among these is the 
computational efficiency enjoyed by the k-means approach 
because we need only a flat clustering there is little to be 
gained by the more expensive hierarchical algorithms in 
future work we will turn to model-based clustering as a 
more principled method of selecting the number of clusters 
and of representing clusters 
browser moreover the granularity of these clusters was 
unsuitably fine for instance the -cluster solution derived 
a cluster whose most highly associated words in terms of 
log-odds ratio were drug pharmacy and chemist these 
words are certainly related but they are related at a level 
of specificity far below what we sought 
to remedy the high dimensionality of the data we 
resolved to limit the algorithm to a subset of the collection 
in consultation with employees of the bls we continued 
our analysis on documents that form a series titled from 
the editor s desk 
 these are brief articles written by bls 
employees bls agents suggested that we focus on the 
editor s desk because it is intended to span the intellectual 
domain of the agency the column is published daily and 
each entry describes an important current issue in the bls 
domain the editor s desk column has been written daily 
 five times per week since as such we operated on a 
set of n documents 
limiting attention to these documents not only 
reduced the dimensionality of the problem it also allowed 
the clustering process to learn on a relatively clean data set 
while the entire bls collection contains a great deal of 
nonprose text i e tables lists etc the editor s desk 
documents are all written in clear journalistic prose each 
document is highly topical further aiding the discovery of 
termtopic relations finally the editor s desk column provided 
an ideal learning environment because it is well-supplied 
with topical metadata each of the documents 
contains a list of one or more keywords additionally a subset 
of the documents contained a subject heading this 
metadata informed our learning and evaluation as described 
in section 
 combining supervised and 
unsupervised learning fortopic 
discovery 
to derive suitably general topics for the application of a 
dynamic interface to the bls collection we combined 
document clustering with text classification techniques 
specifically using k-means we clustered each of the 
documents into one of k clusters with the number of clusters 
chosen by analyzing the within-cluster mean squared 
distance at different values of k see section 
constructing mutually exclusive clusters violates our assumption that 
documents may belong to multiple classes however these 
clusters mark only the first step in a two-phase process of 
topic identification at the end of the process 
documentcluster affinity is measured by a real-valued number 
once the editor s desk documents were assigned to 
clusters we constructed a k-way classifier that estimates the 
strength of evidence that a new document di is a member 
of class ck we tested three statistical classification 
techniques probabilistic rocchio prind naive bayes and 
support vector machines svms all were implemented using 
mccallum s bow text classification library prind is a 
probabilistic version of the rocchio classification algorithm 
 interested readers are referred to joachims article for 
 
http www bls gov opub ted 
 
further details of the classification method like prind naive 
bayes attempts to classify documents into the most 
probable class it is described in detail in finally support 
vector machines were thoroughly explicated by vapnik 
and applied specifically to text in they define a 
decision boundary by finding the maximally separating 
hyperplane in a high-dimensional vector space in which document 
classes become linearly separable 
having clustered the documents and trained a suitable 
classifier the remaining documents in the collection 
are labeled by means of automatic classification that is for 
each document di we derive a k-dimensional vector 
quantifying the association between di and each class c ck 
deriving topic scores via naive bayes for the entire 
 document collection required less than two hours of cpu 
time the output of this process is a score for every 
document in the collection on each of the automatically 
discovered topics these scores may then be used to populate a 
relation browser interface or they may be added to a 
traditional information retrieval system to use these weights in 
the relation browser we currently assign to each document 
the two topics on which it scored highest in future work we 
will adopt a more rigorous method of deriving 
documenttopic weight thresholds also evaluation of the utility of 
the learned topics for users will be undertaken 
 evaluation of concept 
discovery 
prior to implementing a relation browser interface and 
undertaking the attendant user studies it is of course 
important to evaluate the quality of the inferred concepts and 
the ability of the automatic classifier to assign documents 
to the appropriate subjects to evaluate the success of the 
two-stage approach described in section we undertook 
two experiments during the first experiment we compared 
three methods of document representation for the 
clustering task the goal here was to compare the quality of 
document clusters derived by analysis of full-text documents 
documents represented only by their titles and documents 
represented by human-created keyword metadata during 
the second experiment we analyzed the ability of the 
statistical classifiers to discern the subject matter of documents 
from portions of the database in addition to the editor s 
desk 
 comparing document representations 
documents from the editor s desk column came 
supplied with human-generated keyword metadata 
additionally the titles of the editor s desk documents tend to be 
germane to the topic of their respective articles with such 
an array of distilled evidence of each document s subject 
matter we undertook a comparison of document 
representations for topic discovery by clustering we hypothesized 
that keyword-based clustering would provide a useful model 
but we hoped to see whether comparable performance could 
be attained by methods that did not require extensive 
human indexing such as the title-only or full-text 
representations to test this hypothesis we defined three modes of 
document representation-full-text title-only and keyword 
only-we generated three sets of topics tfull ttitle and 
tkw respectively 
topics based on full-text documents were derived by 
application of k-means clustering to the editor s desk 
documents where each document was represented by a 
 dimensional vector these dimensions captured the 
tf idf weights of each term ti in document dj for all 
terms that occurred at least three times in the data to 
arrive at the appropriate number of clusters for these data we 
inspected the within-cluster mean-squared distance for each 
value of k as k approached the reduction in 
error with the addition of more clusters declined notably 
suggesting that k â‰ˆ would yield good divisions to 
select a single integer value we calculated which value of k led 
to the least variation in cluster size this metric stemmed 
from a desire to suppress the common result where one large 
cluster emerges from the k-means algorithm accompanied 
by several accordingly small clusters without reason to 
believe that any single topic should have dramatically high 
prior odds of document membership this heuristic led to 
kfull 
clusters based on document titles were constructed 
similarly however in this case each document was represented 
in the vector space spanned by the terms that occur 
at least twice in document titles using the same method 
of minimizing the variance in cluster membership ktitle-the 
number of clusters in the title-based representation-was also 
set to 
the dimensionality of the keyword-based clustering was 
very similar to that of the title-based approach there were 
 keywords in the data all of which were retained the 
median number of keywords per document was where a 
keyword is understood to be either a single word or a 
multiword term such as consumer price index it is worth noting 
that the keywords were not drawn from any controlled 
vocabulary they were assigned to documents by publishers at 
the bls using the keywords the documents were clustered 
into classes 
to evaluate the clusters derived by each method of 
document representation we used the subject headings that were 
included with of the editor s desk documents each 
of these documents was assigned one or more subject 
headings which were withheld from all of the cluster 
applications like the keywords subject headings were assigned 
to documents by bls publishers unlike the keywords 
however subject headings were drawn from a controlled 
vocabulary our analysis began with the assumption that 
documents with the same subject headings should cluster 
together to facilitate this analysis we took a conservative 
approach we considered multi-subject classifications to be 
unique thus if document di was assigned to a single 
subject prices while document dj was assigned to two subjects 
international comparisons prices documents di and dj are 
not considered to come from the same class 
table shows all editor s desk subject headings that were 
assigned to at least documents as noted in the table 
 
table top editor s desk subject headings 
subject count 
prices 
unemployment 
occupational safety health 
international comparisons prices 
manufacturing prices 
employment 
productivity 
consumer expenditures 
earnings wages 
employment unemployment 
compensation costs 
earnings wages metro areas 
benefits compensation costs 
earnings wages occupations 
employment occupations 
benefits 
earnings wage regions 
work stoppages 
earnings wages industries 
total 
table contingecy table for three document 
representations 
representation right wrong accuracy 
full-text 
title 
keyword 
there were such subject headings which altogether 
covered of the documents with subjects assigned 
these document-subject pairings formed the basis of our 
analysis limiting analysis to subjects with n kept 
the resultant Ï‡ 
tests suitably robust 
the clustering derived by each document representation 
was tested by its ability to collocate documents with the 
same subjects thus for each of the subject headings 
in table si we calculated the proportion of documents 
assigned to si that each clustering co-classified further 
we assumed that whichever cluster captured the majority of 
documents for a given class constituted the right answer 
for that class for instance there were documents whose 
subject heading was prices taking the bls editors 
classifications as ground truth all of these documents should 
have ended up in the same cluster under the full-text 
representation of these documents were clustered into category 
 while were in category and documents were in 
category taking the majority cluster as the putative right 
home for these documents we consider the accuracy of this 
clustering on this subject to be repeating 
this process for each topic across all three representations 
led to the contingency table shown in table 
the obvious superiority of the keyword-based clustering 
evidenced by table was borne out by a Ï‡ 
test on the 
accuracy proportions comparing the proportion right and 
table keyword-based clusters 
benefits costs international jobs 
plans compensation import employment 
benefits costs prices jobs 
employees benefits petroleum youth 
occupations prices productivity safety 
workers prices productivity safety 
earnings index output health 
operators inflation nonfarm occupational 
spending unemployment 
expenditures unemployment 
consumer mass 
spending jobless 
wrong achieved by keyword and title-based clustering led to 
p due to this result in the remainder of this paper 
we focus our attention on the clusters derived by analysis of 
the editor s desk keywords the ten keyword-based clusters 
are shown in table represented by the three terms most 
highly associated with each cluster in terms of the log-odds 
ratio additionally each cluster has been given a label by 
the researchers 
evaluating the results of clustering is notoriously difficult 
in order to lend our analysis suitable rigor and utility we 
made several simplifying assumptions most problematic is 
the fact that we have assumed that each document belongs 
in only a single category this assumption is certainly false 
however by taking an extremely rigid view of what 
constitutes a subject-that is by taking a fully qualified and 
often multipart subject heading as our unit of analysis-we 
mitigate this problem analogically this is akin to 
considering the location of books on a library shelf although a 
given book may cover many subjects a classification system 
should be able to collocate books that are extremely similar 
say books about occupational safety and health the most 
serious liability with this evaluation then is the fact that 
we have compressed multiple subject headings say prices 
international into single subjects this flattening obscures 
the multivalence of documents we turn to a more realistic 
assessment of document-class relations in section 
 accuracy of the document classifiers 
although the keyword-based clusters appear to classify 
the editor s desk documents very well their discovery only 
solved half of the problem required for the successful 
implementation of a dynamic user interface such as the 
relation browser the matter of roughly fourteen thousand 
unclassified documents remained to be addressed to solve 
this problem we trained the statistical classifiers described 
above in section for each document in the collection 
di these classifiers give pi a k-vector of probabilities or 
distances depending on the classification method used where 
pik quantifies the strength of association between the ith 
document and the kth 
class all classifiers were trained on 
the full text of each document regardless of the 
representation used to discover the initial clusters the different 
training sets were thus constructed simply by changing the 
 
table cross validation results for classifiers 
method av percent accuracy se 
prind 
naive bayes 
svm 
class variable for each instance document to reflect its 
assigned cluster under a given model 
to test the ability of each classifier to locate documents 
correctly we first performed a -fold cross validation on 
the editor s desk documents during cross-validation the 
data are split randomly into n subsets in this case n 
the process proceeds by iteratively holding out each of the 
n subsets as a test collection for a model trained on the 
remaining n âˆ’ subsets cross validation is described in 
 using this methodology we compared the performance 
of the three classification models described above table 
gives the results from cross validation 
although naive bayes is not significantly more accurate 
for these data than the svm classifier we limit the 
remainder of our attention to analysis of its performance our 
selection of naive bayes is due to the fact that it appears to 
work comparably to the svm approach for these data while 
being much simpler both in theory and implementation 
because we have only documents and classes the 
number of training documents per class is relatively small 
in addition to models fitted to the editor s desk data then 
we constructed a fourth model supplementing the training 
sets of each class by querying the google search engine 
and 
applying naive bayes to the augmented test set for each 
class we created a query by submitting the three terms 
with the highest log-odds ratio with that class further 
each query was limited to the domain www bls gov for 
each class we retrieved up to documents from google 
 the actual number varied depending on the size of the 
result set returned by google this led to a training set 
of documents in the augmented model as we call 
it below 
 cross validation suggested that the augmented 
model decreased classification accuracy accuracy 
with standard error as we discuss below however 
augmenting the training set appeared to help generalization 
during our second experiment 
the results of our cross validation experiment are 
encouraging however the success of our classifiers on the editor s 
desk documents that informed the cross validation study 
may not be good predictors of the models performance on 
the remainder to the bls website to test the generality 
of the naive bayes classifier we solicited input from 
human judges who were familiar with the bls website the 
sample was chosen by convenience and consisted of faculty 
and graduate students who work on the govstat project 
however none of the reviewers had prior knowledge of the 
outcome of the classification before their participation for 
the experiment a random sample of documents was 
drawn from the entire bls collection on average each 
re 
http www google com 
 
a more formal treatment of the combination of labeled and 
unlabeled data is available in 
table human-model agreement on sample 
docs 
human judge st 
choice 
model model st 
choice model nd 
choice 
n bayes aug 
n bayes 
human judge nd 
choice 
model model st 
choice model nd 
choice 
n bayes aug 
n bayes 
viewer classified documents placing each document into 
as many of the categories shown in table as he or she saw 
fit 
results from this experiment suggest that room for 
improvement remains with respect to generalizing to the whole 
collection from the class models fitted to the editor s desk 
documents in table we see for each classifier the 
number of documents for which it s first or second most probable 
class was voted best or second best by the human judges 
in the context of this experiment we consider a first- or 
second-place classification by the machine to be accurate 
because the relation browser interface operates on a 
multiway classification where each document is classified into 
multiple categories thus a document with the correct 
class as its second choice would still be easily available to 
a user likewise a correct classification on either the most 
popular or second most popular category among the human 
judges is considered correct in cases where a given document 
was classified into multiple classes there were 
multiclass documents in our sample as seen in figure the 
remaining documents were assigned to or classes 
under this rationale the augmented naive bayes 
classifier correctly grouped documents while the smaller model 
 not augmented by a google search correctly classified 
the resultant Ï‡ 
test gave p suggesting that 
increasing the training set improved the ability of the naive 
bayes model to generalize from the editor s desk documents 
to the collection as a whole however the improvement 
afforded by the augmented model comes at some cost in 
particular the augmented model is significantly inferior to the 
model trained solely on editor s desk documents if we 
concern ourselves only with documents selected by the majority 
of human reviewers-i e only first-choice classes limiting 
the right answers to the left column of table gives p 
in favor of the non-augmented model for the purposes of 
applying the relation browser to complex digital library 
content where documents will be classified along multiple 
categories the augmented model is preferable but this is not 
necessarily the case in general 
it must also be said that accuracy under a fairly 
liberal test condition leaves room for improvement in our 
assignment of topics to categories we may begin to 
understand the shortcomings of the described techniques by 
consulting figure which shows the distribution of 
categories across documents given by humans and by the 
augmented naive bayes model the majority of reviewers put 
 
number of human-assigned classesmnumber of human-assigned classesm 
number of human-assigned classes 
frequencymfrequencym 
frequency 
m m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
m m m m m m m m m m m m m m m m m 
figure number of classes assigned to 
documents by judges 
documents into only three categories jobs benefits and 
occupations on the other hand the naive bayes classifier 
distributed classes more evenly across the topics this behavior 
suggests areas for future improvement most importantly 
we observed a strong correlation among the three most 
frequent classes among the human judges for instance there 
was correlation between benefits and occupations this 
suggests that improving the clustering to produce topics 
that were more nearly orthogonal might improve 
performance 
 conclusions and future work 
many developers and maintainers of digital libraries share 
the basic problem pursued here given increasingly large 
complex bodies of data how may we improve access to 
collections without incurring extraordinary cost and while also 
keeping systems receptive to changes in content over time 
data mining and machine learning methods hold a great deal 
of promise with respect to this problem empirical 
methods of knowledge discovery can aid in the organization and 
retrieval of information as we have argued in this paper 
these methods may also be brought to bear on the design 
and implementation of advanced user interfaces 
this study explored a hybrid technique for aiding 
information architects as they implement dynamic interfaces such 
as the relation browser our approach combines 
unsupervised learning techniques applied to a focused subset of the 
bls website the goal of this initial stage is to discover the 
most basic and far-reaching topics in the collection based 
mjobsjobsmjobsm 
jobs 
benefitsunemploymentpricespricesmpricesm 
prices 
safetyinternationalspendingspendingmspendingm 
spending 
occupationscostscostsmcostsm 
costs 
productivityhuman classificationsmhuman classificationsm 
human classifications 
m m m 
 
 m m 
 
 mjobsjobsmjobsm 
jobs 
benefitsunemploymentpricespricesmpricesm 
prices 
safetyinternationalspendingspendingmspendingm 
spending 
occupationscostscostsmcostsm 
costs 
productivitymachine classificationsmmachine classificationsm 
machine classifications 
m m m 
 
 m m 
 
 
figure distribution of classes across documents 
on a statistical model of these topics the second phase of 
our approach uses supervised learning in particular a naive 
bayes classifier trained on individual words to assign 
topical relations to the remaining documents in the collection 
in the study reported here this approach has 
demonstrated promise in its favor our approach is highly scalable 
it also appears to give fairly good results comparing three 
modes of document representation-full-text title only and 
keyword-we found accuracy as measured by 
collocation of documents with identical subject headings while it 
is not surprising that editor-generated keywords should give 
strong evidence for such learning their superiority over 
fulltext and titles was dramatic suggesting that even a small 
amount of metadata can be very useful for data mining 
however we also found evidence that learning topics from 
a subset of the collection may lead to overfitted models 
after clustering editor s desk documents into 
categories we fitted a -way naive bayes classifier to categorize 
the remaining documents in the collection while we 
saw fairly good results classification accuracy of with 
respect to a small sample of human judges this experiment 
forced us to reconsider the quality of the topics learned by 
clustering the high correlation among human judgments 
in our sample suggests that the topics discovered by 
analysis of the editor s desk were not independent while we do 
not desire mutually exclusive categories in our setting we 
do desire independence among the topics we model 
overall then the techniques described here provide an 
encouraging start to our work on acquiring subject 
metadata for dynamic interfaces automatically it also suggests 
that a more sophisticated modeling approach might yield 
 
better results in the future in upcoming work we will 
experiment with streamlining the two-phase technique described 
here instead of clustering documents to find topics and 
then fitting a model to the learned clusters our goal is to 
expand the unsupervised portion of our analysis beyond a 
narrow subset of the collection such as the editor s desk 
in current work we have defined algorithms to identify 
documents likely to help the topic discovery task supplied with 
a more comprehensive training set we hope to experiment 
with model-based clustering which combines the clustering 
and classification processes into a single modeling procedure 
topic discovery and document classification have long been 
recognized as fundamental problems in information retrieval 
and other forms of text mining what is increasingly clear 
however as digital libraries grow in scope and complexity 
is the applicability of these techniques to problems at the 
front-end of systems such as information architecture and 
interface design finally then in future work we will build 
on the user studies undertaken by marchionini and brunk 
in efforts to evaluate the utility of automatically populated 
dynamic interfaces for the users of digital libraries 
 references 
 a agresti an introduction to categorical data 
analysis wiley new york 
 c ahlberg c williamson and b shneiderman 
dynamic queries for information exploration an 
implementation and evaluation in proceedings of the 
sigchi conference on human factors in computing 
systems pages - 
 r baeza-yates and b ribeiro-neto modern 
information retrieval acm press 
 a blum and t mitchell combining labeled and 
unlabeled data with co-training in proceedings of the 
eleventh annual conference on computational learning 
theory pages - acm press 
 h chen and s dumais hierarchical classification of 
web content in proceedings of the rd annual 
international acm sigir conference on research and 
development in information retrieval pages - 
 
 m efron g marchionini and j zhang implications 
of the recursive representation problem for automatic 
concept identification in on-line governmental 
information in proceedings of the asist special 
interest group on classification research asist 
sig-cr 
 c fraley and a e raftery how many clusters 
which clustering method answers via model-based 
cluster analysis the computer journal 
 - 
 a k jain m n murty and p j flynn data 
clustering a review acm computing surveys 
 - september 
 t joachims a probabilistic analysis of the rocchio 
algorithm with tfidf for text categorization in 
d h fisher editor proceedings of icml- th 
international conference on machine learning pages 
 - nashville us morgan kaufmann 
publishers san francisco us 
 t joachims text categorization with support vector 
machines learning with many relevant features in 
c nÂ´edellec and c rouveirol editors proceedings of 
ecml- th european conference on machine 
learning pages - chemnitz de 
springer verlag heidelberg de 
 i t jolliffe principal component analysis springer 
 nd edition 
 l kaufman and p j rosseeuw finding groups in 
data an introduction to cluster analysis wiley 
 
 g marchionini and b brunk toward a general 
relation browser a gui for information architects 
journal of digital information 
http jodi ecs soton ac uk articles v i marchionini 
 a k mccallum bow a toolkit for statistical 
language modeling text retrieval classification and 
clustering http www cs cmu edu Ëœmccallum bow 
 
 t mitchell machine learning mcgraw hill 
 e rasmussen clustering algorithms in w b frakes 
and r baeza-yates editors information retrieval 
data structures and algorithms pages - 
prentice hall 
 r tibshirani g walther and t hastie estimating 
the number of clusters in a dataset via the gap 
statistic 
http citeseer nj nec com tibshirani estimating html 
 v n vapnik the nature of statistical learning 
theory springer 
 
