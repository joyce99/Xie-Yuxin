learning user interaction models 
for predicting web search result preferences 
eugene agichtein 
microsoft research 
eugeneag microsoft com 
eric brill 
microsoft research 
brill microsoft com 
susan dumais 
microsoft research 
sdumais microsoft com 
robert ragno 
microsoft research 
rragno microsoft com 
abstract 
evaluating user preferences of web search results is crucial for 
search engine development deployment and maintenance we 
present a real-world study of modeling the behavior of web search 
users to predict web search result preferences accurate modeling 
and interpretation of user behavior has important applications to 
ranking click spam detection web search personalization and 
other tasks our key insight to improving robustness of 
interpreting implicit feedback is to model query-dependent 
deviations from the expected noisy user behavior we show that 
our model of clickthrough interpretation improves prediction 
accuracy over state-of-the-art clickthrough methods we 
generalize our approach to model user behavior beyond 
clickthrough which results in higher preference prediction 
accuracy than models based on clickthrough information alone 
we report results of a large-scale experimental evaluation that 
show substantial improvements over published implicit feedback 
interpretation methods 
categories and subject descriptors 
h information search and retrieval search process 
relevance feedback 
general terms 
algorithms measurement performance experimentation 
 introduction 
relevance measurement is crucial to web search and to 
information retrieval in general traditionally search relevance is 
measured by using human assessors to judge the relevance of 
query-document pairs however explicit human ratings are 
expensive and difficult to obtain at the same time millions of 
people interact daily with web search engines providing valuable 
implicit feedback through their interactions with the search 
results if we could turn these interactions into relevance 
judgments we could obtain large amounts of data for evaluating 
maintaining and improving information retrieval systems 
recently automatic or implicit relevance feedback has 
developed into an active area of research in the information 
retrieval community at least in part due to an increase in 
available resources and to the rising popularity of web search 
however most traditional ir work was performed over 
controlled test collections and carefully-selected query sets and 
tasks therefore it is not clear whether these techniques will 
work for general real-world web search a significant distinction 
is that web search is not controlled individual users may behave 
irrationally or maliciously or may not even be real users all of 
this affects the data that can be gathered but the amount of the 
user interaction data is orders of magnitude larger than anything 
available in a non-web-search setting by using the aggregated 
behavior of large numbers of users and not treating each user as 
an individual expert we can correct for the noise inherent in 
individual interactions and generate relevance judgments that 
are more accurate than techniques not specifically designed for 
the web search setting 
furthermore observations and insights obtained in laboratory 
settings do not necessarily translate to real world usage hence 
it is preferable to automatically induce feedback interpretation 
strategies from large amounts of user interactions automatically 
learning to interpret user behavior would allow systems to adapt 
to changing conditions changing user behavior patterns and 
different search settings we present techniques to automatically 
interpret the collective behavior of users interacting with a web 
search engine to predict user preferences for search results our 
contributions include 
 a distributional model of user behavior robust to noise 
within individual user sessions that can recover relevance 
preferences from user interactions section 
 extensions of existing clickthrough strategies to include 
richer browsing and interaction features section 
 a thorough evaluation of our user behavior models as well 
as of previously published state-of-the-art techniques over 
a large set of web search sessions sections and 
we discuss our results and outline future directions and 
various applications of this work in section which concludes 
the paper 
 background and related work 
ranking search results is a fundamental problem in 
information retrieval the most common approaches in the 
context of the web use both the similarity of the query to the 
page content and the overall quality of a page a 
state-ofthe-art search engine may use hundreds of features to describe a 
candidate page employing sophisticated algorithms to rank 
pages based on these features current search engines are 
commonly tuned on human relevance judgments human 
annotators rate a set of pages for a query according to perceived 
relevance creating the gold standard against which different 
ranking algorithms can be evaluated reducing the dependence on 
explicit human judgments by using implicit relevance feedback 
has been an active topic of research 
several research groups have evaluated the relationship 
between implicit measures and user interest in these studies 
both reading time and explicit ratings of interest are collected 
morita and shinoda studied the amount of time that users 
spent reading usenet news articles and found that reading time 
could predict a user s interest levels konstan et al showed 
that reading time was a strong predictor of user interest in their 
grouplens system oard and kim studied whether implicit 
feedback could substitute for explicit ratings in recommender 
systems more recently oard and kim presented a 
framework for characterizing observable user behaviors using two 
dimensions-the underlying purpose of the observed behavior and 
the scope of the item being acted upon 
goecks and shavlik approximated human labels by 
collecting a set of page activity measures while users browsed the 
world wide web the authors hypothesized correlations between 
a high degree of page activity and a user s interest while the 
results were promising the sample size was small and the 
implicit measures were not tested against explicit judgments of 
user interest claypool et al studied how several implicit 
measures related to the interests of the user they developed a 
custom browser called the curious browser to gather data in a 
computer lab about implicit interest indicators and to probe for 
explicit judgments of web pages visited claypool et al found 
that the time spent on a page the amount of scrolling on a page 
and the combination of time and scrolling have a strong positive 
relationship with explicit interest while individual scrolling 
methods and mouse-clicks were not correlated with explicit 
interest fox et al explored the relationship between implicit 
and explicit measures in web search they built an instrumented 
browser to collect data and then developed bayesian models to 
relate implicit measures and explicit relevance judgments for both 
individual queries and search sessions they found that 
clickthrough was the most important individual variable but that 
predictive accuracy could be improved by using additional 
variables notably dwell time on a page 
joachims developed valuable insights into the collection of 
implicit measures introducing a technique based entirely on 
clickthrough data to learn ranking functions more recently 
joachims et al presented an empirical evaluation of 
interpreting clickthrough evidence by performing eye tracking 
studies and correlating predictions of their strategies with explicit 
ratings the authors showed that it is possible to accurately 
interpret clickthrough events in a controlled laboratory setting a 
more comprehensive overview of studies of implicit measures is 
described in kelly and teevan 
unfortunately the extent to which existing research applies to 
real-world web search is unclear in this paper we build on 
previous research to develop robust user behavior interpretation 
models for the real web search setting 
 learning user behavior models 
as we noted earlier real web search user behavior can be 
noisy in the sense that user behaviors are only probabilistically 
related to explicit relevance judgments and preferences hence 
instead of treating each user as a reliable expert we aggregate 
information from many unreliable user search session traces our 
main approach is to model user web search behavior as if it were 
generated by two components a relevance component - 
queryspecific behavior influenced by the apparent result relevance and 
a background component - users clicking indiscriminately 
our general idea is to model the deviations from the expected 
user behavior hence in addition to basic features which we 
will describe in detail in section we compute derived 
features that measure the deviation of the observed feature value 
for a given search result from the expected values for a result 
with no query-dependent information we motivate our 
intuitions with a particularly important behavior feature result 
clickthrough analyzed next and then introduce our general 
model of user behavior that incorporates other user actions 
 section 
 a case study in click distributions 
as we discussed we aggregate statistics across many user 
sessions a click on a result may mean that some user found the 
result summary promising it could also be caused by people 
clicking indiscriminately in general individual user behavior 
clickthrough and otherwise is noisy and cannot be relied upon 
for accurate relevance judgments the data set is described in 
more detail in section for the present it suffices to note that 
we focus on a random sample of queries that were 
randomly sampled from query logs for these queries we 
aggregate click data over more than searches performed 
over a three week period we also have explicit relevance 
judgments for the top results for each query 
figure shows the relative clickthrough frequency as a 
function of result position the aggregated click frequency at 
result position p is calculated by first computing the frequency of 
a click at p for each query i e approximating the probability 
that a randomly chosen click for that query would land on 
position p these frequencies are then averaged across queries 
and normalized so that relative frequency of a click at the top 
position is the resulting distribution agrees with previous 
observations that users click more often on top-ranked results 
this reflects the fact that search engines do a reasonable job of 
ranking results as well as biases to click top results and 
noisewe attempt to separate these components in the analysis that 
follows 
 
 
 
 
 
 
 
 
 
 
 
 
result position 
relativeclickfrequency 
figure relative click frequency for top result 
positions over queries and searches 
first we consider the distribution of clicks for the relevant 
documents for these queries figure reports the aggregated 
click distribution for queries with varying position of top 
relevant document ptr while there are many clicks above 
the first relevant document for each distribution there are 
clearly peaks in click frequency for the first relevant result 
for example for queries with top relevant result in position 
the relative click frequency at that position second bar is higher 
than the click frequency at other positions for these queries 
nevertheless many users still click on the non-relevant results 
in position for such queries this shows a stronger property of 
the bias in the click distribution towards top results - users click 
more often on results that are ranked higher even when they are 
not relevant 
 
 
 
 
 
 
 
 
 
 
 
 
result position 
relativeclickfrequency 
ptr 
ptr 
ptr 
ptr 
ptr 
background 
figure relative click frequency for queries with varying 
ptr position of top relevant document 
- 
- 
- 
 
 
 
 
 
 
 
 
 
 
result position 
correctedrelativeclickfrequency 
ptr 
ptr 
ptr 
ptr 
ptr 
figure relative corrected click frequency for relevant 
documents with varying ptr position of top relevant 
if we subtract the background distribution of figure from the 
mixed distribution of figure we obtain the distribution in 
figure where the remaining click frequency distribution can 
be interpreted as the relevance component of the results note that 
the corrected click distribution correlates closely with actual 
result relevance as explicitly rated by human judges 
 robust user behavior model 
clicks on search results comprise only a small fraction of the 
post-search activities typically performed by users we now 
introduce our techniques for going beyond the clickthrough 
statistics and explicitly modeling post-search user behavior 
although clickthrough distributions are heavily biased towards 
top results we have just shown how the  relevance-driven click 
distribution can be recovered by correcting for the prior 
background distribution we conjecture that other aspects of user 
behavior e g page dwell time are similarly distorted our 
general model includes two feature types for describing user 
behavior direct and deviational where the former is the directly 
measured values and latter is deviation from the expected values 
estimated from the overall query-independent distributions for 
the corresponding directly observed features 
more formally we postulate that the observed value o of a 
feature f for a query q and result r can be expressed as a mixture 
of two components 
 frqrelfcfrqo 
where fc is the prior background distribution for values of f 
aggregated across all queries and rel q r f is the component of 
the behavior influenced by the relevance of the result r as 
illustrated above with the clickthrough feature if we subtract the 
background distribution i e the expected clickthrough for a 
result at a given position from the observed clickthrough 
frequency at a given position we can approximate the relevance 
component of the clickthrough value 
 in order to reduce the 
effect of individual user variations in behavior we average 
observed feature values across all users and search sessions for 
each query-url pair this aggregation gives additional 
robustness of not relying on individual noisy user interactions 
in summary the user behavior for a query-url pair is 
represented by a feature vector that includes both the directly 
observed features and the derived corrected feature values 
we now describe the actual features we use to represent user 
behavior 
 features for representing user behavior 
our goal is to devise a sufficiently rich set of features that 
allow us to characterize when a user will be satisfied with a web 
search result once the user has submitted a query they perform 
many different actions reading snippets clicking results 
navigating refining their query which we capture and 
summarize this information was obtained via opt-in client-side 
instrumentation from users of a major web search engine 
this rich representation of user behavior is similar in many 
respects to the recent work by fox et al an important 
difference is that many of our features are by design query 
specific whereas theirs was by design a general 
queryindependent model of user behavior furthermore we include 
derived distributional features computed as described above 
the features we use to represent user search interactions are 
summarized in table for clarity we organize the features 
into the groups query-text clickthrough and browsing 
query-text features users decide which results to examine in 
more detail by looking at the result title url and summary - in 
some cases looking at the original document is not even 
necessary to model this aspect of user experience we defined 
features to characterize the nature of the query and its relation to 
the snippet text these include features such as overlap between 
the words in title and in query titleoverlap the fraction of 
words shared by the query and the result summary 
 summaryoverlap etc 
browsing features simple aspects of the user web page 
interactions can be captured and quantified these features are 
used to characterize interactions with pages beyond the results 
page for example we compute how long users dwell on a page 
 timeonpage or domain timeondomain and the deviation 
of dwell time from expected page dwell time for a query these 
features allows us to model intra-query diversity of page 
browsing behavior e g navigational queries on average are 
likely to have shorter page dwell time than transactional or 
informational queries we include both the direct features and 
the derived features described above 
clickthrough features clicks are a special case of user 
interaction with the search engine we include all the features 
necessary to learn the clickthrough-based strategies described 
in sections and for example for a query-url pair we 
provide the number of clicks for the result clickfrequency as 
 
of course this is just a rough estimate as the observed 
background distribution also includes the relevance 
component 
well as whether there was a click on result below or above the 
current url isclickbelow isclickabove the derived feature 
values such as clickrelativefrequency and clickdeviation are 
computed as described in equation 
query-text features 
titleoverlap fraction of shared words between query and title 
summaryoverlap fraction of shared words between query and summary 
queryurloverlap fraction of shared words between query and url 
querydomainoverlap fraction of shared words between query and domain 
querylength number of tokens in query 
querynextoverlap average fraction of words shared with next query 
browsing features 
timeonpage page dwell time 
cumulativetimeonpage cumulative time for all subsequent pages after search 
timeondomain cumulative dwell time for this domain 
timeonshorturl cumulative time on url prefix dropping parameters 
isfollowedlink if followed link to result otherwise 
isexacturlmatch if aggressive normalization used otherwise 
isredirected if initial url same as final url otherwise 
ispathfromsearch if only followed links after query otherwise 
clicksfromsearch number of hops to reach page from query 
averagedwelltime average time on page for this query 
dwelltimedeviation deviation from overall average dwell time on page 
cumulativedeviation deviation from average cumulative time on page 
domaindeviation deviation from average time on domain 
shorturldeviation deviation from average time on short url 
clickthrough features 
position position of the url in current ranking 
clickfrequency number of clicks for this query url pair 
clickrelativefrequency relative frequency of a click for this query and url 
clickdeviation deviation from expected click frequency 
isnextclicked if there is a click on next position otherwise 
ispreviousclicked if there is a click on previous position otherwise 
isclickabove if there is a click above otherwise 
isclickbelow if there is click below otherwise 
table features used to represent post-search interactions 
for a given query and search result url 
 learning a predictive behavior model 
having described our features we now turn to the actual 
method of mapping the features to user preferences we attempt 
to learn a general implicit feedback interpretation strategy 
automatically instead of relying on heuristics or insights we 
consider this approach to be preferable to heuristic strategies 
because we can always mine more data instead of relying only 
on our intuition and limited laboratory evidence our general 
approach is to train a classifier to induce weights for the user 
behavior features and consequently derive a predictive model of 
user preferences the training is done by comparing a wide range 
of implicit behavior measures with explicit user judgments for a 
set of queries 
for this we use a large random sample of queries in the search 
query log of a popular web search engine the sets of results 
 identified by urls returned for each of the queries and any 
explicit relevance judgments available for each query result pair 
we can then analyze the user behavior for all the instances where 
these queries were submitted to the search engine 
to learn the mapping from features to relevance preferences 
we use a scalable implementation of neural networks ranknet 
 capable of learning to rank a set of given items more 
specifically for each judged query we check if a result link has 
been judged if so the label is assigned to the query url pair and 
to the corresponding feature vector for that search result these 
vectors of feature values corresponding to urls judged relevant 
or non-relevant by human annotators become our training set 
ranknet has demonstrated excellent performance in learning to 
rank objects in a supervised setting hence we use ranknet for 
our experiments 
 predicting user preferences 
in our experiments we explore several models for predicting 
user preferences these models range from using no implicit 
user feedback to using all available implicit user feedback 
ranking search results to predict user preferences is a 
fundamental problem in information retrieval most traditional 
ir and web search approaches use a combination of page and 
link features to rank search results and a representative 
state-ofthe-art ranking system will be used as our baseline ranker 
 section at the same time user interactions with a search 
engine provide a wealth of information a commonly considered 
type of interaction is user clicks on search results previous work 
 as described above also examined which results were 
skipped e g  skip above and  skip next and other related 
strategies to induce preference judgments from the users 
skipping over results and not clicking on following results we 
have also added refinements of these strategies to take into 
account the variability observed in realistic web scenarios we 
describe these strategies in section 
as clickthroughs are just one aspect of user interaction we 
extend the relevance estimation by introducing a machine 
learning model that incorporates clicks as well as other aspects 
of user behavior such as follow-up queries and page dwell time 
 section we conclude this section by briefly describing our 
baseline - a state-of-the-art ranking algorithm used by an 
operational web search engine 
 baseline model 
a key question is whether browsing behavior can provide 
information absent from existing explicit judgments used to train 
an existing ranker for our baseline system we use a 
state-of-theart page ranking system currently used by a major web search 
engine hence we will call this system current for the 
subsequent discussion while the specific algorithms used by the 
search engine are beyond the scope of this paper the algorithm 
ranks results based on hundreds of features such as query to 
document similarity query to anchor text similarity and 
intrinsic page quality the current web search engine rankings 
provide a strong system for comparison and experiments of the 
next two sections 
 clickthrough model 
if we assume that every user click was motivated by a rational 
process that selected the most promising result summary we can 
then interpret each click as described in joachims et al by 
studying eye tracking and comparing clicks with explicit 
judgments they identified a few basic strategies we discuss the 
two strategies that performed best in their experiments skip 
above and skip next 
strategy sa skip above for a set of results for a query 
and a clicked result at position p all unclicked results 
ranked above p are predicted to be less relevant than the 
result at p 
in addition to information about results above the clicked 
result we also have information about the result immediately 
following the clicked one eye tracking study performed by 
joachims et al showed that users usually consider the result 
immediately following the clicked result in current ranking their 
skip next strategy uses this observation to predict that a result 
following the clicked result at p is less relevant than the clicked 
result with accuracy comparable to the sa strategy above for 
better coverage we combine the sa strategy with this extension to 
derive the skip above skip next strategy 
strategy sa n skip above skip next this strategy 
predicts all un-clicked results immediately following a 
clicked result as less relevant than the clicked result and 
combines these predictions with those of the sa strategy 
above 
we experimented with variations of these strategies and found 
that sa n outperformed both sa and the original skip next 
strategy so we will consider the sa and sa n strategies in the 
rest of the paper these strategies are motivated and empirically 
tested for individual users in a laboratory setting as we will 
show these strategies do not work as well in real web search 
setting due to inherent inconsistency and noisiness of individual 
users behavior 
the general approach for using our clickthrough models 
directly is to filter clicks to those that reflect higher-than-chance 
click frequency we then use the same sa and sa n strategies 
but only for clicks that have higher-than-expected frequency 
according to our model for this we estimate the relevance 
component rel q r f of the observed clickthrough feature f as the 
deviation from the expected background clickthrough 
distribution fc 
strategy cd deviation d for a given query compute the 
observed click frequency distribution o r p for all results r 
in positions p the click deviation for a result r in position p 
dev r p is computed as 
 pcproprdev − 
where c p is the expected clickthrough at position p if 
dev r p d retain the click as input to the sa n strategy 
above and apply sa n strategy over the filtered set of click 
events 
the choice of d selects the tradeoff between recall and 
precision while the above strategy extends sa and sa n it still 
assumes that a filtered clicked result is preferred over all 
unclicked results presented to the user above a clicked position 
however for informational queries multiple results may be 
clicked with varying frequency hence it is preferable to 
individually compare results for a query by considering the 
difference between the estimated relevance components of the 
click distribution of the corresponding query results we now 
define a generalization of the previous clickthrough interpretation 
strategy 
strategy cdiff margin m compute deviation dev r p for 
each result r rn in position p for each pair of results ri and 
rj predict preference of ri over rj iff dev ri pi -dev ri pj m 
as in cd the choice of m selects the tradeoff between recall 
and precision the pairs may be preferred in the original order or 
in reverse of it given the margin two results might be effectively 
indistinguishable but only one can possibly be preferred over the 
other intuitively cdiff generalizes the skip idea above to include 
cases where the user skipped i e clicked less than expected 
on uj and preferred i e clicked more than expected on ui 
furthermore this strategy allows for differentiation within the set 
of clicked results making it more appropriate to noisy user 
behavior 
cdiff and cd are complimentary cdiff is a generalization of 
the clickthrough frequency model of cd but it ignores the 
positional information used in cd hence combining the two 
strategies to improve coverage is a natural approach 
strategy cd cdiff deviation d margin m union 
of cd and cdiff predictions 
other variations of the above strategies were considered but 
these five methods cover the range of observed performance 
 general user behavior model 
the strategies described in the previous section generate 
orderings based solely on observed clickthrough frequencies as 
we discussed clickthrough is just one albeit important aspect 
of user interactions with web search engine results we now 
present our general strategy that relies on the automatically 
derived predictive user behavior models section 
the userbehavior strategy for a given query each 
result is represented with the features in table 
relative user preferences are then estimated using the 
learned user behavior model described in section 
recall that to learn a predictive behavior model we used the 
features from table along with explicit relevance judgments 
as input to ranknet which learns an optimal weighting of 
features to predict preferences 
this strategy models user interaction with the search engine 
allowing it to benefit from the wisdom of crowds interacting 
with the results and the pages beyond as our experiments in the 
subsequent sections demonstrate modeling a richer set of user 
interactions beyond clickthroughs results in more accurate 
predictions of user preferences 
 experimental setup 
we now describe our experimental setup we first describe 
the methodology used including our evaluation metrics section 
 then we describe the datasets section and the 
methods we compared in this study section 
 evaluation methodology and metrics 
our evaluation focuses on the pairwise agreement between 
preferences for results this allows us to compare to previous 
work furthermore for many applications such as tuning 
ranking functions pairwise preference can be used directly for 
training the evaluation is based on comparing 
preferences predicted by various models to the correct 
preferences derived from the explicit user relevance judgments 
we discuss other applications of our models beyond web search 
ranking in section 
to create our set of test pairs we take each query and 
compute the cross-product between all search results returning 
preferences for pairs according to the order of the associated 
relevance labels to avoid ambiguity in evaluation we discard 
all ties i e pairs with equal label 
in order to compute the accuracy of our preference predictions 
with respect to the correct preferences we adapt the standard 
recall and precision measures while our task of computing 
pairwise agreement is different from the absolute relevance 
ranking task the metrics are used in the similar way 
specifically we report the average query recall and precision 
for our task query precision and query recall for a query q are 
defined as 
 query precision fraction of predicted preferences for results 
for q that agree with preferences obtained from explicit 
human judgment 
 query recall fraction of preferences obtained from explicit 
human judgment for q that were correctly predicted 
the overall recall and precision are computed as the average of 
query recall and query precision respectively a drawback of 
this evaluation measure is that some preferences may be more 
valuable than others which pairwise agreement does not capture 
we discuss this issue further when we consider extensions to the 
current work in section 
 datasets 
for evaluation we used queries that were randomly 
sampled from query logs for a major web search engine for each 
query the top returned search results were manually rated on a 
 -point scale by trained judges as part of ongoing relevance 
improvement effort in addition for these queries we also had user 
interaction data for more than instances of these queries 
the user interactions were harvested from anonymous 
browsing traces that immediately followed a query submitted to 
the web search engine this data collection was part of voluntary 
opt-in feedback submitted by users from october through 
october these three weeks days of user interaction data 
was filtered to include only the users in the english-u s market 
in order to better understand the effect of the amount of user 
interaction data available for a query on accuracy we created 
subsets of our data q q and q that contain different 
amounts of interaction data 
 q human-rated queries with at least click on results 
recorded queries query-url pairs 
 q queries in q with at least clicks queries 
 query-url pairs 
 q queries in q with at least clicks queries total 
 query-url pairs 
these datasets were collected as part of normal user experience 
and hence have different characteristics than previously reported 
datasets collected in laboratory settings furthermore the data 
size is order of magnitude larger than any study reported in the 
literature 
 methods compared 
we considered a number of methods for comparison we 
compared our userbehavior model section to previously 
published implicit feedback interpretation techniques and some 
variants of these approaches section and to the current 
search engine ranking based on query and page features alone 
 section specifically we compare the following strategies 
 sa the skip above clickthrough strategy section 
 sa n a more comprehensive extension of sa that takes 
better advantage of current search engine ranking 
 cd our refinement of sa n that takes advantage of our 
mixture model of clickthrough distribution to select trusted 
clicks for interpretation section 
 cdiff our generalization of the cd strategy that explicitly 
uses the relevance component of clickthrough probabilities to 
induce preferences between search results section 
 cd cdiff the strategy combining cd and cdiff as the 
union of predicted preferences from both section 
 userbehavior we order predictions based on decreasing 
highest score of any page in our preliminary experiments 
we observed that higher ranker scores indicate higher 
confidence in the predictions this heuristic allows us to 
do graceful recall-precision tradeoff using the score of the 
highest ranked result to threshold the queries section 
 current current search engine ranking section note 
that the current ranker implementation was trained over a 
superset of the rated query url pairs in our datasets but 
using the same truth labels as we do for our evaluation 
training test split the only strategy for which splitting the 
datasets into training and test was required was the 
userbehavior method to evaluate userbehavior we train and 
validate on of labeled queries and test on the remaining 
 the sampling was done per query i e all results for a 
chosen query were included in the respective dataset and there 
was no overlap in queries between training and test sets 
it is worth noting that both the ad-hoc sa and sa n as well 
as the distribution-based strategies cd cdiff and cd cdiff 
do not require a separate training and test set since they are 
based on heuristics for detecting anomalous click frequencies 
for results hence all strategies except for userbehavior were 
tested on the full set of queries and associated relevance 
preferences while userbehavior was tested on a randomly 
chosen hold-out subset of the queries as described above to 
make sure we are not favoring userbehavior we also tested all 
other strategies on the same hold-out test sets resulting in the 
same accuracy results as testing over the complete datasets 
 results 
we now turn to experimental evaluation of predicting 
relevance preference of web search results figure shows the 
recall-precision results over the q query set section the 
results indicate that previous click interpretation strategies sa 
and sa n perform suboptimally in this setting exhibiting 
precision and respectively furthermore there is no 
mechanism to do recall-precision trade-off with sa and sa n 
as they do not provide prediction confidence in contrast our 
clickthrough distribution-based techniques cd and cd cdiff 
exhibit somewhat higher precision than sa and sa n 
and at recall of maximum achieved by sa or 
sa n 
sa n 
sa 
 
 
 
 
 
 
 
 
 
 
 
 
recall 
precision 
sa sa n 
cd cdiff 
cd cdiff userbehavior 
current 
figure precision vs recall of sa sa n cd cdiff 
cd cdiff userbehavior and current relevance prediction 
methods over the q dataset 
interestingly cdiff alone exhibits precision equal to sa 
 at the same recall at in contrast by combining cd 
and cdiff strategies cd cdiff method we achieve the best 
performance of all clickthrough-based strategies exhibiting 
precision of above for recall values up to and higher at 
lower recall levels clearly aggregating and intelligently 
interpreting clickthroughs results in significant gain for realistic 
web search than previously described strategies however even 
the cd cdiff clickthrough interpretation strategy can be 
improved upon by automatically learning to interpret the 
aggregated clickthrough evidence 
but first we consider the best performing strategy 
userbehavior incorporating post-search navigation history in 
addition to clickthroughs browsing features results in the 
highest recall and precision among all methods compared browse 
exhibits precision of above at recall of significantly 
outperforming our baseline and clickthrough-only strategies 
furthermore browse is able to achieve high recall as high as 
 while maintaining precision significantly higher than 
the baseline ranking 
to further analyze the value of different dimensions of implicit 
feedback modeled by the userbehavior strategy we consider each 
group of features in isolation figure reports precision vs 
recall for each feature group interestingly query-text alone has 
low accuracy only marginally better than random furthermore 
browsing features alone have higher precision with lower 
maximum recall achieved than considering all of the features in 
our userbehavior model applying different machine learning 
methods for combining classifier predictions may increase 
performance of using all features for all recall values 
 
 
 
 
 
 
 
 
recall 
precision 
all features 
clickthrough 
query-text 
browsing 
figure precision vs recall for predicting relevance with 
each group of features individually 
 
 
 
 
 
 
 
 
 
 
 
 
recall 
precision 
cd cdiff q userbehavior q 
cd cdiff q userbehavior q 
cd cdiff q userbehavior q 
figure recall vs precision of cd cdiff and 
userbehavior for query sets q q and q queries with 
at least at least and at least clicks respectively 
interestingly the ranker trained over clickthrough-only 
features achieves substantially higher recall and precision than 
human-designed clickthrough-interpretation strategies described 
earlier for example the clickthrough-trained classifier achieves 
 precision at recall vs the maximum recall of 
achieved by the cd cdiff strategy 
our clickthrough and user behavior interpretation strategies 
rely on extensive user interaction data we consider the effects 
of having sufficient interaction data available for a query before 
proposing a re-ranking of results for that query figure 
reports recall-precision curves for the cd cdiff and 
userbehavior methods for different test query sets with at least 
 click q clicks q and clicks q available per 
query not surprisingly cd cdiff improves with more clicks 
this indicates that accuracy will improve as more user 
interaction histories become available and more queries from 
the q set will have comprehensive interaction histories 
similarly the userbehavior strategy performs better for queries 
with and clicks although the improvement is less dramatic 
than for cd cdiff for queries with sufficient clicks cd cdiff 
exhibits precision comparable with browse at lower recall 
 
 
 
 
 
 
days of user interaction data harvested 
recall 
cd cdiff 
userbehavior 
figure recall of cd cdiff and userbehavior strategies 
at fixed minimum precision for varying amounts of user 
activity data days 
our techniques often do not make relevance predictions for 
search results i e if no interaction data is available for the 
lower-ranked results consequently maintaining higher precision 
at the expense of recall in contrast the current search engine 
always makes a prediction for every result for a given query as 
a consequence the recall of current is high at the 
expense of lower precision as another dimension of acquiring 
training data we consider the learning curve with respect to 
amount days of training data available figure reports the 
recall of cd cdiff and userbehavior strategies for varying 
amounts of training data collected over time we fixed minimum 
precision for both strategies at as a point substantially higher 
than the baseline as expected recall of both strategies 
improves quickly with more days of interaction data examined 
we now briefly summarize our experimental results we 
showed that by intelligently aggregating user clickthroughs 
across queries and users we can achieve higher accuracy on 
predicting user preferences because of the skewed distribution 
of user clicks our clickthrough-only strategies have high 
precision but low recall i e do not attempt to predict relevance 
of many search results nevertheless our cd cdiff 
clickthrough strategy outperforms most recent state-of-the-art 
results by a large margin precision for cd cdiff vs 
for sa n at the highest recall level of sa n 
furthermore by considering the comprehensive userbehavior 
features that model user interactions after the search and beyond 
the initial click we can achieve substantially higher precision 
and recall than considering clickthrough alone our 
userbehavior strategy achieves recall of over with precision 
of over with much higher precision at lower recall levels 
substantially outperforms the current search engine preference 
ranking and all other implicit feedback interpretation methods 
 conclusions and future work 
our paper is the first to our knowledge to interpret 
postsearch user behavior to estimate user preferences in a real web 
search setting we showed that our robust models result in higher 
prediction accuracy than previously published techniques 
we introduced new robust probabilistic techniques for 
interpreting clickthrough evidence by aggregating across users 
and queries our methods result in clickthrough interpretation 
substantially more accurate than previously published results not 
specifically designed for web search scenarios our methods 
predictions of relevance preferences are substantially more 
accurate than the current state-of-the-art search result ranking that 
does not consider user interactions we also presented a general 
model for interpreting post-search user behavior that incorporates 
clickthrough browsing and query features by considering the 
complete search experience after the initial query and click we 
demonstrated prediction accuracy far exceeding that of 
interpreting only the limited clickthrough information 
furthermore we showed that automatically learning to 
interpret user behavior results in substantially better performance 
than the human-designed ad-hoc clickthrough interpretation 
strategies another benefit of automatically learning to interpret 
user behavior is that such methods can adapt to changing 
conditions and changing user profiles for example the user 
behavior model on intranet search may be different from the web 
search behavior our general userbehavior method would be able 
to adapt to these changes by automatically learning to map new 
behavior patterns to explicit relevance ratings 
a natural application of our preference prediction models is to 
improve web search ranking in addition our work has many 
potential applications including click spam detection search 
abuse detection personalization and domain-specific ranking for 
example our automatically derived behavior models could be 
trained on examples of search abuse or click spam behavior 
instead of relevance labels alternatively our models could be 
used directly to detect anomalies in user behavior - either due to 
abuse or to operational problems with the search engine 
while our techniques perform well on average our 
assumptions about clickthrough distributions and learning the 
user behavior models may not hold equally well for all queries 
for example queries with divergent access patterns e g for 
ambiguous queries with multiple meanings may result in 
behavior inconsistent with the model learned for all queries 
hence clustering queries and learning different predictive models 
for each query type is a promising research direction query 
distributions also change over time and it would be productive to 
investigate how that affects the predictive ability of these models 
furthermore some predicted preferences may be more valuable 
than others and we plan to investigate different metrics to capture 
the utility of the predicted preferences 
as we showed in this paper using the wisdom of crowds can 
give us accurate interpretation of user interactions even in the 
inherently noisy web search setting our techniques allow us to 
automatically predict relevance preferences for web search results 
with accuracy greater than the previously published methods the 
predicted relevance preferences can be used for automatic 
relevance evaluation and tuning for deploying search in new 
settings and ultimately for improving the overall web search 
experience 
 references 
 e agichtein e brill and s dumais improving web search ranking 
by incorporating user behavior in proceedings of the acm 
conference on research and development on information retrieval 
 sigir 
 j allan hard track overview in trec high accuracy 
retrieval from documents in proceedings of trec - 
 
 s brin and l page the anatomy of a large-scale hypertextual web 
search engine in proceedings of www - 
 c j c burges t shaked e renshaw a lazier m deeds n 
hamilton and g hullender learning to rank using gradient 
descent in proceedings of the international conference on machine 
learning icml 
 d m chickering the winmine toolkit microsoft technical report 
msr-tr- - 
 m claypool d brown p lee and m waseda inferring user interest 
in ieee internet computing 
 s fox k karnawat m mydland s t dumais and t white 
evaluating implicit measures to improve the search experience in 
acm transactions on information systems 
 j goecks and j shavlick learning users interests by unobtrusively 
observing their normal behavior in proceedings of the ijcai 
workshop on machine learning for information filtering 
 t joachims optimizing search engines using clickthrough data in 
proceedings of the acm conference on knowledge discovery and 
datamining sigkdd 
 t joachims l granka b pang h hembrooke and g gay 
accurately interpreting clickthrough data as implicit feedback in 
proceedings of the acm conference on research and development 
on information retrieval sigir 
 t joachims making large-scale svm learning practical advances 
in kernel methods in support vector learning mit press 
 d kelly and j teevan implicit feedback for inferring user preference 
a bibliography in sigir forum 
 j konstan b miller d maltz j herlocker l gordon and j riedl 
grouplens applying collaborative filtering to usenet news in 
communications of acm 
 m morita and y shinoda information filtering based on user 
behavior analysis and best match text retrieval in proceedings of the 
acm conference on research and development on information 
retrieval sigir 
 d oard and j kim implicit feedback for recommender systems in 
proceedings of aaai workshop on recommender systems 
 d oard and j kim modeling information content using observable 
behavior in proceedings of the th annual meeting of the 
american society for information science and technology 
 p pirolli the use of proximal information scent to forage for distal 
content on the world wide web in working with technology in 
mind brunswikian resources for cognitive science and 
engineering oxford university press 
 f radlinski and t joachims query chains learning to rank from 
implicit feedback in proceedings of the acm conference on 
knowledge discovery and data mining kdd acm 
 f radlinski and t joachims evaluating the robustness of learning 
from implicit feedback in the icml workshop on learning in web 
search 
 g salton and m mcgill introduction to modern information 
retrieval mcgraw-hill 
 e m voorhees d harman overview of trec 
