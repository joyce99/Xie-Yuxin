a new approach for evaluating query expansion 
query-document term mismatch 
tonya custis 
thomson corporation 
 opperman drive 
st paul mn 
tonya custis thomson com 
khalid al-kofahi 
thomson corporation 
 opperman drive 
st paul mn 
khalid al-kofahi thomson com 
abstract 
the effectiveness of information retrieval ir systems is 
influenced by the degree of term overlap between user queries 
and relevant documents query-document term mismatch 
whether partial or total is a fact that must be dealt with by 
ir systems query expansion qe is one method for 
dealing with term mismatch ir systems implementing query 
expansion are typically evaluated by executing each query 
twice with and without query expansion and then 
comparing the two result sets while this measures an overall 
change in performance it does not directly measure the 
effectiveness of ir systems in overcoming the inherent issue of 
term mismatch between the query and relevant documents 
nor does it provide any insight into how such systems would 
behave in the presence of query-document term mismatch 
in this paper we propose a new approach for evaluating 
query expansion techniques the proposed approach is 
attractive because it provides an estimate of system 
performance under varying degrees of query-document term 
mismatch it makes use of readily available test collections and 
it does not require any additional relevance judgments or 
any form of manual processing 
categories and subject descriptors 
h information storage and retrieval information 
search and retrieval 
general terms 
measurement experimentation 
 introduction 
in our domain 
and unlike web search it is very 
important for attorneys to find all documents e g cases that 
are relevant to an issue missing relevant documents may 
have non-trivial consequences on the outcome of a court 
proceeding attorneys are especially concerned about missing 
relevant documents when researching a legal topic that is 
new to them as they may not be aware of all language 
variations in such topics therefore it is important to develop 
information retrieval systems that are robust with respect to 
language variations or term mismatch between queries and 
relevant documents during our work on developing such 
systems we concluded that current evaluation methods are 
not sufficient for this purpose 
 whooping cough pertussis heart attack myocardial 
infarction car wash automobile cleaning attorney 
legal counsel lawyer are all examples of things that share 
the same meaning often the terms chosen by users in their 
queries are different than those appearing in the documents 
relevant to their information needs this query-document 
term mismatch arises from two sources the synonymy 
found in natural language both at the term and the phrasal 
level and the degree to which the user is an expert at 
searching and or has expert knowledge in the domain of the 
collection being searched 
ir evaluations are comparative in nature cf trec 
generally ir evaluations show how system a did in 
relation to system b on the same test collection based on various 
precision- and recall-based metrics similarly ir systems 
with qe capabilities are typically evaluated by executing 
each search twice once with and once without query 
expansion and then comparing the two result sets while this 
approach shows which system may have performed better 
overall with respect to a particular test collection it does 
not directly or systematically measure the effectiveness of 
ir systems in overcoming query-document term mismatch 
if the goal of qe is to increase search performance by 
mitigating the effects of query-document term mismatch then 
the degree to which a system does so should be measurable 
in evaluation an effective evaluation method should 
measure the performance of ir systems under varying degrees of 
query-document term mismatch not just in terms of overall 
performance on a collection relative to another system 
 
thomson corporation builds information based solutions 
to the professional markets including legal financial health 
care scientific and tax and accounting 
in order to measure that a particular ir system is able 
to overcome query-document term mismatch by retrieving 
documents that are relevant to a user s query but that do 
not necessarily contain the query terms themselves we 
systematically introduce term mismatch into the test collection 
by removing query terms from known relevant documents 
because we are purposely inducing term mismatch between 
the queries and known relevant documents in our test 
collections the proposed evaluation framework is able to measure 
the effectiveness of qe in a way that testing on the whole 
collection is not if a qe search method finds a document 
that is known to be relevant but that is nonetheless missing 
query terms it shows that qe technique is indeed robust 
with respect to query-document term mismatch 
 related work 
accounting for term mismatch between the terms in user 
queries and the documents relevant to users information 
needs has been a fundamental issue in ir research for 
almost years query expansion qe is one 
technique used in ir to improve search performance by 
increasing the likelihood of term overlap either explicitly or 
implicitly between queries and documents that are relevant 
to users information needs explicit query expansion 
occurs at run-time based on the initial search results as is 
the case with relevance feedback and pseudo relevance 
feedback implicit query expansion can be based on 
statistical properties of the document collection or it may 
rely on external knowledge sources such as a thesaurus or an 
ontology regardless of method qe 
algorithms that are capable of retrieving relevant documents 
despite partial or total term mismatch between queries and 
relevant documents should increase the recall of ir systems 
 by retrieving documents that would have previously been 
missed as well as their precision by retrieving more 
relevant documents 
in practice qe tends to improve the average overall 
retrieval performance doing so by improving performance on 
some queries while making it worse on others qe 
techniques are judged as effective in the case that they help 
more than they hurt overall on a particular collection 
 often the expansion terms added to a query 
in the query expansion phase end up hurting the overall 
retrieval performance because they introduce semantic noise 
causing the meaning of the query to drift as such much 
work has been done with respect to different strategies for 
choosing semantically relevant qe terms to include in order 
to avoid query drift 
the evaluation of ir systems has received much attention 
in the research community both in terms of developing test 
collections for the evaluation of different systems 
 and in terms of the utility of evaluation metrics such as 
recall precision mean average precision precision at rank 
bpref etc in addition there have been 
comparative evaluations of different qe techniques on various 
test collections 
in addition the ir research community has given 
attention to differences between the performance of individual 
queries research efforts have been made to predict which 
queries will be improved by qe and then selectively 
applying it only to those queries to achieve 
optimal overall performance in addition related work on 
predicting query difficulty or which queries are likely to 
perform poorly has been done there is general 
interest in the research community to improve the 
robustness of ir systems by improving retrieval performance on 
difficult queries as is evidenced by the robust track in the 
trec competitions and new evaluation measures such as 
gmap gmap geometric mean average precision gives 
more weight to the lower end of the average precision as 
opposed to map thereby emphasizing the degree to which 
difficult or poorly performing queries contribute to the score 
 
however no attention is given to evaluating the 
robustness of ir systems implementing qe with respect to 
querydocument term mismatch in quantifiable terms by 
purposely inducing mismatch between the terms in queries and 
relevant documents our evaluation framework allows us a 
controlled manner in which to degrade the quality of the 
queries with respect to their relevant documents and then 
to measure the both the degree of induced difficulty of the 
query and the degree to which qe improves the retrieval 
performance of the degraded query 
the work most similar to our own in the literature consists 
of work in which document collections or queries are altered 
in a systematic way to measure differences query 
performance introduces into the document collection 
pseudowords that are ambiguous with respect to word sense in 
order to measure the degree to which word sense 
disambiguation is useful in ir experiments with altering the 
document collection by adding semantically related expansion 
terms to documents at indexing time in cross-language ir 
 explores different query expansion techniques while 
purposely degrading their translation resources in what amounts 
to expanding a query with only a controlled percentage of 
its translation terms although similar in introducing a 
controlled amount of variance into their test collections these 
works differ from the work being presented in this paper 
in that the work being presented here explicitly and 
systematically measures query effectiveness in the presence of 
query-document term mismatch 
 methodology 
in order to accurately measure ir system performance in 
the presence of query-term mismatch we need to be able 
to adjust the degree of term mismatch in a test corpus in 
a principled manner our approach is to introduce 
querydocument term mismatch into a corpus in a controlled 
manner and then measure the performance of ir systems as 
the degree of term mismatch changes we systematically 
remove query terms from known relevant documents 
creating alternate versions of a test collection that differ only in 
how many or which query terms have been removed from 
the documents relevant to a particular query introducing 
query-document term mismatch into the test collection in 
this manner allows us to manipulate the degree of term 
mismatch between relevant documents and queries in a 
controlled manner 
this removal process affects only the relevant documents 
in the search collection the queries themselves remain 
unaltered query terms are removed from documents one by 
one so the differences in ir system performance can be 
measured with respect to missing terms in the most extreme 
case i e when the length of the query is less than or equal 
to the number of query terms removed from the relevant 
documents there will be no term overlap between a query 
and its relevant documents notice that for a given query 
only relevant documents are modified non-relevant 
documents are left unchanged even in the case that they contain 
query terms 
although on the surface we are changing the 
distribution of terms between the relevant and non-relevant 
documents sets by removing query terms from the relevant 
documents doing so does not change the conceptual relevancy 
of these documents systematically removing query terms 
from known relevant documents introduces a controlled amount 
of query-document term mismatch by which we can 
evaluate the degree to which particular qe techniques are able 
to retrieve conceptually relevant documents despite a lack 
of actual term overlap removing a query term from 
relevant documents simply masks the presence of that query 
term in those documents it does not in any way change the 
conceptual relevancy of the documents 
the evaluation framework presented in this paper consists 
of three elements a test collection c a strategy for selecting 
which query terms to remove from the relevant documents in 
that collection s and a metric by which to compare 
performance of the ir systems m the test collection c consists 
of a document collection queries and relevance judgments 
the strategy s determines the order and manner in which 
query terms are removed from the relevant documents in c 
this evaluation framework is not metric-specific any metric 
 map p  recall etc can be used to measure ir system 
performance 
although test collections are difficult to come by it should 
be noted that this evaluation framework can be used on 
any available test collection in fact using this framework 
stretches the value of existing test collections in that one 
collection becomes several when query terms are removed from 
relevant documents thereby increasing the amount of 
information that can be gained from evaluating on a particular 
collection 
in other evaluations of qe effectiveness the controlled 
variable is simply whether or not queries have been 
expanded or not compared in terms of some metric in 
contrast the controlled variable in this framework is the query 
term that has been removed from the documents relevant to 
that query as determined by the removal strategy s query 
terms are removed one by one in a manner and order 
determined by s so that collections differ only with respect 
to the one term that has been removed or masked in the 
documents relevant to that query it is in this way that we 
can explicitly measure the degree to which an ir system 
overcomes query-document term mismatch 
the choice of a query term removal strategy is relatively 
flexible the only restriction in choosing a strategy s is that 
query terms must be removed one at a time two 
decisions must be made when choosing a removal strategy s 
the first is the order in which s removes terms from the 
relevant documents possible orders for removal could be 
based on metrics such as idf or the global probability of a 
term in a document collection based on the purpose of the 
evaluation and the retrieval algorithm being used it might 
make more sense to choose a removal order for s based on 
query term idf or perhaps based on a measure of query 
term probability in the document collection 
once an order for removal has been decided a manner for 
term removal masking must be decided it must be 
determined if s will remove the terms individually i e remove 
just one different term each time or additively i e remove 
one term first then that term in addition to another and so 
on the incremental additive removal of query terms from 
relevant documents allows the evaluation to show the 
degree to which ir system performance degrades as more and 
more query terms are missing thereby increasing the degree 
of query-document term mismatch removing terms 
individually allows for a clear comparison of the contribution of 
qe in the absence of each individual query term 
 experimental set-up 
 ir systems 
we used the proposed evaluation framework to evaluate 
four ir systems on two test collections of the four 
systems used in the evaluation two implement query 
expansion techniques okapi with pseudo-feedback for qe and 
a proprietary concept search engine we ll call it tcs for 
thomson concept search tcs is a language modeling 
based retrieval engine that utilizes a subject-appropriate 
external corpus i e legal or news as a knowledge source 
this external knowledge source is a corpus separate from 
but thematically related to the document collection to be 
searched translation probabilities for qe are calculated 
from these large external corpora 
okapi without feedback and a language model query 
likelihood ql model implemented using indri are 
included as keyword-only baselines okapi without feedback 
is intended as an analogous baseline for okapi with 
feedback and the ql model is intended as an appropriate 
baseline for tcs as they both implement language-modeling 
based retrieval algorithms we choose these as baselines 
because they are dependent only on the words appearing in 
the queries and have no qe capabilities as a result we 
expect that when query terms are removed from relevant 
documents the performance of these systems should degrade 
more dramatically than their counterparts that implement 
qe 
the okapi and ql model results were obtained using the 
lemur toolkit 
okapi was run with the parameters k 
b and k when run with feedback the feedback 
parameters used in okapi were set at documents and 
terms the ql model used jelinek-mercer smoothing with 
Î» 
 test collections 
we evaluated the performance of the four ir systems 
outlined above on two different test collections the two test 
collections used were the trec ap collection tipster 
disk and the fsupp collection 
the fsupp collection is a proprietary collection of 
case law documents for which we have queries ranging 
from four to twenty-two words after stop word removal with 
full relevance judgments 
the average length of documents 
in the fsupp collection is words 
 
www lemurproject org 
 
each of the documents was evaluated by domain 
experts with respect to each of the queries 
the trec ap test collection contains 
documents averaging words in length in our evaluation we 
used both the title and the description fields of topics 
 as queries so we have two sets of results for the ap 
collection after stop word removal the title queries range 
from two to eleven words and the description queries range 
from four to twenty-six terms 
 query term removal strategy 
in our experiments we chose to sequentially and 
additively remove query terms from highest-to-lowest inverse 
document frequency idf with respect to the entire 
document collection terms with high idf values tend to 
influence document ranking more than those with lower idf 
values additionally high idf terms tend to be 
domainspecific terms that are less likely to be known to non-expert 
user hence we start by removing these first 
for the fsupp collection queries were evaluated 
incrementally with one two three five and seven terms 
removed from their corresponding relevant documents the 
longer description queries from trec topics - were 
likewise evaluated on the ap collection with one two 
three five and seven query terms removed from their 
relevant documents for the shorter trec title queries we 
removed one two three and five terms from the relevant 
documents 
 metrics 
in this implementation of the evaluation framework we 
chose three metrics by which to compare ir system 
performance mean average precision map precision at 
documents p and recall at documents although 
these are the metrics we chose to demonstrate this 
framework any appropriate ir metrics could be used within the 
framework 
 results 
 fsupp collection 
figures and show the performance in terms of 
map p and recall respectively for the four search 
engines on the fsupp collection as expected the 
performance of the keyword-only ir systems ql and okapi drops 
quickly as query terms are removed from the relevant 
documents in the collection the performance of okapi with 
feedback okapi fb is somewhat surprising in that on the 
original collection i e prior to query term removal its 
performance is worse than that of okapi without feedback on 
all three measures 
tcs outperforms the ql keyword baseline on every 
measure except for map on the original collection i e prior 
to removing any query terms because tcs employs 
implicit query expansion using an external domain specific 
knowledge base it is less sensitive to term removal i e 
mismatch than the okapi fb which relies on terms from 
the top-ranked documents retrieved by an initial 
keywordonly search because overall search engine performance is 
frequently measured in terms of map and because other 
evaluations of qe often only consider performance on the 
entire collection i e they do not consider term mismatch 
the qe implemented in tcs would be considered in 
an 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
meanaverageprecision map 
okapi fb 
okapi 
tcs 
ql 
fsupp mean average precision with query terms removed 
figure the performance of the four retrieval 
systems on the fsupp collection in terms of mean 
average precision map and as a function of the 
number of query terms removed the horizontal axis 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
 
 
 
precisionat documents p 
okapi fb 
okapi 
tcs 
ql 
fsupp p with query terms removed 
figure the performance of the four retrieval 
systems on the fsupp collection in terms of precision 
at and as a function of the number of query terms 
removed the horizontal axis 
other evaluation to hurt performance on the fsupp 
collection however when we look at the comparison of tcs to 
ql when query terms are removed from the relevant 
documents we can see that the qe in tcs is indeed contributing 
positively to the search 
 the ap collection using the 
description queries 
figures and show the performance of the four ir 
systems on the ap collection using the trec topic 
descriptions as queries the most interesting difference 
between the performance on the fsupp collection and the 
ap collection is the reversal of okapi fb and tcs on 
fsupp tcs outperformed the other engines consistently 
 see figures and on the ap collection okapi 
fb is clearly the best performer see figures and 
this is all the more interesting based on the fact that qe in 
okapi fb takes place after the first search iteration which 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
 
recall 
okapi fb 
okapi 
tcs 
indri 
fsupp recall at documents with query terms removed 
figure the recall at of the four retrieval 
systems on the fsupp collection as a function of 
the number of query terms removed the horizontal 
axis 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
meanaverageprecision map 
okapi fb 
okapi 
tcs 
ql 
ap mean average precision with query terms removed description queries 
figure map of the four ir systems on the ap 
collection using trec description queries map 
is measured as a function of the number of query 
terms removed 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
precisionat documents p 
okapi fb 
okapi 
tcs 
ql 
ap p with query terms removed description queries 
figure precision at of the four ir systems 
on the ap collection using trec description 
queries p at is measured as a function of the 
number of query terms removed 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
recall 
okapi fb 
okapi 
tcs 
ql 
ap recall at documents with query terms removed description queries 
figure recall at of the four ir systems 
on the ap collection using trec description 
queries and as a function of the number of query 
terms removed 
we would expect to be handicapped when query terms are 
removed 
looking at p in figure we can see that tcs and 
okapi fb score similarly on p starting at the point where 
one query term is removed from relevant documents at two 
query terms removed tcs starts outperforming okapi fb 
if modeling this in terms of expert versus non-expert users 
we could conclude that tcs might be a better search engine 
for non-experts to use on the ap collection while okapi 
fb would be best for an expert searcher 
it is interesting to note that on each metric for the ap 
description queries tcs performs more poorly than all the 
other systems on the original collection but quickly 
surpasses the baseline systems and approaches okapi fb s 
performance as terms are removed this is again a case where 
the performance of a system on the entire collection is not 
necessarily indicative of how it handles query-document term 
mismatch 
 the ap collection using the title queries 
figures and show the performance of the four ir 
systems on the ap collection using the trec topic titles 
as queries as with the ap description queries okapi 
fb is again the best performer of the four systems in the 
evaluation as before the performance of the okapi and 
ql systems the non-qe baseline systems sharply degrades 
as query terms are removed on the shorter queries tcs 
seems to have a harder time catching up to the performance 
of okapi fb as terms are removed 
perhaps the most interesting result from our evaluation 
is that although the keyword-only baselines performed 
consistently and as expected on both collections with respect 
to query term removal from relevant documents the 
performances of the engines implementing qe techniques differed 
dramatically between collections 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
meanaverageprecision map 
okapi fb 
okapi 
tcs 
ql 
ap mean average precision with query terms removed title queries 
figure map of the four ir systems on the ap 
collection using trec title queries and as a 
function of the number of query terms removed 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
 
precisionat documents p 
okapi fb 
okapi 
tcs 
ql 
ap p with query terms removed title queries 
figure precision at of the four ir systems on 
the ap collection using trec title queries and 
as a function of the number of query terms removed 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
recall 
okapi fb 
okapi 
tcs 
ql 
ap recall at documents with query terms removed title queries 
figure recall at of the four ir systems on 
the ap collection using trec title queries and 
as a function of the number of query terms removed 
 discussion 
the intuition behind this evaluation framework is to 
measure the degree to which various qe techniques overcome 
term mismatch between queries and relevant documents in 
general it is easy to evaluate the overall performance of 
different techniques for qe in comparison to each other or 
against a non-qe variant on any complete test collection 
such an approach does tell us which systems perform better 
on a complete test collection but it does not measure the 
ability of a particular qe technique to retrieve relevant 
documents despite partial or complete term mismatch between 
queries and relevant documents 
a systematic evaluation of ir systems as outlined in this 
paper is useful not only with respect to measuring the 
general success or failure of particular qe techniques in the 
presence of query-document term mismatch but it also 
provides insight into how a particular ir system will perform 
when used by expert versus non-expert users on a 
particular collection the less a user knows about the domain of 
the document collection on which they are searching the 
more prevalent query-document term mismatch is likely to 
be this distinction is especially relevant in the case that 
the test collection is domain-specific i e medical or legal as 
opposed to a more general domain such as news where the 
distinction between experts and non-experts may be more 
marked for example a non-expert in the medical domain 
might search for whooping cough but relevant documents 
might instead contain the medical term pertussis 
since query terms are masked only the in relevant 
documents this evaluation framework is actually biased against 
retrieving relevant documents this is because non-relevant 
documents may also contain query terms which can cause 
a retrieval system to rank such documents higher than it 
would have before terms were masked in relevant documents 
still we think this is a more realistic scenario than removing 
terms from all documents regardless of relevance 
the degree to which a qe technique is well-suited to a 
particular collection can be evaluated in terms of its ability 
to still find the relevant documents even when they are 
missing query terms despite the bias of this approach against 
relevant documents however given that okapi fb and tcs 
outperformed each other on two different collection sets 
further investigation into the degree of compatibility between 
qe expansion approach and target collection is probably 
warranted furthermore the investigation of other term 
removal strategies could provide insight into the behavior of 
different qe techniques and their overall impact on the user 
experience 
as mentioned earlier our choice of the term removal 
strategy was motivated by our desire to see the highest 
impact on system performance as terms are removed and 
because high idf terms in our domain context are more 
likely to be domain specific which allows us to better 
understand the performance of an ir system as experienced 
by expert and non-expert users 
although not attempted in our experiments another 
application of this evaluation framework would be to remove 
query terms individually rather than incrementally to 
analyze which terms or possibly which types of terms are 
being helped most by a qe technique on a particular test 
collection this could lead to insight as to when qe should 
and should not be applied 
this evaluation framework allows us to see how ir 
systems perform in the presence of query-document term 
mismatch in other evaluations the performance of a system is 
measured only on the entire collection in which the degree 
of query-term document mismatch is not known by 
systematically introducing this mismatch we can see that even 
if an ir system is not the best performer on the entire 
collection its performance may nonetheless be more robust to 
query-document term mismatch than other systems such 
robustness makes a system more user-friendly especially to 
non-expert users 
this paper presents a novel framework for ir system 
evaluation the applications of which are numerous the results 
presented in this paper are not by any means meant to be 
exhaustive or entirely representative of the ways in which 
this evaluation could be applied to be sure there is much 
future work that could be done using this framework 
in addition to looking at average performance of ir 
systems the results of individual queries could be examined and 
compared more closely perhaps giving more insight into the 
classification and prediction of difficult queries or perhaps 
showing which qe techniques improve or degrade 
individual query performance under differing degrees of 
querydocument term mismatch indeed this framework would 
also benefit from further testing on a larger collection 
 conclusion 
the proposed evaluation framework allows us to measure 
the degree to which different ir systems overcome or don t 
overcome term mismatch between queries and relevant 
documents evaluations of ir systems employing qe performed 
only on the entire collection do not take into account that 
the purpose of qe is to mitigate the effects of term mismatch 
in retrieval by systematically removing query terms from 
relevant documents we can measure the degree to which 
qe contributes to a search by showing the difference 
between the performances of a qe system and its 
keywordonly baseline when query terms have been removed from 
known relevant documents further we can model the 
behavior of expert versus non-expert users by manipulating 
the amount of query-document term mismatch introduced 
into the collection 
the evaluation framework proposed in this paper is 
attractive for several reasons most importantly it provides 
a controlled manner in which to measure the performance 
of qe with respect to query-document term mismatch in 
addition this framework takes advantage and stretches the 
amount of information we can get from existing test 
collections further this evaluation framework is not 
metricspecific information in terms of any metric map p  
etc can be gained from evaluating an ir system this way 
it should also be noted that this framework is 
generalizable to any ir system in that it evaluates how well ir 
systems evaluate users information needs as represented by 
their queries an ir system that is easy to use should be 
good at retrieving documents that are relevant to users 
information needs even if the queries provided by the users do 
not contain the same keywords as the relevant documents 
 references 
 amati g c carpineto and g romano query 
difficulty robustness and selective application of query 
expansion in proceedings of the th european 
conference on information retrieval ecir 
pp - 
 berger a and j d lafferty information 
retrieval as statistical translation in research and 
development in information retrieval pages - 
 billerbeck b f scholer h e williams and j 
zobel query expansion using associated queries 
in proceedings of cikm pp - 
 billerbeck b and j zobel when query 
expansion fails in proceedings of sigir pp 
 - 
 billerbeck b and j zobel questioning query 
expansion an examination of behaviour and 
parameters in proceedings of the th australasian 
database conference adc pp - 
 billerbeck b and j zobel document 
expansion versus query expansion for ad-hoc 
retrieval in proceedings of the th australasian 
document computing symposium 
 buckley c and e m voorhees evaluating 
evaluation measure stability in proceedings of sigir 
 pp - 
 buckley c and e m voorhees retrieval 
evaluation with incomplete information in 
proceedings of sigir pp - 
 carmel d e yom-tov a darlow d pelleg 
what makes a query difficult in proceedings of 
sigir pp - 
 carpineto c r mori and g romano 
informative term selection for automatic query 
expansion in the th text retrieval conference 
pp 
 carterette b and j allan incremental test 
collections in proceedings of cikm pp 
 - 
 carterette b j allan and r sitaraman 
minimal test collections for retrieval evaluation in 
proceedings of sigir pp - 
 cormack g v c r palmer and c l clarke 
efficient construction of large test collections in 
proceedings of sigir pp - 
 cormack g and t r lynam statistical 
precision of information retrieval evaluation in 
proceedings of sigir pp - 
 cronen-townsend s y zhou and w b croft 
a language modeling framework for selective query 
expansion ciir technical report 
 efthimiadis e n query expansion in martha 
e williams ed annual review of information 
systems and technology arist v pp - 
 evans d a and lefferts r g clarit-trec 
experiments information processing management 
 - 
 fang h and c x zhai semantic term 
matching in axiomatic approaches to information 
retrieval in proceedings of sigir pp - 
 gao j j nie g wu and g cao dependence 
language model for information retrieval in 
proceedings of sigir pp - 
 harman d k relevance feedback revisited in 
proceedings of acm sigir pp - 
 harman d k ed the first text retrieval 
conference trec- 
 harman d k ed the second text retrieval 
conference trec- 
 harman d k ed the third text retrieval 
conference trec- 
 harman d k towards interactive query 
expansion in proceedings of sigir pp - 
 hofmann t probabilistic latent semantic 
indexing in proceedings of sigir pp - 
 jing y and w b croft the association 
thesaurus for information retrieval in proceedings of 
riao pp - 
 lu x a and r b keefer query expansion reduction 
and its impact on retrieval effectiveness in d k 
harman ed the third text retrieval conference 
 trec- gaithersburg md national institute of 
standards and technology - 
 mcnamee p and j mayfield comparing 
cross-language query expansion techniques by 
degrading translation resources in proceedings of 
sigir pp - 
 mitra m a singhal and c buckley 
improving automatic query expansion in 
proceedings of sigir pp - 
 peat h j and p willett the limitations of 
term co-occurrence data for query expansion in 
document retrieval systems journal of the american 
society for information science - 
 ponte j m and w b croft a language 
modeling approach to information retrieval in 
proceedings of sigir pp - 
 qiu y and frei h concept based query 
expansion in proceedings of sigir pp - 
 robertson s on gmap - and other 
transformations in proceedings of cikm pp 
 - 
 robertson s e and k sparck jones relevance 
weighting of search terms journal of the american 
society for information science - 
 robertson s e s walker s jones m m 
hancock-beaulieu and m gatford okapi at 
trec- in d k harman ed the second text 
retrieval conference trec- pp - 
 robertson s e s walker s jones m m 
hancock-beaulieu and m gatford okapi at 
trec- in d k harman ed the third text 
retrieval conference trec- pp - 
 rocchio j j relevance feedback in information 
retrieval in g salton ed the smart retrieval 
system prentice-hall inc englewood cliffs nj pp 
 - 
 salton g automatic information organization 
and retrieval mcgraw-hill 
 salton g the smart retrieval system 
experiments in automatic document processing 
englewood cliffs nj prentice-hall 
 salton g automatic term class construction 
using relevance-a summary of work in automatic 
pseudoclassification information processing 
management - 
 salton g and c buckley on the use of 
spreading activation methods in automatic 
information retrieval in proceedings of sigir 
pp - 
 sanderson m word sense disambiguation and 
information retrieval in proceedings of sigir 
pp - 
 sanderson m and h joho forming test 
collections with no system pooling in proceedings of 
sigir pp - 
 sanderson m and zobel j information 
retrieval system evaluation effort sensitivity and 
reliability in proceedings of sigir pp - 
 smeaton a f and c j van rijsbergen the 
retrieval effects of query expansion on a feedback 
document retrieval system computer journal 
 - 
 song f and w b croft a general language 
model for information retrieval in proceedings of the 
eighth international conference on information and 
knowledge management pages - 
 sparck jones k automatic keyword 
classification for information retrieval london 
butterworths 
 terra e and c l clarke scoring missing 
terms in information retrieval tasks in proceedings of 
cikm pp - 
 turtle howard natural language vs boolean 
query evaluation a comparison of retrieval 
performance in proceedings of sigir pp 
 - 
 voorhees e m a on expanding query vectors 
with lexically related words in harman d k ed 
text retrieval conference trec- 
 voorhees e m b query expansion using 
lexical-semantic relations in proceedings of sigir 
 pp - 
