using asymmetric distributions 
to improve text classifier probability estimates 
paul n bennett 
computer science dept 
carnegie mellon university 
pittsburgh pa 
pbennett  cs cmu edu 
abstract 
text classifiers that give probability estimates are more readily 
applicable in a variety of scenarios for example rather than 
choosing one set decision threshold they can be used in a bayesian 
risk model to issue a run-time decision which minimizes a 
userspecified cost function dynamically chosen at prediction time 
however the quality of the probability estimates is crucial we review a 
variety of standard approaches to converting scores and poor 
probability estimates from text classifiers to high quality estimates and 
introduce new models motivated by the intuition that the empirical 
score distribution for the extremely irrelevant hard to 
discriminate and obviously relevant items are often significantly 
different finally we analyze the experimental performance of these 
models over the outputs of two text classifiers the analysis 
demonstrates that one of these models is theoretically attractive 
 introducing few new parameters while increasing flexibility 
computationally efficient and empirically preferable 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval i artificial intelligence learning i 
 pattern recognition design methodology 
general terms 
algorithms experimentation reliability 
 introduction 
text classifiers that give probability estimates are more flexible 
in practice than those that give only a simple classification or even a 
ranking for example rather than choosing one set decision 
threshold they can be used in a bayesian risk model to issue a 
runtime decision which minimizes the expected cost of a user-specified 
cost function dynamically chosen at prediction time this can be 
used to minimize a linear utility cost function for filtering tasks 
where pre-specified costs of relevant irrelevant are not available 
during training but are specified at prediction time furthermore 
the costs can be changed without retraining the model 
additionally probability estimates are often used as the basis of deciding 
which document s label to request next during active learning 
 effective active learning can be key in many information 
retrieval tasks where obtaining labeled data can be costly - severely 
reducing the amount of labeled data needed to reach the same 
performance as when new labels are requested randomly finally 
they are also amenable to making other types of cost-sensitive 
decisions and for combining decisions however in all of 
these tasks the quality of the probability estimates is crucial 
parametric models generally use assumptions that the data 
conform to the model to trade-off flexibility with the ability to estimate 
the model parameters accurately with little training data since 
many text classification tasks often have very little training data we 
focus on parametric methods however most of the existing 
parametric methods that have been applied to this task have an 
assumption we find undesirable while some of these methods allow the 
distributions of the documents relevant and irrelevant to the topic 
to have different variances they typically enforce the unnecessary 
constraint that the documents are symmetrically distributed around 
their respective modes we introduce several asymmetric 
parametric models that allow us to relax this assumption without 
significantly increasing the number of parameters and demonstrate how 
we can efficiently fit the models additionally these models can be 
interpreted as assuming the scores produced by the text classifier 
have three basic types of empirical behavior - one corresponding 
to each of the extremely irrelevant hard to discriminate and 
obviously relevant items 
we first review related work on improving probability estimates 
and score modeling in information retrieval then we discuss in 
further detail the need for asymmetric models after this we 
describe two specific asymmetric models and using two standard text 
classifiers na¨ıve bayes and svms demonstrate how they can be 
efficiently used to recalibrate poor probability estimates or produce 
high quality probability estimates from raw scores we then review 
experiments using previously proposed methods and the 
asymmetric methods over several text classification corpora to demonstrate 
the strengths and weaknesses of the various methods finally we 
summarize our contributions and discuss future directions 
 related work 
parametric models have been employed to obtain probability 
estimates in several areas of information retrieval lewis gale 
use logistic regression to recalibrate na¨ıve bayes though the quality 
of the probability estimates are not directly evaluated it is simply 
performed as an intermediate step in active learning manmatha 
et al introduced models appropriate to produce probability 
estimates from relevance scores returned from search engines and 
demonstrated how the resulting probability estimates could be 
subsequently employed to combine the outputs of several search 
engines they use a different parametric distribution for the relevant 
and irrelevant classes but do not pursue two-sided asymmetric 
distributions for a single class as described here they also survey the 
long history of modeling the relevance scores of search engines 
our work is similar in flavor to these previous attempts to model 
search engine scores but we target text classifier outputs which we 
have found demonstrate a different type of score distribution 
behavior because of the role of training data 
focus on improving probability estimates has been growing lately 
zadrozny elkan provide a corrective measure for decision 
trees termed curtailment and a non-parametric method for 
recalibrating na¨ıve bayes in more recent work they investigate 
using a semi-parametric method that uses a monotonic 
piecewiseconstant fit to the data and apply the method to na¨ıve bayes and a 
linear svm while they compared their methods to other 
parametric methods based on symmetry they fail to provide significance 
test results our work provides asymmetric parametric methods 
which complement the non-parametric and semi-parametric 
methods they propose when data scarcity is an issue in addition their 
methods reduce the resolution of the scores output by the classifier 
 the number of distinct values output but the methods here do not 
have such a weakness since they are continuous functions 
there is a variety of other work that this paper extends platt 
 uses a logistic regression framework that models noisy class 
labels to produce probabilities from the raw output of an svm 
his work showed that this post-processing method not only can 
produce probability estimates of similar quality to svms directly 
trained to produce probabilities regularized likelihood kernel 
methods but it also tends to produce sparser kernels which generalize 
better finally bennett obtained moderate gains by applying 
platt s method to the recalibration of na¨ıve bayes but found there 
were more problematic areas than when it was applied to svms 
recalibrating poorly calibrated classifiers is not a new problem 
lindley et al first proposed the idea of recalibrating classifiers 
and degroot fienberg gave the now accepted standard 
formalization for the problem of assessing calibration initiated by 
others 
 problem definition approach 
our work differs from earlier approaches primarily in three points 
 we provide asymmetric parametric models suitable for use when 
little training data is available we explicitly analyze the quality 
of probability estimates these and competing methods produce and 
provide significance tests for these results we target text 
classifier outputs where a majority of the previous literature targeted the 
output of search engines 
 problem definition 
the general problem we are concerned with is highlighted in 
figure a text classifier produces a prediction about a document 
and gives a score s d indicating the strength of its decision that 
the document belongs to the positive class relevant to the topic 
we assume throughout there are only two classes the positive and 
the negative or irrelevant class and - respectively 
there are two general types of parametric approaches the first 
of these tries to fit the posterior function directly i e there is one 
p s p s − 
bayes rulep p − 
classifier 
p s d 
predict class c d − 
confidence s d that c d 
document d 
and give unnormalized 
figure we are concerned with how to perform the box 
highlighted in grey the internals are for one type of approach 
function estimator that performs a direct mapping of the score s to 
the probability p s d the second type of approach breaks the 
problem down as shown in the grey box of figure an estimator 
for each of the class-conditional densities i e p s and p s − 
is produced then bayes rule and the class priors are used to obtain 
the estimate for p s d 
 motivation for asymmetric distributions 
most of the previous parametric approaches to this problem 
either directly or indirectly when fitting only the posterior 
correspond to fitting gaussians to the class-conditional densities they 
differ only in the criterion used to estimate the parameters we can 
visualize this as depicted in figure since increasing s usually 
indicates increased likelihood of belonging to the positive class then 
the rightmost distribution usually corresponds to p s 
a b 
c 
 
 
 
 
 
 
− − 
p s class − 
unnormalized confidence score s 
p s class 
p s class − 
figure typical view of discrimination based on gaussians 
however using standard gaussians fails to capitalize on a basic 
characteristic commonly seen namely if we have a raw output 
score that can be used for discrimination then the empirical 
behavior between the modes label b in figure is often very different 
than that outside of the modes labels a and c in figure 
intuitively the area between the modes corresponds to the hard 
examples which are difficult for this classifier to distinguish while the 
areas outside the modes are the extreme examples that are usually 
easily distinguished this suggests that we may want to uncouple 
the scale of the outside and inside segments of the distribution as 
depicted by the curve denoted as a-gaussian in figure as a 
result an asymmetric distribution may be a more appropriate choice 
for application to the raw output score of a classifier 
ideally i e perfect classification there will exist scores θ− and 
θ such that all examples with score greater than θ are relevant 
and all examples with scores less than θ− are irrelevant 
furthermore no examples fall between θ− and θ the distance 
 θ− − θ corresponds to the margin in some classifiers and 
an attempt is often made to maximize this quantity because text 
classifiers have training data to use to separate the classes the 
final behavior of the score distributions is primarily a factor of the 
amount of training data and the consequent separation in the classes 
achieved this is in contrast to search engine retrieval where the 
distribution of scores is more a factor of language distribution across 
documents the similarity function and the length and type of query 
perfect classification corresponds to using two very asymmetric 
distributions but in this case the probabilities are actually one and 
zero and many methods will work for typical purposes practically 
some examples will fall between θ− and θ and it is often 
important to estimate the probabilities of these examples well since they 
correspond to the hard examples justifications can be given for 
both why you may find more and less examples between θ− and θ 
than outside of them but there are few empirical reasons to believe 
that the distributions should be symmetric 
a natural first candidate for an asymmetric distribution is to 
generalize a common symmetric distribution e g the laplace or the 
gaussian an asymmetric laplace distribution can be achieved by 
placing two exponentials around the mode in the following manner 
p x θ β γ 
 
 
 
βγ 
β γ 
exp −β θ − x x ≤ θ 
 β γ 
βγ 
β γ 
exp −γ x − θ x θ 
 
where θ β and γ are the model parameters θ is the mode of the 
distribution β is the inverse scale of the exponential to the left of 
the mode and γ is the inverse scale of the exponential to the right 
we will use the notation λ x θ β γ to refer to this distribution 
 
 
 
 
 
 
- - - 
p s class - 
unnormalized confidence score s 
gaussian 
a-gaussian 
figure gaussians vs asymmetric gaussians a 
shortcoming of symmetric distributions - the vertical lines show the 
modes as estimated nonparametrically 
we can create an asymmetric gaussian in the same manner 
p x θ σl σr 
 
 
 
 √ 
 π σl σr 
exp − x−θ 
 σ 
l 
x ≤ θ 
 σl σr 
 √ 
 π σl σr 
exp − x−θ 
 σ 
r 
x θ 
 
where θ σl and σr are the model parameters to refer to this 
asymmetric gaussian we use the notation γ x θ σl σr while 
these distributions are composed of halves the resulting function 
is a single continuous distribution 
these distributions allow us to fit our data with much greater 
flexibility at the cost of only fitting six parameters we could 
instead try mixture models for each component or other extensions 
but most other extensions require at least as many parameters and 
can often be more computationally expensive in addition the 
motivation above should provide significant cause to believe the 
underlying distributions actually behave in this way furthermore 
this family of distributions can still fit a symmetric distribution 
and finally in the empirical evaluation evidence is presented that 
demonstrates this asymmetric behavior see figure 
to our knowledge neither family of distributions has been 
previously used in machine learning or information retrieval both are 
termed generalizations of an asymmetric laplace in but we 
refer to them as described above to reflect the nature of how we 
derived them for this task 
 estimating the parameters of the 
asymmetric distributions 
this section develops the method for finding maximum 
likelihood estimates mle of the parameters for the above asymmetric 
distributions in order to find the mles we have two choices 
use numerical estimation to estimate all three parameters at once 
 fix the value of θ and estimate the other two β and γ or σl 
and σr given our choice of θ then consider alternate values of θ 
because of the simplicity of analysis in the latter alternative we 
choose this method 
 asymmetric laplace mles 
for d x x xn where the xi are i i d and x ∼ 
λ x θ β γ the likelihood is n 
i λ x θ β γ now we fix 
θ and compute the maximum likelihood for that choice of θ then 
we can simply consider all choices of θ and choose the one with 
the maximum likelihood over all choices of θ 
the complete derivation is omitted because of space but is 
available in we define the following values 
nl x ∈ d x ≤ θ nr x ∈ d x θ 
sl 
x∈d x≤θ 
x sr 
x∈d x θ 
x 
dl nlθ − sl dr sr − nrθ 
note that dl and dr are the sum of the absolute differences 
between the x belonging to the left and right halves of the distribution 
 respectively and θ finally the mles for β and γ for a fixed θ are 
βmle 
n 
dl 
√ 
drdl 
γmle 
n 
dr 
√ 
drdl 
 
these estimates are not wholly unexpected since we would obtain 
nl 
dl 
if we were to estimate β independently of γ the elegance of 
the formulae is that the estimates will tend to be symmetric only 
insofar as the data dictate it i e the closer dl and dr are to being 
equal the closer the resulting inverse scales 
by continuity arguments when n we assign β γ 
where is a small constant that acts to disperse the distribution to 
a uniform similarly when n and dl we assign β inf 
where inf is a very large constant that corresponds to an extremely 
sharp distribution i e almost all mass at θ for that half dr 
is handled similarly 
assuming that θ falls in some range φ ψ dependent upon only 
the observed documents then this alternative is also easily 
computable given nl sl nr sr we can compute the posterior and 
the mles in constant time in addition if the scores are sorted 
then we can perform the whole process quite efficiently starting 
with the minimum θ φ we would like to try we loop through the 
scores once and set nl sl nr sr appropriately then we increase 
θ and just step past the scores that have shifted from the right side 
of the distribution to the left assuming the number of candidate 
θs are o n this process is o n and the overall process is 
dominated by sorting the scores o n log n or expected linear time 
 asymmetric gaussian mles 
for d x x xn where the xi are i i d and x ∼ 
γ x θ σl σr the likelihood is n 
i γ x θ β γ the mles 
can be worked out similar to the above 
we assume the same definitions as above the complete 
derivation omitted for space is available in and in addition let 
sl 
x∈d x≤θ 
x 
sr 
x∈d x θ 
x 
dl sl − slθ θ 
nl dr sr − srθ θ 
nr 
the analytical solution for the mles for a fixed θ is 
σl mle 
dl d 
 
l d 
 
r 
n 
 
σr mle 
dr d 
 
r d 
 
l 
n 
 
by continuity arguments when n we assign σr σl 
inf and when n and dl resp dr we 
assign σl resp σr again the same computational 
complexity analysis applies to estimating these parameters 
 experimental analysis 
 methods 
for each of the methods that use a class prior we use a smoothed 
add-one estimate i e p c c 
n 
where n is the number of 
documents for methods that fit the class-conditional densities p s 
and p s − the resulting densities are inverted using bayes rule as 
described above all of the methods below are fit using maximum 
likelihood estimates 
for recalibrating a classifier i e correcting poor probability 
estimates output by the classifier it is usual to use the log-odds of 
the classifier s estimate as s d the log-odds are defined to be 
log p d 
p − d 
 the normal decision threshold minimizing error in 
terms of log-odds is at zero i e p d p − d 
since it scales the outputs to a space −∞ ∞ the log-odds 
make normal and similar distributions applicable lewis 
gale give a more motivating viewpoint that fitting the log-odds 
is a dampening effect for the inaccurate independence assumption 
and a bias correction for inaccurate estimates of the priors in 
general fitting the log-odds can serve to boost or dampen the signal 
from the original classifier as the data dictate 
gaussians 
a gaussian is fit to each of the class-conditional densities using 
the usual maximum likelihood estimates this method is denoted 
in the tables below as gauss 
asymmetric gaussians 
an asymmetric gaussian is fit to each of the class-conditional 
densities using the maximum likelihood estimation procedure 
described above intervals between adjacent scores are divided by 
in testing candidate θs i e points between actual scores 
occurring in the data set are tested this method is denoted as a gauss 
laplace distributions 
even though laplace distributions are not typically applied to 
this task we also tried this method to isolate why benefit is gained 
from the asymmetric form the usual mles were used for 
estimating the location and scale of a classical symmetric laplace 
distribution as described in we denote this method as laplace below 
asymmetric laplace distributions 
an asymmetric laplace is fit to each of the class-conditional 
densities using the maximum likelihood estimation procedure 
described above as with the asymmetric gaussian intervals between 
adjacent scores are divided by in testing candidate θs this 
method is denoted as a laplace below 
logistic regression 
this method is the first of two methods we evaluated that 
directly fit the posterior p s d both methods restrict the set 
of families to a two-parameter sigmoid family they differ 
primarily in their model of class labels as opposed to the above 
methods one can argue that an additional boon of these methods is they 
completely preserve the ranking given by the classifier when this 
is desired these methods may be more appropriate the previous 
methods will mostly preserve the rankings but they can deviate if 
the data dictate it thus they may model the data behavior better at 
the cost of departing from a monotonicity constraint in the output 
of the classifier 
lewis gale use logistic regression to recalibrate na¨ıve 
bayes for subsequent use in active learning the model they use is 
p s d 
exp a b s d 
 exp a b s d 
 
instead of using the probabilities directly output by the classifier 
they use the loglikelihood ratio of the probabilities log p d 
p d − 
 as 
the score s d instead of using this below we will use the 
logodds ratio this does not affect the model as it simply shifts all of 
the scores by a constant determined by the priors we refer to this 
method as logreg below 
logistic regression with noisy class labels 
platt proposes a framework that extends the logistic 
regression model above to incorporate noisy class labels and uses it to 
produce probability estimates from the raw output of an svm 
this model differs from the logreg model only in how the 
parameters are estimated the parameters are still fit using maximum 
likelihood estimation but a model of noisy class labels is used in 
addition to allow for the possibility that the class was mislabeled 
the noise is modeled by assuming there is a finite probability of 
mislabeling a positive example and of mislabeling a negative 
example these two noise estimates are determined by the number 
of positive examples and the number of negative examples using 
bayes rule to infer the probability of incorrect label 
even though the performance of this model would not be 
expected to deviate much from logreg we evaluate it for 
completeness we refer to this method below as lr noise 
 data 
we examined several corpora including the msn web directory 
reuters and trec-ap 
msn web directory 
the msn web directory is a large collection of heterogeneous 
web pages from a may web snapshot that have been 
hierarchically classified we used the same train test split of 
documents as that reported in the msn web hierarchy is a 
seven-level hierarchy we used all of the top-level categories 
the class proportions in the training set vary from to 
in the testing set they range from to the classes 
are general subjects such as health fitness and travel vacation 
human indexers assigned the documents to zero or more categories 
for the experiments below we used only the top words with 
highest mutual information for each class approximately k 
words appear in at least three training documents 
reuters 
the reuters corpus contains reuters news articles 
from for this data set we used the modapte standard train 
test split of documents unused documents the 
classes are economic subjects e g acq for acquisitions earn 
for earnings etc that human taggers applied to the document 
a document may have multiple subjects there are actually 
classes in this domain only of which occur in the training and 
testing set however we only examined the ten most frequent classes 
since small numbers of testing examples make interpreting some 
performance measures difficult due to high variance 
limiting to 
the ten largest classes allows us to compare our results to 
previously published results the class proportions in 
the training set vary from to in the testing set they 
range from to 
for the experiments below we used only the top words with 
highest mutual information for each class approximately k words 
appear in at least three training documents 
trec-ap 
the trec-ap corpus is a collection of ap news stories from 
 to we used the same train test split of 
documents that was used in as described in see also 
 the categories are defined by keywords in a keyword field 
the title and body fields are used in the experiments below there 
are twenty categories in total the class proportions in the training 
set vary from to in the testing set they range from 
 to 
for the experiments described below we use only the top 
words with the highest mutual information for each class 
approximately k words appear in at least training documents 
 classifiers 
we selected two classifiers for evaluation a linear svm 
classifier which is a discriminative classifier that does not normally 
output probability values and a na¨ıve bayes classifier whose 
probability outputs are often poor but can be improved 
 
a separate comparison of only logreg lr noise and 
a laplace over all categories of reuters was also conducted 
after accounting for the variance that evaluation also supported 
the claims made here 
svm 
for linear svms we use the smox toolkit which is based on 
platt s sequential minimal optimization algorithm the features 
were represented as continuous values we used the raw output 
score of the svm as s d since it has been shown to be appropriate 
before the normal decision threshold assuming we are 
seeking to minimize errors for this classifier is at zero 
na¨ıve bayes 
the na¨ıve bayes classifier model is a multinomial model 
we smoothed word and class probabilities using a bayesian 
estimate with the word prior and a laplace m-estimate respectively 
we use the log-odds estimated by the classifier as s d the normal 
decision threshold is at zero 
 performance measures 
we use log-loss and squared error to evaluate the 
quality of the probability estimates for a document d with class c d ∈ 
 − i e the data have known labels and not probabilities 
logloss is defined as δ c d log p d δ c d − log p − d 
where δ a b 
 
 if a b and otherwise the squared error is 
δ c d − p d 
 δ c d − − p − d 
 when the 
class of a document is correctly predicted with a probability of one 
log-loss is zero and squared error is zero when the class of a 
document is incorrectly predicted with a probability of one log-loss 
is −∞ and squared error is one thus both measures assess how 
close an estimate comes to correctly predicting the item s class but 
vary in how harshly incorrect predictions are penalized 
we report only the sum of these measures and omit the averages 
for space their averages average log-loss and mean squared 
error mse can be computed from these totals by dividing by the 
number of binary decisions in a corpus 
in addition we also compare the error of the classifiers at their 
default thresholds and with the probabilities this evaluates how 
the probability estimates have improved with respect to the 
decision threshold p d thus error only indicates how the 
methods would perform if a false positive was penalized the same 
as a false negative and not the general quality of the probability 
estimates it is presented simply to provide the reader with a more 
complete understanding of the empirical tendencies of the methods 
we use a a standard paired micro sign test to determine 
statistical significance in the difference of all measures only pairs 
that the methods disagree on are used in the sign test this test 
compares pairs of scores from two systems with the null 
hypothesis that the number of items they disagree on are binomially 
distributed we use a significance level of p 
 experimental methodology 
as the categories under consideration in the experiments are not 
mutually exclusive the classification was done by training n binary 
classifiers where n is the number of classes 
in order to generate the scores that each method uses to fit its 
probability estimates we use five-fold cross-validation on the 
training data we note that even though it is computationally efficient 
to perform leave-one-out cross-validation for the na¨ıve bayes 
classifier this may not be desirable since the distribution of scores can 
be skewed as a result of course as with any application of n-fold 
cross-validation it is also possible to bias the results by holding n 
too low and underestimating the performance of the final classifier 
 results discussion 
the results for recalibrating na¨ıve bayes are given in table a 
table b gives results for producing probabilistic outputs for svms 
log-loss error 
errors 
msn web 
gauss - 
a gauss - 
laplace - 
a laplace - † 
 
logreg - 
lr noise - 
na¨ıve bayes - 
reuters 
gauss - 
a gauss - 
laplace - 
a laplace - ‡ 
 ‡ 
 
logreg - 
lr noise - 
na¨ıve bayes - 
trec-ap 
gauss - 
a gauss - 
laplace - 
a laplace - ‡ 
 
logreg - 
lr noise - 
na¨ıve bayes - 
log-loss error 
errors 
msn web 
gauss - 
a gauss - 
laplace - 
a laplace - 
logreg - 
lr noise - 
linear svm n a n a 
reuters 
gauss - 
a gauss - 
laplace - 
a laplace - 
logreg - 
lr noise - 
linear svm n a n a 
trec-ap 
gauss - 
a gauss - 
laplace - 
a laplace - ‡ 
 ‡ 
logreg - 
lr noise - 
linear svm n a n a 
table a results for na¨ıve bayes left and b svm right the best entry for a corpus is in bold entries that are statistically 
significantly better than all other entries are underlined a † denotes the method is significantly better than all other methods except 
for na¨ıve bayes a ‡ denotes the entry is significantly better than all other methods except for a gauss and na¨ıve bayes for the table 
on the left the reason for this distinction in significance tests is described in the text 
we start with general observations that result from examining 
the performance of these methods over the various corpora the 
first is that a laplace lr noise and logreg quite clearly 
outperform the other methods there is usually little difference 
between the performance of lr noise and logreg both as shown 
here and on a decision by decision basis but this is unsurprising 
since lr noise just adds noisy class labels to the logreg model 
with respect to the three different measures lr noise and 
logreg tend to perform slightly better but never significantly than 
a laplace at some tasks with respect to log-loss and squared error 
however a laplace always produces the least number of errors 
for all of the tasks though at times the degree of improvement is 
not significant 
in order to give the reader a better sense of the behavior of these 
methods figures - show the fits produced by the most 
competitive of these methods versus the actual data behavior as estimated 
nonparametrically by binning for class earn in reuters figure 
shows the class-conditional densities and thus only a laplace is 
shown since logreg fits the posterior directly figure shows the 
estimations of the log-odds i e log p earn s d 
p ¬earn s d 
 viewing the 
log-odds rather than the posterior usually enables errors in 
estimation to be detected by the eye more easily 
we can break things down as the sign test does and just look at 
wins and losses on the items that the methods disagree on looked 
at in this way only two methods na¨ıve bayes and a gauss ever 
have more pairwise wins than a laplace those two sometimes 
have more pairwise wins on log-loss and squared error even though 
the total never wins i e they are dragged down by heavy penalties 
in addition this comparison of pairwise wins means that for 
those cases where logreg and lr noise have better scores than 
a laplace it would not be deemed significant by the sign test at 
any level since they do not have more wins for example of the 
 k binary decisions over the msn web dataset a laplace had 
approximately k pairwise wins versus logreg and lr noise 
no method ever has more pairwise wins than a laplace for the 
error comparison nor does any method every achieve a better total 
the basic observation made about na¨ıve bayes in previous work 
is that it tends to produce estimates very close to zero and one 
 this means if it tends to be right enough of the time it will 
produce results that do not appear significant in a sign test that 
ignores size of difference as the one here the totals of the squared 
error and log-loss bear out the previous observation that when it s 
wrong it s really wrong 
there are several interesting points about the performance of the 
asymmetric distributions as well first a gauss performs poorly 
because similar to na¨ıve bayes there are some examples where 
it is penalized a large amount this behavior results from a 
general tendency to perform like the picture shown in figure note 
the crossover at the tails while the asymmetric gaussian tends 
to place the mode much more accurately than a symmetric 
gaussian its asymmetric flexibility combined with its distance function 
causes it to distribute too much mass to the outside tails while 
failing to fit around the mode accurately enough to compensate figure 
 is actually a result of fitting the two distributions to real data as 
a result at the tails there can be a large discrepancy between the 
likelihood of belonging to each class thus when there are no 
outliers a gauss can perform quite competitively but when there is an 
 
 
 
 
 
 
 
- - - 
p s d class - 
s d naive bayes log-odds 
train 
test 
a laplace 
 
 
 
 
 
 
 
 
 
 
- - - 
p s d class - 
s d linear svm raw score 
train 
test 
a laplace 
figure the empirical distribution of classifier scores for documents in the training and the test set for class earn in reuters 
also shown is the fit of the asymmetric laplace distribution to the training score distribution the positive class i e earn is the 
distribution on the right in each graph and the negative class i e ¬earn is that on the left in each graph 
- 
- 
- 
 
 
 
 
 
- - - - - 
logodds logp s d -logp - s d 
s d naive bayes log-odds 
train 
test 
a laplace 
logreg 
- 
 
 
 
 
- - 
logodds logp s d -logp - s d 
s d linear svm raw score 
train 
test 
a laplace 
logreg 
figure the fit produced by various methods compared to the empirical log-odds of the training data for class earn in reuters 
outlier a gauss is penalized quite heavily there are enough such 
cases overall that it seems clearly inferior to the top three methods 
however the asymmetric laplace places much more emphasis 
around the mode figure because of the different distance 
function think of the sharp peak of an exponential as a result most 
of the mass stays centered around the mode while the asymmetric 
parameters still allow more flexibility than the standard laplace 
since the standard laplace also corresponds to a piecewise fit in the 
log-odds space this highlights that part of the power of the 
asymmetric methods is their sensitivity in placing the knots at the actual 
modes - rather than the symmetric assumption that the means 
correspond to the modes additionally the asymmetric methods have 
greater flexibility in fitting the slopes of the line segments as well 
even in cases where the test distribution differs from the training 
distribution figure a laplace still yields a solution that gives 
a better fit than logreg figure the next best competitor 
finally we can make a few observations about the usefulness 
of the various performance metrics first log-loss only awards a 
finite amount of credit as the degree to which something is 
correct improves i e there are diminishing returns as it approaches 
zero but it can infinitely penalize for a wrong estimate thus it 
is possible for one outlier to skew the totals but misclassifying this 
example may not matter for any but a handful of actual utility 
functions used in practice secondly squared error has a weakness in 
the other direction that is its penalty and reward are bounded in 
 but if the number of errors is small enough it is possible for 
a method to appear better when it is producing what we generally 
consider unhelpful probability estimates for example consider a 
method that only estimates probabilities as zero or one which na¨ıve 
bayes tends to but doesn t quite reach if you use smoothing this 
method could win according to squared error but with just one 
error it would never perform better on log-loss than any method that 
assigns some non-zero probability to each outcome for these 
reasons we recommend that neither of these are used in isolation as 
they each give slightly different insights to the quality of the 
estimates produced these observations are straightforward from the 
definitions but are underscored by the evaluation 
 future work 
a promising extension to the work presented here is a hybrid 
distribution of a gaussian on the outside slopes and exponentials 
 on the inner slopes from the empirical evidence presented in 
 the expectation is that such a distribution might allow more 
emphasis of the probability mass around the modes as with the 
exponential while still providing more accurate estimates toward 
the tails 
just as logistic regression allows the log-odds of the posterior 
distribution to be fit directly with a line we could directly fit the 
log-odds of the posterior with a three-piece line a spline instead of 
indirectly doing the same thing by fitting the asymmetric laplace 
this approach may provide more power since it retains the 
asymmetry assumption but not the assumption that the class-conditional 
densities are from an asymmetric laplace 
finally extending these methods to the outputs of other 
discriminative classifiers is an open area we are currently evaluating the 
appropriateness of these methods for the output of a voted 
perceptron by analogy to the log-odds the operative score that 
appears promising is log 
weight perceptrons voting 
weight perceptrons voting − 
 
 summary and conclusions 
we have reviewed a wide variety of parametric methods for 
producing probability estimates from the raw scores of a discriminative 
classifier and for recalibrating an uncalibrated probabilistic 
classifier in addition we have introduced two new families that attempt 
to capitalize on the asymmetric behavior that tends to arise from 
learning a discrimination function we have given an efficient way 
to estimate the parameters of these distributions 
while these distributions attempt to strike a balance between the 
generalization power of parametric distributions and the flexibility 
that the added asymmetric parameters give the asymmetric 
gaussian appears to have too great of an emphasis away from the modes 
in striking contrast the asymmetric laplace distribution appears to 
be preferable over several large text domains and a variety of 
performance measures to the primary competing parametric methods 
though comparable performance is sometimes achieved with one 
of two varieties of logistic regression given the ease of 
estimating the parameters of this distribution it is a good first choice for 
producing quality probability estimates 
acknowledgments 
we are grateful to francisco pereira for the sign test code anton 
likhodedov for logistic regression code and john platt for the code 
support for the linear svm classifier toolkit smox also we 
sincerely thank chris meek and john platt for the very useful advice 
provided in the early stages of this work thanks also to jaime 
carbonell and john lafferty for their useful feedback on the final 
versions of this paper 
 references 
 p n bennett assessing the calibration of naive bayes 
posterior estimates technical report cmu-cs- - 
carnegie mellon school of computer science 
 p n bennett using asymmetric distributions to improve 
classifier probabilities a comparison of new and standard 
parametric methods technical report cmu-cs- - 
carnegie mellon school of computer science 
 h bourlard and n morgan a continuous speech 
recognition system embedding mlp into hmm in nips 
 
 g brier verification of forecasts expressed in terms of 
probability monthly weather review - 
 m h degroot and s e fienberg the comparison and 
evaluation of forecasters statistician - 
 m h degroot and s e fienberg comparing probability 
forecasters basic binary concepts and multivariate 
extensions in p goel and a zellner editors bayesian 
inference and decision techniques elsevier science 
publishers b v 
 p domingos and m pazzani beyond independence 
conditions for the optimality of the simple bayesian 
classifier in icml 
 r duda p hart and d stork pattern classification john 
wiley sons inc 
 s t dumais and h chen hierarchical classification of web 
content in sigir 
 s t dumais j platt d heckerman and m sahami 
inductive learning algorithms and representations for text 
categorization in cikm 
 y freund and r schapire large margin classification using 
the perceptron algorithm machine learning - 
 
 i good rational decisions journal of the royal statistical 
society series b 
 t joachims text categorization with support vector 
machines learning with many relevant features in ecml 
 
 s kotz t j kozubowski and k podgorski the laplace 
distribution and generalizations a revisit with 
applications to communications economics engineering 
and finance birkh¨auser 
 d d lewis a sequential algorithm for training text 
classifiers corrigendum and additional data sigir forum 
 - fall 
 d d lewis reuters- distribution 
http www daviddlewis com resources 
testcollections reuters january 
 d d lewis and w a gale a sequential algorithm for 
training text classifiers in sigir 
 d d lewis r e schapire j p callan and r papka 
training algorithms for linear text classifiers in sigir 
 
 d lindley a tversky and r brown on the reconciliation 
of probability assessments journal of the royal statistical 
society 
 r manmatha t rath and f feng modeling score 
distributions for combining the outputs of search engines in 
sigir 
 a mccallum and k nigam a comparison of event models 
for naive bayes text classification in aaai workshop on 
learning for text categorization 
 j c platt probabilistic outputs for support vector machines 
and comparisons to regularized likelihood methods in a j 
smola p bartlett b scholkopf and d schuurmans editors 
advances in large margin classifiers mit press 
 m saar-tsechansky and f provost active learning for class 
probability estimation and ranking in ijcai 
 r l winkler scoring rules and the evaluation of probability 
assessors journal of the american statistical association 
 
 y yang and x liu a re-examination of text categorization 
methods in sigir 
 b zadrozny and c elkan obtaining calibrated probability 
estimates from decision trees and naive bayesian classifiers 
in icml 
 b zadrozny and c elkan reducing multiclass to binary by 
coupling probability estimates in kdd 
