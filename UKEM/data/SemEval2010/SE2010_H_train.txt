context-sensitive information retrieval using 
implicit feedback 
xuehua shen 
department of computer 
science 
university of illinois at 
urbana-champaign 
bin tan 
department of computer 
science 
university of illinois at 
urbana-champaign 
chengxiang zhai 
department of computer 
science 
university of illinois at 
urbana-champaign 
abstract 
a major limitation of most existing retrieval models and systems 
is that the retrieval decision is made based solely on the query and 
document collection information about the actual user and search 
context is largely ignored in this paper we study how to 
exploit implicit feedback information including previous queries and 
clickthrough information to improve retrieval accuracy in an 
interactive information retrieval setting we propose several 
contextsensitive retrieval algorithms based on statistical language models 
to combine the preceding queries and clicked document summaries 
with the current query for better ranking of documents we use 
the trec ap data to create a test collection with search context 
information and quantitatively evaluate our models using this test 
set experiment results show that using implicit feedback 
especially the clicked document summaries can improve retrieval 
performance substantially 
categories and subject descriptors 
h information search and retrieval retrieval models 
general terms 
algorithms 
 introduction 
in most existing information retrieval models the retrieval 
problem is treated as involving one single query and a set of documents 
from a single query however the retrieval system can only have 
very limited clue about the user s information need an optimal 
retrieval system thus should try to exploit as much additional context 
information as possible to improve retrieval accuracy whenever it 
is available indeed context-sensitive retrieval has been identified 
as a major challenge in information retrieval research 
there are many kinds of context that we can exploit relevance 
feedback can be considered as a way for a user to provide 
more context of search and is known to be effective for 
improving retrieval accuracy however relevance feedback requires that 
a user explicitly provides feedback information such as specifying 
the category of the information need or marking a subset of 
retrieved documents as relevant documents since it forces the user 
to engage additional activities while the benefits are not always 
obvious to the user a user is often reluctant to provide such feedback 
information thus the effectiveness of relevance feedback may be 
limited in real applications 
for this reason implicit feedback has attracted much attention 
recently in general the retrieval results using the 
user s initial query may not be satisfactory often the user would 
need to revise the query to improve the retrieval ranking accuracy 
 for a complex or difficult information need the user may need 
to modify his her query and view ranked documents with many 
iterations before the information need is completely satisfied in such 
an interactive retrieval scenario the information naturally available 
to the retrieval system is more than just the current user query and 
the document collection - in general all the interaction history can 
be available to the retrieval system including past queries 
information about which documents the user has chosen to view and even 
how a user has read a document e g which part of a document the 
user spends a lot of time in reading we define implicit feedback 
broadly as exploiting all such naturally available interaction history 
to improve retrieval results 
a major advantage of implicit feedback is that we can improve 
the retrieval accuracy without requiring any user effort for 
example if the current query is java without knowing any extra 
information it would be impossible to know whether it is intended 
to mean the java programming language or the java island in 
indonesia as a result the retrieved documents will likely have both 
kinds of documents - some may be about the programming 
language and some may be about the island however any particular 
user is unlikely searching for both types of documents such an 
ambiguity can be resolved by exploiting history information for 
example if we know that the previous query from the user is cgi 
programming it would strongly suggest that it is the programming 
language that the user is searching for 
implicit feedback was studied in several previous works in 
joachims explored how to capture and exploit the clickthrough 
information and demonstrated that such implicit feedback 
information can indeed improve the search accuracy for a group of 
people in a simulation study of the effectiveness of different 
implicit feedback algorithms was conducted and several retrieval 
models designed for exploiting clickthrough information were 
proposed and evaluated in some existing retrieval algorithms are 
adapted to improve search results based on the browsing history of 
a user other related work on using context includes personalized 
search query log analysis context factors 
and implicit queries 
while the previous work has mostly focused on using 
clickthrough information in this paper we use both clickthrough 
information and preceding queries and focus on developing new 
context-sensitive language models for retrieval specifically we 
develop models for using implicit feedback information such as 
query and clickthrough history of the current search session to 
improve retrieval accuracy we use the kl-divergence retrieval model 
 as the basis and propose to treat context-sensitive retrieval as 
estimating a query language model based on the current query and 
any search context information we propose several statistical 
language models to incorporate query and clickthrough history into 
the kl-divergence model 
one challenge in studying implicit feedback models is that there 
does not exist any suitable test collection for evaluation we thus 
use the trec ap data to create a test collection with implicit 
feedback information which can be used to quantitatively evaluate 
implicit feedback models to the best of our knowledge this is the 
first test set for implicit feedback we evaluate the proposed 
models using this data set the experimental results show that using 
implicit feedback information especially the clickthrough data can 
substantially improve retrieval performance without requiring 
additional effort from the user 
the remaining sections are organized as follows in section 
we attempt to define the problem of implicit feedback and introduce 
some terms that we will use later in section we propose several 
implicit feedback models based on statistical language models in 
section we describe how we create the data set for implicit 
feedback experiments in section we evaluate different implicit 
feedback models on the created data set section is our conclusions 
and future work 
 problem definition 
there are two kinds of context information we can use for 
implicit feedback one is short-term context which is the immediate 
surrounding information which throws light on a user s current 
information need in a single session a session can be considered as a 
period consisting of all interactions for the same information need 
the category of a user s information need e g kids or sports 
previous queries and recently viewed documents are all examples of 
short-term context such information is most directly related to the 
current information need of the user and thus can be expected to be 
most useful for improving the current search in general short-term 
context is most useful for improving search in the current session 
but may not be so helpful for search activities in a different 
session the other kind of context is long-term context which refers 
to information such as a user s education level and general interest 
accumulated user query history and past user clickthrough 
information such information is generally stable for a long time and is 
often accumulated over time long-term context can be applicable 
to all sessions but may not be as effective as the short-term context 
in improving search accuracy for a particular session in this paper 
we focus on the short-term context though some of our methods 
can also be used to naturally incorporate some long-term context 
in a single search session a user may interact with the search 
system several times during interactions the user would 
continuously modify the query therefore for the current query qk 
 except for the first query of a search session there is a query history 
hq q qk− associated with it which consists of the 
preceding queries given by the same user in the current session note 
that we assume that the session boundaries are known in this paper 
in practice we need techniques to automatically discover session 
boundaries which have been studied in traditionally the 
retrieval system only uses the current query qk to do retrieval but 
the short-term query history clearly may provide useful clues about 
the user s current information need as seen in the java example 
given in the previous section indeed our previous work has 
shown that the short-term query history is useful for improving 
retrieval accuracy 
in addition to the query history there may be other short-term 
context information available for example a user would 
presumably frequently click some documents to view we refer to data 
associated with these actions as clickthrough history the 
clickthrough data may include the title summary and perhaps also the 
content and location e g the url of the clicked document 
although it is not clear whether a viewed document is actually 
relevant to the user s information need we may safely assume that 
the displayed summary title information about the document is 
attractive to the user thus conveys information about the user s 
information need suppose we concatenate all the displayed text 
information about a document usually title and summary together we 
will also have a clicked summary ci in each round of retrieval in 
general we may have a history of clicked summaries c ck− 
we will also exploit such clickthrough history hc c ck− 
to improve our search accuracy for the current query qk previous 
work has also shown positive results using similar clickthrough 
information 
both query history and clickthrough history are implicit 
feedback information which naturally exists in interactive information 
retrieval thus no additional user effort is needed to collect them in 
this paper we study how to exploit such information hq and hc 
develop models to incorporate the query history and clickthrough 
history into a retrieval ranking function and quantitatively evaluate 
these models 
 language models for 
contextsensitiveinformationretrieval 
intuitively the query history hq and clickthrough history hc 
are both useful for improving search accuracy for the current query 
qk an important research question is how we can exploit such 
information effectively we propose to use statistical language 
models to model a user s information need and develop four specific 
context-sensitive language models to incorporate context 
information into a basic retrieval model 
 basic retrieval model 
we use the kullback-leibler kl divergence method as 
our basic retrieval method according to this model the retrieval 
task involves computing a query language model θq for a given 
query and a document language model θd for a document and then 
computing their kl divergence d θq θd which serves as the 
score of the document 
one advantage of this approach is that we can naturally 
incorporate the search context as additional evidence to improve our 
estimate of the query language model 
formally let hq q qk− be the query history and 
the current query be qk let hc c ck− be the 
clickthrough history note that ci is the concatenation of all clicked 
documents summaries in the i-th round of retrieval since we may 
reasonably treat all these summaries equally our task is to estimate 
a context query model which we denote by p w θk based on the 
current query qk as well as the query history hq and clickthrough 
history hc we now describe several different language models for 
exploiting hq and hc to estimate p w θk we will use c w x 
to denote the count of word w in text x which could be either a 
query or a clicked document s summary or any other text we will 
use x to denote the length of text x or the total number of words 
in x 
 fixed coefficient interpolation fixint 
our first idea is to summarize the query history hq with a 
unigram language model p w hq and the clickthrough history hc 
with another unigram language model p w hc then we linearly 
interpolate these two history models to obtain the history model 
p w h finally we interpolate the history model p w h with 
the current query model p w qk these models are defined as 
follows 
p w qi 
c w qi 
 qi 
p w hq 
 
k − 
i k− 
i 
p w qi 
p w ci 
c w ci 
 ci 
p w hc 
 
k − 
i k− 
i 
p w ci 
p w h βp w hc − β p w hq 
p w θk αp w qk − α p w h 
where β ∈ is a parameter to control the weight on each 
history model and where α ∈ is a parameter to control the 
weight on the current query and the history information 
if we combine these equations we see that 
p w θk αp w qk − α βp w hc − β p w hq 
that is the estimated context query model is just a fixed coefficient 
interpolation of three models p w qk p w hq and p w hc 
 bayesian interpolation bayesint 
one possible problem with the fixint approach is that the 
coefficients especially α are fixed across all the queries but intuitively 
if our current query qk is very long we should trust the current 
query more whereas if qk has just one word it may be beneficial 
to put more weight on the history to capture this intuition we treat 
p w hq and p w hc as dirichlet priors and qk as the observed 
data to estimate a context query model using bayesian estimator 
the estimated model is given by 
p w θk 
c w qk µp w hq νp w hc 
 qk µ ν 
 
 qk 
 qk µ ν 
p w qk 
µ ν 
 qk µ ν 
 
µ 
µ ν 
p w hq 
ν 
µ ν 
p w hc 
where µ is the prior sample size for p w hq and ν is the prior 
sample size for p w hc we see that the only difference between 
bayesint and fixint is the interpolation coefficients are now 
adaptive to the query length indeed when viewing bayesint as fixint 
we see that α qk 
 qk µ ν 
 β ν 
ν µ 
 thus with fixed µ and ν 
we will have a query-dependent α later we will show that such an 
adaptive α empirically performs better than a fixed α 
 online bayesian updating onlineup 
both fixint and bayesint summarize the history information by 
averaging the unigram language models estimated based on 
previous queries or clicked summaries this means that all previous 
queries are treated equally and so are all clicked summaries 
however as the user interacts with the system and acquires more 
knowledge about the information in the collection presumably the 
reformulated queries will become better and better thus assigning 
decaying weights to the previous queries so as to trust a recent query 
more than an earlier query appears to be reasonable interestingly 
if we incrementally update our belief about the user s information 
need after seeing each query we could naturally obtain decaying 
weights on the previous queries since such an incremental online 
updating strategy can be used to exploit any evidence in an 
interactive retrieval system we present it in a more general way 
in a typical retrieval system the retrieval system responds to 
every new query entered by the user by presenting a ranked list 
of documents in order to rank documents the system must have 
some model for the user s information need in the kl divergence 
retrieval model this means that the system must compute a query 
model whenever a user enters a new query a principled way of 
updating the query model is to use bayesian estimation which we 
discuss below 
 bayesian updating 
we first discuss how we apply bayesian estimation to update a 
query model in general let p w φ be our current query model 
and t be a new piece of text evidence observed e g t can be a 
query or a clicked summary to update the query model based on 
t we use φ to define a dirichlet prior parameterized as 
dir µt p w φ µt p wn φ 
where µt is the equivalent sample size of the prior we use 
dirichlet prior because it is a conjugate prior for multinomial 
distributions with such a conjugate prior the predictive distribution of φ 
 or equivalently the mean of the posterior distribution of φ is given 
by 
p w φ 
c w t µt p w φ 
 t µt 
 
where c w t is the count of w in t and t is the length of t 
parameter µt indicates our confidence in the prior expressed in 
terms of an equivalent text sample comparable with t for 
example µt indicates that the influence of the prior is equivalent to 
adding one extra word to t 
 sequential query model updating 
we now discuss how we can update our query model over time 
during an interactive retrieval process using bayesian estimation 
in general we assume that the retrieval system maintains a current 
query model φi at any moment as soon as we obtain some implicit 
feedback evidence in the form of a piece of text ti we will update 
the query model 
initially before we see any user query we may already have 
some information about the user for example we may have some 
information about what documents the user has viewed in the past 
we use such information to define a prior on the query model 
which is denoted by φ after we observe the first query q we 
can update the query model based on the new observed data q 
the updated query model φ can then be used for ranking 
documents in response to q as the user views some documents the 
displayed summary text for such documents c i e clicked 
summaries can serve as some new data for us to further update the 
query model to obtain φ as we obtain the second query q from 
the user we can update φ to obtain a new model φ in general 
we may repeat such an updating process to iteratively update the 
query model 
clearly we see two types of updating updating based on a 
new query qi updating based on a new clicked summary ci in 
both cases we can treat the current model as a prior of the context 
query model and treat the new observed query or clicked summary 
as observed data thus we have the following updating equations 
p w φi 
c w qi µip w φi− 
 qi µi 
p w φi 
c w ci νip w φi 
 ci νi 
where µi is the equivalent sample size for the prior when updating 
the model based on a query while νi is the equivalent sample size 
for the prior when updating the model based on a clicked summary 
if we set µi or νi we essentially ignore the prior model 
thus would start a completely new query model based on the query 
qi or the clicked summary ci on the other hand if we set µi 
 ∞ or νi ∞ we essentially ignore the observed query or 
the clicked summary and do not update our model thus the model 
remains the same as if we do not observe any new text evidence in 
general the parameters µi and νi may have different values for 
different i for example at the very beginning we may have very 
sparse query history thus we could use a smaller µi but later as the 
query history is richer we can consider using a larger µi but in 
our experiments unless otherwise stated we set them to the same 
constants i e ∀i j µi µj νi νj 
note that we can take either p w φi or p w φi as our context 
query model for ranking documents this suggests that we do not 
have to wait until a user enters a new query to initiate a new round 
of retrieval instead as soon as we collect clicked summary ci we 
can update the query model and use p w φi to immediately rerank 
any documents that a user has not yet seen 
to score documents after seeing query qk we use p w φk i e 
p w θk p w φk 
 batch bayesian updating batchup 
if we set the equivalent sample size parameters to fixed 
constant the onlineup algorithm would introduce a decaying factor 
- repeated interpolation would cause the early data to have a low 
weight this may be appropriate for the query history as it is 
reasonable to believe that the user becomes better and better at query 
formulation as time goes on but it is not necessarily appropriate for 
the clickthrough information especially because we use the 
displayed summary rather than the actual content of a clicked 
document one way to avoid applying a decaying interpolation to 
the clickthrough data is to do onlineup only for the query history 
q q qi− but not for the clickthrough data c we first 
buffer all the clickthrough data together and use the whole chunk 
of clickthrough data to update the model generated through 
running onlineup on previous queries the updating equations are as 
follows 
p w φi 
c w qi µip w φi− 
 qi µi 
p w ψi 
i− 
j c w cj νip w φi 
i− 
j cj νi 
where µi has the same interpretation as in onlineup but νi now 
indicates to what extent we want to trust the clicked summaries as 
in onlineup we set all µi s and νi s to the same value and to rank 
documents after seeing the current query qk we use 
p w θk p w ψk 
 data collection 
in order to quantitatively evaluate our models we need a data set 
which includes not only a text database and testing topics but also 
query history and clickthrough history for each topic since there 
is no such data set available to us we have to create one there 
are two choices one is to extract topics and any associated query 
history and clickthrough history for each topic from the log of a 
retrieval system e g search engine but the problem is that we 
have no relevance judgments on such data the other choice is to 
use a trec data set which has a text database topic description 
and relevance judgment file unfortunately there are no query 
history and clickthrough history data we decide to augment a trec 
data set by collecting query history and clickthrough history data 
we select trec ap ap and ap data as our text database 
because ap data has been used in several trec tasks and has 
relatively complete judgments there are altogether news 
articles and the average document length is words most articles 
have titles if not we select the first sentence of the text as the 
title for the preprocessing we only do case folding and do not do 
stopword removal or stemming 
we select relatively difficult topics from trec topics - 
these topics have the worst average precision performance among 
trec topics - according to some baseline experiments using 
the kl-divergence model with bayesian prior smoothing the 
reason why we select difficult topics is that the user then would 
have to have several interactions with the retrieval system in order 
to get satisfactory results so that we can expect to collect a 
relatively richer query history and clickthrough history data from the 
user in real applications we may also expect our models to be 
most useful for such difficult topics so our data collection strategy 
reflects the real world applications well 
we index the trec ap data set and set up a search engine and 
web interface for trec ap news articles we use subjects to do 
experiments to collect query history and clickthrough history data 
each subject is assigned topics and given the topic descriptions 
provided by trec for each topic the first query is the title of 
the topic given in the original trec topic description after the 
subject submits the query the search engine will do retrieval and 
return a ranked list of search results to the subject the subject will 
browse the results and maybe click one or more results to browse 
the full text of article s the subject may also modify the query to 
do another search for each topic the subject composes at least 
queries in our experiment only the first queries for each topic 
are used the user needs to select the topic number from a 
selection menu before submitting the query to the search engine so that 
we can easily detect the session boundary which is not the focus of 
our study we use a relational database to store user interactions 
including the submitted queries and clicked documents for each 
query we store the query terms and the associated result pages 
and for each clicked document we store the summary as shown 
on the search result page the summary of the article is query 
dependent and is computed online using fixed-length passage retrieval 
 kl divergence model with bayesian prior smoothing 
among for each of topics queries which we study in 
the experiment the average query length is words altogether 
there are documents clicked to view so on average there are 
around clicks per topic the average length of clicked summary 
fixint bayesint onlineup batchup 
query α β µ ν µ ν µ ν 
map pr  docs map pr  docs map pr  docs map pr  docs 
q 
q 
q hq hc 
improve - - - - - 
q 
q hq hc 
improve 
q 
q hq hc 
improve 
table effect of using query history and clickthrough data for document ranking 
is words among clicked documents documents are 
judged relevant according to trec judgment file this data set is 
publicly available 
 
 experiments 
 experiment design 
our major hypothesis is that using search context i e query 
history and clickthrough information can help improve search 
accuracy in particular the search context can provide extra information 
to help us estimate a better query model than using just the current 
query so most of our experiments involve comparing the retrieval 
performance using the current query only thus ignoring any 
context with that using the current query as well as the search context 
since we collected four versions of queries for each topic we 
make such comparisons for each version of queries we use two 
performance measures mean average precision map this 
is the standard non-interpolated average precision and serves as a 
good measure of the overall ranking accuracy precision at 
documents pr  docs this measure does not average well but 
it is more meaningful than map and reflects the utility for users 
who only read the top documents in all cases the reported 
figure is the average over all of the topics 
we evaluate the four models for exploiting search context i e 
fixint bayesint onlineup and batchup each model has 
precisely two parameters α and β for fixint µ and ν for others 
note that µ and ν may need to be interpreted differently for 
different methods we vary these parameters and identify the optimal 
performance for each method we also vary the parameters to study 
the sensitivity of our algorithms to the setting of the parameters 
 result analysis 
 overall effect of search context 
we compare the optimal performances of four models with those 
using the current query only in table a row labeled with qi is 
the baseline performance and a row labeled with qi hq hc 
is the performance of using search context we can make several 
observations from this table 
 comparing the baseline performances indicates that on average 
reformulated queries are better than the previous queries with the 
performance of q being the best users generally formulate better 
and better queries 
 using search context generally has positive effect especially 
when the context is rich this can be seen from the fact that the 
 
http sifaka cs uiuc edu ir ucair qchistory zip 
improvement for q and q is generally more substantial compared 
with q actually in many cases with q using the context may 
hurt the performance probably because the history at that point is 
sparse when the search context is rich the performance 
improvement can be quite substantial for example batchup achieves 
 improvement in the mean average precision over q and 
 improvement over q the generally low precisions also 
make the relative improvement deceptively high though 
 among the four models using search context the performances 
of fixint and onlineup are clearly worse than those of bayesint 
and batchup since bayesint performs better than fixint and the 
main difference between bayesint and fixint is that the former uses 
an adaptive coefficient for interpolation the results suggest that 
using adaptive coefficient is quite beneficial and a bayesian style 
interpolation makes sense the main difference between onlineup 
and batchup is that onlineup uses decaying coefficients to 
combine the multiple clicked summaries while batchup simply 
concatenates all clicked summaries therefore the fact that batchup 
is consistently better than onlineup indicates that the weights for 
combining the clicked summaries indeed should not be decaying 
while onlineup is theoretically appealing its performance is 
inferior to bayesint and batchup likely because of the decaying 
coefficient overall batchup appears to be the best method when we 
vary the parameter settings 
we have two different kinds of search context - query history 
and clickthrough data we now look into the contribution of each 
kind of context 
 using query history only 
in each of four models we can turn off the clickthrough 
history data by setting parameters appropriately this allows us to 
evaluate the effect of using query history alone we use the same 
parameter setting for query history as in table the results are 
shown in table here we see that in general the benefit of using 
query history is very limited with mixed results this is different 
from what is reported in a previous study where using query 
history is consistently helpful another observation is that the 
context runs perform poorly at q but generally perform slightly 
better than the baselines for q and q this is again likely because 
at the beginning the initial query which is the title in the original 
trec topic description may not be a good query indeed on 
average performances of these first-generation queries are clearly 
poorer than those of all other user-formulated queries in the later 
generations yet another observation is that when using query 
history only the bayesint model appears to be better than other 
models since the clickthrough data is ignored onlineup and batchup 
fixint bayesint onlineup batchup 
query α β µ ν µ ν ∞ µ ν ∞ 
map pr  docs map pr  docs map pr  docs map pr  docs 
q 
q hq 
improve - - - - - - - 
q 
q hq 
improve - - - - 
q 
q hq 
improve - - - - - 
table effect of using query history only for document ranking 
µ 
q hq map 
q hq map 
q hq map 
table average precision of batchup using query history only 
are essentially the same algorithm the displayed results thus 
reflect the variation caused by parameter µ a smaller setting of 
is seen better than a larger value of a more complete picture 
of the influence of the setting of µ can be seen from table where 
we show the performance figures for a wider range of values of µ 
the value of µ can be interpreted as how many words we regard 
the query history is worth a larger value thus puts more weight 
on the history and is seen to hurt the performance more when the 
history information is not rich thus while for q the best 
performance tends to be achieved for µ ∈ only when µ we 
see some small benefit for q as we would expect an excessively 
large µ would hurt the performance in general but q is hurt most 
and q is barely hurt indicating that as we accumulate more and 
more query history information we can put more and more weight 
on the history information this also suggests that a better strategy 
should probably dynamically adjust parameters according to how 
much history information we have 
the mixed query history results suggest that the positive effect 
of using implicit feedback information may have largely come from 
the use of clickthrough history which is indeed true as we discuss 
in the next subsection 
 using clickthrough history only 
we now turn off the query history and only use the clicked 
summaries plus the current query the results are shown in table we 
see that the benefit of using clickthrough information is much more 
significant than that of using query history we see an overall 
positive effect often with significant improvement over the baseline it 
is also clear that the richer the context data is the more 
improvement using clicked summaries can achieve other than some 
occasional degradation of precision at documents the improvement 
is fairly consistent and often quite substantial 
these results show that the clicked summary text is in general 
quite useful for inferring a user s information need intuitively 
using the summary text rather than the actual content of the 
document makes more sense as it is quite possible that the document 
behind a seemingly relevant summary is actually non-relevant 
 out of the clicked documents are relevant updating the 
query model based on such summaries would bring up the ranks 
of these relevant documents causing performance improvement 
however such improvement is really not beneficial for the user as 
the user has already seen these relevant documents to see how 
much improvement we have achieved on improving the ranks of 
the unseen relevant documents we exclude these relevant 
documents from our judgment file and recompute the performance of 
bayesint and the baseline using the new judgment file the results 
are shown in table note that the performance of the baseline 
method is lower due to the removal of the relevant documents 
which would have been generally ranked high in the results from 
table we see clearly that using clicked summaries also helps 
improve the ranks of unseen relevant documents significantly 
query bayesint µ ν 
map pr  docs 
q 
q hc 
improve 
q 
q hc 
improve 
q 
q hc 
improve 
table bayesint evaluated on unseen relevant documents 
one remaining question is whether the clickthrough data is still 
helpful if none of the clicked documents is relevant to answer 
this question we took out the relevant summaries from our 
clickthrough history data hc to obtain a smaller set of clicked 
summaries hc and re-evaluated the performance of the bayesint 
method using hc with the same setting of parameters as in 
table the results are shown in table we see that although the 
improvement is not as substantial as in table the average 
precision is improved across all generations of queries these results 
should be interpreted as very encouraging as they are based on only 
 non-relevant clickthroughs in reality a user would more likely 
click some relevant summaries which would help bring up more 
relevant documents as we have seen in table and table 
fixint bayesint onlineup batchup 
query α β µ ν µk ν ∀i k µi ∞ µ ν 
map pr  docs map pr  docs map pr  docs map pr  docs 
q 
q hc 
improve - - 
q 
q hc 
improve 
q 
q hc 
improve 
table effect of using clickthrough data only for document ranking 
query bayesint µ ν 
map pr  docs 
q 
q hc 
improve - 
q 
q hc 
improve 
q 
q hc 
improve - 
table effect of using only non-relevant clickthrough data 
 additive effect of context information 
by comparing the results across table table and table 
we can see that the benefit of the query history information and 
that of clickthrough information are mostly additive i e 
combining them can achieve better performance than using each alone 
but most improvement has clearly come from the clickthrough 
information in table we show this effect for the batchup method 
 parameter sensitivity 
all four models have two parameters to control the relative weights 
of hq hc and qk though the parameterization is different from 
model to model in this subsection we study the parameter 
sensitivity for batchup which appears to perform relatively better than 
others batchup has two parameters µ and ν 
we first look at µ when µ is set to the query history is not 
used at all and we essentially just use the clickthrough data 
combined with the current query if we increase µ we will gradually 
incorporate more information from the previous queries in table 
we show how the average precision of batchup changes as we vary 
µ with ν fixed to where the best performance of batchup is 
achieved we see that the performance is mostly insensitive to the 
change of µ for q and q but is decreasing as µ increases for q 
the pattern is also similar when we set ν to other values 
in addition to the fact that q is generally worse than q q and 
q another possible reason why the sensitivity is lower for q and 
q may be that we generally have more clickthrough data 
available for q and q than for q and the dominating influence of the 
clickthrough data has made the small differences caused by µ less 
visible for q and q 
the best performance is generally achieved when µ is around 
 which means that the past query information is as useful as 
about words in the current query except for q there is clearly 
some tradeoff between the current query and the previous queries 
query map pr  docs 
q 
q hq 
improve - - 
q hc 
improve 
q hq hc 
improve - 
q 
q hq 
improve - 
q hc 
improve 
q hq hc 
improve 
q 
q hq 
improve - 
q hc 
improve 
q hq hc 
improve 
table additive benefit of context information 
and using a balanced combination of them achieves better 
performance than using each of them alone 
we now turn to the other parameter ν when ν is set to we 
only use the clickthrough data when ν is set to ∞ we only use 
the query history and the current query with µ set to where 
the best performance of batchup is achieved we vary ν and show 
the results in table we see that the performance is also not very 
sensitive when ν ≤ with the best performance often achieved 
at ν this means that the combined information of query 
history and the current query is as useful as about words in the 
clickthrough data indicating that the clickthrough information is 
highly valuable 
overall these sensitivity results show that batchup not only 
performs better than other methods but also is quite robust 
 conclusions and future work 
in this paper we have explored how to exploit implicit 
feedback information including query history and clickthrough history 
within the same search session to improve information retrieval 
performance using the kl-divergence retrieval model as the 
basis we proposed and studied four statistical language models for 
context-sensitive information retrieval i e fixint bayesint 
onlineup and batchup we use trec ap data to create a test set 
µ 
map 
q hq hc pr  
map 
q hq hc pr  
map 
q hq hc pr  
table sensitivity of µ in batchup 
ν 
map 
q hq hc pr  
map 
q hq hc pr  
map 
q hq hc pr  
table sensitivity of ν in batchup 
for evaluating implicit feedback models experiment results show 
that using implicit feedback especially clickthrough history can 
substantially improve retrieval performance without requiring any 
additional user effort 
the current work can be extended in several ways first we 
have only explored some very simple language models for 
incorporating implicit feedback information it would be interesting to 
develop more sophisticated models to better exploit query history 
and clickthrough history for example we may treat a clicked 
summary differently depending on whether the current query is a 
generalization or refinement of the previous query second the 
proposed models can be implemented in any practical systems we are 
currently developing a client-side personalized search agent which 
will incorporate some of the proposed algorithms we will also do 
a user study to evaluate effectiveness of these models in the real 
web search finally we should further study a general retrieval 
framework for sequential decision making in interactive 
information retrieval and study how to optimize some of the parameters in 
the context-sensitive retrieval models 
 acknowledgments 
this material is based in part upon work supported by the 
national science foundation under award numbers iis- and 
iis- we thank the anonymous reviewers for their useful 
comments 
 references 
 e adar and d karger haystack per-user information 
environments in proceedings of cikm 
 j allan and et al challenges in information retrieval and 
language modeling workshop at university of amherst 
 
 k bharat searchpad explicit capture of search context to 
support web search in proceeding of www 
 w b croft s cronen-townsend and v larvrenko 
relevance feedback and personalization a language 
modeling perspective in proeedings of second delos 
workshop personalisation and recommender systems in 
digital libraries 
 h cui j -r wen j -y nie and w -y ma probabilistic 
query expansion using query logs in proceedings of www 
 
 s t dumais e cutrell r sarin and e horvitz implicit 
queries iq for contextualized search demo description in 
proceedings of sigir page 
 l finkelstein e gabrilovich y matias e rivlin z solan 
g wolfman and e ruppin placing search in context the 
concept revisited in proceedings of www 
 c huang l chien and y oyang query session based term 
suggestion for interactive web search in proceedings of 
www 
 x huang f peng a an and d schuurmans dynamic 
web log session identification with statistical language 
models journal of the american society for information 
science and technology - 
 g jeh and j widom scaling personalized web search in 
proceeding of www 
 t joachims optimizing search engines using clickthrough 
data in proceedings of sigkdd 
 d kelly and n j belkin display time as implicit feedback 
understanding task effects in proceedings of sigir 
 
 d kelly and j teevan implicit feedback for inferring user 
preference sigir forum 
 j rocchio relevance feedback information retrieval in the 
smart retrieval system-experiments in automatic document 
processing pages - kansas city mo 
prentice-hall 
 x shen and c zhai exploiting query history for document 
ranking in interactive information retrieval poster in 
proceedings of sigir 
 s sriram x shen and c zhai a session-based search 
engine poster in proceedings of sigir 
 k sugiyama k hatano and m yoshikawa adaptive web 
search based on user profile constructed without any effort 
from users in proceedings of www 
 r w white j m jose c j van rijsbergen and 
i ruthven a simulated study of implicit feedback models 
in proceedings of ecir pages - 
 c zhai and j lafferty model-based feedback in the 
kl-divergence retrieval model in proceedings of cikm 
 
 c zhai and j lafferty a study of smoothing methods for 
language models applied to ad-hoc information retrieval in 
proceedings of sigir 
event threading within news topics 
ramesh nallapati ao feng fuchun peng james allan 
center for intelligent information retrieval 
department of computer science 
university of massachusetts 
amherst ma 
nmramesh aofeng fuchun allan  cs umass edu 
abstract 
with the overwhelming volume of online news available today 
there is an increasing need for automatic techniques to analyze and 
present news to the user in a meaningful and efficient manner 
previous research focused only on organizing news stories by their 
topics into a flat hierarchy we believe viewing a news topic as a 
flat collection of stories is too restrictive and inefficient for a user 
to understand the topic quickly 
in this work we attempt to capture the rich structure of events 
and their dependencies in a news topic through our event models 
we call the process of recognizing events and their dependencies 
event threading we believe our perspective of modeling the 
structure of a topic is more effective in capturing its semantics than a flat 
list of on-topic stories 
we formally define the novel problem suggest evaluation 
metrics and present a few techniques for solving the problem besides 
the standard word based features our approaches take into account 
novel features such as temporal locality of stories for event 
recognition and time-ordering for capturing dependencies our 
experiments on a manually labeled data sets show that our models 
effectively identify the events and capture dependencies among them 
categories and subject descriptors 
h information search and retrieval clustering 
general terms 
algorithms experimentation measurement 
 introduction 
news forms a major portion of information disseminated in the 
world everyday common people and news analysts alike are very 
interested in keeping abreast of new things that happen in the news 
but it is becoming very difficult to cope with the huge volumes 
of information that arrives each day hence there is an increasing 
need for automatic techniques to organize news stories in a way that 
helps users interpret and analyze them quickly this problem is 
addressed by a research program called topic detection and tracking 
 tdt that runs an open annual competition on standardized 
tasks of news organization 
one of the shortcomings of current tdt evaluation is its view of 
news topics as flat collection of stories for example the detection 
task of tdt is to arrange a collection of news stories into clusters 
of topics however a topic in news is more than a mere collection 
of stories it is characterized by a definite structure of inter-related 
events this is indeed recognized by tdt which defines a topic as 
 a set of news stories that are strongly related by some seminal 
realworld event where an event is defined as  something that happens 
at a specific time and location for example when a bomb 
explodes in a building that is the seminal event that triggers the 
topic other events in the topic may include the rescue attempts 
the search for perpetrators arrests and trials and so on we see 
that there is a pattern of dependencies between pairs of events in 
the topic in the above example the event of rescue attempts is 
 influenced by the event of bombing and so is the event of search 
for perpetrators 
in this work we investigate methods for modeling the structure 
of a topic in terms of its events by structure we mean not only 
identifying the events that make up a topic but also establishing 
dependencies-generally causal-among them we call the 
process of recognizing events and identifying dependencies among 
them event threading an analogy to email threading that shows 
connections between related email messages we refer to the 
resulting interconnected structure of events as the event model of the 
topic although this paper focuses on threading events within an 
existing news topic we expect that such event based dependency 
structure more accurately reflects the structure of news than strictly 
bounded topics do from a user s perspective we believe that our 
view of a news topic as a set of interconnected events helps him her 
get a quick overview of the topic and also allows him her navigate 
through the topic faster 
the rest of the paper is organized as follows in section we 
discuss related work in section we define the problem and use 
an example to illustrate threading of events within a news topic in 
section we describe how we built the corpus for our problem 
section presents our evaluation techniques while section 
describes the techniques we use for modeling event structure in 
section we present our experiments and results section concludes 
the paper with a few observations on our results and comments on 
future work 
 
 related work 
the process of threading events together is related to threading 
of electronic mail only by name for the most part email usually 
incorporates a strong structure of referenced messages and 
consistently formatted subject headings-though information retrieval 
techniques are useful when the structure breaks down email 
threading captures reference dependencies between messages and 
does not attempt to reflect any underlying real-world structure of 
the matter under discussion 
another area of research that looks at the structure within a topic 
is hierarchical text classification of topics the hierarchy 
within a topic does impose a structure on the topic but we do not 
know of an effort to explore the extent to which that structure 
reflects the underlying event relationships 
barzilay and lee proposed a content structure modeling 
technique where topics within text are learnt using unsupervised 
methods and a linear order of these topics is modeled using hidden 
markov models our work differs from theirs in that we do not 
constrain the dependency to be linear also their algorithms are tuned 
to work on specific genres of topics such as earthquakes accidents 
etc while we expect our algorithms to generalize over any topic 
in tdt researchers have traditionally considered topics as 
flatclusters however in tdt- a hierarchical structure of 
topic detection has been proposed and made useful attempts 
to adopt the new structure however this structure still did not 
explicitly model any dependencies between events 
in a work closest to ours makkonen suggested modeling 
news topics in terms of its evolving events however the paper 
stopped short of proposing any models to the problem other 
related work that dealt with analysis within a news topic includes 
temporal summarization of news topics 
 problem definition and notation 
in this work we have adhered to the definition of event and topic 
as defined in tdt we present some definitions in italics and our 
interpretations regular-faced below for clarity 
 story a story is a news article delivering some information 
to users in tdt a story is assumed to refer to only a single 
topic in this work we also assume that each story discusses 
a single event in other words a story is the smallest atomic 
unit in the hierarchy topic event story clearly both 
the assumptions are not necessarily true in reality but we 
accept them for simplicity in modeling 
 event an event is something that happens at some specific 
time and place in our work we represent an event by 
a set of stories that discuss it following the assumption of 
atomicity of a story this means that any set of distinct events 
can be represented by a set of non-overlapping clusters of 
news stories 
 topic a set of news stories strongly connected by a seminal 
event we expand on this definition and interpret a topic as 
a series of related events thus a topic can be represented 
by clusters of stories each representing an event and a set of 
 directed or undirected edges between pairs of these clusters 
representing the dependencies between these events we will 
describe this representation of a topic in more detail in the 
next section 
 topic detection and tracking tdt topic detection 
detects clusters of stories that discuss the same topic topic 
tracking detects stories that discuss a previously known topic 
thus tdt concerns itself mainly with clustering stories into 
topics that discuss them 
 event threading event threading detects events within in a 
topic and also captures the dependencies among the events 
thus the main difference between event threading and tdt 
is that we focus our modeling effort on microscopic events 
rather than larger topics additionally event threading 
models the relatedness or dependencies between pairs of events 
in a topic while tdt models topics as unrelated clusters of 
stories 
we first define our problem and representation of our model 
formally and then illustrate with the help of an example we are 
given a set of ò news stories ë ×½ ×ò on a given topic 
ì and their time of publication we define a set of events 
½ ñ with the following constraints 
¾ ¾ 
ë 
× ø 
× ¾ × ø × ¾ 
while the first constraint says that each event is an element in the 
power set of s the second constraint ensures that each story can 
belong to at most one event the last constraint tells us that every 
story belongs to one of the events in in fact this allows us to 
define a mapping function from stories to events as follows 
´× µ iff × ¾ 
further we also define a set of directed edges ´ µ 
which denote dependencies between events it is important to 
explain what we mean by this directional dependency while the 
existence of an edge itself represents relatedness of two events the 
direction could imply causality or temporal-ordering by causal 
dependency we mean that the occurrence of event b is related to 
and is a consequence of the occurrence of event a by temporal 
ordering we mean that event b happened after event a and is related 
to a but is not necessarily a consequence of a for example 
consider the following two events  plane crash event a and 
 subsequent investigations event b in a topic on a plane crash incident 
clearly the investigations are a result of the crash hence an 
arrow from a to b falls under the category of causal dependency 
now consider the pair of events  pope arrives in cuba event a 
and  pope meets castro event b in a topic that discusses pope s 
visit to cuba now events a and b are closely related through their 
association with the pope and cuba but event b is not necessarily 
a consequence of the occurrence of event a an arrow in such 
scenario captures what we call time ordering in this work we do not 
make an attempt to distinguish between these two kinds of 
dependencies and our models treats them as identical a simpler and 
hence less controversial choice would be to ignore direction in the 
dependencies altogether and consider only undirected edges this 
choice definitely makes sense as a first step but we chose the former 
since we believe directional edges make more sense to the user as 
they provide a more illustrative flow-chart perspective to the topic 
to make the idea of event threading more concrete consider the 
example of tdt topic titled  osama bin laden s 
indictment in the news this topic has stories which form 
events an event model of this topic can be represented as in figure 
 each box in the figure indicates an event in the topic of osama s 
indictment the occurrence of event namely  trial and 
indictment of osama is dependent on the event of  evidence gathered 
by cia i e event similarly event influences the occurrences 
of events and namely  threats from militants  reactions 
 
from muslim world and  announcement of reward thus all the 
dependencies in the example are causal 
extending our notation further we call an event a a parent of b 
and b the child of a if ´ µ ¾ we define an event model 
å ´ µ to be a tuple of the set of events and set of 
dependencies 
trial and 
 
 
 
cia announces reward 
muslim world 
reactions from 
islamic militants 
threats from 
 
 
osama 
indictment of 
cia 
gathered by 
evidence 
figure an event model of tdt topic  osama bin laden s 
indictment 
event threading is strongly related to topic detection and 
tracking but also different from it significantly it goes beyond topics 
and models the relationships between events thus event 
threading can be considered as a further extension of topic detection and 
tracking and is more challenging due to at least the following 
difficulties 
 the number of events is unknown 
 the granularity of events is hard to define 
 the dependencies among events are hard to model 
 since it is a brand new research area no standard evaluation 
metrics and benchmark data is available 
in the next few sections we will describe our attempts to tackle 
these problems 
 labeled data 
we picked topics from the tdt corpus and topics from 
the tdt corpus the criterion we used for selecting a topic is that 
it should contain at least on-topic stories from cnn headline 
news if the topic contained more than cnn stories we picked 
only the first stories to keep the topic short enough for 
annotators the reason for choosing only cnn as the source is that the 
stories from this source tend to be short and precise and do not tend 
to digress or drift too far away from the central theme we believe 
modeling such stories would be a useful first step before dealing 
with more complex data sets 
we hired an annotator to create truth data annotation includes 
defining the event membership for each story and also the 
dependencies we supervised the annotator on a set of three topics that 
we did our own annotations on and then asked her to annotate the 
 topics from tdt and topics from tdt 
in identifying events in a topic the annotator was asked to broadly 
follow the tdt definition of an event i e  something that happens 
at a specific time and location the annotator was encouraged to 
merge two events a and b into a single event c if any of the 
stories discusses both a and b this is to satisfy our assumption that 
each story corresponds to a unique event the annotator was also 
encouraged to avoid singleton events events that contain a single 
news story if possible we realized from our own experience that 
people differ in their perception of an event especially when the 
number of stories in that event is small as part of the guidelines 
we instructed the annotator to assign titles to all the events in each 
topic we believe that this would help make her understanding of 
the events more concrete we however do not use or model these 
titles in our algorithms 
in defining dependencies between events we imposed no 
restrictions on the graph structure each event could have single 
multiple or no parents further the graph could have cycles or 
orphannodes the annotator was however instructed to assign a 
dependency from event a to event b if and only if the occurrence of b 
is  either causally influenced by a or is closely related to a and 
follows a in time 
from the annotated topics we created a training set of topics 
and a test set of topics by merging the topics from tdt and 
 from tdt and splitting them randomly table shows that the 
training and test sets have fairly similar statistics 
feature training set test set 
num topics 
avg num stories topic 
avg doc len 
avg num stories event 
avg num events topic 
avg num dependencies topic 
avg num dependencies event 
avg num days topic 
table statistics of annotated data 
 evaluation 
a system can generate some event model å¼ ´ 
¼ ¼µ using 
certain algorithms which is usually different from the truth model 
å ´ µ we assume the annotator did not make any 
mistake comparing a system event model å¼ with the true model 
å requires comparing the entire event models including their 
dependency structure and different event granularities may bring 
huge discrepancy between å¼ and å this is certainly non-trivial 
as even testing whether two graphs are isomorphic has no known 
polynomial time solution hence instead of comparing the actual 
structure we examine a pair of stories at a time and verify if the 
system and true labels agree on their event-memberships and 
dependencies specifically we compare two kinds of story pairs 
¯ cluster pairs ´åµ these are the complete set of 
unordered pairs ´× × µ of stories × and × that fall within the 
same event given a model å formally 
´åµ ´× × µ × × ¾ ë ´× µ ´× µ 
where is the function in å that maps stories to events as 
defined in equation 
¯ dependency pairs ´åµ these are the set of all ordered 
pairs of stories ´× × µ such that there is a dependency from 
the event of × to the event of × in the model å 
´åµ ´× × µ ´ ´× µ ´× µµ ¾ 
note the story pair is ordered here so ´× × µ is not 
equivalent to ´× × µ in our evaluation a correct pair with wrong 
 
 b- d 
cluster pairs 
 a c 
dependency pairs 
 a- b 
 c- b 
 b- d 
d e 
d e 
 d e 
 d e 
 a- c a- e 
 b- c b- e 
 b- e 
cluster precision 
cluster recall 
dependency recall 
dependency precision 
 a- d 
true event model system event model 
a b 
c 
a c b 
cluster pairs 
 a b 
dependency pairs 
figure evaluation measures 
direction will be considered a mistake as we mentioned 
earlier in section ignoring the direction may make the 
problem simpler but we will lose the expressiveness of our 
representation 
given these two sets of story pairs corresponding to the true 
event model å and the system event model å¼ we define recall 
and precision for each category as follows 
¯ cluster precision cp it is the probability that two 
randomly selected stories × and × are in the same true-event 
given that they are in the same system event 
è è´ ´× µ ´× µ 
¼´× µ 
¼´× µµ 
´åµ ´å¼µ 
´å¼µ 
 
where ¼ is the story-event mapping function corresponding 
to the model å¼ 
¯ cluster recall cr it is the probability that two randomly 
selected stories × and × are in the same system-event given 
that they are in the same true event 
ê è´ 
¼´× µ 
¼´× µ ´× µ ´× µµ 
´åµ ´å¼µ 
´åµ 
 
¯ dependency precision dp it is the probability that there is 
a dependency between the events of two randomly selected 
stories × and × in the true model å given that they have a 
dependency in the system model å¼ note that the direction 
of dependency is important in comparison 
è è´´ ´× µ ´× µµ ¾ ´ 
¼´× µ 
¼´× µµ ¾ 
¼µ 
´åµ ´å¼µ 
´å¼µ 
 
¯ dependency recall dr it is the probability that there is 
a dependency between the events of two randomly selected 
stories × and × in the system model å¼ given that they have 
a dependency in the true model å again the direction of 
dependency is taken into consideration 
ê è´´ 
¼´× µ 
¼´× µµ ¾ 
¼ ´ ´× µ ´× µµ ¾ µ 
´åµ ´å¼µ 
´åµ 
 
the measures are illustrated by an example in figure we also 
combine these measures using the well known f -measure 
commonly used in text classification and other research areas as shown 
below 
¾ ¢ è ¢ ê 
è · ê 
¾ ¢ è ¢ ê 
è · ê 
â ¾ ¢ ¢ 
· 
 
where and are the cluster and dependency f -measures 
respectively and â is the joint f -measure â that we use to 
measure the overall performance 
 techniques 
the task of event modeling can be split into two parts clustering 
the stories into unique events in the topic and constructing 
dependencies among them in the following subsections we describe 
techniques we developed for each of these sub-tasks 
 clustering 
each topic is composed of multiple events so stories must be 
clustered into events before we can model the dependencies among 
them for simplicity all stories in the same topic are assumed to 
be available at one time rather than coming in a text stream this 
task is similar to traditional clustering but features other than word 
distributions may also be critical in our application 
in many text clustering systems the similarity between two 
stories is the inner product of their tf-idf vectors hence we use it as 
one of our features stories in the same event tend to follow 
temporal locality so the time stamp of each story can be a useful feature 
additionally named-entities such as person and location names are 
another obvious feature when forming events stories in the same 
event tend to be related to the same person s and locations s 
in this subsection we present an agglomerative clustering 
algorithm that combines all these features in our experiments 
however we study the effect of each feature on the performance 
separately using modified versions of this algorithm 
 agglomerative clustering with 
time decay acdt 
we initialize our events to singleton events clusters i e each 
cluster contains exactly one story so the similarity between two 
events to start with is exactly the similarity between the 
corresponding stories the similarity û×ùñ´×½ ×¾µ between two 
stories ×½ and ×¾ is given by the following formula 
û×ùñ´×½ ×¾µ ½ ó×´×½ ×¾µ · ¾äó ´×½ ×¾µ · è ö´×½ ×¾µ 
 
here ½ ¾ are the weights on different features in this work 
we determined them empirically but in the future one can 
consider more sophisticated learning techniques to determine them 
ó×´×½ ×¾µ is the cosine similarity of the term vectors äó ´×½ ×¾µ 
is if there is some location that appears in both stories otherwise 
it is è ö´×½ ×¾µ is similarly defined for person name 
we use time decay when calculating similarity of story pairs 
i e the larger time difference between two stories the smaller their 
similarities the time period of each topic differs a lot from a few 
days to a few months so we normalize the time difference using 
the whole duration of that topic the time decay adjusted similarity 
 
× ñ´×½ ×¾µ is given by 
× ñ´×½ ×¾µ û×ùñ´×½ ×¾µ 
 ø½ ø¾ 
ì 
where ø½ and ø¾ are the time stamps for story and respectively 
t is the time difference between the earliest and the latest story in 
the given topic is the time decay factor 
in each iteration we find the most similar event pair and merge 
them we have three different ways to compute the similarity 
between two events ù and ú 
¯ average link in this case the similarity is the average of the 
similarities of all pairs of stories between ù and ú as shown 
below 
× ñ´ ù ú µ 
è×ù¾ ù 
è×ú¾ ú × ñ´×ù ×ú µ 
ù ú 
 
¯ complete link the similarity between two events is given 
by the smallest of the pair-wise similarities 
× ñ´ ù ú µ ñ ò 
×ù¾ ù ×ú¾ ú 
× ñ´×ù ×ú µ 
¯ single link here the similarity is given by the best similarity 
between all pairs of stories 
× ñ´ ù ú µ ñ ü 
×ù¾ ù ×ú¾ ú 
× ñ´×ù ×ú µ 
this process continues until the maximum similarity falls below 
the threshold or the number of clusters is smaller than a given 
number 
 dependency modeling 
capturing dependencies is an extremely hard problem because 
it may require a  deeper understanding of the events in question 
a human annotator decides on dependencies not just based on the 
information in the events but also based on his her vast repertoire 
of domain-knowledge and general understanding of how things 
operate in the world for example in figure a human knows  trial 
and indictment of osama is influenced by  evidence gathered by 
cia because he she understands the process of law in general 
we believe a robust model should incorporate such domain 
knowledge in capturing dependencies but in this work as a first step we 
will rely on surface-features such as time-ordering of news stories 
and word distributions to model them our experiments in later 
sections demonstrate that such features are indeed useful in capturing 
dependencies to a large extent 
in this subsection we describe the models we considered for 
capturing dependencies in the rest of the discussion in this subsection 
we assume that we are already given the mapping ¼ ë and 
we focus only on modeling the edges ¼ first we define a couple 
of features that the following models will employ 
first we define a - time-ordering function ø ë ½ ò 
that sorts stories in ascending order by their time of publication 
now the event-time-ordering function ø is defined as follows 
ø ½ ñ × ø 
ù ú ¾ ø ´ ùµ ø ´ úµ ´µ ñ ò 
×ù¾ ù 
ø´×ùµ ñ ò 
×ú¾ ú 
ø´×úµ 
 
in other words ø time-orders events based on the time-ordering of 
their respective first stories 
we will also use average cosine similarity between two events as 
a feature and it is defined as follows 
ú ë ñ´ ù ú µ 
è×ù¾ ù 
è×ú¾ ú ó×´×ù ×ú µ 
ù ú 
 
 complete-link model 
in this model we assume that there are dependencies between all 
pairs of events the direction of dependency is determined by the 
time-ordering of the first stories in the respective events formally 
the system edges are defined as follows 
¼ ´ ù ú µ ø ´ ùµ ø ´ ú µ 
where ø is the event-time-ordering function in other words the 
dependency edge is directed from event ù to event ú if the first 
story in event ù is earlier than the first story in event ú we point 
out that this is not to be confused with the complete-link algorithm 
in clustering although we use the same names it will be clear 
from the context which one we refer to 
 simple thresholding 
this model is an extension of the complete link model with an 
additional constraint that there is a dependency between any two 
events ù and ú only if the average cosine similarity between 
event ù and event ú is greater than a threshold ì formally 
¼ ´ ù úµ ú ë ñ´ ù ú µ ì 
ø ´ ùµ ø ´ ú µ 
 nearest parent model 
in this model we assume that each event can have at most one 
parent we define the set of dependencies as follows 
¼ ´ ù úµ ú ë ñ´ ù ú µ ì 
ø ´ úµ ø ´ ùµ · ½ 
thus for each event ú the nearest parent model considers only 
the event preceding it as defined by ø as a potential candidate the 
candidate is assigned as the parent only if the average similarity 
exceeds a pre-defined threshold ì 
 best similarity model 
this model also assumes that each event can have at most one 
parent an event ú is assigned a parent ù if and only if ù is 
the most similar earlier event to ú and the similarity exceeds a 
threshold ì mathematically this can be expressed as 
¼ ´ ù ú µ ú ë ñ´ ù úµ ì 
ù ö ñ ü 
û ø ´ ûµ ø ´ úµ 
ú ë ñ´ û ú µ 
 
 maximum spanning tree model 
in this model we first build a maximum spanning tree mst 
using a greedy algorithm on the following fully connected weighted 
undirected graph whose vertices are the events and whose edges 
are defined as follows 
´ ù ú µ û´ ù ú µ ú ë ñ´ ù úµ 
let åëì´ µ be the set of edges in the maximum spanning tree of 
¼ now our directed dependency edges are defined as follows 
¼ ´ ù ú µ ´ ù ú µ ¾ åëì´ µ ø ´ ùµ ø ´ úµ 
ú ë ñ´ ù ú µ ì 
 
thus in this model we assign dependencies between the most 
similar events in the topic 
 experiments 
our experiments consists of three parts first we modeled only 
the event clustering part defining the mapping function ¼ using 
clustering algorithms described in section then we modeled 
only the dependencies by providing to the system the true clusters 
and running only the dependency algorithms of section finally 
we experimented with combinations of clustering and dependency 
algorithms to produce the complete event model this way of 
experimentation allows us to compare the performance of our 
algorithms in isolation and in association with other components the 
following subsections present the three parts of our 
experimentation 
 clustering 
we have tried several variations of the ì algorithm to study 
the effects of various features on the clustering performance all 
the parameters are learned by tuning on the training set we also 
tested the algorithms on the test set with parameters fixed at their 
optimal values learned from training we used agglomerative 
clusmodel best t cp cr cf p-value 
cos -lnk 
 cos all-lnk 
 cos loc avg-lnk 
 cos per avg-lnk 
 cos td avg-lnk e- 
cos n t avg-lnk - e- 
cos n t t avg-lnk e- 
cos td n t avg-lnk - e- 
cos td n t t avg-lnk e- 
baseline cos avg-lnk 
 table comparison of agglomerative clustering algorithms 
 training set 
tering based on only cosine similarity as our clustering baseline 
the results on the training and test sets are in table and 
respectively we use the cluster f -measure cf averaged over all topics 
as our evaluation criterion 
model cp cr cf p-value 
cos -lnk 
 cos all-lnk 
 cos loc avg-lnk 
 cos per avg-lnk 
 cos td avg-lnk 
cos n t avg-lnk 
cos n t t avg-lnk 
cos td n t avg-lnk 
cos td n t t avg-lnk 
baseline cos avg-lnk 
 table comparison of agglomerative clustering algorithms 
 test set 
p-value marked with a £ means that it is a statistically significant 
improvement over the baseline confidence level one tailed 
t-test the methods shown in table and are 
¯ baseline tf-idf vector weight cosine similarity average link 
in clustering in equation ½ ½ ¾ ¼ and 
 ¼ in equation this f-value is the maximum obtained 
by tuning the threshold 
¯ cos -lnk single link comparison see equation is used 
where similarity of two clusters is the maximum of all story 
pairs other configurations are the same as the baseline run 
¯ cos all-lnk complete link algorithm of equation is used 
similar to single link but it takes the minimum similarity of 
all story pairs 
¯ cos loc avg-lnk location names are used when 
calculating similarity ¾ ¼ ¼ in equation all algorithms 
starting from this one use average link equation since 
single link and complete link do not show any improvement 
of performance 
¯ cos per avg-lnk ¼ ¼ in equation i e we put 
some weight on person names in the similarity 
¯ cos td avg-lnk time decay coefficient ½ in equation 
 which means the similarity between two stories will be 
decayed to ½ if they are at different ends of the topic 
¯ cos n t avg-lnk use the number of true events to control 
the agglomerative clustering algorithm when the number 
of clusters is fewer than that of truth events stop merging 
clusters 
¯ cos n t t avg-lnk similar to n t but also stop 
agglomeration if the maximal similarity is below the threshold ì 
¯ cos td n t avg-lnk similar to n t but the similarities 
are decayed ½ in equation 
¯ cos td n t t avg-lnk similar to td n truth but 
calculation halts when the maximal similarity is smaller than 
the threshold ì 
our experiments demonstrate that single link and complete link 
similarities perform worse than average link which is reasonable 
since average link is less sensitive to one or two story pairs we 
had expected locations and person names to improve the result but 
it is not the case analysis of topics shows that many on-topic 
stories share the same locations or persons irrespective of the event 
they belong to so these features may be more useful in identifying 
topics rather than events time decay is successful because events 
are temporally localized i e stories discussing the same event tend 
to be adjacent to each other in terms of time also we noticed 
that providing the number of true events improves the performance 
since it guides the clustering algorithm to get correct granularity 
however for most applications it is not available we used it only 
as a cheat experiment for comparison with other algorithms on 
the whole time decay proved to the most powerful feature besides 
cosine similarity on both training and test sets 
 dependencies 
in this subsection our goal is to model only dependencies we 
use the true mapping function and by implication the true events 
î we build our dependency structure ¼ using all the five 
models described in section we first train our models on the 
training topics training involves learning the best threshold ì 
for each of the models we then test the performances of all the 
trained models on the test topics we evaluate our performance 
 
using the average values of dependency precision dp 
dependency recall dr and dependency f-measure df we consider 
the complete-link model to be our baseline since for each event it 
trivially considers all earlier events to be parents 
table lists the results on the training set we see that while all 
the algorithms except mst outperform the baseline complete-link 
algorithm the nearest parent algorithm is statistically significant 
from the baseline in terms of its df-value using a one-tailed paired 
t-test at confidence level 
model best ì dp dr df p-value 
nearest parent 
best similarity 
mst 
 simple thresh 
complete-link - 
 table results on the training set best ì is the optimal value 
of the threshold ì indicates the corresponding model is 
statistically significant compared to the baseline using a one-tailed 
paired t-test at confidence level 
in table we present the comparison of the models on the test 
set here we do not use any tuning but set the threshold to the 
corresponding optimal values learned from the training set the 
results throw some surprises the nearest parent model which was 
significantly better than the baseline on training set turns out to be 
worse than the baseline on the test set however all the other 
models are better than the baseline including the best similarity which 
is statistically significant notice that all the models that perform 
better than the baseline in terms of df actually sacrifice their 
recall performance compared to the baseline but improve on their 
precision substantially thereby improving their performance on the 
df-measure 
we notice that both simple-thresholding and best similarity are 
better than the baseline on both training and test sets although the 
improvement is not significant on the whole we observe that the 
surface-level features we used capture the dependencies to a 
reasonable level achieving a best value of df on the test set 
although there is a lot of room for improvement we believe this is 
a good first step 
model dp dr df p-value 
nearest parent 
 best similarity 
mst 
simple thresh 
baseline complete-link 
 table results on the test set 
 combining clustering and dependencies 
now that we have studied the clustering and dependency 
algorithms in isolation we combine the best performing algorithms and 
build the entire event model since none of the dependency 
algorithms has been shown to be consistently and significantly better 
than the others we use all of them in our experimentation from 
the clustering techniques we choose the best performing cos td 
as a baseline we use a combination of the baselines in each 
components i e cos for clustering and complete-link for dependencies 
note that we need to retrain all the algorithms on the training 
set because our objective function to optimize is now jf the joint 
f-measure for each algorithm we need to optimize both the 
clustering threshold and the dependency threshold we did this 
empirically on the training set and the optimal values are listed in table 
 
the results on the training set also presented in table indicate 
that cos td simple-thresholding is significantly better than the 
baseline in terms of the joint f-value jf using a one-tailed paired 
ttest at confidence level on the whole we notice that while the 
clustering performance is comparable to the experiments in section 
 the overall performance is undermined by the low dependency 
performance unlike our experiments in section where we had 
provided the true clusters to the system in this case the system 
has to deal with deterioration in the cluster quality hence the 
performance of the dependency algorithms has suffered substantially 
thereby lowering the overall performance 
the results on the test set present a very similar story as shown 
in table we also notice a fair amount of consistency in the 
performance of the combination algorithms cos td simple-thresholding 
outperforms the baseline significantly the test set results also point 
to the fact that the clustering component remains a bottleneck in 
achieving an overall good performance 
 discussion and conclusions 
in this paper we have presented a new perspective of modeling 
news topics contrary to the tdt view of topics as flat 
collection of news stories we view a news topic as a relational structure 
of events interconnected by dependencies in this paper we also 
proposed a few approaches for both clustering stories into events 
and constructing dependencies among them we developed a 
timedecay based clustering approach that takes advantage of 
temporallocalization of news stories on the same event and showed that it 
performs significantly better than the baseline approach based on 
cosine similarity our experiments also show that we can do fairly 
well on dependencies using only surface-features such as 
cosinesimilarity and time-stamps of news stories as long as true events 
are provided to the system however the performance deteriorates 
rapidly if the system has to discover the events by itself despite 
that discouraging result we have shown that our combined 
algorithms perform significantly better than the baselines 
our results indicate modeling dependencies can be a very hard 
problem especially when the clustering performance is below ideal 
level errors in clustering have a magnifying effect on errors in 
dependencies as we have seen in our experiments hence we should 
focus not only on improving dependencies but also on clustering at 
the same time 
as part of our future work we plan to investigate further into 
the data and discover new features that influence clustering as well 
as dependencies and for modeling dependencies a probabilistic 
framework should be a better choice since there is no definite 
answer of yes no for the causal relations among some events we also 
hope to devise an iterative algorithm which can improve clustering 
and dependency performance alternately as suggested by one of 
the reviewers we also hope to expand our labeled corpus further 
to include more diverse news sources and larger and more complex 
event structures 
acknowledgments 
we would like to thank the three anonymous reviewers for their 
valuable comments this work was supported in part by the center 
 
model cluster t dep t cp cr cf dp dr df jf p-value 
cos td nearest-parent 
 cos td best-similarity 
 cos td mst 
 cos td simple-thresholding 
baseline cos complete-link - 
 table combined results on the training set 
model cp cr cf dp dr df jf p-value 
cos td nearest parent 
 cos td best similarity 
 cos td mst 
 cos td simple thresholding 
baseline cos complete-link 
 table combined results on the test set 
for intelligent information retrieval and in part by 
spawarsyscensd grant number n - - - any opinions findings and 
conclusions or recommendations expressed in this material are the 
authors and do not necessarily reflect those of the sponsor 
 references 
 j allan j carbonell g doddington j yamron and 
y yang topic detection and tracking pilot study final 
report in proceedings of the darpa broadcast news 
transcription and understanding workshop pages - 
 
 j allan a feng and a bolivar flexible intrinsic 
evaluation of hierarchical clustering for tdt volume in the 
proc of the acm twelfth international conference on 
information and knowledge management pages - 
nov 
 james allan editor topic detection and tracking event 
based information organization kluwer academic 
publishers 
 james allan rahul gupta and vikas khandelwal temporal 
summaries of new topics in proceedings of the th annual 
international acm sigir conference on research and 
development in information retrieval pages - acm 
press 
 regina barzilay and lillian lee catching the drift 
probabilistic content models with applications to generation 
and summarization in proceedings of human language 
technology conference and north american chapter of the 
association for computational linguistics hlt-naacl 
pages - 
 d lawrie and w b croft discovering and comparing topic 
hierarchies in proceedings of riao conference pages 
 - 
 david d lewis and kimberly a knowles threading 
electronic mail a preliminary study inf process manage 
 - 
 juha makkonen investigations on event evolution in tdt in 
proceedings of hlt-naacl student workshop pages 
 - 
 aixin sun and ee-peng lim hierarchical text classification 
and evaluation in proceedings of the ieee 
international conference on data mining pages - 
ieee computer society 
 yiming yang jaime carbonell ralf brown thomas pierce 
brian t archibald and xin liu learning approaches for 
detecting and tracking news events in ieee intelligent 
systems special issue on applications of intelligent 
information retrieval volume pages - 
 
relaxed online svms for spam filtering 
d sculley 
tufts university 
department of computer science 
 college ave medford ma usa 
dsculleycs tufts edu 
gabriel m wachman 
tufts university 
department of computer science 
 college ave medford ma usa 
gwachm cs tufts edu 
abstract 
spam is a key problem in electronic communication 
including large-scale email systems and the growing number of 
blogs content-based filtering is one reliable method of 
combating this threat in its various forms but some academic 
researchers and industrial practitioners disagree on how best 
to filter spam the former have advocated the use of 
support vector machines svms for content-based filtering 
as this machine learning methodology gives state-of-the-art 
performance for text classification however similar 
performance gains have yet to be demonstrated for online spam 
filtering additionally practitioners cite the high cost of 
svms as reason to prefer faster if less statistically robust 
bayesian methods in this paper we offer a resolution to this 
controversy first we show that online svms indeed give 
state-of-the-art classification performance on online spam 
filtering on large benchmark data sets second we show 
that nearly equivalent performance may be achieved by a 
relaxed online svm rosvm at greatly reduced 
computational cost our results are experimentally verified on 
email spam blog spam and splog detection tasks 
categories and subject descriptors 
h information storage and retrieval information 
search and retrieval - spam 
general terms 
measurement experimentation algorithms 
 introduction 
electronic communication is increasingly plagued by 
unwanted or harmful content known as spam the most well 
known form of spam is email spam which remains a major 
problem for large email systems other forms of spam are 
also becoming problematic including blog spam in which 
spammers post unwanted comments in blogs and splogs 
which are fake blogs constructed to enable link spam with 
the hope of boosting the measured importance of a given 
webpage in the eyes of automated search engines there 
are a variety of methods for identifying these many forms 
of spam including compiling blacklists of known spammers 
and conducting link analysis 
the approach of content analysis has shown particular 
promise and generality for combating spam in content 
analysis the actual message text often including hyper-text and 
meta-text such as html and headers is analyzed using 
machine learning techniques for text classification to 
determine if the given content is spam content analysis has 
been widely applied in detecting email spam and has 
also been used for identifying blog spam and splogs 
in this paper we do not explore the related problem of link 
spam which is currently best combated by link analysis 
 an anti-spam controversy 
the anti-spam community has been divided on the choice 
of the best machine learning method for content-based spam 
detection academic researchers have tended to favor the 
use of support vector machines svms a statistically 
robust machine learning method which yields 
state-of-theart performance on general text classification however 
svms typically require training time that is quadratic in the 
number of training examples and are impractical for 
largescale email systems practitioners requiring content-based 
spam filtering have typically chosen to use the faster if 
less statistically robust machine learning method of naive 
bayes text classification this bayesian method 
requires only linear training time and is easily implemented 
in an online setting with incremental updates this allows a 
deployed system to easily adapt to a changing environment 
over time other fast methods for spam filtering include 
compression models and logistic regression it has 
not yet been empirically demonstrated that svms give 
improved performance over these methods in an online spam 
detection setting 
 contributions 
in this paper we address the anti-spam controversy and 
offer a potential resolution we first demonstrate that 
online svms do indeed provide state-of-the-art spam detection 
through empirical tests on several large benchmark data sets 
of email spam we then analyze the effect of the tradeoff 
parameter in the svm objective function which shows that 
the expensive svm methodology may in fact be overkill for 
spam detection we reduce the computational cost of svm 
learning by relaxing this requirement on the maximum 
margin in online settings and create a relaxed online svm 
rosvm appropriate for high performance content-based 
spam filtering in large-scale settings 
 spam and online svms 
the controversy between academics and practitioners in 
spam filtering centers on the use of svms the former 
advocate their use but have yet to demonstrate strong 
performance with svms on online spam filtering indeed the 
results of show that when used with default parameters 
svms actually perform worse than other methods in this 
section we review the basic workings of svms and describe 
a simple online svm algorithm we then show that online 
svms indeed achieve state-of-the-art performance on 
filtering email spam blog comment spam and splogs so long as 
the tradeoff parameter c is set to a high value however the 
cost of online svms turns out to be prohibitive for 
largescale applications these findings motivate our proposal of 
relaxed online svms in the following section 
 background svms 
svms are a robust machine learning methodology which 
has been shown to yield state-of-the-art performance on text 
classification by finding a hyperplane that separates 
two classes of data in data space while maximizing the 
margin between them 
we use the following notation to describe svms which 
draws from a data set x contains n labeled example 
vectors { x y xn yn } where each xi is a vector 
containing features describing example i and each yi is the class 
label for that example in spam detection the classes spam 
and ham i e not spam are assigned the numerical class 
labels and − respectively the linear svms we employ 
in this paper use a hypothesis vector w and bias term b to 
classify a new example x by generating a predicted class 
label f x 
f x sign w x b 
svms find the hypothesis w which defines the separating 
hyperplane by minimizing the following objective function 
over all n training examples 
τ w ξ 
 
 
 w 
 c 
nx 
i i 
ξi 
under the constraints that 
∀i { n} yi w xi b ≥ − ξi ξi ≥ 
in this objective function each slack variable ξi shows the 
amount of error that the classifier makes on a given example 
xi minimizing the sum of the slack variables corresponds 
to minimizing the loss function on the training data while 
minimizing the term 
 
 w 
corresponds to maximizing the 
margin between the two classes these two optimization 
goals are often in conflict the tradeoff parameter c 
determines how much importance to give each of these tasks 
linear svms exploit data sparsity to classify a new 
instance in o s time where s is the number of non-zero 
features this is the same classification time as other linear 
given data set x x y xn yn c m 
initialize w b seendata { } 
for each xi ∈ x do 
classify xi using f xi sign w xi b 
if yif xi 
find w b using smo on seendata 
using w b as seed hypothesis 
add xi to seendata 
done 
figure pseudo code for online svm 
classifiers and as naive bayesian classification training 
svms however typically takes o n 
 time for n training 
examples a variant for linear svms was recently proposed 
which trains in o ns time but because this method 
has a high constant we do not explore it here 
 online svms 
in many traditional machine learning applications svms 
are applied in batch mode that is an svm is trained on 
an entire set of training data and is then tested on a 
separate set of testing data spam filtering is typically tested 
and deployed in an online setting which proceeds 
incrementally here the learner classifies a new example is told if 
its prediction is correct updates its hypothesis accordingly 
and then awaits a new example online learning allows a 
deployed system to adapt itself in a changing environment 
re-training an svm from scratch on the entire set of 
previously seen data for each new example is cost prohibitive 
however using an old hypothesis as the starting point for 
re-training reduces this cost considerably one method of 
incremental and decremental svm learning was proposed in 
 because we are only concerned with incremental 
learning we apply a simpler algorithm for converting a batch 
svm learner into an online svm see figure for 
pseudocode which is similar to the approach of 
each time the online svm encounters an example that 
was poorly classified it retrains using the old hypothesis as 
a starting point note that due to the karush-kuhn-tucker 
 kkt conditions it is not necessary to re-train on 
wellclassified examples that are outside the margins 
we used platt s smo algorithm as a core svm solver 
because it is an iterative method that is well suited to 
converge quickly from a good initial hypothesis because 
previous work and our own initial testing indicates that binary 
feature values give the best results for spam filtering 
 we optimized our implementation of the online smo to 
exploit fast inner-products with binary vectors 
 feature mapping spam content 
extracting machine learning features from text may be 
done in a variety of ways especially when that text may 
include hyper-content and meta-content such as html and 
header information however previous research has shown 
that simple methods from text classification such as bag 
of words vectors and overlapping character-level n-grams 
can achieve strong results formally a bag of words 
vector is a vector x with a unique dimension for each possible 
 
our source code is freely available at 
www cs tufts edu ∼dsculley onlinesmo 
 
 
 
 
 
 
 
rocarea 
c 
 -grams 
 -grams 
 -grams 
words 
figure tuning the tradeoff parameter c tests 
were conducted with online smo using binary 
feature vectors on the spamassassin data set of 
examples graph plots c versus area under the 
roc curve 
word defined as a contiguous substring of non-whitespace 
characters an n-gram vector is a vector x with a unique 
dimension for each possible substring of n total characters 
note that n-grams may include whitespace and are 
overlapping we use binary feature scoring which has been shown 
to be most effective for a variety of spam detection 
methods we normalize the vectors with the euclidean 
norm furthermore with email data we reduce the impact 
of long messages for example with attachments by 
considering only the first characters of each string for blog 
comments and splogs we consider the whole text 
including any meta-data such as html tags as given no other 
feature selection or domain knowledge was used 
 tuning the tradeoff parameter c 
the svm tradeoff parameter c must be tuned to balance 
the potentially conflicting goals of maximizing the 
margin and minimizing the training error early work on svm 
based spam detection showed that high values of c give 
best performance with binary features later work has not 
always followed this lead a low default setting of c was 
used on splog detection and also on email spam 
following standard machine learning practice we tuned c 
on separate tuning data not used for later testing we used 
the publicly available spamassassin email spam data set 
and created an online learning task by randomly interleaving 
all labeled messages to create a single ordered set 
for tuning we performed a coarse parameter search for c 
using powers of ten from to we used the online 
svm described above and tested both binary bag of words 
vectors and n-gram vectors with n { } we used the 
first characters of each message which included header 
information body of the email and possibly attachments 
following the recommendation of we use area under 
the roc curve as our evaluation measure the results see 
figure agree with there is a plateau of high 
performance achieved with all values of c ≥ and performance 
degrades sharply with c for the remainder of our 
experiments with svms in this paper we set c we 
will return to the observation that very high values of c do 
not degrade performance as support for the intuition that 
relaxed svms should perform well on spam 
table results for email spam filtering with 
online svm on benchmark data sets score reported 
is -roca where is optimal 
trec p- trec p 
onsvm words - - 
 -grams - - 
 -grams - - 
spamprobe - - 
bogofilter - - 
trec winners - - 
 -ensemble - - 
table results for blog comment spam detection 
using svms and leave one out cross validation 
we report the same performance measures as in the 
prior work for meaningful comparison 
accuracy precision recall 
svm c words 
 -grams 
 -grams 
prior best method 
 email spam and online svms 
with c tuned on a separate tuning set we then tested the 
performance of online svms in spam detection we used 
two large benchmark data sets of email spam as our test 
corpora these data sets are the trec public data set 
trec p- of messages and the trec public 
data sets trec p containing messages in english 
 we do not report our strong results on the trec c corpus 
of chinese messages as there have been questions raised over 
the validity of this test set we used the canonical ordering 
provided with each of these data sets for fair comparison 
results for these experiments with bag of words vectors 
and and n-gram vectors appear in table to compare our 
results with previous scores on these data sets we use the 
same -roca measure described in which is one 
minus the area under the roc curve expressed as a percent 
this measure shows the percent chance of error made by 
a classifier asserting that one message is more likely to be 
spam than another these results show that online svms 
do give state of the art performance on email spam the 
only known system that out-performs the online svms on 
the trec p- data set is a recent ensemble classifier which 
combines the results of unique spam filters to 
our knowledge the online svm has out-performed every 
other single filter on these data sets including those using 
bayesian methods compression models logistic 
regression and perceptron variants the trec 
competition winners and open source email spam filters 
bogofilter v and spamprobe v d 
 blog comment spam and svms 
blog comment spam is similar to email spam in many 
regards and content-based methods have been proposed for 
detecting these spam comments however large 
benchmark data sets of labeled blog comment spam do not yet 
exist thus we run experiments on the only publicly available 
data set we know of which was used in content-based blog 
table results for splog vs blog detection using 
svms and leave one out cross validation we 
report the same evaluation measures as in the prior 
work for meaningful comparison 
features precision recall f 
svm c words 
 -grams 
 -grams 
prior svm with words 
 -grams 
words urls 
comment spam detection experiments by because of 
the small size of the data set and because prior researchers 
did not conduct their experiments in an on-line setting we 
test the performance of linear svms using leave-one-out 
cross validation with svm-light a standard open-source 
svm implementation we use the parameter setting 
c with the same feature space mappings as above 
we report accuracy precision and recall to compare these to 
the results given on the same data set by these results 
 see table show that svms give superior performance on 
this data set to the prior methodology 
 splogs and svms 
as with blog comment spam there is not yet a large 
publicly available benchmark corpus of labeled splog detection 
test data however the authors of kindly provided us 
with the labeled data set of blogs and splogs that they 
used to test content-based splog detection using svms the 
only difference between our methodology and that of is 
that they used default parameters for c which svm-light 
sets to 
avg x 
 for normalized vectors this default value 
sets c they also tested several domain-informed 
feature mappings such as giving special features to url tags 
for our experiments we used the same feature mappings 
as above and tested the effect of setting c as with 
the methodology of we performed leave one out cross 
validation for apples-to-apples comparison on this data the 
results see table show that a high value of c produces 
higher performance for the same feature space mappings 
and even enables the simple -gram mapping to out-perform 
the previous best mapping which incorporated domain 
knowledge by using words and urls 
 computational cost 
the results presented in this section demonstrate that 
linfeatures trec p trec p- 
words s s 
 -grams s s 
 -grams s s 
corpus size 
table execution time for online svms with email 
spam detection in cpu seconds these times do 
not include the time spent mapping strings to 
feature vectors the number of examples in each data 
set is given in the last row as corpus size 
a 
b 
figure visualizing the effect of c 
hyperplane a maximizes the margin while accepting a 
small amount of training error this corresponds 
to setting c to a low value hyperplane b 
accepts a smaller margin in order to reduce 
training error this corresponds to setting c to a high 
value content-based spam filtering appears to do 
best with high values of c 
ear svms give state of the art performance on content-based 
spam filtering however this performance comes at a price 
although the blog comment spam and splog data sets are 
too small for the quadratic training time of svms to 
appear problematic the email data sets are large enough to 
illustrate the problems of quadratic training cost 
table shows computation time versus data set size for 
each of the online learning tasks on same system the 
training cost of svms are prohibitive for large-scale content 
based spam detection or a large blog host in the 
following section we reduce this cost by relaxing the expensive 
requirements of svms 
 relaxed online svms rosvm 
one of the main benefits of svms is that they find a 
decision hyperplane that maximizes the margin between classes 
in the data space maximizing the margin is expensive 
typically requiring quadratic training time in the number 
of training examples however as we saw in the previous 
section the task of content-based spam detection is best 
achieved by svms with a high value of c setting c to a 
high value for this domain implies that minimizing 
training loss is more important than maximizing the margin see 
figure 
thus while svms do create high performance spam 
filters applying them in practice is overkill the full margin 
maximization feature that they provide is unnecessary and 
relaxing this requirement can reduce computational cost 
we propose three ways to relax online svms 
 reduce the size of the optimization problem by only 
optimizing over the last p examples 
 reduce the number of training updates by only 
training on actual errors 
 reduce the number of iterations in the iterative svm 
given dataset x x y xn yn c m p 
initialize w b seendata { } 
for each xi ∈ x do 
classify xi using f xi sign w xi b 
if yif xi m 
find w b with smo on seendata 
using w b as seed hypothesis 
set w b w b 
if size seendata p 
remove oldest example from seendata 
add xi to seendata 
done 
figure pseudo-code for relaxed online svm 
solver by allowing an approximate solution to the 
optimization problem 
as we describe in the remainder of this subsection all of 
these methods trade statistical robustness for reduced 
computational cost experimental results reported in the 
following section show that they equal or approach the 
performance of full online svms on content-based spam detection 
 reducing problem size 
in the full online svms we re-optimize over the full set 
of seen data on every update which becomes expensive as 
the number of seen data points grows we can bound this 
expense by only considering the p most recent examples for 
optimization see figure for pseudo-code 
note that this is not equivalent to training a new svm 
classifier from scratch on the p most recent examples 
because each successive optimization problem is seeded with 
the previous hypothesis w this hypothesis may contain 
values for features that do not occur anywhere in the p most 
recent examples and these will not be changed this allows 
the hypothesis to remember rare but informative features 
that were learned further than p examples in the past 
formally the optimization problem is now defined most 
clearly in the dual form in this case the original 
softmargin svm is computed by maximizing at example n 
w α 
nx 
i 
αi − 
 
 
nx 
i j 
αiαjyiyj xi xj 
subject to the previous constraints 
∀i ∈ { n} ≤ αi ≤ c and 
nx 
i 
αiyi 
to this we add the additional lookback buffer constraint 
∀j ∈ { n − p } αj cj 
where cj is a constant fixed as the last value found for αj 
while j n − p thus the margin found by an 
optimization is not guaranteed to be one that maximizes the margin 
for the global data set of examples {x xn } but rather 
one that satisfies a relaxed requirement that the margin be 
maximized over the examples { x n−p xn} subject 
to the fixed constraints on the hyperplane that were found 
in previous optimizations over examples {x x n−p } 
 for completeness when p ≥ n define n − p this 
set of constraints reduces the number of free variables in the 
optimization problem reducing computational cost 
 reducing number of updates 
as noted before the kkt conditions show that a well 
classified example will not change the hypothesis thus it is 
not necessary to re-train when we encounter such an 
example under the kkt conditions an example xi is considered 
well-classified when yif xi if we re-train on every 
example that is not well-classified our hyperplane will be 
guaranteed to be optimal at every step 
the number of re-training updates can be reduced by 
relaxing the definition of well classified an example xi is 
now considered well classified when yif xi m for some 
 ≤ m ≤ here each update still produces an optimal 
hyperplane the learner may encounter an example that lies 
within the margins but farther from the margins than m 
such an example means the hypothesis is no longer globally 
optimal for the data set but it is considered good enough 
for continued use without immediate retraining 
this update procedure is similar to that used by 
variants of the perceptron algorithm in the extreme case 
we can set m which creates a mistake driven online 
svm in the experimental section we show that this 
version of online svms which updates only on actual errors 
does not significantly degrade performance on content-based 
spam detection but does significantly reduce cost 
 reducing iterations 
as an iterative solver smo makes repeated passes over 
the data set to optimize the objective function smo has 
one main loop which can alternate between passing over 
the entire data set or the smaller active set of current 
support vectors successive iterations of this loop bring 
the hyperplane closer to an optimal value however it is 
possible that these iterations provide less benefit than their 
expense justifies that is a close first approximation may 
be good enough we introduce a parameter t to control the 
maximum number of iterations we allow as we will see in 
the experimental section this parameter can be set as low 
as with little impact on the quality of results providing 
computational savings 
 experiments 
in section we argued that the strong performance on 
content-based spam detection with svms with a high value 
of c show that the maximum margin criteria is overkill 
incurring unnecessary computational cost in section we 
proposed rosvm to address this issue as both of these 
methods trade away guarantees on the maximum margin 
hyperplane in return for reduced computational cost in this 
section we test these methods on the same benchmark data 
sets to see if state of the art performance may be achieved by 
these less costly methods we find that rosvm is capable 
of achieving these high levels of performance with greatly 
reduced cost our main tests on content-based spam 
detection are performed on large benchmark sets of email data 
we then apply these methods on the smaller data sets of 
blog comment spam and blogs with similar performance 
 rosvm tests 
in section we proposed three approaches for reducing 
the computational cost of online smo reducing the 
prob 
 
 
 
 
 
 -roca 
buffer size 
trec p- 
trec p 
 
 
 
 
 
 
 
cpusec 
buffer size 
trec p- 
trec p 
figure reduced size tests 
lem size reducing the number of optimization iterations 
and reducing the number of training updates each of these 
approaches relax the maximum margin criteria on the global 
set of previously seen data here we test the effect that each 
of these methods has on both effectiveness and efficiency in 
each of these tests we use the large benchmark email data 
sets trec p- and trec p 
 testing reduced size 
for our first rosvm test we experiment on the effect 
of reducing the size of the optimization problem by only 
considering the p most recent examples as described in the 
previous section for this test we use the same -gram 
mappings as for the reference experiments in section with the 
same value c we test a range of values p in a coarse 
grid search figure reports the effect of the buffer size p in 
relationship to the -roca performance measure top 
and the number of cpu seconds required bottom 
the results show that values of p do result in 
degraded performance although they evaluate very quickly 
however p values from to perform almost as 
well as the original online smo represented here as p 
 at dramatically reduced computational cost 
these results are important for making state of the art 
performance on large-scale content-based spam detection 
practical with online svms ordinarily the training time 
would grow quadratically with the number of seen examples 
however fixing a value of p ensures that the training time 
is independent of the size of the data set furthermore a 
lookback buffer allows the filter to adjust to concept drift 
 
 
 
 
 
 
 -roca 
max iters 
trec p 
trec p- 
 
 
 
 
 
 
cpusec 
max iters 
trec p 
trec p- 
figure reduced iterations tests 
 testing reduced iterations 
in the second rosvm test we experiment with reducing 
the number of iterations our initial tests showed that the 
maximum number of iterations used by online smo was 
rarely much larger than on content-based spam detection 
thus we tested values of t { ∞} other parameters 
were identical to the original online svm tests 
the results on this test were surprisingly stable see 
figure reducing the maximum number of smo iterations 
per update had essentially no impact on classification 
performance but did result in a moderate increase in speed this 
suggests that any additional iterations are spent attempting 
to find improvements to a hyperplane that is already very 
close to optimal these results show that for content-based 
spam detection we can reduce computational cost by 
allowing only a single smo iteration that is t with 
effectively equivalent performance 
 testing reduced updates 
for our third rosvm experiment we evaluate the impact 
of adjusting the parameter m to reduce the total number of 
updates as noted before when m the hyperplane is 
globally optimal at every step reducing m allows a slightly 
inconsistent hyperplane to persist until it encounters an 
example for which it is too inconsistent we tested values of 
m from to at increments of note that we used 
p to decrease the cost of evaluating these tests 
the results for these tests are appear in figure and 
show that there is a slight degradation in performance with 
reduced values of m and that this degradation in 
performance is accompanied by an increase in efficiency values of 
 
 
 
 
 
 
 -roca 
m 
trec p- 
trec p 
 
 
 
 
 
 
 
 
 
cpusec 
m 
trec p- 
trec p 
figure reduced updates tests 
m give effectively equivalent performance as m 
and still reduce cost 
 online svms and rosvm 
we now compare rosvm against online svms on the 
email spam blog comment spam and splog detection tasks 
these experiments show comparable performance on these 
tasks at radically different costs in the previous section 
the effect of the different relaxation methods was tested 
separately here we tested these methods together to 
create a full implementation of rosvm we chose the values 
p t m for the email spam detection 
tasks note that these parameter values were selected as 
those allowing rosvm to achieve comparable performance 
results with online svms in order to test total difference 
in computational cost the splog and blog data sets were 
much smaller so we set p for these tasks to allow 
meaningful comparisons between the reduced size and full 
size optimization problems because these values were not 
hand-tuned both generalization performance and runtime 
results are meaningful in these experiments 
 experimental setup 
we compared online svms and rosvm on email spam 
blog comment spam and splog detection for the email 
spam we used the two large benchmark corpora trec p- 
and trec p in the standard online ordering we randomly 
ordered both the blog comment spam corpus and the splog 
corpus to create online learning tasks note that this is a 
different setting than the leave-one-out cross validation task 
presented on these corpora in section - the results are 
not directly comparable however this experimental design 
table email spam benchmark data these 
results compare online svm and rosvm on email 
spam detection using binary -gram feature space 
score reported is -roca where is optimal 
trec p- trec p- trec p trec p 
 -roc cpus -roc cpus 
onsvm 
rosvm 
table blog comment spam these results 
comparing online svm and rosvm on blog comment 
spam detection using binary -gram feature space 
acc prec recall f cpus 
onsvm 
rosvm 
does allow meaningful comparison between our two online 
methods on these content-based spam detection tasks 
we ran each method on each task and report the results 
in tables and note that the cpu time reported for 
each method was generated on the same computing system 
this time reflects only the time needed to complete online 
learning on tokenized data we do not report the time taken 
to tokenize the data into binary -grams as this is the same 
additive constant for all methods on each task in all cases 
rosvm was significantly less expensive computationally 
 discussion 
the comparison results shown in tables and are 
striking in two ways first they show that the performance 
of online svms can be matched and even exceeded by 
relaxed margin methods second they show a dramatic 
disparity in computational cost rosvm is an order of 
magnitude more efficient than the normal online svm and gives 
comparable results furthermore the fixed lookback buffer 
ensures that the cost of each update does not depend on the 
size of the data set already seen unlike online svms note 
the blog and splog data sets are relatively small and results 
on these data sets must be considered preliminary overall 
these results show that there is no need to pay the high cost 
of svms to achieve this level of performance on 
contentbased detection of spam rosvms offer a far cheaper 
alternative with little or no performance loss 
 conclusions 
in the past academic researchers and industrial 
practitioners have disagreed on the best method for online 
contentbased detection of spam on the web we have presented one 
resolution to this debate online svms do indeed 
protable splog data set these results compare 
online svm and rosvm on splog detection using 
binary -gram feature space 
acc prec recall f cpus 
onsvm 
rosvm 
duce state-of-the-art performance on this task with proper 
adjustment of the tradeoff parameter c but with cost that 
grows quadratically with the size of the data set the high 
values of c required for best performance with svms show 
that the margin maximization of online svms is overkill for 
this task thus we have proposed a less expensive 
alternative rosvm that relaxes this maximum margin 
requirement and produces nearly equivalent results these 
methods are efficient enough for large-scale filtering of 
contentbased spam in its many forms 
it is natural to ask why the task of content-based spam 
detection gets strong performance from rosvm after all not 
all data allows the relaxation of svm requirements we 
conjecture that email spam blog comment spam and splogs all 
share the characteristic that a subset of features are 
particularly indicative of content being either spam or not spam 
these indicative features may be sparsely represented in the 
data set because of spam methods such as word obfuscation 
in which common spam words are intentionally misspelled in 
an attempt to reduce the effectiveness of word-based spam 
detection maximizing the margin may cause these sparsely 
represented features to be ignored creating an overall 
reduction in performance it appears that spam data is highly 
separable allowing rosvm to be successful with high 
values of c and little effort given to maximizing the margin 
future work will determine how applicable relaxed svms 
are to the general problem of text classification 
finally we note that the success of relaxed svm methods 
for content-based spam detection is a result that depends 
on the nature of spam data which is potentially subject to 
change although it is currently true that ham and spam 
are linearly separable given an appropriate feature space 
this assumption may be subject to attack while our current 
methods appear robust against primitive attacks along these 
lines such as the good word attack we must explore the 
feasibility of more sophisticated attacks 
 references 
 a bratko and b filipic spam filtering using 
compression models technical report ijs-dp- 
department of intelligent systems jozef stefan 
institute l jubljana slovenia 
 g cauwenberghs and t poggio incremental and 
decremental support vector machine learning in 
nips pages - 
 g v cormack trec spam track overview in 
to appear in the fifteenth text retrieval 
conference trec proceedings 
 g v cormack and a bratko batch and on-line 
spam filter comparison in proceedings of the third 
conference on email and anti-spam ceas 
 g v cormack and t r lynam trec spam 
track overview in the fourteenth text retrieval 
conference trec proceedings 
 g v cormack and t r lynam on-line supervised 
spam filter evaluation technical report david r 
cheriton school of computer science university of 
waterloo canada february 
 n cristianini and j shawe-taylor an introduction to 
support vector machines cambridge university press 
 
 d decoste and k wagstaff alpha seeding for 
support vector machines in kdd proceedings of 
the sixth acm sigkdd international conference on 
knowledge discovery and data mining pages - 
 
 h drucker v vapnik and d wu support vector 
machines for spam categorization ieee transactions 
on neural networks - 
 j goodman and w yin online discriminative spam 
filter training in proceedings of the third conference 
on email and anti-spam ceas 
 p graham a plan for spam 
 p graham better bayesian filtering 
 z gyongi and h garcia-molina spam it s not just 
for inboxes anymore computer - 
 t joachims text categorization with suport vector 
machines learning with many relevant features in 
ecml proceedings of the th european 
conference on machine learning pages - 
 
 t joachims training linear svms in linear time in 
kdd proceedings of the th acm sigkdd 
international conference on knowledge discovery and 
data mining pages - 
 j kivinen a smola and r williamson online 
learning with kernels in advances in neural 
information processing systems pages - 
mit press 
 p kolari t finin and a joshi svms for the 
blogosphere blog identification and splog detection 
aaai spring symposium on computational 
approaches to analyzing weblogs 
 w krauth and m m´ezard learning algorithms with 
optimal stability in neural networks journal of 
physics a - 
 t lynam g cormack and d cheriton on-line 
spam filter fusion in sigir proceedings of the 
 th annual international acm sigir conference on 
research and development in information retrieval 
pages - 
 v metsis i androutsopoulos and g paliouras 
spam filtering with naive bayes - which naive bayes 
third conference on email and anti-spam ceas 
 
 g mishne d carmel and r lempel blocking blog 
spam with language model disagreement proceedings 
of the st international workshop on adversarial 
information retrieval on the web airweb may 
 
 j platt sequenital minimal optimization a fast 
algorithm for training support vector machines in 
b scholkopf c burges and a smola editors 
advances in kernel methods - support vector 
learning mit press 
 b scholkopf and a smola learning with kernels 
support vector machines regularization 
optimization and beyond mit press 
 g l wittel and s f wu on attacking statistical 
spam filters ceas first conference on email and 
anti-spam 
impedance coupling in content-targeted advertising 
berthier ribeiro-neto 
computer science department 
federal university of minas gerais 
belo horizonte brazil 
berthier dcc ufmg br 
marco cristo 
computer science department 
federal university of minas gerais 
belo horizonte brazil 
marco dcc ufmg br 
paulo b golgher 
akwan information technologies 
av abra˜ao caram - pampulha 
belo horizonte brazil 
golgher akwan com br 
edleno silva de moura 
computer science department 
federal university of amazonas 
manaus brazil 
edleno dcc ufam edu br 
abstract 
the current boom of the web is associated with the revenues 
originated from on-line advertising while search-based 
advertising is dominant the association of ads with a web 
page during user navigation is becoming increasingly 
important in this work we study the problem of associating 
ads with a web page referred to as content-targeted 
advertising from a computer science perspective we assume that 
we have access to the text of the web page the keywords 
declared by an advertiser and a text associated with the 
advertiser s business using no other information and 
operating in fully automatic fashion we propose ten strategies 
for solving the problem and evaluate their effectiveness our 
methods indicate that a matching strategy that takes into 
account the semantics of the problem referred to as aak 
for ads and keywords can yield gains in average precision 
figures of compared to a trivial vector-based strategy 
further a more sophisticated impedance coupling strategy 
which expands the text of the web page to reduce 
vocabulary impedance with regard to an advertisement can yield 
extra gains in average precision of these are first 
results they suggest that great accuracy in content-targeted 
advertising can be attained with appropriate algorithms 
categories and subject descriptors 
h information storage and retrieval 
information search and retrieval i pattern recognition 
applications-text processing 
general terms 
algorithms experimentation 
 introduction 
the emergence of the internet has opened up new 
marketing opportunities in fact a company has now the possibility 
of showing its advertisements ads to millions of people at a 
low cost during the s many companies invested heavily 
on advertising in the internet with apparently no concerns 
about their investment return this situation radically 
changed in the following decade when the failure of many 
web companies led to a dropping in supply of cheap venture 
capital and a considerable reduction in on-line advertising 
investments 
it was clear then that more effective strategies for on-line 
advertising were required for that it was necessary to take 
into account short-term and long-term interests of the users 
related to their information needs as a consequence 
many companies intensified the adoption of intrusive 
techniques for gathering information of users mostly without 
their consent this raised privacy issues which 
stimulated the research for less invasive measures 
more recently internet information gatekeepers as for 
example search engines recommender systems and 
comparison shopping services have employed what is called paid 
placement strategies in such methods an advertiser 
company is given prominent positioning in advertisement 
lists in return for a placement fee amongst these methods 
the most popular one is a non-intrusive technique called 
keyword targeted marketing in this technique keywords 
extracted from the user s search query are matched against 
keywords associated with ads provided by advertisers a 
ranking of the ads which also takes into consideration the 
amount that each advertiser is willing to pay is computed 
the top ranked ads are displayed in the search result page 
together with the answers for the user query 
the success of keyword targeted marketing has motivated 
information gatekeepers to offer their advertisement services 
in different contexts for example as shown in figure 
relevant ads could be shown to users directly in the pages of 
information portals the motivation is to take advantage of 
 
the users immediate information interests at browsing time 
the problem of matching ads to a web page that is browsed 
which we also refer to as content-targeted advertising 
is different from that of keyword marketing in this case 
instead of dealing with users keywords we have to use the 
contents of a web page to decide which ads to display 
figure example of content-based advertising in 
the page of a newspaper the middle slice of the 
page shows the beginning of an article about the 
launch of a dvd movie at the bottom slice we can 
see advertisements picked for this page by google s 
content-based advertising system adsense 
it is important to notice that paid placement 
advertising strategies imply some risks to information gatekeepers 
for instance there is the possibility of a negative impact 
on their credibility which at long term can demise their 
market share this makes investments in the quality of 
ad recommendation systems even more important to 
minimize the possibility of exhibiting ads unrelated to the user s 
interests by investing in their ad systems information 
gatekeepers are investing in the maintenance of their credibility 
and in the reinforcement of a positive user attitude towards 
the advertisers and their ads further that can 
translate into higher clickthrough rates that lead to an increase in 
revenues for information gatekeepers and advertisers with 
gains to all parts 
in this work we focus on the problem of content-targeted 
advertising we propose new strategies for associating ads 
with a web page five of these strategies are referred to as 
matching strategies they are based on the idea of matching 
the text of the web page directly to the text of the ads and 
its associated keywords five other strategies which we here 
introduce are referred to as impedance coupling strategies 
they are based on the idea of expanding the web page with 
new terms to facilitate the task of matching ads and web 
pages this is motivated by the observation that there is 
frequently a mismatch between the vocabulary of a web page 
and the vocabulary of an advertisement we say that there 
is a vocabulary impedance problem and that our technique 
provides a positive effect of impedance coupling by reducing 
the vocabulary impedance further all our strategies rely 
on information that is already available to information 
gatekeepers that operate keyword targeted advertising systems 
thus no other data from the advertiser is required 
using a sample of a real case database with over 
ads and web pages selected for testing we evaluate our 
ad recommendation strategies first we evaluate the five 
matching strategies they match ads to a web page 
using a standard vector model and provide what we may call 
trivial solutions our results indicate that a strategy that 
matches the ad plus its keywords to a web page requiring 
the keywords to appear in the web page provides 
improvements in average precision figures of roughly relative 
to a strategy that simply matches the ads to the web page 
such strategy which we call aak for ads and keywords 
is then taken as our baseline 
following we evaluate the five impedance coupling 
strategies they are based on the idea of expanding the ad and 
the web page with new terms to reduce the vocabulary 
impedance between their texts our results indicate that it 
is possible to generate extra improvements in average 
precision figures of roughly relative to the aak strategy 
the paper is organized as follows in section we 
introduce five matching strategies to solve content-targeted 
advertising in section we present our impedance 
coupling strategies in section we describe our experimental 
methodology and datasets and discuss our results in 
section we discuss related work in section we present our 
conclusions 
 matching strategies 
keyword advertising relies on matching search queries to 
ads and its associated keywords context-based 
advertising which we address here relies on matching ads and its 
associated keywords to the text of a web page 
given a certain web page p which we call triggering page 
our task is to select advertisements related to the contents 
of p without loss of generality we consider that an 
advertisement ai is composed of a title a textual description 
and a hyperlink to illustrate for the first ad by google 
shown in figure the title is star wars trilogy full 
the description is get this popular dvd free free w free 
shopping sign up now and the hyperlink points to the site 
www freegiftworld com advertisements can be grouped 
by advertisers in groups called campaigns such that a 
campaign can have one or more advertisements 
given our triggering page p and a set a of ads a simple 
way of ranking ai ∈ a with regard to p is by matching the 
contents of p to the contents of ai for this we use the vector 
space model as discussed in the immediately following 
in the vector space model queries and documents are 
represented as weighted vectors in an n-dimensional space let 
wiq be the weight associated with term ti in the query q 
and wij be the weight associated with term ti in the 
document dj then q w q w q wiq wnq and dj 
 w j w j wij wnj are the weighted vectors used to 
represent the query q and the document dj these weights 
can be computed using classic tf-idf schemes in such schemes 
weights are taken as the product between factors that 
quantify the importance of a term in a document given by the 
term frequency or tf factor and its rarity in the whole 
collection given by the inverse document factor or idf factor 
see for details the ranking of the query q with regard 
to the document dj is computed by the cosine similarity 
 
formula that is the cosine of the angle between the two 
corresponding vectors 
sim q dj 
q dj 
 q × dj 
 
pn 
i wiq · wij 
qpn 
i w 
iq 
qpn 
i w 
ij 
 
by considering p as the query and ai as the document we 
can rank the ads with regard to the web page p this is our 
first matching strategy it is represented by the function ad 
given by 
ad p ai sim p ai 
where ad stands for direct match of the ad composed by 
title and description and sim p ai is computed according 
to eq 
in our second method we use other source of evidence 
provided by the advertisers the keywords with each 
advertisement ai an advertiser associates a keyword ki which 
may be composed of one or more terms we denote the 
association between an advertisement ai and a keyword ki 
as the pair ai ki ∈ k where k is the set of associations 
made by the advertisers in the case of keyword targeted 
advertising such keywords are used to match the ads to the 
user queries in here we use them to match ads to the web 
page p this provides our second method for ad matching 
given by 
kw p ai sim p ki 
where ai ki ∈ k and kw stands for match the ad 
keywords 
we notice that most of the keywords selected by 
advertisers are also present in the ads associated with those 
keywords for instance in our advertisement test collection 
this is true for of the ads thus instead of using the 
keywords as matching devices we can use them to emphasize 
the main concepts in an ad in an attempt to improve our 
ad strategy this leads to our third method of ad matching 
given by 
ad kw p ai sim p ai ∪ ki 
where ai ki ∈ k and ad kw stands for match the ad and 
its keywords 
finally it is important to notice that the keyword ki 
associated with ai could not appear at all in the triggering page 
p even when ai is highly ranked however if we assume that 
ki summarizes the main topic of ai according to an 
advertiser viewpoint it can be interesting to assure its presence 
in p this reasoning suggests that requiring the occurrence 
of the keyword ki in the triggering page p as a condition 
to associate ai with p might lead to improved results this 
leads to two extra matching strategies as follows 
andkw p ai 
 
sim p ai if ki p 
 if otherwise 
ad andkw p ai aak p ai 
 
sim p ai ∪ ki if ki p 
 if otherwise 
where ai ki ∈ k andkw stands for match the ad keywords 
and force their appearance and ad andkw or aak for ads 
and keywords stands for match the ad its keywords and 
force their appearance 
as we will see in our results the best among these simple 
methods is aak thus it will be used as baseline for our 
impedance coupling strategies which we now discuss 
 impedance coupling strategies 
two key issues become clear as one plays with the 
contenttargeted advertising problem first the triggering page 
normally belongs to a broader contextual scope than that of the 
advertisements second the association between a good 
advertisement and the triggering page might depend on a topic 
that is not mentioned explicitly in the triggering page 
the first issue is due to the fact that web pages can be 
about any subject and that advertisements are concise in 
nature that is ads tend to be more topic restricted than 
web pages the second issue is related to the fact that as 
we later discuss most advertisers place a small number of 
advertisements as a result we have few terms describing 
their interest areas consequently these terms tend to be 
of a more general nature for instance a car shop probably 
would prefer to use car instead of super sport to describe 
its core business topic as a consequence many specific 
terms that appear in the triggering page find no match in 
the advertisements to make matters worst a page might 
refer to an entity or subject of the world through a label 
that is distinct from the label selected by an advertiser to 
refer to the same entity 
a consequence of these two issues is that vocabularies of 
pages and ads have low intersection even when an ad is 
related to a page we cite this problem from now on as 
the vocabulary impedance problem in our experiments we 
realized that this problem limits the final quality of direct 
matching strategies therefore we studied alternatives to 
reduce the referred vocabulary impedance 
for this we propose to expand the triggering pages with 
new terms figure illustrates our intuition we already 
know that the addition of keywords selected by the 
advertiser to the ads leads to improved results we say that a 
keyword reduces the vocabulary impedance by providing an 
alternative matching path our idea is to add new terms 
 words to the web page p to also reduce the vocabulary 
impedance by providing a second alternative matching path 
we refer to our expansion technique as impedance coupling 
for this we proceed as follows 
expansion 
terms keyword 
vocabulary impedance 
triggering 
page p ad 
figure addition of new terms to a web page to 
reduce the vocabulary impedance 
an advertiser trying to describe a certain topic in a concise 
way probably will choose general terms to characterize that 
topic to facilitate the matching between this ad and our 
triggering page p we need to associate new general terms 
with p for this we assume that web documents similar 
to the triggering page p share common topics therefore 
 
by inspecting the vocabulary of these similar documents we 
might find good terms for better characterizing the main 
topics in the page p we now describe this idea using a 
bayesian network model depicted in figure 
r 
d d dj dk 
t t t ti tm 
 
 
figure bayesian network model for our 
impedance coupling technique 
in our model which is based on the belief network in 
the nodes represent pieces of information in the domain 
with each node is associated a binary random variable 
which takes the value to mean that the corresponding 
entity a page or terms is observed and thus relevant in our 
computations in this case we say that the information was 
observed node r represents the page r a new 
representation for the triggering page p let n be the set of the k 
most similar documents to the triggering page including the 
triggering page p itself in a large enough web collection c 
root nodes d through dk represent the documents in n 
that is the triggering page d and its k nearest neighbors 
d through dk among all pages in c there is an edge 
from node dj to node r if document dj is in n nodes 
t through tm represent the terms in the vocabulary of c 
there is an edge from node dj to a node ti if term ti occurs 
in document dj in our model the observation of the pages 
in n leads to the observation of a new representation of the 
triggering page p and to a set of terms describing the main 
topics associated with p and its neighbors 
given these definitions we can now use the network to 
determine the probability that a term ti is a good term for 
representing a topic of the triggering page p in other words 
we are interested in the probability of observing the final 
evidence regarding a term ti given that the new 
representation of the page p has been observed p ti r 
this translates into the following equation 
 
p ti r 
 
p r 
x 
d 
p ti d p r d p d 
where d represents the set of states of the document nodes 
since we are interested just in the states in which only a 
single document dj is observed and p d can be regarded as 
a constant we can rewrite eq as 
p ti r 
ν 
p r 
kx 
j 
p ti dj p r dj 
where dj represents the state of the document nodes in 
which only document dj is observed and ν is a constant 
 
to simplify our notation we represent the probabilities 
p x as p x and p x as p x 
associated with p dj eq is the general equation to 
compute the probability that a term ti is related to the 
triggering page we now define the probabilities p ti dj and 
p r dj as follows 
p ti dj η wij 
p r dj 
 
 − α j 
α sim r dj ≤ j ≤ k 
 
where η is a normalizing constant wij is the weight 
associated with term ti in the document dj and sim p dj is 
given by eq i e is the cosine similarity between p and 
dj the weight wij is computed using a classic tf-idf scheme 
and is zero if term ti does not occur in document dj notice 
that p ti dj − p ti dj and p r dj − p r dj 
by defining the constant α it is possible to determine how 
important should be the influence of the triggering page p 
to its new representation r by substituting eq and 
eq into eq we obtain 
p ti r ρ − α wi α 
kx 
j 
wij sim r dj 
where ρ η ν is a normalizing constant 
we use eq to determine the set of terms that will 
compose r as illustrated in figure let ttop be the top 
ranked term according to eq the set r is composed 
of the terms ti such that p ti r 
p ttop r 
≥ β where β is a given 
threshold in our experiments we have used β 
notice that the set r might contain terms that already occur 
in p that is while we will refer to the set r as expansion 
terms it should be clear that p ∩ r ∅ 
by using α we simply consider the terms originally 
in page p by increasing α we relax the context of the page 
p adding terms from neighbor pages turning page p into its 
new representation r this is important because sometimes 
a topic apparently not important in the triggering page offers 
a good opportunity for advertising for example consider 
a triggering page that describes a congress in london about 
digital photography although london is probably not an 
important topic in this page advertisements about hotels 
in london would be appropriate thus adding hotels to 
page p is important this suggests using α that is 
preserving the contents of p and using the terms in r to 
expand p 
in this paper we examine both approaches thus in our 
sixth method we match r the set of new expansion terms 
directly to the ads as follows 
aak t p ai aak r ai 
where aak t stands for match the ad and keywords to the 
set r of expansion terms 
in our seventh method we match an expanded page p to 
the ads as follows 
aak exp p ai aak p ∪ r ai 
where aak exp stands for match the ad and keywords to 
the expanded triggering page 
 
to improve our ad placement methods other external 
source that we can use is the content of the page h pointed to 
by the advertisement s hyperlink that is its landing page 
after all this page comprises the real target of the ad and 
perhaps could present a more detailed description of the 
product or service being advertised given that the 
advertisement ai points to the landing page hi we denote this 
association as the pair ai hi ∈ h where h is the set of 
associations between the ads and the pages they point to 
our eighth method consists of matching the triggering page 
p to the landing pages pointed to by the advertisements as 
follows 
h p ai sim p hi 
where ai hi ∈ h and h stands for match the hyperlink 
pointed to by the ad 
we can also combine this information with the more 
promising methods previously described aak and aak exp as 
follows given that ai hi ∈ h and ai ki ∈ k we have our 
last two methods 
aak h p ai 
 
sim p ai ∪ hi ∪ ki if ki p 
 if otherwise 
aak exp h p ai 
 
sim p ∪ r ai ∪ hi ∪ ki if ki p ∪ r 
 if otherwise 
where aak h stands for match ads and keywords also 
considering the page pointed by the ad and aah exp h stands 
for match ads and keywords with expanded triggering page 
also considering the page pointed by the ad 
notice that other combinations were not considered in this 
study due to space restrictions these other combinations 
led to poor results in our experimentation and for this reason 
were discarded 
 experiments 
 methodology 
to evaluate our ad placement strategies we performed 
a series of experiments using a sample of a real case ad 
collection with advertisements advertisers and 
 keywords 
 the advertisements are grouped in 
campaigns with an average of campaigns per advertiser 
for the strategies aak t and aak exp we had to 
generate a set of expansion terms for that we used a database 
of web pages crawled by the todobr search engine 
 http www todobr com br this database is composed 
of pages of the brazilian web under the domain 
 br for the strategies h aak h and aak exp h we also 
crawled the pages pointed to by the advertisers no other 
filtering method was applied to these pages besides the 
removal of html tags 
since we are initially interested in the placement of 
advertisements in the pages of information portals our test 
collection was composed of pages extracted from a 
brazilian newspaper these are our triggering pages they were 
crawled in such a way that only the contents of their 
articles was preserved as we have no preferences for particular 
 
data in portuguese provided by an on-line advertisement 
company that operates in brazil 
topics the crawled pages cover topics as diverse as politics 
economy sports and culture 
for each of our triggering pages we selected the top 
three ranked ads provided by each of our ad placement 
strategies thus for each triggering page we select no more 
than ads these top ads were then inserted in a pool 
for that triggering page each pool contained an average of 
 advertisements all advertisements in each pool were 
submitted to a manual evaluation by a group of users 
the average number of relevant advertisements per page 
pool was notice that we adopted the same pooling 
method used to evaluate the trec web-based collection 
to quantify the precision of our results we used -point 
average figures since we are not able to evaluate the 
entire ad collection recall values are relative to the set of 
evaluated advertisements 
 tuning idf factors 
we start by analyzing the impact of different idf factors 
in our advertisement collection idf factors are important 
because they quantify how discriminative is a term in the 
collection in our ad collection idf factors can be computed 
by taking ads advertisers or campaigns as documents to 
exemplify consider the computation of ad idf for a term 
ti that occurs times in a collection of ads then the 
inverse document frequency of ti is given by 
idfi log 
 
 
hence we can compute ad advertiser or campaign idf 
factors as we observe in figure for the ad strategy the best 
ranking is obtained by the use of campaign idf that is by 
calculating our idf factor so that it discriminates campaigns 
similar results were obtained for all the other methods 
 
 
 
 
 
 
 
 
 
 
precision 
recall 
campaign idf 
advertiser idf 
ad idf 
figure precision-recall curves obtained for the 
ad strategy using ad advertiser and campaign idf 
factors 
this reflects the fact that terms might be better 
discriminators for a business topic than for an specific ad this 
effect can be accomplished by calculating the factor relative 
to idf advertisers or campaigns instead of ads in fact 
campaign idf factors yielded the best results thus they will be 
used in all the experiments reported from now on 
 
 results 
matching strategies 
figure displays the results for the matching strategies 
presented in section as shown directly matching the 
contents of the ad to the triggering page ad strategy is not so 
effective the reason is that the ad contents are very noisy 
it may contain messages that do not properly describe the 
ad topics such as requisitions for user actions e g visit our 
site and general sentences that could be applied to any 
product or service e g we delivery for the whole 
country on the other hand an advertiser provided keyword 
summarizes well the topic of the ad as a consequence the 
kw strategy is superior to the ad and ad kw strategies this 
situation changes when we require the keywords to appear 
in the target web page by filtering out ads whose keywords 
do not occur in the triggering page much noise is discarded 
this makes andkw a better alternative than kw further in 
this new situation the contents of the ad becomes useful 
to rank the most relevant ads making ad andkw or aak for 
ads and keywords the best among all described methods 
for this reason we adopt aak as our baseline in the next set 
of experiments 
 
 
 
 
 
 
 
 
precision 
recall 
aak 
andkw 
kw 
ad kw 
ad 
figure comparison among our five matching 
strategies aak ads and keywords is superior 
table illustrates average precision figures for figure 
we also present actual hits per advertisement slot we call 
hit an assignment of an ad to the triggering page that 
was considered relevant by the evaluators we notice that 
our aak strategy provides a gain in average precision of 
relative to the trivial ad strategy this shows that careful 
consideration of the evidence related to the problem does 
pay off 
impedance coupling strategies 
table shows top ranked terms that occur in a page 
covering argentinean wines produced using grapes derived from 
the bordeaux region of france the p column includes the 
top terms for this page ranked according to our tf-idf 
weighting scheme the r column includes the top ranked 
expansion terms generated according to eq notice that the 
expansion terms not only emphasize important terms of the 
target page by increasing their weights such as wines and 
methods hits -pt average 
 total score gain 
ad 
ad kw 
kw 
andkw 
ad andkw aak 
table average precision figures corresponding to 
figure for our five matching strategies columns 
labelled and indicate total of hits in 
first second and third advertisement slots 
respectively the aak strategy provides improvements of 
 relative to the ad strategy 
rank p r 
term score term score 
 argentina wines 
 obtained wine 
 class whites 
 whites red 
 french grape 
 origin bordeaux 
 france acideness 
 grape argentina 
 sweet aroma 
 country blanc 
 
 wines 
- 
table top ranked terms for the triggering page 
p according to our tf-idf weighting scheme and top 
ranked terms for r the expansion terms for p 
generated according to eq ranking scores were 
normalized in order to sum up to terms marked 
with   are not shared by the sets p and r 
whites but also reveal new terms related to the main topic 
of the page such as aroma and red further they avoid 
some uninteresting terms such as obtained and country 
figure illustrates our results when the set r of 
expansion terms is used they show that matching the ads to 
the terms in the set r instead of to the triggering page p 
 aak t strategy leads to a considerable improvement over 
our baseline aak the gain is even larger when we use the 
terms in r to expand the triggering page aak exp method 
this confirms our hypothesis that the triggering page could 
have some interesting terms that should not be completely 
discarded 
finally we analyze the impact on the ranking of using the 
contents of pages pointed by the ads figure displays our 
results it is clear that using only the contents of the pages 
pointed by the ads h strategy yields very poor results 
however combining evidence from the pages pointed by the 
ads with our baseline yields improved results most 
important combining our best strategy so far aak exp with 
pages pointed by ads aak exp h strategy leads to superior 
results this happens because the two additional sources 
of evidence expansion terms and pages pointed by the ads 
are distinct and complementary providing extra and 
valuable information for matching ads to a web page 
 
 
 
 
 
 
 
 
 
 
 
precision 
recall 
aak exp 
aak t 
aak 
figure impact of using a new representation for 
the triggering page one that includes expansion 
terms 
 
 
 
 
 
 
 
 
 
 
precision 
recall 
aak exp h 
aak h 
aak 
h 
figure impact of using the contents of the page 
pointed by the ad the hyperlink 
figure and table summarize all results described in 
this section in figure we show precision-recall curves 
and in table we show -point average figures we also 
present actual hits per advertisement slot and gains in 
average precision relative to our baseline aak we notice that 
the highest number of hits in the first slot was generated by 
the method aak exp however the method with best 
overall retrieval performance was aak exp h yielding a gain in 
average precision figures of roughly over the baseline 
 aak 
 performance issues 
in a keyword targeted advertising system ads are assigned 
at query time thus the performance of the system is a very 
important issue in content-targeted advertising systems 
we can associate ads with a page at publishing or 
updating time also if a new ad comes in we might consider 
assigning this ad to already published pages in oﬄine mode 
that is we might design the system such that its 
performance depends fundamentally on the rate that new pages 
 
 
 
 
 
 
 
 
 
 
precision 
recall 
aak exp h 
aak exp 
aak t 
aak h 
aak 
h 
figure comparison among our ad placement 
strategies 
methods hits -pt average 
 total score gain 
h - 
aak 
aak h 
aak t 
aak exp 
aak exp h 
table results for our impedance coupling 
strategies 
are published and the rate that ads are added or modified 
further the data needed by our strategies page crawling 
page expansion and ad link crawling can be gathered and 
processed oﬄine not affecting the user experience thus 
from this point of view the performance is not critical and 
will not be addressed in this work 
 related work 
several works have stressed the importance of relevance 
in advertising for example in it was shown that 
advertisements that are presented to users when they are not 
interested on them are viewed just as annoyance thus 
in order to be effective the authors conclude that 
advertisements should be relevant to consumer concerns at the 
time of exposure the results in enforce this conclusion 
by pointing out that the more targeted the advertising the 
more effective it is 
therefore it is not surprising that other works have 
addressed the relevance issue for instance in it is proposed 
a system called adwiz that is able to adapt online 
advertisement to a user s short-term interests in a non-intrusive 
way contrary to our work adwiz does not directly use 
the content of the page viewed by the user it relies on search 
keywords supplied by the user to search engines and on the 
url of the page requested by the user on the other hand 
in the authors presented an intrusive approach in which 
an agent sits between advertisers and the user s browser 
allowing a banner to be placed into the currently viewed page 
in spite of having the opportunity to use the page s content 
 
the agent infers relevance based on category information and 
user s private information collected along the time 
in the authors provide a comparison between the 
ranking strategies used by google and overture for their keyword 
advertising systems both systems select advertisements by 
matching them to the keywords provided by the user in a 
search query and rank the resulting advertisement list 
according to the advertisers willingness to pay in 
particular google approach also considers the clickthrough rate 
of each advertisement as an additional evidence for its 
relevance the authors conclude that google s strategy is better 
than that used by overture as mentioned before the 
ranking problem in keyword advertising is different from that of 
content-targeted advertising instead of dealing with 
keywords provided by users in search queries we have to deal 
with the contents of a page which can be very diffuse 
finally the work in focuses on improving search 
engine results in a trec collection by means of an automatic 
query expansion method based on knn such method 
resembles our expansion approach presented in section 
our method is different from that presented by they 
expand user queries applied to a document collection with 
terms extracted from the top k documents returned as 
answer to the query in the same collection in our case we 
use two collections an advertisement and a web collection 
we expand triggering pages with terms extracted from the 
web collection and then we match these expanded pages to 
the ads from the advertisement collection by doing this we 
emphasize the main topics of the triggering pages increasing 
the possibility of associating relevant ads with them 
 conclusions 
in this work we investigated ten distinct strategies for 
associating ads with a web page that is browsed 
 contenttargeted advertising five of our strategies attempt to 
match the ads directly to the web page because of that 
they are called matching strategies the other five 
strategies recognize that there is a vocabulary impedance problem 
among ads and web pages and attempt to solve the problem 
by expanding the web pages and the ads with new terms 
because of that they are called impedance coupling 
strategies 
using a sample of a real case database with over 
thousand ads we evaluated our strategies for the five matching 
strategies our results indicated that planned consideration 
of additional evidence such as the keywords provided by the 
advertisers yielded gains in average precision figures for 
our test collection of this was obtained by a 
strategy called aak for ads and keywords which is taken as 
the baseline for evaluating our more advanced impedance 
coupling strategies 
for our five impedance coupling strategies the results 
indicate that additional gains in average precision of now 
relative to the aak strategy are possible these were 
generated by expanding the web page with new terms obtained 
using a sample web collection containing over five million 
pages and the ads with the contents of the page they point 
to a hyperlink provided by the advertisers 
these are first time results that indicate that high quality 
content-targeted advertising is feasible and practical 
 acknowledgements 
this work was supported in part by the gerindo 
project grant mct cnpq ct-info - by cnpq 
grant - berthier ribeiro-neto and by cnpq 
grant - edleno silva de moura marco cristo 
is supported by fucapi manaus am brazil 
 references 
 the google adwords google content-targeted advertising 
http adwords google com select ct faq html november 
 
 r baeza-yates and b ribeiro-neto modern information 
retrieval addison-wesley-longman st edition 
 h k bhargava and j feng paid placement strategies for 
internet search engines in proceedings of the eleventh 
international conference on world wide web pages - 
acm press 
 e p chan s garcia and s roukos trec- ad-hoc retrieval 
using k nearest-neighbors re-scoring in the fifth text 
retrieval conference trec- national institute of 
standards and technology nist november 
 j feng h k bhargava and d pennock comparison of 
allocation rules for paid placement advertising in search 
engines in proceedings of the th international conference on 
electronic commerce pages - acm press 
 d hawking n craswell and p b thistlewaite overview of 
trec- very large collection track in the seventh text 
retrieval conference trec- pages - gaithersburg 
maryland usa november 
 y kohda and s endo ubiquitous advertising on the www 
merging advertisement on the browser comput netw isdn 
syst - - 
 m langheinrich a nakamura n abe t kamba and 
y koseki unintrusive customization techniques for web 
advertising comput networks - - 
 t p novak and d l hoffman new metrics for new media 
toward the development of web measurement standards world 
wide web j - 
 j pearl probabilistic reasoning in intelligent systems 
networks of plausible inference morgan kaufmann publishers 
 nd edition 
 b ribeiro-neto and r muntz a belief network model for ir 
in proceedings of the th annual international acm sigir 
conference on research and development in information 
retrieval pages - zurich switzerland august 
 a silva e veloso p golgher b ribeiro-neto a laender 
and n ziviani cobweb - a crawler for the brazilian web in 
proceedings of the string processing and information 
retrieval symposium spire pages - cancun 
mexico september 
 h turtle and w b croft evaluation of an inference 
network-based retrieval model acm transactions on 
information systems - july 
 c wang p zhang r choi and m daeredita understanding 
consumers attitude toward advertising in eighth americas 
conference on information systems pages - august 
 
 m weideman ethical issues on content distribution to digital 
consumers via paid placement as opposed to website visibility 
in search engine results in the seventh ethicomp 
international conference on the social and ethical impacts 
of information and communication technologies pages 
 - troubador publishing ltd april 
 m weideman and t haig-smith an investigation into search 
engines as a form of targeted advert delivery in proceedings of 
the annual research conference of the south african 
institute of computer scientists and information technologists 
on enablement through technology pages - south 
african institute for computer scientists and information 
technologists 
 y yang expert network effective and efficient learning from 
human decisions in text categorization and retrieval in w b 
croft and e c j van rijsbergen editors proceedings of the 
 rd annual international acm sigir conference on 
research and development in information retrieval pages 
 - springer-verlag 
 
knowledge-intensive conceptual retrieval and passage 
extraction of biomedical literature 
wei zhou clement yu 
department of computer science 
university of illinois at chicago 
wzhou  uic edu 
yu cs uic edu 
neil smalheiser vetle torvik 
department of psychiatry and 
psychiatric institute mc 
university of illinois at chicago 
{neils vtorvik} uic edu 
jie hong 
division of epidemiology and 
biostatistics school of public health 
university of illinois at chicago 
jhong  uic edu 
abstract 
this paper presents a study of incorporating domain-specific 
knowledge i e information about concepts and relationships 
between concepts in a certain domain in an information retrieval 
 ir system to improve its effectiveness in retrieving biomedical 
literature the effects of different types of domain-specific 
knowledge in performance contribution are examined based on 
the trec platform we show that appropriate use of 
domainspecific knowledge in a proposed conceptual retrieval model 
yields about improvement over the best reported result in 
passage retrieval in the genomics track of trec 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval - retrieval models query formulation information 
filtering h information storage and retrieval content 
analysis and indexing - thesauruses 
general terms 
algorithms performance experimentation 
 introduction 
biologists search for literature on a daily basis for most 
biologists pubmed an online service of u s national library of 
medicine nlm is the most commonly used tool for searching 
the biomedical literature pubmed allows for keyword search by 
using boolean operators for example if one desires documents on 
the use of the drug propanolol in the disease hypertension a 
typical pubmed query might be propanolol and hypertension 
which will return all the documents having the two keywords 
keyword search in pubmed is effective if the query is well-crafted 
by the users using their expertise however information needs of 
biologists in some cases are expressed as complex questions 
 which pubmed is not designed to handle while nlm does 
maintain an experimental tool for free-text queries it is still 
based on pubmed keyword search 
the genomics track of the text retrieval conference 
 trec provides a common platform to assess the methods and 
techniques proposed by various groups for biomedical information 
retrieval the queries were collected from real biologists and they 
are expressed as complex questions such as how do mutations in 
the huntingtin gene affect huntington s disease the document 
collection contains highwire full-text documents in 
html format systems from participating groups are expected to 
find relevant passages within the full-text documents a passage is 
defined as any span of text that does not include the html 
paragraph tag i e p or p 
we approached the problem by utilizing domain-specific 
knowledge in a conceptual retrieval model domain-specific 
knowledge in this paper refers to information about concepts and 
relationships between concepts in a certain domain we assume 
that appropriate use of domain-specific knowledge might improve 
the effectiveness of retrieval for example given a query what is 
the role of gene prnp in the mad cow disease expanding the 
gene symbol prnp with its synonyms prp prpsc and 
prion protein more relevant documents might be retrieved 
pubmed and many other biomedical systems also 
make use of domain-specific knowledge to improve retrieval 
effectiveness 
intuitively retrieval on the level of concepts should outperform 
bag-of-words approaches since the semantic relationships 
among words in a concept are utilized in some recent studies 
 positive results have been reported for this hypothesis in 
this paper concepts are entry terms of the ontology medical 
subject headings mesh a controlled vocabulary maintained by 
nlm for indexing biomedical literature or gene symbols in the 
entrez gene database also from nlm a concept could be a word 
such as the gene symbol prnp or a phrase such as mad cow 
diseases in the conceptual retrieval model presented in this 
paper the similarity between a query and a document is measured 
on both concept and word levels 
this paper makes two contributions 
 we propose a conceptual approach to utilize domain-specific 
knowledge in an ir system to improve its effectiveness in 
retrieving biomedical literature based on this approach our 
system achieved significant improvement over the best 
reported result in passage retrieval in the genomics track of 
trec 
 we examine the effects of utilizing concepts and of different 
types of domain-specific knowledge in performance 
contribution 
this paper is organized as follows problem statement is given in 
the next section the techniques are introduced in section in 
section we present the experimental results related works are 
given in section and finally we conclude the paper in section 
 problem statement 
we describe the queries document collection and the system 
output in this section 
the query set used in the genomics track of trec consists 
of questions collected from real biologists as described in 
these questions all have the following general format 
biological object m 
relationship 
←⎯⎯⎯⎯→ biological process n 
where a biological object might be a gene protein or gene 
mutation and a biological process can be a physiological process 
or disease a question might involve multiple biological objects 
 m and multiple biological processes n these questions were 
derived from four templates table 
table query templates and examples in the genomics track 
of trec 
template example 
what is the role of gene in 
disease 
what is the role of drd in 
alcoholism 
what effect does gene have 
on biological process 
what effect does the insulin 
receptor gene have on 
tumorigenesis 
how do genes interact in 
organ function 
how do hmg and hmgb 
interact in hepatitis 
how does a mutation in 
gene influence biological 
process 
how does a mutation in ret 
influence thyroid function 
features of the queries they are different from the typical 
web queries and the pubmed queries both of which usually 
consist of to keywords they are generated from structural 
templates which can be used by a system to identify the query 
components the biological object or process 
the document collection contains highwire full-text 
documents in html format 
the output of the system is a list of passages ranked according to 
their similarities with the query a passage is defined as any span 
of text that does not include the html paragraph tag i e p or 
 p a passage could be a part of a sentence a sentence a set of 
consecutive sentences or a paragraph i e the whole span of text 
that are inside of p and p html tags 
this is a passage-level information retrieval problem with the 
attempt to put biologists in contexts where relevant information is 
provided 
 techniques and methods 
we approached the problem by first retrieving the top-k most 
relevant paragraphs then extracting passages from these 
paragraphs and finally ranking the passages in this process we 
employed several techniques and methods which will be 
introduced in this section first we give two definitions 
definition a concept is a entry term in the mesh 
ontology or a gene symbol in the entrez gene database this 
definition of concept can be generalized to include other 
biomedical dictionary terms 
definition a semantic type is a category defined in the 
semantic network of the unified medical language system 
 umls the current release of the umls semantic network 
contains semantic types such as disease or syndrome each 
entry term in the mesh ontology is assigned one or more semantic 
types each gene symbol in the entrez gene database maps to the 
semantic type gene or genome in addition these semantic 
types are linked by relationships for example antibiotic 
prevents disease or syndrome these relationships among 
semantic types represent general biomedical knowledge we 
utilized these semantic types and their relationships to identify 
related concepts 
the rest of this section is organized as follows in section we 
explain how the concepts are identified within a query in section 
 we specify five different types of domain-specific knowledge 
and introduce how they are compiled in section we present 
our conceptual ir model finally our strategy for passage 
extraction is described in section 
 identifying concepts within a query 
a concept defined in definition is a gene symbol or a mesh 
term we make use of the query templates to identify gene 
symbols for example the query how do hmg and hmgb 
interact in hepatitis is derived from the template how do genes 
interact in organ function in this case hmg and hmgb 
will be identified as gene symbols in cases where the query 
templates are not provided programs for recognition of gene 
symbols within texts are needed 
we use the query translation functionality of pubmed to extract 
mesh terms in a query this is done by submitting the whole 
query to pubmed which will then return a file in which the mesh 
terms in the query are labeled in table three mesh terms 
within the query what is the role of gene prnp in the mad cow 
disease are found in the pubmed translation encephalopathy 
bovine spongiform for mad cow disease genes for gene 
and role for role 
table the pubmed translation of the query what is the 
role of gene prnp in the mad cow disease 
term pubmed translation 
mad cow 
disease 
 bovine spongiform encephalopathy text word 
or encephalopathy bovine spongiform mesh 
terms or mad cow disease text word 
gene 
 genes tiab not medline sb or 
 genes mesh terms or gene text word 
role role mesh terms or role text word 
 compiling domain-specific knowledge 
in this paper domain-specific knowledge refers to information 
about concepts and their relationships in a certain domain we 
used five types of domain-specific knowledge in the domain of 
genomics 
type synonyms terms listed in the thesauruses that refer to 
the same meaning 
type hypernyms more generic terms one level only 
type hyponyms more specific terms one level only 
type lexical variants different forms of the same concept 
such as abbreviations they are commonly used in the 
literature but might not be listed in the thesauruses 
type implicitly related concepts terms that are semantically 
related and also co-occur more frequently than being 
independent in the biomedical texts 
knowledge of type - is retrieved from the following two 
thesauruses mesh a controlled vocabulary maintained by 
nlm for indexing biomedical literature the version of 
mesh contains information about concepts these 
concepts are organized in a tree hierarchy entrez gene one of 
the most widely used searchable databases of genes the current 
version of entrez gene contains information about million 
genes it does not have a hierarchy only synonyms are retrieved 
from entrez gene the compiling of type - knowledge is 
introduced in section and respectively 
 lexical variants 
lexical variants of gene symbols 
new gene symbols and their lexical variants are regularly 
introduced into the biomedical literature however many 
reference databases such as umls and entrez gene may not be 
able to keep track of all this kind of variants for example for the 
gene symbol nf-kappa b at least different lexical variants can 
be found in the biomedical literature nf-kappab nfkappab 
 nfkappa b nf-kb and nfkb three of which are not in the 
current umls and two not in the entrez gene have shown 
that expanding gene symbols with their lexical variants improved 
the retrieval effectiveness of their biomedical ir systems in our 
system we employed the following two strategies to retrieve 
lexical variants of gene symbols 
strategy i this strategy is to automatically generate lexical 
variants according to a set of manually crafted heuristics 
for example given a gene symbol pla a variant plaii is 
generated according to the heuristic that roman numerals and 
arabic numerals are convertible when naming gene symbols 
another variant pla is also generated since a hyphen or a 
space could be inserted at the transition between alphabetic and 
numerical characters in a gene symbol 
strategy ii this strategy is to retrieve lexical variants from an 
abbreviation database adam is an abbreviation database 
which covers frequently used abbreviations and their definitions 
 or long-forms within medline the authoritative repository of 
citations from the biomedical literature maintained by the nlm 
given a query how does nucleoside diphosphate kinase nm 
contribute to tumor progression we first identify the 
abbreviation nm and its long-form nucleoside diphosphate 
kinase using the abbreviation identification program from 
searching the long-form nucleoside diphosphate kinase in 
adam other abbreviations such as ndpk or ndk are 
retrieved these abbreviations are considered as the lexical 
variants of nm 
lexical variants of mesh concepts 
adam is used to obtain the lexical variants of mesh concepts as 
well all the abbreviations of a mesh concept in adam are 
considered as lexical variants to each other in addition those 
long-forms that share the same abbreviation with the mesh 
concept and are different by an edit distance of or are also 
considered as its lexical variants as an example human 
papilloma viruses and human papillomaviruses have the same 
abbreviation hpv in adam and their edit distance is thus 
they are considered as lexical variants to each other the edit 
distance between two strings is measured by the minimum number 
of insertions deletions and substitutions of a single character 
required to transform one string into the other 
 implicitly related concepts 
motivation in some cases there are few documents in the 
literature that directly answer a given query in this situation those 
documents that implicitly answer their questions or provide 
supporting information would be very helpful for example there 
are few documents in pubmed that directly answer the query 
 what is the role of the genes hnf and coup-tf i in the 
suppression in the function of the liver however there exist 
some documents about the role of hnf and coup-tf i in 
regulating hepatitis b virus transcription it is very likely that the 
biologists would be interested in these documents because 
 hepatitis b virus is known as a virus that could cause serious 
damage to the function of liver in the given example hepatitis b 
virus is not a synonym hypernym hyponym nor a lexical variant 
of any of the query concepts but it is semantically related to the 
query concepts according to the umls semantic network we 
call this type of concepts implicitly related concepts of the 
query this notion is similar to the b-term used in for 
relating two disjoint literatures for biomedical hypothesis 
generation the difference is that we utilize the semantic 
relationships among query concepts to exclusively focus on 
concepts of certain semantic types 
a query q in format of section can be represented by 
q a c 
where a is the set of biological objects and c is the set of 
biological processes those concepts that are semantically related 
to both a and c according to the umls semantic network are 
considered as the implicitly related concepts of the query in the 
above example a {hnf coup-tf i} c {function of 
liver} and hepatitis b virus is one of the implicitly related 
concepts 
we make use of the medline database to extract the implicitly 
related concepts the version of medline database 
contains citations i e abstracts titles and etc of over million 
biomedical articles each document in medline is manually 
indexed by a list of mesh terms to describe the topics covered by 
that document implicitly related concepts are extracted and 
ranked in the following steps 
step let list a be the set of mesh terms that are used for 
indexing those medline citations having a and semantically 
related to a according to the umls semantic network similarly 
list c is created for c concepts in b list a ∩ list c are 
considered as implicitly related concepts of the query 
step for each concept b∈b compute the association between 
b and a using the mutual information measure 
p 
 log 
p p 
b a 
i b a 
b a 
 
where p x n n n is the number of medline citations having x 
and n is the size of medline a large value for i b a means 
that b and a co-occur much more often than being independent 
i b c is computed similarly 
step let r b i b a i b c for b∈ b given b b ∈ b 
we say r b ≤ r b if i b a ≤ i b a and i b c ≤ i b c 
then the association between b and the query q is measured by 
{ and } 
 
{ and } 
x x b r x r b 
score b q 
x x b r b r x 
∈ ≤ 
 
∈ ≤ 
 
the numerator in formula is the number of the concepts in b 
that are associated with both a and c equally with or less than b 
the denominator is the number of the concepts in b that are 
associated with both a and c equally with or more than b figure 
 shows the top implicitly related concepts for the sample 
query 
figure top implicitly related concepts for the query 
 how do interactions between hnf and coup-tf suppress 
liver function 
in figure the top implicitly related concepts are all highly 
associated with liver hepatocytes are liver cells 
hepatoblastoma is a malignant liver neoplasm occurring in 
young children the vast majority of gluconeogenesis takes 
place in the liver and hepatitis b virus is a virus that could 
cause serious damage to the function of liver 
the top-k ranked concepts in b are used for query expansion if 
i b a ≥ i b c then b is considered as an implicit related 
concept of a a document having b but not a will receive a partial 
weight of a the expansion is similar for c when i b a i b c 
 conceptual ir model 
we now discuss our conceptual ir model we first give the basic 
conceptual ir model in section then we explain how the 
domain-specific knowledge is incorporated in the model using 
query expansion in section a pseudo-feedback strategy is 
introduced in section in section we give a strategy to 
improve the ranking by avoiding incorrect match of abbreviations 
 basic model 
given a query q and a document d our model measures two 
similarities concept similarity and word similarity 
 
concept word 
sim q d sim q d sim q d 
concept similarity 
two vectors are derived from a query q 
 
 
 
 
 
 
m 
n 
q v v 
v c c c 
v c c c 
 
 
 
where v is a vector of concepts describing the biological object s 
and v is a vector of concepts describing the biological process es 
given a vector of concepts v let s v be the set of concepts in v 
the weight of vi is then measured by 
 max{log and }i i v 
v 
n 
w v s v s v n 
n 
 ⊆ 
where v is a vector that contains a subset of concepts in vi and nv is 
the number of documents having all the concepts in v 
the concept similarity between q and d is then computed by 
 
 
 i i 
concept i 
w vsim q d α 
 
 ×∑ 
where αi is a parameter to indicate the completeness of vi that 
document d has covered αi is measured by 
and i 
i 
i 
c 
c d c v 
c 
c v 
idf 
idf 
α 
∈ ∈ 
∈ 
 
∑ 
∑ 
 
where idfc is the inverse document frequency of concept c 
an example suppose we have a query how does nurr- delete 
t cells before they migrate to the spleen or lymph nodes and how 
does this impact autoimmunity after identifying the concepts in 
the query we have 
 
 
 nurr- 
 t cells spleen autoimmunity lymph nodes 
v 
v 
 
 
suppose that some document frequencies of different 
combinations of concepts are as follows 
 df nurr- 
 df t cells spleen autoimmunity lymph nodes 
 df t cells spleen autoimmunity 
 df spleen autoimmunity lymph nodes 
 df t cells autoimmunity lymph nodes 
 df t cells spleen lymph nodes 
the weight of vi is then computed by note that there does not exist 
a document having all the concepts in v 
 
 
 log 
 log 
w v n 
w v n 
 
 
 
now suppose a document d contains concepts  nurr- t cells 
 spleen and lymph nodes but not  autoimmunity then the value 
of parameter αi is computed as follows 
 
 
 
 t cells spleen lymph nodes 
 t cells spleen lymph nodes autoimmunity 
idf idf idf 
idf idf idf idf 
α 
α 
 
 
word similarity 
the similarity between q and d on the word level is computed 
using okapi 
 
log 
 word w q 
n n k tf 
sim q d 
n k tf∈ 
− 
 
 
∑ 
where n is the size of the document collection n is the number of 
documents containing w k k × -b b × dl avdl and k 
c 
function 
of liver 
implicitly related concepts b 
hepatocytes 
hepatoblastoma 
gluconeogenesis 
hepatitis b virus 
hnf and 
coup-tf i 
a 
b are constants dl is the document length of d and avdl is the 
average document length tf is the term frequency of w within d 
the model 
given two documents d and d we say sim q d sim q d or 
d will be ranked higher than d with respect to the same query q 
if either 
 
concept concept 
sim q d sim q d or 
 and 
concept concept word word 
sim q d sim q d sim q d sim q d 
this conceptual ir model emphasizes the similarity on the concept 
level a similar model but applied to non-biomedical domain has 
been given in 
 incorporating domain-specific knowledge 
given a concept c a vector u is derived by incorporating its 
domain-specific knowledge 
 u c u u u 
where u is a vector of its synonyms hyponyms and lexical 
variants u is a vector of its hypernyms and u is a vector of its 
implicitly related concepts an occurrence of any term in u will 
be counted as an occurrence of c idfc in formula is updated as 
 
logc 
c u 
n 
d 
idf 
 c ud is the set of documents having c or any term in u the 
weight that a document d receives from u is given by 
max{ and }tw t u t d∈ ∈ 
where wt β cidf× the weighting factor β is an empirical tuning 
parameter determined as 
 β if t is the original concept its synonym its hyponym or 
its lexical variant 
 β if t is a hypernym 
 β × k-i k if t is an implicitly related concept k is 
the number of selected top ranked implicitly related concepts 
 see section i is the position of t in the ranking of 
implicitly related concepts 
 pseudo-feedback 
pseudo-feedback is a technique commonly used to improve 
retrieval performance by adding new terms into the original query 
we used a modified pseudo-feedback strategy described in 
step let c be the set of concepts in the top ranked 
documents for each concept c in c compute the similarity 
between c and the query q the computation of sim q c can be 
found in 
step the top-k ranked concepts by sim q c are selected 
step associate each selected concept c with the concept cq in 
q that has the same semantic type as c and is most related to 
c among all the concepts in q the association between c and cq 
is computed by 
p 
 log 
p p 
q 
q 
q 
c c 
i c c 
c c 
 
where p x n n n is the number of documents having x and n is 
the size of the document collection a document having c but not 
cq receives a weight given by × k-i k qcidf× where i is the 
position of c in the ranking of step 
 avoid incorrect match of abbreviations 
some gene symbols are very short and thus ambiguous for 
example the gene symbol apc could be the abbreviation for 
many non-gene long-forms such as air pollution control 
aerobic plate count or argon plasma coagulation this step is 
to avoid incorrect match of abbreviations in the top ranked 
documents 
given an abbreviation x with the long-form l in the query we 
scan the top-k ranked k documents and when a document 
is found with x we compare l with all the long-forms of x in that 
document if none of these long-forms is equal or close to l i e 
the edit distance between l and the long-form of x in that 
document is or then the concept similarity of x is subtracted 
 passage extraction 
the goal of passage extraction is to highlight the most relevant 
fragments of text in paragraphs a passage is defined as any span 
of text that does not include the html paragraph tag i e p or 
 p a passage could be a part of a sentence a sentence a set of 
consecutive sentences or a paragraph i e the whole span of text 
that are inside of p and p html tags it is also possible to 
have more than one relevant passage in a single paragraph our 
strategy for passage extraction assumes that the optimal passage s 
in a paragraph should have all the query concepts that the whole 
paragraph has also they should have higher density of query 
concepts than other fragments of text in the paragraph 
suppose we have a query q and a paragraph p represented by a 
sequence of sentences np s s s let c be the set of concepts in 
q that occur in p and s φ 
step for each sequence of consecutive sentences i i js s s ≤ 
i ≤ j ≤ n let s s { }i i js s s ∪ if i i js s s satisfies that 
 every query concept in c occurs in i i js s s and 
 there does not exist k such that i k j and every query 
concept in c occurs in i i ks s s or k k js s s 
condition requires i i js s s having all the query concepts in p 
and condition requires i i js s s be the minimal 
step let min{ }i i jl j i s s s s − ∈ for every 
 i i js s s in s let { }i i js s s s s − if j - i l this step is to 
remove those sequences of sentences in s that have lower density 
of query concepts 
step for every two sequences of consecutive 
sentences and i i j i i js s s s s s s s ∈ ∈ if 
 
 
 and 
 
i i j j 
i j 
≤ ≤ 
≤ 
 
then do 
repeat this step until for every two sequences of consecutive 
sentences in s condition does not apply this step is to merge 
those sequences of sentences in s that are adjacent or overlapped 
finally the remaining sequences of sentences in s are returned as 
the optimal passages in the paragraph p with respect to the query 
 
 
 
 
 
 
{ } 
{ } 
{ } 
i i j 
i i j 
i i j 
s s s s s 
s s s s s 
s s s s s 
 
 
 
 ∪ 
 − 
 − 
 experimental results 
the evaluation of our techniques and the experimental results are 
given in this section we first describe the datasets and evaluation 
metrics used in our experiments and then present the results 
 data sets and evaluation metrics 
our experiments were performed on the platform of the genomics 
track of trec the document collection contains 
full-text documents from highwire biomedical journals the set 
of queries consists of queries collected from real biologists 
the performance is measured on three different levels passage 
aspect and document to provide better insight on how the 
question is answered from different perspectives passage map 
as described in this is a character-based precision calculated 
as follows at each relevant retrieved passage precision will be 
computed as the fraction of characters overlapping with the gold 
standard passages divided by the total number of characters 
included in all nominated passages from this system for the topic 
up until that point similar to regular map relevant passages that 
were not retrieved will be added into the calculation as well with 
precision set to for relevant passages not retrieved then the 
mean of these average precisions over all topics will be calculated 
to compute the mean average passage precision aspect map a 
question could be addressed from different aspects for example 
the question what is the role of gene prnp in the mad cow 
disease could be answered from aspects like diagnosis 
neurologic manifestations or prions genetics this measure 
indicates how comprehensive the question is answered document 
map this is the standard ir measure the precision is measured 
at every point where a relevant document is obtained and then 
averaged over all relevant documents to obtain the average 
precision for a given query for a set of queries the mean of the 
average precision for all queries is the map of that ir system 
the output of the system is a list of passages ranked according to 
their similarities with the query the performances on the three 
levels are then calculated based on the ranking of the passages 
 results 
the wilcoxon signed-rank test was employed to determine the 
statistical significance of the results in the tables of the following 
sections statistically significant improvements at the level 
are marked with an asterisk 
 conceptual ir model vs term-based model 
the initial baseline was established using word similarity only 
computed by the okapi formula another run based on our 
basic conceptual ir model was performed without using query 
expansion pseudo-feedback or abbreviation correction the 
experimental result is shown in table our basic conceptual 
ir model significantly outperforms the okapi on all three levels 
which suggests that although it requires additional efforts to 
identify concepts retrieval on the concept level can achieve 
substantial improvements over purely term-based retrieval model 
 contribution of different types of knowledge 
a series of experiments were performed to examine how each type 
of domain-specific knowledge contributes to the retrieval 
performance a new baseline was established using the basic 
conceptual ir model without incorporating any type of 
domainspecific knowledge then five runs were conducted by adding 
each individual type of domain-specific knowledge we also 
conducted a run by adding all types of domain-specific knowledge 
results of these experiments are shown in table 
we found that any available type of domain-specific 
knowledge improved the performance in passage retrieval the 
biggest improvement comes from the lexical variants which is 
consistent with the result reported in this result also indicates 
that biologists are likely to use different variants of the same 
concept according to their own writing preferences and these 
variants might not be collected in the existing biomedical 
thesauruses it also suggests that the biomedical ir systems can 
benefit from the domain-specific knowledge extracted from the 
literature by text mining systems 
synonyms provided the second biggest improvement 
hypernyms hyponyms and implicitly related concepts provided 
similar degrees of improvement the overall performance is an 
accumulative result of adding different types of domain-specific 
knowledge and it is better than any individual addition it is clearly 
shown that the performance is significantly improved on 
passage level on aspect level and on document 
level when the domain-specific knowledge is appropriately 
incorporated although it is not explicitly shown in table 
different types of domain-specific knowledge affect different 
subsets of queries more specifically each of these types with the 
exception of the lexical variants which affects a large number of 
queries affects only a few queries but for those affected queries 
their improvement is significant as a consequence the 
accumulative improvement is very significant 
 pseudo-feedback and abbreviation correction 
using the baseline all in table as a new baseline the 
contribution of abbreviation correction and pseudo-feedback is 
given in table there is little improvement by avoiding 
incorrect matching of abbreviations the pseudo-feedback 
contributed about improvement in passage retrieval 
 performance compared with best-reported results 
we compared our result with the results reported in the genomics 
track of trec on the conditions that systems are 
automatic systems and passages are extracted from paragraphs 
the performance of our system relative to the best reported results 
is shown in table in trec some systems returned the 
whole paragraphs as passages as a consequence excellent 
retrieval results were obtained on document and aspect levels at 
the expense of performance on the passage level we do not 
include the results of such systems here 
table performance compared with best-reported results 
passage map aspect map document map 
best reported results 
our results 
improvement 
the best reported results in the first row of table on three 
levels passage aspect and document are from different systems 
our result is from a single run on passage retrieval in which it is 
better than the best reported result by in passage retrieval 
and at the same time better in aspect retrieval and 
better in document retrieval since the average precision of each 
individual query was not reported we can not apply the wilcoxon 
signed-rank test to calculate the significance of difference between 
our performance and the best reported result 
table basic conceptual ir model vs term-based model 
run passage aspect document 
map imprvd qs map imprvd qs map imprvd qs 
okapi n a n a n a 
basic conceptual ir model 
table contribution of different types of domain-specific knowledge 
run passage aspect document 
map imprvd qs map imprvd qs map imprvd qs 
baseline 
 basic conceptual ir model 
 n a n a n a 
baseline synonyms 
baseline hypernyms - 
baseline hyponyms - 
baseline variants 
baseline related - 
baseline all 
table contribution of abbreviation correction and pseudo-feedback 
run passage aspect document 
map imprvd qs map imprvd qs map imprvd qs 
baseline all n a n a n a 
baseline all abbr - - 
baseline all abbr pf 
a separate experiment has been done using a second testbed the 
ad-hoc task of trec genomics to evaluate our 
knowledge-intensive conceptual ir model for document retrieval 
of biomedical literature the overall performance in terms of map 
is which is about above the best reported result 
 notice that the performance was only measured on the 
document level for the ad-hoc task of trec genomics 
 related works 
many studies used manually-crafted thesauruses or knowledge 
databases created by text mining systems to improve retrieval 
effectiveness based on either word-statistical retrieval systems or 
conceptual retrieval systems 
 assessed query expansion using the umls 
metathesaurus based on a word-statistical retrieval system 
used definitions and different types of thesaurus relationships for 
query expansion and a deteriorated performance was reported 
expanded queries with phrases and umls concepts determined by 
the metamap a program which maps biomedical text to umls 
concepts and no significant improvement was shown we used 
mesh entrez gene and other non-thesaurus knowledge resources 
such as an abbreviation database for query expansion a critical 
difference between our work and those in is that our 
retrieval model is based on concepts not on individual words 
the genomics track in trec provides a common platform to 
evaluate methods and techniques proposed by various groups for 
biomedical information retrieval as summarized in 
many groups utilized domain-specific knowledge to improve 
retrieval effectiveness among these groups assessed both 
thesaurus-based knowledge such as gene information and non 
thesaurus-based knowledge such as lexical variants of gene 
symbols for query expansion they have shown that query 
expansion with acronyms and lexical variants of gene symbols 
produced the biggest improvement whereas the query expansion 
with gene information from gene databases deteriorated the 
performance used a similar approach for generating lexical 
variants of gene symbols and reported significant improvements 
our system utilized more types of domain-specific knowledge 
including hyponyms hypernyms and implicitly related concepts 
in addition under the conceptual retrieval framework we 
examined more comprehensively the effects of different types of 
domain-specific knowledge in performance contribution 
 utilized wordnet a database of english words and 
their lexical relationships developed by princeton university for 
query expansion in the non-biomedical domain in their studies 
queries were expanded using the lexical semantic relations such as 
synonyms hypernyms or hyponyms little benefit has been 
shown in this has been due to ambiguity of the query terms 
which have different meanings in different contexts when these 
synonyms having multiple meanings are added to the query 
substantial irrelevant documents are retrieved in the biomedical 
domain this kind of ambiguity of query terms is relatively less 
frequent because although the abbreviations are highly 
ambiguous general biomedical concepts usually have only one 
meaning in the thesaurus such as umls whereas a term in 
wordnet usually have multiple meanings represented as synsets 
in wordnet besides we have implemented a post-ranking step 
to reduce the number of incorrect matches of abbreviations which 
will hopefully decrease the negative impact caused by the 
abbreviation ambiguity besides we have implemented a 
postranking step to reduce the number of incorrect matches of 
abbreviations which will hopefully decrease the negative impact 
caused by the abbreviation ambiguity the retrieval model in 
emphasized the similarity between a query and a document on the 
phrase level assuming that phrases are more important than 
individual words when retrieving documents although the 
assumption is similar our conceptual model is based on the 
biomedical concepts not phrases 
 presented a good study of the role of knowledge in the 
document retrieval of clinical medicine they have shown that 
appropriate use of semantic knowledge in a conceptual retrieval 
framework can yield substantial improvements although the 
retrieval model is similar we made a study in the domain of 
genomics in which the problem structure and task knowledge is 
not as well-defined as in the domain of clinical medicine 
also our similarity function is very different from that in 
in summary our approach differs from previous works in four 
important ways first we present a case study of conceptual 
retrieval in the domain of genomics where many knowledge 
resources can be used to improve the performance of biomedical 
ir systems second we have studied more types of 
domainspecific knowledge than previous researchers and carried out more 
comprehensive experiments to look into the effects of different 
types of domain-specific knowledge in performance contribution 
third although some of the techniques seem similar to previously 
published ones they are actually quite different in details for 
example in our pseudo-feedback process we require that the unit 
of feedback is a concept and the concept has to be of the same 
semantic type as a query concept this is to ensure that our 
conceptual model of retrieval can be applied as another example 
the way in which implicitly related concepts are extracted in this 
paper is significantly different from that given in finally our 
conceptual ir model is actually based on complex concepts 
because some biomedical meanings such as biological processes 
are represented by multiple simple concepts 
 conclusion 
this paper proposed a conceptual approach to utilize 
domainspecific knowledge in an ir system to improve its effectiveness in 
retrieving biomedical literature we specified five different types 
of domain-specific knowledge i e synonyms hyponyms 
hypernyms lexical variants and implicitly related concepts and 
examined their effects in performance contribution we also 
evaluated other two techniques pseudo-feedback and abbreviation 
correction experimental results have shown that appropriate use 
of domain-specific knowledge in a conceptual ir model yields 
significant improvements in passage retrieval over the best 
known results in our future work we will explore the use of other 
existing knowledge resources such as umls and the wikipedia 
and evaluate techniques such as disambiguation of gene symbols 
for improving retrieval effectiveness the application of our 
conceptual ir model in other domains such as clinical medicine 
will be investigated 
 acknowledgments 
insightful discussion 
 references 
 aronson a r rindflesch t c query expansion using the 
umls metathesaurus proc amia annu fall symp 
 - 
 baeza-yates r ribeiro-neto b modern information 
retrieval addison-wesley - 
 buttcher s clarke c l a cormack g v domain-specific 
synonym expansion and validation for biomedical 
information retrieval multitext experiments for trec 
 trec 
 chang j t schutze h altman r b creating an online 
dictionary of abbreviations from medline journal of the 
american medical informatics association 
 church k w hanks p word association norms mutual 
information and lexicography computational linguistics 
 c 
 fontelo p liu f ackerman m askmedline a free-text 
natural language query tool for medline pubmed bmc 
med inform decis mak mar 
 fukuda k tamura a tsunoda t takagi t toward 
information extraction identifying protein names from 
biological papers pac symp biocomput - 
 hersh w r and etc trec genomics track overview 
trec 
 hersh w r and etc trec genomics track overview 
in trec 
 hersh w r and etc trec genomics track overview 
in trec 
 hersh w r price s donohoe l assessing thesaurus-based 
query expansion using the umls metathesaurus proc amia 
symp - 
 levenshtein v binary codes capable of correcting deletions 
insertions and reversals soviet physics - doklady 
 - 
 lin j demner-fushman d the role of knowledge in 
conceptual retrieval a study in the domain of clinical 
medicine sigir - 
 lindberg d humphreys b and mccray a the unified 
medical language system methods of information in 
medicine - 
 liu s liu f yu c and meng w y an effective 
approach to document retrieval via utilizing wordnet and 
recognizing phrases sigir - 
 proux d rechenmann f julliard l pillet v v jacq b 
detecting gene symbols and names in biological texts a 
first step toward pertinent information extraction genome 
inform ser workshop genome inform - 
 robertson s e walker s okapi keenbow at trec- nist 
special publication - trec 
 sackett d l and etc evidence-based medicine how to 
practice and teach ebm churchill livingstone second 
edition 
 swanson d r smalheiser n r an interactive system for 
finding complemen-tary literatures a stimulus to scientific 
discovery artificial intelligence - 
 voorhees e query expansion using lexical-semantic 
relations sigir - 
 zhong m huang x j concept-based biomedical text 
retrieval sigir - 
 zhou w torvik v i smalheiser n r adam another 
database of abbreviations in medline bioinformatics 
 - 
cross-lingual query suggestion using query logs 
of different languages 
wei gao 
 cheng niu 
 jian-yun nie 
 ming zhou 
 jian hu 
 kam-fai wong 
 
hsiao-wuen hon 
 
the chinese university of hong kong hong kong china 
{wgao kfwong} se cuhk edu hk 
 
microsoft research asia beijing china 
{chengniu mingzhou jianh hon} microsoft com 
 
université de montréal montréal qc canada 
nie iro umontreal ca 
abstract 
query suggestion aims to suggest relevant queries for a given 
query which help users better specify their information needs 
previously the suggested terms are mostly in the same language 
of the input query in this paper we extend it to cross-lingual 
query suggestion clqs for a query in one language we suggest 
similar or relevant queries in other languages this is very 
important to scenarios of cross-language information retrieval 
 clir and cross-lingual keyword bidding for search engine 
advertisement instead of relying on existing query translation 
technologies for clqs we present an effective means to map the 
input query of one language to queries of the other language in the 
query log important monolingual and cross-lingual information 
such as word translation relations and word co-occurrence 
statistics etc are used to estimate the cross-lingual query 
similarity with a discriminative model benchmarks show that the 
resulting clqs system significantly outperforms a baseline 
system based on dictionary-based query translation besides the 
resulting clqs is tested with french to english clir tasks on 
trec collections the results demonstrate higher effectiveness 
than the traditional query translation methods 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval - query formulation 
general terms 
algorithms performance experimentation theory 
 introduction 
query suggestion is a functionality to help users of a search 
engine to better specify their information need by narrowing 
down or expanding the scope of the search with synonymous 
queries and relevant queries or by suggesting related queries that 
have been frequently used by other users search engines such as 
google yahoo msn ask jeeves all have implemented query 
suggestion functionality as a valuable addition to their core search 
method in addition the same technology has been leveraged to 
recommend bidding terms to online advertiser in the 
pay-forperformance search market 
query suggestion is closely related to query expansion which 
extends the original query with new search terms to narrow the 
scope of the search but different from query expansion query 
suggestion aims to suggest full queries that have been formulated 
by users so that the query integrity and coherence are preserved in 
the suggested queries 
typical methods for query suggestion exploit query logs and 
document collections by assuming that in the same period of 
time many users share the same or similar interests which can be 
expressed in different manners by suggesting the 
related and frequently used formulations it is hoped that the new 
query can cover more relevant documents however all of the 
existing studies dealt with monolingual query suggestion and to 
our knowledge there is no published study on cross-lingual query 
suggestion clqs clqs aims to suggest related queries but in a 
different language it has wide applications on world wide web 
for cross-language search or for suggesting relevant bidding terms 
in a different language 
clqs can be approached as a query translation problem i e to 
suggest the queries that are translations of the original query 
dictionaries large size of parallel corpora and existing 
commercial machine translation systems can be used for 
translation however these kinds of approaches usually rely on 
static knowledge and data it cannot effectively reflect the quickly 
shifting interests of web users moreover there are some 
problems with translated queries in target language for instance 
the translated terms can be reasonable translations but they are 
not popularly used in the target language for example the french 
query aliment biologique is translated into biologic food by 
google translation tool 
 yet the correct formulation nowadays 
should be organic food therefore there exist many mismatch 
cases between the translated terms and the really used terms in 
target language this mismatch makes the suggested terms in the 
target language ineffective 
a natural thinking of solving this mismatch is to map the 
queries in the source language and the queries in the target 
language by using the query log of a search engine we exploit 
the fact that the users of search engines in the same period of time 
have similar interests and they submit queries on similar topics in 
different languages as a result a query written in a source 
language likely has an equivalent in a query log in the target 
language in particular if the user intends to perform clir then 
original query is even more likely to have its correspondent 
included in the target language query log therefore if a 
candidate for clqs appears often in the query log then it is more 
likely the appropriate one to be suggested 
in this paper we propose a method of calculating the similarity 
between source language query and the target language query by 
exploiting in addition to the translation information a wide 
spectrum of bilingual and monolingual information such as term 
co-occurrences query logs with click-through data etc a 
discriminative model is used to learn the cross-lingual query 
similarity based on a set of manually translated queries the 
model is trained by optimizing the cross-lingual similarity to best 
fit the monolingual similarity between one query and the other 
query s translation besides being benchmarked as an independent 
module the resulting clqs system is tested as a new means of 
query translation in clir task on trec collections the results 
show that this new translation method is more effective than the 
traditional query translation method 
the remainder of this paper is organized as follows section 
introduces the related work section describes in detail the 
discriminative model for estimating cross-lingual query similarity 
section presents a new clir approach using cross-lingual query 
suggestion as a bridge across language boundaries section 
discusses the experiments and benchmarks finally the paper is 
concluded in section 
 related work 
most approaches to clir perform a query translation followed by 
a monolingual ir typically queries are translated either using a 
bilingual dictionary a machine translation software or a 
parallel corpus 
despite the various types of resources used out-of-vocabulary 
 oov words and translation disambiguation are the two major 
bottlenecks for clir in oov term translations are 
mined from the web using a search engine in bilingual 
knowledge is acquired based on anchor text analysis in addition 
word co-occurrence statistics in the target language has been 
leveraged for translation disambiguation 
 
http www google com language tools 
nevertheless it is arguable that accurate query translation may 
not be necessary for clir indeed in many cases it is helpful to 
introduce words even if they are not direct translations of any 
query word but are closely related to the meaning of the query 
this observation has led to the development of cross-lingual query 
expansion clqe techniques reports the 
enhancement on clir by post-translation expansion 
develops a cross-lingual relevancy model by leveraging the 
crosslingual co-occurrence statistics in parallel texts makes 
performance comparison on multiple clqe techniques including 
pre-translation expansion and post-translation expansion 
however there is lack of a unified framework to combine the 
wide spectrum of resources and recent advances of mining 
techniques for clqe 
clqs is different from clqe in that it aims to suggest full 
queries that have been formulated by users in another language 
as clqs exploits up-to-date query logs it is expected that for 
most user queries we can find common formulations on these 
topics in the query log in the target language therefore clqs 
also plays a role of adapting the original query formulation to the 
common formulations of similar topics in the target language 
query logs have been successfully used for monolingual ir 
 especially in monolingual query suggestions and 
relating the semantically relevant terms for query expansion 
 in the target language query log has been exploited to 
help query translation in clir 
 estimating cross-lingual 
query similarity 
a search engine has a query log containing user queries in 
different languages within a certain period of time in addition to 
query terms click-through information is also recorded therefore 
we know which documents have been selected by users for each 
query given a query in the source language our clqs task is to 
determine one or several similar queries in the target language 
from the query log 
the key problem with cross-lingual query suggestion is how to 
learn a similarity measure between two queries in different 
languages although various statistical similarity measures have 
been studied for monolingual terms most of them are 
based on term co-occurrence statistics and can hardly be applied 
directly in cross-lingual settings 
in order to define a similarity measure across languages one 
has to use at least one translation tool or resource so the measure 
is based on both translation relation and monolingual similarity in 
this paper as our purpose is to provide up-to-date query similarity 
measure it may not be sufficient to use only a static translation 
resource therefore we also integrate a method to mine possible 
translations on the web this method is particularly useful for 
dealing with oov terms 
given a set of resources of different natures the next question 
is how to integrate them in a principled manner in this paper we 
propose a discriminative model to learn the appropriate similarity 
measure the principle is as follows we assume that we have a 
reasonable monolingual query similarity measure for any training 
query example for which a translation exists its similarity 
measure with any other query is transposed to its translation 
therefore we have the desired cross-language similarity value for 
this example then we use a discriminative model to learn the 
cross-language similarity function which fits the best these 
examples 
in the following sections let us first describe the detail of the 
discriminative model for cross-lingual query similarity estimation 
then we introduce all the features monolingual and cross-lingual 
information that we will use in the discriminative model 
 discriminative model for estimating 
cross-lingual query similarity 
in this section we propose a discriminative model to learn 
crosslingual query similarities in a principled manner the principle is 
as follows for a reasonable monolingual query similarity between 
two queries a cross-lingual correspondent can be deduced 
between one query and another query s translation in other 
words for a pair of queries in different languages their 
crosslingual similarity should fit the monolingual similarity between 
one query and the other query s translation for example the 
similarity between french query pages jaunes i e yellow 
page in english and english query telephone directory should 
be equal to the monolingual similarity between the translation of 
the french query yellow page and telephone directory there 
are many ways to obtain a monolingual similarity measure 
between terms e g term co-occurrence based mutual information 
and 
χ any of them can be used as the target for the cross-lingual 
similarity function to fit in this way cross-lingual query 
similarity estimation is formulated as a regression task as follows 
given a source language query fq a target language query eq 
and a monolingual query similarity mlsim the corresponding 
cross-lingual query similarity clsim is defined as follows 
 eqmlefcl qtsimqqsim f 
 
where fqt is the translation of fq in the target language 
based on equation it would be relatively easy to create a 
training corpus all it requires is a list of query translations then 
an existing monolingual query suggestion system can be used to 
automatically produce similar query to each translation and create 
the training corpus for cross-lingual similarity estimation another 
advantage is that it is fairly easy to make use of arbitrary 
information sources within a discriminative modeling framework 
to achieve optimal performance 
in this paper support vector machine svm regression 
algorithm is used to learn the cross-lingual term similarity 
function given a vector of feature functions f between fq and 
eq efcl ttsim is represented as an inner product between a 
weight vector and the feature vector in a kernel space as follows 
 efefcl ttfwttsim φ 
where φ is the mapping from the input feature space onto the 
kernel space and wis the weight vector in the kernel space which 
will be learned by the svm regression training once the weight 
vector is learned the equation can be used to estimate the 
similarity between queries of different languages 
we want to point out that instead of regression one can 
definitely simplify the task as a binary or ordinal classification in 
which case clqs can be categorized according to discontinuous 
class labels e g relevant and irrelevant or a series of levels of 
relevancies e g strongly relevant weakly relevant and 
irrelevant in either case one can resort to discriminative 
classification approaches such as an svm or maximum entropy 
model in a straightforward way however the regression 
formalism enables us to fully rank the suggested queries based on 
the similarity score given by equation 
the equations and construct a regression model for 
cross-lingual query similarity estimation in the following 
sections the monolingual query similarity measure see section 
 and the feature functions used for svm regression see 
section will be presented 
 monolingual query similarity measure 
based on click-through information 
any monolingual term similarity measure can be used as the 
regression target in this paper we select the monolingual query 
similarity measure presented in which reports good 
performance by using search users click-through information in 
query logs the reason to choose this monolingual similarity is 
that it is defined in a similar context as ours − according to a user 
log that reflects users intention and behavior therefore we can 
expect that the cross-language term similarity learned from it can 
also reflect users intention and expectation 
following our monolingual query similarity is defined by 
combining both query content-based similarity and click-through 
commonality in the query log 
first the content similarity between two queries p and q is 
defined as follows 
 
 
 
qknpknmax 
qpkn 
qpsimilarity content 
where xkn is the number of keywords in a query x qpkn is 
the number of common keywords in the two queries 
secondly the click-through based similarity is defined as 
follows 
 
 
 
qrdprdmax 
qprd 
qpsimilarity throughclick − 
 
where xrd is the number of clicked urls for a query x and 
 qprd is the number of common urls clicked for two queries 
finally the similarity between two queries is a linear 
combination of the content-based and click-through-based 
similarities and is presented as follows 
 
 
qpsimilarity 
qpsimilarityqpsimilarity 
throughclick 
content 
− 
 
β 
α 
where α and β are the relative importance of the two similarity 
measures in this paper we set α and β following the 
practice in queries with similarity measure higher than a 
threshold with another query will be regarded as relevant 
monolingual query suggestions mlqs for the latter in this 
paper the threshold is set as empirically 
 features used for learning cross-lingual 
query similarity measure 
this section presents the extraction of candidate relevant queries 
from the log with the assistance of various monolingual and 
bilingual resources meanwhile feature functions over source 
query and the cross-lingual relevant candidates are defined some 
of the resources being used here such as bilingual lexicon and 
parallel corpora were for query translation in previous work but 
note that we employ them here as an assistant means for finding 
relevant candidates in the log rather than for acquiring accurate 
translations 
 bilingual dictionary 
in this subsection a built-in-house bilingual dictionary containing 
 unique entries is used to retrieve candidate queries since 
multiple translations may be associated with each source word 
co-occurrence based translation disambiguation is performed 
 the process is presented as follows 
given an input query }{ fnfff wwwq k in the source 
language for each query term fiw a set of unique translations are 
provided by the bilingual dictionary d } { imiifi tttwd k 
then the cohesion between the translations of two query terms is 
measured using mutual information which is computed as follows 
 
 
log 
klij 
klij 
klijklij 
tptp 
ttp 
ttpttmi 
where 
 
 
 
 
n 
tc 
tp 
n 
ttc 
ttp 
klij 
klij 
here yxc is the number of queries in the log containing both 
x and y xc is the number of queries containing term x and 
n is the total number of queries in the log 
based on the term-term cohesion defined in equation all the 
possible query translations are ranked using the summation of the 
term-term cohesion ∑≠ 
 
kiki 
klijqdict ttmits f 
 
 the set of 
top- query translations is denoted as fqts for each possible 
query translation fqtst∈ we retrieve all the queries containing 
the same keywords as t from the target language log the 
retrieved queries are candidate target queries and are assigned 
 tsdict 
as the value of the feature dictionary-based translation 
score 
 parallel corpora 
parallel corpora are precious resources for bilingual knowledge 
acquisition different from the bilingual dictionary the bilingual 
knowledge learned from parallel corpora assigns probability for 
each translation candidate which is useful in acquiring dominant 
query translations 
in this paper the europarl corpus a set of parallel french and 
english texts from the proceedings of the european parliament is 
used the corpus is first sentence aligned then word alignments 
are derived by training an ibm translation model using 
giza the learned bilingual knowledge is used to extract 
candidate queries from the query log the process is presented as 
follows 
given a pair of queries fq in the source language and eq in the 
target language the bi-directional translation score is defined as 
follows 
 feibmefibmefibm qqpqqpqqs 
where xypibm 
is the word sequence translation probability 
given by ibm model which has the following form 
∏∑ 
 
 
 
 
 
 
 
 
 
y 
j 
x 
i 
ijyibm xyp 
x 
xyp 
where ij xyp is the word to word translation probability 
derived from the word-aligned corpora 
the reason to use bidirectional translation probability is to deal 
with the fact that common words can be considered as possible 
translations of many words by using bidirectional translation we 
test whether the translation words can be translated back to the 
source words this is helpful to focus on the translation 
probability onto the most specific translation candidates 
now given an input query fq the top queries }{ eq with the 
highest bidirectional translation scores with fq are retrieved from 
the query log and efibm qqs in equation is assigned as the 
value for the feature bi-directional translation score 
 online mining for related queries 
oov word translation is a major knowledge bottleneck for query 
translation and clir to overcome this knowledge bottleneck 
web mining has been exploited in to acquire 
englishchinese term translations based on the observation that chinese 
terms may co-occur with their english translations in the same 
web page in this section this web mining approach is adapted to 
acquire not only translations but semantically related queries in 
the target language 
it is assumed that if a query in the target language co-occurs 
with the source query in many web pages they are probably 
semantically related therefore a simple method is to send the 
source query to a search engine google in our case for web 
pages in the target language in order to find related queries in the 
target language for instance by sending a french query pages 
jaunes to search for english pages the english snippets 
containing the key words yellow pages or telephone directory 
will be returned however this simple approach may induce 
significant amount of noise due to the non-relevant returns from 
the search engine in order to improve the relevancy of the 
bilingual snippets we extend the simple approach by the 
following query modification the original query is used to search 
with the dictionary-based query keyword translations which are 
unified by the ∧ and ∨ or operators into a single boolean 
query for example for a given query abcq where the set of 
translation entries in the dictionary of for a is } { aaa b is 
} { bb and c is }{ c we issue cbbaaaq ∧∨∧∨∨∧ as 
one web query 
from the returned top snippets the most frequent target 
queries are identified and are associated with the feature 
frequency in the snippets 
furthermore we use co-occurrence double-check codc 
measure to weight the association between the source and target 
queries codc measure is proposed in as an association 
measure based on snippet analysis named web search with 
double checking wsdc model in wsdc model two objects a 
and b are considered to have an association if b can be found by 
using a as query forward process and a can be found by using b 
as query backward process by web search the forward process 
counts the frequency of b in the top n snippets of query a denoted 
as   abfreq similarly the backward process count the 
frequency of a in the top n snippets of query b denoted 
as   bafreq then the codc association score is defined as 
follows 
⎪ 
⎩ 
⎪ 
⎨ 
⎧ × 
 
⎥ 
⎥ 
⎦ 
⎤ 
⎢ 
⎢ 
⎣ 
⎡ 
× 
otherwise 
     if 
 
 
   
 
   
log 
α 
e 
ef 
f 
fe 
qfreq 
qqfreq 
qfreq 
qqfreq 
effe 
efcodc 
e 
qqfreqqqfreq 
qqs 
codc measures the association of two terms in the range 
between and where under the two extreme cases eq and fq 
are of no association when   fe qqfreq 
or   ef qqfreq and are of the strongest association when 
   ffe qfreqqqfreq and   eef qfreqqqfreq in 
our experiment α is set at following the practice in 
any query eq mined from the web will be associated with a 
feature codc measure with efcodc qqs as its value 
 monolingual query suggestion 
for all the candidate queries q being retrieved using dictionary 
 see section parallel data see section and web 
mining see section monolingual query suggestion system 
 described in section is called to produce more related 
queries in the target language for each target query eq its 
monolingual source query eml qsq is defined as the query in 
 q with the highest monolingual similarity with eq i e 
 maxarg eemlqqeml qqsimqsq e 
′ ∈′ 
 
then the monolingual similarity between eq and eml qsq is 
used as the value of the eq s monolingual query suggestion 
feature for any target query qq∈ its monolingual query 
suggestion feature is set as 
for any query qqe ∉ its values of dictionary-based 
translation score bi-directional translation score frequency 
in the snippet and codc measure are set to be equal to the 
feature values of eml qsq 
 estimating cross-lingual query similarity 
in summary four categories of features are used to learn the 
crosslingual query similarity svm regression algorithm is used to 
learn the weights in equation in this paper libsvm toolkit 
 is used for the regression training 
in the prediction stage the candidate queries will be ranked 
using the cross-lingual query similarity score computed in terms 
of efefcl ttfwttsim φ and the queries with 
similarity score lower than a threshold will be regarded as 
nonrelevant the threshold is learned using a development data set by 
fitting mlqs s output 
 clir based on cross-lingual 
query suggestion 
in section we presented a discriminative model for cross lingual 
query suggestion however objectively benchmarking a query 
suggestion system is not a trivial task in this paper we propose to 
use clqs as an alternative to query translation and test its 
effectiveness in clir tasks the resulting good performance of 
clir corresponds to the high quality of the suggested queries 
given a source query fq a set of relevant queries }{ eq in the 
target language are recommended using the cross-lingual query 
suggestion system then a monolingual ir system based on the 
bm model is called using each }{ eqq∈ as queries to 
retrieve documents then the retrieved documents are re-ranked 
based on the sum of the bm scores associated with each 
monolingual retrieval 
 performacnce evaluation 
in this section we will benchmark the cross-lingual query 
suggestion system comparing its performance with monolingual 
query suggestion studying the contribution of various information 
sources and testing its effectiveness when being used in clir 
tasks 
 data resources 
in our experiments french and english are selected as the source 
and target language respectively such selection is due to the fact 
that large scale query logs are readily available for these two 
languages a one-month english query log containing million 
unique english queries with occurrence frequency more than of 
msn search engine is used as the target language log and a 
monolingual query suggestion system is built based on it in 
addition french queries are selected randomly from a 
french query log containing around million queries and are 
manually translated into english by professional french-english 
translators among the french queries queries have 
their translations in the english query log and are used for clqs 
training and testing furthermore among the french 
queries are used for cross-lingual query similarity training 
 are used as the development data to determine the relevancy 
threshold and are used for testing to retrieve the 
crosslingual related queries a built-in-house french-english bilingual 
lexicon containing unique entries and the europarl 
corpus are used 
besides benchmarking clqs as an independent system the 
clqs is also tested as a query translation system for clir 
tasks based on the observation that the clir performance 
heavily relies on the quality of the suggested queries this 
benchmark measures the quality of clqs in terms of its 
effectiveness in helping clir to perform such benchmark we 
use the documents of trec clir data ap - newswire 
 mb with officially provided short french-english queries 
pairs cl -cl the selection of this data set is due to the fact 
that the average length of the queries are words long which 
matches the web query logs we use to train clqs 
 performance of cross-lingual query 
suggestion 
mean-square-error mse is used to measure the regression error 
and it is defined as follows 
 
 
 
∑ − 
i 
eiqmleificl qtsimqqsim 
l 
mse fi 
where l is the total number of cross-lingual query pairs in the 
testing data 
as described in section a relevancy threshold is learned 
using the development data and only clqs with similarity value 
above the threshold is regarded as truly relevant to the input 
query in this way clqs can also be benchmarked as a 
classification task using precision p and recall r which are 
defined as follows 
clqs 
mlqsclqs 
p 
s 
ss i 
 
mlqs 
mlqsclqs 
r 
s 
ss i 
 
where clqss is the set of relevant queries suggested by clqs 
mlqss is the set of relevant queries suggested by mlqs see 
section 
the benchmarking results with various feature configurations 
are shown in table 
regression classification 
features 
mse p r 
dd 
dd pc 
dd pc 
web 
 
dd pc 
web ml 
qs 
 
table clqs performance with different feature settings 
 dd dictionary only dd pc dictionary and parallel corpora 
dd pc web dictionary parallel corpora and web mining 
dd pc web mlqs dictionary parallel corpora web mining 
and monolingual query suggestion 
table reports the performance comparison with various 
feature settings the baseline system dd uses a conventional 
query translation approach i e a bilingual dictionary with 
cooccurrence-based translation disambiguation the baseline system 
only covers less than of the suggestions made by mlqs 
using additional features obviously enables clqs to generate 
more relevant queries the most significant improvement on recall 
is achieved by exploiting mlqs the final clqs system is able 
to generate of the queries suggested by mlqs among all 
the feature combinations there is no significant change in 
precision this indicates that our methods can improve the recall 
by effectively leveraging various information sources without 
losing the accuracy of the suggestions 
besides benchmarking clqs by comparing its output with 
mlqs output french queries are randomly selected from the 
french query log these queries are double-checked to make sure 
that they are not in the clqs training corpus then clqs system 
is used to suggest relevant english queries for them on average 
for each french query relevant english queries are suggested 
then the total suggested english queries are manually 
checked by two professional english french translators with 
cross-validation among the suggested queries 
queries are recognized as relevant to the original ones hence the 
accuracy is figure shows an example of clqs of the 
french query terrorisme international international terrorism 
in english 
 clir performance 
in this section clqs is tested with french to english clir tasks 
we conduct clir experiments using the trec clir dataset 
described in section the clir is performed using a query 
translation system followed by a bm -based monolingual 
ir module the following three different systems have been used 
to perform query translation clqs our clqs system 
mt google french to english machine translation system 
dt a dictionary based query translation system using 
cooccurrence statistics for translation disambiguation the 
translation disambiguation algorithm is presented in section 
besides the monolingual ir performance is also reported as a 
reference the average precision of the four ir systems are 
reported in table and the -point precision-recall curves are 
shown in figure 
table average precision of clir on trec dataset 
 monolingual monolingual ir system mt clir based on 
machine translation dt clir based on dictionary 
translation clqs clqs-based clir 
ir system average precision of monolingual ir 
monolingual 
mt 
dt 
clqs 
figure an example of clqs of the french query 
terrorisme international 
international terrorism what is terrorism 
counter terrorism terrorist 
terrorist attacks international terrorist 
world terrorism global terrorism 
transnational terrorism human rights 
terrorist groups patterns of global terrorism 
september 
 -point p-r curves trec 
 
 
 
 
 
 
 
 
 
recall 
precison 
monolingual 
mt 
dt 
clqs 
the benchmark shows that using clqs as a query translation 
tool outperforms clir based on machine translation by 
outperforms clir based on dictionary translation by and 
achieves of the monolingual ir performance 
the effectiveness of clqs lies in its ability in suggesting 
closely related queries besides accurate translations for example 
for the query cl terrorisme international international 
terrorism although the machine translation tool translates the 
query correctly clqs system still achieves higher score by 
recommending many additional related terms such as global 
terrorism world terrorism etc as shown in figure another 
example is the query la pollution causée par l automobile air 
pollution due to automobile of cl the mt tool provides the 
translation the pollution caused by the car while clqs system 
enumerates all the possible synonyms of car and suggest the 
following queries car pollution auto pollution automobile 
pollution besides other related queries such as global 
warming are also suggested for the query cl la culture 
écologique organic farming the mt tool fails to generate the 
correct translation although the correct translation is neither in 
our french-english dictionary clqs system generates organic 
farm as a relevant query due to successful web mining 
the above experiment demonstrates the effectiveness of using 
clqs to suggest relevant queries for clir enhancement a 
related research is to perform query expansion to enhance clir 
 so it is very interesting to compare the clqs approach 
with the conventional query expansion approaches following 
 post-translation expansion is performed based on 
pseudorelevance feedback prf techniques we first perform clir in 
the same way as before then we use the traditional prf 
algorithm described in to select expansion terms in our 
experiments the top terms are selected to expand the original 
query and the new query is used to search the collection for the 
second time the new clir performance in terms of average 
precision is shown in table the -point p-r curves are drawn 
in figure 
although being enhanced by pseudo-relevance feedback the 
clir using either machine translation or dictionary-based query 
translation still does not perform as well as clqs-based 
approach statistical t-test is conducted to indicate whether 
the clqs-based clir performs significantly better pair-wise 
pvalues are shown in table clearly clqs significantly 
outperforms mt and dt without prf as well as dt prf but its 
superiority over mt prf is not significant however when 
combined with prf clqs significant outperforms all the other 
methods this indicates the higher effectiveness of clqs in 
related term identification by leveraging a wide spectrum of 
resources furthermore post-translation expansion is capable of 
improving clqs-based clir this is due to the fact that clqs 
and pseudo-relevance feedback are leveraging different categories 
of resources and both approaches can be complementary 
ir system ap without prf ap with prf 
monolingual 
mt 
dt 
clqs 
 -point p-r curves with pseudo relevance feedback trec 
 
 
 
 
 
 
 
 
 
 
recall 
precison 
monolingual 
mt 
dt 
clqs 
mt dt mt prf dt prf 
clqs e- 
clqs pr 
f 
 e- 
 conclusions 
in this paper we proposed a new approach to cross-lingual query 
suggestion by mining relevant queries in different languages from 
query logs the key solution to this problem is to learn a 
crosslingual query similarity measure by a discriminative model 
exploiting multiple monolingual and bilingual resources the 
model is trained based on the principle that cross-lingual 
similarity should best fit the monolingual similarity between one 
query and the other query s translation 
figure points precision-recall on trec clir data set 
figure points precision-recall on trec clir 
dataset with pseudo relevance feedback 
table comparison of average precision ap on trec 
without and with post-translation expansion are the 
relative percentages over the monolingual ir performance 
table the results of pair-wise significance t-test here 
pvalue is considered statistically significant 
the baseline clqs system applies a typical query translation 
approach using a bilingual dictionary with co-occurrence-based 
translation disambiguation this approach only covers of the 
relevant queries suggested by an mlqs system when the exact 
translation of the original query is given by leveraging 
additional resources such as parallel corpora web mining and 
logbased monolingual query expansion the final system is able to 
cover of the relevant queries suggested by an mlqs system 
with precision as high as 
to further test the quality of the suggested queries clqs system 
is used as a query translation system in clir tasks 
benchmarked using trec french to english clir task clqs 
demonstrates higher effectiveness than the traditional query 
translation methods using either bilingual dictionary or 
commercial machine translation tools 
the improvement on trec french to english clir task by 
using clqs demonstrates the high quality of the suggested 
queries this also shows the strong correspondence between the 
input french queries and english queries in the log in the future 
we will build clqs system between languages which may be 
more loosely correlated e g english and chinese and study the 
clqs performance change due to the less strong correspondence 
among queries in such languages 
 references 
 ambati v and rohini u using monolingual clickthrough 
data to build cross-lingual search systems in proceedings 
of new directions in multilingual information access 
workshop of sigir 
 ballestors l a and croft w b phrasal translation and 
query expansion techniques for cross-language 
information retrieval in proc sigir pp - 
 ballestors l a and croft w b resolving ambiguity for 
cross-language retrieval in proc sigir pp - 
 brown p f pietra d s a pietra d v j and mercer r 
l the mathematics of statistical machine translation 
parameter estimation computational linguistics 
 
 chang c c and lin c libsvm a library for support 
vector machines version 
http citeseer ist psu edu chang libsvm html 
 chen h -h lin m -s and wei y -c novel association 
measures using web search with double checking in proc 
coling acl pp - 
 cheng p -j teng j -w chen r -c wang j -h lu 
w h and chien l -f translating unknown queries with web 
corpora for cross-language information retrieval in proc 
sigir pp - 
 cui h wen j r nie j -y and ma w y query 
expansion by mining user logs ieee trans on knowledge 
and data engineering - 
 fujii a and ishikawa t applying machine translation to 
two-stage cross-language information retrieval in 
proceedings of th conference of the association for 
machine translation in the americas pp - 
 gao j f nie j -y xun e zhang j zhou m and 
huang c improving query translation for clir using 
statistical models in proc sigir pp - 
 gao j f nie j -y he h chen w and zhou m 
resolving query translation ambiguity using a decaying 
co-occurrence model and syntactic dependence relations 
in proc sigir pp - 
 gleich d and zhukov l svd subspace projections for 
term suggestion ranking and clustering in technical 
report yahoo research labs 
 hull d using statistical testing in the evaluation of 
retrieval experiments in proc sigir pp - 
 jeon j croft w b and lee j finding similar questions 
in large question and answer archives in proc cikm 
 pp - 
 joachims t optimizing search engines using clickthrough 
data in proc sigkdd pp - 
 lavrenko v choquette m and croft w b cross-lingual 
relevance models in proc sigir pp - 
 lu w -h chien l -f and lee h -j anchor text mining 
for translation extraction of query terms in proc sigir 
 pp - 
 mcnamee p and mayfield j comparing cross-language 
query expansion techniques by degrading translation 
resources in proc sigir pp - 
 monz c and dorr b j iterative translation 
disambiguation for cross-language information retrieval 
in proc sigir pp - 
 nie j -y simard m isabelle p and durand r 
crosslanguage information retrieval based on parallel text and 
automatic mining of parallel text from the web in proc 
sigir pp - 
 och f j and ney h a systematic comparison of various 
statistical alignment models computational linguistics 
 - 
 pirkola a hedlund t keshusalo h and järvelin k 
dictionary-based cross-language information retrieval 
problems methods and research findings information 
retrieval - 
 robertson s e walker s hancock-beaulieu m m and 
gatford m okapi at trec- in proc trec- pp 
 
 robertson s e and jones k s relevance weighting of 
search terms journal of the american society of 
information science - 
 smola a j and schölkopf b a tutorial on support vector 
regression statistics and computing - 
 wen j r nie j -y and zhang h j query clustering 
using user logs acm trans information systems 
 - 
 zhang y and vines p using the web for automated 
translation extraction in cross-language information 
retrieval in proc sigir pp - 
a study of factors affecting the utility of 
implicit relevance feedback 
ryen w white 
human-computer interaction laboratory 
institute for advanced computer studies 
university of maryland 
college park md usa 
ryen umd edu 
ian ruthven 
department of computer and 
information sciences 
university of strathclyde 
glasgow scotland g xh 
ir cis strath ac uk 
joemon m jose 
department of computing science 
university of glasgow 
glasgow scotland g rz 
jj dcs gla ac uk 
abstract 
implicit relevance feedback irf is the process by which a search 
system unobtrusively gathers evidence on searcher interests from their 
interaction with the system irf is a new method of gathering 
information on user interest and if irf is to be used in operational ir 
systems it is important to establish when it performs well and when it 
performs poorly in this paper we investigate how the use and 
effectiveness of irf is affected by three factors search task 
complexity the search experience of the user and the stage in the 
search our findings suggest that all three of these factors contribute 
to the utility of irf 
categories and subject descriptors 
h information search and retrieval 
general terms 
experimentation human factors 
 introduction 
information retrieval ir systems are designed to help searchers 
solve problems in the traditional interaction metaphor employed by 
web search systems such as yahoo and msn search the system 
generally only supports the retrieval of potentially relevant documents 
from the collection however it is also possible to offer support to 
searchers for different search activities such as selecting the terms to 
present to the system or choosing which search strategy to adopt 
 both of which can be problematic for searchers 
as the quality of the query submitted to the system directly affects the 
quality of search results the issue of how to improve search queries 
has been studied extensively in ir research techniques such as 
relevance feedback rf have been proposed as a way in which 
the ir system can support the iterative development of a search query 
by suggesting alternative terms for query modification however in 
practice rf techniques have been underutilised as they place an 
increased cognitive burden on searchers to directly indicate relevant 
results 
implicit relevance feedback irf has been proposed as a way in 
which search queries can be improved by passively observing 
searchers as they interact irf has been implemented either through 
the use of surrogate measures based on interaction with documents 
 such as reading time scrolling or document retention or using 
interaction with browse-based result interfaces irf has been 
shown to display mixed effectiveness because the factors that are good 
indicators of user interest are often erratic and the inferences drawn 
from user interaction are not always valid 
in this paper we present a study into the use and effectiveness of irf 
in an online search environment the study aims to investigate the 
factors that affect irf in particular three research questions i is the 
use of and perceived quality of terms generated by irf affected by the 
search task ii is the use of and perceived quality of terms generated 
by irf affected by the level of search experience of system users iii 
is irf equally used and does it generate terms that are equally useful 
at all search stages this study aims to establish when and under what 
circumstances irf performs well in terms of its use and the query 
modification terms selected as a result of its use 
the main experiment from which the data are taken was designed to 
test techniques for selecting query modification terms and techniques 
for displaying retrieval results in this paper we use data derived 
from that experiment to study factors affecting the utility of irf 
 study 
in this section we describe the user study conducted to address our 
research questions 
 systems 
our study used two systems both of which suggested new query terms 
to the user one system suggested terms based on the user s 
interaction irf the other used explicit rf erf asking the user to 
explicitly indicate relevant material both systems used the same term 
suggestion algorithm and used a common interface 
 interface overview 
in both systems retrieved documents are represented at the interface 
by their full-text and a variety of smaller query-relevant 
representations created at retrieval time we used the web as the test 
collection in this study and google 
as the underlying search engine 
document representations include the document title and a summary 
of the document a list of top-ranking sentences trs extracted from 
the top documents retrieved scored in relation to the query a sentence 
in the document summary and each summary sentence in the context 
it occurs in the document i e with the preceding and following 
sentence each summary sentence and top-ranking sentence is 
regarded as a representation of the document the default display 
contains the list of top-ranking sentences and the list of the first ten 
document titles interacting with a representation guides searchers to a 
different representation from the same document e g moving the 
mouse over a document title displays a summary of the document 
this presentation of progressively more information from documents 
to aid relevance assessments has been shown to be effective in earlier 
work in appendix a we show the complete interface to the 
irf system with the document representations marked and in 
appendix b we show a fragment from the erf interface with the 
checkboxes used by searchers to indicate relevant information both 
systems provide an interactive query expansion feature by suggesting 
new query terms to the user the searcher has the responsibility for 
choosing which if any of these terms to add to the query the 
searcher can also add or remove terms from the query at will 
 explicit rf system 
this version of the system implements explicit rf next to each 
document representation are checkboxes that allow searchers to mark 
individual representations as relevant marking a representation is an 
indication that its contents are relevant only the representations 
marked relevant by the user are used for suggesting new query terms 
this system was used as a baseline against which the irf system 
could be compared 
 implicit rf system 
this system makes inferences about searcher interests based on the 
information with which they interact as described in section 
interacting with a representation highlights a new representation from 
the same document to the searcher this is a way they can find out 
more information from a potentially interesting source to the implicit 
rf system each interaction with a representation is interpreted as an 
implicit indication of interest in that representation interacting with a 
representation is assumed to be an indication that its contents are 
relevant the query modification terms are selected using the same 
algorithm as in the explicit rf system therefore the only difference 
between the systems is how relevance is communicated to the system 
the results of the main experiment indicated that these two 
systems were comparable in terms of effectiveness 
 tasks 
search tasks were designed to encourage realistic search behaviour by 
our subjects the tasks were phrased in the form of simulated work 
task situations i e short search scenarios that were designed to 
reflect real-life search situations and allow subjects to develop 
personal assessments of relevance we devised six search topics i e 
applying to university allergies in the workplace art galleries in 
rome third generation mobile phones internet music piracy and 
petrol prices based on pilot testing with a small representative group 
of subjects these subjects were not involved in the main experiment 
for each topic three versions of each work task situation were 
devised each version differing in their predicted level of task 
complexity as described in task complexity is a variable that 
affects subject perceptions of a task and their interactive behaviour 
e g subjects perform more filtering activities with highly complex 
search tasks by developing tasks of different complexity we can 
assess how the nature of the task affects the subjects interactive 
behaviour and hence the evidence supplied to irf algorithms task 
complexity was varied according to the methodology described in 
specifically by varying the number of potential information sources 
and types of information required to complete a task in our pilot 
tests and in a posteriori analysis of the main experiment results we 
verified that subjects reporting of individual task complexity matched 
our estimation of the complexity of the task 
subjects attempted three search tasks one high complexity one 
moderate complexity and one low complexity 
 they were asked to 
read the task place themselves in the situation it described and find 
the information they felt was required to complete the task figure 
shows the task statements for three levels of task complexity for one 
of the six search topics 
hc task high complexity 
whilst having dinner with an american colleague they comment on the 
high price of petrol in the uk compared to other countries despite large 
volumes coming from the same source unaware of any major differences 
you decide to find out how and why petrol prices vary worldwide 
mc task moderate complexity 
whilst out for dinner one night one of your friends guests is complaining 
about the price of petrol and the factors that cause it throughout the night 
they seem to be complaining about everything they can reducing the 
credibility of their earlier statements so you decide to research which 
factors actually are important in determining the price of petrol in the uk 
lc task low complexity 
while out for dinner one night your friend complains about the rising 
price of petrol however as you have not been driving for long you are 
unaware of any major changes in price you decide to find out how the 
price of petrol has changed in the uk in recent years 
figure varying task complexity petrol prices topic 
 subjects 
 volunteers expressed an interest in participating in our study 
subjects were selected from this set with the aim of populating two 
groups each with subjects inexperienced infrequent 
inexperienced searchers and experienced frequent experienced 
searchers subjects were not chosen and classified into their groups 
until they had completed an entry questionnaire that asked them about 
their search experience and computer use 
the average age of the subjects was years maximum 
minimum σ years and had a university diploma or a 
higher degree of subjects had or were pursuing a 
qualification in a discipline related to computer science the subjects 
were a mixture of students researchers academic staff and others 
with different levels of computer and search experience the subjects 
were divided into the two groups depending on their search 
experience how often they searched and the types of searches they 
performed all were familiar with web searching and some with 
searching in other domains 
 methodology 
the experiment had a factorial design with levels of search 
experience experimental systems although we only report on the 
findings from the erf and irf systems and levels of search task 
complexity subjects attempted one task of each complexity 
 
the main experiment from which these results are drawn had a third 
comparator system which had a different interface each subject 
carried out three tasks one on each system we only report on the 
results from the erf and irf systems as these are the only pertinent 
ones for this paper 
switched systems after each task and used each system once the 
order in which systems were used and search tasks attempted was 
randomised according to a latin square experimental design 
questionnaires used likert scales semantic differentials and 
openended questions to elicit subject opinions system logging was 
also used to record subject interaction 
a tutorial carried out prior to the experiment allowed subjects to use a 
non-feedback version of the system to attempt a practice task before 
using the first experimental system experiments lasted between 
oneand-a-half and two hours dependent on variables such as the time 
spent completing questionnaires subjects were offered a minute 
break after the first hour in each experiment 
i the subject was welcomed and asked to read an introduction to 
the experiments and sign consent forms this set of instructions 
was written to ensure that each subject received precisely the 
same information 
ii the subject was asked to complete an introductory questionnaire 
this contained questions about the subject s education general 
search experience computer experience and web search 
experience 
iii the subject was given a tutorial on the interface followed by a 
training topic on a version of the interface with no rf 
iv the subject was given three task sheets and asked to choose one 
task from the six topics on each sheet no guidelines were given 
to subjects when choosing a task other than they could not 
choose a task from any topic more than once task complexity 
was rotated by the experimenter so each subject attempted one 
high complexity task one moderate complexity task and one low 
complexity task 
v the subject was asked to perform the search and was given 
minutes to search the subject could terminate a search early if 
they were unable to find any more information they felt helped 
them complete the task 
vi after completion of the search the subject was asked to complete 
a post-search questionnaire 
vii the remaining tasks were attempted by the subject following 
steps v and vi 
viii the subject completed a post-experiment questionnaire and 
participated in a post-experiment interview 
subjects were told that their interaction may be used by the irf 
system to help them as they searched they were not told which 
behaviours would be used or how it would be used 
we now describe the findings of our analysis 
 findings 
in this section we use the data derived from the experiment to answer 
our research questions about the effect of search task complexity 
search experience and stage in search on the use and effectiveness of 
irf we present our findings per research question due to the 
ordinal nature of much of the data non-parametric statistical testing is 
used in this analysis and the level of significance is set to p 
unless otherwise stated we use the method proposed by to 
determine the significance of differences in multiple comparisons and 
that of to test for interaction effects between experimental 
variables the occurrence of which we report where appropriate all 
likert scales and semantic differentials were on a -point scale where 
a rating closer to signifies more agreement with the attitude 
statement the category labels hc mc and lc are used to denote the 
high moderate and low complexity tasks respectively the highest or 
most positive values in each table are shown in bold our analysis 
uses data from questionnaires post-experiment interviews and 
background system logging on the erf and irf systems 
 search task 
searchers attempted three search tasks of varying complexity each on 
a different experimental system in this section we present an analysis 
on the use and usefulness of irf for search tasks of different 
complexities we present our findings in terms of the rf provided by 
subjects and the terms recommended by the systems 
 feedback 
we use questionnaires and system logs to gather data on subject 
perceptions and provision of rf for different search tasks in the 
postsearch questionnaire subjects were asked about how rf was conveyed 
using differentials to elicit their opinion on 
 the value of the feedback technique how you conveyed relevance 
to the system i e ticking boxes or viewing information was easy 
difficult effective ineffective useful not useful 
 the process of providing the feedback how you conveyed relevance 
to the system made you feel comfortable uncomfortable in 
control not in control 
the average obtained differential values are shown in table for irf 
and each task category the value corresponding to the differential 
all represents the mean of all differentials for a particular attitude 
statement this gives some overall understanding of the subjects 
feelings which can be useful as the subjects may not answer individual 
differentials very precisely the values for erf are included for 
reference in this table and all other tables and figures in the findings 
section since the aim of the paper is to investigate situations in which 
irf might perform well not a direct comparison between irf and 
erf we make only limited comparisons between these two types of 
feedback 
table subject perceptions of rf method lower better 
each cell in table summarises the subject responses for 
tasksystem pairs subjects who ran a high complexity hc task on the 
erf system subjects who ran a medium complexity mc task on 
the erf system etc kruskal-wallis tests were applied to each 
differential for each type of rf 
 subject responses suggested that 
 
since this analysis involved many differentials we use a bonferroni 
correction to control the experiment-wise error rate and set the alpha 
level α to and for both statements and 
respectively i e divided by the number of differentials this 
correction reduces the number of type i errors i e rejecting null 
hypotheses that are true 
explicit rf implicit rf 
differential 
hc mc lc hc mc lc 
easy 
effective 
useful 
all 
comfortable 
in control 
all 
irf was most effective and useful for more complex search tasks 
and that the differences in all pair-wise comparisons between tasks 
were significant 
 subject perceptions of irf elicited using the other 
differentials did not appear to be affected by the complexity of the 
search task 
 to determine whether a relationship exists between the 
effectiveness and usefulness of the irf process and task complexity 
we applied spearman s rank order correlation coefficient to 
participant responses the results of this analysis suggest that the 
effectiveness of irf and usefulness of irf are both related to task 
complexity as task complexity increases subject preference for irf 
also increases 
 
on the other hand subjects felt erf was more effective and useful 
for low complexity tasks 
 their verbal reporting of erf where 
perceived utility and effectiveness increased as task complexity 
decreased supports this finding in tasks of lower complexity the 
subjects felt they were better able to provide feedback on whether or 
not documents were relevant to the task 
we analyse interaction logs generated by both interfaces to investigate 
the amount of rf subjects provided to do this we use a measure of 
search precision that is the proportion of all possible document 
representations that a searcher assessed divided by the total number 
they could assess in erf this is the proportion of all possible 
representations that were marked relevant by the searcher i e those 
representations explicitly marked relevant in irf this is the 
proportion of representations viewed by a searcher over all possible 
representations that could have been viewed by the searcher this 
proportion measures the searcher s level of interaction with a 
document we take it to measure the user s interest in the document 
the more document representations viewed the more interested we 
assume a user is in the content of the document 
there are a maximum of representations per document 
topranking sentences title summary summary sentences and 
summary sentences in document context since the interface shows 
document representations from the top- documents there are 
representations that a searcher can assess table shows proportion 
of representations provided as rf by subjects 
table feedback and documents viewed 
explicit rf implicit rf 
measure 
hc mc lc hc mc lc 
proportion 
feedback 
 
documents 
viewed 
 
for irf there is a clear pattern as complexity increases the subjects 
viewed fewer documents but viewed more representations for each 
document this suggests a pattern where users are investigating 
retrieved documents in more depth it also means that the amount of 
 
effective χ 
 p useful χ 
 p 
 
dunn s post-hoc tests multiple comparison using rank sums all z ≥ 
 all p ≤ 
 
all χ 
 ≤ all p ≥ kruskal-wallis tests 
 
effective all r ≥ p ≤ useful all r ≥ p ≤ 
 
effective χ 
 p useful χ 
 p 
 kruskal-wallis test all pair-wise differences significant all z ≥ 
 all p ≤ dunn s post-hoc tests 
feedback varies based on the complexity of the search task since irf 
is based on the interaction of the searcher the more they interact the 
more feedback they provide this has no effect on the number of rf 
terms chosen but may affect the quality of the terms selected 
correlation analysis revealed a strong negative correlation between the 
number of documents viewed and the amount of feedback searchers 
provide 
 as the number of documents viewed increases the proportion 
of feedback falls searchers view less representations of each 
document this may be a natural consequence of their being less 
time to view documents in a time constrained task environment but as 
we will show as complexity changes the nature of information 
searchers interact with also appears to change in the next section we 
investigate the effect of task complexity on the terms chosen as a 
result of irf 
 terms 
the same rf algorithm was used to select query modification terms in 
all systems we use subject opinions of terms recommended by 
the systems as a measure of the effectiveness of irf with respect to 
the terms generated for different search tasks to test this subjects 
were asked to complete two semantic differentials that completed the 
statement the words chosen by the system were 
relevant irrelevant and useful not useful table presents 
average responses grouped by search task 
table subject perceptions of system terms lower better 
explicit rf implicit rf 
differential 
hc mc lc hc mc lc 
relevant 
useful 
kruskal-wallis tests were applied within each type of rf the 
results indicate that the relevance and usefulness of the terms chosen 
by irf is affected by the complexity of the search task the terms 
chosen are more relevant and useful when the search task is more 
complex 
relevant here was explained as being related to their task 
whereas useful was for terms that were seen as being helpful in the 
search task for erf the results indicate that the terms generated are 
perceived to be more relevant and useful for less complex search 
tasks although differences between tasks were not significant 
 this 
suggests that subject perceptions of the terms chosen for query 
modification are affected by task complexity comparison between 
erf and irf shows that subject perceptions also vary for different 
types of rf 
 
as well as using data on relevance and utility of the terms chosen we 
used data on term acceptance to measure the perceived value of the 
terms suggested explicit and implicit rf systems made 
recommendations about which terms could be added to the original 
search query in table we show the proportion of the top six terms 
 
r − p pearson s correlation coefficient 
 
relevant χ 
 p useful χ 
 p α 
 
 
all χ 
 ≤ all p ≥ kruskal-wallis test 
 
all t ≥ all p ≤ wilcoxon signed-rank test 
 
that were shown to the searcher that were added to the search 
query for each type of task and each type of rf 
table term acceptance percentage of top six terms 
explicit rf implicit rfproportion 
of terms hc mc lc hc mc lc 
accepted 
the average number of terms accepted from irf is approximately the 
same across all search tasks and generally the same as that of erf 
 
as table shows subjects marked fewer documents relevant for 
highly complex tasks therefore when task complexity increases the 
erf system has fewer examples of relevant documents and the 
expansion terms generated may be poorer this could explain the 
difference in the proportion of recommended terms accepted in erf 
as task complexity increases for irf there is little difference in how 
many of the recommended terms were chosen by subjects for each 
level of task complexity 
 subjects may have perceived irf terms as 
more useful for high complexity tasks but this was not reflected in the 
proportion of irf terms accepted differences may reside in the 
nature of the terms accepted future work will investigate this issue 
 summary 
in this section we have presented an investigation on the effect of 
search task complexity on the utility of irf from the results there 
appears to be a strong relation between the complexity of the task and 
the subject interaction subjects preferring irf for highly complex 
tasks task complexity did not affect the proportion of terms accepted 
in either rf method despite there being a difference in how 
relevant and useful subjects perceived the terms to be for different 
complexities complexity may affect term selection in ways other than 
the proportion of terms accepted 
 search experience 
experienced searchers may interact differently and give different 
types of evidence to rf than inexperienced searchers as such levels 
of search experience may affect searchers use and perceptions of irf 
in our experiment subjects were divided into two groups based on 
their level of search experience the frequency with which they 
searched and the types of searches they performed in this section we 
use their perceptions and logging to address the next research 
question the relationship between the usefulness and use of irf and 
the search experience of experimental subjects the data are the same 
as that analysed in the previous section but here we focus on search 
experience rather than the search task 
 feedback 
we analyse the results from the attitude statements described at the 
beginning of section i e how you conveyed relevance to the 
system was  and how you conveyed relevance to the system made 
you feel  these differentials elicited opinion from experimental 
subjects about the rf method used in table we show the mean 
average responses for inexperienced and experienced subject groups 
on erf and irf subjects per cell 
 
this was the smallest number of query modification terms that were 
offered in both systems 
 
all t ≥ all p ≤ wilcoxon signed-rank test 
 
erf χ 
 p irf χ 
 p 
 kruskalwallis tests 
table subject perceptions of rf method lower better 
the results demonstrate a strong preference in inexperienced subjects 
for irf they found it more easy and effective than experienced 
subjects 
the differences for all other irf differentials were not 
statistically significant for all differentials apart from in control 
inexperienced subjects generally preferred irf over erf 
 
inexperienced subjects also felt that irf was more difficult to control 
than experienced subjects 
 as these subjects have less search 
experience they may be less able to understand rf processes and may 
be more comfortable with the system gathering feedback implicitly 
from their interaction experienced subjects tended to like erf more 
than inexperienced subjects and felt more comfortable with this 
feedback method 
 it appears from these results that experienced 
subjects found erf more useful and were more at ease with the erf 
process 
in a similar way to section we analysed the proportion of 
feedback that searchers provided to the experimental systems our 
analysis suggested that search experience does not affect the amount 
of feedback subjects provide 
 
 terms 
we used questionnaire responses to gauge subject opinion on the 
relevance and usefulness of the terms from the perspective of 
experienced and inexperienced subjects table shows the average 
differential responses obtained from both subject groups 
table subject perceptions of system terms lower better 
explicit rf implicit rf 
differential 
inexp exp inexp exp 
relevant 
useful 
the differences between subject groups were significant 
 
experienced subjects generally reacted to the query modification 
terms chosen by the system more positively than inexperienced 
 
easy u p effective u p α 
 mann-whitney tests 
 
all t ≥ all p ≤ wilcoxon signed-rank test 
 
u p α mann-whitney test 
 
t p wilcoxon signed-rank test 
 
erf all u ≤ p ≥ irf all u ≤ p ≥ 
 mannwhitney tests 
 
erf all u ≥ p ≤ irf all u ≥ p ≤ 
explicit rf implicit rf 
differential 
inexp exp inexp exp 
easy 
effective 
useful 
all 
comfortable 
in control 
all 
subjects this finding was supported by the proportion of query 
modification terms these subjects accepted in the same way as in 
section we analysed the number of query modification terms 
recommended by the system that were used by experimental subjects 
table shows the average number of accepted terms per subject 
group 
table term acceptance percentage of top six terms 
explicit rf implicit rfproportion 
of terms inexp exp inexp exp 
accepted 
our analysis of the data show that differences between subject groups 
for each type of rf are significant experienced subjects accepted 
more expansion terms regardless of type of rf however the 
differences between the same groups for different types of rf are not 
significant subjects chose roughly the same percentage of expansion 
terms offered irrespective of the type of rf 
 
 summary 
in this section we have analysed data gathered from two subject 
groups - inexperienced searchers and experienced searchers - on how 
they perceive and use irf the results indicate that inexperienced 
subjects found irf more easy and effective than experienced 
subjects who in turn found the terms chosen as a result of irf more 
relevant and useful we also showed that inexperienced subjects 
generally accepted less recommended terms than experienced 
subjects perhaps because they were less comfortable with rf or 
generally submitted shorter search queries search experience appears 
to affect how subjects use the terms recommended as a result of the 
rf process 
 search stage 
from our observations of experimental subjects as they searched we 
conjectured that rf may be used differently at different times during a 
search to test this our third research question concerned the use and 
usefulness of irf during the course of a search in this section we 
investigate whether the amount of rf provided by searchers or the 
proportion of terms accepted are affected by how far through their 
search they are for the purposes of this analysis a search begins when 
a subject poses the first query to the system and progresses until they 
terminate the search or reach the maximum allowed time for a search 
task of minutes we do not divide tasks based on this limit as 
subjects often terminated their search in less than minutes 
in this section we use data gathered from interaction logs and subject 
opinions to investigate the extent to which rf was used and the extent 
to which it appeared to benefit our experimental subjects at different 
stages in their search 
 feedback 
the interaction logs for all searches on the explicit rf and implicit 
rf were analysed and each search is divided up into nine equal length 
time slices this number of slices gave us an equal number per stage 
and was a sufficient level of granularity to identify trends in the 
results slices - correspond to the start of the search - to the 
middle of the search and - to the end in figure we plot the 
measure of precision described in section i e the proportion 
of all possible representations that were provided as rf at each of the 
 
irf u p erf u p 
nine slices per search task averaged across all subjects this allows us 
to see how the provision of rf was distributed during a search the 
total amount of feedback for a single rf method task complexity 
pairing across all nine slices corresponds to the value recorded in the 
first row of table e g the sum of the rf for irf hc across all nine 
slices of figure is to simplify the statistical analysis and 
comparison we use the grouping of start middle and end 
 
 
 
 
 
 
 
 
 
 
 
slice 
search precision oftotalrepsprovidedasrf 
explicit rf hc 
explicit rf mc 
explicit rf lc 
implicit rf hc 
implicit rf mc 
implicit rf lc 
figure distribution of rf provision per search task 
figure appears to show the existence of a relationship between the 
stage in the search and the amount of relevance information provided 
to the different types of feedback algorithm these are essentially 
differences in the way users are assessing documents in the case of 
erf subjects provide explicit relevance assessments throughout most 
of the search but there is generally a steep increase in the end phase 
towards the completion of the search 
 
when using the irf system the data indicates that at the start of the 
search subjects are providing little relevance information 
 which 
corresponds to interacting with few document representations at this 
stage the subjects are perhaps concentrating more on reading the 
retrieved results implicit relevance information is generally offered 
extensively in the middle of the search as they interact with results and 
it then tails off towards the end of the search this would appear to 
correspond to stages of initial exploration detailed analysis of 
document representations and storage and presentation of findings 
figure also shows the proportion of feedback for tasks of different 
complexity the results appear to show a difference 
in how irf is 
used that relates to the complexity of the search task more 
specifically as complexity increases it appears as though subjects take 
longer to reach their most interactive point this suggests that task 
complexity affects how irf is distributed during the search and that 
they may be spending more time initially interpreting search results 
for more complex tasks 
 
irf all z ≥ p ≤ erf start vs end z p 
 dunn s post-hoc tests 
 
although increasing toward the end of the start stage 
 
although not statistically significant χ 
 p 
 friedman rank sum test 
 terms 
the terms recommended by the system are chosen based on the 
frequency of their occurrence in the relevant items that is 
nonstopword non-query terms occurring frequently in search results 
regarded as relevant are likely to be recommended to the searcher for 
query modification since there is a direct association between the rf 
and the terms selected we use the number of terms accepted by 
searchers at different points in the search as an indication of how 
effective the rf has been up until the current point in the search in 
this section we analysed the average number of terms from the top six 
terms recommended by explicit rf and implicit rf over the course of 
a search the average proportion of the top six recommended terms 
that were accepted at each stage are shown in table each cell 
contains data from all subjects 
table term acceptance proportion of top six terms 
explicit rf implicit rfproportion 
of terms start middle end start middle end 
accepted 
the results show an apparent association between the stage in the 
search and the number of feedback terms subjects accept search 
stage affects term acceptance in irf but not in erf 
 the further 
into a search a searcher progresses the more likely they are to accept 
terms recommended via irf significantly more than erf 
 a 
correlation analysis between the proportion of terms accepted at each 
search stage and cumulative rf i e the sum of all precision at each 
slice in figure up to and including the end of the search stage 
suggests that in both types of rf the quality of system terms improves 
as more rf is provided 
 
 summary 
the results from this section indicate that the location in a search 
affects the amount of feedback given by the user to the system and 
hence the amount of information that the rf mechanism has to decide 
which terms to offer the user further trends in the data suggest that 
the complexity of the task affects how subjects provide irf and the 
proportion of system terms accepted 
 discussion and implications 
in this section we discuss the implications of the findings presented in 
the previous section for each research question 
 search task 
the results of our study showed that erf was preferred for less 
complex tasks and irf for more complex tasks from observations 
and subject comments we perceived that when using erf systems 
subjects generally forgot to provide the feedback but also employed 
different criteria during the erf process i e they were assessing 
relevance rather than expressing an interest when the search was 
more complex subjects rarely found results they regarded as 
completely relevant therefore they struggled to find relevant 
 
erf χ 
 p irf χ 
 p friedman 
rank sum tests irf all pair-wise comparisons significant at z ≥ 
 all p ≤ dunn s post-hoc tests 
 
all t ≥ all p ≤ wilcoxon signed-rank test 
 
irf r p erf r p pearson 
correlation coefficient 
information and were unable to communicate rf to the search system 
in these situations subjects appeared to prefer irf as they do not need 
to make a relevance decision to obtain the benefits of rf i e term 
suggestions whereas in erf they do 
the association between rf method and task complexity has 
implications for the design of user studies of rf systems and the rf 
systems themselves it implies that in the design of user studies 
involving erf or irf systems care should be taken to include tasks of 
varying complexities to avoid task bias also in the design of search 
systems it implies that since different types of rf may be appropriate 
for different task complexities then a system that could automatically 
detect complexity could use both erf and irf simultaneously to 
benefit the searcher for example on the irf system we noticed that 
as task complexity falls search behaviour shifts from results interface 
to retrieved documents monitoring such interaction across a number 
of studies may lead to a set of criteria that could help ir systems 
automatically detect task complexity and tailor support to suit 
 search experience 
we analysed the affect of search experience on the utility of irf our 
analysis revealed a general preference across all subjects for irf over 
erf that is the average ratings assigned to irf were generally more 
positive than those assigned to erf however irf was generally 
liked by both subject groups perhaps because it removed the burden 
of providing relevance information and erf was generally preferred 
by experienced subjects more than inexperienced subjects perhaps 
because it allowed them to specify which results were used by the 
system when generating term recommendations 
all subjects felt more in control with erf than irf but for 
inexperienced subjects this did not appear to affect their overall 
preferences 
 these subjects may understand the rf process less but 
may be more willing to sacrifice control over feedback in favour of 
irf a process that they perceive more positively 
 search stage 
we also analysed the effects of search stage on the use and usefulness 
of irf through analysis of this nature we can build a more complete 
picture of how searchers used rf and how this varies based on the rf 
method the results suggest that irf is used more in the middle of 
the search than at the beginning or end whereas erf is used more 
towards the end the results also show the effects of task complexity 
on the irf process and how rapidly subjects reach their most 
interactive point without an analysis of this type it would not have 
been possible to establish the existence of such patterns of behaviour 
the findings suggest that searchers interact differently for irf and 
erf since erf is not traditionally used until toward the end of the 
search it may be possible to incorporate both irf and erf into the 
same ir system with irf being used to gather evidence until subjects 
decide to use erf the development of such a system represents part 
of our ongoing work in this area 
 conclusions 
in this paper we have presented an investigation of implicit relevance 
feedback irf we aimed to answer three research questions about 
factors that may affect the provision and usefulness of irf these 
factors were search task complexity the subjects search experience 
and the stage in the search our overall conclusion was that all factors 
 
this may also be true for experienced subjects but the data we have 
is insufficient to draw this conclusion 
appear to have some effect on the use and effectiveness of irf 
although the interaction effects between factors are not statistically 
significant 
our conclusions per each research question are i irf is generally 
more useful for complex search tasks where searchers want to focus 
on the search task and get new ideas for their search from the system 
 ii irf is preferred to erf overall and generally preferred by 
inexperienced subjects wanting to reduce the burden of providing rf 
and iii within a single search session irf is affected by temporal 
location in a search i e it is used in the middle not the beginning or 
end and task complexity 
studies of this nature are important to establish the circumstances 
where a promising technique such as irf are useful and those when it 
is not it is only after such studies have been run and analysed in this 
way can we develop an understanding of irf that allow it to be 
successfully implemented in operational ir systems 
 references 
 bell d j and ruthven i searchers assessments of task 
complexity for web searching proceedings of the th european 
conference on information retrieval - 
 borlund p experimental components for the evaluation 
of interactive information retrieval systems journal of 
documentation - 
 brajnik g mizzaro s tasso c and venuti f 
strategic help for user interfaces for information retrieval 
journal of the american society for information science and 
technology - 
 busha c h and harter s p research methods in 
librarianship techniques and interpretation library and 
information science series new york academic press 
 campbell i and van rijsbergen c j the ostensive 
model of developing information needs proceedings of the rd 
international conference on conceptions of library and 
information science - 
 harman d relevance feedback and other query 
modification techniques in information retrieval data 
structures and algorithms new york prentice-hall 
 kelly d and teevan j implicit feedback for inferring 
user preference sigir forum - 
 koenemann j and belkin n j a case for interaction a 
study of interactive information retrieval behavior and 
effectiveness proceedings of the acm sigchi conference on 
human factors in computing systems - 
 meddis r statistics using ranks a unified approach 
oxford basil blackwell - 
 morita m and shinoda y information filtering based 
on user behavior analysis and best match text retrieval 
proceedings of the th annual acm sigir conference on 
research and development in information retrieval - 
 salton g and buckley c improving retrieval 
performance by relevance feedback journal of the american 
society for information science - 
 siegel s and castellan n j nonparametric statistics for 
the behavioural sciences nd ed singapore mcgraw-hill 
 white r w implicit feedback for interactive information 
retrieval unpublished doctoral dissertation university of 
glasgow glasgow united kingdom 
 white r w jose j m and ruthven i an implicit 
feedback approach for interactive information retrieval 
information processing and management in press 
 white r w jose j m ruthven i and van rijsbergen c j 
 a simulated study of implicit feedback models 
proceedings of the th european conference on information 
retrieval - 
 zellweger p t regli s h mackinlay j d and chang b -w 
 the impact of fluid documents on reading and browsing 
an observational study proceedings of the acm sigchi 
conference on human factors in computing systems - 
appendix b checkboxes to mark 
relevant document titles in the 
explicit rf system 
appendix a interface to implicit rf system 
 top-ranking sentence title summary summary sentence sentence in context 
 
 
 
 
 
downloading textual hidden web content through 
keyword queries 
alexandros ntoulas 
ucla computer science 
ntoulas cs ucla edu 
petros zerfos 
ucla computer science 
pzerfos cs ucla edu 
junghoo cho 
ucla computer science 
cho cs ucla edu 
abstract 
an ever-increasing amount of information on the web today is 
available only through search interfaces the users have to type in a 
set of keywords in a search form in order to access the pages from 
certain web sites these pages are often referred to as the hidden 
web or the deep web since there are no static links to the hidden 
web pages search engines cannot discover and index such pages 
and thus do not return them in the results however according to 
recent studies the content provided by many hidden web sites is 
often of very high quality and can be extremely valuable to many 
users 
in this paper we study how we can build an effective hidden web 
crawler that can autonomously discover and download pages from 
the hidden web since the only entry point to a hidden web site 
is a query interface the main challenge that a hidden web crawler 
has to face is how to automatically generate meaningful queries to 
issue to the site here we provide a theoretical framework to 
investigate the query generation problem for the hidden web and we 
propose effective policies for generating queries automatically our 
policies proceed iteratively issuing a different query in every 
iteration we experimentally evaluate the effectiveness of these policies 
on real hidden web sites and our results are very promising for 
instance in one experiment one of our policies downloaded more 
than of a hidden web site that contains million 
documents after issuing fewer than queries 
categories and subject descriptors h information systems 
digital libraries h information systems content analysis 
and indexing h information systems information search 
and retrieval 
general terms algorithms performance design 
 introduction 
recent studies show that a significant fraction of web content 
cannot be reached by following links in particular a large 
part of the web is hidden behind search forms and is reachable 
only when users type in a set of keywords or queries to the forms 
these pages are often referred to as the hidden web or the 
deep web because search engines typically cannot index the 
pages and do not return them in their results thus the pages are 
essentially hidden from a typical web user 
according to many studies the size of the hidden web increases 
rapidly as more organizations put their valuable content online 
through an easy-to-use web interface in chang et al 
estimate that well over hidden-web sites currently exist 
on the web moreover the content provided by many hidden-web 
sites is often of very high quality and can be extremely valuable 
to many users for example pubmed hosts many high-quality 
papers on medical research that were selected from careful 
peerreview processes while the site of the us patent and trademarks 
office 
makes existing patent documents available helping 
potential inventors examine prior art 
in this paper we study how we can build a hidden-web crawler 
that can automatically download pages from the hidden web so 
that search engines can index them conventional crawlers rely 
on the hyperlinks on the web to discover pages so current search 
engines cannot index the hidden-web pages due to the lack of 
links we believe that an effective hidden-web crawler can have 
a tremendous impact on how users search information on the web 
 tapping into unexplored information the hidden-web 
crawler will allow an average web user to easily explore the 
vast amount of information that is mostly hidden at present 
since a majority of web users rely on search engines to discover 
pages when pages are not indexed by search engines they are 
unlikely to be viewed by many web users unless users go 
directly to hidden-web sites and issue queries there they cannot 
access the pages at the sites 
 improving user experience even if a user is aware of a 
number of hidden-web sites the user still has to waste a significant 
amount of time and effort visiting all of the potentially relevant 
sites querying each of them and exploring the result by making 
the hidden-web pages searchable at a central location we can 
significantly reduce the user s wasted time and effort in 
searching the hidden web 
 reducing potential bias due to the heavy reliance of many web 
users on search engines for locating information search engines 
influence how the users perceive the web users do not 
necessarily perceive what actually exists on the web but what 
is indexed by search engines according to a recent 
article several organizations have recognized the importance of 
bringing information of their hidden web sites onto the surface 
and committed considerable resources towards this effort our 
 
us patent office http www uspto gov 
 
crawlers are the programs that traverse the web automatically and 
download pages for search engines 
 
figure a single-attribute search interface 
hidden-web crawler attempts to automate this process for 
hidden web sites with textual content thus minimizing the 
associated costs and effort required 
given that the only entry to hidden web pages is through 
querying a search form there are two core challenges to 
implementing an effective hidden web crawler a the crawler has to 
be able to understand and model a query interface and b the 
crawler has to come up with meaningful queries to issue to the 
query interface the first challenge was addressed by raghavan 
and garcia-molina in where a method for learning search 
interfaces was presented here we present a solution to the second 
challenge i e how a crawler can automatically generate queries so 
that it can discover and download the hidden web pages 
clearly when the search forms list all possible values for a query 
 e g through a drop-down list the solution is straightforward we 
exhaustively issue all possible queries one query at a time when 
the query forms have a free text input however an infinite 
number of queries are possible so we cannot exhaustively issue all 
possible queries in this case what queries should we pick can the 
crawler automatically come up with meaningful queries without 
understanding the semantics of the search form 
in this paper we provide a theoretical framework to investigate 
the hidden-web crawling problem and propose effective ways of 
generating queries automatically we also evaluate our proposed 
solutions through experiments conducted on real hidden-web sites 
in summary this paper makes the following contributions 
 we present a formal framework to study the problem of 
hiddenweb crawling section 
 we investigate a number of crawling policies for the hidden 
web including the optimal policy that can potentially download 
the maximum number of pages through the minimum number of 
interactions unfortunately we show that the optimal policy is 
np-hard and cannot be implemented in practice section 
 we propose a new adaptive policy that approximates the optimal 
policy our adaptive policy examines the pages returned from 
previous queries and adapts its query-selection policy 
automatically based on them section 
 we evaluate various crawling policies through experiments on 
real web sites our experiments will show the relative 
advantages of various crawling policies and demonstrate their 
potential the results from our experiments are very promising in 
one experiment for example our adaptive policy downloaded 
more than of the pages within pubmed that contains 
million documents after it issued fewer than queries 
 framework 
in this section we present a formal framework for the study of 
the hidden-web crawling problem in section we describe our 
assumptions on hidden-web sites and explain how users interact 
with the sites based on this interaction model we present a 
highlevel algorithm for a hidden-web crawler in section finally in 
section we formalize the hidden-web crawling problem 
 hidden-web database model 
there exists a variety of hidden web sources that provide 
information on a multitude of topics depending on the type of 
information we may categorize a hidden-web site either as a textual 
database or a structured database a textual database is a site that 
figure a multi-attribute search interface 
mainly contains plain-text documents such as pubmed and 
lexisnexis an online database of legal documents since 
plaintext documents do not usually have well-defined structure most 
textual databases provide a simple search interface where users 
type a list of keywords in a single search box figure in 
contrast a structured database often contains multi-attribute relational 
data e g a book on the amazon web site may have the fields 
title  harry potter author  j k rowling and 
isbn   and supports multi-attribute search 
interfaces figure in this paper we will mainly focus on 
textual databases that support single-attribute keyword queries we 
discuss how we can extend our ideas for the textual databases to 
multi-attribute structured databases in section 
typically the users need to take the following steps in order to 
access pages in a hidden-web database 
 step first the user issues a query say liver through the 
search interface provided by the web site such as the one shown 
in figure 
 step shortly after the user issues the query she is presented 
with a result index page that is the web site returns a list of 
links to potentially relevant web pages as shown in figure a 
 step from the list in the result index page the user identifies 
the pages that look interesting and follows the links clicking 
on a link leads the user to the actual web page such as the one 
shown in figure b that the user wants to look at 
 a generic hidden web crawling algorithm 
given that the only entry to the pages in a hidden-web site 
is its search from a hidden-web crawler should follow the three 
steps described in the previous section that is the crawler has 
to generate a query issue it to the web site download the result 
index page and follow the links to download the actual pages in 
most cases a crawler has limited time and network resources so 
the crawler repeats these steps until it uses up its resources 
in figure we show the generic algorithm for a hidden-web 
crawler for simplicity we assume that the hidden-web crawler 
issues single-term queries only 
the crawler first decides which 
query term it is going to use step issues the query and 
retrieves the result index page step finally based on the links 
found on the result index page it downloads the hidden web pages 
from the site step this same process is repeated until all the 
available resources are used up step 
given this algorithm we can see that the most critical decision 
that a crawler has to make is what query to issue next if the 
crawler can issue successful queries that will return many matching 
pages the crawler can finish its crawling early on using minimum 
resources in contrast if the crawler issues completely irrelevant 
queries that do not return any matching pages it may waste all 
of its resources simply issuing queries without ever retrieving 
actual pages therefore how the crawler selects the next query can 
greatly affect its effectiveness in the next section we formalize 
this query selection problem 
 
for most web sites that assume and for multi-keyword 
queries single-term queries return the maximum number of results 
extending our work to multi-keyword queries is straightforward 
 
 a list of matching pages for query liver b the first matching page for liver 
figure pages from the pubmed web site 
algorithm crawling a hidden web site 
procedure 
 while there are available resources do 
 select a term to send to the site 
 qi selectterm 
 send query and acquire result index page 
 r qi querywebsite qi 
 download the pages of interest 
 download r qi 
 done 
figure algorithm for crawling a hidden web site 
s 
q 
q 
qq 
 
 
figure a set-formalization of the optimal query selection 
problem 
 problem formalization 
theoretically the problem of query selection can be formalized 
as follows we assume that the crawler downloads pages from a 
web site that has a set of pages s the rectangle in figure we 
represent each web page in s as a point dots in figure every 
potential query qi that we may issue can be viewed as a subset of s 
containing all the points pages that are returned when we issue qi 
to the site each subset is associated with a weight that represents 
the cost of issuing the query under this formalization our goal is to 
find which subsets queries cover the maximum number of points 
 web pages with the minimum total weight cost this problem 
is equivalent to the set-covering problem in graph theory 
there are two main difficulties that we need to address in this 
formalization first in a practical situation the crawler does not 
know which web pages will be returned by which queries so the 
subsets of s are not known in advance without knowing these 
subsets the crawler cannot decide which queries to pick to 
maximize the coverage second the set-covering problem is known to 
be np-hard so an efficient algorithm to solve this problem 
optimally in polynomial time has yet to be found 
in this paper we will present an approximation algorithm that 
can find a near-optimal solution at a reasonable computational cost 
our algorithm leverages the observation that although we do not 
know which pages will be returned by each query qi that we issue 
we can predict how many pages will be returned based on this 
information our query selection algorithm can then select the best 
queries that cover the content of the web site we present our 
prediction method and our query selection algorithm in section 
 performance metric 
before we present our ideas for the query selection problem we 
briefly discuss some of our notation and the cost performance 
metrics 
given a query qi we use p qi to denote the fraction of pages 
that we will get back if we issue query qi to the site for example if 
a web site has pages in total and if pages are returned 
for the query qi medicine then p qi we use p q ∧ 
q to represent the fraction of pages that are returned from both 
q and q i e the intersection of p q and p q similarly we 
use p q ∨ q to represent the fraction of pages that are returned 
from either q or q i e the union of p q and p q 
we also use cost qi to represent the cost of issuing the query 
qi depending on the scenario the cost can be measured either in 
time network bandwidth the number of interactions with the site 
or it can be a function of all of these as we will see later our 
proposed algorithms are independent of the exact cost function 
in the most common case the query cost consists of a number 
of factors including the cost for submitting the query to the site 
retrieving the result index page figure a and downloading the 
actual pages figure b we assume that submitting a query 
incurs a fixed cost of cq the cost for downloading the result index 
page is proportional to the number of matching documents to the 
query while the cost cd for downloading a matching document is 
also fixed then the overall cost of query qi is 
cost qi cq crp qi cdp qi 
in certain cases some of the documents from qi may have already 
been downloaded from previous queries in this case the crawler 
may skip downloading these documents and the cost of qi can be 
cost qi cq crp qi cdpnew qi 
here we use pnew qi to represent the fraction of the new 
documents from qi that have not been retrieved from previous queries 
later in section we will study how we can estimate p qi and 
pnew qi to estimate the cost of qi 
since our algorithms are independent of the exact cost function 
we will assume a generic cost function cost qi in this paper when 
we need a concrete cost function however we will use equation 
given the notation we can formalize the goal of a hidden-web 
crawler as follows 
 
problem find the set of queries q qn that maximizes 
p q ∨ · · · ∨ qn 
under the constraint 
n 
i 
cost qi ≤ t 
here t is the maximum download resource that the crawler has 
 keyword selection 
how should a crawler select the queries to issue given that the 
goal is to download the maximum number of unique documents 
from a textual database we may consider one of the following 
options 
 random we select random keywords from say an english 
dictionary and issue them to the database the hope is that a random 
query will return a reasonable number of matching documents 
 generic-frequency we analyze a generic document corpus 
collected elsewhere say from the web and obtain the generic 
frequency distribution of each keyword based on this generic 
distribution we start with the most frequent keyword issue it to the 
hidden-web database and retrieve the result we then continue 
to the second-most frequent keyword and repeat this process 
until we exhaust all download resources the hope is that the 
frequent keywords in a generic corpus will also be frequent in the 
hidden-web database returning many matching documents 
 adaptive we analyze the documents returned from the previous 
queries issued to the hidden-web database and estimate which 
keyword is most likely to return the most documents based on 
this analysis we issue the most promising query and repeat 
the process 
among these three general policies we may consider the 
random policy as the base comparison point since it is expected to 
perform the worst between the generic-frequency and the 
adaptive policies both policies may show similar performance if the 
crawled database has a generic document collection without a 
specialized topic the adaptive policy however may perform 
significantly better than the generic-frequency policy if the database has a 
very specialized collection that is different from the generic corpus 
we will experimentally compare these three policies in section 
while the first two policies random and generic-frequency 
policies are easy to implement we need to understand how we can 
analyze the downloaded pages to identify the most promising query 
in order to implement the adaptive policy we address this issue in 
the rest of this section 
 estimating the number of matching pages 
in order to identify the most promising query we need to 
estimate how many new documents we will download if we issue the 
query qi as the next query that is assuming that we have issued 
queries q qi− we need to estimate p q ∨· · ·∨qi− ∨qi for 
every potential next query qi and compare this value in estimating 
this number we note that we can rewrite p q ∨ · · · ∨ qi− ∨ qi 
as 
p q ∨ · · · ∨ qi− ∨ qi 
 p q ∨ · · · ∨ qi− p qi − p q ∨ · · · ∨ qi− ∧ qi 
 p q ∨ · · · ∨ qi− p qi 
− p q ∨ · · · ∨ qi− p qi q ∨ · · · ∨ qi− 
in the above formula note that we can precisely measure p q ∨ 
· · · ∨ qi− and p qi q ∨ · · · ∨ qi− by analyzing 
previouslydownloaded pages we know p q ∨ · · · ∨ qi− the union of 
all pages downloaded from q qi− since we have already 
issued q qi− and downloaded the matching pages 
we can 
also measure p qi q ∨ · · · ∨ qi− the probability that qi 
appears in the pages from q qi− by counting how many times 
qi appears in the pages from q qi− therefore we only need 
to estimate p qi to evaluate p q ∨ · · · ∨ qi we may consider a 
number of different ways to estimate p qi including the 
following 
 independence estimator we assume that the appearance of the 
term qi is independent of the terms q qi− that is we 
assume that p qi p qi q ∨ · · · ∨ qi− 
 zipf estimator in ipeirotis et al proposed a method to 
estimate how many times a particular term occurs in the entire 
corpus based on a subset of documents from the corpus their 
method exploits the fact that the frequency of terms inside text 
collections follows a power law distribution that is 
if we rank all terms based on their occurrence frequency with 
the most frequent term having a rank of second most frequent 
a rank of etc then the frequency f of a term inside the text 
collection is given by 
f α r β −γ 
 
where r is the rank of the term and α β and γ are constants that 
depend on the text collection 
their main idea is to estimate the three parameters α β and 
γ based on the subset of documents that we have downloaded 
from previous queries and use the estimated parameters to 
predict f given the ranking r of a term within the subset for 
a more detailed description on how we can use this method to 
estimate p qi we refer the reader to the extended version of 
this paper 
after we estimate p qi and p qi q ∨ · · · ∨ qi− values we 
can calculate p q ∨ · · · ∨ qi in section we explain how 
we can efficiently compute p qi q ∨ · · · ∨ qi− by maintaining a 
succinct summary table in the next section we first examine how 
we can use this value to decide which query we should issue next 
to the hidden web site 
 query selection algorithm 
the goal of the hidden-web crawler is to download the 
maximum number of unique documents from a database using its 
limited download resources given this goal the hidden-web crawler 
has to take two factors into account the number of new 
documents that can be obtained from the query qi and the cost of 
issuing the query qi for example if two queries qi and qj incur 
the same cost but qi returns more new pages than qj qi is more 
desirable than qj similarly if qi and qj return the same number 
of new documents but qi incurs less cost then qj qi is more 
desirable based on this observation the hidden-web crawler may 
use the following efficiency metric to quantify the desirability of 
the query qi 
efficiency qi 
pnew qi 
cost qi 
here pnew qi represents the amount of new documents returned 
for qi the pages that have not been returned for previous queries 
cost qi represents the cost of issuing the query qi 
intuitively the efficiency of qi measures how many new 
documents are retrieved per unit cost and can be used as an indicator of 
 
for exact estimation we need to know the total number of pages in 
the site however in order to compare only relative values among 
queries this information is not actually needed 
 
algorithm greedy selectterm 
parameters 
t the list of potential query keywords 
procedure 
 foreach tk in t do 
 estimate efficiency tk pnew tk 
cost tk 
 done 
 return tk with maximum efficiency tk 
figure algorithm for selecting the next query term 
how well our resources are spent when issuing qi thus the 
hidden web crawler can estimate the efficiency of every candidate qi 
and select the one with the highest value by using its resources 
more efficiently the crawler may eventually download the 
maximum number of unique documents in figure we show the query 
selection function that uses the concept of efficiency in principle 
this algorithm takes a greedy approach and tries to maximize the 
potential gain in every step 
we can estimate the efficiency of every query using the 
estimation method described in section that is the size of the new 
documents from the query qi pnew qi is 
pnew qi 
 p q ∨ · · · ∨ qi− ∨ qi − p q ∨ · · · ∨ qi− 
 p qi − p q ∨ · · · ∨ qi− p qi q ∨ · · · ∨ qi− 
from equation where p qi can be estimated using one of the 
methods described in section we can also estimate cost qi 
similarly for example if cost qi is 
cost qi cq crp qi cdpnew qi 
 equation we can estimate cost qi by estimating p qi and 
pnew qi 
 efficient calculation of query statistics 
in estimating the efficiency of queries we found that we need to 
measure p qi q ∨· · ·∨qi− for every potential query qi this 
calculation can be very time-consuming if we repeat it from scratch for 
every query qi in every iteration of our algorithm in this section 
we explain how we can compute p qi q ∨ · · · ∨ qi− efficiently 
by maintaining a small table that we call a query statistics table 
the main idea for the query statistics table is that p qi q ∨· · ·∨ 
qi− can be measured by counting how many times the keyword 
qi appears within the documents downloaded from q qi− 
we record these counts in a table as shown in figure a the 
left column of the table contains all potential query terms and the 
right column contains the number of previously-downloaded 
documents containing the respective term for example the table in 
figure a shows that we have downloaded documents so far and 
the term model appears in of these documents given this 
number we can compute that p model q ∨ · · · ∨ qi− 
 
 
we note that the query statistics table needs to be updated 
whenever we issue a new query qi and download more documents this 
update can be done efficiently as we illustrate in the following 
example 
example after examining the query statistics table of 
figure a we have decided to use the term computer as our next 
query qi from the new query qi computer we downloaded 
 more new pages out of these contain the keyword model 
term tk n tk 
model 
computer 
digital 
term tk n tk 
model 
computer 
disk 
total pages new pages 
 a after q qi− b new from qi computer 
term tk n tk 
model 
computer 
disk 
digital 
total pages 
 c after q qi 
figure updating the query statistics table 
q 
i i− 
q\ \ q 
q 
i 
 
s 
figure a web site that does not return all the results 
and the keyword disk the table in figure b shows the 
frequency of each term in the newly-downloaded pages 
we can update the old table figure a to include this new 
information by simply adding corresponding entries in figures a 
and b the result is shown on figure c for example keyword 
model exists in pages within the pages retrieved 
from q qi according to this new table p model q ∨· · ·∨qi 
is now 
 
 
 crawling sites that limit the number of 
results 
in certain cases when a query matches a large number of pages 
the hidden web site returns only a portion of those pages for 
example the open directory project allows the users to see only 
up to results after they issue a query obviously this kind 
of limitation has an immediate effect on our hidden web crawler 
first since we can only retrieve up to a specific number of pages 
per query our crawler will need to issue more queries and 
potentially will use up more resources in order to download all the 
pages second the query selection method that we presented in 
section assumes that for every potential query qi we can find 
p qi q ∨ · · · ∨ qi− that is for every query qi we can find the 
fraction of documents in the whole text database that contains qi 
with at least one of q qi− however if the text database 
returned only a portion of the results for any of the q qi− then 
the value p qi q ∨ · · · ∨ qi− is not accurate and may affect our 
decision for the next query qi and potentially the performance of 
our crawler since we cannot retrieve more results per query than 
the maximum number the web site allows our crawler has no other 
choice besides submitting more queries however there is a way 
to estimate the correct value for p qi q ∨ · · · ∨ qi− in the case 
where the web site returns only a portion of the results 
 
again assume that the hidden web site we are currently 
crawling is represented as the rectangle on figure and its pages as 
points in the figure assume that we have already issued queries 
q qi− which returned a number of results less than the 
maximum number than the site allows and therefore we have 
downloaded all the pages for these queries big circle in figure that 
is at this point our estimation for p qi q ∨· · ·∨qi− is accurate 
now assume that we submit query qi to the web site but due to a 
limitation in the number of results that we get back we retrieve the 
set qi small circle in figure instead of the set qi dashed circle 
in figure now we need to update our query statistics table so 
that it has accurate information for the next step that is although 
we got the set qi back for every potential query qi we need to 
find p qi q ∨ · · · ∨ qi 
p qi q ∨ · · · ∨ qi 
 
 
p q ∨ · · · ∨ qi 
· p qi ∧ q ∨ · · · ∨ qi− 
p qi ∧ qi − p qi ∧ qi ∧ q ∨ · · · ∨ qi− 
in the previous equation we can find p q ∨· · ·∨qi by 
estimating p qi with the method shown in section additionally we 
can calculate p qi ∧ q ∨ · · · ∨ qi− and p qi ∧ qi ∧ q ∨ 
· · · ∨ qi− by directly examining the documents that we have 
downloaded from queries q qi− the term p qi ∧ qi 
however is unknown and we need to estimate it assuming that qi 
is a random sample of qi then 
p qi ∧ qi 
p qi ∧ qi 
 
p qi 
p qi 
 
from equation we can calculate p qi ∧ qi and after we 
replace this value to equation we can find p qi q ∨ · · · ∨ qi 
 experimental evaluation 
in this section we experimentally evaluate the performance of 
the various algorithms for hidden web crawling presented in this 
paper our goal is to validate our theoretical analysis through 
realworld experiments by crawling popular hidden web sites of 
textual databases since the number of documents that are discovered 
and downloaded from a textual database depends on the selection 
of the words that will be issued as queries 
to the search interface 
of each site we compare the various selection policies that were 
described in section namely the random generic-frequency and 
adaptive algorithms 
the adaptive algorithm learns new keywords and terms from the 
documents that it downloads and its selection process is driven by 
a cost model as described in section to keep our experiment 
and its analysis simple at this point we will assume that the cost for 
every query is constant that is our goal is to maximize the number 
of downloaded pages by issuing the least number of queries later 
in section we will present a comparison of our policies based 
on a more elaborate cost model in addition we use the 
independence estimator section to estimate p qi from downloaded 
pages although the independence estimator is a simple estimator 
our experiments will show that it can work very well in practice 
for the generic-frequency policy we compute the frequency 
distribution of words that appear in a -million-web-page corpus 
 
throughout our experiments once an algorithm has submitted a 
query to a database we exclude the query from subsequent 
submissions to the same database from the same algorithm 
 
we defer the reporting of results based on the zipf estimation to a 
future work 
downloaded from web sites of various topics keywords 
are selected based on their decreasing frequency with which they 
appear in this document set with the most frequent one being 
selected first followed by the second-most frequent keyword etc 
regarding the random policy we use the same set of words 
collected from the web corpus but in this case instead of selecting 
keywords based on their relative frequency we choose them 
randomly uniform distribution in order to further investigate how 
the quality of the potential query-term list affects the random-based 
algorithm we construct two sets one with the most 
frequent words of the term collection used in the generic-frequency 
policy hereafter the random policy with the set of words 
will be referred to as random- k and another set with the 
million most frequent words of the same collection as above hereafter 
referred to as random- m the former set has frequent words that 
appear in a large number of documents at least in our 
collection and therefore can be considered of high-quality terms 
the latter set though contains a much larger collection of words 
among which some might be bogus and meaningless 
the experiments were conducted by employing each one of the 
aforementioned algorithms adaptive generic-frequency 
random k and random- m to crawl and download contents from three 
hidden web sites the pubmed medical library 
amazon 
and 
the open directory project according to the information on 
pubmed s web site its collection contains approximately 
million abstracts of biomedical articles we consider these abstracts 
as the documents in the site and in each iteration of the adaptive 
policy we use these abstracts as input to the algorithm thus our 
goal is to discover as many unique abstracts as possible by 
repeatedly querying the web query interface provided by pubmed the 
hidden web crawling on the pubmed web site can be considered 
as topic-specific due to the fact that all abstracts within pubmed 
are related to the fields of medicine and biology 
in the case of the amazon web site we are interested in 
downloading all the hidden pages that contain information on books 
the querying to amazon is performed through the software 
developer s kit that amazon provides for interfacing to its web site 
and which returns results in xml form the generic keyword 
field is used for searching and as input to the adaptive policy we 
extract the product description and the text of customer reviews 
when present in the xml reply since amazon does not provide 
any information on how many books it has in its catalogue we use 
random sampling on the -digit isbn number of the books to 
estimate the size of the collection out of the random isbn 
numbers queried are found in the amazon catalogue therefore 
the size of its book collection is estimated to be 
 
· 
 
million books it s also worth noting here that amazon poses an 
upper limit on the number of results books in our case returned 
by each query which is set to 
as for the third hidden web site the open directory project 
 hereafter also referred to as dmoz the site maintains the links to 
 million sites together with a brief summary of each listed site 
the links are searchable through a keyword-search interface we 
consider each indexed link together with its brief summary as the 
document of the dmoz site and we provide the short summaries 
to the adaptive algorithm to drive the selection of new keywords 
for querying on the dmoz web site we perform two hidden web 
crawls the first is on its generic collection of -million indexed 
 
we did not manually exclude stop words e g the is of etc 
from the keyword list as it turns out all web sites except pubmed 
return matching documents for the stop words such as the 
 
pubmed medical library http www pubmed org 
 
amazon inc http www amazon com 
 
 
 
 
 
 
 
 
 
 
 
 
 
fractionofdocuments 
query number 
cumulative fraction of unique documents - pubmed website 
adaptive 
generic-frequency 
random- k 
random- m 
figure coverage of policies for pubmed 
 
 
 
 
 
 
 
 
 
 
 
 
fractionofdocuments 
query number 
cumulative fraction of unique documents - amazon website 
adaptive 
generic-frequency 
random- k 
random- m 
figure coverage of policies for amazon 
sites regardless of the category that they fall into the other crawl 
is performed specifically on the arts section of dmoz http 
dmoz org arts which comprises of approximately 
indexed sites that are relevant to arts making this crawl 
topicspecific as in pubmed like amazon dmoz also enforces an upper 
limit on the number of returned results which is links with 
their summaries 
 comparison of policies 
the first question that we seek to answer is the evolution of the 
coverage metric as we submit queries to the sites that is what 
fraction of the collection of documents stored in the hidden web 
site can we download as we continuously query for new words 
selected using the policies described above more formally we are 
interested in the value of p q ∨ · · · ∨ qi− ∨ qi after we submit 
q qi queries and as i increases 
in figures and we present the coverage metric for 
each policy as a function of the query number for the web sites 
of pubmed amazon general dmoz and the art-specific dmoz 
respectively on the y-axis the fraction of the total documents 
downloaded from the website is plotted while the x-axis represents the 
query number a first observation from these graphs is that in 
general the generic-frequency and the adaptive policies perform much 
better than the random-based algorithms in all of the figures the 
graphs for the random- m and the random- k are significantly 
below those of other policies 
between the generic-frequency and the adaptive policies we can 
see that the latter outperforms the former when the site is topic 
spe 
 
 
 
 
 
 
 
 
 
 
 
fractionofdocuments 
query number 
cumulative fraction of unique documents - dmoz website 
adaptive 
generic-frequency 
random- k 
random- m 
figure coverage of policies for general dmoz 
 
 
 
 
 
 
 
 
 
 
 
 
fractionofdocuments 
query number 
cumulative fraction of unique documents - dmoz arts website 
adaptive 
generic-frequency 
random- k 
random- m 
figure coverage of policies for the arts section of dmoz 
cific for example for the pubmed site figure the adaptive 
algorithm issues only queries to download almost of the 
documents stored in pubmed but the generic-frequency algorithm 
requires queries for the same coverage for the dmoz arts 
crawl figure the difference is even more substantial the 
adaptive policy is able to download of the total sites indexed in 
the directory by issuing queries while the frequency-based 
algorithm is much less effective using the same number of queries 
and discovers only of the total number of indexed sites the 
adaptive algorithm by examining the contents of the pages that it 
downloads at each iteration is able to identify the topic of the site as 
expressed by the words that appear most frequently in the result-set 
consequently it is able to select words for subsequent queries that 
are more relevant to the site than those preferred by the 
genericfrequency policy which are drawn from a large generic collection 
table shows a sample of keywords out of chosen and 
submitted to the pubmed web site by the adaptive algorithm but not 
by the other policies for each keyword we present the number of 
the iteration along with the number of results that it returned as 
one can see from the table these keywords are highly relevant to 
the topics of medicine and biology of the public medical library 
and match against numerous articles stored in its web site 
in both cases examined in figures and the random-based 
policies perform much worse than the adaptive algorithm and the 
generic-frequency it is worthy noting however that the 
randombased policy with the small carefully selected set of 
quality words manages to download a considerable fraction of 
 
iteration keyword number of results 
 department 
 patients 
 clinical 
 treatment 
 medical 
 hospital 
 disease 
 protein 
table sample of keywords queried to pubmed exclusively by 
the adaptive policy 
from the pubmed web site after queries while the coverage 
for the arts section of dmoz reaches after queried 
keywords on the other hand the random-based approach that makes 
use of the vast collection of million words among which a large 
number is bogus keywords fails to download even a mere of the 
total collection after submitting the same number of query words 
for the generic collections of amazon and the dmoz sites shown 
in figures and respectively we get mixed results the 
genericfrequency policy shows slightly better performance than the 
adaptive policy for the amazon site figure and the adaptive method 
clearly outperforms the generic-frequency for the general dmoz site 
 figure a closer look at the log files of the two hidden web 
crawlers reveals the main reason amazon was functioning in a 
very flaky way when the adaptive crawler visited it resulting in 
a large number of lost results thus we suspect that the slightly 
poor performance of the adaptive policy is due to this 
experimental variance we are currently running another experiment to 
verify whether this is indeed the case aside from this experimental 
variance the amazon result indicates that if the collection and the 
words that a hidden web site contains are generic enough then the 
generic-frequency approach may be a good candidate algorithm for 
effective crawling 
as in the case of topic-specific hidden web sites the 
randombased policies also exhibit poor performance compared to the other 
two algorithms when crawling generic sites for the amazon web 
site random- k succeeds in downloading almost after 
issuing queries alas for the generic collection of dmoz the 
fraction of the collection of links downloaded is after the th 
query finally as expected random- m is even worse than 
random k downloading only of amazon and of the generic 
dmoz 
in summary the adaptive algorithm performs remarkably well in 
all cases it is able to discover and download most of the documents 
stored in hidden web sites by issuing the least number of queries 
when the collection refers to a specific topic it is able to identify 
the keywords most relevant to the topic of the site and consequently 
ask for terms that is most likely that will return a large number of 
results on the other hand the generic-frequency policy proves to 
be quite effective too though less than the adaptive it is able to 
retrieve relatively fast a large portion of the collection and when the 
site is not topic-specific its effectiveness can reach that of 
adaptive e g amazon finally the random policy performs poorly in 
general and should not be preferred 
 impact of the initial query 
an interesting issue that deserves further examination is whether 
the initial choice of the keyword used as the first query issued by 
the adaptive algorithm affects its effectiveness in subsequent 
iterations the choice of this keyword is not done by the selection of the 
 
 
 
 
 
 
 
 
 
 
 
 
fractionofdocuments 
query number 
convergence of adaptive under different initial queries - pubmed website 
pubmed 
data 
information 
return 
figure convergence of the adaptive algorithm using 
different initial queries for crawling the pubmed web site 
adaptive algorithm itself and has to be manually set since its query 
statistics tables have not been populated yet thus the selection is 
generally arbitrary so for purposes of fully automating the whole 
process some additional investigation seems necessary 
for this reason we initiated three adaptive hidden web crawlers 
targeting the pubmed web site with different seed-words the word 
data which returns results the word information 
that reports documents and the word return that 
retrieves pages out of million these keywords 
represent varying degrees of term popularity in pubmed with the first 
one being of high popularity the second of medium and the third 
of low we also show results for the keyword pubmed used in 
the experiments for coverage of section and which returns 
articles as we can see from figure after a small number of 
queries all four crawlers roughly download the same fraction of 
the collection regardless of their starting point their coverages 
are roughly equivalent from the th query eventually all four 
crawlers use the same set of terms for their queries regardless of 
the initial query in the specific experiment from the th query 
onward all four crawlers use the same terms for their queries in each 
iteration or the same terms are used off by one or two query 
numbers our result confirms the observation of that the choice of 
the initial query has minimal effect on the final performance we 
can explain this intuitively as follows our algorithm approximates 
the optimal set of queries to use for a particular web site once 
the algorithm has issued a significant number of queries it has an 
accurate estimation of the content of the web site regardless of 
the initial query since this estimation is similar for all runs of the 
algorithm the crawlers will use roughly the same queries 
 impact of the limit in the number of results 
while the amazon and dmoz sites have the respective limit of 
 and in their result sizes these limits may be larger 
than those imposed by other hidden web sites in order to 
investigate how a tighter limit in the result size affects the 
performance of our algorithms we performed two additional crawls to 
the generic-dmoz site we ran the generic-frequency and adaptive 
policies but we retrieved only up to the top results for 
every query in figure we plot the coverage for the two policies 
as a function of the number of queries as one might expect by 
comparing the new result in figure to that of figure where 
the result limit was we conclude that the tighter limit 
requires a higher number of queries to achieve the same coverage 
for example when the result limit was the adaptive 
pol 
 
 
 
 
 
 
 
 
 
 
 
 
fractionofuniquepages 
query number 
cumulative fraction of unique pages downloaded per query - dmoz web site cap limit 
adaptive 
generic-frequency 
figure coverage of general dmoz after limiting the number 
of results to 
icy could download of the site after issuing queries while 
it had to issue queries to download of the site when 
the limit was on the other hand our new result shows that 
even with a tight result limit it is still possible to download most 
of a hidden web site after issuing a reasonable number of queries 
the adaptive policy could download more than of the site 
after issuing queries when the limit was finally our 
result shows that our adaptive policy consistently outperforms the 
generic-frequency policy regardless of the result limit in both 
figure and figure our adaptive policy shows significantly larger 
coverage than the generic-frequency policy for the same number of 
queries 
 incorporating the document download 
cost 
for brevity of presentation the performance evaluation results 
provided so far assumed a simplified cost-model where every query 
involved a constant cost in this section we present results regarding 
the performance of the adaptive and generic-frequency algorithms 
using equation to drive our query selection process as we 
discussed in section this query cost model includes the cost for 
submitting the query to the site retrieving the result index page 
and also downloading the actual pages for these costs we 
examined the size of every result in the index page and the sizes of the 
documents and we chose cq cr and cd 
as values for the parameters of equation and for the particular 
experiment that we ran on the pubmed website the values that 
we selected imply that the cost for issuing one query and retrieving 
one result from the result index page are roughly the same while 
the cost for downloading an actual page is times larger we 
believe that these values are reasonable for the pubmed web site 
figure shows the coverage of the adaptive and 
genericfrequency algorithms as a function of the resource units used 
during the download process the horizontal axis is the amount of 
resources used and the vertical axis is the coverage as it is 
evident from the graph the adaptive policy makes more efficient use of 
the available resources as it is able to download more articles than 
the generic-frequency using the same amount of resource units 
however the difference in coverage is less dramatic in this case 
compared to the graph of figure the smaller difference is due 
to the fact that under the current cost metric the download cost of 
documents constitutes a significant portion of the cost therefore 
when both policies downloaded the same number of documents 
the saving of the adaptive policy is not as dramatic as before that 
 
 
 
 
 
 
 
 
 
 
 
 
fractionofuniquepages 
total cost cq cr cd 
cumulative fraction of unique pages downloaded per cost unit - pubmed web site 
adaptive 
frequency 
figure coverage of pubmed after incorporating the 
document download cost 
is the savings in the query cost and the result index download cost 
is only a relatively small portion of the overall cost still we 
observe noticeable savings from the adaptive policy at the total cost 
of for example the coverage of the adaptive policy is roughly 
 while the coverage of the frequency policy is only 
 related work 
in a recent study raghavan and garcia-molina present an 
architectural model for a hidden web crawler the main focus of 
this work is to learn hidden-web query interfaces not to 
generate queries automatically the potential queries are either provided 
manually by users or collected from the query interfaces in 
contrast our main focus is to generate queries automatically without 
any human intervention 
the idea of automatically issuing queries to a database and 
examining the results has been previously used in different contexts 
for example in callan and connel try to acquire an 
accurate language model by collecting a uniform random sample from 
the database in lawrence and giles issue random queries to 
a number of web search engines in order to estimate the fraction 
of the web that has been indexed by each of them in a similar 
fashion bharat and broder issue random queries to a set of 
search engines in order to estimate the relative size and overlap of 
their indexes in barbosa and freire experimentally evaluate 
methods for building multi-keyword queries that can return a large 
fraction of a document collection our work differs from the 
previous studies in two ways first it provides a theoretical framework 
for analyzing the process of generating queries for a database and 
examining the results which can help us better understand the 
effectiveness of the methods presented in the previous work second 
we apply our framework to the problem of hidden web crawling 
and demonstrate the efficiency of our algorithms 
cope et al propose a method to automatically detect whether 
a particular web page contains a search form this work is 
complementary to ours once we detect search interfaces on the web 
using the method in we may use our proposed algorithms to 
download pages automatically from those web sites 
reference reports methods to estimate what fraction of a 
text database can be eventually acquired by issuing queries to the 
database in the authors study query-based techniques that can 
extract relational data from large text databases again these works 
study orthogonal issues and are complementary to our work 
in order to make documents in multiple textual databases 
searchable at a central place a number of harvesting approaches have 
 
been proposed e g oai dp these approaches 
essentially assume cooperative document databases that willingly share 
some of their metadata and or documents to help a third-party search 
engine to index the documents our approach assumes 
uncooperative databases that do not share their data publicly and whose 
documents are accessible only through search interfaces 
there exists a large body of work studying how to identify the 
most relevant database given a user query this 
body of work is often referred to as meta-searching or database 
selection problem over the hidden web for example 
suggests the use of focused probing to classify databases into a topical 
category so that given a query a relevant database can be selected 
based on its topical category our vision is different from this body 
of work in that we intend to download and index the hidden pages 
at a central location in advance so that users can access all the 
information at their convenience from one single location 
 conclusion and future work 
traditional crawlers normally follow links on the web to 
discover and download pages therefore they cannot get to the hidden 
web pages which are only accessible through query interfaces in 
this paper we studied how we can build a hidden web crawler that 
can automatically query a hidden web site and download pages 
from it we proposed three different query generation policies for 
the hidden web a policy that picks queries at random from a list 
of keywords a policy that picks queries based on their frequency 
in a generic text collection and a policy which adaptively picks a 
good query based on the content of the pages downloaded from the 
hidden web site experimental evaluation on real hidden web 
sites shows that our policies have a great potential in particular in 
certain cases the adaptive policy can download more than of 
a hidden web site after issuing approximately queries given 
these results we believe that our work provides a potential 
mechanism to improve the search-engine coverage of the web and the 
user experience of web search 
 future work 
we briefly discuss some future-research avenues 
multi-attribute databases we are currently investigating how 
to extend our ideas to structured multi-attribute databases while 
generating queries for multi-attribute databases is clearly a more 
difficult problem we may exploit the following observation to 
address this problem when a site supports multi-attribute queries 
the site often returns pages that contain values for each of the query 
attributes for example when an online bookstore supports queries 
on title author and isbn the pages returned from a query 
typically contain the title author and isbn of corresponding books 
thus if we can analyze the returned pages and extract the values 
for each field e g title  harry potter author 
 j k rowling etc we can apply the same idea that we 
used for the textual database estimate the frequency of each 
attribute value and pick the most promising one the main challenge 
is to automatically segment the returned pages so that we can 
identify the sections of the pages that present the values corresponding 
to each attribute since many web sites follow limited formatting 
styles in presenting multiple attributes - for example most book 
titles are preceded by the label title - we believe we may learn 
page-segmentation rules automatically from a small set of training 
examples 
other practical issues in addition to the automatic query 
generation problem there are many practical issues to be addressed 
to build a fully automatic hidden-web crawler for example in 
this paper we assumed that the crawler already knows all query 
interfaces for hidden-web sites but how can the crawler discover 
the query interfaces the method proposed in may be a good 
starting point in addition some hidden-web sites return their 
results in batches of say pages so the user has to click on a 
next button in order to see more results in this case a fully 
automatic hidden-web crawler should know that the first result index 
page contains only a partial result and press the next button 
automatically finally some hidden web sites may contain an infinite 
number of hidden web pages which do not contribute much 
significant content e g a calendar with links for every day in this 
case the hidden-web crawler should be able to detect that the site 
does not have much more new content and stop downloading pages 
from the site page similarity detection algorithms may be useful 
for this purpose 
 references 
 lexisnexis http www lexisnexis com 
 the open directory project http www dmoz org 
 e agichtein and l gravano querying text databases for efficient information 
extraction in icde 
 e agichtein p ipeirotis and l gravano modeling query-based access to text 
databases in webdb 
 article on new york times old search engine the library tries to fit into a 
google world available at http 
 www nytimes com technology libr html 
june 
 l barbosa and j freire siphoning hidden-web data through keyword-based 
interfaces in sbbd 
 m k bergman the deep web surfacing hidden value http 
 www press umich edu jep - bergman html 
 k bharat and a broder a technique for measuring the relative size and 
overlap of public web search engines in www 
 a z broder s c glassman m s manasse and g zweig syntactic 
clustering of the web in www 
 j callan m connell and a du automatic discovery of language models for 
text databases in sigmod 
 j p callan and m e connell query-based sampling of text databases 
information systems - 
 k c -c chang b he c li and z zhang structured databases on the web 
observations and implications technical report uiuc 
 j cho n shivakumar and h garcia-molina finding replicated web 
collections in sigmod 
 w cohen and y singer learning to query the web in aaai workshop on 
internet-based information systems 
 j cope n craswell and d hawking automated discovery of search 
interfaces on the web in th australasian conference on database 
technologies 
 t h cormen c e leiserson and r l rivest introduction to algorithms 
 nd edition mit press mcgraw hill 
 d florescu a y levy and a o mendelzon database techniques for the 
world-wide web a survey sigmod record - 
 b he and k c -c chang statistical schema matching across web query 
interfaces in sigmod conference 
 p ipeirotis and l gravano distributed search over the hidden web 
hierarchical database sampling and selection in vldb 
 p g ipeirotis l gravano and m sahami probe count and classify 
categorizing hidden web databases in sigmod 
 c lagoze and h v sompel the open archives initiative building a 
low-barrier interoperability framework in jcdl 
 s lawrence and c l giles searching the world wide web science 
 - 
 v z liu j c richard c luo and and w w chu dpro a probabilistic 
approach for hidden web database selection using dynamic probing in icde 
 
 x liu k maly m zubair and m l nelson dp -an oai gateway service 
for web crawlers in jcdl 
 b b mandelbrot fractal geometry of nature w h freeman co 
 a ntoulas j cho and c olston what s new on the web the evolution of the 
web from a search engine perspective in www 
 a ntoulas p zerfos and j cho downloading hidden web content technical 
report ucla 
 s olsen does search engine s power threaten web s independence 
http news com com - - html 
 s raghavan and h garcia-molina crawling the hidden web in vldb 
 g k zipf human behavior and the principle of least-effort 
addison-wesley cambridge ma 
 
vocabulary independent spoken term detection 
jonathan mamou 
ibm haifa research labs 
haifa israel 
mamou il ibm com 
bhuvana ramabhadran olivier siohan 
ibm t j watson research center 
yorktown heights n y usa 
{bhuvana siohan} us ibm com 
abstract 
we are interested in retrieving information from speech data 
like broadcast news telephone conversations and roundtable 
meetings today most systems use large vocabulary 
continuous speech recognition tools to produce word transcripts 
the transcripts are indexed and query terms are retrieved 
from the index however query terms that are not part 
of the recognizer s vocabulary cannot be retrieved and the 
recall of the search is affected in addition to the output 
word transcript advanced systems provide also phonetic 
transcripts against which query terms can be matched 
phonetically such phonetic transcripts suffer from lower 
accuracy and cannot be an alternative to word transcripts 
we present a vocabulary independent system that can 
handle arbitrary queries exploiting the information provided 
by having both word transcripts and phonetic transcripts 
a speech recognizer generates word confusion networks and 
phonetic lattices the transcripts are indexed for query 
processing and ranking purpose the value of the proposed 
method is demonstrated by the relative high performance of 
our system which received the highest overall ranking for 
us english speech data in the recent nist spoken term 
detection evaluation 
categories and subject descriptors 
h information storage and retrieval information 
search and retrieval 
general terms 
algorithms 
 introduction 
the rapidly increasing amount of spoken data calls for 
solutions to index and search this data 
the classical approach consists of converting the speech to 
word transcripts using a large vocabulary continuous speech 
recognition lvcsr tool in the past decade most of the 
research efforts on spoken data retrieval have focused on 
extending classical ir techniques to word transcripts some of 
these works have been done in the framework of the nist 
trec spoken document retrieval tracks and are described 
by garofolo et al these tracks focused on retrieval 
from a corpus of broadcast news stories spoken by 
professionals one of the conclusions of those tracks was that 
the effectiveness of retrieval mostly depends on the 
accuracy of the transcripts while the accuracy of automatic 
speech recognition asr systems depends on the scenario 
and environment state-of-the-art systems achieved better 
than accuracy in transcription of such data in 
garofolo et al concluded that spoken document retrieval 
is a solved problem 
however a significant drawback of such approaches is that 
search on queries containing out-of-vocabulary oov terms 
will not return any results oov terms are missing words 
from the asr system vocabulary and are replaced in the 
output transcript by alternatives that are probable given 
the recognition acoustic model and the language model it 
has been experimentally observed that over of user 
queries can contain oov terms as queries often 
relate to named entities that typically have a poor coverage 
in the asr vocabulary the effects of oov query terms in 
spoken data retrieval are discussed by woodland et al 
in many applications the oov rate may get worse over time 
unless the recognizer s vocabulary is periodically updated 
another approach consists of converting the speech to 
phonetic transcripts and representing the query as a 
sequence of phones the retrieval is based on searching the 
sequence of phones representing the query in the phonetic 
transcripts the main drawback of this approach is the 
inherent high error rate of the transcripts therefore such 
approach cannot be an alternative to word transcripts 
especially for in-vocabulary iv query terms that are part of 
the vocabulary of the asr system 
a solution would be to combine the two different 
approaches presented above we index both word transcripts 
and phonetic transcripts during query processing the 
information is retrieved from the word index for iv terms and 
from the phonetic index for oov terms we would like to 
be able to process also hybrid queries i e queries that 
include both iv and oov terms consequently we need to 
merge pieces of information retrieved from word index and 
phonetic index proximity information on the occurrences 
of the query terms is required for phrase search and for 
proximity-based ranking in classical ir the index stores for 
each occurrence of a term its offset therefore we cannot 
merge posting lists retrieved by phonetic index with those 
retrieved by word index since the offset of the occurrences 
retrieved from the two different indices are not comparable 
the only element of comparison between phonetic and word 
transcripts are the timestamps no previous work 
combining word and phonetic approach has been done on phrase 
search we present a novel scheme for information retrieval 
that consists of storing during the indexing process for each 
unit of indexing phone or word its timestamp we search 
queries by merging the information retrieved from the two 
different indices word index and phonetic index according 
to the timestamps of the query terms we analyze the 
retrieval effectiveness of this approach on the nist spoken 
term detection evaluation data 
the paper is organized as follows we describe the audio 
processing in section the indexing and retrieval methods 
are presented in section experimental setup and results 
are given in section in section we give an overview of 
related work finally we conclude in section 
 automatic speech recognition 
system 
we use an asr system for transcribing speech data it 
works in speaker-independent mode for best recognition 
results a speaker-independent acoustic model and a 
language model are trained in advance on data with similar 
characteristics 
typically asr generates lattices that can be considered 
as directed acyclic graphs each vertex in a lattice is 
associated with a timestamp and each edge u v is labeled with 
a word or phone hypothesis and its prior probability which 
is the probability of the signal delimited by the timestamps 
of the vertices u and v given the hypothesis the -best 
path transcript is obtained from the lattice using dynamic 
programming techniques 
mangu et al and hakkani-tur et al propose a 
compact representation of a word lattice called word 
confusion network wcn each edge u v is labeled with a word 
hypothesis and its posterior probability i e the probability 
of the word given the signal one of the main advantages 
of wcn is that it also provides an alignment for all of the 
words in the lattice as explained in the three main 
steps for building a wcn from a word lattice are as follows 
 compute the posterior probabilities for all edges in the 
word lattice 
 extract a path from the word lattice which can be 
the -best the longest or any random path and call 
it the pivot path of the alignment 
 traverse the word lattice and align all the transitions 
with the pivot merging the transitions that 
correspond to the same word or label and occur in the 
same time interval by summing their posterior 
probabilities 
the -best path of a wcn is obtained from the path 
containing the best hypotheses as stated in although 
wcns are more compact than word lattices in general the 
 -best path obtained from wcn has a better word accuracy 
than the -best path obtained from the corresponding word 
lattice 
typical structures of a lattice and a wcn are given in 
figure 
figure typical structures of a lattice and a wcn 
 retrieval model 
the main problem with retrieving information from 
spoken data is the low accuracy of the transcription 
particularly on terms of interest such as named entities and 
content words generally the accuracy of a word transcript 
is characterized by its word error rate wer there are 
three kinds of errors that can occur in a transcript 
substitution of a term that is part of the speech by another 
term deletion of a spoken term that is part of the speech 
and insertion of a term that is not part of the speech 
substitutions and deletions reflect the fact that an 
occurrence of a term in the speech signal is not recognized these 
misses reduce the recall of the search substitutions and 
insertions reflect the fact that a term which is not part of the 
speech signal appears in the transcript these misses reduce 
the precision of the search 
search recall can be enhanced by expanding the transcript 
with extra words these words can be taken from the other 
alternatives provided by the wcn these alternatives may 
have been spoken but were not the top choice of the asr 
such an expansion tends to correct the substitutions and 
the deletions and consequently might improve recall but 
will probably reduce precision using an appropriate 
ranking model we can avoid the decrease in precision mamou et 
al have presented in the enhancement in the recall and 
the map by searching on wcn instead of considering only 
the -best path word transcript in the context of spoken 
document retrieval we have adapted this model of iv search to 
term detection in word transcripts oov terms are deleted 
or substituted therefore the usage of phonetic transcripts 
is more desirable however due to their low accuracy we 
have preferred to use only the -best path extracted from the 
phonetic lattices we will show that the usage of phonetic 
transcripts tends to improve the recall without affecting the 
precision too much using an appropriate ranking 
 spoken document detection task 
as stated in the std evaluation plan the task 
consists in finding all the exact matches of a specific query 
in a given corpus of speech data a query is a phrase 
containing several words the queries are text and not speech 
note that this task is different from the more classical task of 
spoken document retrieval manual transcripts of the speech 
are not provided but are used by the evaluators to find true 
occurrences by definition true occurrences of a query are 
found automatically by searching the manual transcripts 
using the following rule the gap between adjacent words in 
a query must be less than seconds in the corresponding 
speech for evaluating the results each system output 
occurrence is judged as correct or not according to whether it 
is close in time to a true occurrence of the query retrieved 
from manual transcripts it is judged as correct if the 
midpoint of the system output occurrence is less than or equal 
to seconds from the time span of a true occurrence of 
the query 
 indexing 
we have used the same indexing process for wcn and 
phonetic transcripts each occurrence of a unit of indexing 
 word or phone u in a transcript d is indexed with the 
following information 
 the begin time t of the occurrence of u 
 the duration d of the occurrence of u 
in addition for wcn indexing we store 
 the confidence level of the occurrence of u at the 
time t that is evaluated by its posterior probability 
pr u t d 
 the rank of the occurrence of u among the other 
hypotheses beginning at the same time t rank u t d 
note that since the task is to find exact matches of the 
phrase queries we have not filtered stopwords and the 
corpus is not stemmed before indexing 
 search 
in the following we present our approach for 
accomplishing the std task using the indices described above the 
terms are extracted from the query the vocabulary of the 
asr system building word transcripts is given terms that 
are part of this vocabulary are iv terms the other terms 
are oov for an iv query term the posting list is extracted 
from the word index for an oov query term the term is 
converted to a sequence of phones using a joint maximum 
entropy n-gram model for example the term prosody 
is converted to the sequence of phones p r aa z ih 
d iy the posting list of each phone is extracted from the 
phonetic index 
the next step consists of merging the different posting 
lists according to the timestamp of the occurrences in order 
to create results matching the query first we check that 
the words and phones appear in the right order according to 
their begin times second we check that the gap in time 
between adjacent words and phones is reasonable 
conforming to the requirements of the std evaluation the distance 
in time between two adjacent query terms must be less than 
 seconds for oov search we check that the distance 
in time between two adjacent phones of a query term is less 
that seconds this value has been determined empirically 
in such a way we can reduce the effect of insertion errors 
since we allow insertions between the adjacent words and 
phones our query processing does not allow substitutions 
and deletions 
example let us consider the phrase query prosody 
research the term prosody is oov and the term research 
is iv the term prosody is converted to the sequence of 
phones p r aa z ih d iy the posting list of each 
phone is extracted from the phonetic index we merge the 
posting lists of the phones such that the sequence of phones 
appears in the right order and the gap in time between the 
pairs of phones p r r aa aa z z ih ih d d iy is 
less than seconds we obtain occurrences of the term 
prosody the posting list of research is extracted from the 
word index and we merge it with the occurrences found for 
prosody such that they appear in the right order and the 
distance in time between prosody and research is less than 
 seconds 
note that our indexing model allows to search for different 
types of queries 
 queries containing only iv terms using the word index 
 queries containing only oov terms using the phonetic 
index 
 keyword queries containing both iv and oov terms 
using the word index for iv terms and the phonetic 
index for oov terms for query processing the 
different sets of matches are unified if the query terms have 
or semantics and intersected if the query terms have 
and semantics 
 phrase queries containing both iv and oov terms for 
query processing the posting lists of the iv terms 
retrieved from the word index are merged with the 
posting lists of the oov terms retrieved from the phonetic 
index the merging is possible since we have stored 
the timestamps for each unit of indexing word and 
phone in both indices 
the std evaluation has focused on the fourth query type 
it is the hardest task since we need to combine posting lists 
retrieved from phonetic and word indices 
 ranking 
since iv terms and oov terms are retrieved from two 
different indices we propose two different functions for scoring 
an occurrence of a term afterward an aggregate score is 
assigned to the query based on the scores of the query terms 
because the task is term detection we do not use a 
document frequency criterion for ranking the occurrences 
let us consider a query q k kn associated with 
a boosting vector b b bj this vector associates 
a boosting factor to each rank of the different hypotheses 
the boosting factors are normalized between and if the 
rank r is larger than j we assume br 
 in vocabulary term ranking 
for iv term ranking we extend the work of mamou et 
al on spoken document retrieval to term detection we 
use the information provided by the word index we define 
the score score k t d of a keyword k occurring at a time t 
in the transcript d by the following formula 
score k t d brank k t d × pr k t d 
note that ≤ score k t d ≤ 
 out of vocabulary term ranking 
for oov term ranking we use the information provided 
by the phonetic index we give a higher rank to occurrences 
of oov terms that contain phones close in time to each 
other we define a scoring function that is related to the 
average gap in time between the different phones let us 
consider a keyword k converted to the sequence of phones 
 pk 
 pk 
l we define the normalized score score k tk 
 d 
of a keyword k pk 
 pk 
l where each pk 
i occurs at time 
tk 
i with a duration of dk 
i in the transcript d by the following 
formula 
score k tk 
 d − 
l 
i × tk 
i − tk 
i− dk 
i− 
l 
note that according to what we have ex-plained in 
section we have ∀ ≤ i ≤ l tk 
i − tk 
i− dk 
i− 
 sec × tk 
i − tk 
i− dk 
i− and consequently 
 score k tk 
 d ≤ the duration of the keyword 
occurrence is tk 
l − tk 
 dk 
l 
example let us consider the sequence p r aa z 
ih d iy and two different occurrences of the sequence 
for each phone we give the begin time and the duration in 
second 
occurrence p r aa 
 z ih d iy 
occurrence p r aa 
 z ih d iy 
according to our formula the score of the first occurrence 
is and the score of the second occurrence is in the 
first occurrence there are probably some insertion or silence 
between the phone p and r and between the phone d and iy 
the silence can be due to the fact that the phones belongs 
to two different words ans therefore it is not an occurrence 
of the term prosody 
 combination 
the score of an occurrence of a query q at time t in the 
document d is determined by the multiplication of the score 
of each keyword ki where each ki occurs at time ti with a 
duration di in the transcript d 
score q t d 
n 
i 
score ki ti d γn 
note that according to what we have ex-plained in 
section we have ∀ ≤ i ≤ n ti − ti− di− sec 
our goal is to estimate for each found occurrence how 
likely the query appears it is different from classical ir 
that aims to rank the results and not to score them since 
the probability to have a false alarm is inversely proportional 
to the length of the phrase query we have boosted the score 
of queries by a γn exponent that is related to the number 
of keywords in the phrase we have determined empirically 
the value of γn n 
the begin time of the query occurrence is determined by 
the begin time t of the first query term and the duration 
of the query occurrence by tn − t dn 
 experiments 
 experimental setup 
our corpus consists of the evaluation set provided by nist 
for the std evaluation it includes three 
different source types in us english three hours of broadcast 
news bnews three hours of conversational telephony 
speech cts and two hours of conference room meetings 
 confmtg as shown in section these different 
collections have different accuracies cts and confmtg are 
spontaneous speech for the experiments we have processed 
the query set provided by nist that includes queries 
each query is a phrase containing between one to five terms 
common and rare terms terms that are in the manual 
transcripts and those that are not testing and determination 
of empirical values have been achieved on another set of 
speech data and queries the development set also provided 
by nist 
we have used the ibm research prototype asr system 
described in for transcribing speech data we have 
produced wcns for the three different source types -best 
phonetic transcripts were generated only for bnews and 
cts since confmtg phonetic transcripts have too low 
accuracy we have adapted juru a full-text search 
library written in java to index the transcripts and to store 
the timestamps of the words and phones search results have 
been retrieved as described in section 
for each found occurrence of the given query our system 
outputs the location of the term in the audio recording 
 begin time and duration the score indicating how likely 
is the occurrence of query as defined in section and a 
hard binary decision as to whether the detection is 
correct we measure precision and recall by comparing the 
results obtained over the automatic transcripts only the 
results having true hard decision to the results obtained over 
the reference manual transcripts our aim is to evaluate the 
ability of the suggested retrieval approach to handle 
transcribed speech data thus the closer the automatic results 
to the manual results is the better the search effectiveness 
over the automatic transcripts will be the results returned 
from the manual transcription for a given query are 
considered relevant and are expected to be retrieved with highest 
scores this approach for measuring search effectiveness 
using manual data as a reference is very common in speech 
retrieval research 
beside the recall and the precision we use the evaluation 
measures defined by nist for the std evaluation 
the actual term-weighted value atwv and the 
maximum term-weighted value mtwv the term-weighted 
value twv is computed by first computing the miss and 
false alarm probabilities for each query separately then 
using these and an arbitrarily chosen prior probability to 
compute query-specific values and finally averaging these 
query-specific values over all queries q to produce an overall 
system value 
twv θ − averageq{pmiss q θ β × pf a q θ } 
where β c 
v 
 pr− 
q − θ is the detection threshold for 
the evaluation the cost value ratio c v has been 
determined to and the prior probability of a query prq to 
 − 
 therefore β 
miss and false alarm probabilities for a given query q are 
functions of θ 
pmiss q θ − 
ncorrect q θ 
ntrue q 
pf a q θ 
nspurious q θ 
nnt q 
corpus wer subr delr insr 
bnews wcn 
cts wcn 
confmtg wcn 
table wer and distribution of the error types over word -best path extracted from wcns for the 
different source types 
where 
 ncorrect q θ is the number of correct detections 
 retrieved by the system of the query q with a score 
greater than or equal to θ 
 nspurious q θ is the number of spurious detections of 
the query q with a score greater than or equal to θ 
 ntrue q is the number of true occurrences of the query 
q in the corpus 
 nnt q is the number of opportunities for incorrect 
detection of the query q in the corpus it is the 
nontarget query trials it has been defined by the 
following formula nnt q tspeech − ntrue q tspeech 
is the total amount of speech in the collection in 
seconds 
atwv is the actual term-weighted value it is the 
detection value attained by the system as a result of the system 
output and the binary decision output for each putative 
occurrence it ranges from −∞ to mtwv is the 
maximum term-weighted value over the range of all possible 
values of θ it ranges from to 
we have also provided the detection error tradeoff det 
curve of miss probability pmiss vs false alarm 
probability pf a 
we have used the stdeval tool to extract the relevant 
results from the manual transcripts and to compute atwv 
mtwv and the det curve 
we have determined empirically the following values for 
the boosting vector defined in section bi 
i 
 
 wer analysis 
we use the word error rate wer in order to characterize 
the accuracy of the transcripts wer is defined as follows 
s d i 
n 
× 
where n is the total number of words in the corpus and 
s i and d are the total number of substitution insertion 
and deletion errors respectively the substitution error rate 
 subr is defined by 
s 
s d i 
× 
deletion error rate delr and insertion error rate insr 
are defined in a similar manner 
table gives the wer and the distribution of the error 
types over -best path transcripts extracted from wcns 
the wer of the -best path phonetic transcripts is 
approximately two times worse than the wer of word transcripts 
that is the reason why we have not retrieved from phonetic 
transcripts on confmtg speech data 
 theta threshold 
we have determined empirically a detection threshold θ 
per source type and the hard decision of the occurrences 
having a score less than θ is set to false false occurrences 
returned by the system are not considered as retrieved and 
therefore are not used for computing atwv precision and 
recall 
the value of the threshold θ per source type is reported in 
table it is correlated to the accuracy of the transcripts 
basically setting a threshold aims to eliminate from the 
retrieved occurrences false alarms without adding misses 
the higher the wer is the higher the θ threshold should 
be 
bnews cts confmtg 
 
table values of the θ threshold per source type 
 processing resource profile 
we report in table the processing resource profile 
concerning the index size note that our index is compressed 
using ir index compression techniques the indexing time 
includes both audio processing generation of word and 
phonetic transcripts and building of the searchable indices 
index size mb hs 
indexing time hp hs 
index memory usage mb 
search speed sec p hs 
search memory usage mb 
table processing resource profile hs hours of 
speech hp processing hours sec p processing 
seconds 
 retrieval measures 
we compare our approach wcn phonetic presented in 
section with another approach -best-wcn phonetic 
the only difference between these two approaches is that 
in -best-wcn phonetic we index only the -best path 
extracted from the wcn instead of indexing all the wcn 
wcn phonetic was our primary system for the evaluation 
and -best-wcn phonetic was one of our contrastive 
systems average precision and recall mtwv and atwv on 
the queries are given in table we provide also the 
det curve for wcn phonetic approach in figure the 
point that maximizes the twv the mtwv is specified on 
each curve note that retrieval performance has been 
evaluated separately for each source type since the accuracy of 
the speech differs per source type as shown in section 
as expected we can see that mtwv and atwv decrease 
in higher wer the retrieval performance is improved when 
measure bnews cts confmtg 
wcn phonetic atwv 
mtwv 
precision 
recall 
 -best-wcn phonetic atwv 
mtwv 
precision 
recall 
table atwv mtwv precision and recall per source type 
figure det curve for wcn phonetic approach 
using wcns relatively to -best path it is due to the fact 
that miss probability is improved by indexing all the 
hypotheses provided by the wcns this observation confirms 
the results shown by mamou et al in the context of 
spoken document retrieval the atwv that we have obtained 
is close to the mtwv we have combined our ranking model 
with appropriate threshold θ to eliminate results with lower 
score therefore the effect of false alarms added by wcns 
is reduced 
wcn phonetic approach was used in the recent nist std 
evaluation and received the highest overall ranking among 
eleven participants for comparison the system that ranked 
at the third place obtained an atwv of for bnews 
 for cts and for confmtg 
 influence of the duration of the query on 
the retrieval performance 
we have analysed the retrieval performance according to 
the average duration of the occurrences in the manual 
transcripts the query set was divided into three different 
quantiles according to the duration we have reported in table 
atwv and mtwv according to the duration we can see 
that we performed better on longer queries one of the 
reasons is the fact that the asr system is more accurate on 
long words hence it was justified to boost the score of the 
results with the exponent γn as explained in section 
according to the length of the query 
quantile - - - 
bnews atwv 
mtwv 
cts atwv 
mtwv 
confmtg atwv 
mtwv 
table atwv mtwv according to the duration 
of the query occurrences per source type 
 oov vs iv query processing 
we have randomly chosen three sets of queries from the 
query sets provided by nist queries containing only iv 
terms queries containing only oov terms and hybrid 
queries containing both iv and oov terms the following 
experiment has been achieved on the bnews collection and 
iv and oov terms has been determined according to the 
vocabulary of bnews asr system 
we would like to compare three different approaches of 
retrieval using only word index using only phonetic index 
combining word and phonetic indices table summarizes 
the retrieval performance according to each approach and 
to each type of queries using a word-based approach for 
dealing with oov and hybrid queries affects drastically the 
performance of the retrieval precision and recall are null 
using a phone-based approach for dealing with iv queries 
affects also the performance of the retrieval relatively to the 
word-based approach 
as expected the approach combining word and phonetic 
indices presented in section leads to the same retrieval 
performance as the word approach for iv queries and to 
the same retrieval performance as the phonetic approach for 
oov queries this approach always outperforms the others 
and it justifies the fact that we need to combine word and 
phonetic search 
 related work 
in the past decade the research efforts on spoken data 
retrieval have focused on extending classical ir techniques 
to spoken documents some of these works have been done 
in the context of the trec spoken document retrieval 
evaluations and are described by garofolo et al an 
lvcsr system is used to transcribe the speech into -best 
path word transcripts the transcripts are indexed as clean 
text for each occurrence its document its word offset and 
additional information are stored in the index a generic ir 
system over the text is used for word spotting and search 
as described by brown et al and james this 
stratindex word phonetic word and phonetic 
precision recall precision recall precision recall 
iv queries 
oov queries 
hybrid queries 
table comparison of word and phonetic approach on iv and oov queries 
egy works well for transcripts like broadcast news collections 
that have a low wer in the range of - and are 
redundant by nature the same piece of information is 
spoken several times in different manners moreover the 
algorithms have been mostly tested over long queries stated in 
plain english and retrieval for such queries is more robust 
against speech recognition errors 
an alternative approach consists of using word lattices in 
order to improve the effectiveness of sdr singhal et al 
 propose to add some terms to the transcript in order 
to alleviate the retrieval failures due to asr errors from 
an ir perspective a classical way to bring new terms is 
document expansion using a similar corpus their approach 
consists in using word lattices in order to determine which 
words returned by a document expansion algorithm should 
be added to the original transcript the necessity to use a 
document expansion algorithm was justified by the fact that 
the word lattices they worked with lack information about 
word probabilities 
chelba and acero in propose a more compact word 
lattice the position specific posterior lattice pspl this 
data structure is similar to wcn and leads to a more 
compact index the offset of the terms in the speech documents 
is also stored in the index however the evaluation 
framework is carried out on lectures that are relatively planned 
in contrast to conversational speech their ranking model 
is based on the term confidence level but does not take into 
consideration the rank of the term among the other 
hypotheses mamou et al propose a model for spoken document 
retrieval using wcns in order to improve the recall and the 
map of the search however in the above works the 
problem of queries containing oov terms is not addressed 
popular approaches to deal with oov queries are based 
on sub-words transcripts where the sub-words are typically 
phones syllables or word fragments sequences of phones 
 the classical approach consists of using 
phonetic transcripts the transcripts are indexed in the same 
manner as words in using classical text retrieval techniques 
during query processing the query is represented as a 
sequence of phones the retrieval is based on searching the 
string of phones representing the query in the phonetic 
transcript to account for the high recognition error rates some 
other systems use richer transcripts like phonetic lattices 
they are attractive as they accommodate high error rate 
conditions as well as allow for oov queries to be used 
 however phonetic lattices contain many 
edges that overlap in time with the same phonetic label and 
are difficult to index moreover beside the improvement in 
the recall of the search the precision is affected since 
phonetic lattices are often inaccurate consequently phonetic 
approaches should be used only for oov search for 
searching queries containing also iv terms this technique affects 
the performance of the retrieval in comparison to the word 
based approach 
saraclar and sproat in show improvement in word 
spotting accuracy for both iv and oov queries using 
phonetic and word lattices where a confidence measure of a 
word or a phone can be derived they propose three 
different retrieval strategies search both the word and the 
phonetic indices and unify the two different sets of results 
search the word index for iv queries search the phonetic 
index for oov queries search the word index and if no result 
is returned search the phonetic index however no strategy 
is proposed to deal with phrase queries containing both iv 
and oov terms amir et al in propose to merge a 
word approach with a phonetic approach in the context of 
video retrieval however the phonetic transcript is obtained 
from a text to phonetic conversion of the -best path of the 
word transcript and is not based on a phonetic decoding of 
the speech data 
an important issue to be considered when looking at the 
state-of-the-art in retrieval of spoken data is the lack of a 
common test set and appropriate query terms this paper 
uses such a task and the std evaluation is a good summary 
of the performance of different approaches on the same test 
conditions 
 conclusions 
this work studies how vocabulary independent spoken 
term detection can be performed efficiently over different 
data sources previously phonetic-based and word-based 
approaches have been used for ir on speech data the 
former suffers from low accuracy and the latter from limited 
vocabulary of the recognition system in this paper we have 
presented a vocabulary independent model of indexing and 
search that combines both the approaches the system can 
deal with all kinds of queries although the phrases that need 
to combine for the retrieval information extracted from two 
different indices a word index and a phonetic index the 
scoring of oov terms is based on the proximity in time 
between the different phones the scoring of iv terms is based 
on information provided by the wcns we have shown an 
improvement in the retrieval performance when using all the 
wcn and not only the -best path and when using phonetic 
index for search of oov query terms this approach always 
outperforms the other approaches using only word index or 
phonetic index 
as a future work we will compare our model for oov 
search on phonetic transcripts with a retrieval model based 
on the edit distance 
 acknowledgements 
jonathan mamou is grateful to david carmel and ron 
hoory for helpful and interesting discussions 
 references 
 nist spoken term detection evaluation 
website http www nist gov speech tests std 
 nist spoken term detection std evaluation 
plan 
http www nist gov speech tests std docs std -evalplan-v pdf 
 c allauzen m mohri and m saraclar general 
indexation of weighted automata - application to 
spoken utterance retrieval in proceedings of the 
hlt-naacl workshop on interdiciplinary 
approaches to speech indexing and retrieval boston 
ma usa 
 a amir m berg and h permuter mutual relevance 
feedback for multimodal query formulation in video 
retrieval in mir proceedings of the th acm 
sigmm international workshop on multimedia 
information retrieval pages - new york ny 
usa acm press 
 a amir a efrat and s srinivasan advances in 
phonetic word spotting in cikm proceedings of 
the tenth international conference on information and 
knowledge management pages - new york 
ny usa acm press 
 m brown j foote g jones k jones and s young 
open-vocabulary speech indexing for voice and video 
mail retrieval in proceedings acm multimedia 
pages - hong-kong november 
 d carmel e amitay m herscovici y s maarek 
y petruschka and a soffer juru at trec 
 experiments with index pruning in proceedings of the 
tenth text retrieval conference trec- national 
institute of standards and technology nist 
 c chelba and a acero indexing uncertainty for 
spoken document search in interspeech pages 
 - lisbon portugal 
 c chelba and a acero position specific posterior 
lattices for indexing speech in proceedings of the rd 
annual conference of the association for 
computational linguistics acl ann arbor mi 
 
 s chen conditional and joint models for 
grapheme-to-phoneme conversion in eurospeech 
geneva switzerland 
 m clements s robertson and m miller phonetic 
searching applied to on-line distance learning modules 
in digital signal processing workshop and the 
 nd signal processing education workshop 
proceedings of ieee th pages - 
 j garofolo g auzanne and e voorhees the trec 
spoken document retrieval track a success story in 
proceedings of the ninth text retrieval conference 
 trec- national institute of standards and 
technology nist 
 d hakkani-tur and g riccardi a general algorithm 
for word graph matrix decomposition in proceedings 
of the ieee internation conference on acoustics 
speech and signal processing icassp pages 
 - hong-kong 
 d james the application of classical information 
retrieval techniques to spoken documents phd thesis 
university of cambridge downing college 
 d a james a system for unrestricted topic retrieval 
from radio news broadcasts in proc icassp 
pages - atlanta ga 
 b logan p moreno j v thong and e whittaker 
an experimental study of an audio indexing system 
for the web in proceedings of icslp 
 j mamou d carmel and r hoory spoken 
document retrieval from call-center conversations in 
sigir proceedings of the th annual 
international acm sigir conference on research and 
development in information retrieval pages - 
new york ny usa acm press 
 l mangu e brill and a stolcke finding consensus 
in speech recognition word error minimization and 
other applications of confusion networks computer 
speech and language - 
 a martin g doddington t kamm m ordowski 
and m przybocki the det curve in assessment of 
detection task performance in proc eurospeech 
pages - rhodes greece 
 k ng and v w zue subword-based approaches for 
spoken document retrieval speech commun 
 - 
 y peng and f seide fast two-stage 
vocabulary-independent search in spontaneous speech 
in acoustics speech and signal processing 
proceedings icassp ieee international 
conference volume pages - 
 m saraclar and r sproat lattice-based search for 
spoken utterance retrieval in hlt-naacl 
main proceedings pages - boston 
massachusetts usa 
 f seide p yu c ma and e chang 
vocabulary-independent search in spontaneous speech 
in icassp- ieee international conference on 
acoustics speech and signal processing 
 a singhal j choi d hindle d lewis and 
f pereira at t at trec- in proceedings of the 
seventh text retrieval conference trec- 
national institute of standards and technology 
nist 
 a singhal and f pereira document expansion for 
speech retrieval in sigir proceedings of the 
 nd annual international acm sigir conference on 
research and development in information retrieval 
pages - new york ny usa acm press 
 h soltau b kingsbury l mangu d povey 
g saon and g zweig the ibm conversational 
telephony system for rich transcription in proceedings 
of the ieee international conference on acoustics 
speech and signal processing icassp march 
 k thambiratnam and s sridharan dynamic match 
phone-lattice searches for very fast and accurate 
unrestricted vocabulary keyword spotting in 
acoustics speech and signal processing proceedings 
 icassp ieee international conference 
 p c woodland s e johnson p jourlin and k s 
jones effects of out of vocabulary words in spoken 
document retrieval poster session in sigir 
proceedings of the rd annual international acm 
sigir conference on research and development in 
information retrieval pages - new york ny 
usa acm press 
broad expertise retrieval in sparse data environments 
krisztian balog 
isla university of amsterdam 
kruislaan sj 
amsterdam the netherlands 
kbalog science uva nl 
toine bogers 
ilk tilburg university 
p o box le 
tilburg the netherlands 
a m bogers uvt nl 
leif azzopardi 
dept of computing science 
university of glasgow 
glasgow g qq 
leif dcs gla ac uk 
maarten de rijke 
isla university of amsterdam 
kruislaan sj 
amsterdam the netherlands 
mdr science uva nl 
antal van den bosch 
ilk tilburg university 
p o box le 
tilburg the netherlands 
antal vdnbosch uvt nl 
abstract 
expertise retrieval has been largely unexplored on data other than 
the w c collection at the same time many intranets of 
universities and other knowledge-intensive organisations offer examples 
of relatively small but clean multilingual expertise data covering 
broad ranges of expertise areas we first present two main 
expertise retrieval tasks along with a set of baseline approaches based on 
generative language modeling aimed at finding expertise relations 
between topics and people for our experimental evaluation we 
introduce and release a new test set based on a crawl of a 
university site using this test set we conduct two series of experiments 
the first is aimed at determining the effectiveness of baseline 
expertise retrieval methods applied to the new test set the second 
is aimed at assessing refined models that exploit characteristic 
features of the new test set such as the organizational structure of the 
university and the hierarchical structure of the topics in the test set 
expertise retrieval models are shown to be robust with respect to 
environments smaller than the w c collection and current 
techniques appear to be generalizable to other settings 
categories and subject descriptors 
h information storage and retrieval h content 
analysis and indexing h information search and retrieval h 
systems and software h information systems applications 
h types of systems h m miscellaneous 
general terms 
algorithms measurement performance experimentation 
 introduction 
an organization s intranet provides a means for exchanging 
information between employees and for facilitating employee 
collaborations to efficiently and effectively achieve this it is necessary 
to provide search facilities that enable employees not only to access 
documents but also to identify expert colleagues 
at the trec enterprise track the need to study and 
understand expertise retrieval has been recognized through the 
introduction of expert finding tasks the goal of expert finding is to 
identify a list of people who are knowledgeable about a given topic 
this task is usually addressed by uncovering associations between 
people and topics commonly a co-occurrence of the name 
of a person with topics in the same context is assumed to be 
evidence of expertise an alternative task which using the same idea 
of people-topic associations is expert profiling where the task is to 
return a list of topics that a person is knowledgeable about 
the launch of the expert finding task at trec has generated a 
lot of interest in expertise retrieval with rapid progress being made 
in terms of modeling algorithms and evaluation aspects however 
nearly all of the expert finding or profiling work performed has 
been validated experimentally using the w c collection from 
the enterprise track while this collection is currently the only 
publicly available test collection for expertise retrieval tasks it only 
represents one type of intranet with only one test collection it is 
not possible to generalize conclusions to other realistic settings 
in this paper we focus on expertise retrieval in a realistic setting 
that differs from the w c setting-one in which relatively small 
amounts of clean multilingual data are available that cover a broad 
range of expertise areas as can be found on the intranets of 
universities and other knowledge-intensive organizations typically this 
setting features several additional types of structure topical 
structure e g topic hierarchies as employed by the organization 
organizational structure faculty department as well as multiple 
types of documents research and course descriptions publications 
and academic homepages this setting is quite different from the 
w c setting in ways that might impact upon the performance of 
expertise retrieval tasks 
we focus on a number of research questions in this paper does 
the relatively small amount of data available on an intranet affect 
the quality of the topic-person associations that lie at the heart of 
expertise retrieval algorithms how do state-of-the-art algorithms 
developed on the w c data set perform in the alternative scenario 
of the type described above more generally do the lessons from 
the expert finding task at trec carry over to this setting how 
does the inclusion or exclusion of different documents affect 
expertise retrieval tasks in addition to how can the topical and 
organizational structure be used for retrieval purposes 
to answer our research questions we first present a set of 
baseline approaches based on generative language modeling aimed at 
finding associations between topics and people this allows us to 
formulate the expert finding and expert profiling tasks in a uniform 
way and has the added benefit of allowing us to understand the 
relations between the two tasks for our experimental evaluation we 
introduce a new data set the uvt expert collection which is 
representative of the type of intranet that we described above our 
collection is based on publicly available data crawled from the 
website of tilburg university uvt this type of data is particularly 
interesting since it is clean heterogeneous structured and 
focused but comprises a limited number of documents contains 
information on the organizational hierarchy it is bilingual 
 english and dutch and the list of expertise areas of an individual 
are provided by the employees themselves using the uvt expert 
collection we conduct two sets of experiments the first is aimed 
at determining the effectiveness of baseline expertise finding and 
profiling methods in this new setting a second group of 
experiments is aimed at extensions of the baseline methods that exploit 
characteristic features of the uvt expert collection specifically 
we propose and evaluate refined expert finding and profiling 
methods that incorporate topicality and organizational structure 
apart from the research questions and data set that we contribute 
our main contributions are as follows the baseline models 
developed for expertise finding perform well on the new data set while 
on the w c setting the expert finding task appears to be more 
difficult than profiling for the uvt data the opposite is the case we 
find that profiling on the uvt data set is considerably more 
difficult than on the w c set which we believe is due to the large 
 but realistic number of topical areas that we used for profiling 
about for the uvt set versus in the w c case 
taking the similarity between topics into account can significantly 
improve retrieval performance the best performing similarity 
measures are content-based therefore they can be applied on the w c 
 and other settings as well finally we demonstrate that the 
organizational structure can be exploited in the form of a context model 
improving map scores for certain models by up to 
the remainder of this paper is organized as follows in the next 
section we review related work then in section we provide 
detailed descriptions of the expertise retrieval tasks that we address 
in this paper expert finding and expert profiling in section we 
present our baseline models of which the performance is then 
assessed in section using the uvt data set that we introduce in 
section advanced models exploiting specific features of our data are 
presented in section and evaluated in section we formulate our 
conclusions in section 
 related work 
initial approaches to expertise finding often employed databases 
containing information on the skills and knowledge of each 
individual in the organization most of these tools usually called 
yellow pages or people-finding systems rely on people to self-assess 
their skills against a predefined set of keywords for updating 
profiles in these systems in an automatic fashion there is a need for 
intelligent technologies more recent approaches use specific 
document sets such as email or software to find expertise 
in contrast with focusing on particular document types there is also 
an increased interest in the development of systems that index and 
mine published intranet documents as sources of evidence for 
expertise one such published approach is the p noptic system 
which builds a representation of each person by concatenating all 
documents associated with that person-this is similar to model 
of balog et al who formalize and compare two methods balog 
et al s model directly models the knowledge of an expert from 
associated documents while their model first locates documents 
on the topic and then finds the associated experts in the reported 
experiments the second method performs significantly better when 
there are sufficiently many associated documents per candidate 
most systems that took part in the and editions of the 
expert finding task at trec implemented variations on one of 
these two models see macdonald and ounis propose 
a different approach for ranking candidate expertise with respect to 
a topic based on data fusion techniques without using 
collectionspecific heuristics they find that applying field-based weighting 
models improves the ranking of candidates petkova and croft 
propose yet another approach based on a combination of the above 
model and explicitly modeling topics 
turning to other expert retrieval tasks that can also be addressed 
using topic-people associations balog and de rijke addressed 
the task of determining topical expert profiles while their methods 
proved to be efficient on the w c corpus they require an amount 
of data that may not be available in the typical knowledge-intensive 
organization balog and de rijke study the related task of 
finding experts that are similar to a small set of experts given as input 
as an aside creating a textual summary of a person shows 
some similarities to biography finding which has received a 
considerable amount of attention recently see e g 
we use generative language modeling to find associations 
between topics and people in our modeling of expert finding and 
profiling we collect evidence for expertise from multiple sources in 
a heterogeneous collection and integrate it with the co-occurrence 
of candidates names and query terms-the language modeling 
setting allows us to do this in a transparent manner our modeling 
proceeds in two steps in the first step we consider three baseline 
models two taken from the models and mentioned above 
and one a refined version of a model introduced in which we 
refer to as model below this third model is also similar to the 
model described by petkova and croft the models we 
consider in our second round of experiments are mixture models 
similar to contextual language models and to the expanded 
documents of tao et al however the features that we use for 
definining our expansions-including topical structure and 
organizational structure-have not been used in this way before 
 tasks 
in the expertise retrieval scenario that we envisage users seeking 
expertise within an organization have access to an interface that 
combines a search box where they can search for experts or topics 
with navigational structures of experts and of topics that allows 
them to click their way to an expert page providing the profile of a 
person or a topic page providing a list of experts on the topic 
to feed the above interface we face two expertise retrieval 
tasks expert finding and expert profiling that we first define and 
then formalize using generative language models in order to model 
either task the probability of the query topic being associated to a 
candidate expert plays a key role in the final estimates for searching 
and profiling by using language models both the candidates and 
the query are characterized by distributions of terms in the 
vocabulary used in the documents made available by the organization 
whose expertise retrieval needs we are addressing 
 expert finding 
expert finding involves the task of finding the right person with 
the appropriate skills and knowledge who are the experts on topic 
x e g an employee wants to ascertain who worked on a 
particular project to find out why particular decisions were made without 
having to trawl through documentation if there is any or they 
may be in need a trained specialist for consultancy on a specific 
problem 
within an organization there are usually many possible 
candidates who could be experts for given topic we can state this 
problem as follows 
what is the probability of a candidate ca being an 
expert given the query topic q 
that is we determine p ca q and rank candidates ca according to 
this probability the candidates with the highest probability given 
the query are deemed the most likely experts for that topic the 
challenge is how to estimate this probability accurately since the 
query is likely to consist of only a few terms to describe the 
expertise required we should be able to obtain a more accurate estimate 
by invoking bayes theorem and estimating 
p ca q 
p q ca p ca 
p q 
 
where p ca is the probability of a candidate and p q is the 
probability of a query since p q is a constant it can be ignored for 
ranking purposes thus the probability of a candidate ca being an 
expert given the query q is proportional to the probability of a query 
given the candidate p q ca weighted by the a priori belief p ca 
that candidate ca is an expert 
p ca q ∝ p q ca p ca 
in this paper our main focus is on estimating the probability of 
a query given the candidate p q ca because this probability 
captures the extent to which the candidate knows about the query topic 
whereas the candidate priors are generally assumed to be 
uniformand thus will not influence the ranking-it has been demonstrated 
that a sensible choice of priors may improve the performance 
 expert profiling 
while the task of expert searching was concerned with 
finding experts given a particular topic the task of expert profiling 
seeks to answer a related question what topics does a candidate 
know about essentially this turns the questions of expert finding 
around the profiling of an individual candidate involves the 
identification of areas of skills and knowledge that they have expertise 
about and an evaluation of the level of proficiency in each of these 
areas this is the candidate s topical profile 
generally topical profiles within organizations consist of 
tabular structures which explicitly catalogue the skills and knowledge 
of each individual in the organization however such practice is 
limited by the resources available for defining creating 
maintaining and updating these profiles over time by focusing on 
automatic methods which draw upon the available evidence within the 
document repositories of an organization our aim is to reduce the 
human effort associated with the maintenance of topical profiles 
 
a topical profile of a candidate then is defined as a vector where 
each element i of the vector corresponds to the candidate ca s 
expertise on a given topic ki i e s ca ki each topic ki defines a 
particular knowledge area or skill that the organization uses to 
define the candidate s topical profile thus it is assumed that a list of 
topics {k kn} where n is the number of pre-defined topics 
is given 
profile ca s ca k s ca k s ca kn 
 
context and evidence are needed to help users of expertise 
finding systems to decide whom to contact when seeking expertise in a 
particular area examples of such context are who does she work 
with what are her contact details is she well-connected just 
in case she is not able to help us herself what is her role in the 
organization who is her superior collaborators and affiliations 
etc are all part of the candidate s social profile and can serve as 
a background against which the system s recommendations should 
be interpreted in this paper we only address the problem of 
determining topical profiles and leave social profiling to further work 
we state the problem of quantifying the competence of a person on 
a certain knowledge area as follows 
what is the probability of a knowledge area ki being 
part of the candidate s expertise profile 
where s ca ki is defined by p ki ca our task then is to 
estimate p ki ca which is equivalent to the problem of obtaining 
p q ca where the topic ki is represented as a query topic q i e a 
sequence of keywords representing the expertise required 
both the expert finding and profiling tasks rely on the accurate 
estimation of p q ca the only difference derives from the prior 
probability that a person is an expert p ca which can be 
incorporated into the expert finding task this prior does not apply to 
the profiling task since the candidate individual is fixed 
 baseline models 
in this section we describe our baseline models for estimating 
p q ca i e associations between topics and people both expert 
finding and expert profiling boil down to this estimation we 
employ three models for calculating this probability 
 from topics to candidates 
using candidate models model model defines the 
probability of a query given a candidate p q ca using standard 
language modeling techniques based on a multinomial unigram 
language model for each candidate ca a candidate language model 
θca is inferred such that the probability of a term given θca is 
nonzero for all terms i e p t θca from the candidate model the 
query is generated with the following probability 
p q θca 
y 
t∈q 
p t θca n t q 
 
where each term t in the query q is sampled identically and 
independently and n t q is the number of times t occurs in q the 
candidate language model is inferred as follows an empirical 
model p t ca is computed it is smoothed with background 
probabilities using the associations between a candidate and a 
document the probability p t ca can be approximated by 
p t ca 
x 
d 
p t d p d ca 
where p d ca is the probability that candidate ca generates a 
supporting document d and p t d is the probability of a term t 
occurring in the document d we use the maximum-likelihood estimate 
of a term that is the normalised frequency of the term t in 
document d the strength of the association between document d and 
candidate ca expressed by p d ca reflects the degree to which the 
candidates expertise is described using this document the 
estimation of this probability is presented later in section 
the candidate model is then constructed as a linear interpolation 
of p t ca and the background model p t to ensure there are no 
zero probabilities which results in the final estimation 
p q θca 
y 
t∈q 
 
 − λ 
x 
d 
p t d p d ca 
 
 λp t 
 n t q 
 
model amasses all the term information from all the documents 
associated with the candidate and uses this to represent that 
candidate this model is used to predict how likely a candidate would 
produce a query q this can can be intuitively interpreted as the 
probability of this candidate talking about the query topic where 
we assume that this is indicative of their expertise 
using document models model model takes a 
different approach here the process is broken into two parts given 
a candidate ca a document that is associated with a candidate 
is selected with probability p d ca and from this document a 
query q is generated with probability p q d then the sum over all 
documents is taken to obtain p q ca such that 
p q ca 
x 
d 
p q d p d ca 
the probability of a query given a document is estimated by 
inferring a document language model θd for each document d in a 
similar manner as the candidate model was inferred 
p t θd − λ p t d λp t 
where p t d is the probability of the term in the document the 
probability of a query given the document model is 
p q θd 
y 
t∈q 
p t θd n t q 
 
the final estimate of p q ca is obtained by substituting p q d for 
p q θd into eq see for full details conceptually model 
differs from model because the candidate is not directly modeled 
instead the document acts like a hidden variable in the process 
which separates the query from the candidate this process is akin 
to how a user may search for candidates with a standard search 
engine initially by finding the documents which are relevant and 
then seeing who is associated with that document by examining a 
number of documents the user can obtain an idea of which 
candidates are more likely to discuss the topic q 
using topic models model we introduce a third model model 
instead of attempting to model the query generation process via 
candidate or document models we represent the query as a topic 
language model and directly estimate the probability of the 
candidate p ca q this approach is similar to the model presented 
in as with the previous models a language model is 
inferred but this time for the query we adapt the work of lavrenko 
and croft to estimate a topic model from the query 
the procedure is as follows given a collection of documents 
and a query topic q it is assumed that there exists an unknown 
topic model θk that assigns probabilities p t θk to the term 
occurrences in the topic documents both the query and the documents 
are samples from θk as opposed to the previous approaches where 
a query is assumed to be sampled from a specific document or 
candidate model the main task is to estimate p t θk the probability 
of a term given the topic model since the query q is very sparse 
and as there are no examples of documents on the topic this 
distribution needs to be approximated lavrenko and croft suggest 
a reasonable way of obtaining such an approximation by assuming 
that p t θk can be approximated by the probability of term t given 
the query q we can then estimate p t q using the joint probability 
of observing the term t together with the query terms q qm 
and dividing by the joint probability of the query terms 
p t θk ≈ p t q 
p t q qm 
p q qm 
 
p t q qm 
p 
t ∈t p t q qm 
 
where p q qm 
p 
t ∈t p t q qm and t is the 
entire vocabulary of terms in order to estimate the joint probability 
p t q qm we follow and assume t and q qm 
are mutually independent once we pick a source distribution from 
the set of underlying source distributions u if we choose u to be 
a set of document models then to construct this set the query q 
would be issued against the collection and the top n returned are 
assumed to be relevant to the topic and thus treated as samples 
from the topic model note that candidate models could be used 
instead with the document models forming u the joint 
probability of term and query becomes 
p t q qm 
x 
d∈u 
p d 
˘ 
p t θd 
my 
i 
p qi θd 
¯ 
 
here p d denotes the prior distribution over the set u which 
reflects the relevance of the document to the topic we assume that 
p d is uniform across u in order to rank candidates according 
to the topic model defined we use the kullback-leibler divergence 
metric kl to measure the difference between the candidate 
models and the topic model 
kl θk θca 
x 
t 
p t θk log 
p t θk 
p t θca 
 
candidates with a smaller divergence from the topic model are 
considered to be more likely experts on that topic the candidate model 
θca is defined in eq by using kl divergence instead of the 
probability of a candidate given the topic model p ca θk we avoid 
normalization problems 
 document-candidate associations 
for our models we need to be able to estimate the probability 
p d ca which expresses the extent to which a document d 
characterizes the candidate ca in two methods are presented for 
estimating this probability based on the number of person names 
recognized in a document however in our intranet setting it is 
reasonable to assume that authors of documents can unambiguously be 
identified e g as the author of an article the teacher assigned to a 
course the owner of a web page etc hence we set p d ca to be 
 if candidate ca is author of document d otherwise the probability 
is in section we describe how authorship can be determined 
on different types of documents within the collection 
 the uvt expert collection 
the uvt expert collection used in the experiments in this paper 
fits the scenario outlined in section the collection is based on 
the webwijs webwise system developed at tilburg university 
 uvt in the netherlands webwijs http www uvt nl 
webwijs is a publicly accessible database of uvt employees 
who are involved in research or teaching currently webwijs 
contains information about experts each of whom has a page with 
contact information and if made available by the expert a research 
description and publications list in addition each expert can 
select expertise areas from a list of topics and is encouraged to 
suggest new topics that need to be approved by the webwijs editor 
each topic has a separate page that shows all experts associated 
with that topic and if available a list of related topics 
webwijs is available in dutch and english and this bilinguality 
has been preserved in the collection every dutch webwijs page 
has an english translation not all dutch topics have an english 
translation but the reverse is true the english topics all have a 
dutch equivalent 
about of the experts teach courses at tilburg university 
these courses were also crawled and included in the profile in 
addition about of the experts link to their academic homepage 
from their webwijs page these home pages were crawled and 
added to the collection this means that if experts put the full-text 
versions of their publications on their academic homepage these 
were also available for indexing we also obtained full-text 
versions of publications from the uvt institutional repository and 
dutch english 
no of experts 
no of experts with ≥ topic 
no of topics 
no of expert-topic pairs 
avg no of topics expert 
max no of topics expert no of experts 
min no of topics expert no of experts 
avg no of experts topic 
max no of experts topic no of topics 
min no of experts topic no of topics 
no of experts with hp 
no of experts with cd 
avg no of cds per teaching expert 
no of experts with rd 
no of experts with pub 
avg no of pubs per expert 
avg no of pub citations per expert 
avg no of full-text pubs per expert 
table descriptive statistics of the dutch and english versions 
of the uvt expert collection 
converted them to plain text we ran the textcat language 
identifier to classify the language of the home pages and the 
fulltext publications we restricted ourselves to pages where the 
classifier was confident about the language used on the page 
this resulted in four document types research descriptions rd 
course descriptions cd publications pub full-text and 
citationonly versions and academic homepages hp everything was 
bundled into the uvt expert collection which is available at http 
 ilk uvt nl uvt-expert-collection 
the uvt expert collection was extracted from a different 
organizational setting than the w c collection and differs from it in 
a number of ways the uvt setting is one with relatively small 
amounts of multilingual data document-author associations are 
clear and the data is structured and clean the collection covers a 
broad range of expertise areas as one can typically find on intranets 
of universities and other knowledge-intensive institutes 
additionally our university setting features several types of structure 
 topical and organizational as well as multiple document types 
another important difference between the two data sets is that the 
expertise areas in the uvt expert collection are self-selected instead 
of being based on group membership or assignments by others 
size is another dimension along which the w c and uvt expert 
collections differ the latter is the smaller of the two also realistic 
are the large differences in the amount of information available for 
each expert utilizing webwijs is voluntary dutch experts 
did not select any topics at all this leaves us with dutch and 
 english usable expert profiles table provides descriptive 
statistics for the uvt expert collection 
universities tend to have a hierarchical structure that goes from 
the faculty level to departments research groups down to the 
individual researchers in the uvt expert collection we have 
information about the affiliations of researchers with faculties and 
institutes providing us with a two-level organizational hierarchy 
tilburg university has organizational units at the faculty level 
 including the university office and several research institutes and 
 departments which amounts to departments per faculty 
as to the topical hierarchy used by webwijs of the 
topics are top nodes in the hierarchy this hierarchy has an average 
topic chain length of and a maximum length of topics 
 evaluation 
below we evaluate section s models for expert finding and 
profiling onthe uvt expert collection we detail our research 
questions and experimental setup and then present our results 
 research questions 
we address the following research questions both expert finding 
and profiling rely on the estimations of p q ca the question is 
how the models compare on the different tasks and in the setting of 
the uvt expert collection in model outperformed model 
on the w c collection how do they compare on our data set and 
how does model compare to model what about performance 
differences between the two languages in our test collection 
 experimental setup 
the output of our models was evaluated against the self-assigned 
topic labels which were treated as relevance judgements results 
were evaluated separately for english and dutch for english we 
only used topics for which the dutch translation was available for 
dutch all topics were considered the results were averaged for 
the queries in the intersection of relevance judgements and results 
missing queries do not contribute a value of to the scores 
we use standard information retrieval measures such as mean 
average precision map and mean reciprocal rank mrr we 
also report the percentage of topics q and candidates ca 
covered for the expert finding and profiling tasks respectively 
 results 
table shows the performance of model and on the 
expert finding and profiling tasks the rows of the table correspond 
to the various document types rd cd pub and hp and to their 
combinations rd cd pub hp is equivalent to the full 
collection and will be referred as the baseline of our experiments 
looking at table we see that model performs the best across 
the board however when the data is clean and very focused rd 
model outperforms it in a number of cases model has the 
best coverage of candidates ca and topics q the various 
document types differ in their characteristics and how they improve 
the finding and profiling tasks expert profiling benefits much from 
the clean data present in the rd and cd document types while the 
publications contribute the most to the expert finding task adding 
the homepages does not prove to be particularly useful 
when we compare the results across languages we find that the 
coverage of english topics q is higher than of the dutch ones 
for expert finding apart from that the scores fall in the same range 
for both languages for the profiling task the coverage of the 
candidates ca is very similar for both languages however the 
performance is substantially better for the english topics 
while it is hard to compare scores across collections we 
conclude with a brief comparison of the absolute scores in table to 
those reported in on the w c test set edition for 
expert finding the map scores for model reported here are about 
 higher than the corresponding figures in while our mrr 
scores are slightly below those in for expert profiling the 
differences are far more dramatic the map scores for model 
reported here are around below the scores in while the best 
mrr scores are about the same as those in the cause for the 
latter differences seems to reside in the number of knowledge areas 
considered here-approx times more than in the w c setting 
 advanced models 
now that we have developed and assessed basic language 
modeling techniques for expertise retrieval we turn to refined models 
that exploit special features of our test collection 
 exploiting knowledge area similarity 
one way to improve the scoring of a query given a candidate is 
to consider what other requests the candidate would satisfy and use 
them as further evidence to support the original query proportional 
expert finding expert profiling 
document types model model model model model model 
 q map mrr q map mrr q map mrr ca map mrr ca map mrr ca map mrr 
english 
rd 
cd 
pub 
hp 
rd cd 
rd cd pub 
rd cd pub hp 
dutch 
rd 
cd 
pub 
hp 
rd cd 
rd cd pub 
rd cd pub hp 
table performance of the models on the expert finding and profiling tasks using different document types and their combinations 
 q is the number of topics covered applies to the expert finding task ca is the number of candidates covered applies to the 
expert profiling task the top and bottom blocks correspond to english and dutch respectively the best scores are in boldface 
to how related the other requests are to the original query this can 
be modeled by interpolating between the p q ca and the further 
supporting evidence from all similar requests q as follows 
p q ca λp q ca − λ 
x 
q 
p q q p q ca 
where p q q represents the similarity between the two topics q 
and q to be able to work with similarity methods that are not 
necessarily probabilities we set p q q w q q 
γ 
 where γ is 
a normalizing constant such that γ 
p 
q w q q we 
consider four methods for calculating the similarity score between two 
topics three approaches are strictly content-based and establish 
similarity by examining co-occurrence patterns of topics within the 
collection while the last approach exploits the hierarchical 
structure of topical areas that may be present within an organization see 
 for further examples of integrating word relationships into 
language models 
the kullback-leibler kl divergence metric defined in eq 
provides a measure of how different or similar two probability 
distributions are a topic model is inferred for q and q using the 
method presented in section to describe the query across the 
entire vocabulary since a lower kl score means the queries are 
more similar we let w q q max kl θq · − kl θq θq 
pointwise mutual information pmi is a measure of 
association used in information theory to determine the extent of 
independence between variables the dependence between two queries 
is reflected by the si q q score where scores greater than zero 
indicate that it is likely that there is a dependence which we take 
to mean that the queries are likely to be similar 
si q q log 
p q q 
p q p q 
 
we estimate the probability of a topic p q using the number of 
documents relevant to query q within the collection the joint 
probability p q q is estimated similarly by using the 
concatenation of q and q as a query to obtain p q q we then set 
w q q si q q when si q q otherwise w q q 
because we are only interested in including queries that are similar 
the log-likelihood statistic provides another measure of 
dependence which is more reliable than the pointwise mutual 
information measure let k be the number of co-occurrences of q 
and q k the number of occurrences of q not co-occurring with q 
n the total number of occurrences of q and n the total number 
of topic tokens minus the number of occurrences of q then let 
p k n p k n and p k k n n 
 q q p k n p k n 
− p k n − p k n 
where p n k k log p n − k log − p the higher 
score indicate that queries are also likely to be similar thus we set 
w q q q q 
finally we also estimate the similarity of two topics based on 
their distance within the topic hierarchy the topic hierarchy is 
viewed as a directed graph and for all topic-pairs the shortest path 
sp q q is calculated we set the similarity score to be the 
reciprocal of the shortest path w q q sp q q 
 contextual information 
given the hierarchy of an organization the units to which a 
person belong are regarded as a context so as to compensate for data 
sparseness we model it as follows 
p q ca 
 
 − 
p 
ou∈ou ca λou 
 
· p q ca 
 
p 
ou∈ou ca λou · p q ou 
where ou ca is the set of organizational units of which 
candidate ca is a member of and p q o expresses the strength of the 
association between query q and the unit ou the latter probability 
can be estimated using either of the three basic models by simply 
replacing ca with ou in the corresponding equations an 
organizational unit is associated with all the documents that its members 
have authored that is p d ou maxca∈ou p d ca 
 a simple multilingual model 
for knowledge institutes in europe academic or otherwise a 
multilingual or at least bilingual setting is typical the following 
model builds on a kind of independence assumption there is no 
spill-over of expertise profiles across language boundaries while a 
simplification this is a sensible first approach that is p q ca p 
l∈l λl · p ql ca where l is the set of languages used in the 
collection ql is the translation of the query q to language l and λl is 
a language specific smoothing parameter such that 
p 
l∈l λl 
 advanced models evaluation 
in this section we present an experimental evaluation of our 
advanced models 
expert finding expert profiling 
language model model model model model model 
 q map mrr q map mrr q map mrr ca map mrr ca map mrr ca map mrr 
english only 
dutch only 
combination 
table performance of the combination of languages on the expert finding and profiling tasks on candidates best scores for each 
model are in italic absolute best scores for the expert finding and profiling tasks are in boldface 
method model model model 
map mrr map mrr map mrr 
english 
baseline 
kldiv 
pmi 
ll 
hdist 
dutch 
baseline 
kldiv 
pmi 
ll 
hdist 
method model model model 
map mrr map mrr map mrr 
english 
baseline 
kldiv 
pmi 
ll 
hdist 
dutch 
baseline 
kldiv 
pmi 
ll 
hdist 
table performance on the expert finding top and profiling 
 bottom tasks using knowledge area similarities runs were 
evaluated on the main topics set best scores are in boldface 
 research questions 
our questions follow the refinements presented in the preceding 
section does exploiting the knowledge area similarity improve 
effectiveness which of the various methods for capturing word 
relationships is most effective furthermore is our way of bringing 
in contextual information useful for which tasks and finally is 
our simple way of combining the monolingual scores sufficient for 
obtaining significant improvements 
 experimental setup 
given that the self-assessments are also sparse in our collection 
in order to be able to measure differences between the various 
models we selected a subset of topics and evaluated some of the runs 
only on this subset this set is referred as main topics and consists 
of topics that are located at the top level of the topical hierarchy a 
main topic has subtopics but is not a subtopic of any other topic 
this main set consists of dutch and english topics the 
relevance judgements were restricted to the main topic set but were 
not expanded with subtopics 
 exploiting knowledge area similarity 
table presents the results the four methods used for 
estimating knowledge-area similarity are kl divergence kldiv 
pointlang topics model model model 
map mrr map mrr map mrr 
expert finding 
uk all 
uk main 
nl all 
nl main 
expert profiling 
uk all 
uk main 
nl all 
nl main 
table evaluating the context models on organizational units 
wise mutual information pmi log-likelihood ll and distance 
within topic hierarchy hdist we managed to improve upon the 
baseline in all cases but the improvement is more noticeable for 
the profiling task for both tasks the ll method performed best 
the content-based approaches performed consistently better than 
hdist 
 contextual information 
a two level hierarchy of organizational units faculties and 
institutes is available in the uvt expert collection the unit a person 
belongs to is used as a context for that person first we evaluated 
the models of the organizational units using all topics all and 
only the main topics main an organizational unit is considered 
to be relevant for a given topic or vice versa if at least one member 
of the unit selected the given topic as an expertise area 
table reports on the results as far as expert finding goes given 
a topic the corresponding organizational unit can be identified with 
high precision however the expert profiling task shows a different 
picture the scores are low and the task seems hard the 
explanation may be that general concepts i e our main topics may belong 
to several organizational units 
second we performed another evaluation where we combined 
the contextual models with the candidate models to score 
candidates again table reports on the results we find a positive 
impact of the context models only for expert finding noticably 
for expert finding and model it improves over for 
english and over for dutch on map the poor performance 
on expert profiling may be due to the fact that context models alone 
did not perform very well on the profiling task to begin with 
 multilingual models 
in this subsection we evaluate the method for combining 
results across multiple languages that we described in section 
in our setting the set of languages consists of english and dutch 
l {uk nl} the weights on these languages were set to be 
identical λuk λnl we performed experiments with 
various λ settings but did not observe significant differences in 
performance 
table reports on the multilingual results where performance is 
evaluated on the full topic set all three models significantly 
imlang method model model model 
map mrr map mrr map mrr 
expert finding 
uk bl 
uk ct 
nl bl 
nl ct 
expert profiling 
uk bl 
uk ct 
nl bl 
nl ct 
table performance of the context models ct compared to 
the baseline bl best scores are in boldface 
proved over all measures for both tasks the coverage of topics 
and candidates for the expert finding and profiling tasks 
respectively is close to in all cases the relative improvement 
of the precision scores ranges from to these scores 
demonstrate that despite its simplicity our method for combining 
results over multiple languages achieves substantial improvements 
over the baseline 
 conclusions 
in this paper we focused on expertise retrieval expert finding 
and profiling in a new setting of a typical knowledge-intensive 
organization in which the available data is of high quality 
multilingual and covering a broad range of expertise area typically the 
amount of available data in such an organization e g a university 
a research institute or a research lab is limited when compared to 
the w c collection that has mostly been used for the experimental 
evaluation of expertise retrieval so far 
to examine expertise retrieval in this setting we introduced and 
released the uvt expert collection as a representative case of such 
knowledge intensive organizations the new collection reflects the 
typical properties of knowledge-intensive institutes noted above and 
also includes several features which may are potentially useful for 
expertise retrieval such as topical and organizational structure 
we evaluated how current state-of-the-art models for expert 
finding and profiling performed in this new setting and then refined 
these models in order to try and exploit the different 
characteristics within the data environment language topicality and 
organizational structure we found that current models of expertise 
retrieval generalize well to this new environment in addition we 
found that refining the models to account for the differences results 
in significant improvements thus making up for problems caused 
by data sparseness issues 
future work includes setting up manual assessments of 
automatically generated profiles by the employees themselves especially in 
cases where the employees have not provided a profile themselves 
 acknowledgments 
krisztian balog was supported by the netherlands organisation 
for scientific research nwo under project number - - 
maarten de rijke was also supported by nwo under project 
numbers - - - - - - 
 - - 
 and by the e u ist programme of the th 
fp for rtd under project multimatch contract ist- 
the work of toine bogers and antal van den bosch was funded 
by the iop-mmi-program of senternovem the dutch ministry 
of economic affairs as part of the `a propos project 
 references 
 l azzopardi incorporating context in the language modeling 
framework for ad-hoc information retrieval phd thesis university 
of paisley 
 k balog and m de rijke finding similar experts in this volume 
 
 k balog and m de rijke determining expert profiles with an 
application to expert finding in ijcai proc th intern joint conf 
on artificial intelligence pages - 
 k balog l azzopardi and m de rijke formal models for expert 
finding in enterprise corpora in sigir proc th annual 
intern acm sigir conf on research and development in information 
retrieval pages - 
 i becerra-fernandez the role of artificial intelligence technologies 
in the implementation of people-finder knowledge management 
systems in aaai workshop on bringing knowledge to business 
processes march 
 c s campbell p p maglio a cozzi and b dom expertise 
identification using email communications in cikm proc twelfth 
intern conf on information and knowledge management pages 
 
 g cao j -y nie and j bai integrating word relationships into 
language models in sigir proc th annual intern acm sigir 
conf on research and development in information retrieval pages 
 - 
 t m cover and j a thomas elements of information theory 
wiley-interscience 
 n craswell d hawking a m vercoustre and p wilkins p noptic 
expert searching for experts not just for documents in ausweb 
 n craswell a de vries and i soboroff overview of the 
trec enterprise track in the fourteenth text retrieval conf proc 
 trec 
 t h davenport and l prusak working knowledge how 
organizations manage what they know harvard business school press 
boston ma 
 t dunning accurate methods for the statistics of surprise and 
coincidence computational linguistics - 
 e filatova and j prager tell me what you do and i ll tell you what 
you are learning occupation-related activities for biographies in 
hlt emnlp 
 v lavrenko and w b croft relevance based language models in 
sigir proc th annual intern acm sigir conf on research 
and development in information retrieval pages - 
 v lavrenko m choquette and w b croft cross-lingual relevance 
models in sigir proc th annual intern acm sigir conf on 
research and development in information retrieval pages - 
 
 c macdonald and i ounis voting for candidates adapting data 
fusion techniques for an expert search task in cikm proc th 
acm intern conf on information and knowledge management pages 
 - 
 c manning and h sch¨utze foundations of statistical natural 
language processing the mit press 
 a mockus and j d herbsleb expertise browser a quantitative 
approach to identifying expertise in icse proc th intern conf 
on software engineering pages - 
 d petkova and w b croft hierarchical language models for expert 
finding in enterprise corpora in proc ictai pages - 
 
 i soboroff a de vries and n craswell overview of the trec 
 enterprise track in trec working notes 
 t tao x wang q mei and c zhai language model information 
retrieval with document expansion in hlt-naacl 
 trec enterprise track url http www ins cwi 
nl projects trec-ent wiki 
 g van noord textcat language guesser url http www 
let rug nl ˜vannoord textcat 
 w c the w c test collection url http research 
microsoft com users nickcr w c-summary html 
unified utility maximization framework for resource 
selection 
luo si 
language technology inst 
school of compute science 
carnegie mellon university 
pittsburgh pa 
lsi cs cmu edu 
jamie callan 
language technology inst 
school of compute science 
carnegie mellon university 
pittsburgh pa 
callan cs cmu edu 
abstract 
this paper presents a unified utility framework for resource 
selection of distributed text information retrieval this new 
framework shows an efficient and effective way to infer the 
probabilities of relevance of all the documents across the text 
databases with the estimated relevance information resource 
selection can be made by explicitly optimizing the goals of 
different applications specifically when used for database 
recommendation the selection is optimized for the goal of 
highrecall include as many relevant documents as possible in the 
selected databases when used for distributed document 
retrieval the selection targets the high-precision goal high 
precision in the final merged list of documents this new model 
provides a more solid framework for distributed information 
retrieval empirical studies show that it is at least as effective as 
other state-of-the-art algorithms 
categories and subject descriptors 
h information search and retrieval 
general terms 
algorithms 
 introduction 
conventional search engines such as google or altavista use 
ad-hoc information retrieval solution by assuming all the 
searchable documents can be copied into a single centralized 
database for the purpose of indexing distributed information 
retrieval also known as federated search is 
different from ad-hoc information retrieval as it addresses the 
cases when documents cannot be acquired and stored in a single 
database for example hidden web contents also called 
invisible or deep web contents are information on the web 
that cannot be accessed by the conventional search engines 
hidden web contents have been estimated to be - times 
larger than the contents that can be searched by conventional 
search engines therefore it is very important to search this type 
of valuable information 
the architecture of distributed search solution is highly 
influenced by different environmental characteristics in a small 
local area network such as small company environments the 
information providers may cooperate to provide corpus statistics 
or use the same type of search engines early distributed 
information retrieval research focused on this type of 
cooperative environments on the other side in a wide 
area network such as very large corporate environments or on 
the web there are many types of search engines and it is difficult 
to assume that all the information providers can cooperate as 
they are required even if they are willing to cooperate in these 
environments it may be hard to enforce a single solution for all 
the information providers or to detect whether information 
sources provide the correct information as they are required 
many applications fall into the latter type of uncooperative 
environments such as the mind project which integrates 
non-cooperating digital libraries or the qprober system 
which supports browsing and searching of uncooperative hidden 
web databases in this paper we focus mainly on uncooperative 
environments that contain multiple types of independent search 
engines 
there are three important sub-problems in distributed 
information retrieval first information about the contents of 
each individual database must be acquired resource 
representation second given a query a set of 
resources must be selected to do the search resource selection 
 third the results retrieved from all the selected 
resources have to be merged into a single final list before it can 
be presented to the end user retrieval and results merging 
 
many types of solutions exist for distributed information 
retrieval invisible-web net 
provides guided browsing of hidden 
web databases by collecting the resource descriptions of these 
databases and building hierarchies of classes that group them by 
similar topics a database recommendation system goes a step 
further than a browsing system like invisible-web net by 
recommending most relevant information sources to users 
queries it is composed of the resource description and the 
resource selection components this solution is useful when the 
users want to browse the selected databases by themselves 
instead of asking the system to retrieve relevant documents 
automatically distributed document retrieval is a more 
sophisticated task it selects relevant information sources for 
users queries as the database recommendation system does 
furthermore users queries are forwarded to the corresponding 
selected databases and the returned individual ranked lists are 
merged into a single list to present to the users 
the goal of a database recommendation system is to select a 
small set of resources that contain as many relevant documents 
as possible which we call a high-recall goal on the other side 
the effectiveness of distributed document retrieval is often 
measured by the precision of the final merged document result 
list which we call a high-precision goal prior research 
indicated that these two goals are related but not identical 
however most previous solutions simply use effective resource 
selection algorithm of database recommendation system for 
distributed document retrieval system or solve the inconsistency 
with heuristic methods 
this paper presents a unified utility maximization framework to 
integrate the resource selection problem of both database 
recommendation and distributed document retrieval together by 
treating them as different optimization goals 
first a centralized sample database is built by randomly 
sampling a small amount of documents from each database with 
query-based sampling database size statistics are also 
estimated a logistic transformation model is learned off 
line with a small amount of training queries to map the 
centralized document scores in the centralized sample database 
to the corresponding probabilities of relevance 
second after a new query is submitted the query can be used to 
search the centralized sample database which produces a score 
for each sampled document the probability of relevance for 
each document in the centralized sample database can be 
estimated by applying the logistic model to each document s 
score then the probabilities of relevance of all the mostly 
unseen documents among the available databases can be 
estimated using the probabilities of relevance of the documents 
in the centralized sample database and the database size 
estimates 
for the task of resource selection for a database 
recommendation system the databases can be ranked by the 
expected number of relevant documents to meet the high-recall 
goal for resource selection for a distributed document retrieval 
system databases containing a small number of documents with 
large probabilities of relevance are favored over databases 
containing many documents with small probabilities of 
relevance this selection criterion meets the high-precision goal 
of distributed document retrieval application furthermore the 
semi-supervised learning ssl algorithm is applied to 
merge the returned documents into a final ranked list 
the unified utility framework makes very few assumptions and 
works in uncooperative environments two key features make it 
a more solid model for distributed information retrieval i it 
formalizes the resource selection problems of different 
applications as various utility functions and optimizes the utility 
functions to achieve the optimal results accordingly and ii it 
shows an effective and efficient way to estimate the probabilities 
of relevance of all documents across databases specifically the 
framework builds logistic models on the centralized sample 
database to transform centralized retrieval scores to the 
corresponding probabilities of relevance and uses the centralized 
sample database as the bridge between individual databases and 
the logistic model the human effort relevance judgment 
required to train the single centralized logistic model does not 
scale with the number of databases this is a large advantage 
over previous research which required the amount of human 
effort to be linear with the number of databases 
the unified utility framework is not only more theoretically 
solid but also very effective empirical studies show the new 
model to be at least as accurate as the state-of-the-art algorithms 
in a variety of configurations 
the next section discusses related work section describes the 
new unified utility maximization model section explains our 
experimental methodology sections and present our 
experimental results for resource selection and document 
retrieval section concludes 
 prior research 
there has been considerable research on all the sub-problems of 
distributed information retrieval we survey the most related 
work in this section 
the first problem of distributed information retrieval is resource 
representation the starts protocol is one solution for 
acquiring resource descriptions in cooperative environments 
however in uncooperative environments even the databases are 
willing to share their information it is not easy to judge whether 
the information they provide is accurate or not furthermore it 
is not easy to coordinate the databases to provide resource 
representations that are compatible with each other thus in 
uncooperative environments one common choice is query-based 
sampling which randomly generates and sends queries to 
individual search engines and retrieves some documents to build 
the descriptions as the sampled documents are selected by 
random queries query-based sampling is not easily fooled by 
any adversarial spammer that is interested to attract more traffic 
experiments have shown that rather accurate resource 
descriptions can be built by sending about queries and 
downloading about documents 
many resource selection algorithms such as ggloss vgloss 
 and cori have been proposed in the last decade the 
cori algorithm represents each database by its terms the 
document frequencies and a small number of corpus statistics 
 details in as prior research on different datasets has shown 
the cori algorithm to be the most stable and effective of the 
three algorithms we use it as a baseline algorithm in 
this work the relevant document distribution estimation 
 redde resource selection algorithm is a recent algorithm 
that tries to estimate the distribution of relevant documents 
across the available databases and ranks the databases 
accordingly although the redde algorithm has been shown to 
be effective it relies on heuristic constants that are set 
empirically 
the last step of the document retrieval sub-problem is results 
merging which is the process of transforming database-specific 
 
document scores into comparable database-independent 
document scores the semi supervised learning ssl 
result merging algorithm uses the documents acquired by 
querybased sampling as training data and linear regression to learn the 
database-specific query-specific merging models these linear 
models are used to convert the database-specific document 
scores into the approximated centralized document scores the 
ssl algorithm has been shown to be effective it serves as 
an important component of our unified utility maximization 
framework section 
in order to achieve accurate document retrieval results many 
previous methods simply use resource selection algorithms that 
are effective of database recommendation system but as 
pointed out above a good resource selection algorithm 
optimized for high-recall may not work well for document 
retrieval which targets the high-precision goal this type of 
inconsistency has been observed in previous research 
the research in tried to solve the problem with a heuristic 
method 
the research most similar to what we propose here is the 
decision-theoretic framework dtf this framework 
computes a selection that minimizes the overall costs e g 
retrieval quality time of document retrieval system and several 
methods have been proposed to estimate the retrieval 
quality however two points distinguish our research from the 
dtf model first the dtf is a framework designed specifically 
for document retrieval but our new model integrates two 
distinct applications with different requirements database 
recommendation and distributed document retrieval into the 
same unified framework second the dtf builds a model for 
each database to calculate the probabilities of relevance this 
requires human relevance judgments for the results retrieved 
from each database in contrast our approach only builds one 
logistic model for the centralized sample database the 
centralized sample database can serve as a bridge to connect the 
individual databases with the centralized logistic model thus the 
probabilities of relevance of documents in different databases 
can be estimated this strategy can save large amount of human 
judgment effort and is a big advantage of the unified utility 
maximization framework over the dtf especially when there 
are a large number of databases 
 unified utility maximization 
framework 
the unified utility maximization uum framework is based 
on estimating the probabilities of relevance of the mostly 
unseen documents available in the distributed search 
environment in this section we describe how the probabilities of 
relevance are estimated and how they are used by the unified 
utility maximization model we also describe how the model 
can be optimized for the high-recall goal of a database 
recommendation system and the high-precision goal of a 
distributed document retrieval system 
 estimating probabilities of relevance 
as pointed out above the purpose of resource selection is 
highrecall and the purpose of document retrieval is high-precision in 
order to meet these diverse goals the key issue is to estimate the 
probabilities of relevance of the documents in various databases 
this is a difficult problem because we can only observe a 
sample of the contents of each database using query-based 
sampling our strategy is to make full use of all the available 
information to calculate the probability estimates 
 learning probabilities of relevance 
in the resource description step the centralized sample database 
is built by query-based sampling and the database sizes are 
estimated using the sample-resample method at the same 
time an effective retrieval algorithm inquery is applied on 
the centralized sample database with a small number e g 
of training queries for each training query the cori resource 
selection algorithm is applied to select some number 
 e g of databases and retrieve document ids from each 
database the ssl results merging algorithm is used to 
merge the results then we can download the top documents 
in the final merged list and calculate their corresponding 
centralized scores using inquery and the corpus statistics of the 
centralized sample database the centralized scores are further 
normalized divided by the maximum centralized score for each 
query as this method has been suggested to improve estimation 
accuracy in previous research human judgment is acquired 
for those documents and a logistic model is built to transform 
the normalized centralized document scores to probabilities of 
relevance as follows 
 
 exp 
 exp 
 
 
dsba 
dsba 
drelpdr 
ccc 
ccc 
 
 
 
where 
 
dsc 
is the normalized centralized document score and 
ac and bc are the two parameters of the logistic model these two 
parameters are estimated by maximizing the probabilities of 
relevance of the training queries the logistic model provides us 
the tool to calculate the probabilities of relevance from 
centralized document scores 
 estimating centralized document scores 
when the user submits a new query the centralized document 
scores of the documents in the centralized sample database are 
calculated however in order to calculate the probabilities of 
relevance we need to estimate centralized document scores for 
all documents across the databases instead of only the sampled 
documents this goal is accomplished using the centralized 
scores of the documents in the centralized sample database and 
the database size statistics 
we define the database scale factor for the ith 
database as the 
ratio of the estimated database size and the number of 
documents sampled from this database as follows 
 
 
i 
i 
i 
db 
db 
db samp 
n 
sf 
n 
 
where 
 
idbn is the estimated database size and idb sampn is the 
number of documents from the ith 
database in the centralized 
sample database the intuition behind the database scale factor 
is that for a database whose scale factor is if one document 
from this database in the centralized sample database has a 
centralized document score of we may guess that there are 
about documents in that database which have scores of about 
 actually we can apply a finer non-parametric linear 
interpolation method to estimate the centralized document score 
curve for each database formally we rank all the sampled 
documents from the ith 
database by their centralized document 
 
scores to get the sampled centralized document score list 
{sc dsi sc dsi sc dsi   } for the ith 
database we assume 
that if we could calculate the centralized document scores for all 
the documents in this database and get the complete centralized 
document score list the top document in the sampled list would 
have rank sfdbi the second document in the sampled list 
would rank sfdbi and so on therefore the data points of 
sampled documents in the complete list are { sfdbi sc dsi 
 sfdbi sc dsi sfdbi sc dsi   } piecewise linear 
interpolation is applied to estimate the centralized document 
score curve as illustrated in figure the complete centralized 
document score list can be estimated by calculating the values of 
different ranks on the centralized document curve as 
 s 
 
c idbij njd ∈ 
it can be seen from figure that more sample data points 
produce more accurate estimates of the centralized document 
score curves however for databases with large database scale 
ratios this kind of linear interpolation may be rather inaccurate 
especially for the top ranked e g sfdbi documents 
therefore an alternative solution is proposed to estimate the 
centralized document scores of the top ranked documents for 
databases with large scale ratios e g larger than 
specifically a logistic model is built for each of these databases 
the logistic model is used to estimate the centralized document 
score of the top document in the corresponding database by 
using the two sampled documents from that database with 
highest centralized scores 
 exp 
 exp 
 
 
 
 
 
iciicii 
iciicii 
ic 
dssdss 
dssdss 
ds 
ααα 
ααα 
 
 
 
 iα iα and iα are the parameters of the logistic model for 
each training query the top retrieved document of each database 
is downloaded and the corresponding centralized document 
score is calculated together with the scores of the top two 
sampled documents these parameters can be estimated 
after the centralized score of the top document is estimated an 
exponential function is fitted for the top part sfdbi of the 
centralized document score curve as 
 exp 
 
idbiiijc sfjjds ∈ ββ 
 
 log i c i is dβ β − 
 
 log log 
 
 
 
− 
− 
 
idb 
icic 
i 
sf 
dsdss 
β 
the two parameters iβ and iβ are fitted to make sure the 
exponential function passes through the two points 
 
 ic ds 
and sfdbi sc dsi the exponential function is only used to 
adjust the top part of the centralized document score curve and 
the lower part of the curve is still fitted with the linear 
interpolation method described above the adjustment by fitting 
exponential function of the top ranked documents has been 
shown empirically to produce more accurate results 
from the centralized document score curves we can estimate 
the complete centralized document score lists accordingly for all 
the available databases after the estimated centralized 
document scores are normalized the complete lists of 
probabilities of relevance can be constructed out of the complete 
centralized document score lists by equation formally for the 
ith 
database the complete list of probabilities of relevance is 
 r 
 
idbij njd ∈ 
 the unified utility maximization model 
in this section we formally define the new unified utility 
maximization model which optimizes the resource selection 
problems for two goals of high-recall database 
recommendation and high-precision distributed document 
retrieval in the same framework 
in the task of database recommendation the system needs to 
decide how to rank databases in the task of document retrieval 
the system not only needs to select the databases but also needs 
to decide how many documents to retrieve from each selected 
database we generalize the database recommendation selection 
process which implicitly recommends all documents in every 
selected database as a special case of the selection decision for 
the document retrieval task formally we denote di as the 
number of documents we would like to retrieve from the ith 
database and } { ddd as a selection action for all the 
databases 
the database selection decision is made based on the complete 
lists of probabilities of relevance for all the databases the 
complete lists of probabilities of relevance are inferred from all 
the available information specifically sr which stands for the 
resource descriptions acquired by query-based sampling and the 
database size estimates acquired by sample-resample cs stands 
for the centralized document scores of the documents in the 
centralized sample database 
if the method of estimating centralized document scores and 
probabilities of relevance in section is acceptable then the 
most probable complete lists of probabilities of relevance can be 
derived and we denote them as 
 
 
 { r dbjd j nθ ∈ 
 
 
 r }dbjd j n∈ random vector 
 
denotes an 
arbitrary set of complete lists of probabilities of relevance and 
 cs srp θ as the probability of generating this set of lists 
finally to each selection action d and a set of complete lists of 
figure linear interpolation construction of the complete 
centralized document score list database scale factor is 
 
probabilities of relevance θ we associate a utility function 
 du θ which indicates the benefit from making the d 
selection when the true complete lists of probabilities of 
relevance are θ 
therefore the selection decision defined by the bayesian 
framework is 
θθθ 
θ 
dsrpdud cs 
d 
 maxarg 
 
 
one common approach to simplify the computation in the 
bayesian framework is to only calculate the utility function at 
the most probable parameter values instead of calculating the 
whole expectation in other words we only need to calculate 
 
du θ and equation is simplified as follows 
 maxarg 
 
θdud 
d 
 
this equation serves as the basic model for both the database 
recommendation system and the document retrieval system 
 resource selection for high-recall 
high-recall is the goal of the resource selection algorithm in 
federated search tasks such as database recommendation the 
goal is to select a small set of resources e g less than nsdb 
databases that contain as many relevant documents as possible 
which can be formally defined as 
 
 
i 
n 
j 
iji 
idb 
ddidu 
 
 
 
 
 r θ 
i di is the indicator function which is when the ith 
database is 
selected and otherwise plug this equation into the basic model 
in equation and associate the selected database number 
constraint to obtain the following 
sdb 
i 
i 
i 
n 
j 
iji 
d 
nditosubject 
ddid 
idb 
 
 
 
 
 r maxarg 
 
 
 
 
the solution of this optimization problem is very simple we 
can calculate the expected number of relevant documents for 
each database as follows 
 
 
idb 
i 
n 
j 
ijrd dn 
 
 
 
 r 
the nsdb databases with the largest expected number of relevant 
documents can be selected to meet the high-recall goal we call 
this the uum hr algorithm unified utility maximization for 
high-recall 
 resource selection for high-precision 
high-precision is the goal of resource selection algorithm in 
federated search tasks such as distributed document retrieval it 
is measured by the precision at the top part of the final merged 
document list this high-precision criterion is realized by the 
following utility function which measures the precision of 
retrieved documents from the selected databases 
 
 
i 
d 
j 
iji 
i 
ddidu 
 
 
 
 r θ 
note that the key difference between equation and equation 
 is that equation sums up the probabilities of relevance of all 
the documents in a database while equation only considers a 
much smaller part of the ranking specifically we can calculate 
the optimal selection decision by 
 
 
i 
d 
j 
iji 
d 
i 
ddid 
 
 
 r maxarg 
different kinds of constraints caused by different characteristics 
of the document retrieval tasks can be associated with the above 
optimization problem the most common one is to select a fixed 
number nsdb of databases and retrieve a fixed number nrdoc of 
documents from each selected database formally defined as 
 
 
 r maxarg 
 
 
≠ 
 
 
 
irdoci 
sdb 
i 
i 
i 
d 
j 
iji 
d 
difnd 
nditosubject 
ddid 
i 
 
this optimization problem can be solved easily by calculating 
the number of expected relevant documents in the top part of the 
each database s complete list of probabilities of relevance 
 
 
rdoc 
i 
n 
j 
ijrdtop dn 
 
 
 r 
then the databases can be ranked by these values and selected 
we call this the uum hp-fl algorithm unified utility 
maximization for high-precision with fixed length document 
rankings from each selected database 
a more complex situation is to vary the number of retrieved 
documents from each selected database more specifically we 
allow different selected databases to return different numbers of 
documents for simplification the result list lengths are required 
to be multiples of a baseline number this value can also be 
varied but for simplification it is set to in this paper this 
restriction is set to simulate the behavior of commercial search 
engines on the web search engines such as google and 
altavista return only or document ids for every result 
page this procedure saves the computation time of calculating 
optimal database selection by allowing the step of dynamic 
programming to be instead of more detail is discussed 
latterly for further simplification we restrict to select at most 
 documents from each database di then the 
selection optimization problem is formalized as follows 
 
 
 r maxarg 
 
 
 
∈ 
 
 
 
 
kkd 
nd 
nditosubject 
ddid 
i 
rdoctotal 
i 
i 
sdb 
i 
i 
i 
d 
j 
iji 
d 
i 
 
ntotal rdoc is the total number of documents to be retrieved 
unfortunately there is no simple solution for this optimization 
problem as there are for equations and however a 
 
dynamic programming algorithm can be applied to calculate the 
optimal solution the basic steps of this dynamic programming 
method are described in figure as this algorithm allows 
retrieving result lists of varying lengths from each selected 
database it is called uum hp-vl algorithm 
after the selection decisions are made the selected databases are 
searched and the corresponding document ids are retrieved from 
each database the final step of document retrieval is to merge 
the returned results into a single ranked list with the 
semisupervised learning algorithm it was pointed out before that the 
ssl algorithm maps the database-specific scores into the 
centralized document scores and builds the final ranked list 
accordingly which is consistent with all our selection 
procedures where documents with higher probabilities of 
relevance thus higher centralized document scores are selected 
 experimental methodology 
 testbeds 
it is desirable to evaluate distributed information retrieval 
algorithms with testbeds that closely simulate the real world 
applications 
the trec web collections wt g or wt g provide a 
way to partition documents by different web servers in this 
way a large number o of databases with rather diverse 
contents could be created which may make this testbed a good 
candidate to simulate the operational environments such as open 
domain hidden web however two weakness of this testbed are 
i each database contains only a small amount of document 
documents by average for wt g and ii the contents of 
wt g or wt g are arbitrarily crawled from the web it is not 
likely for a hidden web database to provide personal homepages 
or web pages indicating that the pages are under construction 
and there is no useful information at all these types of web 
pages are contained in the wt g wt g datasets therefore 
the noisy web data is not similar with that of high-quality 
hidden web database contents which are usually organized by 
domain experts 
another choice is the trec news government data 
 trec news government data is concentrated on 
relatively narrow topics compared with trec web data i the 
news government documents are much more similar to the 
contents provided by a topic-oriented database than an arbitrary 
web page ii a database in this testbed is larger than that of 
trec web data by average a database contains thousands of 
documents which is more realistic than a database of trec 
web data with about documents as the contents and sizes 
of the databases in the trec news government testbed are more 
similar with that of a topic-oriented database it is a good 
candidate to simulate the distributed information retrieval 
environments of large organizations companies or 
domainspecific hidden web sites such as west that provides access to 
legal financial and news text databases as most current 
distributed information retrieval systems are developed for the 
environments of large organizations companies or 
domainspecific hidden web other than open domain hidden web 
trec news government testbed was chosen in this work 
trec - col-bysource testbed is one of the most used trec 
news government testbed it was chosen in this 
work three testbeds in with skewed database size 
distributions and different types of relevant document 
distributions were also used to give more thorough simulation 
for real environments 
trec - col-bysource databases were created from 
trec cds and they were organized by source and 
publication date the sizes of the databases are not skewed 
details are in table 
three testbeds built in were based on the 
trec - colbysource testbed each testbed contains many small databases 
and two large databases created by merging about - small 
databases together 
input complete lists of probabilities of relevance for all 
the db databases 
output optimal selection solution for equation 
i create the three-dimensional array 
sel db ntotal rdoc nsdb 
each sel x y z is associated with a selection 
decision xyzd which represents the best selection 
decision in the condition only databases from number 
to number x are considered for selection totally y 
documents will be retrieved only z databases are 
selected out of the x database candidates and 
sel x y z is the corresponding utility value by 
choosing the best selection 
ii initialize sel ntotal rdoc nsdb with only the 
estimated relevance information of the st 
database 
iii iterate the current database candidate i from to db 
for each entry sel i y z 
find k such that 
 min 
 maxarg 
 
 
 
yktosubject 
drzkyiselk 
kj 
ij 
k 
≤≤ 
 −−− 
≤ 
 
 
 
 
 
zyiseldrzkyiselif 
kj 
ij − −−− 
≤ 
this means that we should retrieve 
 k∗ documents 
from the ith 
database otherwise we should not select this 
database and the previous best solution sel i- y z 
should be kept 
then set the value of iyzd and sel i y z accordingly 
iv the best selection solution is given by toral rdoc sdbdb n nd 
and the corresponding utility value is sel db 
ntotal rdoc nsdb 
figure the dynamic programming optimization 
procedure for equation 
table testbed statistics 
number of documents size mb 
testbed 
size 
 gb min avg max min avg max 
trec 
table query set statistics 
name 
trec 
topic set 
trec 
topic field 
average length 
 words 
trec - title 
 
trec - ldb- col representative the databases in the 
trec - col-bysource were sorted with alphabetical order 
two large databases were created by merging small 
databases with the round-robin method thus the two large 
databases have more relevant documents due to their large sizes 
even though the densities of relevant documents are roughly the 
same as the small databases 
trec -ap-wsj- col relevant the associated press 
collections and the wall street journal collections in the 
trec - col-bysource testbed were collapsed into two large 
databases apall and wsjall the other collections were left 
unchanged the apall and wsjall databases have higher 
densities of documents relevant to trec queries than the small 
databases thus the two large databases have many more 
relevant documents than the small databases 
trec -fr-doe- col nonrelevant the federal 
register collections and the department of energy collections 
in the trec - col-bysource testbed were collapsed into two 
large databases frall and doeall the other collections were 
left unchanged the frall and doeall databases have lower 
densities of documents relevant to trec queries than the small 
databases even though they are much larger 
 queries were created from the title fields of trec topics 
 - the queries - were used as training queries and 
the queries - were used as test queries details in table 
 search engines 
in the uncooperative distributed information retrieval 
environments of large organizations companies or 
domainspecific hidden web different databases may use different types 
of search engine to simulate the multiple type-engine 
environment three different types of search engines were used 
in the experiments inquery a unigram statistical 
language model with linear smoothing and a tfidf 
retrieval algorithm with ltc weight all these 
algorithms were implemented with the lemur toolkit 
these three kinds of search engines were assigned to the 
databases among the four testbeds in a round-robin manner 
 results resource selection of 
database recommendation 
all four testbeds described in section were used in the 
experiments to evaluate the resource selection effectiveness of 
the database recommendation system 
the resource descriptions were created using query-based 
sampling about queries were sent to each database to 
download unique documents the database size statistics 
were estimated by the sample-resample method fifty 
queries - were used as training queries to build the 
relevant logistic model and to fit the exponential functions of the 
centralized document score curves for large ratio databases 
 details in section another queries - were used 
as test data 
resource selection algorithms of database recommendation 
systems are typically compared using the recall metric nr 
 let b denote a baseline ranking which is often the 
rbr relevance based ranking and e as a ranking provided by 
a resource selection algorithm and let bi and ei denote the 
number of relevant documents in the ith 
ranked database of b or 
e then rn is defined as follows 
 
 
 k 
i i 
k 
i i 
k 
b 
e 
r 
 
 
 
usually the goal is to search only a few databases so our figures 
only show results for selecting up to databases 
the experiments summarized in figure compared the 
effectiveness of the three resource selection algorithms namely 
the cori redde and uum hr the uum hr algorithm is 
described in section it can be seen from figure that the 
redde and uum hr algorithms are more effective on the 
representative relevant and nonrelevant testbeds or as good as 
 on the trec - col testbed the cori resource selection 
algorithm the uum hr algorithm is more effective than the 
redde algorithm on the representative and relevant testbeds 
and is about the same as the redde algorithm on the 
trec col and the nonrelevant testbeds this suggests that the 
uum hr algorithm is more robust than the redde algorithm 
it can be noted that when selecting only a few databases on the 
trec - col or the nonrelevant testbeds the redee 
algorithm has a small advantage over the uum hr algorithm 
we attribute this to two causes i the redde algorithm was 
tuned on the trec - col testbed and ii although the 
difference is small this may suggest that our logistic model of 
estimating probabilities of relevance is not accurate enough 
more training data or a more sophisticated model may help to 
solve this minor puzzle 
collections selected collections selected 
trec - col testbed representative testbed 
collection selected collection selected 
relevant testbed nonrelevant testbed 
figure resource selection experiments on the four testbeds 
 
 results document retrieval 
effectiveness 
for document retrieval the selected databases are searched and 
the returned results are merged into a single final list in all of 
the experiments discussed in this section the results retrieved 
from individual databases were combined by the 
semisupervised learning results merging algorithm this version of 
the ssl algorithm is allowed to download a small number 
of returned document texts on the fly to create additional 
training data in the process of learning the linear models which 
map database-specific document scores into estimated 
centralized document scores it has been shown to be very 
effective in environments where only short result-lists are 
retrieved from each selected database this is a common 
scenario in operational environments and was the case for our 
experiments 
document retrieval effectiveness was measured by precision at 
the top part of the final document list the experiments in this 
section were conducted to study the document retrieval 
effectiveness of five selection algorithms namely the cori 
redde uum hr uum hp-fl and uum hp-vl algorithms 
the last three algorithms were proposed in section all the 
first four algorithms selected or databases and documents 
were retrieved from each selected database the uum hp-fl 
algorithm also selected or databases but it was allowed to 
adjust the number of documents to retrieve from each selected 
database the number retrieved was constrained to be from to 
 and a multiple of 
the trec - col and representative testbeds were selected 
for document retrieval as they represent two extreme cases of 
resource selection effectiveness in one case the cori algorithm 
is as good as the other algorithms and in the other case it is quite 
table precision on the representative testbed when databases were selected the first baseline is cori the second baseline for 
uum hp methods is uum hr 
precision at 
doc rank 
cori redde uum hr uum hp-fl uum hp-vl 
 docs - 
 docs - 
 docs - 
 docs - 
 docs - 
table precision on the representative testbed when databases were selected the first baseline is cori the second baseline for 
uum hp methods is uum hr 
precision at 
doc rank 
cori redde uum hr uum hp-fl uum hp-vl 
 docs - - 
 docs 
 docs 
 docs 
 docs - 
table precision on the trec - col-bysource testbed when databases were selected the first baseline is cori the second 
baseline for uum hp methods is uum hr 
precision at 
doc rank 
cori redde uum hr uum hp-fl uum hp-vl 
 docs - 
 docs - 
 docs - 
 docs - 
 docs - 
table precision on the trec - col-bysource testbed when databases were selected the first baseline is cori the second 
baseline for uum hp methods is uum hr 
precision at 
doc rank 
cori redde uum hr uum hp-fl uum hp-vl 
 docs - 
 docs - 
 docs 
 docs - 
 docs - 
 
a lot worse than the other algorithms tables and show the 
results on the trec - col testbed and tables and show 
the results on the representative testbed 
on the trec - col testbed the document retrieval 
effectiveness of the cori selection algorithm is roughly the 
same or a little bit better than the redde algorithm but both of 
them are worse than the other three algorithms tables and 
the uum hr algorithm has a small advantage over the cori 
and redde algorithms one main difference between the 
uum hr algorithm and the redde algorithm was pointed out 
before the uum hr uses training data and linear interpolation 
to estimate the centralized document score curves while the 
redde algorithm uses a heuristic method assumes the 
centralized document score curves are step functions and makes 
no distinction among the top part of the curves this difference 
makes uum hr better than the redde algorithm at 
distinguishing documents with high probabilities of relevance 
from low probabilities of relevance therefore the uum hr 
reflects the high-precision retrieval goal better than the redde 
algorithm and thus is more effective for document retrieval 
the uum hr algorithm does not explicitly optimize the 
selection decision with respect to the high-precision goal as the 
uum hp-fl and uum hp-vl algorithms are designed to do 
it can be seen that on this testbed the uum hp-fl and 
uum hp-vl algorithms are much more effective than all the 
other algorithms this indicates that their power comes from 
explicitly optimizing the high-precision goal of document 
retrieval in equations and 
on the representative testbed cori is much less effective than 
other algorithms for distributed document retrieval tables and 
 the document retrieval results of the redde algorithm are 
better than that of the cori algorithm but still worse than the 
results of the uum hr algorithm on this testbed the three 
uum algorithms are about equally effective detailed analysis 
shows that the overlap of the selected databases between the 
uum hr uum hp-fl and uum hp-vl algorithms is much 
larger than the experiments on the trec - col testbed 
since all of them tend to select the two large databases this 
explains why they are about equally effective for document 
retrieval 
in real operational environments databases may return no 
document scores and report only ranked lists of results as the 
unified utility maximization model only utilizes retrieval scores 
of sampled documents with a centralized retrieval algorithm to 
calculate the probabilities of relevance it makes database 
selection decisions without referring to the document scores 
from individual databases and can be easily generalized to this 
case of rank lists without document scores the only adjustment 
is that the ssl algorithm merges ranked lists without document 
scores by assigning the documents with pseudo-document scores 
normalized for their ranks in a ranked list of documents the 
first one has a score of the second has a score of etc 
 which has been studied in the experiment results on 
trec - col-bysource testbed with selected databases are 
shown in table the experiment setting was the same as 
before except that the document scores were eliminated 
intentionally and the selected databases only return ranked lists 
of document ids it can be seen from the results that the 
uum hp-fl and uum hp-vl work well with databases 
returning no document scores and are still more effective than 
other alternatives other experiments with databases that return 
no document scores are not reported but they show similar 
results to prove the effectiveness of uum hp-fl and 
uum hpvl algorithms 
the above experiments suggest that it is very important to 
optimize the high-precision goal explicitly in document 
retrieval the new algorithms based on this principle achieve 
better or at least as good results as the prior state-of-the-art 
algorithms in several environments 
 conclusion 
distributed information retrieval solves the problem of finding 
information that is scattered among many text databases on local 
area networks and internets most previous research use 
effective resource selection algorithm of database 
recommendation system for distributed document retrieval 
application we argue that the high-recall resource selection 
goal of database recommendation and high-precision goal of 
document retrieval are related but not identical this kind of 
inconsistency has also been observed in previous work but the 
prior solutions either used heuristic methods or assumed 
cooperation by individual databases e g all the databases used 
the same kind of search engines which is frequently not true in 
the uncooperative environment 
in this work we propose a unified utility maximization model to 
integrate the resource selection of database recommendation and 
document retrieval tasks into a single unified framework in this 
framework the selection decisions are obtained by optimizing 
different objective functions as far as we know this is the first 
work that tries to view and theoretically model the distributed 
information retrieval task in an integrated manner 
the new framework continues a recent research trend studying 
the use of query-based sampling and a centralized sample 
database a single logistic model was trained on the centralized 
table precision on the trec - col-bysource testbed when databases were selected the first baseline is cori the second 
baseline for uum hp methods is uum hr search engines do not return document scores 
precision at 
doc rank 
cori redde uum hr uum hp-fl uum hp-vl 
 docs - 
 docs - 
 docs - 
 docs - 
 docs - 
 
sample database to estimate the probabilities of relevance of 
documents by their centralized retrieval scores while the 
centralized sample database serves as a bridge to connect the 
individual databases with the centralized logistic model 
therefore the probabilities of relevance for all the documents 
across the databases can be estimated with very small amount of 
human relevance judgment which is much more efficient than 
previous methods that build a separate model for each database 
this framework is not only more theoretically solid but also 
very effective one algorithm for resource selection uum hr 
and two algorithms for document retrieval uum hp-fl and 
uum hp-vl are derived from this framework empirical 
studies have been conducted on testbeds to simulate the 
distributed search solutions of large organizations companies 
or domain-specific hidden web furthermore the uum hp-fl 
and uum hp-vl resource selection algorithms are extended 
with a variant of ssl results merging algorithm to address the 
distributed document retrieval task when selected databases do 
not return document scores experiments have shown that these 
algorithms achieve results that are at least as good as the prior 
state-of-the-art and sometimes considerably better detailed 
analysis indicates that the advantage of these algorithms comes 
from explicitly optimizing the goals of the specific tasks 
the unified utility maximization framework is open for different 
extensions when cost is associated with searching the online 
databases the utility framework can be adjusted to automatically 
estimate the best number of databases to search so that a large 
amount of relevant documents can be retrieved with relatively 
small costs another extension of the framework is to consider 
the retrieval effectiveness of the online databases which is an 
important issue in the operational environments all of these are 
the directions of future research 
acknowledgement 
this research was supported by nsf grants eia- and 
iis- any opinions findings conclusions or 
recommendations expressed in this paper are the authors and 
do not necessarily reflect those of the sponsor 
references 
 j callan distributed information retrieval in w b 
croft editor advances in information retrieval kluwer 
academic publishers pp - 
 j callan w b croft and j broglio trec and 
tipster experiments with inquery information 
processing and management pp - 
 j g conrad x s guo p jackson and m meziou 
 database selection using actual physical and 
acquired logical collection resources in a massive 
domainspecific operational environment distributed search over 
the hidden web hierarchical database sampling and 
selection in proceedings of the th 
international 
conference on very large databases vldb 
 n craswell methods for distributed information 
retrieval ph d thesis the australian nation university 
 n craswell d hawking and p thistlewaite 
merging results from isolated search engines in 
proceedings of th australasian database conference 
 d d souza j thom and j zobel a comparison 
of techniques for selecting text collections in proceedings 
of the th australasian database conference 
 n fuhr a decision-theoretic approach to 
database selection in networked ir acm transactions on 
information systems pp - 
 l gravano c chang h garcia-molina and a paepcke 
 starts stanford proposal for internet 
metasearching in proceedings of the th acm-sigmod 
international conference on management of data 
 l gravano p ipeirotis and m sahami qprober 
a system for automatic classification of hidden-web 
databases acm transactions on information systems 
 
 p ipeirotis and l gravano distributed search over 
the hidden web hierarchical database sampling and 
selection in proceedings of the th international 
conference on very large databases vldb 
 invisibleweb com http www invisibleweb com 
 the lemur toolkit http www cs cmu edu  lemur 
 j lu and j callan content-based information 
retrieval in peer-to-peer networks in proceedings of the 
 th international conference on information and 
knowledge management 
 w meng c t yu and k l liu building efficient 
and effective metasearch engines acm comput surv 
 
 h nottelmann and n fuhr evaluating different 
method of estimating retrieval quality for resource 
selection in proceedings of the th annual international 
acm sigir conference on research and development in 
information retrieval 
 h nottelmann and n fuhr the mind 
architecture for heterogeneous multimedia federated digital 
libraries acm sigir workshop on distributed 
information retrieval 
 a l powell j c french j callan m connell and c l 
viles the impact of database selection on 
distributed searching in proceedings of the rd annual 
international acm sigir conference on research and 
development in information retrieval 
 a l powell and j c french comparing the 
performance of database selection algorithms acm 
transactions on information systems pp - 
 c sherman search for the invisible web guardian 
unlimited 
 l si and j callan using sampled data and 
regression to merge search engine results in proceedings 
of the th annual international acm sigir conference 
on research and development in information retrieval 
 l si and j callan relevant document distribution 
estimation method for resource selection in proceedings of 
the th annual international acm sigir conference on 
research and development in information retrieval 
 l si and j callan a semi-supervised learning 
method to merge search engine results acm transactions 
on information systems pp - 
 
beyond pagerank machine learning for static ranking 
matthew richardson 
microsoft research 
one microsoft way 
redmond wa 
 - 
mattri microsoft com 
amit prakash 
msn 
one microsoft way 
redmond wa 
 - 
amitp microsoft com 
eric brill 
microsoft research 
one microsoft way 
redmond wa 
 - 
brill microsoft com 
abstract 
since the publication of brin and page s paper on pagerank 
many in the web community have depended on pagerank for the 
static query-independent ordering of web pages we show that 
we can significantly outperform pagerank using features that are 
independent of the link structure of the web we gain a further 
boost in accuracy by using data on the frequency at which users 
visit web pages we use ranknet a ranking machine learning 
algorithm to combine these and other static features based on 
anchor text and domain characteristics the resulting model 
achieves a static ranking pairwise accuracy of vs 
for pagerank or for random 
categories and subject descriptors 
i artificial intelligence learning h information 
storage and retrieval information search and retrieval 
general terms 
algorithms measurement performance experimentation 
 introduction 
over the past decade the web has grown exponentially in size 
unfortunately this growth has not been isolated to good-quality 
pages the number of incorrect spamming and malicious e g 
phishing sites has also grown rapidly the sheer number of both 
good and bad pages on the web has led to an increasing reliance 
on search engines for the discovery of useful information users 
rely on search engines not only to return pages related to their 
search query but also to separate the good from the bad and 
order results so that the best pages are suggested first 
to date most work on web page ranking has focused on 
improving the ordering of the results returned to the user 
 querydependent ranking or dynamic ranking however having a good 
query-independent ranking static ranking is also crucially 
important for a search engine a good static ranking algorithm 
provides numerous benefits 
 relevance the static rank of a page provides a general 
indicator to the overall quality of the page this is a 
useful input to the dynamic ranking algorithm 
 efficiency typically the search engine s index is 
ordered by static rank by traversing the index from 
highquality to low-quality pages the dynamic ranker may 
abort the search when it determines that no later page 
will have as high of a dynamic rank as those already 
found the more accurate the static rank the better this 
early-stopping ability and hence the quicker the search 
engine may respond to queries 
 crawl priority the web grows and changes as quickly 
as search engines can crawl it search engines need a way 
to prioritize their crawl-to determine which pages to 
recrawl how frequently and how often to seek out new 
pages among other factors the static rank of a page is 
used to determine this prioritization a better static rank 
thus provides the engine with a higher quality more 
upto-date index 
google is often regarded as the first commercially successful 
search engine their ranking was originally based on the 
pagerank algorithm due to this and possibly due to 
google s promotion of pagerank to the public pagerank is 
widely regarded as the best method for the static ranking of web 
pages 
though pagerank has historically been thought to perform quite 
well there has yet been little academic evidence to support this 
claim even worse there has recently been work showing that 
pagerank may not perform any better than other simple measures 
on certain tasks upstill et al have found that for the task of 
finding home pages the number of pages linking to a page and the 
type of url were as or more effective than pagerank they 
found similar results for the task of finding high quality 
companies pagerank has also been used in systems for 
trec s very large collection and web track competitions 
but with much less success than had been expected finally 
amento et al found that simple features such as the number 
of pages on a site performed as well as pagerank 
despite these the general belief remains among many both 
academic and in the public that pagerank is an essential factor 
for a good static rank failing this it is still assumed that using the 
link structure is crucial in the form of the number of inlinks or the 
amount of anchor text 
in this paper we show there are a number of simple url- or 
pagebased features that significantly outperform pagerank for the 
purposes of statically ranking web pages despite ignoring the 
structure of the web we combine these and other static features 
using machine learning to achieve a ranking system that is 
significantly better than pagerank in pairwise agreement with 
human labels 
a machine learning approach for static ranking has other 
advantages besides the quality of the ranking because the 
measure consists of many features it is harder for malicious users 
to manipulate it i e to raise their page s static rank to an 
undeserved level through questionable techniques also known as 
web spamming this is particularly true if the feature set is not 
known in contrast a single measure like pagerank can be easier 
to manipulate because spammers need only concentrate on one 
goal how to cause more pages to point to their page with an 
algorithm that learns a feature that becomes unusable due to 
spammer manipulation will simply be reduced or removed from 
the final computation of rank this flexibility allows a ranking 
system to rapidly react to new spamming techniques 
a machine learning approach to static ranking is also able to take 
advantage of any advances in the machine learning field for 
example recent work on adversarial classification suggests 
that it may be possible to explicitly model the web page 
spammer s the adversary actions adjusting the ranking model in 
advance of the spammer s attempts to circumvent it another 
example is the elimination of outliers in constructing the model 
which helps reduce the effect that unique sites may have on the 
overall quality of the static rank by moving static ranking to a 
machine learning framework we not only gain in accuracy but 
also gain in the ability to react to spammer s actions to rapidly 
add new features to the ranking algorithm and to leverage 
advances in the rapidly growing field of machine learning 
finally we believe there will be significant advantages to using 
this technique for other domains such as searching a local hard 
drive or a corporation s intranet these are domains where the 
link structure is particularly weak or non-existent but there are 
other domain-specific features that could be just as powerful for 
example the author of an intranet page and his her position in the 
organization e g ceo manager or developer could provide 
significant clues as to the importance of that page a machine 
learning approach thus allows rapid development of a good static 
algorithm in new domains 
this paper s contribution is a systematic study of static features 
including pagerank for the purposes of statically ranking web 
pages previous studies on pagerank typically used subsets of the 
web that are significantly smaller e g the trec vlc corpus 
used by many contains only million pages also the 
performance of pagerank and other static features has typically 
been evaluated in the context of a complete system for dynamic 
ranking or for other tasks such as question answering in contrast 
we explore the use of pagerank and other features for the direct 
task of statically ranking web pages 
we first briefly describe the pagerank algorithm in section we 
introduce ranknet the machine learning technique used to 
combine static features into a final ranking section describes 
the static features the heart of the paper is in section which 
presents our experiments and results we conclude with a 
discussion of related and future work 
 pagerank 
the basic idea behind pagerank is simple a link from a web 
page to another can be seen as an endorsement of that page in 
general links are made by people as such they are indicative of 
the quality of the pages to which they point - when creating a 
page an author presumably chooses to link to pages deemed to be 
of good quality we can take advantage of this linkage 
information to order web pages according to their perceived 
quality 
imagine a web surfer who jumps from web page to web page 
choosing with uniform probability which link to follow at each 
step in order to reduce the effect of dead-ends or endless cycles 
the surfer will occasionally jump to a random page with some 
small probability α or when on a page with no out-links if 
averaged over a sufficient number of steps the probability the 
surfer is on page j at some point in time is given by the formula 
∑∈ 
 
− 
 
ji i 
ip 
n 
jp 
b f 
 
 α 
α 
where fi is the set of pages that page i links to and bj is the set of 
pages that link to page j the pagerank score for node j is defined 
as this probability pr j p j because equation is recursive 
it must be iteratively evaluated until p j converges typically the 
initial distribution for p j is uniform the intuition is because a 
random surfer would end up at the page more frequently it is 
likely a better page an alternative view for equation is that 
each page is assigned a quality p j a page gives an equal 
share of its quality to each page it points to 
pagerank is computationally expensive our collection of 
billion pages contains approximately billion links computing 
pagerank requires iterating over these billions of links multiple 
times until convergence it requires large amounts of memory 
 or very smart caching schemes that slow the computation down 
even further and if spread across multiple machines requires 
significant communication between them though much work has 
been done on optimizing the pagerank computation see e g 
 and it remains a relatively slow computationally 
expensive property to compute 
 ranknet 
much work in machine learning has been done on the problems of 
classification and regression let x {xi} be a collection of feature 
vectors typically a feature is any real valued number and 
y {yi} be a collection of associated classes where yi is the class 
of the object described by feature vector xi the classification 
problem is to learn a function f that maps yi f xi for all i when 
yi is real-valued as well this is called regression 
static ranking can be seen as a regression problem if we let xi 
represent features of page i and yi be a value say the rank for 
each page we could learn a regression function that mapped each 
page s features to their rank however this over-constrains the 
problem we wish to solve all we really care about is the order of 
the pages not the actual value assigned to them 
recent work on this ranking problem directly 
attempts to optimize the ordering of the objects rather than the 
value assigned to them for these let z { i j } be a collection of 
pairs of items where item i should be assigned a higher value than 
item j the goal of the ranking problem then is to learn a 
function f such that 
 ji ffji xxz ∈∀ 
 
note that as with learning a regression function the result of this 
process is a function f that maps feature vectors to real values 
this function can still be applied anywhere that a 
regressionlearned function could be applied the only difference is the 
technique used to learn the function by directly optimizing the 
ordering of objects these methods are able to learn a function that 
does a better job of ranking than do regression techniques 
we used ranknet one of the aforementioned techniques for 
learning ranking functions to learn our static rank function 
ranknet is a straightforward modification to the standard neural 
network back-prop algorithm as with back-prop ranknet 
attempts to minimize the value of a cost function by adjusting 
each weight in the network according to the gradient of the cost 
function with respect to that weight the difference is that while a 
typical neural network cost function is based on the difference 
between the network output and the desired output the ranknet 
cost function is based on the difference between a pair of network 
outputs that is for each pair of feature vectors i j in the 
training set ranknet computes the network outputs oi and oj 
since vector i is supposed to be ranked higher than vector j the 
larger is oj-oi the larger the cost 
ranknet also allows the pairs in z to be weighted with a 
confidence posed as the probability that the pair satisfies the 
ordering induced by the ranking function in this paper we used 
a probability of one for all pairs in the next section we will 
discuss the features used in our feature vectors xi 
 features 
to apply ranknet or other machine learning techniques to the 
ranking problem we needed to extract a set of features from each 
page we divided our feature set into four mutually exclusive 
categories page-level page domain-level domain anchor text 
and inlinks anchor and popularity popularity we also 
optionally used the pagerank of a page as a feature below we 
describe each of these feature categories in more detail 
pagerank 
we computed pagerank on a web graph of billion crawled 
pages and billion known urls linked to by these pages 
this represents a significant portion of the web and is 
approximately the same number of pages as are used by 
google yahoo and msn for their search engines 
because pagerank is a graph-based algorithm it is important 
that it be run on as large a subset of the web as possible most 
previous studies on pagerank used subsets of the web that are 
significantly smaller e g the trec vlc corpus used by 
many contains only million pages 
we computed pagerank using the standard value of for α 
popularity 
another feature we used is the actual popularity of a web page 
measured as the number of times that it has been visited by 
users over some period of time we have access to such data 
from users who have installed the msn toolbar and have opted 
to provide it to msn the data is aggregated into a count for 
each web page of the number of users who viewed that page 
though popularity data is generally unavailable there are two 
other sources for it the first is from proxy logs for example a 
university that requires its students to use a proxy has a record 
of all the pages they have visited while on campus 
unfortunately proxy data is quite biased and relatively small 
another source internal to search engines are records of which 
results their users clicked on such data was used by the search 
engine direct hit and has recently been explored for 
dynamic ranking purposes an advantage of the toolbar 
data over this is that it contains information about url visits 
that are not just the result of a search 
the raw popularity is processed into a number of features such 
as the number of times a page was viewed and the number of 
times any page in the domain was viewed more details are 
provided in section 
anchor text and inlinks 
these features are based on the information associated with 
links to the page in question it includes features such as the 
total amount of text in links pointing to the page anchor 
text the number of unique words in that text etc 
page 
this category consists of features which may be determined by 
looking at the page and its url alone we used only eight 
simple features such as the number of words in the body the 
frequency of the most common term etc 
domain 
this category contains features that are computed as averages 
across all pages in the domain for example the average 
number of outlinks on any page and the average pagerank 
many of these features have been used by others for ranking web 
pages particularly the anchor and page features as mentioned 
the evaluation is typically for dynamic ranking and we wish to 
evaluate the use of them for static ranking also to our 
knowledge this is the first study on the use of actual page 
visitation popularity for static ranking the closest similar work is 
on using click-through behavior that is which search engine 
results the users click on to affect dynamic ranking see e g 
 
because we use a wide variety of features to come up with a static 
ranking we refer to this as frank for feature-based ranking 
frank uses ranknet and the set of features described in this 
section to learn a ranking function for web pages unless 
otherwise specified frank was trained with all of the features 
 experiments 
in this section we will demonstrate that we can out perform 
pagerank by applying machine learning to a straightforward set 
of features before the results we first discuss the data the 
performance metric and the training method 
 data 
in order to evaluate the quality of a static ranking we needed a 
gold standard defining the correct ordering for a set of pages 
for this we employed a dataset which contains human judgments 
for queries for each query a number of results are 
manually assigned a rating from to by human judges the 
rating is meant to be a measure of how relevant the result is for 
the query where means poor and means excellent there 
are approximately k judgments in all or an average of 
ratings per query 
the queries are selected by randomly choosing queries from 
among those issued to the msn search engine the probability 
that a query is selected is proportional to its frequency among all 
 
of the queries as a result common queries are more likely to be 
judged than uncommon queries as an example of how diverse 
the queries are the first four queries in the training set are chef 
schools chicagoland speedway eagles fan club and 
turkish culture the documents selected for judging are those 
that we expected would on average be reasonably relevant for 
example the top ten documents returned by msn s search 
engine this provides significantly more information than 
randomly selecting documents on the web the vast majority of 
which would be irrelevant to a given query 
because of this process the judged pages tend to be of higher 
quality than the average page on the web and tend to be pages 
that will be returned for common search queries this bias is good 
when evaluating the quality of static ranking for the purposes of 
index ordering and returning relevant documents this is because 
the most important portion of the index to be well-ordered and 
relevant is the portion that is frequently returned for search 
queries because of this bias however the results in this paper are 
not applicable to crawl prioritization in order to obtain 
experimental results on crawl prioritization we would need 
ratings on a random sample of web pages 
to convert the data from query-dependent to query-independent 
we simply removed the query taking the maximum over 
judgments for a url that appears in more than one query the 
reasoning behind this is that a page that is relevant for some query 
and irrelevant for another is probably a decent page and should 
have a high static rank because we evaluated the pages on 
queries that occur frequently our data indicates the correct index 
ordering and assigns high value to pages that are likely to be 
relevant to a common query 
we randomly assigned queries to a training validation or test set 
such that they contained and of the queries 
respectively each set contains all of the ratings for a given query 
and no query appears in more than one set the training set was 
used to train frank the validation set was used to select the 
model that had the highest performance the test set was used for 
the final results 
this data gives us a query-independent ordering of pages the 
goal for a static ranking algorithm will be to reproduce this 
ordering as closely as possible in the next section we describe 
the measure we used to evaluate this 
 measure 
we chose to use pairwise accuracy to evaluate the quality of a 
static ranking the pairwise accuracy is the fraction of time that 
the ranking algorithm and human judges agree on the ordering of 
a pair of web pages 
if s x is the static ranking assigned to page x and h x is the 
human judgment of relevance for x then consider the following 
sets 
 } { yhxhyx ph and } { ysxsyx ps 
the pairwise accuracy is the portion of hp that is also contained 
in sp 
p 
pp 
h 
sh ∩ 
 accuracypairwise 
this measure was chosen for two reasons first the discrete 
human judgments provide only a partial ordering over web pages 
making it difficult to apply a measure such as the spearman rank 
order correlation coefficient in the pairwise accuracy measure a 
pair of documents with the same human judgment does not affect 
the score second the pairwise accuracy has an intuitive 
meaning it is the fraction of pairs of documents that when the 
humans claim one is better than the other the static rank 
algorithm orders them correctly 
 method 
we trained frank a ranknet based neural network using the 
following parameters we used a fully connected layer network 
the hidden layer had hidden nodes the input weights to this 
layer were all initialized to be zero the output layer just a 
single node weights were initialized using a uniform random 
distribution in the range - we used tanh as the transfer 
function from the inputs to the hidden layer and a linear function 
from the hidden layer to the output the cost function is the 
pairwise cross entropy cost function as discussed in section 
the features in the training set were normalized to have zero mean 
and unit standard deviation the same linear transformation was 
then applied to the features in the validation and test sets 
for training we presented the network with million pairings of 
pages where one page had a higher rating than the other the 
pairings were chosen uniformly at random with replacement 
from all possible pairings when forming the pairs we ignored the 
magnitude of the difference between the ratings the rating spread 
for the two urls hence the weight for each pair was constant 
 one and the probability of a pair being selected was 
independent of its rating spread 
we trained the network for epochs on each epoch the 
training pairs were randomly shuffled the initial training rate was 
 at each epoch we checked the error on the training set if 
the error had increased then we decreased the training rate under 
the hypothesis that the network had probably overshot the 
training rate at each epoch was thus set to 
training rate 
 ε 
κ 
where κ is the initial rate and ε is the number of times 
the training set error has increased after each epoch we 
measured the performance of the neural network on the validation 
set using million pairs chosen randomly with replacement 
the network with the highest pairwise accuracy on the validation 
set was selected and then tested on the test set we report the 
pairwise accuracy on the test set calculated using all possible 
pairs 
these parameters were determined and fixed before the static rank 
experiments in this paper in particular the choice of initial 
training rate number of epochs and training rate decay function 
were taken directly from burges et al 
though we had the option of preprocessing any of the features 
before they were input to the neural network we refrained from 
doing so on most of them the only exception was the popularity 
features as with most web phenomenon we found that the 
distribution of site popularity is zipfian to reduce the dynamic 
range and hopefully make the feature more useful we presented 
the network with both the unpreprocessed as well as the 
logarithm of the popularity features as with the others the 
logarithmic feature values were also normalized to have zero 
mean and unit standard deviation 
 
applying frank to a document is computationally efficient taking 
time that is only linear in the number of input features it is thus 
within a constant factor of other simple machine learning methods 
such as naïve bayes in our experiments computing the frank for 
all five billion web pages was approximately times faster 
than computing the pagerank for the same set 
 results 
as table shows frank significantly outperforms pagerank for 
the purposes of static ranking with a pairwise accuracy of 
frank more than doubles the accuracy of pagerank relative to 
the baseline of which is the accuracy that would be achieved 
by a random ordering of web pages note that one of frank s 
input features is the pagerank of the page so we would expect it 
to perform no worse than pagerank the significant increase in 
accuracy implies that the other features anchor popularity etc 
do in fact contain useful information regarding the overall quality 
of a page 
table basic results 
technique accuracy 
none baseline 
pagerank 
frank 
there are a number of decisions that go into the computation of 
pagerank such as how to deal with pages that have no outlinks 
the choice of α numeric precision convergence threshold etc 
we were able to obtain a computation of pagerank from a 
completely independent implementation provided by marc 
najork that varied somewhat in these parameters it achieved a 
pairwise accuracy of nearly identical to that obtained by 
our implementation we thus concluded that the quality of the 
pagerank is not sensitive to these minor variations in algorithm 
nor was pagerank s low accuracy due to problems with our 
implementation of it 
we also wanted to find how well each feature set performed to 
answer this for each feature set we trained and tested frank 
using only that set of features the results are shown in table 
as can be seen every single feature set individually outperformed 
pagerank on this test perhaps the most interesting result is that 
the page-level features had the highest performance out of all the 
feature sets this is surprising because these are features that do 
not depend on the overall graph structure of the web nor even on 
what pages point to a given page this is contrary to the common 
belief that the web graph structure is the key to finding a good 
static ranking of web pages 
table results for individual feature sets 
feature set accuracy 
pagerank 
popularity 
anchor 
page 
domain 
all features 
because we are using a two-layer neural network the features in 
the learned network can interact with each other in interesting 
nonlinear ways this means that a particular feature that appears 
to have little value in isolation could actually be very important 
when used in combination with other features to measure the 
final contribution of a feature set in the context of all the other 
features we performed an ablation study that is for each set of 
features we trained a network to contain all of the features except 
that set we then compared the performance of the resulting 
network to the performance of the network with all of the features 
table shows the results of this experiment where the decrease 
in accuracy is the difference in pairwise accuracy between the 
network trained with all of the features and the network missing 
the given feature set 
table ablation study shown is the decrease in accuracy 
when we train a network that has all but the given set of 
features the last line is shows the effect of removing the 
anchor pagerank and domain features hence a model 
containing no network or link-based information whatsoever 
feature set decrease in 
accuracy 
pagerank 
popularity 
anchor 
page 
domain 
anchor pagerank domain 
 
 
the results of the ablation study are consistent with the individual 
feature set study both show that the most important feature set is 
the page-level feature set and the second most important is the 
popularity feature set 
finally we wished to see how the performance of frank 
improved as we added features we wanted to find at what point 
adding more feature sets became relatively useless beginning 
with no features we greedily added the feature set that improved 
performance the most the results are shown in table for 
example the fourth line of the table shows that frank using the 
page popularity and anchor features outperformed any network 
that used the page popularity and some other feature set and that 
the performance of this network was 
table frank performance as feature sets are added at each 
row the feature set that gave the greatest increase in accuracy 
was added to the list of features i e we conducted a greedy 
search over feature sets 
feature set accuracy 
none 
 page 
 popularity 
 anchor 
 pagerank 
 domain 
 
finally we present a qualitative comparison of pagerank vs 
frank in table are the top ten urls returned for pagerank and 
for frank pagerank s results are heavily weighted towards 
technology sites it contains two quicktime urls apple s video 
playback software as well as internet explorer and firefox 
urls both of which are web browsers frank on the other 
hand contains more consumer-oriented sites such as american 
express target dell etc pagerank s bias toward technology can 
be explained through two processes first there are many pages 
with buttons at the bottom suggesting that the site is optimized 
for internet explorer or that the visitor needs quicktime these 
generally link back to in these examples the internet explorer 
and quicktime download sites consequently pagerank ranks 
those pages highly though these pages are important they are 
not as important as it may seem by looking at the link structure 
alone one fix for this is to add information about the link to the 
pagerank computation such as the size of the text whether it was 
at the bottom of the page etc 
the other bias comes from the fact that the population of web site 
authors is different than the population of web users web 
authors tend to be technologically-oriented and thus their linking 
behavior reflects those interests frank by knowing the actual 
visitation popularity of a site the popularity feature set is able to 
eliminate some of that bias it has the ability to depend more on 
where actual web users visit rather than where the web site 
authors have linked 
the results confirm that frank outperforms pagerank in pairwise 
accuracy the two most important feature sets are the page and 
popularity features this is surprising as the page features 
consisted only of a few simple features further experiments 
found that of the page features those based on the text of the 
page as opposed to the url performed the best in the next 
section we explore the popularity feature in more detail 
 popularity data 
as mentioned in section our popularity data came from msn 
toolbar users for privacy reasons we had access only to an 
aggregate count of for each url how many times it was visited 
by any toolbar user this limited the possible features we could 
derive from this data for possible extensions see section 
future work 
for each url in our train and test sets we provided a feature to 
frank which was how many times it had been visited by a toolbar 
user however this feature was quite noisy and sparse 
particularly for urls with query parameters e g 
http search msn com results aspx q machine learning form qbhp one 
solution was to provide an additional feature which was the 
number of times any url at the given domain was visited by a 
toolbar user adding this feature dramatically improved the 
performance of frank 
we took this one step further and used the built-in hierarchical 
structure of urls to construct many levels of backoff between the 
full url and the domain we did this by using the set of features 
shown in table 
table url functions used to compute the popularity 
feature set 
function example 
exact url cnn com tech wikipedia html v mobile 
no params cnn com tech wikipedia html 
page wikipedia html 
url- cnn com tech 
url- cnn com 
  
domain cnn com 
domain cnn com 
  
each url was assigned one feature for each function shown in 
the table the value of the feature was the count of the number of 
times a toolbar user visited a url where the function applied to 
that url matches the function applied to the url in question 
for example a user s visit to cnn com sports html would 
increment the domain and domain features for the url 
cnn com tech wikipedia html 
as seen in table adding the domain counts significantly 
improved the quality of the popularity feature and adding the 
numerous backoff functions listed in table improved the 
accuracy even further 
table effect of adding backoff to the popularity feature set 
features accuracy 
url count 
url and domain counts 
all backoff functions table 
table top ten urls for pagerank vs frank 
pagerank frank 
google com google com 
apple com quicktime download yahoo com 
amazon com americanexpress com 
yahoo com hp com 
microsoft com windows ie target com 
apple com quicktime bestbuy com 
mapquest com dell com 
ebay com autotrader com 
mozilla org products firefox dogpile com 
ftc gov bankofamerica com 
 
backing off to subsets of the url is one technique for dealing 
with the sparsity of data it is also informative to see how the 
performance of frank depends on the amount of popularity data 
that we have collected in figure we show the performance of 
frank trained with only the popularity feature set vs the amount 
of data we have for the popularity feature set each day we 
receive additional popularity data and as can be seen in the plot 
this increases the performance of frank the relation is 
logarithmic doubling the amount of popularity data provides a 
constant improvement in pairwise accuracy 
in summary we have found that the popularity features provide a 
useful boost to the overall frank accuracy gathering more 
popularity data as well as employing simple backoff strategies 
improve this boost even further 
 summary of results 
the experiments provide a number of conclusions first frank 
performs significantly better than pagerank even without any 
information about the web graph second the page level and 
popularity features were the most significant contributors to 
pairwise accuracy third by collecting more popularity data we 
can continue to improve frank s performance 
the popularity data provides two benefits to frank first we see 
that qualitatively frank s ordering of web pages has a more 
favorable bias than pagerank s frank s ordering seems to 
correspond to what web users rather than web page authors 
prefer second the popularity data is more timely than 
pagerank s link information the toolbar provides information 
about which web pages people find interesting right now 
whereas links are added to pages more slowly as authors find the 
time and interest 
 related and future work 
 improvements to pagerank 
since the original pagerank paper there has been work on 
improving it much of that work centers on speeding up and 
parallelizing the computation 
one recognized problem with pagerank is that of topic drift a 
page about dogs will have high pagerank if it is linked to by 
many pages that themselves have high rank regardless of their 
topic in contrast a search engine user looking for good pages 
about dogs would likely prefer to find pages that are pointed to by 
many pages that are themselves about dogs hence a link that is 
on topic should have higher weight than a link that is not 
richardson and domingos s query dependent pagerank 
and haveliwala s topic-sensitive pagerank are two 
approaches that tackle this problem 
other variations to pagerank include differently weighting links 
for inter- vs intra-domain links adding a backwards step to the 
random surfer to simulate the back button on most browsers 
 and modifying the jump probability α see langville 
and meyer for a good survey of these and other 
modifications to pagerank 
 other related work 
pagerank is not the only link analysis algorithm used for ranking 
web pages the most well-known other is hits which is 
used by the teoma search engine hits produces a list of 
hubs and authorities where hubs are pages that point to many 
authority pages and authorities are pages that are pointed to by 
many hubs previous work has shown hits to perform 
comparably to pagerank 
one field of interest is that of static index pruning see e g 
carmel et al static index pruning methods reduce the size of 
the search engine s index by removing documents that are 
unlikely to be returned by a search query the pruning is typically 
done based on the frequency of query terms similarly pandey 
and olston suggest crawling pages frequently if they are 
likely to incorrectly appear or not appear as a result of a search 
similar methods could be incorporated into the static rank e g 
how many frequent queries contain words found on this page 
others have investigated the effect that pagerank has on the web 
at large they argue that pages with high pagerank are more 
likely to be found by web users thus more likely to be linked to 
and thus more likely to maintain a higher pagerank than other 
pages the same may occur for the popularity data if we increase 
the ranking for popular pages they are more likely to be clicked 
on thus further increasing their popularity cho et al argue 
that a more appropriate measure of web page quality would 
depend on not only the current link structure of the web but also 
on the change in that link structure the same technique may be 
applicable to popularity data the change in popularity of a page 
may be more informative than the absolute popularity 
one interesting related work is that of ivory and hearst 
their goal was to build a model of web sites that are considered 
high quality from the perspective of content structure and 
navigation visual design functionality interactivity and overall 
experience they used over page level features as well as 
features encompassing the performance and structure of the site 
this let them qualitatively describe the qualities of a page that 
make it appear attractive e g rare use of italics at least point 
font   and in later work to build a system that assists novel 
web page authors in creating quality pages by evaluating it 
according to these features the primary differences between this 
work and ours are the goal discovering what constitutes a good 
web page vs ordering web pages for the purposes of web 
search the size of the study they used a dataset of less than 
pages vs our set of and our comparison with pagerank 
y ln x 
r 
 
 
 
 
 
 
 
 
 
 
days of toolbar data 
pairwiseaccuracy 
figure relation between the amount of popularity data and 
the performance of the popularity feature set note the x-axis 
is a logarithmic scale 
 
nevertheless their work provides insights to additional useful 
static features that we could incorporate into frank in the future 
recent work on incorporating novel features into dynamic ranking 
includes that by joachims et al who investigate the use of 
implicit feedback from users in the form of which search engine 
results are clicked on craswell et al present a method for 
determining the best transformation to apply to query independent 
features such as those used in this paper for the purposes of 
improving dynamic ranking other work such as boyan et al 
and bartell et al apply machine learning for the purposes of 
improving the overall relevance of a search engine i e the 
dynamic ranking they do not apply their techniques to the 
problem of static ranking 
 future work 
there are many ways in which we would like to extend this work 
first frank uses only a small number of features we believe we 
could achieve even more significant results with more features in 
particular the existence or lack thereof of certain words could 
prove very significant for instance under construction 
probably signifies a low quality page other features could 
include the number of images on a page size of those images 
number of layout elements tables divs and spans use of style 
sheets conforming to w c standards like xhtml strict 
background color of a page etc 
many pages are generated dynamically the contents of which may 
depend on parameters in the url the time of day the user 
visiting the site or other variables for such pages it may be 
useful to apply the techniques found in to form a static 
approximation for the purposes of extracting features the 
resulting grammar describing the page could itself be a source of 
additional features describing the complexity of the page such as 
how many non-terminal nodes it has the depth of the grammar 
tree etc 
frank allows one to specify a confidence in each pairing of 
documents in the future we will experiment with probabilities 
that depend on the difference in human judgments between the 
two items in the pair for example a pair of documents where one 
was rated and the other should have a higher confidence than 
a pair of documents rated and 
the experiments in this paper are biased toward pages that have 
higher than average quality also frank with all of the features 
can only be applied to pages that have already been crawled 
thus frank is primarily useful for index ordering and improving 
relevance not for directing the crawl we would like to 
investigate a machine learning approach for crawl prioritization as 
well it may be that a combination of methods is best for 
example using pagerank to select the best billion of the 
billion pages on the web then using frank to order the index and 
affect search relevancy 
another interesting direction for exploration is to incorporate 
frank and page-level features directly into the pagerank 
computation itself work on biasing the pagerank jump vector 
 and transition matrix have demonstrated the feasibility 
and advantages of such an approach there is reason to believe 
that a direct application of using the frank of a page for its 
relevance could lead to an improved overall static rank 
finally the popularity data can be used in other interesting ways 
the general surfing and searching habits of web users varies by 
time of day activity in the morning daytime and evening are 
often quite different e g reading the news solving problems 
and accessing entertainment respectively we can gain insight 
into these differences by using the popularity data divided into 
segments of the day when a query is issued we would then use 
the popularity data matching the time of query in order to do the 
ranking of web pages we also plan to explore popularity features 
that use more than just the counts of how often a page was visited 
for example how long users tended to dwell on a page did they 
leave the page by clicking a link or by hitting the back button etc 
fox et al did a study that showed that features such as this can be 
valuable for the purposes of dynamic ranking finally the 
popularity data could be used as the label rather than as a feature 
using frank in this way to predict the popularity of a page may 
useful for the tasks of relevance efficiency and crawl priority 
there is also significantly more popularity data than human 
labeled data potentially enabling more complex machine learning 
methods and significantly more features 
 conclusions 
a good static ranking is an important component for today s 
search engines and information retrieval systems we have 
demonstrated that pagerank does not provide a very good static 
ranking there are many simple features that individually out 
perform pagerank by combining many static features frank 
achieves a ranking that has a significantly higher pairwise 
accuracy than pagerank alone a qualitative evaluation of the top 
documents shows that frank is less technology-biased than 
pagerank by using popularity data it is biased toward pages that 
web users rather than web authors visit the machine learning 
component of frank gives it the additional benefit of being more 
robust against spammers and allows it to leverage further 
developments in the machine learning community in areas such as 
adversarial classification we have only begun to explore the 
options and believe that significant strides can be made in the 
area of static ranking by further experimentation with additional 
features other machine learning techniques and additional 
sources of data 
 acknowledgments 
thank you to marc najork for providing us with additional 
pagerank computations and to timo burkard for assistance with 
the popularity data many thanks to chris burges for providing 
code and significant support in using training ranknets also we 
thank susan dumais and nick craswell for their edits and 
suggestions 
 references 
 b amento l terveen and w hill does authority mean 
quality predicting expert quality ratings of web documents 
in proceedings of the rd 
annual international acm sigir 
conference on research and development in information 
retrieval 
 b bartell g cottrell and r belew automatic combination 
of multiple ranked retrieval systems in proceedings of the 
 th annual international acm sigir conference on 
research and development in information retrieval 
 p boldi m santini and s vigna pagerank as a function 
of the damping factor in proceedings of the international 
world wide web conference may 
 
 j boyan d freitag and t joachims a machine learning 
architecture for optimizing web search engines in aaai 
workshop on internet based information systems august 
 
 s brin and l page the anatomy of a large-scale 
hypertextual web search engine in proceedings of the 
seventh international wide web conference brisbane 
australia elsevier 
 a broder r lempel f maghoul and j pederson 
efficient pagerank approximation via graph aggregation in 
proceedings of the international world wide web 
conference may 
 c burges t shaked e renshaw a lazier m deeds n 
hamilton g hullender learning to rank using gradient 
descent in proceedings of the nd 
international conference 
on machine learning bonn germany 
 d carmel d cohen r fagin e farchi m herscovici y 
s maarek and a soffer static index pruning for 
information retrieval systems in proceedings of the th 
annual international acm sigir conference on research 
and development in information retrieval pages - 
new orleans louisiana usa september 
 j cho and s roy impact of search engines on page 
popularity in proceedings of the international world wide 
web conference may 
 j cho s roy r adams page quality in search of an 
unbiased web ranking in proceedings of the acm sigmod 
 conference baltimore maryland june 
 n craswell s robertson h zaragoza and m taylor 
relevance weighting for query independent evidence in 
proceedings of the th 
annual conference on research and 
development in information retrieval sigir august 
 
 n dalvi p domingos mausam s sanghai d verma 
adversarial classification in proceedings of the tenth 
international conference on knowledge discovery and data 
mining pp - seattle wa 
 o dekel c manning and y singer log-linear models for 
label-ranking in advances in neural information processing 
systems cambridge ma mit press 
 s fox k s fox k karnawat m mydland s t dumais 
and t white evaluating implicit measures to 
improve the search experiences in the acm transactions on 
information systems pp - april 
 t haveliwala efficient computation of pagerank stanford 
university technical report 
 t haveliwala topic-sensitive pagerank in proceedings of 
the international world wide web conference may 
 d hawking and n craswell very large scale retrieval and 
web search in d harman and e voorhees eds the 
trec book mit press 
 r herbrich t graepel and k obermayer support vector 
learning for ordinal regression in proceedings of the ninth 
international conference on artificial neural networks pp 
 - 
 m ivory and m hearst statistical profiles of highly-rated 
web sites in proceedings of the acm sigchi conference 
on human factors in computing systems 
 t joachims optimizing search engines using clickthrough 
data in proceedings of the acm conference on knowledge 
discovery and data mining kdd 
 t joachims l granka b pang h hembrooke and g 
gay accurately interpreting clickthrough data as implicit 
feedback in proceedings of the conference on research and 
development in information retrieval sigir 
 j kleinberg authoritative sources in a hyperlinked 
environment journal of the acm pp - 
 a langville and c meyer deeper inside pagerank 
internet mathematics - 
 f matthieu and m bouklit the effect of the back button in 
a random walk application for pagerank in alternate track 
papers and posters of the thirteenth international world 
wide web conference 
 f mcsherry a uniform approach to accelerated pagerank 
computation in proceedings of the international world 
wide web conference may 
 y minamide static approximation of dynamically generated 
web pages in proceedings of the international world wide 
web conference may 
 l page s brin r motwani and t winograd the 
pagerank citation ranking bringing order to the web 
technical report stanford university stanford ca 
 s pandey and c olston user-centric web crawling in 
proceedings of the international world wide web 
conference may 
 m richardson and p domingos the intelligent surfer 
probabilistic combination of link and content information in 
pagerank in advances in neural information processing 
systems pp - cambridge ma mit press 
 
 c sherman teoma vs google round available from 
world wide web http dc internet com news article php 
 
 t upstill n craswell and d hawking predicting fame 
and fortune pagerank or indegree in the eighth 
australasian document computing symposium 
 t upstill n craswell and d hawking query-independent 
evidence in home page finding in acm transactions on 
information systems 
 
learning user interaction models 
for predicting web search result preferences 
eugene agichtein 
microsoft research 
eugeneag microsoft com 
eric brill 
microsoft research 
brill microsoft com 
susan dumais 
microsoft research 
sdumais microsoft com 
robert ragno 
microsoft research 
rragno microsoft com 
abstract 
evaluating user preferences of web search results is crucial for 
search engine development deployment and maintenance we 
present a real-world study of modeling the behavior of web search 
users to predict web search result preferences accurate modeling 
and interpretation of user behavior has important applications to 
ranking click spam detection web search personalization and 
other tasks our key insight to improving robustness of 
interpreting implicit feedback is to model query-dependent 
deviations from the expected noisy user behavior we show that 
our model of clickthrough interpretation improves prediction 
accuracy over state-of-the-art clickthrough methods we 
generalize our approach to model user behavior beyond 
clickthrough which results in higher preference prediction 
accuracy than models based on clickthrough information alone 
we report results of a large-scale experimental evaluation that 
show substantial improvements over published implicit feedback 
interpretation methods 
categories and subject descriptors 
h information search and retrieval search process 
relevance feedback 
general terms 
algorithms measurement performance experimentation 
 introduction 
relevance measurement is crucial to web search and to 
information retrieval in general traditionally search relevance is 
measured by using human assessors to judge the relevance of 
query-document pairs however explicit human ratings are 
expensive and difficult to obtain at the same time millions of 
people interact daily with web search engines providing valuable 
implicit feedback through their interactions with the search 
results if we could turn these interactions into relevance 
judgments we could obtain large amounts of data for evaluating 
maintaining and improving information retrieval systems 
recently automatic or implicit relevance feedback has 
developed into an active area of research in the information 
retrieval community at least in part due to an increase in 
available resources and to the rising popularity of web search 
however most traditional ir work was performed over 
controlled test collections and carefully-selected query sets and 
tasks therefore it is not clear whether these techniques will 
work for general real-world web search a significant distinction 
is that web search is not controlled individual users may behave 
irrationally or maliciously or may not even be real users all of 
this affects the data that can be gathered but the amount of the 
user interaction data is orders of magnitude larger than anything 
available in a non-web-search setting by using the aggregated 
behavior of large numbers of users and not treating each user as 
an individual expert we can correct for the noise inherent in 
individual interactions and generate relevance judgments that 
are more accurate than techniques not specifically designed for 
the web search setting 
furthermore observations and insights obtained in laboratory 
settings do not necessarily translate to real world usage hence 
it is preferable to automatically induce feedback interpretation 
strategies from large amounts of user interactions automatically 
learning to interpret user behavior would allow systems to adapt 
to changing conditions changing user behavior patterns and 
different search settings we present techniques to automatically 
interpret the collective behavior of users interacting with a web 
search engine to predict user preferences for search results our 
contributions include 
 a distributional model of user behavior robust to noise 
within individual user sessions that can recover relevance 
preferences from user interactions section 
 extensions of existing clickthrough strategies to include 
richer browsing and interaction features section 
 a thorough evaluation of our user behavior models as well 
as of previously published state-of-the-art techniques over 
a large set of web search sessions sections and 
we discuss our results and outline future directions and 
various applications of this work in section which concludes 
the paper 
 background and related work 
ranking search results is a fundamental problem in 
information retrieval the most common approaches in the 
context of the web use both the similarity of the query to the 
page content and the overall quality of a page a 
state-ofthe-art search engine may use hundreds of features to describe a 
candidate page employing sophisticated algorithms to rank 
pages based on these features current search engines are 
commonly tuned on human relevance judgments human 
annotators rate a set of pages for a query according to perceived 
relevance creating the gold standard against which different 
ranking algorithms can be evaluated reducing the dependence on 
explicit human judgments by using implicit relevance feedback 
has been an active topic of research 
several research groups have evaluated the relationship 
between implicit measures and user interest in these studies 
both reading time and explicit ratings of interest are collected 
morita and shinoda studied the amount of time that users 
spent reading usenet news articles and found that reading time 
could predict a user s interest levels konstan et al showed 
that reading time was a strong predictor of user interest in their 
grouplens system oard and kim studied whether implicit 
feedback could substitute for explicit ratings in recommender 
systems more recently oard and kim presented a 
framework for characterizing observable user behaviors using two 
dimensions-the underlying purpose of the observed behavior and 
the scope of the item being acted upon 
goecks and shavlik approximated human labels by 
collecting a set of page activity measures while users browsed the 
world wide web the authors hypothesized correlations between 
a high degree of page activity and a user s interest while the 
results were promising the sample size was small and the 
implicit measures were not tested against explicit judgments of 
user interest claypool et al studied how several implicit 
measures related to the interests of the user they developed a 
custom browser called the curious browser to gather data in a 
computer lab about implicit interest indicators and to probe for 
explicit judgments of web pages visited claypool et al found 
that the time spent on a page the amount of scrolling on a page 
and the combination of time and scrolling have a strong positive 
relationship with explicit interest while individual scrolling 
methods and mouse-clicks were not correlated with explicit 
interest fox et al explored the relationship between implicit 
and explicit measures in web search they built an instrumented 
browser to collect data and then developed bayesian models to 
relate implicit measures and explicit relevance judgments for both 
individual queries and search sessions they found that 
clickthrough was the most important individual variable but that 
predictive accuracy could be improved by using additional 
variables notably dwell time on a page 
joachims developed valuable insights into the collection of 
implicit measures introducing a technique based entirely on 
clickthrough data to learn ranking functions more recently 
joachims et al presented an empirical evaluation of 
interpreting clickthrough evidence by performing eye tracking 
studies and correlating predictions of their strategies with explicit 
ratings the authors showed that it is possible to accurately 
interpret clickthrough events in a controlled laboratory setting a 
more comprehensive overview of studies of implicit measures is 
described in kelly and teevan 
unfortunately the extent to which existing research applies to 
real-world web search is unclear in this paper we build on 
previous research to develop robust user behavior interpretation 
models for the real web search setting 
 learning user behavior models 
as we noted earlier real web search user behavior can be 
noisy in the sense that user behaviors are only probabilistically 
related to explicit relevance judgments and preferences hence 
instead of treating each user as a reliable expert we aggregate 
information from many unreliable user search session traces our 
main approach is to model user web search behavior as if it were 
generated by two components a relevance component - 
queryspecific behavior influenced by the apparent result relevance and 
a background component - users clicking indiscriminately 
our general idea is to model the deviations from the expected 
user behavior hence in addition to basic features which we 
will describe in detail in section we compute derived 
features that measure the deviation of the observed feature value 
for a given search result from the expected values for a result 
with no query-dependent information we motivate our 
intuitions with a particularly important behavior feature result 
clickthrough analyzed next and then introduce our general 
model of user behavior that incorporates other user actions 
 section 
 a case study in click distributions 
as we discussed we aggregate statistics across many user 
sessions a click on a result may mean that some user found the 
result summary promising it could also be caused by people 
clicking indiscriminately in general individual user behavior 
clickthrough and otherwise is noisy and cannot be relied upon 
for accurate relevance judgments the data set is described in 
more detail in section for the present it suffices to note that 
we focus on a random sample of queries that were 
randomly sampled from query logs for these queries we 
aggregate click data over more than searches performed 
over a three week period we also have explicit relevance 
judgments for the top results for each query 
figure shows the relative clickthrough frequency as a 
function of result position the aggregated click frequency at 
result position p is calculated by first computing the frequency of 
a click at p for each query i e approximating the probability 
that a randomly chosen click for that query would land on 
position p these frequencies are then averaged across queries 
and normalized so that relative frequency of a click at the top 
position is the resulting distribution agrees with previous 
observations that users click more often on top-ranked results 
this reflects the fact that search engines do a reasonable job of 
ranking results as well as biases to click top results and 
noisewe attempt to separate these components in the analysis that 
follows 
 
 
 
 
 
 
 
 
 
 
 
 
result position 
relativeclickfrequency 
figure relative click frequency for top result 
positions over queries and searches 
first we consider the distribution of clicks for the relevant 
documents for these queries figure reports the aggregated 
click distribution for queries with varying position of top 
relevant document ptr while there are many clicks above 
the first relevant document for each distribution there are 
clearly peaks in click frequency for the first relevant result 
for example for queries with top relevant result in position 
the relative click frequency at that position second bar is higher 
than the click frequency at other positions for these queries 
nevertheless many users still click on the non-relevant results 
in position for such queries this shows a stronger property of 
the bias in the click distribution towards top results - users click 
more often on results that are ranked higher even when they are 
not relevant 
 
 
 
 
 
 
 
 
 
 
 
 
result position 
relativeclickfrequency 
ptr 
ptr 
ptr 
ptr 
ptr 
background 
figure relative click frequency for queries with varying 
ptr position of top relevant document 
- 
- 
- 
 
 
 
 
 
 
 
 
 
 
result position 
correctedrelativeclickfrequency 
ptr 
ptr 
ptr 
ptr 
ptr 
figure relative corrected click frequency for relevant 
documents with varying ptr position of top relevant 
if we subtract the background distribution of figure from the 
mixed distribution of figure we obtain the distribution in 
figure where the remaining click frequency distribution can 
be interpreted as the relevance component of the results note that 
the corrected click distribution correlates closely with actual 
result relevance as explicitly rated by human judges 
 robust user behavior model 
clicks on search results comprise only a small fraction of the 
post-search activities typically performed by users we now 
introduce our techniques for going beyond the clickthrough 
statistics and explicitly modeling post-search user behavior 
although clickthrough distributions are heavily biased towards 
top results we have just shown how the  relevance-driven click 
distribution can be recovered by correcting for the prior 
background distribution we conjecture that other aspects of user 
behavior e g page dwell time are similarly distorted our 
general model includes two feature types for describing user 
behavior direct and deviational where the former is the directly 
measured values and latter is deviation from the expected values 
estimated from the overall query-independent distributions for 
the corresponding directly observed features 
more formally we postulate that the observed value o of a 
feature f for a query q and result r can be expressed as a mixture 
of two components 
 frqrelfcfrqo 
where fc is the prior background distribution for values of f 
aggregated across all queries and rel q r f is the component of 
the behavior influenced by the relevance of the result r as 
illustrated above with the clickthrough feature if we subtract the 
background distribution i e the expected clickthrough for a 
result at a given position from the observed clickthrough 
frequency at a given position we can approximate the relevance 
component of the clickthrough value 
 in order to reduce the 
effect of individual user variations in behavior we average 
observed feature values across all users and search sessions for 
each query-url pair this aggregation gives additional 
robustness of not relying on individual noisy user interactions 
in summary the user behavior for a query-url pair is 
represented by a feature vector that includes both the directly 
observed features and the derived corrected feature values 
we now describe the actual features we use to represent user 
behavior 
 features for representing user behavior 
our goal is to devise a sufficiently rich set of features that 
allow us to characterize when a user will be satisfied with a web 
search result once the user has submitted a query they perform 
many different actions reading snippets clicking results 
navigating refining their query which we capture and 
summarize this information was obtained via opt-in client-side 
instrumentation from users of a major web search engine 
this rich representation of user behavior is similar in many 
respects to the recent work by fox et al an important 
difference is that many of our features are by design query 
specific whereas theirs was by design a general 
queryindependent model of user behavior furthermore we include 
derived distributional features computed as described above 
the features we use to represent user search interactions are 
summarized in table for clarity we organize the features 
into the groups query-text clickthrough and browsing 
query-text features users decide which results to examine in 
more detail by looking at the result title url and summary - in 
some cases looking at the original document is not even 
necessary to model this aspect of user experience we defined 
features to characterize the nature of the query and its relation to 
the snippet text these include features such as overlap between 
the words in title and in query titleoverlap the fraction of 
words shared by the query and the result summary 
 summaryoverlap etc 
browsing features simple aspects of the user web page 
interactions can be captured and quantified these features are 
used to characterize interactions with pages beyond the results 
page for example we compute how long users dwell on a page 
 timeonpage or domain timeondomain and the deviation 
of dwell time from expected page dwell time for a query these 
features allows us to model intra-query diversity of page 
browsing behavior e g navigational queries on average are 
likely to have shorter page dwell time than transactional or 
informational queries we include both the direct features and 
the derived features described above 
clickthrough features clicks are a special case of user 
interaction with the search engine we include all the features 
necessary to learn the clickthrough-based strategies described 
in sections and for example for a query-url pair we 
provide the number of clicks for the result clickfrequency as 
 
of course this is just a rough estimate as the observed 
background distribution also includes the relevance 
component 
well as whether there was a click on result below or above the 
current url isclickbelow isclickabove the derived feature 
values such as clickrelativefrequency and clickdeviation are 
computed as described in equation 
query-text features 
titleoverlap fraction of shared words between query and title 
summaryoverlap fraction of shared words between query and summary 
queryurloverlap fraction of shared words between query and url 
querydomainoverlap fraction of shared words between query and domain 
querylength number of tokens in query 
querynextoverlap average fraction of words shared with next query 
browsing features 
timeonpage page dwell time 
cumulativetimeonpage cumulative time for all subsequent pages after search 
timeondomain cumulative dwell time for this domain 
timeonshorturl cumulative time on url prefix dropping parameters 
isfollowedlink if followed link to result otherwise 
isexacturlmatch if aggressive normalization used otherwise 
isredirected if initial url same as final url otherwise 
ispathfromsearch if only followed links after query otherwise 
clicksfromsearch number of hops to reach page from query 
averagedwelltime average time on page for this query 
dwelltimedeviation deviation from overall average dwell time on page 
cumulativedeviation deviation from average cumulative time on page 
domaindeviation deviation from average time on domain 
shorturldeviation deviation from average time on short url 
clickthrough features 
position position of the url in current ranking 
clickfrequency number of clicks for this query url pair 
clickrelativefrequency relative frequency of a click for this query and url 
clickdeviation deviation from expected click frequency 
isnextclicked if there is a click on next position otherwise 
ispreviousclicked if there is a click on previous position otherwise 
isclickabove if there is a click above otherwise 
isclickbelow if there is click below otherwise 
table features used to represent post-search interactions 
for a given query and search result url 
 learning a predictive behavior model 
having described our features we now turn to the actual 
method of mapping the features to user preferences we attempt 
to learn a general implicit feedback interpretation strategy 
automatically instead of relying on heuristics or insights we 
consider this approach to be preferable to heuristic strategies 
because we can always mine more data instead of relying only 
on our intuition and limited laboratory evidence our general 
approach is to train a classifier to induce weights for the user 
behavior features and consequently derive a predictive model of 
user preferences the training is done by comparing a wide range 
of implicit behavior measures with explicit user judgments for a 
set of queries 
for this we use a large random sample of queries in the search 
query log of a popular web search engine the sets of results 
 identified by urls returned for each of the queries and any 
explicit relevance judgments available for each query result pair 
we can then analyze the user behavior for all the instances where 
these queries were submitted to the search engine 
to learn the mapping from features to relevance preferences 
we use a scalable implementation of neural networks ranknet 
 capable of learning to rank a set of given items more 
specifically for each judged query we check if a result link has 
been judged if so the label is assigned to the query url pair and 
to the corresponding feature vector for that search result these 
vectors of feature values corresponding to urls judged relevant 
or non-relevant by human annotators become our training set 
ranknet has demonstrated excellent performance in learning to 
rank objects in a supervised setting hence we use ranknet for 
our experiments 
 predicting user preferences 
in our experiments we explore several models for predicting 
user preferences these models range from using no implicit 
user feedback to using all available implicit user feedback 
ranking search results to predict user preferences is a 
fundamental problem in information retrieval most traditional 
ir and web search approaches use a combination of page and 
link features to rank search results and a representative 
state-ofthe-art ranking system will be used as our baseline ranker 
 section at the same time user interactions with a search 
engine provide a wealth of information a commonly considered 
type of interaction is user clicks on search results previous work 
 as described above also examined which results were 
skipped e g  skip above and  skip next and other related 
strategies to induce preference judgments from the users 
skipping over results and not clicking on following results we 
have also added refinements of these strategies to take into 
account the variability observed in realistic web scenarios we 
describe these strategies in section 
as clickthroughs are just one aspect of user interaction we 
extend the relevance estimation by introducing a machine 
learning model that incorporates clicks as well as other aspects 
of user behavior such as follow-up queries and page dwell time 
 section we conclude this section by briefly describing our 
baseline - a state-of-the-art ranking algorithm used by an 
operational web search engine 
 baseline model 
a key question is whether browsing behavior can provide 
information absent from existing explicit judgments used to train 
an existing ranker for our baseline system we use a 
state-of-theart page ranking system currently used by a major web search 
engine hence we will call this system current for the 
subsequent discussion while the specific algorithms used by the 
search engine are beyond the scope of this paper the algorithm 
ranks results based on hundreds of features such as query to 
document similarity query to anchor text similarity and 
intrinsic page quality the current web search engine rankings 
provide a strong system for comparison and experiments of the 
next two sections 
 clickthrough model 
if we assume that every user click was motivated by a rational 
process that selected the most promising result summary we can 
then interpret each click as described in joachims et al by 
studying eye tracking and comparing clicks with explicit 
judgments they identified a few basic strategies we discuss the 
two strategies that performed best in their experiments skip 
above and skip next 
strategy sa skip above for a set of results for a query 
and a clicked result at position p all unclicked results 
ranked above p are predicted to be less relevant than the 
result at p 
in addition to information about results above the clicked 
result we also have information about the result immediately 
following the clicked one eye tracking study performed by 
joachims et al showed that users usually consider the result 
immediately following the clicked result in current ranking their 
skip next strategy uses this observation to predict that a result 
following the clicked result at p is less relevant than the clicked 
result with accuracy comparable to the sa strategy above for 
better coverage we combine the sa strategy with this extension to 
derive the skip above skip next strategy 
strategy sa n skip above skip next this strategy 
predicts all un-clicked results immediately following a 
clicked result as less relevant than the clicked result and 
combines these predictions with those of the sa strategy 
above 
we experimented with variations of these strategies and found 
that sa n outperformed both sa and the original skip next 
strategy so we will consider the sa and sa n strategies in the 
rest of the paper these strategies are motivated and empirically 
tested for individual users in a laboratory setting as we will 
show these strategies do not work as well in real web search 
setting due to inherent inconsistency and noisiness of individual 
users behavior 
the general approach for using our clickthrough models 
directly is to filter clicks to those that reflect higher-than-chance 
click frequency we then use the same sa and sa n strategies 
but only for clicks that have higher-than-expected frequency 
according to our model for this we estimate the relevance 
component rel q r f of the observed clickthrough feature f as the 
deviation from the expected background clickthrough 
distribution fc 
strategy cd deviation d for a given query compute the 
observed click frequency distribution o r p for all results r 
in positions p the click deviation for a result r in position p 
dev r p is computed as 
 pcproprdev − 
where c p is the expected clickthrough at position p if 
dev r p d retain the click as input to the sa n strategy 
above and apply sa n strategy over the filtered set of click 
events 
the choice of d selects the tradeoff between recall and 
precision while the above strategy extends sa and sa n it still 
assumes that a filtered clicked result is preferred over all 
unclicked results presented to the user above a clicked position 
however for informational queries multiple results may be 
clicked with varying frequency hence it is preferable to 
individually compare results for a query by considering the 
difference between the estimated relevance components of the 
click distribution of the corresponding query results we now 
define a generalization of the previous clickthrough interpretation 
strategy 
strategy cdiff margin m compute deviation dev r p for 
each result r rn in position p for each pair of results ri and 
rj predict preference of ri over rj iff dev ri pi -dev ri pj m 
as in cd the choice of m selects the tradeoff between recall 
and precision the pairs may be preferred in the original order or 
in reverse of it given the margin two results might be effectively 
indistinguishable but only one can possibly be preferred over the 
other intuitively cdiff generalizes the skip idea above to include 
cases where the user skipped i e clicked less than expected 
on uj and preferred i e clicked more than expected on ui 
furthermore this strategy allows for differentiation within the set 
of clicked results making it more appropriate to noisy user 
behavior 
cdiff and cd are complimentary cdiff is a generalization of 
the clickthrough frequency model of cd but it ignores the 
positional information used in cd hence combining the two 
strategies to improve coverage is a natural approach 
strategy cd cdiff deviation d margin m union 
of cd and cdiff predictions 
other variations of the above strategies were considered but 
these five methods cover the range of observed performance 
 general user behavior model 
the strategies described in the previous section generate 
orderings based solely on observed clickthrough frequencies as 
we discussed clickthrough is just one albeit important aspect 
of user interactions with web search engine results we now 
present our general strategy that relies on the automatically 
derived predictive user behavior models section 
the userbehavior strategy for a given query each 
result is represented with the features in table 
relative user preferences are then estimated using the 
learned user behavior model described in section 
recall that to learn a predictive behavior model we used the 
features from table along with explicit relevance judgments 
as input to ranknet which learns an optimal weighting of 
features to predict preferences 
this strategy models user interaction with the search engine 
allowing it to benefit from the wisdom of crowds interacting 
with the results and the pages beyond as our experiments in the 
subsequent sections demonstrate modeling a richer set of user 
interactions beyond clickthroughs results in more accurate 
predictions of user preferences 
 experimental setup 
we now describe our experimental setup we first describe 
the methodology used including our evaluation metrics section 
 then we describe the datasets section and the 
methods we compared in this study section 
 evaluation methodology and metrics 
our evaluation focuses on the pairwise agreement between 
preferences for results this allows us to compare to previous 
work furthermore for many applications such as tuning 
ranking functions pairwise preference can be used directly for 
training the evaluation is based on comparing 
preferences predicted by various models to the correct 
preferences derived from the explicit user relevance judgments 
we discuss other applications of our models beyond web search 
ranking in section 
to create our set of test pairs we take each query and 
compute the cross-product between all search results returning 
preferences for pairs according to the order of the associated 
relevance labels to avoid ambiguity in evaluation we discard 
all ties i e pairs with equal label 
in order to compute the accuracy of our preference predictions 
with respect to the correct preferences we adapt the standard 
recall and precision measures while our task of computing 
pairwise agreement is different from the absolute relevance 
ranking task the metrics are used in the similar way 
specifically we report the average query recall and precision 
for our task query precision and query recall for a query q are 
defined as 
 query precision fraction of predicted preferences for results 
for q that agree with preferences obtained from explicit 
human judgment 
 query recall fraction of preferences obtained from explicit 
human judgment for q that were correctly predicted 
the overall recall and precision are computed as the average of 
query recall and query precision respectively a drawback of 
this evaluation measure is that some preferences may be more 
valuable than others which pairwise agreement does not capture 
we discuss this issue further when we consider extensions to the 
current work in section 
 datasets 
for evaluation we used queries that were randomly 
sampled from query logs for a major web search engine for each 
query the top returned search results were manually rated on a 
 -point scale by trained judges as part of ongoing relevance 
improvement effort in addition for these queries we also had user 
interaction data for more than instances of these queries 
the user interactions were harvested from anonymous 
browsing traces that immediately followed a query submitted to 
the web search engine this data collection was part of voluntary 
opt-in feedback submitted by users from october through 
october these three weeks days of user interaction data 
was filtered to include only the users in the english-u s market 
in order to better understand the effect of the amount of user 
interaction data available for a query on accuracy we created 
subsets of our data q q and q that contain different 
amounts of interaction data 
 q human-rated queries with at least click on results 
recorded queries query-url pairs 
 q queries in q with at least clicks queries 
 query-url pairs 
 q queries in q with at least clicks queries total 
 query-url pairs 
these datasets were collected as part of normal user experience 
and hence have different characteristics than previously reported 
datasets collected in laboratory settings furthermore the data 
size is order of magnitude larger than any study reported in the 
literature 
 methods compared 
we considered a number of methods for comparison we 
compared our userbehavior model section to previously 
published implicit feedback interpretation techniques and some 
variants of these approaches section and to the current 
search engine ranking based on query and page features alone 
 section specifically we compare the following strategies 
 sa the skip above clickthrough strategy section 
 sa n a more comprehensive extension of sa that takes 
better advantage of current search engine ranking 
 cd our refinement of sa n that takes advantage of our 
mixture model of clickthrough distribution to select trusted 
clicks for interpretation section 
 cdiff our generalization of the cd strategy that explicitly 
uses the relevance component of clickthrough probabilities to 
induce preferences between search results section 
 cd cdiff the strategy combining cd and cdiff as the 
union of predicted preferences from both section 
 userbehavior we order predictions based on decreasing 
highest score of any page in our preliminary experiments 
we observed that higher ranker scores indicate higher 
confidence in the predictions this heuristic allows us to 
do graceful recall-precision tradeoff using the score of the 
highest ranked result to threshold the queries section 
 current current search engine ranking section note 
that the current ranker implementation was trained over a 
superset of the rated query url pairs in our datasets but 
using the same truth labels as we do for our evaluation 
training test split the only strategy for which splitting the 
datasets into training and test was required was the 
userbehavior method to evaluate userbehavior we train and 
validate on of labeled queries and test on the remaining 
 the sampling was done per query i e all results for a 
chosen query were included in the respective dataset and there 
was no overlap in queries between training and test sets 
it is worth noting that both the ad-hoc sa and sa n as well 
as the distribution-based strategies cd cdiff and cd cdiff 
do not require a separate training and test set since they are 
based on heuristics for detecting anomalous click frequencies 
for results hence all strategies except for userbehavior were 
tested on the full set of queries and associated relevance 
preferences while userbehavior was tested on a randomly 
chosen hold-out subset of the queries as described above to 
make sure we are not favoring userbehavior we also tested all 
other strategies on the same hold-out test sets resulting in the 
same accuracy results as testing over the complete datasets 
 results 
we now turn to experimental evaluation of predicting 
relevance preference of web search results figure shows the 
recall-precision results over the q query set section the 
results indicate that previous click interpretation strategies sa 
and sa n perform suboptimally in this setting exhibiting 
precision and respectively furthermore there is no 
mechanism to do recall-precision trade-off with sa and sa n 
as they do not provide prediction confidence in contrast our 
clickthrough distribution-based techniques cd and cd cdiff 
exhibit somewhat higher precision than sa and sa n 
and at recall of maximum achieved by sa or 
sa n 
sa n 
sa 
 
 
 
 
 
 
 
 
 
 
 
 
recall 
precision 
sa sa n 
cd cdiff 
cd cdiff userbehavior 
current 
figure precision vs recall of sa sa n cd cdiff 
cd cdiff userbehavior and current relevance prediction 
methods over the q dataset 
interestingly cdiff alone exhibits precision equal to sa 
 at the same recall at in contrast by combining cd 
and cdiff strategies cd cdiff method we achieve the best 
performance of all clickthrough-based strategies exhibiting 
precision of above for recall values up to and higher at 
lower recall levels clearly aggregating and intelligently 
interpreting clickthroughs results in significant gain for realistic 
web search than previously described strategies however even 
the cd cdiff clickthrough interpretation strategy can be 
improved upon by automatically learning to interpret the 
aggregated clickthrough evidence 
but first we consider the best performing strategy 
userbehavior incorporating post-search navigation history in 
addition to clickthroughs browsing features results in the 
highest recall and precision among all methods compared browse 
exhibits precision of above at recall of significantly 
outperforming our baseline and clickthrough-only strategies 
furthermore browse is able to achieve high recall as high as 
 while maintaining precision significantly higher than 
the baseline ranking 
to further analyze the value of different dimensions of implicit 
feedback modeled by the userbehavior strategy we consider each 
group of features in isolation figure reports precision vs 
recall for each feature group interestingly query-text alone has 
low accuracy only marginally better than random furthermore 
browsing features alone have higher precision with lower 
maximum recall achieved than considering all of the features in 
our userbehavior model applying different machine learning 
methods for combining classifier predictions may increase 
performance of using all features for all recall values 
 
 
 
 
 
 
 
 
recall 
precision 
all features 
clickthrough 
query-text 
browsing 
figure precision vs recall for predicting relevance with 
each group of features individually 
 
 
 
 
 
 
 
 
 
 
 
 
recall 
precision 
cd cdiff q userbehavior q 
cd cdiff q userbehavior q 
cd cdiff q userbehavior q 
figure recall vs precision of cd cdiff and 
userbehavior for query sets q q and q queries with 
at least at least and at least clicks respectively 
interestingly the ranker trained over clickthrough-only 
features achieves substantially higher recall and precision than 
human-designed clickthrough-interpretation strategies described 
earlier for example the clickthrough-trained classifier achieves 
 precision at recall vs the maximum recall of 
achieved by the cd cdiff strategy 
our clickthrough and user behavior interpretation strategies 
rely on extensive user interaction data we consider the effects 
of having sufficient interaction data available for a query before 
proposing a re-ranking of results for that query figure 
reports recall-precision curves for the cd cdiff and 
userbehavior methods for different test query sets with at least 
 click q clicks q and clicks q available per 
query not surprisingly cd cdiff improves with more clicks 
this indicates that accuracy will improve as more user 
interaction histories become available and more queries from 
the q set will have comprehensive interaction histories 
similarly the userbehavior strategy performs better for queries 
with and clicks although the improvement is less dramatic 
than for cd cdiff for queries with sufficient clicks cd cdiff 
exhibits precision comparable with browse at lower recall 
 
 
 
 
 
 
days of user interaction data harvested 
recall 
cd cdiff 
userbehavior 
figure recall of cd cdiff and userbehavior strategies 
at fixed minimum precision for varying amounts of user 
activity data days 
our techniques often do not make relevance predictions for 
search results i e if no interaction data is available for the 
lower-ranked results consequently maintaining higher precision 
at the expense of recall in contrast the current search engine 
always makes a prediction for every result for a given query as 
a consequence the recall of current is high at the 
expense of lower precision as another dimension of acquiring 
training data we consider the learning curve with respect to 
amount days of training data available figure reports the 
recall of cd cdiff and userbehavior strategies for varying 
amounts of training data collected over time we fixed minimum 
precision for both strategies at as a point substantially higher 
than the baseline as expected recall of both strategies 
improves quickly with more days of interaction data examined 
we now briefly summarize our experimental results we 
showed that by intelligently aggregating user clickthroughs 
across queries and users we can achieve higher accuracy on 
predicting user preferences because of the skewed distribution 
of user clicks our clickthrough-only strategies have high 
precision but low recall i e do not attempt to predict relevance 
of many search results nevertheless our cd cdiff 
clickthrough strategy outperforms most recent state-of-the-art 
results by a large margin precision for cd cdiff vs 
for sa n at the highest recall level of sa n 
furthermore by considering the comprehensive userbehavior 
features that model user interactions after the search and beyond 
the initial click we can achieve substantially higher precision 
and recall than considering clickthrough alone our 
userbehavior strategy achieves recall of over with precision 
of over with much higher precision at lower recall levels 
substantially outperforms the current search engine preference 
ranking and all other implicit feedback interpretation methods 
 conclusions and future work 
our paper is the first to our knowledge to interpret 
postsearch user behavior to estimate user preferences in a real web 
search setting we showed that our robust models result in higher 
prediction accuracy than previously published techniques 
we introduced new robust probabilistic techniques for 
interpreting clickthrough evidence by aggregating across users 
and queries our methods result in clickthrough interpretation 
substantially more accurate than previously published results not 
specifically designed for web search scenarios our methods 
predictions of relevance preferences are substantially more 
accurate than the current state-of-the-art search result ranking that 
does not consider user interactions we also presented a general 
model for interpreting post-search user behavior that incorporates 
clickthrough browsing and query features by considering the 
complete search experience after the initial query and click we 
demonstrated prediction accuracy far exceeding that of 
interpreting only the limited clickthrough information 
furthermore we showed that automatically learning to 
interpret user behavior results in substantially better performance 
than the human-designed ad-hoc clickthrough interpretation 
strategies another benefit of automatically learning to interpret 
user behavior is that such methods can adapt to changing 
conditions and changing user profiles for example the user 
behavior model on intranet search may be different from the web 
search behavior our general userbehavior method would be able 
to adapt to these changes by automatically learning to map new 
behavior patterns to explicit relevance ratings 
a natural application of our preference prediction models is to 
improve web search ranking in addition our work has many 
potential applications including click spam detection search 
abuse detection personalization and domain-specific ranking for 
example our automatically derived behavior models could be 
trained on examples of search abuse or click spam behavior 
instead of relevance labels alternatively our models could be 
used directly to detect anomalies in user behavior - either due to 
abuse or to operational problems with the search engine 
while our techniques perform well on average our 
assumptions about clickthrough distributions and learning the 
user behavior models may not hold equally well for all queries 
for example queries with divergent access patterns e g for 
ambiguous queries with multiple meanings may result in 
behavior inconsistent with the model learned for all queries 
hence clustering queries and learning different predictive models 
for each query type is a promising research direction query 
distributions also change over time and it would be productive to 
investigate how that affects the predictive ability of these models 
furthermore some predicted preferences may be more valuable 
than others and we plan to investigate different metrics to capture 
the utility of the predicted preferences 
as we showed in this paper using the wisdom of crowds can 
give us accurate interpretation of user interactions even in the 
inherently noisy web search setting our techniques allow us to 
automatically predict relevance preferences for web search results 
with accuracy greater than the previously published methods the 
predicted relevance preferences can be used for automatic 
relevance evaluation and tuning for deploying search in new 
settings and ultimately for improving the overall web search 
experience 
 references 
 e agichtein e brill and s dumais improving web search ranking 
by incorporating user behavior in proceedings of the acm 
conference on research and development on information retrieval 
 sigir 
 j allan hard track overview in trec high accuracy 
retrieval from documents in proceedings of trec - 
 
 s brin and l page the anatomy of a large-scale hypertextual web 
search engine in proceedings of www - 
 c j c burges t shaked e renshaw a lazier m deeds n 
hamilton and g hullender learning to rank using gradient 
descent in proceedings of the international conference on machine 
learning icml 
 d m chickering the winmine toolkit microsoft technical report 
msr-tr- - 
 m claypool d brown p lee and m waseda inferring user interest 
in ieee internet computing 
 s fox k karnawat m mydland s t dumais and t white 
evaluating implicit measures to improve the search experience in 
acm transactions on information systems 
 j goecks and j shavlick learning users interests by unobtrusively 
observing their normal behavior in proceedings of the ijcai 
workshop on machine learning for information filtering 
 t joachims optimizing search engines using clickthrough data in 
proceedings of the acm conference on knowledge discovery and 
datamining sigkdd 
 t joachims l granka b pang h hembrooke and g gay 
accurately interpreting clickthrough data as implicit feedback in 
proceedings of the acm conference on research and development 
on information retrieval sigir 
 t joachims making large-scale svm learning practical advances 
in kernel methods in support vector learning mit press 
 d kelly and j teevan implicit feedback for inferring user preference 
a bibliography in sigir forum 
 j konstan b miller d maltz j herlocker l gordon and j riedl 
grouplens applying collaborative filtering to usenet news in 
communications of acm 
 m morita and y shinoda information filtering based on user 
behavior analysis and best match text retrieval in proceedings of the 
acm conference on research and development on information 
retrieval sigir 
 d oard and j kim implicit feedback for recommender systems in 
proceedings of aaai workshop on recommender systems 
 d oard and j kim modeling information content using observable 
behavior in proceedings of the th annual meeting of the 
american society for information science and technology 
 p pirolli the use of proximal information scent to forage for distal 
content on the world wide web in working with technology in 
mind brunswikian resources for cognitive science and 
engineering oxford university press 
 f radlinski and t joachims query chains learning to rank from 
implicit feedback in proceedings of the acm conference on 
knowledge discovery and data mining kdd acm 
 f radlinski and t joachims evaluating the robustness of learning 
from implicit feedback in the icml workshop on learning in web 
search 
 g salton and m mcgill introduction to modern information 
retrieval mcgraw-hill 
 e m voorhees d harman overview of trec 
a frequency-based and a poisson-based definition of the 
probability of being informative 
thomas roelleke 
department of computer science 
queen mary university of london 
thor dcs qmul ac uk 
abstract 
this paper reports on theoretical investigations about the 
assumptions underlying the inverse document frequency idf 
we show that an intuitive idf -based probability function for 
the probability of a term being informative assumes disjoint 
document events by assuming documents to be 
independent rather than disjoint we arrive at a poisson-based 
probability of being informative the framework is useful for 
understanding and deciding the parameter estimation and 
combination in probabilistic retrieval models 
categories and subject descriptors 
h information search and retrieval retrieval 
models 
general terms 
theory 
 introduction and background 
the inverse document frequency idf is one of the most 
successful parameters for a relevance-based ranking of 
retrieved objects with n being the total number of 
documents and n t being the number of documents in which 
term t occurs the idf is defined as follows 
idf t − log 
n t 
n 
 idf t ∞ 
ranking based on the sum of the idf -values of the query 
terms that occur in the retrieved documents works well this 
has been shown in numerous applications also it is well 
known that the combination of a document-specific term 
weight and idf works better than idf alone this approach 
is known as tf-idf where tf t d tf t d is 
the so-called term frequency of term t in document d the 
idf reflects the discriminating power informativeness of a 
term whereas the tf reflects the occurrence of a term 
the idf alone works better than the tf alone does an 
explanation might be the problem of tf with terms that occur 
in many documents let us refer to those terms as noisy 
terms we use the notion of noisy terms rather than 
frequent terms since frequent terms leaves open whether we 
refer to the document frequency of a term in a collection or 
to the so-called term frequency also referred to as 
withindocument frequency of a term in a document we 
associate noise with the document frequency of a term in a 
collection and we associate occurrence with the 
withindocument frequency of a term the tf of a noisy term might 
be high in a document but noisy terms are not good 
candidates for representing a document therefore the removal 
of noisy terms known as stopword removal is essential 
when applying tf in a tf-idf approach the removal of 
stopwords is conceptually obsolete if stopwords are just words 
with a low idf 
from a probabilistic point of view tf is a value with a 
frequency-based probabilistic interpretation whereas idf has 
an informative rather than a probabilistic interpretation 
the missing probabilistic interpretation of idf is a problem 
in probabilistic retrieval models where we combine uncertain 
knowledge of different dimensions e g informativeness of 
terms structure of documents quality of documents age 
of documents etc such that a good estimate of the 
probability of relevance is achieved an intuitive solution is a 
normalisation of idf such that we obtain values in the 
interval for example consider a normalisation based on 
the maximal idf -value let t be the set of terms occurring 
in a collection 
pfreq t is informative 
idf t 
maxidf 
maxidf max {idf t t ∈ t} maxidf − log n 
minidf min {idf t t ∈ t} minidf 
minidf 
maxidf 
≤ pfreq t is informative ≤ 
this frequency-based probability function covers the interval 
 if the minimal idf is equal to zero which is the case 
if we have at least one term that occurs in all documents 
can we interpret pfreq the normalised idf as the probability 
that the term is informative 
when investigating the probabilistic interpretation of the 
 
normalised idf we made several observations related to 
disjointness and independence of document events these 
observations are reported in section we show in section 
that the frequency-based noise probability n t 
n 
used in the 
classic idf -definition can be explained by three assumptions 
binary term occurrence constant document containment and 
disjointness of document containment events in section 
we show that by assuming independence of documents we 
obtain − e− 
≈ − as the upper bound of the noise 
probability of a term the value e− 
is related to the 
logarithm and we investigate in section the link to 
information theory in section we link the results of the previous 
sections to probability theory we show the steps from 
possible worlds to binomial distribution and poisson distribution 
in section we emphasise that the theoretical framework 
of this paper is applicable for both idf and tf finally in 
section we base the definition of the probability of 
being informative on the results of the previous sections and 
compare frequency-based and poisson-based definitions 
 background 
the relationship between frequencies probabilities and 
information theory entropy has been the focus of many 
researchers in this background section we focus on work 
that investigates the application of the poisson distribution 
in ir since a main part of the work presented in this paper 
addresses the underlying assumptions of poisson 
 proposes a -poisson model that takes into account 
the different nature of relevant and non-relevant documents 
rare terms content words and frequent terms noisy terms 
function words stopwords shows experimentally that 
most of the terms words in a collection are distributed 
according to a low dimension n-poisson model uses a 
 -poisson model for including term frequency-based 
probabilities in the probabilistic retrieval model the non-linear 
scaling of the poisson function showed significant 
improvement compared to a linear frequency-based probability the 
poisson model was here applied to the term frequency of a 
term in a document we will generalise the discussion by 
pointing out that document frequency and term frequency 
are dual parameters in the collection space and the 
document space respectively our discussion of the poisson 
distribution focuses on the document frequency in a collection 
rather than on the term frequency in a document 
 and address the deviation of idf and poisson and 
apply poisson mixtures to achieve better poisson-based 
estimates the results proved again experimentally that a 
onedimensional poisson does not work for rare terms therefore 
poisson mixtures and additional parameters are proposed 
 section illustrates and summarises 
comprehensively the relationships between frequencies probabilities 
and poisson different definitions of idf are put into 
context and a notion of noise is defined where noise is viewed 
as the complement of idf we use in our paper a different 
notion of noise we consider a frequency-based noise that 
corresponds to the document frequency and we consider a 
term noise that is based on the independence of document 
events 
 and link frequencies and probability 
estimation to information theory establishes a framework 
in which information retrieval models are formalised based 
on probabilistic inference a key component is the use of a 
space of disjoint events where the framework mainly uses 
terms as disjoint events the probability of being 
informative defined in our paper can be viewed as the probability 
of the disjoint terms in the term space of 
 address entropy and bibliometric distributions 
entropy is maximal if all events are equiprobable and the 
frequency-based lotka law n iλ 
is the number of scientists 
that have written i publications where n and λ are 
distribution parameters zipf and the pareto distribution are 
related the pareto distribution is the continuous case of the 
lotka and lotka and zipf show equivalences the pareto 
distribution is used by for term frequency normalisation 
the pareto distribution compares to the poisson 
distribution in the sense that pareto is fat-tailed i e pareto 
assigns larger probabilities to large numbers of events than 
poisson distributions do this makes pareto interesting 
since poisson is felt to be too radical on frequent events 
we restrict in this paper to the discussion of poisson 
however our results show that indeed a smoother distribution 
than poisson promises to be a good candidate for improving 
the estimation of probabilities in information retrieval 
 establishes a theoretical link between tf-idf and 
information theory and the theoretical research on the meaning 
of tf-idf clarifies the statistical model on which the different 
measures are commonly based this motivation matches 
the motivation of our paper we investigate theoretically 
the assumptions of classical idf and poisson for a better 
understanding of parameter estimation and combination 
 from disjoint to independent 
we define and discuss in this section three probabilities 
the frequency-based noise probability definition the 
total noise probability for disjoint documents definition 
and the noise probability for independent documents 
 definition 
 binary occurrence constant containment 
and disjointness of documents 
we show in this section that the frequency-based noise 
probability n t 
n 
in the idf definition can be explained as 
a total probability with binary term occurrence constant 
document containment and disjointness of document 
containments 
we refer to a probability function as binary if for all events 
the probability is either or the occurrence 
probability p t d is binary if p t d is equal to if t ∈ d and 
p t d is equal to otherwise 
p t d is binary ⇐⇒ p t d ∨ p t d 
we refer to a probability function as constant if for all 
events the probability is equal the document containment 
probability reflect the chance that a document occurs in a 
collection this containment probability is constant if we 
have no information about the document containment or 
we ignore that documents differ in containment 
containment could be derived for example from the size quality 
age links etc of a document for a constant containment 
in a collection with n documents 
n 
is often assumed as 
the containment probability we generalise this definition 
and introduce the constant λ where ≤ λ ≤ n the 
containment of a document d depends on the collection c this 
is reflected by the notation p d c used for the containment 
 
of a document 
p d c is constant ⇐⇒ ∀d p d c 
λ 
n 
for disjoint documents that cover the whole event space 
we set λ and obtain 
èd p d c next we define 
the frequency-based noise probability and the total noise 
probability for disjoint documents we introduce the event 
notation t is noisy and t occurs for making the difference 
between the noise probability p t is noisy c in a collection 
and the occurrence probability p t occurs d in a document 
more explicit thereby keeping in mind that the noise 
probability corresponds to the occurrence probability of a term 
in a collection 
definition the frequency-based term noise 
probability 
pfreq t is noisy c 
n t 
n 
definition the total term noise probability for 
disjoint documents 
pdis t is noisy c 
d 
p t occurs d · p d c 
now we can formulate a theorem that makes assumptions 
explicit that explain the classical idf 
theorem idf assumptions if the occurrence 
probability p t d of term t over documents d is binary and 
the containment probability p d c of documents d is 
constant and document containments are disjoint events then 
the noise probability for disjoint documents is equal to the 
frequency-based noise probability 
pdis t is noisy c pfreq t is noisy c 
proof the assumptions are 
∀d p t occurs d ∨ p t occurs d ∧ 
p d c 
λ 
n 
∧ 
d 
p d c 
we obtain 
pdis t is noisy c 
d t∈d 
 
n 
 
n t 
n 
 pfreq t is noisy c 
the above result is not a surprise but it is a 
mathematical formulation of assumptions that can be used to explain 
the classical idf the assumptions make explicit that the 
different types of term occurrence in documents frequency 
of a term importance of a term position of a term 
document part where the term occurs etc and the different 
types of document containment size quality age etc are 
ignored and document containments are considered as 
disjoint events 
from the assumptions we can conclude that idf 
 frequencybased noise respectively is a relatively simple but strict 
estimate still idf works well this could be explained 
by a leverage effect that justifies the binary occurrence and 
constant containment the term occurrence for small 
documents tends to be larger than for large documents whereas 
the containment for small documents tends to be smaller 
than for large documents from that point of view idf 
means that p t ∧ d c is constant for all d in which t occurs 
and p t ∧ d c is zero otherwise the occurrence and 
containment can be term specific for example set p t∧d c 
 nd c if t occurs in d where nd c is the number of 
documents in collection c we used before just n we choose a 
document-dependent occurrence p t d nt d i e the 
occurrence probability is equal to the inverse of nt d which 
is the total number of terms in document d next we choose 
the containment p d c nt d nt c ·nt c nd c where 
nt d nt c is a document length normalisation number 
of terms in document d divided by the number of terms in 
collection c and nt c nd c is a constant factor of the 
collection number of terms in collection c divided by the 
number of documents in collection c we obtain p t∧d c 
 nd c 
in a tf-idf -retrieval function the tf -component reflects 
the occurrence probability of a term in a document this is 
a further explanation why we can estimate the idf with a 
simple p t d since the combined tf-idf contains the 
occurrence probability the containment probability corresponds 
to a document normalisation document length 
normalisation pivoted document length and is normally attached to 
the tf -component or the tf-idf -product 
the disjointness assumption is typical for frequency-based 
probabilities from a probability theory point of view we 
can consider documents as disjoint events in order to achieve 
a sound theoretical model for explaining the classical idf 
but does disjointness reflect the real world where the 
containment of a document appears to be independent of the 
containment of another document in the next section we 
replace the disjointness assumption by the independence 
assumption 
 the upper bound of the noise probability 
for independent documents 
for independent documents we compute the probability 
of a disjunction as usual namely as the complement of the 
probability of the conjunction of the negated events 
p d ∨ ∨ dn − p ¬d ∧ ∧ ¬dn 
 − 
d 
 − p d 
the noise probability can be considered as the conjunction 
of the term occurrence and the document containment 
p t is noisy c p t occurs ∧ d ∨ ∨ dn c 
for disjoint documents this view of the noise probability 
led to definition for independent documents we use now 
the conjunction of negated events 
definition the term noise probability for 
independent documents 
pin t is noisy c 
d 
 − p t occurs d · p d c 
with binary occurrence and a constant containment p d c 
λ n we obtain the term noise of a term t that occurs in n t 
documents 
pin t is noisy c − − 
λ 
n 
n t 
 
for binary occurrence and disjoint documents the 
containment probability was n now with independent 
documents we can use λ as a collection parameter that controls 
the average containment probability we show through the 
next theorem that the upper bound of the noise probability 
depends on λ 
theorem the upper bound of being noisy if the 
occurrence p t d is binary and the containment p d c 
is constant and document containments are independent 
events then − e−λ 
is the upper bound of the noise 
probability 
∀t pin t is noisy c − e−λ 
proof the upper bound of the independent noise 
probability follows from the limit limn→∞ x 
n 
 n 
 ex 
 see 
any comprehensive math book for example for the 
convergence equation of the euler function with x −λ we 
obtain 
lim 
n→∞ 
 − 
λ 
n 
n 
 e−λ 
for the term noise we have 
pin t is noisy c − − 
λ 
n 
n t 
pin t is noisy c is strictly monotonous the noise of a term 
tn is less than the noise of a term tn where tn occurs in 
n documents and tn occurs in n documents 
therefore a term with n n has the largest noise probability 
for a collection with infinite many documents the upper 
bound of the noise probability for terms tn that occur in all 
documents becomes 
lim 
n→∞ 
pin tn is noisy lim 
n→∞ 
 − − 
λ 
n 
n 
 − e−λ 
by applying an independence rather a disjointness 
assumption we obtain the probability e− 
that a term is not noisy 
even if the term does occur in all documents in the disjoint 
case the noise probability is one for a term that occurs in 
all documents 
if we view p d c λ n as the average containment 
then λ is large for a term that occurs mostly in large 
documents and λ is small for a term that occurs mostly in small 
documents thus the noise of a term t is large if t occurs in 
n t large documents and the noise is smaller if t occurs in 
small documents alternatively we can assume a constant 
containment and a term-dependent occurrence if we 
assume p d c then p t d λ n can be interpreted as 
the average probability that t represents a document the 
common assumption is that the average containment or 
occurrence probability is proportional to n t however here 
is additional potential the statistical laws see on luhn 
and zipf indicate that the average probability could follow 
a normal distribution i e small probabilities for small n t 
and large n t and larger probabilities for medium n t 
for the monotonous case we investigate here the noise of 
a term with n t is equal to − − λ n λ n and 
the noise of a term with n t n is close to − e−λ 
 in the 
next section we relate the value e−λ 
to information theory 
 the probability of a maximal informative 
signal 
the probability e− 
is special in the sense that a signal 
with that probability is a signal with maximal information as 
derived from the entropy definition consider the definition 
of the entropy contribution h t of a signal t 
h t p t · − ln p t 
we form the first derivation for computing the optimum 
∂h t 
∂p t 
 − ln p t 
− 
p t 
· p t 
 − ln p t 
for obtaining optima we use 
 − ln p t 
the entropy contribution h t is maximal for p t e− 
 
this result does not depend on the base of the logarithm as 
we see next 
∂h t 
∂p t 
 − logb p t 
− 
p t · ln b 
· p t 
 − 
 
ln b 
 logb p t − 
 ln p t 
ln b 
we summarise this result in the following theorem 
theorem the probability of a maximal 
informative signal the probability pmax e− 
≈ is the 
probability of a maximal informative signal the entropy of a 
maximal informative signal is hmax e− 
 
proof the probability and entropy follow from the 
derivation above 
the complement of the maximal noise probability is e−λ 
and we are looking now for a generalisation of the entropy 
definition such that e−λ 
is the probability of a maximal 
informative signal we can generalise the entropy definition 
by computing the integral of λ ln p t i e this derivation 
is zero for e−λ 
 we obtain a generalised entropy 
− λ ln p t d p t p t · − λ − ln p t 
the generalised entropy corresponds for λ to the 
classical entropy by moving from disjoint to independent 
documents we have established a link between the complement 
of the noise probability of a term that occurs in all 
documents and information theory next we link independent 
documents to probability theory 
 the link to probability theory 
we review for independent documents three concepts of 
probability theory possible worlds binomial distribution 
and poisson distribution 
 possible worlds 
each conjunction of document events for each document 
we consider two document events the document can be 
true or false is associated with a so-called possible world 
for example consider the eight possible worlds for three 
documents n 
 
world w conjunction 
w d ∧ d ∧ d 
w d ∧ d ∧ ¬d 
w d ∧ ¬d ∧ d 
w d ∧ ¬d ∧ ¬d 
w ¬d ∧ d ∧ d 
w ¬d ∧ d ∧ ¬d 
w ¬d ∧ ¬d ∧ d 
w ¬d ∧ ¬d ∧ ¬d 
with each world w we associate a probability µ w which 
is equal to the product of the single probabilities of the 
document events 
world w probability µ w 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
w 
 λ 
n 
 
· 
 − λ 
n 
 
the sum over the possible worlds in which k documents are 
true and n −k documents are false is equal to the 
probability function of the binomial distribution since the binomial 
coefficient yields the number of possible worlds in which k 
documents are true 
 binomial distribution 
the binomial probability function yields the probability 
that k of n events are true where each event is true with 
the single event probability p 
p k binom n k p 
n 
k 
pk 
 − p n −k 
the single event probability is usually defined as p λ n 
i e p is inversely proportional to n the total number of 
events with this definition of p we obtain for an infinite 
number of documents the following limit for the product of 
the binomial coefficient and pk 
 
lim 
n→∞ 
n 
k 
pk 
 
 lim 
n→∞ 
n · n − · · n −k 
k 
λ 
n 
k 
 
λk 
k 
the limit is close to the actual value for k n for large 
k the actual value is smaller than the limit 
the limit of −p n −k follows from the limit limn→∞ 
x 
n 
 n 
 ex 
 
lim 
n→∞ 
 − p n−k 
 lim 
n→∞ 
 − 
λ 
n 
n −k 
 lim 
n→∞ 
e−λ 
· − 
λ 
n 
−k 
 e−λ 
again the limit is close to the actual value for k n for 
large k the actual value is larger than the limit 
 poisson distribution 
for an infinite number of events the poisson probability 
function is the limit of the binomial probability function 
lim 
n→∞ 
binom n k p 
λk 
k 
· e−λ 
p k poisson k λ 
λk 
k 
· e−λ 
the probability poisson is equal to e− 
 which is the 
probability of a maximal informative signal this shows 
the relationship of the poisson distribution and information 
theory 
after seeing the convergence of the binomial distribution 
we can choose the poisson distribution as an approximation 
of the independent term noise probability first we define 
the poisson noise probability 
definition the poisson term noise probability 
ppoi t is noisy c e−λ 
· 
n t 
k 
λk 
k 
for independent documents the poisson distribution 
approximates the probability of the disjunction for large n t 
since the independent term noise probability is equal to the 
sum over the binomial probabilities where at least one of 
n t document containment events is true 
pin t is noisy c 
n t 
k 
n t 
k 
pk 
 − p n −k 
pin t is noisy c ≈ ppoi t is noisy c 
we have defined a frequency-based and a poisson-based 
probability of being noisy where the latter is the limit of the 
independence-based probability of being noisy before we 
present in the final section the usage of the noise 
probability for defining the probability of being informative we 
emphasise in the next section that the results apply to the 
collection space as well as to the the document space 
 the collection space and the 
document space 
consider the dual definitions of retrieval parameters in 
table we associate a collection space d × t with a 
collection c where d is the set of documents and t is the set 
of terms in the collection let nd d and nt t 
be the number of documents and terms respectively we 
consider a document as a subset of t and a term as a subset 
of d let nt d {t d ∈ t} be the number of terms that 
occur in the document d and let nd t {d t ∈ d} be the 
number of documents that contain the term t 
in a dual way we associate a document space l × t with 
a document d where l is the set of locations also referred 
to as positions however we use the letters l and l and not 
p and p for avoiding confusion with probabilities and t is 
the set of terms in the document the document dimension 
in a collection space corresponds to the location position 
dimension in a document space 
the definition makes explicit that the classical notion of 
term frequency of a term in a document also referred to as 
the within-document term frequency actually corresponds 
to the location frequency of a term in a document for the 
 
space collection document 
dimensions documents and terms locations and terms 
document location 
frequency 
nd t c number of documents in which term t 
occurs in collection c 
nl t d number of locations positions at which 
term t occurs in document d 
nd c number of documents in collection c nl d number of locations positions in 
document d 
term frequency nt d c number of terms that document d 
contains in collection c 
nt l d number of terms that location l contains 
in document d 
nt c number of terms in collection c nt d number of terms in document d 
noise occurrence p t c term noise p t d term occurrence 
containment p d c document p l d location 
informativeness − ln p t c − ln p t d 
conciseness − ln p d c − ln p l d 
p informative ln p t c ln p tmin c ln p t d ln p tmin d 
p concise ln p d c ln p dmin c ln p l d ln p lmin d 
table retrieval parameters 
actual term frequency value it is common to use the 
maximal occurrence number of locations let lf be the location 
frequency 
tf t d lf t d 
pfreq t occurs d 
pfreq tmax occurs d 
 
nl t d 
nl tmax d 
a further duality is between informativeness and 
conciseness shortness of documents or locations informativeness 
is based on occurrence noise conciseness is based on 
containment 
we have highlighted in this section the duality between 
the collection space and the document space we 
concentrate in this paper on the probability of a term to be noisy 
and informative those probabilities are defined in the 
collection space however the results regarding the term noise 
and informativeness apply to their dual counterparts term 
occurrence and informativeness in a document also the 
results can be applied to containment of documents and 
locations 
 the probability of being 
informative 
we showed in the previous sections that the disjointness 
assumption leads to frequency-based probabilities and that 
the independence assumption leads to poisson probabilities 
in this section we formulate a frequency-based definition 
and a poisson-based definition of the probability of being 
informative and then we compare the two definitions 
definition the frequency-based probability of 
being informative 
pfreq t is informative c 
− ln n t 
n 
− ln 
n 
 − logn 
n t 
n 
 − logn n t − 
ln n t 
ln n 
we define the poisson-based probability of being 
informative analogously to the frequency-based probability of being 
informative see definition 
definition the poisson-based probability of 
being informative 
ppoi t is informative c 
− ln e−λ 
· 
èn t 
k 
λk 
k 
− ln e−λ · λ 
 
λ − ln 
èn t 
k 
λk 
k 
λ − ln λ 
for the sum expression the following limit holds 
lim 
n t →∞ 
n t 
k 
λk 
k 
 eλ 
− 
for λ we can alter the noise and informativeness 
poisson by starting the sum from since eλ 
 then the 
minimal poisson informativeness is poisson λ e−λ 
 we 
obtain a simplified poisson probability of being informative 
ppoi t is informative c ≈ 
λ − ln 
èn t 
k 
λk 
k 
λ 
 − 
ln 
èn t 
k 
λk 
k 
λ 
the computation of the poisson sum requires an 
optimisation for large n t the implementation for this paper 
exploits the nature of the poisson density the poisson 
density yields only values significantly greater than zero in an 
interval around λ 
consider the illustration of the noise and 
informativeness definitions in figure the probability functions 
displayed are summarised in figure where the simplified 
poisson is used in the noise and informativeness graphs the 
frequency-based noise corresponds to the linear solid curve 
in the noise figure with an independence assumption we 
obtain the curve in the lower triangle of the noise figure by 
changing the parameter p λ n of the independence 
probability we can lift or lower the independence curve the 
noise figure shows the lifting for the value λ ln n ≈ 
 the setting λ ln n is special in the sense that the 
frequency-based and the poisson-based informativeness have 
the same denominator namely ln n and the poisson sum 
converges to λ whether we can draw more conclusions from 
this setting is an open question 
we can conclude that the lifting is desirable if we know 
for a collection that terms that occur in relatively few 
doc 
 
 
 
 
 
 
 
probabilityofbeingnoisy 
n t number of documents with term t 
frequency 
independence n 
independence ln n n 
poisson 
poisson 
poisson 
 
 
 
 
 
 
 
probabilityofbeinginformative 
n t number of documents with term t 
frequency 
independence n 
independence ln n n 
poisson 
poisson 
poisson 
figure noise and informativeness 
probability function noise informativeness 
frequency pfreq def n t n ln n t n ln n 
interval n ≤ pfreq ≤ ≤ pfreq ≤ 
independence pin def − − p n t 
ln − − p n t 
 ln p 
interval p ≤ pin − e−λ 
ln p ≤ pin ≤ 
poisson ppoi def e−λ èn t 
k 
λk 
k 
 λ − ln 
èn t 
k 
λk 
k 
 λ − ln λ 
interval e−λ 
· λ ≤ ppoi − e−λ 
 λ − ln eλ 
− λ − ln λ ≤ ppoi ≤ 
poisson ppoi simplified def e−λ èn t 
k 
λk 
k 
 λ − ln 
èn t 
k 
λk 
k 
 λ 
interval e−λ 
≤ ppoi ppoi ≤ 
figure probability functions 
uments are no guarantee for finding relevant documents 
i e we assume that rare terms are still relatively noisy on 
the opposite we could lower the curve when assuming that 
frequent terms are not too noisy i e they are considered as 
being still significantly discriminative 
the poisson probabilities approximate the independence 
probabilities for large n t the approximation is better for 
larger λ for n t λ the noise is zero whereas for n t λ 
the noise is one this radical behaviour can be smoothened 
by using a multi-dimensional poisson distribution figure 
shows a poisson noise based on a two-dimensional poisson 
poisson k λ λ π · e−λ 
· 
λk 
 
k 
 − π · e−λ 
· 
λk 
 
k 
the two dimensional poisson shows a plateau between λ 
 and λ we used here π the idea 
behind this setting is that terms that occur in less than 
documents are considered to be not noisy i e they are 
informative that terms between and are half noisy 
and that terms with more than are definitely noisy 
for the informativeness we observe that the radical 
behaviour of poisson is preserved the plateau here is 
approximately at and it is important to realise that this 
plateau is not obtained with the multi-dimensional poisson 
noise using π the logarithm of the noise is 
normalised by the logarithm of a very small number namely 
 · e− 
 · e− 
 that is why the informativeness 
will be only close to one for very little noise whereas for a 
bit of noise informativeness will drop to zero this effect 
can be controlled by using small values for π such that the 
noise in the interval λ λ is still very little the setting 
π e− 
leads to noise values of approximately e− 
in the interval λ λ the logarithms lead then to for 
the informativeness 
the indepence-based and frequency-based informativeness 
functions do not differ as much as the noise functions do 
however for the indepence-based probability of being 
informative we can control the average informativeness by the 
definition p λ n whereas the control on the 
frequencybased is limited as we address next 
for the frequency-based idf the gradient is monotonously 
decreasing and we obtain for different collections the same 
distances of idf -values i e the parameter n does not affect 
the distance for an illustration consider the distance 
between the value idf tn of a term tn that occurs in n 
documents and the value idf tn of a term tn that occurs in 
n documents 
idf tn − idf tn ln 
n 
n 
the first three values of the distance function are 
idf t − idf t ln 
idf t − idf t ln 
idf t − idf t ln 
for the poisson-based informativeness the gradient decreases 
first slowly for small n t then rapidly near n t ≈ λ and 
then it grows again slowly for large n t 
in conclusion we have seen that the poisson-based 
definition provides more control and parameter possibilities than 
 
the frequency-based definition does whereas more control 
and parameter promises to be positive for the 
personalisation of retrieval systems it bears at the same time the 
danger of just too many parameters the framework presented 
in this paper raises the awareness about the probabilistic 
and information-theoretic meanings of the parameters the 
parallel definitions of the frequency-based probability and 
the poisson-based probability of being informative made 
the underlying assumptions explicit the frequency-based 
probability can be explained by binary occurrence constant 
containment and disjointness of documents independence 
of documents leads to poisson where we have to be aware 
that poisson approximates the probability of a disjunction 
for a large number of events but not for a small number 
this theoretical result explains why experimental 
investigations on poisson see show that a poisson estimation 
does work better for frequent bad noisy terms than for 
rare good informative terms 
in addition to the collection-wide parameter setting the 
framework presented here allows for document-dependent 
settings as explained for the independence probability this 
is in particular interesting for heterogeneous and structured 
collections since documents are different in nature size 
quality root document sub document and therefore 
binary occurrence and constant containment are less 
appropriate than in relatively homogeneous collections 
 summary 
the definition of the probability of being informative 
transforms the informative interpretation of the idf into a 
probabilistic interpretation and we can use the idf -based 
probability in probabilistic retrieval approaches we showed that 
the classical definition of the noise document frequency in 
the inverse document frequency can be explained by three 
assumptions the term within-document occurrence 
probability is binary the document containment probability is 
constant and the document containment events are disjoint 
by explicitly and mathematically formulating the 
assumptions we showed that the classical definition of idf does not 
take into account parameters such as the different nature 
 size quality structure etc of documents in a collection 
or the different nature of terms coverage importance 
position etc in a document we discussed that the absence 
of those parameters is compensated by a leverage effect of 
the within-document term occurrence probability and the 
document containment probability 
by applying an independence rather a disjointness 
assumption for the document containment we could 
establish a link between the noise probability term occurrence 
in a collection information theory and poisson from the 
frequency-based and the poisson-based probabilities of 
being noisy we derived the frequency-based and poisson-based 
probabilities of being informative the frequency-based 
probability is relatively smooth whereas the poisson probability 
is radical in distinguishing between noisy or not noisy and 
informative or not informative respectively we showed how 
to smoothen the radical behaviour of poisson with a 
multidimensional poisson 
the explicit and mathematical formulation of idf - and 
poisson-assumptions is the main result of this paper also 
the paper emphasises the duality of idf and tf collection 
space and document space respectively thus the result 
applies to term occurrence and document containment in a 
collection and it applies to term occurrence and position 
containment in a document this theoretical framework is 
useful for understanding and deciding the parameter 
estimation and combination in probabilistic retrieval models the 
links between indepence-based noise as document frequency 
probabilistic interpretation of idf information theory and 
poisson described in this paper may lead to variable 
probabilistic idf and tf definitions and combinations as required 
in advanced and personalised information retrieval systems 
acknowledgment i would like to thank mounia lalmas 
gabriella kazai and theodora tsikrika for their comments 
on the as they said heavy pieces my thanks also go to the 
meta-reviewer who advised me to improve the presentation 
to make it less formidable and more accessible for those 
without a theoretic bent this work was funded by a 
research fellowship from queen mary university of london 
 references 
 a aizawa an information-theoretic perspective of 
tf-idf measures information processing and 
management - january 
 g amati and c j rijsbergen term frequency 
normalization via pareto distributions in th 
bcs-irsg european colloquium on ir research 
glasgow scotland 
 r k belew finding out about cambridge university 
press 
 a bookstein and d swanson probabilistic models 
for automatic indexing journal of the american 
society for information science - 
 i n bronstein taschenbuch der mathematik harri 
deutsch thun frankfurt am main 
 k church and w gale poisson mixtures natural 
language engineering - 
 k w church and w a gale inverse document 
frequency a measure of deviations from poisson in 
third workshop on very large corpora acl 
anthology 
 t lafouge and c michel links between information 
construction and information gain entropy and 
bibliometric distribution journal of information 
science - 
 e margulis n-poisson document modelling in 
proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - 
 s e robertson and s walker some simple effective 
approximations to the -poisson model for 
probabilistic weighted retrieval in proceedings of the 
 th annual international acm sigir conference on 
research and development in information retrieval 
pages - london et al springer-verlag 
 s wong and y yao an information-theoric measure 
of term specificity journal of the american society 
for information science - 
 s wong and y yao on modeling information 
retrieval with probabilistic inference acm 
transactions on information systems - 
 
 
controlling overlap in content-oriented xml retrieval 
charles l a clarke 
school of computer science university of waterloo canada 
claclark plg uwaterloo ca 
abstract 
the direct application of standard ranking techniques to 
retrieve individual elements from a collection of xml 
documents often produces a result set in which the top ranks are 
dominated by a large number of elements taken from a small 
number of highly relevant documents this paper presents 
and evaluates an algorithm that re-ranks this result set with 
the aim of minimizing redundant content while preserving 
the benefits of element retrieval including the benefit of 
identifying topic-focused components contained within 
relevant documents the test collection developed by the 
initiative for the evaluation of xml retrieval inex forms 
the basis for the evaluation 
categories and subject descriptors 
h information systems information storage and 
retrieval-information search and retrieval 
general terms 
algorithms measurement performance experimentation 
 introduction 
the representation of documents in xml provides an 
opportunity for information retrieval systems to take 
advantage of document structure returning individual document 
components when appropriate rather than complete 
documents in all circumstances in response to a user query an 
xml information retrieval system might return a mixture 
of paragraphs sections articles bibliographic entries and 
other components this facility is of particular benefit when 
a collection contains very long documents such as product 
manuals or books where the user should be directed to the 
most relevant portions of these documents 
 article 
 fm 
 atl text compression for 
dynamic document databases atl 
 au alistair moffat au 
 au justin zobel au 
 au neil sharman au 
 abs p b abstract b for p abs 
 fm 
 bdy 
 sec st introduction st 
 ip modern document databases ip 
 p there are good reasons to compress p 
 sec 
 sec st reducing memory requirements st 
 ss st method a st 
 sec 
 
 bdy 
 article 
figure a journal article encoded in xml 
figure provides an example of a journal article encoded 
in xml illustrating many of the important characteristics 
of xml documents tags indicate the beginning and end of 
each element with elements varying widely in size from one 
word to thousands of words some elements such as 
paragraphs and sections may be reasonably presented to the user 
as retrieval results but others are not appropriate elements 
overlap each other - articles contain sections sections 
contain subsections and subsections contain paragraphs each 
of these characteristics affects the design of an xml ir 
system and each leads to fundamental problems that must be 
solved in an successful system most of these fundamental 
problems can be solved through the careful adaptation of 
standard ir techniques but the problems caused by overlap 
are unique to this area and form the primary focus of 
this paper 
the article of figure may be viewed as an xml tree 
as illustrated in figure formally a collection of xml 
documents may be represented as a forest of ordered rooted 
trees consisting of a set of nodes n and a set of directed 
edges e connecting these nodes for each node x ∈ n the 
notation x parent refers to the parent node of x if one exists 
and the notation x children refers to the set of child nodes 
sec 
bdyfm 
atl au au au 
abs 
p 
b 
st ip 
sec 
st 
ss 
st 
article 
p 
figure example xml tree 
of x since an element may be represented by the node at 
its root the output of an xml ir system may be viewed as 
a ranked list of the top-m nodes 
the direct application of a standard relevance ranking 
technique to a set of xml elements can produce a result 
in which the top ranks are dominated by many structurally 
related elements a high scoring section is likely to contain 
several high scoring paragraphs and to be contained in an 
high scoring article for example many of the elements in 
figure would receive a high score on the keyword query 
text index compression algorithms if each of these 
elements are presented to a user as an individual and 
separate result she may waste considerable time reviewing and 
rejecting redundant content 
one possible solution is to report only the highest 
scoring element along a given path in the tree and to remove 
from the lower ranks any element containing it or contained 
within it unfortunately this approach destroys some of the 
possible benefits of xml ir for example an outer element 
may contain a substantial amount of information that does 
not appear in an inner element but the inner element may 
be heavily focused on the query topic and provide a short 
overview of the key concepts in such cases it is reasonable 
to report elements which contain or are contained in higher 
ranking elements even when an entire book is relevant a 
user may still wish to have the most important paragraphs 
highlighted to guide her reading and to save time 
this paper presents a method for controlling overlap 
starting with an initial element ranking a re-ranking algorithm 
adjusts the scores of lower ranking elements that contain or 
are contained within higher ranking elements reflecting the 
fact that this information may now be redundant for 
example once an element representing a section appears in the 
ranking the scores for the paragraphs it contains and the 
article that contains it are reduced the inspiration for this 
strategy comes partially from recent work on structured 
documents retrieval where terms appearing in different fields 
such as the title and body are given different weights 
extending that approach the re-ranking algorithm varies 
weights dynamically as elements are processed 
the remainder of the paper is organized as follows after 
a discussion of background work and evaluation 
methodology a baseline retrieval method is presented in section 
this baseline method represents a reasonable adaptation of 
standard ir technology to xml section then outlines a 
strategy for controlling overlap using the baseline method as 
a starting point a re-ranking algorithm implementing this 
strategy is presented in section and evaluated in section 
section discusses an extended version of the algorithm 
 background 
this section provides a general overview of xml 
information retrieval and discusses related work with an emphasis 
on the fundamental problems mentioned in the introduction 
much research in the area of xml retrieval views it from a 
traditional database perspective being concerned with such 
problems as the implementation of structured query 
languages and the processing of joins here we take 
a content oriented ir perceptive focusing on xml 
documents that primarily contain natural language data and 
queries that are primarily expressed in natural language 
we assume that these queries indicate only the nature of 
desired content not its structure and that the role of the 
ir system is to determine which elements best satisfy the 
underlying information need other ir research has 
considered mixed queries in which both content and structural 
requirements are specified 
 term and document statistics 
in traditional information retrieval applications the 
standard unit of retrieval is taken to be the document 
depending on the application this term might be interpreted 
to encompass many different objects including web pages 
newspaper articles and email messages 
when applying standard relevance ranking techniques in 
the context of xml ir a natural approach is to treat each 
element as a separate document with term statistics 
available for each in addition most ranking techniques 
require global statistics e g inverse document frequency 
computed over the collection as a whole if we consider this 
collection to include all elements that might be returned by 
the system a specific occurrence of a term may appear in 
several different documents perhaps in elements 
representing a paragraph a subsection a section and an article 
it is not appropriate to compute inverse document frequency 
under the assumption that the term is contained in all of 
these elements since the number of elements that contain a 
term depends entirely on the structural arrangement of the 
documents 
 retrievable elements 
while an xml ir system might potentially retrieve any 
element many elements may not be appropriate as retrieval 
results this is usually the case when elements contain very 
little text for example a section title containing only 
the query terms may receive a high score from a ranking 
algorithm but alone it would be of limited value to a user who 
might prefer the actual section itself other elements may 
reflect the document s physical rather than logical 
structure which may have little or no meaning to a user an 
effective xml ir system must return only those elements 
that have sufficient content to be usable and are able to 
stand alone as independent objects standard 
document components such as paragraphs sections subsections 
and abstracts usually meet these requirements titles 
italicized phrases and individual metadata fields often do not 
 evaluation methodology 
over the past three years the initiative for the 
evaluation of xml retrieval inex has encouraged research into 
xml information retrieval technology inex is an 
experimental conference series similar to trec with groups 
from different institutions completing one or more 
experimental tasks using their own tools and systems and 
comparing their results at the conference itself over groups 
participated in inex and the conference has become 
as influential in the area of xml ir as trec is in other ir 
areas the research described in this paper as well as much 
of the related work it cites depends on the test collections 
developed by inex 
overlap causes considerable problems with retrieval 
evaluation and the inex organizers and participants have 
wrestled with these problems since the beginning while 
substantial progress has been made these problem are still not 
completely solved kazai et al provide a detailed 
exposition of the overlap problem in the context of inex 
retrieval evaluation and discuss both current and proposed 
evaluation metrics many of these metrics are applied to 
evaluate the experiments reported in this paper and they 
are briefly outlined in the next section 
 inex 
space limitations prevent the inclusion of more than a 
brief summary of inex tasks and evaluation 
methodology for detailed information the proceedings of the 
conference itself should be consulted 
 tasks 
for the main experimental tasks inex participants 
were provided with a collection of articles taken from 
the ieee computer societies magazines and journals 
between and each document is encoded in xml 
using a common dtd with the document of figures and 
providing one example 
at inex the two main experimental tasks were both 
adhoc retrieval tasks investigating the performance of 
systems searching a static collection using previously unseen 
topics the two tasks differed in the types of topics they 
used for one task the content-only or co task the 
topics consist of short natural language statements with no 
direct reference to the structure of the documents in the 
collection for this task the ir system is required to select the 
elements to be returned for the other task the 
contentand-structure or cas task the topics are written in an 
xml query language and contain explicit references to 
document structure which the ir system must attempt to 
satisfy since the work described in this paper is directed 
at the content-only task where the ir system receives no 
guidance regarding the elements to return the cas task is 
ignored in the remainder of our description 
in new co topics were selected by the conference 
organizers from contributions provided by the conference 
participants each topic includes a short keyword query 
which is executed over the collection by each participating 
group on their own xml ir system each group could 
submit up to three experimental runs consisting of the top 
m elements for each topic 
 relevance assessment 
since xml ir is concerned with locating those elements 
that provide complete coverage of a topic while containing as 
little extraneous information as possible simple relevant 
vs not relevant judgments are not sufficient instead the 
inex organizers adopted two dimensions for relevance 
assessment the exhaustivity dimension reflects the degree to 
which an element covers the topic and the specificity 
dimension reflects the degree to which an element is focused on the 
topic a four-point scale is used in both dimensions thus 
a element is highly exhaustive and highly specific a 
 element is marginally exhaustive and highly specific 
and a element is not relevant additional information 
on the assessment methodology may be found in piwowarski 
and lalmas who provide a detailed rationale 
 evaluation metrics 
the principle evaluation metric used at inex is a 
version of mean average precision map adjusted by 
various quantization functions to give different weights to 
different elements depending on their exhaustivity and specificity 
values one variant the strict quantization function gives a 
weight of to elements and a weight of to all others 
this variant is essentially the familiar map value with 
elements treated as relevant and all other elements treated 
as not relevant other quantization functions are designed 
to give partial credit to elements which are near misses 
due to a lack or exhaustivity and or specificity both the 
generalized quantization function and the specificity-oriented 
generalization sog function credit elements according to 
their degree of relevance with the second function 
placing greater emphasis on specificity this paper reports 
results of this metric using all three of these quantization 
functions since this metric was first introduced at inex 
it is generally referred as the inex- metric 
the inex- metric does not penalize overlap in 
particular both the generalized and sog quantization functions 
give partial credit to a near miss even when a 
element overlapping it is reported at a higher rank to address 
this problem kazai et al propose an xml cumulated 
gain metric which compares the cumulated gain of a 
ranked list to an ideal gain vector this ideal gain vector 
is constructed from the relevance judgments by 
eliminating overlap and retaining only best element along a given 
path thus the xcg metric rewards retrieval runs that 
avoid overlap while xcg was not used officially at inex 
 a version of it is likely to be used in the future 
at inex yet another metric was introduced to 
ameliorate the perceived limitations of the inex- metric 
this inex- metric extends the definitions of precision 
and recall to consider both the size of reported components 
and the overlap between them two versions were created 
one that considered only component size and another that 
considered both size and overlap while the inex- 
metric exhibits undesirable anomalies and was not used in 
 values are reported in the evaluation section to provide 
an additional instrument for investigating overlap 
 baseline retrieval method 
this section provides an overview of baseline xml 
information retrieval method currently used in the multitext 
ir system developed by the information retrieval group at 
the university of waterloo this retrieval method results 
from the adaptation and tuning of the okapi bm 
measure to the xml information retrieval task the 
multitext system performed respectably at inex placing 
in the top ten under all of the quantization functions and 
placing first when the quantization function emphasized 
exhaustivity 
to support retrieval from xml and other structured 
document types the system provides generalized queries of the 
form 
rank x by y 
where x is a sub-query specifying a set of document elements 
to be ranked and y is a vector of sub-queries specifying 
individual retrieval terms 
for our inex runs the sub-query x specified a list 
of retrievable elements as those with tag names as follows 
abs app article bb bdy bm fig fm ip 
li p sec ss ss vt 
this list includes bibliographic entries bb and figure 
captions fig as well as paragraphs sections and subsections 
prior to inex the inex collection and the inex 
relevance judgments were manually analyzed to select these 
tag names tag names were selected on the basis of their 
frequency in the collection the average size of their 
associated elements and the relative number of positive relevance 
judgments they received automating this selection process 
is planned as future work 
for inex the term vector y was derived from the 
topic by splitting phrases into individual words eliminating 
stopwords and negative terms those starting with - and 
applying a stemmer for example keyword field of topic 
 
 tree edit distance xml -image 
became the four-term query 
 tree edit distance xml 
where the operator within a quoted string stems the 
term that follows it 
our implementation of okapi bm is derived from the 
formula of robertson et al by setting parameters k 
and k ∞ given a term set q an element x is assigned 
the score 
 
t∈q 
w 
qt 
 k xt 
k xt 
 
where 
w 
 log 
d − dt 
dt ¢ 
d number of documents in the corpus 
dt number of documents containing t 
qt frequency that t occurs in the topic 
xt frequency that t occurs in x 
k k − b b · lx lavg 
lx length of x 
lavg average document length 
 
 
 
 
 
 
 
 
 
 
meanaverageprecision inex- 
k 
strict 
generalized 
sog 
figure impact of k on inex- mean average 
precision with b inex co topics 
prior to inex the inex topics and judgments 
were used to tune the b and k parameters and the impact 
of this tuning is discussed later in this section 
for the purposes of computing document-level statistics 
 d dt and lavg a document is defined to be an article 
these statistics are used for ranking all element types 
following the suggestion of kamps et al the retrieval 
results are filtered to eliminate very short elements those less 
than words in length 
the use of article statistics for all element types might 
be questioned this approach may be justified by 
viewing the collection as a set of articles to be searched using 
standard document-oriented techniques where only articles 
may be returned the score computed for an element is 
essentially the score it would receive if it were added to the 
collection as a new document ignoring the minor 
adjustments needed to the document-level statistics nonetheless 
we plan to examine this issue again in the future 
in our experience the performance of bm typically 
benefits from tuning the b and k parameters to the 
collection whenever training queries are available for this 
purpose prior to inex we trained the multitext system 
using the inex queries as a starting point we used 
the values b and k which perform well on 
trec adhoc collections and are used as default values in 
our system the results were surprising figure shows the 
result of varying k with b on the map values under 
three quantization functions in our experience optimal 
values for k are typically in the range to in this case 
large values are required for good performance between 
k and k map increases by over under 
the strict quantization similar improvements are seen 
under the generalized and sog quantizations in contrast our 
default value of b works well under all quantization 
functions figure after tuning over a wide range of 
values under several quantization functions we selected values 
of k and b for our inex experiments 
and these values are used for the experiments reported in 
section 
 
 
 
 
 
 
 
 
 
 
meanaverageprecision inex- 
b 
strict 
generalized 
sog 
figure impact of b on inex- mean average 
precision with k inex co topics 
 controlling overlap 
starting with an element ranking generated by the 
baseline method described in the previous section elements are 
re-ranked to control overlap by iteratively adjusting the scores 
of those elements containing or contained in higher ranking 
elements at a conceptual level re-ranking proceeds as 
follows 
 report the highest ranking element 
 adjust the scores of the unreported elements 
 repeat steps and until m elements are reported 
one approach to adjusting the scores of unreported elements 
in step might be based on the okapi bm scores of the 
involved elements for example assume a paragraph with 
score p is reported in step in step the section 
containing the paragraph might then have its score s lowered 
by an amount α · p to reflect the reduced contribution the 
paragraph should make to the section s score 
in a related context robertson et al argue strongly 
against the linear combination of okapi scores in this 
fashion that work considers the problem of assigning different 
weights to different document fields such as the title and 
body associated with web pages a common approach to 
this problem scores the title and body separately and 
generates a final score as a linear combination of the two 
robertson et al discuss the theoretical flaws in this approach and 
demonstrate experimentally that it can actually harm 
retrieval effectiveness instead they apply the weights at the 
term frequency level with an occurrence of a query term 
t in the title making a greater contribution to the score 
than an occurrence in the body in equation xt becomes 
α · yt α · zt where yt is the number of times t occurs in 
the title and zt is the number of times t occurs in the body 
translating this approach to our context the 
contribution of terms appearing in elements is dynamically reduced 
as they are reported the next section presents and 
analysis a simple re-ranking algorithm that follows this strategy 
the algorithm is evaluated experimentally in section one 
limitation of the algorithm is that the contribution of terms 
appearing in reported elements is reduced by the same 
factor regardless of the number of reported elements in which 
it appears in section the algorithm is extended to apply 
increasing weights lowering the score when a term appears 
in more than one reported element 
 re-ranking algorithm 
the re-ranking algorithm operates over xml trees such 
as the one appearing in figure input to the algorithm is 
a list of n elements ranked according to their initial bm 
scores during the initial ranking the xml tree is 
dynamically re-constructed to include only those nodes with 
nonzero bm scores so n may be considerably less than n 
output from the algorithm is a list of the top m elements 
ranked according to their adjusted scores 
an element is represented by the node x ∈ n at its root 
associated with this node are fields storing the length of 
element term frequencies and other information required 
by the re-ranking algorithm as follows 
x f - term frequency vector 
x g - term frequency adjustments 
x l - element length 
x score - current okapi bm score 
x reported - boolean flag initially false 
x children - set of child nodes 
x parent - parent node if one exists 
these fields are populated during the initial ranking process 
and updated as the algorithm progresses the vector x f 
contains term frequency information corresponding to each 
term in the query the vector x g is initially zero and is 
updated by the algorithm as elements are reported 
the score field contains the current bm score for the 
element which will change as the values in x g change the 
score is computed using equation with the xt value for 
each term determined by a combination of the values in x f 
and x g given a term t ∈ q let ft be the component of 
x f corresponding to t and let gt be the component of x g 
corresponding to t then 
xt ft − α · gt 
for processing by the re-ranking algorithm nodes are 
stored in priority queues ordered by decreasing score each 
priority queue pq supports three operations 
pq front - returns the node with greatest score 
pq add x - adds node x to the queue 
pq remove x - removes node x from the queue 
when implemented using standard data structures the front 
operation requires o time and the other operations 
require o log n time where n is the size of the queue 
the core of the re-ranking algorithm is presented in 
figure the algorithm takes as input the priority queue s 
containing the initial ranking and produces the top-m 
reranked nodes in the priority queue f after initializing f to 
be empty on line the algorithm loops m times over lines 
 transferring at least one node from s to f during each 
iteration at the start of each iteration the unreported node 
at the front of s has the greatest adjusted score and it is 
removed and added to f the algorithm then traverses the 
 f ← ∅ 
 for i ← to m do 
 x ← s front 
 s remove x 
 x reported ← true 
 f add x 
 
 foreach y ∈ x children do 
 down y 
 end do 
 
 if x is not a root node then 
 up x x parent 
 end if 
 end do 
figure re-ranking algorithm - as input the 
algorithm takes a priority queue s containing xml 
nodes ranked by their initial scores and returns 
its results in priority queue f ranked by adjusted 
scores 
 up x y ≡ 
 s remove y 
 y g ← y g x f − x g 
 recompute y score 
 s add y 
 if y is not a root node then 
 up x y parent 
 end if 
 
 down x ≡ 
 if not x reported then 
 s remove x 
 x g ← x f 
 recompute x score 
 if x score then 
 f add x 
 end if 
 x reported ← true 
 foreach y ∈ x children do 
 down y 
 end do 
 end if 
figure tree traversal routines called by the 
reranking algorithm 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
meanaverageprecision inex- 
xmlcumulatedgain xcg 
alpha 
map strict 
map generalized 
map sog 
xcg sog 
figure impact of α on xcg and inex- map 
 inex co topics assessment set i 
node s ancestors lines - and descendants lines - 
adjusting the scores of these nodes 
the tree traversal routines up and down are given in 
figure the up routine removes each ancestor node from s 
adjusts its term frequency values recomputes its score and 
adds it back into s the adjustment of the term frequency 
values line adds to y g only the previously unreported 
term occurrences in x re-computation of the score on line 
uses equations and the down routine performs a similar 
operation on each descendant however since the contents 
of each descendant are entirely contained in a reported 
element its final score may be computed and it is removed 
from s and added to f 
in order to determine the time complexity of the 
algorithm first note that a node may be an argument to down 
at most once thereafter the reported flag of its parent is 
true during each call to down a node may be moved from 
s to f requiring o log n time thus the total time for all 
calls to down is o n log n and we may temporarily ignore 
lines - of figure when considering the time complexity 
of the loop over lines - during each iteration of this loop 
a node and each of its ancestors are removed from a priority 
queue and then added back into a priority queue since a 
node may have at most h ancestors where h is the maximum 
height of any tree in the collection each of the m iterations 
requires o h log n time combining these observations 
produces an overall time complexity of o n mh log n 
in practice re-ranking an inex result set requires less 
than ms on a three-year-old desktop pc 
 evaluation 
none of the metrics described in section is a close fit 
with the view of overlap advocated by this paper 
nonetheless when taken together they provide insight into the 
behaviour of the re-ranking algorithm the inex evaluation 
packages inex eval and inex eval ng were used to 
compute values for the inex- and inex- metrics values 
for the xcg metrics were computed using software supplied 
by its inventors 
figure plots the three variants of inex- map metric 
together with the xcg metric values for these metrics 
 
 
 
 
 
 
 
 
 
 
 
 
meanaverageprecision inex- 
alpha 
strict overlap not considered 
strict overlap considered 
generalized overlap not considered 
generalized overlap considered 
figure impact of α on inex- map inex 
 co topics assessment set i 
are plotted for values of α between and recalling 
that the xcg metric is designed to penalize overlap while 
the inex- metric ignores overlap the conflict between 
the metrics is obvious the map values at one extreme 
 α and the xcg value at the other extreme α 
 represent retrieval performance comparable to the best 
systems at inex 
figure plots values of the inex- map metric for two 
quantizations with and without consideration of overlap 
once again conflict is apparent with the influence of α 
substantially lessened when overlap is considered 
 extended algorithm 
one limitation of the re-ranking algorithm is that a single 
weight α is used to adjust the scores of both the ancestors 
and descendants of reported elements an obvious extension 
is to use different weights in these two cases furthermore 
the same weight is used regardless of the number of times 
an element is contained in a reported element for example 
a paragraph may form part of a reported section and then 
form part of a reported article since the user may now 
have seen this paragraph twice its score should be further 
lowered by increasing the value of the weight 
motivated by these observations the re-ranking algorithm 
may be extended with a series of weights 
 β ≥ β ≥ β ≥ ≥ βm ≥ 
where βj is the weight applied to a node that has been a 
descendant of a reported node j times note that an upper 
bound on m is h the maximum height of any xml tree 
in the collection however in practice m is likely to be 
relatively small perhaps or 
figure presents replacements for the up and down 
routines of figure incorporating this series of weights one 
extra field is required in each node as follows 
x j - down count 
the value of x j is initially set to zero in all nodes and is 
incremented each time down is called with x as its argument 
when computing the score of node the value of x j selects 
 up x y ≡ 
 if not y reported then 
 s remove y 
 y g ← y g x f − x g 
 recompute y score 
 s add y 
 if y is not a root node then 
 up x y parent 
 end if 
 end if 
 
 down x ≡ 
 if x j m then 
 x j ← x j 
 if not x reported then 
 s remove x 
 recompute x score 
 s add x 
 end if 
 foreach y ∈ x children do 
 down y 
 end do 
 end if 
figure extended tree traversal routines 
the weight to be applied to the node by adjusting the value 
of xt in equation as follows 
xt βx j · ft − α · gt 
where ft and gt are the components of x f and x g 
corresponding to term t 
a few additional changes are required to extend up and 
down the up routine returns immediately line if its 
argument has already been reported since term frequencies 
have already been adjusted in its ancestors the down 
routine does not report its argument but instead recomputes 
its score and adds it back into s 
a node cannot be an argument to down more than m 
times which in turn implies an overall time complexity of 
o nm mh log n since m ≤ h and m ≤ n the time 
complexity is also o nh log n 
 concluding discussion 
when generating retrieval results over an xml collection 
some overlap in the results should be tolerated and may be 
beneficial for example when a highly exhaustive and fairly 
specific element contains a much smaller element 
both should be reported to the user and retrieval algorithms 
and evaluation metrics should respect this relationship the 
algorithm presented in this paper controls overlap by 
weighting the terms occurring in reported elements to reflect their 
reduced importance 
other approaches may also help to control overlap for 
example when xml retrieval results are presented to users 
it may be desirable to cluster structurally related elements 
together visually illustrating the relationships between them 
while this style of user interface may help a user cope with 
overlap the strategy presented in this paper continues to be 
applicable by determining the best elements to include in 
each cluster 
at waterloo we continue to develop and test our ideas 
for inex in particular we are investigating methods 
for learning the α and βj weights we are also re-evaluating 
our approach to document statistics and examining 
appropriate adjustments to the k parameter as term weights 
change 
 acknowledgments 
thanks to gabriella kazai and arjen de vries for 
providing an early version of their software for computing the xcg 
metric and thanks to phil tilker and stefan b¨uttcher for 
their help with the experimental evaluation in part 
funding for this project was provided by ibm canada through 
the national institute for software research 
 references 
 n bruno n koudas and d srivastava holistic twig 
joins optimal xml pattern matching in proceedings 
of the acm sigmod international conference 
on the management of data pages - madison 
wisconsin june 
 d carmel y s maarek m mandelbrod y mass 
and a soffer searching xml documents via xml 
fragments in proceedings of the th annual 
international acm sigir conference on research 
and development in information retrieval pages 
 - toronto canada 
 c l a clarke and p l tilker multitext 
experiments for inex in inex workshop 
proceedings published in lncs 
 a p de vries g kazai and m lalmas tolerance to 
irrelevance a user-effort oriented evaluation of 
retrieval systems without predefined retrieval unit in 
riao conference proceedings pages - 
avignon france april 
 d dehaan d toman m p consens and m t 
¨ozsu a comprehensive xquery to sql translation 
using dynamic interval encoding in proceedings of the 
 acm sigmod international conference on the 
management of data san diego june 
 n fuhr and k großjohann xirql a query 
language for information retrieval in xml documents 
in proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - new orleans 
september 
 n fuhr m lalmas and s malik editors initiative 
for the evaluation of xml retrieval proceedings of 
the second workshop inex dagstuhl 
germany december 
 n fuhr m lalmas s malik and zolt´an szl´avik 
editors initiative for the evaluation of xml 
retrieval proceedings of the third workshop inex 
 dagstuhl germany december published 
as advances in xml information retrieval lecture 
notes in computer science volume springer 
 
 k j¨avelin and j kek¨al¨ainen cumulated gain-based 
evaluation of ir techniques acm transactions on 
information systems - 
 j kamps m de rijke and b sigurbj¨ornsson length 
normalization in xml retrieval in proceedings of the 
 th annual international acm sigir conference on 
research and development in information retrieval 
pages - sheffield uk july 
 g kazai m lalmas and a p de vries the overlap 
problem in content-oriented xml retrieval evaluation 
in proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - sheffield uk 
july 
 g kazai m lalmas and a p de vries reliability 
tests for the xcg and inex- metrics in inex 
 workshop proceedings published in lncs 
 
 j kek¨al¨ainen m junkkari p arvola and t aalto 
trix - struggling with the overlap in inex 
 workshop proceedings published in lncs 
 
 s liu q zou and w w chu configurable 
indexing and ranking for xml information retrieval 
in proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - sheffield uk 
july 
 y mass and m mandelbrod retrieving the most 
relevant xml components in inex workshop 
proceedings dagstuhl germany december 
 y mass and m mandelbrod component ranking and 
automatic query refinement for xml retrieval in 
inex workshop proceedings published in 
lncs 
 p ogilvie and j callan hierarchical language models 
for xml component retrieval in inex 
workshop proceedings published in lncs 
 
 j pehcevski j a thom and a vercoustre hybrid 
xml retrieval re-visited in inex workshop 
proceedings published in lncs 
 b piwowarski and m lalmas providing consistent 
and exhaustive relevance assessments for xml 
retrieval evaluation in proceedings of the th acm 
conference on information and knowledge 
management pages - washington dc 
november 
 s robertson h zaragoza and m taylor simple 
bm extension to multiple weighted fields in 
proceedings of the th acm conference on 
information and knowledge management pages 
 - washington dc november 
 s e robertson s walker and m beaulieu okapi at 
trec- automatic ad-hoc filtering vlc and 
interactive track in proceedings of the seventh text 
retrieval conference gaithersburg md november 
 
 a trotman and b sigurbj¨ornsson nexi now and 
next in inex workshop proceedings 
published in lncs 
 j vittaut b piwowarski and p gallinari an 
algebra for structured queries in bayesian networks in 
inex workshop proceedings published in 
lncs 
hits on the web how does it compare 
marc najork 
microsoft research 
 la avenida 
mountain view ca usa 
najork microsoft com 
hugo zaragoza 
∗ 
yahoo research barcelona 
ocata 
barcelona spain 
hugoz es yahoo-inc com 
michael taylor 
microsoft research 
 j j thompson ave 
cambridge cb fb uk 
mitaylor microsoft com 
abstract 
this paper describes a large-scale evaluation of the 
effectiveness of hits in comparison with other link-based 
ranking algorithms when used in combination with a 
state-ofthe-art text retrieval algorithm exploiting anchor text we 
quantified their effectiveness using three common 
performance measures the mean reciprocal rank the mean 
average precision and the normalized discounted cumulative 
gain measurements the evaluation is based on two large 
data sets a breadth-first search crawl of million web 
pages containing billion hyperlinks and referencing 
billion distinct urls and a set of queries sampled 
from a query log each query having on average 
results about of which were labeled by judges we found 
that hits outperforms pagerank but is about as 
effective as web-page in-degree the same holds true when any 
of the link-based features are combined with the text 
retrieval algorithm finally we studied the relationship 
between query specificity and the effectiveness of selected 
features and found that link-based features perform better for 
general queries whereas bm f performs better for specific 
queries 
categories and subject descriptors 
h information search and retrieval information 
storage and retrieval-search process selection process 
general terms 
algorithms measurement experimentation 
 introduction 
link graph features such as in-degree and pagerank have 
been shown to significantly improve the performance of text 
retrieval algorithms on the web the hits algorithm is also 
believed to be of interest for web search to some degree 
one may expect hits to be more informative that other 
link-based features because it is query-dependent it tries to 
measure the interest of pages with respect to a given query 
however it remains unclear today whether there are 
practical benefits of hits over other link graph measures this 
is even more true when we consider that modern retrieval 
algorithms used on the web use a document representation 
which incorporates the document s anchor text i e the text 
of incoming links this at least to some degree takes the 
link graph into account in a query-dependent manner 
comparing hits to pagerank or in-degree empirically is 
no easy task there are two main difficulties scale and 
relevance scale is important because link-based features are 
known to improve in quality as the document graph grows 
if we carry out a small experiment our conclusions won t 
carry over to large graphs such as the web however 
computing hits efficiently on a graph the size of a realistic web 
crawl is extraordinarily difficult relevance is also crucial 
because we cannot measure the performance of a feature in 
the absence of human judgments what is crucial is ranking 
at the top of the ten or so documents that a user will peruse 
to our knowledge this paper is the first attempt to 
evaluate hits at a large scale and compare it to other link-based 
features with respect to human evaluated judgment 
our results confirm many of the intuitions we have about 
link-based features and their relationship to text retrieval 
methods exploiting anchor text this is reassuring in the 
absence of a theoretical model capable of tying these 
measures with relevance the only way to validate our intuitions 
is to carry out realistic experiments however we were quite 
surprised to find that hits a query-dependent feature is 
about as effective as web page in-degree the most 
simpleminded query-independent link-based feature this 
continues to be true when the link-based features are combined 
with a text retrieval algorithm exploiting anchor text 
the remainder of this paper is structured as follows 
section surveys related work section describes the data 
sets we used in our study section reviews the 
performance measures we used sections and describe the 
pagerank and hits algorithms in more detail and sketch 
the computational infrastructure we employed to carry out 
large scale experiments section presents the results of our 
evaluations and section offers concluding remarks 
 related work 
the idea of using hyperlink analysis for ranking web search 
results arose around and manifested itself in the hits 
 and pagerank algorithms the popularity 
of these two algorithms and the phenomenal success of the 
google search engine which uses pagerank have spawned 
a large amount of subsequent research 
there are numerous attempts at improving the 
effectiveness of hits and pagerank query-dependent link-based 
ranking algorithms inspired by hits include salsa 
randomized hits and phits to name a few 
query-independent link-based ranking algorithms inspired 
by pagerank include trafficrank blockrank and 
trustrank and many others 
another line of research is concerned with analyzing the 
mathematical properties of hits and pagerank for 
example borodin et al investigated various theoretical 
properties of pagerank hits salsa and phits including 
their similarity and stability while bianchini et al 
studied the relationship between the structure of the web graph 
and the distribution of pagerank scores and langville and 
meyer examined basic properties of pagerank such as 
existence and uniqueness of an eigenvector and convergence of 
power iteration 
given the attention that has been paid to improving the 
effectiveness of pagerank and hits and the thorough 
studies of the mathematical properties of these algorithms it is 
somewhat surprising that very few evaluations of their 
effectiveness have been published we are aware of two studies 
that have attempted to formally evaluate the effectiveness of 
hits and of pagerank amento et al employed 
quantitative measures but based their experiments on the result 
sets of just queries and the web-graph induced by topical 
crawls around the result set of each query a more recent 
study by borodin et al is based on queries result sets 
of pages per query obtained from google and a 
neighborhood graph derived by retrieving in-links per result 
from google by contrast our study is based on over 
queries and a web graph covering billion urls 
 our data sets 
our evaluation is based on two data sets a large web 
graph and a substantial set of queries with associated results 
some of which were labeled by human judges 
our web graph is based on a web crawl that was 
conducted in a breadth-first-search fashion and successfully 
retrieved html pages these pages contain 
 hyperlinks after eliminating duplicate 
hyperlinks embedded in the same web page which refer to 
a total of urls thus at the end of the 
crawl there were urls in the frontier set 
of the crawler that had been discovered but not yet 
downloaded the mean out-degree of crawled web pages is 
the mean in-degree of discovered pages whether crawled or 
not is also it is worth pointing out that there is a 
lot more variance in in-degrees than in out-degrees some 
popular pages have millions of incoming links as we will 
see this property affects the computational cost of hits 
our query set was produced by sampling queries 
from the msn search query log and retrieving a total of 
 result urls for these queries using commercial 
search engine technology or about results per query 
on average it is important to point out that our billion 
url web graph does not cover all these result urls in 
fact only of the result urls about were 
covered by the graph 
 of the results in the query set about of 
all results or about results per query were rated by 
human judges as to their relevance to the given query and 
labeled on a six-point scale the labels being definitive 
excellent good fair bad and detrimental 
results were selected for judgment based on their commercial 
search engine placement in other words the subset of 
labeled results is not random but biased towards documents 
considered relevant by pre-existing ranking algorithms 
involving a human in the evaluation process is extremely 
cumbersome and expensive however human judgments are 
crucial for the evaluation of search engines this is so 
because no document features have been found yet that can 
effectively estimate the relevance of a document to a user 
query since content-match features are very unreliable and 
even more so link features as we will see we need to ask 
a human to evaluate the results in order to compare the 
quality of features 
evaluating the retrieval results from document scores and 
human judgments is not trivial and has been the subject of 
many investigations in the ir community a good 
performance measure should correlate with user satisfaction 
taking into account that users will dislike having to delve deep 
in the results to find relevant documents for this reason 
standard correlation measures such as the correlation 
coefficient between the score and the judgment of a document 
or order correlation measures such as kendall tau between 
the score and judgment induced orders are not adequate 
 measuring performance 
in this study we quantify the effectiveness of various 
ranking algorithms using three measures ndcg mrr and 
map 
the normalized discounted cumulative gains ndcg 
measure discounts the contribution of a document to the 
overall score as the document s rank increases assuming 
that the best document has the lowest rank such a 
measure is particularly appropriate for search engines as studies 
have shown that search engine users rarely consider anything 
beyond the first few results ndcg values are 
normalized to be between and with being the ndcg of a 
perfect ranking scheme that completely agrees with the 
assessment of the human judges the discounted 
cumulative gain at a particular rank-threshold t dcg t is 
defined to be 
pt 
j 
 
log j 
 
 r j 
− 
 
 where r j is the 
rating detrimental bad fair good excellent 
and definitive at rank j the ndcg is computed by 
dividing the dcg of a ranking by the highest possible dcg 
that can be obtained for that query finally the ndgcs of 
all queries in the query set are averaged to produce a mean 
ndcg 
the reciprocal rank rr of the ranked result set of a 
query is defined to be the reciprocal value of the rank of the 
highest-ranking relevant document in the result set the rr 
at rank-threshold t is defined to be if none of the 
highestranking t documents is relevant the mean reciprocal rank 
 mrr of a query set is the average reciprocal rank of all 
queries in the query set 
given a ranked set of n results let rel i be if the result 
at rank i is relevant and otherwise the precision p j 
at rank j is defined to be 
j 
pj 
i rel i i e the fraction 
of the relevant results among the j highest-ranking results 
the average precision ap at rank-threshold k is defined to 
be 
pk 
i p i rel i 
pn 
i 
rel i 
 the mean average precision map of a 
query set is the mean of the average precisions of all queries 
in the query set 
the above definitions of mrr and map rely on the notion 
of a relevant result we investigated two definitions of 
relevance one where all documents rated fair or better were 
deemed relevant and one were all documents rated good 
or better were deemed relevant for reasons of space we 
only report map and mrr values computed using the 
latter definition using the former definition does not change 
the qualitative nature of our findings similarly we 
computed ndcg map and mrr values for a wide range of 
rank-thresholds we report results here at rank again 
changing the rank-threshold never led us to different 
conclusions 
recall that over of documents are unlabeled we 
chose to treat all these documents as irrelevant to the query 
for some queries however not all relevant documents have 
been judged this introduces a bias into our evaluation 
features that bring new documents to the top of the rank 
may be penalized this will be more acute for features less 
correlated to the pre-existing commercial ranking algorithms 
used to select documents for judgment on the other hand 
most queries have few perfect relevant documents i e home 
page or item searches and they will most often be within 
the judged set 
 computing pagerank on a large 
web graph 
pagerank is a query-independent measure of the 
importance of web pages based on the notion of peer-endorsement 
a hyperlink from page a to page b is interpreted as an 
endorsement of page b s content by page a s author the 
following recursive definition captures this notion of 
endorsement 
r v 
x 
 u v ∈e 
r u 
out u 
where r v is the score importance of page v u v is an 
edge hyperlink from page u to page v contained in the 
edge set e of the web graph and out u is the out-degree 
 number of embedded hyperlinks of page u however this 
definition suffers from a severe shortcoming in the 
fixedpoint of this recursive equation only edges that are part of 
a strongly-connected component receive a non-zero score in 
order to overcome this deficiency page et al grant each page 
a guaranteed minimum score giving rise to the definition 
of standard pagerank 
r v 
d 
 v 
 − d 
x 
 u v ∈e 
r u 
out u 
where v is the size of the vertex set the number of known 
web pages and d is a damping factor typically set to be 
between and 
assuming that scores are normalized to sum up to 
pagerank can be viewed as the stationary probability 
distribution of a random walk on the web graph where at each 
step of the walk the walker with probability − d moves 
from its current node u to a neighboring node v and with 
probability d selects a node uniformly at random from all 
nodes in the graph and jumps to it in the limit the random 
walker is at node v with probability r v 
one issue that has to be addressed when implementing 
pagerank is how to deal with sink nodes nodes that do 
not have any outgoing links one possibility would be to 
select another node uniformly at random and transition to 
it this is equivalent to adding edges from each sink nodes 
to all other nodes in the graph we chose the alternative 
approach of introducing a single phantom node each sink 
node has an edge to the phantom node and the phantom 
node has an edge to itself 
in practice pagerank scores can be computed using power 
iteration since pagerank is query-independent the 
computation can be performed off-line ahead of query time this 
property has been key to pagerank s success since it is a 
challenging engineering problem to build a system that can 
perform any non-trivial computation on the web graph at 
query time 
in order to compute pagerank scores for all billion 
nodes in our web graph we implemented a distributed 
version of pagerank the computation consists of two distinct 
phases in the first phase the link files produced by the web 
crawler which contain page urls and their associated link 
urls in textual form are partitioned among the machines 
in the cluster used to compute pagerank scores and 
converted into a more compact format along the way 
specifically urls are partitioned across the machines in the 
cluster based on a hash of the urls host component and each 
machine in the cluster maintains a table mapping the url 
to a -bit integer the integers are drawn from a densely 
packed space so as to make suitable indices into the array 
that will later hold the pagerank scores the system then 
translates our log of pages and their associated hyperlinks 
into a compact representation where both page urls and 
link urls are represented by their associated -bit 
integers hashing the host component of the urls guarantees 
that all urls from the same host are assigned to the same 
machine in our scoring cluster since over of all 
hyperlinks on the web are relative that is are between two pages 
on the same host this property greatly reduces the amount 
of network communication required by the second stage of 
the distributed scoring computation 
the second phase performs the actual pagerank power 
iteration both the link data and the current pagerank 
vector reside on disk and are read in a streaming fashion 
while the new pagerank vector is maintained in memory 
we represent pagerank scores as -bit floating point 
numbers pagerank contributions to pages assigned to remote 
machines are streamed to the remote machine via a tcp 
connection 
we used a three-machine cluster each machine equipped 
with gb of ram to compute standard pagerank scores 
for all billion urls that were contained in our web 
graph we used a damping factor of and performed 
power iterations starting at iteration the l∞ norm of 
the change in the pagerank vector from one iteration to the 
next had stopped decreasing indicating that we had reached 
as much of a fixed point as the limitations of -bit floating 
point arithmetic would allow 
 
 
 
 
 
 
number of back-links sampled per result 
ndcg  
hits-aut-all 
hits-aut-ih 
hits-aut-id 
 
 
 
 
 
number of back-links sampled per result 
map  
hits-aut-all 
hits-aut-ih 
hits-aut-id 
 
 
 
 
 
 
 
 
 
number of back-links sampled per result 
mrr  
hits-aut-all 
hits-aut-ih 
hits-aut-id 
figure effectiveness of authority scores computed using different parameterizations of hits 
a post-processing phase uses the final pagerank vectors 
 one per machine and the table mapping urls to -bit 
integers representing indices into each pagerank vector to 
score the result url in our query log as mentioned above 
our web graph covered of the result 
urls these urls were annotated with their computed 
pagerank score all other urls received a score of 
 hits 
hits unlike pagerank is a query-dependent ranking 
algorithm hits which stands for hypertext induced topic 
search is based on the following two intuitions first 
hyperlinks can be viewed as topical endorsements a hyperlink 
from a page u devoted to topic t to another page v is likely 
to endorse the authority of v with respect to topic t second 
the result set of a particular query is likely to have a certain 
amount of topical coherence therefore it makes sense to 
perform link analysis not on the entire web graph but rather 
on just the neighborhood of pages contained in the result 
set since this neighborhood is more likely to contain 
topically relevant links but while the set of nodes immediately 
reachable from the result set is manageable given that most 
pages have only a limited number of hyperlinks embedded 
into them the set of pages immediately leading to the result 
set can be enormous for this reason kleinberg suggests 
sampling a fixed-size random subset of the pages linking to 
any high-indegree page in the result set moreover 
kleinberg suggests considering only links that cross host 
boundaries the rationale being that links between pages on the 
same host intrinsic links are likely to be navigational or 
nepotistic and not topically relevant 
given a web graph v e with vertex set v and edge 
set e ⊆ v × v and the set of result urls to a query 
 called the root set r ⊆ v as input hits computes a 
neighborhood graph consisting of a base set b ⊆ v the 
root set and some of its neighboring vertices and some of 
the edges in e induced by b in order to formalize the 
definition of the neighborhood graph it is helpful to first 
introduce a sampling operator and the concept of a 
linkselection predicate 
given a set a the notation sn a draws n elements 
uniformly at random from a sn a a if a ≤ n 
a link section predicate p takes an edge u v ∈ e in 
this study we use the following three link section predicates 
all u v ⇔ true 
ih u v ⇔ host u host v 
id u v ⇔ domain u domain v 
where host u denotes the host of url u and domain u 
denotes the domain of url u so all is true for all links 
whereas ih is true only for inter-host links and id is true 
only for inter-domain links 
the outlinked-set op 
of the root set r w r t a 
linkselection predicate p is defined to be 
op 
 
 
u∈r 
{v ∈ v u v ∈ e ∧ p u v } 
the inlinking-set ip 
s of the root set r w r t a link-selection 
predicate p and a sampling value s is defined to be 
ip 
s 
 
v∈r 
ss {u ∈ v u v ∈ e ∧ p u v } 
the base set bp 
s of the root set r w r t p and s is defined 
to be 
bp 
s r ∪ ip 
s ∪ op 
the neighborhood graph bp 
s np 
s has the base set bp 
s as 
its vertex set and an edge set np 
s containing those edges in 
e that are covered by bp 
s and permitted by p 
np 
s { u v ∈ e u ∈ bp 
s ∧ v ∈ bp 
s ∧ p u v } 
to simplify notation we write b to denote bp 
s and n to 
denote np 
s 
for each node u in the neighborhood graph hits 
computes two scores an authority score a u estimating how 
authoritative u is on the topic induced by the query and a 
hub score h u indicating whether u is a good reference to 
many authoritative pages this is done using the following 
algorithm 
 for all u ∈ b do h u 
q 
 
 b 
 a u 
q 
 
 b 
 
 repeat until h and a converge 
 a for all v ∈ b a v 
p 
 u v ∈n h u 
 b for all u ∈ b h u 
p 
 u v ∈n a v 
 c h h a a 
where x normalizes the vector x to unit length in 
euclidean space i e the squares of its elements sum up to 
in practice implementing a system that can compute hits 
within the time constraints of a major search engine where 
the peak query load is in the thousands of queries per second 
and the desired query response time is well below one 
second is a major engineering challenge among other things 
the web graph cannot reasonably be stored on disk since 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
bm f 
degree-in-id 
degree-in-ih 
hits-aut-id- 
hits-aut-ih- 
degree-in-all 
pagerank 
hits-aut-all- 
hits-hub-all- 
hits-hub-ih- 
hits-hub-id- 
degree-out-all 
degree-out-ih 
degree-out-id 
random 
ndcg  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
bm f 
hits-aut-id- 
degree-in-id 
hits-aut-ih- 
degree-in-ih 
degree-in-all 
pagerank 
hits-aut-all- 
hits-hub-all- 
hits-hub-ih- 
hits-hub-id- 
degree-out-all 
degree-out-ih 
degree-out-id 
random 
map  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
bm f 
hits-aut-id- 
hits-aut-ih- 
degree-in-id 
degree-in-ih 
degree-in-all 
hits-aut-all- 
pagerank 
hits-hub-all- 
hits-hub-ih- 
hits-hub-id- 
degree-out-all 
degree-out-ih 
degree-out-id 
random 
mrr  
figure effectiveness of different features 
seek times of modern hard disks are too slow to retrieve the 
links within the time constraints and the graph does not fit 
into the main memory of a single machine even when using 
the most aggressive compression techniques 
in order to experiment with hits and other 
query-dependent link-based ranking algorithms that require non-regular 
accesses to arbitrary nodes and edges in the web graph we 
implemented a system called the scalable hyperlink store 
or shs for short shs is a special-purpose database 
distributed over an arbitrary number of machines that keeps a 
highly compressed version of the web graph in memory and 
allows very fast lookup of nodes and edges on our 
hardware it takes an average of microseconds to map a url 
to a -bit integer handle called a uid microseconds to 
look up all incoming or outgoing link uids associated with 
a page uid and microseconds to map a uid back to a 
url the last functionality not being required by hits 
the rpc overhead is about microseconds but the shs 
api allows many lookups to be batched into a single rpc 
request 
we implemented the hits algorithm using the shs 
infrastructure we compiled three shs databases one 
containing all billion links in our web graph all one 
containing only links between pages that are on different hosts 
 ih for inter-host and one containing only links between 
pages that are on different domains id we consider two 
urls to belong to different hosts if the host portions of the 
urls differ in other words we make no attempt to 
determine whether two distinct symbolic host names refer to 
the same computer and we consider a domain to be the 
name purchased from a registrar for example we consider 
news bbc co uk and www bbc co uk to be different hosts 
belonging to the same domain using each of these databases 
we computed hits authority and hub scores for various 
parameterizations of the sampling operator s sampling 
between and back-links of each page in the root set 
result urls that were not covered by our web graph 
automatically received authority and hub scores of since they 
were not connected to any other nodes in the neighborhood 
graph and therefore did not receive any endorsements 
we performed forty-five different hits computations each 
combining one of the three link selection predicates all ih 
and id with a sampling value for each combination we 
loaded one of the three databases into an shs system 
running on six machines each equipped with gb of ram 
and computed hits authority and hub scores one query 
at a time the longest-running combination using the all 
database and sampling back-links of each root set 
vertex required seconds to process the entire query set 
or about seconds per query on average 
 experimental results 
for a given query q we need to rank the set of documents 
satisfying q the result set of q our hypothesis is that 
good features should be able to rank relevant documents in 
this set higher than non-relevant ones and this should result 
in an increase in each performance measure over the query 
set we are specifically interested in evaluating the 
usefulness of hits and other link-based features in principle we 
could do this by sorting the documents in each result set by 
their feature value and compare the resulting ndcgs we 
call this ranking with isolated features 
let us first examine the relative performance of the 
different parameterizations of the hits algorithm we 
examined recall that we computed hits for each combination 
of three link section schemes - all links all inter-host links 
only ih and inter-domain links only id - with back-link 
sampling values ranging from to figure shows the 
impact of the number of sampled back-links on the retrieval 
performance of hits authority scores each graph is 
associated with one performance measure the horizontal axis 
of each graph represents the number of sampled back-links 
the vertical axis represents performance under the 
appropriate measure and each curve depicts a link selection scheme 
the id scheme slightly outperforms ih and both vastly 
outperform the all scheme - eliminating nepotistic links pays 
off the performance of the all scheme increases as more 
back-links of each root set vertex are sampled while the 
performance of the id and ih schemes peaks at between 
and samples and then plateaus or even declines 
depending on the performance measure 
having compared different parameterizations of hits we 
will now fix the number of sampled back-links at and 
compare the three link selection schemes against other 
isolated features pagerank in-degree and out-degree 
counting links of all pages of different hosts only and of different 
domains only all ih and id datasets respectively and a 
text retrieval algorithm exploiting anchor text bm f 
bm f is a state-of-the art ranking function solely based on 
textual content of the documents and their associated 
anchor texts bm f is a descendant of bm that combines 
the different textual fields of a document namely title body 
and anchor text this model has been shown to be one of 
the best-performing web search scoring functions over the 
last few years bm f has a number of free 
parameters per field in our case we used the parameter values 
described in 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
degree-in-id 
degree-in-ih 
degree-in-all 
hits-aut-ih- 
hits-aut-all- 
pagerank 
hits-aut-id- 
degree-out-all 
hits-hub-all- 
degree-out-ih 
hits-hub-ih- 
degree-out-id 
hits-hub-id- 
bm f 
ndcg  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
degree-in-ih 
degree-in-id 
degree-in-all 
hits-aut-ih- 
hits-aut-all- 
hits-aut-id- 
pagerank 
hits-hub-all- 
degree-out-ih 
hits-hub-id- 
degree-out-all 
degree-out-id 
hits-hub-ih- 
bm f 
map  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
degree-in-id 
degree-in-ih 
degree-in-all 
hits-aut-ih- 
hits-aut-all- 
pagerank 
hits-aut-id- 
degree-out-all 
hits-hub-all- 
degree-out-ih 
hits-hub-ih- 
degree-out-id 
hits-hub-id- 
bm f 
mrr  
figure effectiveness measures for linear combinations of link-based features with bm f 
figure shows the ndcg mrr and map measures 
of these features again all performance measures and 
for all rank-thresholds we explored agree as expected 
bm f outperforms all link-based features by a large 
margin the link-based features are divided into two groups 
with a noticeable performance drop between the groups 
the better-performing group consists of the features that 
are based on the number and or quality of incoming links 
 in-degree pagerank and hits authority scores and the 
worse-performing group consists of the features that are 
based on the number and or quality of outgoing links 
 outdegree and hits hub scores in the group of features based 
on incoming links features that ignore nepotistic links 
perform better than their counterparts using all links 
moreover using only inter-domain id links seems to be marginally 
better than using inter-host ih links 
the fact that features based on outgoing links 
underperform those based on incoming links matches our 
expectations if anything it is mildly surprising that outgoing links 
provide a useful signal for ranking at all on the other 
hand the fact that in-degree features outperform pagerank 
under all measures is quite surprising a possible 
explanation is that link-spammers have been targeting the published 
pagerank algorithm for many years and that this has led 
to anomalies in the web graph that affect pagerank but 
not other link-based features that explore only a distance- 
neighborhood of the result set likewise it is surprising that 
simple query-independent features such as in-degree which 
might estimate global quality but cannot capture relevance 
to a query would outperform query-dependent features such 
as hits authority scores 
however we cannot investigate the effect of these features 
in isolation without regard to the overall ranking function 
for several reasons first features based on the textual 
content of documents as opposed to link-based features are 
the best predictors of relevance second link-based features 
can be strongly correlated with textual features for several 
reasons mainly the correlation between in-degree and 
numfeature transform function 
bm f t s s 
pagerank t s log s · − 
 
degree-in- t s log s · − 
 
degree-out- t s log s · 
 
hits-aut- t s log s · − 
 
hits-hub- t s log s · − 
 
table near-optimal feature transform functions 
ber of textual anchor matches 
therefore one must consider the effect of link-based 
features in combination with textual features otherwise we 
may find a link-based feature that is very good in isolation 
but is strongly correlated with textual features and results 
in no overall improvement and vice versa we may find a 
link-based feature that is weak in isolation but significantly 
improves overall performance 
for this reason we have studied the combination of the 
link-based features above with bm f all feature 
combinations were done by considering the linear combination of two 
features as a document score using the formula score d pn 
i witi fi d where d is a document or 
documentquery pair in the case of bm f fi d for ≤ i ≤ n is a 
feature extracted from d ti is a transform and wi is a free 
scalar weight that needs to be tuned we chose transform 
functions that we empirically determined to be well-suited 
table shows the chosen transform functions 
this type of linear combination is appropriate if we 
assume features to be independent with respect to relevance 
and an exponential model for link features as discussed 
in we tuned the weights by selecting a random 
subset of queries from the query set used an iterative 
refinement process to find weights that maximized a given 
performance measure on that training set and used the 
remaining queries to measure the performance of the 
thus derived scoring functions 
we explored the pairwise combination of bm f with 
every link-based scoring function figure shows the ndcg 
mrr and map measures of these feature combinations 
together with a baseline bm f score the right-most bar 
in each graph which was computed using the same subset 
of queries that were used as the test set for the 
feature combinations regardless of the performance measure 
applied we can make the following general observations 
 combining any of the link-based features with bm f 
results in a substantial performance improvement over 
bm f in isolation 
 the combination of bm f with features based on 
incoming links pagerank in-degree and hits 
authority scores performs substantially better than the 
combination with features based on outgoing links hits 
hub scores and out-degree 
 the performance differences between the various 
combinations of bm f with features based on incoming 
links is comparatively small and the relative ordering 
of feature combinations is fairly stable across the 
 
 
 
 
 
 
 
 
 
map  
 
bm fnorm pagerank degree-in-id hits-aut-id- 
figure effectiveness measures for selected 
isolated features broken down by query specificity 
ferent performance measures used however the 
combination of bm f with any in-degree variant and in 
particular with id in-degree consistently outperforms 
the combination of bm f with pagerank or hits 
authority scores and can be computed much easier 
and faster 
finally we investigated whether certain features are 
better for some queries than for others particularly we are 
interested in the relationship between the specificity of a query 
and the performance of different ranking features the most 
straightforward measure of the specificity of a query q would 
be the number of documents in a search engine s corpus that 
satisfy q unfortunately the query set available to us did 
not contain this information therefore we chose to 
approximate the specificity of q by summing up the inverse 
document frequencies of the individual query terms 
comprising q the inverse document frequency idf of a term 
t with respect to a corpus c is defined to be logn doc t 
where doc t is the number of documents in c containing t 
and n is the total number of documents in c by summing 
up the idfs of the query terms we make the flawed 
assumption that the individual query terms are independent of 
each other however while not perfect this approximation 
is at least directionally correct 
we broke down our query set into buckets each bucket 
associated with an interval of query idf values and we 
computed performance metrics for all ranking functions applied 
 in isolation to the queries in each bucket in order to 
keep the graphs readable we will not show the performance 
of all the features but rather restrict ourselves to the four 
most interesting ones pagerank id hits authority scores 
id in-degree and bm f figure shows the map  for 
all query specificity buckets buckets on the far left of 
each graph represent very general queries buckets on the far 
right represent very specific queries the figures on the 
upper x axis of each graph show the number of queries in each 
bucket e g the right-most bucket contains queries 
bm f performs best for medium-specific queries peaking 
at the buckets representing the idf sum interval 
by comparison hits peaks at the bucket representing the 
idf sum interval and pagerank and in-degree peak at 
the bucket representing the interval i e more general 
queries 
 conclusions and future work 
this paper describes a large-scale evaluation of the 
effectiveness of hits in comparison with other link-based 
ranking algorithms in particular pagerank and in-degree 
when applied in isolation or in combination with a text 
retrieval algorithm exploiting anchor text bm f 
evaluation is carried out with respect to a large number of human 
evaluated queries using three different measures of 
effectiveness ndcg mrr and map evaluating link-based 
features in isolation we found that web page in-degree 
outperforms pagerank and is about as effwective as hits 
authority scores hits hub scores and web page out-degree are 
much less effective ranking features but still outperform a 
random ordering a linear combination of any link-based 
features with bm f produces a significant improvement in 
performance and there is a clear difference between 
combining bm f with a feature based on incoming links 
 indegree pagerank or hits authority scores and a feature 
based on outgoing links hits hub scores and out-degree 
but within those two groups the precise choice of link-based 
feature matters relatively little 
we believe that the measurements presented in this paper 
provide a solid evaluation of the best well-known link-based 
ranking schemes there are many possible variants of these 
schemes and many other link-based ranking algorithms have 
been proposed in the literature hence we do not claim this 
work to be the last word on this subject but rather the 
first step on a long road future work includes evaluation 
of different parameterizations of pagerank and hits in 
particular we would like to study the impact of changes 
to the pagerank damping factor on effectiveness the 
impact of various schemes meant to counteract the effects of 
link spam and the effect of weighing hyperlinks differently 
depending on whether they are nepotistic or not going 
beyond pagerank and hits we would like to measure the 
effectiveness of other link-based ranking algorithms such as 
salsa finally we are planning to experiment with more 
complex feature combinations 
 references 
 b amento l terveen and w hill does authority 
mean quality predicting expert quality ratings of 
web documents in proc of the rd annual 
international acm sigir conference on research 
and development in information retrieval pages 
 - 
 m bianchini m gori and f scarselli inside 
pagerank acm transactions on internet technology 
 - 
 a borodin g o roberts and j s rosenthal 
finding authorities and hubs from link structures on 
the world wide web in proc of the th 
international world wide web conference pages 
 - 
 a borodin g o roberts j s rosenthal and 
p tsaparas link analysis ranking algorithms 
theory and experiments acm transactions on 
interet technology - 
 s brin and l page the anatomy of a large-scale 
hypertextual web search engine computer networks 
and isdn systems - - 
 c burges t shaked e renshaw a lazier 
m deeds n hamilton and g hullender learning 
to rank using gradient descent in proc of the nd 
international conference on machine learning pages 
 - new york ny usa acm press 
 d cohn and h chang learning to probabilistically 
identify authoritative documents in proc of the th 
international conference on machine learning pages 
 - 
 n craswell s robertson h zaragoza and 
m taylor relevance weighting for query independent 
evidence in proc of the th annual international 
acm sigir conference on research and 
development in information retrieval pages - 
 
 e garfield citation analysis as a tool in journal 
evaluation science - 
 z gy¨ongyi and h garcia-molina web spam 
taxonomy in st international workshop on 
adversarial information retrieval on the web 
 z gy¨ongyi h garcia-molina and j pedersen 
combating web spam with trustrank in proc of the 
 th international conference on very large 
databases pages - 
 b j jansen a spink j bateman and t saracevic 
real life information retrieval a study of user queries 
on the web acm sigir forum - 
 k j¨arvelin and j kek¨al¨ainen cumulated gain-based 
evaluation of ir techniques acm transactions on 
information systems - 
 s d kamvar t h haveliwala c d manning and 
g h golub extrapolation methods for accelerating 
pagerank computations in proc of the th 
international world wide web conference pages 
 - 
 m m kessler bibliographic coupling between 
scientific papers american documentation 
 - 
 j m kleinberg authoritative sources in a 
hyperlinked environment in proc of the th annual 
acm-siam symposium on discrete algorithms pages 
 - 
 j m kleinberg authoritative sources in a 
hyperlinked environment journal of the acm 
 - 
 a n langville and c d meyer deeper inside 
pagerank internet mathematics - 
 r lempel and s moran the stochastic approach for 
link-structure analysis salsa and the tkc effect 
computer networks and isdn systems 
 - - 
 a y ng a x zheng and m i jordan stable 
algorithms for link analysis in proc of the th 
annual international acm sigir conference on 
research and development in information retrieval 
pages - 
 l page s brin r motwani and t winograd the 
pagerank citation ranking bringing order to the 
web technical report stanford digital library 
technologies project 
 j a tomlin a new paradigm for ranking pages on 
the world wide web in proc of the th 
international world wide web conference pages 
 - 
 t upstill n craswell and d hawking predicting 
fame and fortune pagerank or indegree in proc of 
the australasian document computing symposium 
pages - 
 h zaragoza n craswell m taylor s saria and 
s robertson microsoft cambridge at trec- 
web and hard tracks in proc of the th text 
retrieval conference 
estimating the global pagerank of web communities 
jason v davis 
dept of computer sciences 
university of texas at austin 
austin tx 
jdavis cs utexas edu 
inderjit s dhillon 
dept of computer sciences 
university of texas at austin 
austin tx 
inderjit cs utexas edu 
abstract 
localized search engines are small-scale systems that index 
a particular community on the web they offer several 
benefits over their large-scale counterparts in that they are 
relatively inexpensive to build and can provide more precise 
and complete search capability over their relevant domains 
one disadvantage such systems have over large-scale search 
engines is the lack of global pagerank values such 
information is needed to assess the value of pages in the localized 
search domain within the context of the web as a whole 
in this paper we present well-motivated algorithms to 
estimate the global pagerank values of a local domain the 
algorithms are all highly scalable in that given a local 
domain of size n they use o n resources that include 
computation time bandwidth and storage we test our methods 
across a variety of localized domains including site-specific 
domains and topic-specific domains we demonstrate that 
by crawling as few as n or n additional pages our methods 
can give excellent global pagerank estimates 
categories and subject descriptors 
h information storage and retrieval 
information search and retrieval g numerical analysis 
numerical linear algebra g probability and 
statistics markov processes 
general terms 
pagerank markov chain stochastic complementation 
 introduction 
localized search engines are small-scale search engines 
that index only a single community of the web such 
communities can be site-specific domains such as pages within 
the cs utexas edu domain or topic-related 
communitiesfor example political websites compared to the web graph 
crawled and indexed by large-scale search engines the size 
of such local communities is typically orders of magnitude 
smaller consequently the computational resources needed 
to build such a search engine are also similarly lighter by 
restricting themselves to smaller more manageable sections 
of the web localized search engines can also provide more 
precise and complete search capabilities over their respective 
domains 
one drawback of localized indexes is the lack of global 
information needed to compute link-based rankings the 
pagerank algorithm has proven to be an effective such 
measure in general the pagerank of a given page is 
dependent on pages throughout the entire web graph in the 
context of a localized search engine if the pageranks are 
computed using only the local subgraph then we would 
expect the resulting pageranks to reflect the perceived 
popularity within the local community and not of the web as a 
whole for example consider a localized search engine that 
indexes political pages with conservative views a person 
wishing to research the opinions on global warming within 
the conservative political community may encounter 
numerous such opinions across various websites if only local 
pagerank values are available then the search results will reflect 
only strongly held beliefs within the community however if 
global pageranks are also available then the results can 
additionally reflect outsiders views of the conservative 
community those documents that liberals most often access within 
the conservative community 
thus for many localized search engines incorporating 
global pageranks can improve the quality of search results 
however the number of pages a local search engine indexes 
is typically orders of magnitude smaller than the number of 
pages indexed by their large-scale counterparts localized 
search engines do not have the bandwidth storage capacity 
or computational power to crawl download and compute 
the global pageranks of the entire web in this work we 
present a method of approximating the global pageranks of 
a local domain while only using resources of the same 
order as those needed to compute the pageranks of the local 
subgraph 
our proposed method looks for a supergraph of our local 
subgraph such that the local pageranks within this 
supergraph are close to the true global pageranks we construct 
this supergraph by iteratively crawling global pages on the 
current web frontier-i e global pages with inlinks from 
pages that have already been crawled in order to provide 
 
research track paper 
a good approximation to the global pageranks care must 
be taken when choosing which pages to crawl next in this 
paper we present a well-motivated page selection algorithm 
that also performs well empirically this algorithm is 
derived from a well-defined problem objective and has a 
running time linear in the number of local nodes 
we experiment across several types of local subgraphs 
including four topic related communities and several 
sitespecific domains to evaluate performance we measure the 
difference between the current global pagerank estimate 
and the global pagerank as a function of the number of 
pages crawled we compare our algorithm against several 
heuristics and also against a baseline algorithm that chooses 
pages at random and we show that our method outperforms 
these other methods finally we empirically demonstrate 
that given a local domain of size n we can provide good 
approximations to the global pagerank values by crawling 
at most n or n additional pages 
the paper is organized as follows section gives an 
overview of localized search engines and outlines their 
advantages over global search section provides background 
on the pagerank algorithm section formally defines our 
problem and section presents our page selection criteria 
and derives our algorithms section provides 
experimental results section gives an overview of related work and 
finally conclusions are given in section 
 localized search engines 
localized search engines index a single community of the 
web typically either a site-specific community or a 
topicspecific community localized search engines enjoy three 
major advantages over their large-scale counterparts they 
are relatively inexpensive to build they can offer more 
precise search capability over their local domain and they can 
provide a more complete index 
the resources needed to build a global search engine are 
enormous a study by lyman et al found that 
the  surface web publicly available static sites consists of 
 billion pages and that the average size of these pages is 
approximately kilobytes to download a crawl of this 
size approximately terabytes of space is needed for a 
researcher who wishes to build a search engine with access 
to a couple of workstations or a small server storage of this 
magnitude is simply not available however building a 
localized search engine over a web community of a hundred 
thousand pages would only require a few gigabytes of 
storage the computational burden required to support search 
queries over a database this size is more manageable as well 
we note that for topic-specific search engines the relevant 
community can be efficiently identified and downloaded by 
using a focused crawler 
for site-specific domains the local domain is readily 
available on their own web server this obviates the need for 
crawling or spidering and a complete and up-to-date 
index of the domain can thus be guaranteed this is in 
contrast to their large-scale counterparts which suffer from 
several shortcomings first crawling dynamically generated 
pages-pages in the  hidden web -has been the subject of 
research and is a non-trivial task for an external crawler 
second site-specific domains can enable the robots 
exclusion policy this prohibits external search engines crawlers 
from downloading content from the domain and an external 
search engine must instead rely on outside links and anchor 
text to index these restricted pages 
by restricting itself to only a specific domain of the 
internet a localized search engine can provide more precise 
search results consider the canonical ambiguous search 
query  jaguar which can refer to either the car 
manufacturer or the animal a scientist trying to research the 
habitat and evolutionary history of a jaguar may have better 
success using a finely tuned zoology-specific search engine 
than querying google with multiple keyword searches and 
wading through irrelevant results a method to learn 
better ranking functions for retrieval was recently proposed by 
radlinski and joachims and has been applied to various 
local domains including cornell university s website 
 pagerank overview 
the pagerank algorithm defines the importance of web 
pages by analyzing the underlying hyperlink structure of a 
web graph the algorithm works by building a markov chain 
from the link structure of the web graph and computing its 
stationary distribution one way to compute the 
stationary distribution of a markov chain is to find the limiting 
distribution of a random walk over the chain thus the 
pagerank algorithm uses what is sometimes referred to as 
the  random surfer model in each step of the random walk 
the  surfer either follows an outlink from the current page 
 i e the current node in the chain or jumps to a random 
page on the web 
we now precisely define the pagerank problem let u 
be an m × m adjacency matrix for a given web graph such 
that uji if page i links to page j and uji otherwise 
we define the pagerank matrix pu to be 
pu αud− 
u − α vet 
 
where du is the unique diagonal matrix such that ud− 
u 
is column stochastic α is a given scalar such that ≤ α ≤ 
e is the vector of all ones and v is a non-negative 
l normalized vector sometimes called the  random surfer 
vector note that the matrix d− 
u is well-defined only if each 
column of u has at least one non-zero entry-i e each page 
in the webgraph has at least one outlink in the presence of 
such  dangling nodes that have no outlinks one commonly 
used solution proposed by brin et al is to replace each 
zero column of u by a non-negative l -normalized vector 
the pagerank vector r is the dominant eigenvector of the 
pagerank matrix r pu r we will assume without loss of 
generality that r has an l -norm of one computationally 
r can be computed using the power method this method 
first chooses a random starting vector r 
 and iteratively 
multiplies the current vector by the pagerank matrix pu 
see algorithm in general each iteration of the power 
method can take o m 
 operations when pu is a dense 
matrix however in practice the number of links in a web 
graph will be of the order of the number of pages by 
exploiting the sparsity of the pagerank matrix the work per 
iteration can be reduced to o km where k is the average 
number of links per web page it has also been shown that 
the total number of iterations needed for convergence is 
proportional to α and does not depend on the size of the web 
graph finally the total space needed is also o km 
mainly to store the matrix u 
 
research track paper 
algorithm a linear time per iteration algorithm for 
computing pagerank 
computepr u 
input u adjacency matrix 
output r pagerank vector 
choose randomly an initial non-negative vector r 
such that r 
 
i ← 
repeat 
i ← i 
ν ← αud− 
u r i− 
{α is the random surfing 
probability} 
r i 
← ν − α v {v is the random surfer vector } 
until r i 
− r i− 
 δ {δ is the convergence threshold } 
r ← r i 
 problem definition 
given a local domain l let g be an n × n adjacency 
matrix for the entire connected component of the web that 
contains l such that gji if page i links to page j 
and gji otherwise without loss of generality we will 
partition g as 
g 
l gout 
lout gwithin 
 
where l is the n × n local subgraph corresponding to links 
inside the local domain lout is the subgraph that 
corresponds to links from the local domain pointing out to the 
global domain gout is the subgraph containing links from 
the global domain into the local domain and gwithin 
contains links within the global domain we assume that when 
building a localized search engine only pages inside the 
local domain are crawled and the links between these pages 
are represented by the subgraph l the links in lout are 
also known as these point from crawled pages in the local 
domain to uncrawled pages in the global domain 
as defined in equation pg is the pagerank matrix 
formed from the global graph g and we define the global 
pagerank vector of this graph to be g let the n-length 
vector p∗ 
be the l -normalized vector corresponding to the 
global pagerank of the pages in the local domain l 
p∗ 
 
el g 
elg 
 
where el i is the restriction matrix that selects 
the components from g corresponding to nodes in l let p 
denote the pagerank vector constructed from the local 
domain subgraph l in practice the observed local pagerank 
p and the global pagerank p∗ 
will be quite different one 
would expect that as the size of local matrix l approaches 
the size of global matrix g the global pagerank and the 
observed local pagerank will become more similar thus one 
approach to estimating the global pagerank is to crawl the 
entire global domain compute its pagerank and extract 
the pageranks of the local domain 
typically however n n i e the number of global 
pages is much larger than the number of local pages 
therefore crawling all global pages will quickly exhaust all local 
resources computational storage and bandwidth available 
to create the local search engine we instead seek a 
supergraph ˆf of our local subgraph l with size o n our goal 
algorithm the findglobalpr algorithm 
findglobalpr l lout t k 
input l zero-one adjacency matrix for the local 
domain lout zero-one outlink matrix from l to global 
subgraph as in t number of iterations k number of 
pages to crawl per iteration 
output ˆp an improved estimate of the global 
pagerank of l 
f ← l 
fout ← lout 
f ← computepr f 
for i to t 
{determine which pages to crawl next} 
pages ← selectnodes f fout f k 
crawl pages augment f and modify fout 
{update pageranks for new local domain} 
f ← computepr f 
end 
{extract pageranks of original local domain normalize} 
ˆp ← elf 
elf 
is to find such a supergraph ˆf with pagerank ˆf so that 
ˆf when restricted to l is close to p∗ 
 formally we seek to 
minimize 
globaldiff ˆf 
el 
ˆf 
el 
ˆf 
− p∗ 
 
 
we choose the l norm for measuring the error as it does 
not place excessive weight on outliers as the l norm does 
for example and also because it is the most commonly used 
distance measure in the literature for comparing pagerank 
vectors as well as for detecting convergence of the 
algorithm 
we propose a greedy framework given in algorithm 
for constructing ˆf initially f is set to the local subgraph 
l and the pagerank f of this graph is computed the 
algorithm then proceeds as follows first the selectnodes 
algorithm which we discuss in the next section is called 
and it returns a set of k nodes to crawl next from the set 
of nodes in the current crawl frontier fout these selected 
nodes are then crawled to expand the local subgraph f and 
the pageranks of this expanded graph are then recomputed 
these steps are repeated for each of t iterations finally 
the pagerank vector ˆp which is restricted to pages within 
the original local domain is returned given our 
computation bandwidth and memory restrictions we will assume 
that the algorithm will crawl at most o n pages since the 
pageranks are computed in each iteration of the algorithm 
which is an o n operation we will also assume that the 
number of iterations t is a constant of course the main 
challenge here is in selecting which set of k nodes to crawl 
next in the next section we formally define the problem 
and give efficient algorithms 
 node selection 
in this section we present node selection algorithms that 
operate within the greedy framework presented in the 
previous section we first give a well-defined criteria for the 
page selection problem and provide experimental evidence 
that this criteria can effectively identify pages that optimize 
our problem objective we then present our main 
al 
research track paper 
gorithmic contribution of the paper a method with linear 
running time that is derived from this page selection 
criteria finally we give an intuitive analysis of our algorithm in 
terms of  leaks and  flows we show that if only the  flow 
is considered then the resulting method is very similar to a 
widely used page selection heuristic 
 formulation 
for a given page j in the global domain we define the 
expanded local graph fj 
fj 
f s 
ut 
j 
 
where uj is the zero-one vector containing the outlinks from 
f into page j and s contains the inlinks from page j into 
the local domain note that we do not allow self-links in 
this framework in practice self-links are often removed as 
they only serve to inflate a given page s pagerank 
observe that the inlinks into f from node j are not known 
until after node j is crawled therefore we estimate this 
inlink vector as the expectation over inlink counts among 
the set of already crawled pages 
s 
f t 
e 
f t e 
 
in practice for any given page this estimate may not reflect 
the true inlinks from that page furthermore this 
expectation is sampled from the set of links within the crawled 
domain whereas a better estimate would also use links from 
the global domain however the latter distribution is not 
known to a localized search engine and we contend that the 
above estimate will on average be a better estimate than 
the uniform distribution for example 
let the pagerank of f be f we express the pagerank 
f 
j of the expanded local graph fj as 
f 
j 
 − xj fj 
xj 
 
where xj is the pagerank of the candidate global node j 
and fj is the l -normalized pagerank vector restricted to 
the pages in f 
since directly optimizing our problem goal requires 
knowing the global pagerank p∗ 
 we instead propose to crawl 
those nodes that will have the greatest influence on the 
pageranks of pages in the original local domain l 
influence j 
k∈l 
 fj k − f k 
 el fj − f 
experimentally the influence score is a very good predictor 
of our problem objective for each candidate global node 
j figure a shows the objective function value global diff fj 
as a function of the influence of page j the local domain 
used here is a crawl of conservative political pages we will 
provide more details about this dataset in section we 
observed similar results in other domains the correlation 
is quite strong implying that the influence criteria can 
effectively identify pages that improve the global pagerank 
estimate as a baseline figure b compares our 
objective with an alternative criteria outlink count the outlink 
count is defined as the number of outlinks from the local 
domain to page j the correlation here is much weaker 
 
 
 
 
 
influence 
objective 
 
 
 
 
 
outlink count 
objective 
 a b 
figure a the correlation between our influence 
page selection criteria and the actual objective 
function value is quite strong b this is in 
contrast to other criteria such as outlink count which 
exhibit a much weaker correlation 
 computation 
as described for each candidate global page j the 
influence score must be computed if fj is computed 
exactly for each global page j then the pagerank 
algorithm would need to be run for each of the o n such global 
pages j we consider resulting in an o n 
 computational 
cost for the node selection method thus computing the 
exact value of fj will lead to a quadratic algorithm and we 
must instead turn to methods of approximating this vector 
the algorithm we present works by performing one power 
method iteration used by the pagerank algorithm 
 algorithm the convergence rate for the pagerank algorithm 
has been shown to equal the random surfer probability α 
 given a starting vector x 
 if k pagerank iterations 
are performed the current pagerank solution x k 
satisfies 
x k 
− x∗ 
 o αk 
x 
− x∗ 
 
where x∗ 
is the desired pagerank vector therefore if only 
one iteration is performed choosing a good starting vector 
is necessary to achieve an accurate approximation 
we partition the pagerank matrix pfj corresponding to 
the × subgraph fj as 
pfj 
˜f ˜s 
˜ut 
j w 
 
where ˜f αf df diag uj − 
 − α 
e 
 
et 
 
˜s αs − α 
e 
 
 
˜uj α df diag uj − 
uj − α 
e 
 
 
w 
 − α 
 
 
and diag uj is the diagonal matrix with the i i th 
entry 
equal to one if the ith 
element of uj equals one and is zero 
otherwise we have assumed here that the random surfer 
vector is the uniform vector and that l has no  dangling 
links these assumptions are not necessary and serve only 
to simplify discussion and analysis 
a simple approach for estimating fj is the following first 
estimate the pagerank f 
j of fj by computing one 
pagerank iteration over the matrix pfj using the starting 
vector ν 
f 
 
 then estimate fj by removing the last 
 
research track paper 
component from our estimate of f 
j i e the component 
corresponding to the added node j and renormalizing 
the problem with this approach is in the starting vector 
recall from that xj is the pagerank of the added node 
j the difference between the actual pagerank f 
j of pfj 
and the starting vector ν is 
ν − f 
j xj f − − xj fj 
≥ xj f − − xj fj 
 xj xj 
 xj 
thus by after one pagerank iteration we expect our 
estimate of f 
j to still have an error of about αxj in 
particular for candidate nodes j with relatively high pagerank 
xj this method will yield more inaccurate results we will 
next present a method that eliminates this bias and runs in 
o n time 
 stochastic complementation 
since f 
j as given in is the pagerank of the matrix 
pfj we have 
fj − xj 
xj 
 
˜f ˜s 
˜ut 
j w 
fj − xj 
xj 
 
˜f fj − xj ˜sxj 
˜ut 
j fj − xj wxj 
 
solving the above system for fj can be shown to yield 
fj ˜f − w − 
˜s˜ut 
j fj 
the matrix s ˜f −w − 
˜s˜ut 
j is known as the stochastic 
complement of the column stochastic matrix pfj with 
respect to the sub matrix ˜f the theory of stochastic 
complementation is well studied and it can be shown the stochastic 
complement of an irreducible matrix such as the pagerank 
matrix is unique furthermore the stochastic complement 
is also irreducible and therefore has a unique stationary 
distribution as well for an extensive study see 
it can be easily shown that the sub-dominant eigenvalue 
of s is at most 
α where is the size of f for sufficiently 
large this value will be very close to α this is important 
as other properties of the pagerank algorithm notably the 
algorithm s sensitivity are dependent on this value 
in this method we estimate the length vector fj by 
computing one pagerank iteration over the × stochastic 
complement s starting at the vector f 
fj ≈ sf 
this is in contrast to the simple method outlined in the 
previous section which first iterates over the × 
matrix pfj to estimate f 
j and then removes the last 
component from the estimate and renormalizes to approximate 
fj the problem with the latter method is in the choice 
of the length starting vector ν consequently the 
pagerank estimate given by the simple method differs from 
the true pagerank by at least αxj where xj is the 
pagerank of page j by using the stochastic complement we 
can establish a tight lower bound of zero for this difference 
to see this consider the case in which a node k is added 
to f to form the augmented local subgraph fk and that 
the pagerank of this new graph is 
 − xk f 
xk 
 
specifically the addition of page k does not change the pageranks 
of the pages in f and thus fk f by construction of 
the stochastic complement fk sfk so the approximation 
given in equation will yield the exact solution 
next we present the computational details needed to 
efficiently compute the quantity fj −f over all known global 
pages j we begin by expanding the difference fj −f where 
the vector fj is estimated as in 
fj − f ≈ sf − f 
 αf df diag uj − 
f − α 
e 
 
et 
f 
 − w − 
 ˜ut 
j f ˜s − f 
note that the matrix df diag uj − 
is diagonal letting 
o k be the outlink count for page k in f we can express 
the kth 
diagonal element as 
 df diag uj − 
 k k 
 
o k 
if uj k 
 
o k 
if uj k 
noting that o k − 
 o k − 
− o k o k − 
and 
rewriting this in matrix form yields 
 df diag uj − 
 d− 
f −d− 
f df diag uj − 
diag uj 
 
we use the same identity to express 
e 
 
 
e 
− 
e 
 
 
recall that by definition we have pf αf d− 
f −α e 
 
substituting and in yields 
fj − f ≈ pf f − f 
−αf d− 
f df diag uj − 
diag uj f 
− − α 
e 
 
 − w − 
 ˜ut 
j f ˜s 
 x y ˜ut 
j f z 
noting that by definition f pf f and defining the vectors 
x y and z to be 
x −αf d− 
f df diag uj − 
diag uj f 
y − − α 
e 
 
 
z − w − 
˜s 
the first term x is a sparse vector and takes non-zero values 
only for local pages k that are siblings of the global page 
j we define i j ∈ f if and only if f j i 
 equivalently page i links to page j and express the value of the 
component x k as 
x k −α 
k k k ∈f uj k 
f k 
o k o k 
 
where o k as before is the number of outlinks from page k 
in the local domain note that the last two terms y and z 
are not dependent on the current global node j given the 
function hj f y ˜ut 
j f z the quantity fj − f 
 
research track paper 
can be expressed as 
fj − f 
k 
x k y k ˜ut 
j f z k 
 
k x k 
y k ˜ut 
j f z k 
 
k x k 
x k y k ˜ut 
j f z k 
 hj f − 
k x k 
y k ˜ut 
j f z k 
 
k x k 
x k y k ˜ut 
j f z k 
if we can compute the function hj in linear time then we 
can compute each value of fj − f using an additional 
amount of time that is proportional to the number of 
nonzero components in x these optimizations are carried out 
in algorithm note that computes the difference 
between all components of f and fj whereas our node 
selection criteria given in is restricted to the components 
corresponding to nodes in the original local domain l 
let us examine algorithm in more detail first the 
algorithm computes the outlink counts for each page in the 
local domain the algorithm then computes the quantity 
˜ut 
j f for each known global page j this inner product can 
be written as 
 − α 
 
 
 α 
k k j ∈fout 
f k 
o k 
 
where the second term sums over the set of local pages that 
link to page j since the total number of edges in fout was 
assumed to have size o recall that is the number of 
pages in f the running time of this step is also o 
the algorithm then computes the vectors y and z as 
given in and respectively the l normdiff 
method is called on the components of these vectors which 
correspond to the pages in l and it estimates the value of 
el y ˜ut 
j f z for each page j the estimation works 
as follows first the values of ˜ut 
j f are discretized uniformly 
into c values {a ac} the quantity el y aiz is 
then computed for each discretized value of ai and stored in 
a table to evaluate el y az for some a ∈ a ac 
the closest discretized value ai is determined and the 
corresponding entry in the table is used the total running time 
for this method is linear in and the discretization 
parameter c which we take to be a constant we note that if exact 
values are desired we have also developed an algorithm that 
runs in o log time that is not described here 
in the main loop we compute the vector x as defined 
in equation the nested loops iterate over the set of 
pages in f that are siblings of page j typically the size 
of this set is bounded by a constant finally for each page 
j the scores vector is updated over the set of non-zero 
components k of the vector x with k ∈ l this set has 
size equal to the number of local siblings of page j and is 
a subset of the total number of siblings of page j thus 
each iteration of the main loop takes constant time and the 
total running time of the main loop is o since we have 
assumed that the size of f will not grow larger than o n 
the total running time for the algorithm is o n 
algorithm node selection via stochastic 
complementation 
sc-select f fout f k 
input f zero-one adjacency matrix of size 
corresponding to the current local subgraph fout zero-one 
outlink matrix from f to global subgraph f 
pagerank of f k number of pages to return 
output pages set of k pages to crawl next 
{compute outlink sums for local subgraph} 
foreach page j ∈ f 
o j ← k j k ∈f f j k 
end 
{compute scalar ˜ut 
j f for each global node j } 
foreach page j ∈ fout 
g j ← − α 
 
foreach page k k j ∈ fout 
g j ← g j α f k 
o k 
end 
end 
{compute vectors y and z as in and } 
y ← − − α e 
 
z ← − w − 
˜s 
{approximate y g j ∗ z for all values g j } 
norm diffs ←l normdiffs g ely elz 
foreach page j ∈ fout 
{compute sparse vector x as in } 
x ← 
foreach page k k j ∈ fout 
foreach page k k k ∈ f 
x k ← x k − f k 
o k o k 
end 
end 
x ← αx 
scores j ← norm diffs j 
foreach k x k and page k ∈ l 
scores j ← scores j − y k g j ∗ z k 
 x k y k g j ∗z k 
end 
end 
return k pages with highest scores 
 pagerank flows 
we now present an intuitive analysis of the stochastic 
complementation method by decomposing the change in 
pagerank in terms of  leaks and  flows this analysis is 
motivated by the decomposition given in pagerank  flow is 
the increase in the local pageranks originating from global 
page j the flows are represented by the non-negative vector 
 ˜ut 
j f z equations and the scalar ˜ut 
j f can be 
thought of as the total amount of pagerank flow that page 
j has available to distribute the vector z dictates how the 
flow is allocated to the local domain the flow that local 
page k receives is proportional to within a constant factor 
due to the random surfer vector the expected number of its 
inlinks 
the pagerank  leaks represent the decrease in pagerank 
resulting from the addition of page j the leakage can 
be quantified in terms of the non-positive vectors x and 
y equations and for vector x we can see from 
equation that the amount of pagerank leaked by a 
local page is proportional to the weighted sum of the 
page 
research track paper 
ranks of its siblings thus pages that have siblings with 
higher pageranks and low outlink counts will experience 
more leakage the leakage caused by y is an artifact of the 
random surfer vector 
we will next show that if only the  flow term ˜ut 
j f z 
is considered then the resulting method is very similar to 
a heuristic proposed by cho et al that has been widely 
used for the crawling through url ordering problem 
this heuristic is computationally cheaper but as we will see 
later not as effective as the stochastic complementation 
method 
our node selection strategy chooses global nodes that 
have the largest influence equation if this influence is 
approximated using only  flows the optimal node j∗ 
is 
j∗ 
 argmaxj el ˜ut 
j fz 
 argmaxj ˜ut 
j f el z 
 argmaxj ˜ut 
j f 
 argmaxj α df diag uj − 
uj − α 
e 
 
 f 
 argmaxjft 
 df diag uj − 
uj 
the resulting page selection score can be expressed as a sum 
of the pageranks of each local page k that links to j where 
each pagerank value is normalized by o k interestingly 
the normalization that arises in our method differs from the 
heuristic given in which normalizes by o k the 
algorithm pf-select which is omitted due to lack of space 
first computes the quantity ft 
 df diag uj − 
uj for each 
global page j and then returns the pages with the k largest 
scores to see that the running time for this algorithm is 
o n note that the computation involved in this method is 
a subset of that needed for the sc-select method 
 algorithm which was shown to have a running time of o n 
 experiments 
in this section we provide experimental evidence to 
verify the effectiveness of our algorithms we first outline our 
experimental methodology and then provide results across 
a variety of local domains 
 methodology 
given the limited resources available at an academic 
institution crawling a section of the web that is of the same 
magnitude as that indexed by google or yahoo is clearly 
infeasible thus for a given local domain we approximate 
the global graph by crawling a local neighborhood around 
the domain that is several orders of magnitude larger than 
the local subgraph even though such a graph is still orders 
of magnitude smaller than the  true global graph we 
contend that even if there exist some highly influential pages 
that are very far away from our local domain it is 
unrealistic for any local node selection algorithm to find them such 
pages also tend to be highly unrelated to pages within the 
local domain 
when explaining our node selection strategies in section 
 we made the simplifying assumption that our local graph 
contained no dangling nodes this assumption was only 
made to ease our analysis our implementation efficiently 
handles dangling links by replacing each zero column of our 
adjacency matrix with the uniform vector we evaluate the 
algorithm using the two node selection strategies given in 
section and also against the following baseline methods 
 random nodes are chosen uniformly at random among 
the known global nodes 
 outlinkcount global nodes with the highest 
number of outlinks from the local domain are chosen 
at each iteration of the findglobalpr algorithm we 
evaluate performance by computing the difference between the 
current pagerank estimate of the local domain elf 
elf 
 and 
the global pagerank of the local domain elg 
elg 
 all 
pagerank calculations were performed using the uniform 
random surfer vector across all experiments we set the 
random surfer parameter α to be and used a convergence 
threshold of − 
 we evaluate the difference between the 
local and global pagerank vectors using three different 
metrics the l and l∞ norms and kendall s tau the l norm 
measures the sum of the absolute value of the differences 
between the two vectors and the l∞ norm measures the 
absolute value of the largest difference kendall s tau metric is 
a popular rank correlation measure used to compare 
pageranks this metric can be computed by counting 
the number of pairs of pairs that agree in ranking and 
subtracting from that the number of pairs of pairs that disagree 
in ranking the final value is then normalized by the total 
number of n 
 
such pairs resulting in a − range where 
a negative score signifies anti-correlation among rankings 
and values near one correspond to strong rank correlation 
 results 
our experiments are based on two large web crawls and 
were downloaded using the web crawler that is part of the 
nutch open source search engine project all crawls 
were restricted to only  http pages and to limit the 
number of dynamically generated pages that we crawl we 
ignored all pages with urls containing any of the characters 
      or   the first crawl which we will refer to 
as the  edu dataset was seeded by homepages of the top 
 graduate computer science departments in the usa as 
rated by the us news and world report and also by 
the home pages of their respective institutions a crawl of 
depth was performed restricted to pages within the   edu 
domain resulting in a graph with approximately million 
pages and million links the second crawl was seeded 
by the set of pages under the  politics hierarchy in the dmoz 
open directory project we crawled all pages up to four 
links away which yielded a graph with million pages and 
 million links 
within the  edu crawl we identified the five site-specific 
domains corresponding to the websites of the top five 
graduate computer science departments as ranked by the us 
news and world report this yielded local domains of 
various sizes from uiuc to berkeley for each 
of these site-specific domains with size n we performed 
iterations of the findglobalpr algorithm to crawl a total 
of n additional nodes figure a gives the l difference 
from the pagerank estimate at each iteration to the global 
pagerank for the berkeley local domain 
the performance of this dataset was representative of the 
typical performance across the five computer science 
sitespecific local domains initially the l difference between 
the global and local pageranks ranged from 
 stanford to mit for the first several iterations the 
 
research track paper 
 
 
 
 
 
 
 
 
 
 
number of iterations 
globalandlocalpagerankdifference l 
stochastic complement 
pagerank flow 
outlink count 
random 
 
 
 
 
 
 
 
 
 
number of iterations 
globalandlocalpagerankdifference l 
stochastic complement 
pagerank flow 
outlink count 
random 
 
 
 
 
 
 
 
 
 
 
 
number of iterations 
globalandlocalpagerankdifference l 
stochastic complement 
pagerank flow 
outlink count 
random 
 a www cs berkeley edu b www enterstageright com c politics 
figure l difference between the estimated and true global pageranks for a berkeley s computer science 
website b the site-specific domain www enterstageright com and c the  politics topic-specific domain the 
stochastic complement method outperforms all other methods across various domains 
three link-based methods all outperform the random 
selection heuristic after these initial iterations the random 
heuristic tended to be more competitive with or even 
outperform as in the berkeley local domain the outlink count 
and pagerank flow heuristics in all tests the stochastic 
complementation method either outperformed or was 
competitive with the other methods table gives the average 
difference between the final estimated global pageranks and 
the true global pageranks for various distance measures 
algorithm l l∞ kendall 
stoch comp 
pr flow 
outlink 
random 
table average final performance of various node 
selection strategies for the five site-specific 
computer science local domains note that kendall s 
tau measures similarity while the other metrics are 
dissimilarity measures stochastic 
complementation clearly outperforms the other methods in all 
metrics 
within the  politics dataset we also performed two 
sitespecific tests for the largest websites in the crawl 
www adamsmith org the website for the london based adam smith 
institute and www enterstageright com an online 
conservative journal as with the  edu local domains we ran our 
algorithm for iterations crawling a total of n nodes 
figure b plots the results for the www enterstageright com 
domain in contrast to the  edu local domains the random 
and outlinkcount methods were not competitive with 
either the sc-select or the pf-select methods among all 
datasets and all node selection methods the stochastic 
complementation method was most impressive in this dataset 
realizing a final estimate that differed only from the 
global pagerank a ten-fold improvement over the initial 
local pagerank difference of for the adam smith local 
domain the initial difference between the local and global 
pageranks was and the final estimates given by the 
sc-select pf-select outlinkcount and random 
methods were and respectively 
within the  politics dataset we constructed four 
topicspecific local domains the first domain consisted of all 
pages in the dmoz politics category and also all pages within 
each of these sites up to two links away this yielded a local 
domain of pages and the results are given in figure 
 c because of the larger size of the topic-specific domains 
we ran our algorithm for only iterations to crawl a total 
of n nodes 
we also created topic-specific domains from three 
political sub-topics liberalism conservatism and socialism the 
pages in these domains were identified by their 
corresponding dmoz categories for each sub-topic we set the local 
domain to be all pages within three links from the 
corresponding dmoz category pages table summarizes the 
performance of these three topic-specific domains and also 
the larger political domain 
to quantify a global page j s effect on the global 
pagerank values of pages in the local domain we define page 
j s impact to be its pagerank value g j normalized by the 
fraction of its outlinks pointing to the local domain 
impact j 
ol j 
o j 
· g j 
where ol j is the number of outlinks from page j to pages 
in the local domain l and o j is the total number of j s 
outlinks in terms of the random surfer model the impact 
of page j is the probability that the random surfer is 
currently at global page j in her random walk and takes 
an outlink to a local page given that she has already decided 
not to jump to a random page 
for the politics local domain we found that many of the 
pages with high impact were in fact political pages that 
should have been included in the dmoz politics topic but 
were not for example the two most influential global pages 
were the political search engine www askhenry com and the 
home page of the online political magazine 
www policyreview com among non-political pages the home page of 
the journal education next was most influential the 
journal is freely available online and contains articles 
regarding various aspect of k- education in america to provide 
some anecdotal evidence for the effectiveness of our page 
selection methods we note that the sc-select method chose 
 pages within the www educationnext org domain the 
pf-select method discovered such pages while the 
outlinkcount and random methods found only pages each 
for the conservative political local domain the socialist 
website www ornery org had a very high impact score this 
 
research track paper 
all politics 
algorithm l l kendall 
stoch comp 
pr flow 
outlink 
random 
conservativism 
algorithm l l kendall 
stoch comp 
pr flow 
outlink 
random 
liberalism 
algorithm l l kendall 
stoch comp 
pr flow 
outlink 
random 
socialism 
algorithm l l∞ kendall 
stoch comp 
pr flow 
outlink 
random 
table final performance among node selection 
strategies for the four political topic-specific crawls 
note that kendall s tau measures similarity while 
the other metrics are dissimilarity measures 
was largely due to a link from the front page of this site 
to an article regarding global warming published by the 
national center for public policy research a conservative 
research group in washington dc not surprisingly the 
global pagerank of this article which happens to be on the 
home page of the nccpr www nationalresearch com 
was approximately whereas the local pagerank of this 
page was only the sc-select method yielded a 
global pagerank estimate of approximately the 
pfselect method estimated a value of and the 
random and outlinkcount methods yielded values of 
and respectively 
 related work 
the node selection framework we have proposed is similar 
to the url ordering for crawling problem proposed by cho 
et al in whereas our framework seeks to minimize the 
difference between the global and local pagerank the 
objective used in is to crawl the most highly globally ranked 
pages first they propose several node selection algorithms 
including the outlink count heuristic as well as a variant of 
our pf-select algorithm which they refer to as the 
 pagerank ordering metric they found this method to be most 
effective in optimizing their objective as did a recent survey 
of these methods by baeza-yates et al boldi et al also 
experiment within a similar crawling framework in but 
quantify their results by comparing kendall s rank 
correlation between the pageranks of the current set of crawled 
pages and those of the entire global graph they found that 
node selection strategies that crawled pages with the 
highest global pagerank first actually performed worse with 
respect to kendall s tau correlation between the local and 
global pageranks than basic depth first or breadth first 
strategies however their experiments differ from our work 
in that our node selection algorithms do not use or have 
access to global pagerank values 
many algorithmic improvements for computing exact 
pagerank values have been proposed if such 
algorithms are used to compute the global pageranks of our 
local domain they would all require o n computation 
storage and bandwidth where n is the size of the global 
domain this is in contrast to our method which 
approximates the global pagerank and scales linearly with the size 
of the local domain 
wang and dewitt propose a system where the set of 
web servers that comprise the global domain communicate 
with each other to compute their respective global 
pageranks for a given web server hosting n pages the 
computational bandwidth and storage requirements are also 
linear in n one drawback of this system is that the 
number of distinct web servers that comprise the global domain 
can be very large for example our  edu dataset contains 
websites from over different universities coordinating 
such a system among a large number of sites can be very 
difficult 
gan chen and suel propose a method for estimating the 
pagerank of a single page which uses only constant 
bandwidth computation and space their approach relies on the 
availability of a remote connectivity server that can supply 
the set of inlinks to a given page an assumption not used in 
our framework they experimentally show that a reasonable 
estimate of the node s pagerank can be obtained by visiting 
at most a few hundred nodes using their algorithm for our 
problem would require that either the entire global domain 
first be downloaded or a connectivity server be used both 
of which would lead to very large web graphs 
 conclusions and future work 
the internet is growing exponentially and in order to 
navigate such a large repository as the web global search 
engines have established themselves as a necessity along with 
the ubiquity of these large-scale search engines comes an 
increase in search users expectations by providing complete 
and isolated coverage of a particular web domain localized 
search engines are an effective outlet to quickly locate 
content that could otherwise be difficult to find in this work 
we contend that the use of global pagerank in a localized 
search engine can improve performance 
to estimate the global pagerank we have proposed an 
iterative node selection framework where we select which 
pages from the global frontier to crawl next our primary 
contribution is our stochastic complementation page 
selection algorithm this method crawls nodes that will most 
significantly impact the local domain and has running time 
linear in the number of nodes in the local domain 
experimentally we validate these methods across a diverse set of 
local domains including seven site-specific domains and four 
topic-specific domains we conclude that by crawling an 
additional n or n pages our methods find an estimate of the 
global pageranks that is up to ten times better than just 
using the local pageranks furthermore we demonstrate 
that our algorithm consistently outperforms other existing 
heuristics 
 
research track paper 
often times topic-specific domains are discovered using 
a focused web crawler which considers a page s content in 
conjunction with link anchor text to decide which pages to 
crawl next although such crawlers have proven to be 
quite effective in discovering topic-related content many 
irrelevant pages are also crawled in the process typically 
these pages are deleted and not indexed by the localized 
search engine these pages can of course provide valuable 
information regarding the global pagerank of the local 
domain one way to integrate these pages into our framework 
is to start the findglobalpr algorithm with the current 
subgraph f equal to the set of pages that were crawled by 
the focused crawler 
the global pagerank estimation framework along with 
the node selection algorithms presented all require o n 
computation per iteration and bandwidth proportional to 
the number of pages crawled tk if the number of 
iterations t is relatively small compared to the number of pages 
crawled per iteration k then the bottleneck of the algorithm 
will be the crawling phase however as the number of 
iterations increases relative to k the bottleneck will reside in 
the node selection computation in this case our algorithms 
would benefit from constant factor optimizations recall 
that the findglobalpr algorithm algorithm requires 
that the pageranks of the current expanded local domain be 
recomputed in each iteration recent work by langville and 
meyer gives an algorithm to quickly recompute 
pageranks of a given webgraph if a small number of nodes are 
added this algorithm was shown to give speedup of five to 
ten times on some datasets we plan to investigate this and 
other such optimizations as future work 
in this paper we have objectively evaluated our methods 
by measuring how close our global pagerank estimates are 
to the actual global pageranks to determine the 
benefit of using global pageranks in a localized search engine 
we suggest a user study in which users are asked to rate 
the quality of search results for various search queries for 
some queries only the local pageranks are used in 
ranking and for the remaining queries local pageranks and the 
approximate global pageranks as computed by our 
algorithms are used the results of such a study can then be 
analyzed to determine the added benefit of using the global 
pageranks computed by our methods over just using the 
local pageranks 
acknowledgements this research was supported by nsf 
grant ccf- nsf career award aci- and 
a grant from sabre inc 
 references 
 r baeza-yates m marin c castillo and 
a rodriguez crawling a country better strategies 
than breadth-first for web page ordering world-wide 
web conference 
 p boldi m santini and s vigna do your worst to 
make the best paradoxical effects in pagerank 
incremental computations workshop on web graphs 
 - 
 s brin and l page the anatomy of a large-scale 
hypertextual web search engine computer networks 
and isdn systems - - 
 s chakrabarti m van den berg and b dom 
focused crawling a new approach to topic-specific 
web resource discovery world-wide web conference 
 
 y chen q gan and t suel local methods for 
estimating pagerank values conference on 
information and knowledge management 
 j cho h garcia-molina and l page efficient 
crawling through url ordering world-wide web 
conference 
 t h haveliwala and s d kamvar the second 
eigenvalue of the google matrix technical report 
stanford university 
 t joachims f radlinski l granka a cheng 
c tillekeratne and a patel learning retrieval 
functions from implicit feedback 
http www cs cornell edu people tj career 
 s d kamvar t h haveliwala c d manning and 
g h golub exploiting the block structure of the 
web for computing pagerank world-wide web 
conference 
 s d kamvar t h haveliwala c d manning and 
g h golub extrapolation methods for accelerating 
pagerank computation world-wide web conference 
 
 a n langville and c d meyer deeper inside 
pagerank internet mathematics 
 a n langville and c d meyer updating the 
stationary vector of an irreducible markov chain with 
an eye on google s pagerank siam journal on 
matrix analysis 
 p lyman h r varian k swearingen p charles 
n good l l jordan and j pal how much 
information school of information management 
and system university of california at berkely 
 f mcsherry a uniform approach to accelerated 
pagerank computation world-wide web conference 
 
 c d meyer stochastic complementation uncoupling 
markov chains and the theory of nearly reducible 
systems siam review - 
 us news and world report http www usnews com 
 dmoz open directory project http www dmoz org 
 nutch open source search engine 
http www nutch org 
 f radlinski and t joachims query chains learning 
to rank from implicit feedback acm sigkdd 
international conference on knowledge discovery and 
data mining 
 s raghavan and h garcia-molina crawling the 
hidden web in proceedings of the twenty-seventh 
international conference on very large databases 
 
 t tin tang d hawking n craswell and 
k griffiths focused crawling for both topical 
relevance and quality of medical information 
conference on information and knowledge 
management 
 y wang and d j dewitt computing pagerank in a 
distributed internet search system proceedings of the 
 th vldb conference 
 
research track paper 
feature representation for effective action-item detection 
paul n bennett 
computer science department 
carnegie mellon university 
pittsburgh pa 
pbennett  cs cmu edu 
jaime carbonell 
language technologies institute 
carnegie mellon university 
pittsburgh pa 
jgc  cs cmu edu 
abstract 
e-mail users face an ever-growing challenge in managing their 
inboxes due to the growing centrality of email in the workplace for 
task assignment action requests and other roles beyond 
information dissemination whereas information retrieval and machine 
learning techniques are gaining initial acceptance in spam filtering 
and automated folder assignment this paper reports on a new task 
automated action-item detection in order to flag emails that require 
responses and to highlight the specific passage s indicating the 
request s for action unlike standard topic-driven text classification 
action-item detection requires inferring the sender s intent and as 
such responds less well to pure bag-of-words classification 
however using enriched feature sets such as n-grams up to n with 
chi-squared feature selection and contextual cues for action-item 
location improve performance by up to over unigrams using 
in both cases state of the art classifiers such as svms with 
automated model selection via embedded cross-validation 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval i artificial intelligence learning i pattern 
recognition applications 
general terms 
experimentation 
 introduction 
e-mail users are facing an increasingly difficult task of 
managing their inboxes in the face of mounting challenges that result from 
rising e-mail usage this includes prioritizing e-mails over a range 
of sources from business partners to family members filtering and 
reducing junk e-mail and quickly managing requests that demand 
from henry hutchins hhutchins innovative company com 
to sara smith joe johnson william woolings 
subject meeting with prospective customers 
sent fri am 
hi all 
i d like to remind all of you that the group from grty will be visiting us 
next friday at p m the current schedule looks like this 
 a m informal breakfast and discussion in cafeteria 
 a m company overview 
 a m individual meetings continue over lunch 
 p m tour of facilities 
 p m sales pitch 
in order to have this go off smoothly i would like to practice the 
presentation well in advance as a result i will need each of your parts by 
wednesday 
keep up the good work 
-henry 
figure an e-mail with emphasized action-item an explicit 
request that requires the recipient s attention or action 
the receiver s attention or action automated action-item detection 
targets the third of these problems by attempting to detect which 
e-mails require an action or response with information and within 
those e-mails attempting to highlight the sentence or other 
passage length that directly indicates the action request 
such a detection system can be used as one part of an e-mail 
agent which would assist a user in processing important e-mails 
quicker than would have been possible without the agent we view 
action-item detection as one necessary component of a successful 
e-mail agent which would perform spam detection action-item 
detection topic classification and priority ranking among other 
functions the utility of such a detector can manifest as a method of 
prioritizing e-mails according to task-oriented criteria other than 
the standard ones of topic and sender or as a means of ensuring that 
the email user hasn t dropped the proverbial ball by forgetting to 
address an action request 
action-item detection differs from standard text classification in 
two important ways first the user is interested both in 
detecting whether an email contains action items and in locating exactly 
where these action item requests are contained within the email 
body in contrast standard text categorization merely assigns a 
topic label to each text whether that label corresponds to an e-mail 
folder or a controlled indexing vocabulary second 
action-item detection attempts to recover the email sender s intent 
- whether she means to elicit response or action on the part of the 
receiver note that for this task classifiers using only unigrams as 
features do not perform optimally as evidenced in our results 
below instead we find that we need more information-laden features 
such as higher-order n-grams text categorization by topic on the 
other hand works very well using just individual words as features 
 in fact genre-classification which one would think 
may require more than a bag-of-words approach also works quite 
well using just unigram features topic detection and 
tracking tdt also works well with unigram feature sets we 
believe that action-item detection is one of the first clear instances 
of an ir-related task where we must move beyond bag-of-words 
to achieve high performance albeit not too far as bag-of-n-grams 
seem to suffice 
we first review related work for similar text classification 
problems such as e-mail priority ranking and speech act identification 
then we more formally define the action-item detection problem 
discuss the aspects that distinguish it from more common problems 
like topic classification and highlight the challenges in 
constructing systems that can perform well at the sentence and document 
level from there we move to a discussion of feature 
representation and selection techniques appropriate for this problem and how 
standard text classification approaches can be adapted to smoothly 
move from the sentence-level detection problem to the 
documentlevel classification problem we then conduct an empirical analysis 
that helps us determine the effectiveness of our feature extraction 
procedures as well as establish baselines for a number of 
classification algorithms on this task finally we summarize this paper s 
contributions and consider interesting directions for future work 
 related work 
several other researchers have considered very similar text 
classification tasks cohen et al describe an ontology of speech 
acts such as propose a meeting and attempt to predict when an 
e-mail contains one of these speech acts we consider action-items 
to be an important specific type of speech act that falls within their 
more general classification while they provide results for 
several classification methods their methods only make use of human 
judgments at the document-level in contrast we consider whether 
accuracy can be increased by using finer-grained human judgments 
that mark the specific sentences and phrases of interest 
corston-oliver et al consider detecting items in e-mail to 
put on a to-do list this classification task is very similar to 
ours except they do not consider simple factual questions to 
belong to this category we include questions but note that not all 
questions are action-items - some are rhetorical or simply social 
convention how are you from a learning perspective while 
they make use of judgments at the sentence-level they do not 
explicitly compare what if any benefits finer-grained judgments offer 
additionally they do not study alternative choices or approaches to 
the classification task instead they simply apply a standard svm 
at the sentence-level and focus primarily on a linguistic analysis of 
how the sentence can be logically reformulated before adding it to 
the task list in this study we examine several alternative 
classification methods compare document-level and sentence-level 
approaches and analyze the machine learning issues implicit in these 
problems 
interest in a variety of learning tasks related to e-mail has been 
rapidly growing in the recent literature for example in a forum 
dedicated to e-mail learning tasks culotta et al presented 
methods for learning social networks from e-mail in this work we do 
not focus on peer relationships however such methods could 
complement those here since peer relationships often influence word 
choice when requesting an action 
 problem definition approach 
in contrast to previous work we explicitly focus on the benefits 
that finer-grained more costly sentence-level human judgments 
offer over coarse-grained document-level judgments additionally 
we consider multiple standard text classification approaches and 
analyze both the quantitative and qualitative differences that arise 
from taking a document-level vs a sentence-level approach to 
classification finally we focus on the representation necessary to 
achieve the most competitive performance 
 problem definition 
in order to provide the most benefit to the user a system would 
not only detect the document but it would also indicate the specific 
sentences in the e-mail which contain the action-items therefore 
there are three basic problems 
 document detection classify a document as to whether or 
not it contains an action-item 
 document ranking rank the documents such that all 
documents containing action-items occur as high as possible in 
the ranking 
 sentence detection classify each sentence in a document as 
to whether or not it is an action-item 
as in most information retrieval tasks the weight the 
evaluation metric should give to precision and recall depends on the 
nature of the application in situations where a user will eventually 
read all received messages ranking e g via precision at recall of 
 may be most important since this will help encourage shorter 
delays in communications between users in contrast high-precision 
detection at low recall will be of increasing importance when the 
user is under severe time-pressure and therefore will likely not read 
all mail this can be the case for crisis managers during disaster 
management finally sentence detection plays a role in both 
timepressure situations and simply to alleviate the user s required time 
to gist the message 
 approach 
as mentioned above the labeled data can come in one of two 
forms a document-labeling provides a yes no label for each 
document as to whether it contains an action-item a phrase-labeling 
provides only a yes label for the specific items of interest we term 
the human judgments a phrase-labeling since the user s view of the 
action-item may not correspond with actual sentence boundaries or 
predicted sentence boundaries obviously it is straightforward to 
generate a document-labeling consistent with a phrase-labeling by 
labeling a document yes if and only if it contains at least one 
phrase labeled yes 
to train classifiers for this task we can take several viewpoints 
related to both the basic problems we have enumerated and the form 
of the labeled data the document-level view treats each e-mail as 
a learning instance with an associated class-label then the 
document can be converted to a feature-value vector and learning 
progresses as usual applying a document-level classifier to document 
detection and ranking is straightforward in order to apply it to 
sentence detection one must make additional steps for example 
if the classifier predicts a document contains an action-item then 
areas of the document that contain a high-concentration of words 
which the model weights heavily in favor of action-items can be 
indicated the obvious benefit of the document-level approach is 
that training set collection costs are lower since the user only has 
to specify whether or not an e-mail contains an action-item and not 
the specific sentences 
in the sentence-level view each e-mail is automatically segmented 
into sentences and each sentence is treated as a learning instance 
with an associated class-label since the phrase-labeling provided 
by the user may not coincide with the automatic segmentation we 
must determine what label to assign a partially overlapping 
sentence when converting it to a learning instance once trained 
applying the resulting classifiers to sentence detection is now 
straightforward but in order to apply the classifiers to document 
detection and document ranking the individual predictions over each 
sentence must be aggregated in order to make a document-level 
prediction this approach has the potential to benefit from 
morespecific labels that enable the learner to focus attention on the key 
sentences instead of having to learn based on data that the majority 
of the words in the e-mail provide no or little information about 
class membership 
 features 
consider some of the phrases that might constitute part of an 
action item would like to know let me know as soon as 
possible have you each of these phrases consists of common 
words that occur in many e-mails however when they occur in 
the same sentence they are far more indicative of an action-item 
additionally order can be important consider have you versus 
you have because of this we posit that n-grams play a larger 
role in this problem than is typical of problems like topic 
classification therefore we consider all n-grams up to size 
when using n-grams if we find an n-gram of size in a segment 
of text we can represent the text as just one occurrence of the 
ngram or as one occurrence of the n-gram and an occurrence of each 
smaller n-gram contained by it we choose the second of these 
alternatives since this will allow the algorithm itself to smoothly 
back-off in terms of recall methods such as na¨ıve bayes may be 
hurt by such a representation because of double-counting 
since sentence-ending punctuation can provide information we 
retain the terminating punctuation token when it is identifiable 
additionally we add a beginning-of-sentence and end-of-sentence 
token in order to capture patterns that are often indicators at the 
beginning or end of a sentence assuming proper punctuation these 
extra tokens are unnecessary but often e-mail lacks proper 
punctuation in addition for the sentence-level classifiers that use 
ngrams we additionally code for each sentence a binary encoding 
of the position of the sentence relative to the document this 
encoding has eight associated features that represent which octile the 
first eighth second eighth etc contains the sentence 
 implementation details 
in order to compare the document-level to the sentence-level 
approach we compare predictions at the document-level we do not 
address how to use a document-level classifier to make predictions 
at the sentence-level 
in order to automatically segment the text of the e-mail we use 
the rasp statistical parser since the automatically segmented 
sentences may not correspond directly with the phrase-level 
boundaries we treat any sentence that contains at least of a marked 
action-item segment as an action-item when evaluating 
sentencedetection for the sentence-level system we use these class labels 
as ground truth since we are not evaluating multiple segmentation 
approaches this does not bias any of the methods if multiple 
segmentation systems were under evaluation one would need to use a 
metric that matched predicted positive sentences to phrases labeled 
positive the metric would need to punish overly long true 
predictions as well as too short predictions our criteria for converting 
to labeled instances implicitly includes both criteria since the 
segmentation is fixed an overly long prediction would be predicting 
yes for many no instances since presumably the extra length 
corresponds to additional segmented sentences all of which do not 
contain of action-item likewise a too short prediction must 
correspond to a small sentence included in the action-item but not 
constituting all of the action-item therefore in order to consider 
the prediction to be too short there will be an additional 
preceding following sentence that is an action-item where we incorrectly 
predicted no 
once a sentence-level classifier has made a prediction for each 
sentence we must combine these predictions to make both a 
document-level prediction and a document-level score we use the 
simple policy of predicting positive when any of the sentences is 
predicted positive in order to produce a document score for 
ranking the confidence that the document contains an action-item is 
ψ d 
 
n d s∈d π s ψ s if for any s ∈ d π s 
 
n d 
maxs∈d ψ s o w 
where s is a sentence in document d π is the classifier s 
prediction ψ is the score the classifier assigns as its confidence that 
π s and n d is the greater of and the number of unigram 
tokens in the document in other words when any sentence is 
predicted positive the document score is the length normalized sum of 
the sentence scores above threshold when no sentence is predicted 
positive the document score is the maximum sentence score 
normalized by length as in other text problems we are more likely to 
emit false positives for documents with more words or sentences 
thus we include a length normalization factor 
 experimental analysis 
 the data 
our corpus consists of e-mails obtained from volunteers at an 
educational institution and cover subjects such as organizing a 
research workshop arranging for job-candidate interviews 
publishing proceedings and talk announcements the messages were 
anonymized by replacing the names of each individual and 
institution with a pseudonym 
after attempting to identify and eliminate 
duplicate e-mails the corpus contains e-mail messages 
after identity anonymization the corpora has three basic 
versions quoted material refers to the text of a previous e-mail that 
an author often leaves in an e-mail message when responding to the 
e-mail quoted material can act as noise when learning since it may 
include action-items from previous messages that are no longer 
relevant to isolate the effects of quoted material we have three 
versions of the corpora the raw form contains the basic messages 
the auto-stripped version contains the messages after quoted 
material has been automatically removed the hand-stripped version 
contains the messages after quoted material has been removed by 
a human additionally the hand-stripped version has had any xml 
content and e-mail signatures removed - leaving only the essential 
content of the message the studies reported here are performed 
with the hand-stripped version this allows us to balance the 
cognitive load in terms of number of tokens that must be read in the 
user-studies we report - including quoted material would 
complicate the user studies since some users might skip the material while 
others read it additionally ensuring all quoted material is removed 
 
we have an even more highly anonymized version of the 
corpus that can be made available for some outside experimentation 
please contact the authors for more information on obtaining this 
data 
prevents tainting the cross-validation since otherwise a test item 
could occur as quoted material in a training document 
 data labeling 
two human annotators labeled each message as to whether or 
not it contained an action-item in addition they identified each 
segment of the e-mail which contained an action-item a segment 
is a contiguous section of text selected by the human annotators 
and may span several sentences or a complete phrase contained in 
a sentence they were instructed that an action item is an explicit 
request for information that requires the recipient s attention or a 
required action and told to highlight the phrases or sentences that 
make up the request 
annotator 
no yes 
annotator 
no 
yes 
table agreement of human annotators at document level 
annotator one labeled messages as containing action items 
annotator two labeled messages as containing action items 
the agreement of the human annotators is shown in tables and 
 the annotators are said to agree at the document-level when 
both marked the same document as containing no action-items or 
both marked at least one action-item regardless of whether the text 
segments were the same at the document-level the annotators 
agreed of the time the kappa statistic is often used to 
evaluate inter-annotator agreement 
κ 
a − r 
 − r 
a is the empirical estimate of the probability of agreement r 
is the empirical estimate of the probability of random agreement 
given the empirical class priors a value close to − implies the 
annotators agree far less often than would be expected randomly 
while a value close to means they agree more often than randomly 
expected 
at the document-level the kappa statistic for inter-annotator 
agreement is this value is both strong enough to expect the 
problem to be learnable and is comparable with results for similar tasks 
 
in order to determine the sentence-level agreement we use each 
judgment to create a sentence-corpus with labels as described in 
section then consider the agreement over these sentences 
this allows us to compare agreement over no judgments we 
perform this comparison over the hand-stripped corpus since that 
eliminates spurious no judgments that would come from 
including quoted material etc both annotators were free to label the 
subject as an action-item but since neither did we omit the subject 
line of the message as well this only reduces the number of no 
agreements this leaves automatically segmented sentences 
at the sentence-level the annotators agreed of the time and 
the kappa statistic for inter-annotator agreement is 
in order to produce one single set of judgments the human 
annotators went through each annotation where there was 
disagreement and came to a consensus opinion the annotators did not 
collect statistics during this process but anecdotally reported that 
the majority of disagreements were either cases of clear annotator 
oversight or different interpretations of conditional statements for 
example if you would like to keep your job come to tomorrow s 
meeting implies a required action where if you would like to join 
annotator 
no yes 
annotator 
no 
yes 
table agreement of human annotators at sentence level 
the football betting pool come to tomorrow s meeting does not 
the first would be an action-item in most contexts while the 
second would not of course many conditional statements are not so 
clearly interpretable after reconciling the judgments there are 
e-mails with no action-items and e-mails containing 
actionitems of the e-mails containing action-items messages 
have one action-item segment messages have two action-item 
segments messages have three action-item segments two 
messages have four action-item segments and one message has six 
action-item segments computing the sentence-level agreement 
using the reconciled gold standard judgments with each of the 
annotators individual judgments gives a kappa of for annotator 
one and a kappa of for annotator two 
in terms of message characteristics there were on average 
content tokens in the body after stripping for action-item 
messages there were however by examining figure we see 
the length distributions are nearly identical as would be expected 
for e-mail it is a long-tailed distribution with about half the 
messages having more than tokens in the body this paragraph has 
 tokens 
 classifiers 
for this experiment we have selected a variety of standard text 
classification algorithms in selecting algorithms we have chosen 
algorithms that are not only known to work well but which differ 
along such lines as discriminative vs generative and lazy vs 
eager we have done this in order to provide both a competitive and 
thorough sampling of learning methods for the task at hand this 
is important since it is easy to improve a strawman classifier by 
introducing a new representation by thoroughly sampling 
alternative classifier choices we demonstrate that representation 
improvements over bag-of-words are not due to using the information in the 
bag-of-words poorly 
 knn 
we employ a standard variant of the k-nearest neighbor 
algorithm used in text classification knn with s-cut score 
thresholding we use a tfidf-weighting of the terms with a 
distanceweighted vote of the neighbors to compute the score before 
thresholding it in order to choose the value of s for thresholding we 
perform leave-one-out cross-validation over the training set the 
value of k is set to be log n where n is the number of 
training points this rule for choosing k is theoretically motivated 
by results which show such a rule converges to the optimal 
classifier as the number of training points increases in practice 
we have also found it to be a computational convenience that 
frequently leads to comparable results with numerically optimizing k 
via a cross-validation procedure 
 na¨ıve bayes 
we use a standard multinomial na¨ıve bayes classifier in 
using this classifier we smoothed word and class probabilities using a 
bayesian estimate with the word prior and a laplace m-estimate 
respectively 
 
 
 
 
 
 
 
 
 
 
numberofmessages 
number of tokens 
all messages 
action-item messages 
 
 
 
 
 
 
 
 
 
 
 
 
percentageofmessages 
number of tokens 
all messages 
action-item messages 
figure the histogram left and distribution right of message length a bin size of words was used only tokens in the body 
after hand-stripping were counted after stripping the majority of words left are usually actual message content 
classifiers document unigram document ngram sentence unigram sentence ngram 
f 
knn ± ± ± ± 
na¨ıve bayes ± ± ± ± 
svm ± ± ± ± 
voted perceptron ± ± ± ± 
accuracy 
knn ± ± ± ± 
na¨ıve bayes ± ± ± ± 
svm ± ± ± ± 
voted perceptron ± ± ± ± 
table average document-detection performance during cross-validation for each method and the sample standard deviation 
 sn− in italics the best performance for each classifier is shown in bold 
 svm 
we have used a linear svm with a tfidf feature representation 
and l -norm as implemented in the svmlight package v 
all default settings were used 
 voted perceptron 
like the svm the voted perceptron is a kernel-based 
learning method we use the same feature representation and kernel 
as we have for the svm a linear kernel with tfidf-weighting and 
an l -norm the voted perceptron is an online-learning method 
that keeps a history of past perceptrons used as well as a weight 
signifying how often that perceptron was correct with each new 
training example a correct classification increases the weight on 
the current perceptron and an incorrect classification updates the 
perceptron the output of the classifier uses the weights on the 
perceptra to make a final voted classification when used in an 
offline-manner multiple passes can be made through the training 
data both the voted perceptron and the svm give a solution from 
the same hypothesis space - in this case a linear classifier 
furthermore it is well-known that the voted perceptron increases the 
margin of the solution after each pass through the training data 
since cohen et al obtain worse results using an svm than a 
voted perceptron with one training iteration they conclude that the 
best solution for detecting speech acts may not lie in an area with 
a large margin because their tasks are highly similar to ours we 
employ both classifiers to ensure we are not overlooking a 
competitive alternative classifier to the svm for the basic bag-of-words 
representation 
 performance measures 
to compare the performance of the classification methods we 
look at two standard performance measures f and accuracy the 
f measure is the harmonic mean of precision and recall 
where precision correct positives 
predicted positives 
and recall correct positives 
actual positives 
 
 experimental methodology 
we perform standard -fold cross-validation on the set of 
documents for the sentence-level approach all sentences in a 
document are either entirely in the training set or entirely in the test set 
for each fold for significance tests we use a two-tailed t-test 
to compare the values obtained during each cross-validation fold 
with a p-value of 
feature selection was performed using the chi-squared 
statistic different levels of feature selection were considered for each 
classifier each of the following number of features was tried 
 there are 
approximately unigram tokens without feature selection in order to choose 
the number of features to use for each classifier we perform nested 
cross-validation and choose the settings that yield the optimal 
document-level f for that classifier for this study only the body of 
each e-mail message was used feature selection is always applied 
to all candidate features that is for the n-gram representation the 
n-grams and position features are also subject to removal by the 
feature selection method 
 results 
the results for document-level classification are given in table 
 the primary hypothesis we are concerned with is that n-grams 
are critical for this task if this is true we expect to see a significant 
gap in performance between the document-level classifiers that use 
n-grams denoted document ngram and those using only unigram 
features denoted document unigram examining table we 
observe that this is indeed the case for every classifier except na¨ıve 
bayes this difference in performance produced by the n-gram 
representation is statistically significant for each classifier except 
for na¨ıve bayes and the accuracy metric for knn see table 
na¨ıve bayes poor performance with the n-gram representation is 
not surprising since the bag-of-n-grams causes excessive 
doublecounting as mentioned in section however na¨ıve bayes is 
not hurt at the sentence-level because the sparse examples provide 
few chances for agglomerative effects of double counting in either 
case when a language-modeling approach is desired modeling the 
n-grams directly would be preferable to na¨ıve bayes more 
importantly for the n-gram hypothesis the n-grams lead to the best 
document-level classifier performance as well 
as would be expected the difference between the sentence-level 
n-gram representation and unigram representation is small this 
is because the window of text is so small that the unigram 
representation when done at the sentence-level implicitly picks up 
on the power of the n-grams further improvement would 
signify that the order of the words matter even when only 
considering a small sentence-size window therefore the finer-grained 
sentence-level judgments allows a unigram representation to 
succeed but only when performed in a small window - behaving as 
an n-gram representation for all practical purposes 
document winner sentence winner 
knn ngram ngram 
na¨ıve bayes unigram ngram 
svm ngram† 
ngram 
voted perceptron ngram† 
ngram 
table significance results for n-grams versus unigrams for 
document detection using document-level and sentence-level 
classifiers when the f result is statistically significant it is 
shown in bold when the accuracy result is significant it is 
shown with a † 
 
f winner accuracy winner 
knn sentence sentence 
na¨ıve bayes sentence sentence 
svm sentence sentence 
voted perceptron sentence document 
table significance results for sentence-level classifiers vs 
document-level classifiers for the document detection problem 
when the result is statistically significant it is shown in bold 
further highlighting the improvement from finer-grained 
judgments and n-grams figure graphically depicts the edge the svm 
sentence-level classifier has over the standard bag-of-words approach 
with a precision-recall curve in the high precision area of the 
graph the consistent edge of the sentence-level classifier is rather 
impressive - continuing at precision out to recall this 
would mean that a tenth of the user s action-items would be placed 
at the top of their action-item sorted inbox additionally the large 
separation at the top right of the curves corresponds to the area 
where the optimal f occurs for each classifier agreeing with the 
large improvement from to in f score considering 
the relative unexplored nature of classification at the sentence-level 
this gives great hope for further increases in performance 
accuracy f 
unigram ngram unigram ngram 
knn 
na¨ıve bayes 
svm 
voted perceptron 
table performance of the sentence-level classifiers at 
sentence detection 
although cohen et al observed that the voted perceptron 
with a single training iteration outperformed svm in a set of 
similar tasks we see no such behavior here this further strengthens the 
evidence that an alternate classifier with the bag-of-words 
representation could not reach the same level of performance the voted 
perceptron classifier does improve when the number of training 
iterations are increased but it is still lower than the svm classifier 
sentence detection results are presented in table with regard 
to the sentence detection problem we note that the f measure 
gives a better feel for the remaining room for improvement in this 
difficult problem that is unlike document detection where 
actionitem documents are fairly common action-item sentences are very 
rare thus as in other text problems the accuracy numbers are 
deceptively high sheerly because of the default accuracy attainable by 
always predicting no although the results here are significantly 
above-random it is unclear what level of performance is necessary 
for sentence detection to be useful in and of itself and not simply 
as a means to document ranking and classification 
figure users find action-items quicker when assisted by a 
classification system 
finally when considering a new type of classification task one 
of the most basic questions is whether an accurate classifier built 
for the task can have an impact on the end-user in order to 
demonstrate the impact this task can have on e-mail users we conducted 
a user study using an earlier less-accurate version of the sentence 
classifier - where instead of using just a single sentence a 
threesentence windowed-approach was used there were three distinct 
sets of e-mail in which users had to find action-items these sets 
were either presented in a random order unordered ordered by 
the classifier ordered or ordered by the classifier and with the 
 
 
 
 
 
 
 
 
precision 
recall 
action-item detection svm performance post model selection 
document unigram 
sentence ngram 
figure both n-grams and a small prediction window lead to consistent improvements over the standard approach 
center sentence in the highest confidence window highlighted 
 order help in order to perform fair comparisons between 
conditions the overall number of tokens in each message set should be 
approximately equal that is the cognitive reading load should be 
approximately the same before the classifier s reordering 
additionally users typically show practice effects by improving at the 
overall task and thus performing better at later message sets this 
is typically handled by varying the ordering of the sets across users 
so that the means are comparable while omitting further detail 
we note the sets were balanced for the total number of tokens and 
a latin square design was used to balance practice effects 
figure shows that at intervals of and minutes users 
consistently found significantly more action-items when assisted 
by the classifier but were most critically aided in the first five 
minutes although the classifier consistently aids the users we did not 
gain an additional end-user impact by highlighting as mentioned 
above this might be a result of the large room for improvement that 
still exists for sentence detection but anecdotal evidence suggests 
this might also be a result of how the information is presented to the 
user rather than the accuracy of sentence detection for example 
highlighting the wrong sentence near an actual action-item hurts 
the user s trust but if a vague indicator e g an arrow points to the 
approximate area the user is not aware of the near-miss since the 
user studies used a three sentence window we believe this played a 
role as well as sentence detection accuracy 
 discussion 
in contrast to problems where n-grams have yielded little 
difference we believe their power here stems from the fact that many of 
the meaningful n-grams for action-items consist of common words 
e g let me know therefore the document-level unigram 
approach cannot gain much leverage even when modeling their joint 
probability correctly since these words will often co-occur in the 
document but not necessarily in a phrase additionally action-item 
detection is distinct from many text classification tasks in that a 
single sentence can change the class label of the document as a 
result good classifiers cannot rely on aggregating evidence from a 
large number of weak indicators across the entire document 
even though we discarded the header information examining 
the top-ranked features at the document-level reveals that many of 
the features are names or parts of e-mail addresses that occurred in 
the body and are highly associated with e-mails that tend to 
contain many or no action-items a few examples are terms such as 
org bob and gov we note that these features will be 
sensitive to the particular distribution senders receivers and thus the 
document-level approach may produce classifiers that transfer less 
readily to alternate contexts and users at different institutions this 
points out that part of the problem of going beyond bag-of-words 
may be the methodology and investigating such properties as 
learning curves and how well a model transfers may highlight 
differences in models which appear to have similar performance when 
tested on the distributions they were trained on we are currently 
investigating whether the sentence-level classifiers do perform 
better over different test corpora without retraining 
 future work 
while applying text classifiers at the document-level is fairly 
well-understood there exists the potential for significantly 
increasing the performance of the sentence-level classifiers such methods 
include alternate ways of combining the predictions over each 
sentence weightings other than tfidf which may not be appropriate 
since sentences are small better sentence segmentation and other 
types of phrasal analysis additionally named entity tagging time 
expressions etc seem likely candidates for features that can 
further improve this task we are currently pursuing some of these 
avenues to see what additional gains these offer 
finally it would be interesting to investigate the best methods for 
combining the document-level and sentence-level classifiers since 
the simple bag-of-words representation at the document-level leads 
to a learned model that behaves somewhat like a context-specific 
prior dependent on the sender receiver and general topic a first 
choice would be to treat it as such when combining probability 
estimates with the sentence-level classifier such a model might 
serve as a general example for other problems where bag-of-words 
can establish a baseline model but richer approaches are needed to 
achieve performance beyond that baseline 
 summary and conclusions 
the effectiveness of sentence-level detection argues that 
labeling at the sentence-level provides significant value further 
experiments are needed to see how this interacts with the amount of 
training data available sentence detection that is then agglomerated to 
document-level detection works surprisingly better given low recall 
than would be expected with sentence-level items this in turn 
indicates that improved sentence segmentation methods could yield 
further improvements in classification 
in this work we examined how action-items can be effectively 
detected in e-mails our empirical analysis has demonstrated that 
n-grams are of key importance to making the most of 
documentlevel judgments when finer-grained judgments are available then 
a standard bag-of-words approach using a small sentence window 
size and automatic segmentation techniques can produce results 
almost as good as the n-gram based approaches 
acknowledgments 
this material is based upon work supported by the defense 
advanced research projects agency darpa under contract no 
nbchd any opinions findings and conclusions or 
recommendations expressed in this material are those of the author s 
and do not necessarily reflect the views of the defense advanced 
research projects agency darpa or the department of 
interiornational business center doi-nbc 
we would like to extend our sincerest thanks to jill lehman 
whose efforts in data collection were essential in constructing the 
corpus and both jill and aaron steinfeld for their direction of the 
hci experiments we would also like to thank django wexler for 
constructing and supporting the corpus labeling tools and curtis 
huttenhower s support of the text preprocessing package finally 
we gratefully acknowledge scott fahlman for his encouragement 
and useful discussions on this topic 
 references 
 j allan j carbonell g doddington j yamron and 
y yang topic detection and tracking pilot study final 
report in proceedings of the darpa broadcast news 
transcription and understanding workshop washington 
d c 
 c apte f damerau and s m weiss automated learning 
of decision rules for text categorization acm transactions 
on information systems - july 
 j carletta assessing agreement on classification tasks the 
kappa statistic computational linguistics - 
 
 j carroll high precision extraction of grammatical relations 
in proceedings of the th international conference on 
computational linguistics coling pages - 
 w w cohen v r carvalho and t m mitchell learning 
to classify email into speech acts in emnlp- 
 conference on empirical methods in natural language 
processing pages - 
 s corston-oliver e ringger m gamon and r campbell 
task-focused summarization of email in text summarization 
branches out proceedings of the acl- workshop pages 
 - 
 a culotta r bekkerman and a mccallum extracting 
social networks and contact information from email and the 
web in ceas- conference on email and anti-spam 
mountain view ca july 
 l devroye l gy¨orfi and g lugosi a probabilistic theory 
of pattern recognition springer-verlag new york ny 
 
 s t dumais j platt d heckerman and m sahami 
inductive learning algorithms and representations for text 
categorization in cikm proceedings of the th acm 
conference on information and knowledge management 
pages - 
 y freund and r schapire large margin classification using 
the perceptron algorithm machine learning - 
 
 t joachims making large-scale svm learning practical in 
b sch¨olkopf c j burges and a j smola editors 
advances in kernel methods - support vector learning 
pages - mit press 
 l s larkey a patent search and classification system in 
proceedings of the fourth acm conference on digital 
libraries pages - 
 d d lewis an evaluation of phrasal and clustered 
representations on a text categorization task in sigir 
proceedings of the th annual international acm 
conference on research and development in information 
retrieval pages - 
 y liu j carbonell and r jin a pairwise ensemble 
approach for accurate genre classification in proceedings of 
the european conference on machine learning ecml 
 
 y liu r yan r jin and j carbonell a comparison study 
of kernels for multi-label text classification using category 
association in the twenty-first international conference on 
machine learning icml 
 a mccallum and k nigam a comparison of event models 
for naive bayes text classification in working notes of aaai 
 the th national conference on artificial 
intelligence workshop on learning for text categorization 
pages - tr ws- - 
 f sebastiani machine learning in automated text 
categorization acm computing surveys - march 
 
 c j van rijsbergen information retrieval butterworths 
london 
 y yang an evaluation of statistical approaches to text 
categorization information retrieval - 
 y yang j carbonell r brown t pierce b t archibald 
and x liu learning approaches to topic detection and 
tracking ieee expert special issue on applications of 
intelligent information retrieval 
 y yang and x liu a re-examination of text categorization 
methods in sigir proceedings of the nd annual 
international acm conference on research and 
development in information retrieval pages - 
 y yang j zhang j carbonell and c jin 
topic-conditioned novelty detection in proceedings of the 
acm sigkdd international conference on knowledge 
discovery and data mining july 
a semantic approach to contextual advertising 
andrei broder marcus fontoura vanja josifovski lance riedel 
yahoo research mission college blvd santa clara ca 
{broder marcusf vanjaj riedell} yahoo-inc com 
abstract 
contextual advertising or context match cm refers to the 
placement of commercial textual advertisements within the 
content of a generic web page while sponsored search ss 
advertising consists in placing ads on result pages from a web 
search engine with ads driven by the originating query in 
cm there is usually an intermediary commercial ad-network 
entity in charge of optimizing the ad selection with the twin 
goal of increasing revenue shared between the publisher and 
the ad-network and improving the user experience with 
these goals in mind it is preferable to have ads relevant to 
the page content rather than generic ads 
the ss market developed quicker than the cm market 
and most textual ads are still characterized by bid phrases 
representing those queries where the advertisers would like 
to have their ad displayed hence the first technologies 
for cm have relied on previous solutions for ss by simply 
extracting one or more phrases from the given page 
content and displaying ads corresponding to searches on these 
phrases in a purely syntactic approach however due to the 
vagaries of phrase extraction and the lack of context this 
approach leads to many irrelevant ads to overcome this 
problem we propose a system for contextual ad matching 
based on a combination of semantic and syntactic features 
categories and subject descriptors h 
 information storage and retrieval selection process 
general terms algorithms measurement performance 
experimentation 
 introduction 
web advertising supports a large swath of today s internet 
ecosystem the total internet advertiser spend in us alone 
in is estimated at over billion dollars with a growth 
rate of almost year over year a large part of this 
market consists of textual ads that is short text messages 
usually marked as sponsored links or similar the main 
advertising channels used to distribute textual ads are 
 sponsored search or paid search advertising which 
consists in placing ads on the result pages from a web 
search engine with ads driven by the originating query 
all major current web search engines google yahoo 
and microsoft support such ads and act 
simultaneously as a search engine and an ad agency 
 contextual advertising or context match which refers 
to the placement of commercial ads within the 
content of a generic web page in contextual advertising 
usually there is a commercial intermediary called an 
ad-network in charge of optimizing the ad selection 
with the twin goal of increasing revenue shared 
between publisher and ad-network and improving user 
experience again all major current web search 
engines google yahoo and microsoft provide such 
ad-networking services but there are also many smaller 
players 
the ss market developed quicker than the cm market 
and most textual ads are still characterized by bid phrases 
representing those queries where the advertisers would like 
to have their ad displayed see for a brief history 
however today almost all of the for-profit non-transactional 
web sites that is sites that do not sell anything directly 
rely at least in part on revenue from context match cm 
supports sites that range from individual bloggers and small 
niche communities to large publishers such as major 
newspapers without this model the web would be a lot smaller 
the prevalent pricing model for textual ads is that the 
advertisers pay a certain amount for every click on the 
advertisement pay-per-click or ppc there are also other 
models used pay-per-impression where the advertisers pay 
for the number of exposures of an ad and pay-per-action 
where the advertiser pays only if the ad leads to a sale or 
similar transaction for simplicity we only deal with the 
ppc model in this paper 
given a page rather than placing generic ads it seems 
preferable to have ads related to the content to provide a 
better user experience and thus to increase the probability 
of clicks this intuition is supported by the analogy to 
conventional publishing where there are very successful 
magazines e g vogue where a majority of the content is topical 
advertising fashion in the case of vogue and by user 
studies that have confirmed that increased relevance increases 
the number of ad-clicks 
previous published approaches estimated the ad relevance 
based on co-occurrence of the same words or phrases within 
the ad and within the page see and section for 
more details however targeting mechanisms based solely 
on phrases found within the text of the page can lead to 
problems for example a page about a famous golfer named 
john maytag might trigger an ad for maytag 
dishwashers since maytag is a popular brand another example 
could be a page describing the chevy tahoe truck a 
popular vehicle in us triggering an ad about lake tahoe 
vacations polysemy is not the only culprit there is a maybe 
apocryphal story about a lurid news item about a headless 
body found in a suitcase triggering an ad for samsonite 
luggage in all these examples the mismatch arises from the 
fact that the ads are not appropriate for the context 
in order to solve this problem we propose a matching 
mechanism that combines a semantic phase with the 
traditional keyword matching that is a syntactic phase the 
semantic phase classifies the page and the ads into a 
taxonomy of topics and uses the proximity of the ad and page 
classes as a factor in the ad ranking formula hence we 
favor ads that are topically related to the page and thus avoid 
the pitfalls of the purely syntactic approach furthermore 
by using a hierarchical taxonomy we allow for the gradual 
generalization of the ad search space in the case when there 
are no ads matching the precise topic of the page for 
example if the page is about an event in curling a rare winter 
sport and contains the words alpine meadows the 
system would still rank highly ads for skiing in alpine meadows 
as these ads belong to the class skiing which is a sibling 
of the class curling and both of these classes share the 
parent winter sports 
in some sense the taxonomy classes are used to select the 
set of applicable ads and the keywords are used to narrow 
down the search to concepts that are of too small 
granularity to be in the taxonomy the taxonomy contains nodes for 
topics that do not change fast for example brands of digital 
cameras say canon the keywords capture the specificity 
to a level that is more dynamic and granular in the 
digital camera example this would correspond to the level of a 
particular model say canon sd whose advertising life 
might be just a few months updating the taxonomy with 
new nodes or even new vocabulary each time a new model 
comes to the market is prohibitively expensive when we are 
dealing with millions of manufacturers 
in addition to increased click through rate ctr due to 
increased relevance a significant but harder to quantify 
benefit of the semantic-syntactic matching is that the resulting 
page has a unified feel and improves the user experience in 
the chevy tahoe example above the classifier would 
establish that the page is about cars automotive and only those 
ads will be considered even if there are no ads for this 
particular chevy model the chosen ads will still be within the 
automotive domain 
to implement our approach we need to solve a challenging 
problem classify both pages and ads within a large 
taxonomy so that the topic granularity would be small enough 
with high precision to reduce the probability of mis-match 
we evaluated several classifiers and taxonomies and in this 
paper we present results using a taxonomy with close to 
 nodes using a variation of the rocchio s classifier 
this classifier gave the best results in both page and ad 
classification and ultimately in ad relevance 
the paper proceeds as follows in the next section we 
review the basic principles of the contextual advertising 
section overviews the related work section describes 
the taxonomy and document classifier that were used for 
page and ad classification section describes the 
semanticsyntactic method in section we briefly discuss how to 
search efficiently the ad space in order to return the top-k 
ranked ads experimental evaluation is presented in 
section finally section presents the concluding remarks 
 overview of contextual 
advertising 
contextual advertising is an interplay of four players 
 the publisher is the owner of the web pages on which 
the advertising is displayed the publisher typically 
aims to maximize advertising revenue while providing 
a good user experience 
 the advertiser provides the supply of ads usually 
the activity of the advertisers are organized around 
campaigns which are defined by a set of ads with a 
particular temporal and thematic goal e g sale of digital 
cameras during the holiday season as in traditional 
advertising the goal of the advertisers can be broadly 
defined as the promotion of products or services 
 the ad network is a mediator between the advertiser 
and the publisher and selects the ads that are put on 
the pages the ad-network shares the advertisement 
revenue with the publisher 
 users visit the web pages of the publisher and interact 
with the ads 
contextual advertising usually falls into the category of 
direct marketing as opposed to brand advertising that is 
advertising whose aim is a direct response where the 
effect of an campaign is measured by the user reaction one 
of the advantages of online advertising in general and 
contextual advertising in particular is that compared to the 
traditional media it is relatively easy to measure the user 
response usually the desired immediate reaction is for the 
user to follow the link in the ad and visit the advertiser s 
web site and as noted the prevalent financial model is that 
the advertiser pays a certain amount for every click on the 
advertisement ppc the revenue is shared between the 
publisher and the network 
context match advertising has grown from sponsored search 
advertising which consists in placing ads on the result pages 
from a web search engine with ads driven by the originating 
query in most networks the amount paid by the advertiser 
for each ss click is determined by an auction process where 
the advertisers place bids on a search phrase and their 
position in the tower of ads displayed in conjunction with the 
result is determined by their bid thus each ad is 
annotated with one or more bid phrases the bid phrase has no 
direct bearing on the ad placement in cm however it is a 
concise description of target ad audience as determined by 
the advertiser and it has been shown to be an important 
feature for successful cm ad placement in addition to 
the bid phrase an ad is also characterized by a title usually 
displayed in a bold font and an abstract or creative which 
is the few lines of text usually less than characters 
displayed on the page 
the ad-network model aligns the interests of the 
publishers advertisers and the network in general clicks bring 
benefits to both the publisher and the ad network by 
providing revenue and to the advertiser by bringing traffic to 
the target web site the revenue of the network given a 
page p can be estimated as 
r 
x 
i k 
p click p ai price ai i 
where k is the number of ads displayed on page p and price ai i 
is the click-price of the current ad ai at position i the 
price in this model depends on the set of ads presented on 
the page several models have been proposed to determine 
the price most of them based on generalizations of second 
price auctions however for simplicity we ignore the pricing 
model and concentrate on finding ads that will maximize the 
first term of the product that is we search for 
arg max 
i 
p click p ai 
furthermore we assume that the probability of click for a 
given ad and page is determined by its relevance score with 
respect to the page thus ignoring the positional effect of 
the ad placement on the page we assume that this is an 
orthogonal factor to the relevance component and could be 
easily incorporated in the model 
 related work 
online advertising in general and contextual advertising 
in particular are emerging areas of research the published 
literature is very sparse a study presented in confirms 
the intuition that ads need to be relevant to the user s 
interest to avoid degrading the user s experience and increase 
the probability of reaction 
a recent work by ribeiro-neto et al examines a 
number of strategies to match pages to ads based on extracted 
keywords the ads and pages are represented as vectors in 
a vector space the first five strategies proposed in that 
work match the pages and the ads based on the cosine of 
the angle between the ad vector and the page vector to 
find out the important part of the ad the authors explore 
using different ad sections bid phrase title body as a 
basis for the ad vector the winning strategy out of the first 
five requires the bid phrase to appear on the page and then 
ranks all such ads by the cosine of the union of all the ad 
sections and the page vectors 
while both pages and ads are mapped to the same space 
there is a discrepancy impendence mismatch between the 
vocabulary used in the ads and in the pages furthermore 
since in the vector model the dimensions are determined 
by the number of unique words plain cosine similarity will 
not take into account synonyms to solve this problem 
ribeiro-neto et al expand the page vocabulary with terms 
from other similar pages weighted based on the overall 
similarity of the origin page to the matched page and show 
improved matching precision 
in a follow-up work the authors propose a method to 
learn impact of individual features using genetic 
programming to produce a matching function the function is 
represented as a tree composed of arithmetic operators and the log 
function as internal nodes and different numerical features 
of the query and ad terms as leafs the results show that 
genetic programming finds matching functions that 
significantly improve the matching compared to the best method 
 without page side expansion reported in 
another approach to contextual advertising is to reduce it 
to the problem of sponsored search advertising by 
extracting phrases from the page and matching them with the bid 
phrase of the ads in a system for phrase extraction is 
described that used a variety of features to determine the 
importance of page phrases for advertising purposes the 
system is trained with pages that have been hand 
annotated with important phrases the learning algorithm takes 
into account features based on tf-idf html meta data and 
query logs to detect the most important phrases during 
evaluation each page phrase up to length is considered 
as potential result and evaluated against a trained classifier 
in our work we also experimented with a phrase extractor 
based on the work reported in while increasing slightly 
the precision it did not change the relative performance of 
the explored algorithms 
 page and ad classification 
 taxonomy choice 
the semantic match of the pages and the ads is performed 
by classifying both into a common taxonomy the 
matching process requires that the taxonomy provides sufficient 
differentiation between the common commercial topics for 
example classifying all medical related pages into one node 
will not result into a good classification since both sore 
foot and flu pages will end up in the same node the 
ads suitable for these two concepts are however very 
different to obtain sufficient resolution we used a taxonomy of 
around nodes primarily built for classifying commercial 
interest queries rather than pages or ads this taxonomy 
has been commercially built by yahoo us we will explain 
below how we can use the same taxonomy to classify pages 
and ads as well 
each node in our source taxonomy is represented as a 
collection of exemplary bid phrases or queries that correspond 
to that node concept each node has on average around 
queries the queries placed in the taxonomy are high 
volume queries and queries of high interest to advertisers as 
indicated by an unusually high cost-per-click cpc price 
the taxonomy has been populated by human editors 
using keyword suggestions tools similar to the ones used by 
ad networks to suggest keywords to advertisers after 
initial seeding with a few queries using the provided tools a 
human editor can add several hundreds queries to a given 
node nevertheless it has been a significant effort to 
develop this -nodes taxonomy and it has required several 
person-years of work a similar-in-spirit process for 
building enterprise taxonomies via queries has been presented in 
 however the details and tools are completely different 
figure provides some statistics about the taxonomy used 
in this work 
 classification method 
as explained the semantic phase of the matching relies 
on ads and pages being topically close thus we need to 
classify pages into the same taxonomy used to classify ads 
in this section we overview the methods we used to build a 
page and an ad classifier pair the detailed description and 
evaluation of this process is outside the scope of this paper 
given the taxonomy of queries or bid-phrases - we use 
these terms interchangeably described in the previous 
section we tried three methods to build corresponding page 
and ad classifiers for the first two methods we tried to 
find exemplary pages and ads for each concept as follows 
number of categories by level 
 
 
 
 
 
 
 
 
 
 
 
 
level 
numberofcategories 
number of children per nodes 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
number of children 
numberofnodes 
queries per node 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
number queries up to 
numberofnodes 
figure taxonomy statistics categories per level fanout for non-leaf nodes and queries per node 
we generated a page training set by running the queries in 
the taxonomy over a web search index and using the top 
 results after some filtering as documents labeled with the 
query s label on the ad side we generated a training set 
for each class by selecting the ads that have a bid phrase 
assigned to this class using this training sets we then trained 
a hierarchical svm one against all between every group 
of siblings and a log-regression classifier the 
second method differs from the first in the type of secondary 
filtering used this filtering eliminates low content pages 
pages deemed unsuitable for advertising pages that lead to 
excessive class confusion etc 
however we obtained the best performance by using the 
third document classifier based on the information in the 
source taxonomy queries only for each taxonomy node we 
concatenated all the exemplary queries into a single 
metadocument we then used the meta document as a centroid 
for a nearest-neighbor classifier based on rocchio s 
framework with only positive examples and no relevance 
feedback each centroid is defined as a sum of the tf-idf values 
of each term normalized by the number of queries in the 
class 
cj 
 
 cj 
x 
q∈cj 
q 
q 
where cj is the centroid for class cj q iterates over the 
queries in a particular class 
the classification is based on the cosine of the angle 
between the document d and the centroid meta-documents 
cmax arg max 
cj ∈c 
cj 
cj 
· 
d 
d 
 arg max 
cj ∈c 
p 
i∈ f ci 
j· di 
qp 
i∈ f ci 
j 
qp 
i∈ f di 
where f is the set of features the score is normalized by 
the document and class length to produce comparable score 
the terms ci 
and di 
represent the weight of the ith feature 
in the class centroid and the document respectively these 
weights are based on the standard tf-idf formula as the 
score of the max class is normalized with regard to document 
length the scores for different documents are comparable 
we conducted tests using professional editors to judge the 
quality of page and ad class assignments the tests showed 
that for both ads and pages the rocchio classifier returned 
the best results especially on the page side this is 
probably a result of the noise induced by using a search engine 
to machine generate training pages for the svm and 
logregression classifiers it is an area of current investigation 
how to improve the classification using a noisy training set 
based on the test results we decided to use the rocchio s 
classifier on both the ad and the page side 
 semantic-syntactic matching 
contextual advertising systems process the content of the 
page extract features and then search the ad space to find 
the best matching ads given a page p and a set of ads 
a {a as} we estimate the relative probability of click 
p click p a with a score that captures the quality of the 
match between the page and the ad to find the best ads 
for a page we rank the ads in a and select the top few for 
display the problem can be formally defined as matching 
every page in the set of all pages p {p ppc} to one or 
more ads in the set of ads each page is represented as a 
set of page sections pi {pi pi pi m} the sections of 
the page represent different structural parts such as title 
metadata body headings etc in turn each section is an 
unordered bag of terms keywords a page is represented 
by the union of the terms in each section 
pi {pws 
 pws 
 pwsi 
m} 
where pw stands for a page word and the superscript 
indicates the section of each term a term can be a unigram or 
a phrase extracted by a phrase extractor 
similarly we represent each ad as a set of sections a 
{a a al} each section in turn being an unordered set 
of terms 
ai {aws 
 aws 
 awsj 
l } 
where aw is an ad word the ads in our experiments have 
 sections title body and bid phrase in this work to 
produce the match score we use only the ad page textual 
information leaving user information and other data for 
future work 
next each page and ad term is associated with a weight 
based on the tf-idf values the tf value is determined based 
on the individual ad sections there are several choices for 
the value of idf based on different scopes on the ad side 
it has been shown in previous work that the set of ads of 
one campaign provide good scope for the estimation of idf 
that leads to improved matching results however in this 
work for simplicity we do not take into account campaigns 
to combine the impact of the term s section and its tf-idf 
score the ad page term weight is defined as 
tweight kwsi 
 weightsection si · tf idf kw 
where tweight stands for term weight and weightsection si 
is the weight assigned to a page or ad section this weight 
is a fixed parameter determined by experimentation 
each ad and page is classified into the topical taxonomy 
we define these two mappings 
tax pi {pci pciu} 
tax aj {acj acjv} 
where pc and ac are page and ad classes correspondingly 
each assignment is associated with a weight given by the 
classifier the weights are normalized to sum to 
x 
c∈t ax xi 
cweight c 
where xi is either a page or an ad and cweights c is the 
class weight - normalized confidence assigned by the 
classifier the number of classes can vary between different pages 
and ads this corresponds to the number of topics a page ad 
can be associated with and is almost always in the range - 
now we define the relevance score of an ad ai and page 
pi as a convex combination of the keyword syntactic and 
classification semantic score 
score pi ai α · taxscore tax pi tax ai 
 − α · keywordscore pi ai 
the parameter α determines the relative weight of the 
taxonomy score and the keyword score 
to calculate the keyword score we use the vector space 
model where both the pages and ads are represented 
in n-dimensional space - one dimension for each distinct 
term the magnitude of each dimension is determined by 
the tweight formula the keyword score is then defined as 
the cosine of the angle between the page and the ad vectors 
keywordscore pi ai 
 
p 
i∈ k tweight pwi · tweight awi 
qp 
i∈ k tweight pwi 
qp 
i∈ k tweight awi 
where k is the set of all the keywords the formula 
assumes independence between the words in the pages and 
ads furthermore it ignores the order and the proximity of 
the terms in the scoring we experimented with the impact 
of phrases and proximity on the keyword score and did not 
see a substantial impact of these two factors 
we now turn to the definition of the taxscore this 
function indicates the topical match between a given ad and 
a page as opposed to the keywords that are treated as 
independent dimensions here the classes topics are 
organized into a hierarchy one of the goals in the design of 
the taxscore function is to be able to generalize within the 
taxonomy that is accept topically related ads 
generalization can help to place ads in cases when there is no ad that 
matches both the category and the keywords of the page 
the example in figure illustrates this situation in this 
example in the absence of ski ads a page about skiing 
containing the word atomic could be matched to the available 
snowboarding ad for the same brand 
in general we would like the match to be stronger when 
both the ad and the page are classified into the same node 
figure two generalization paths 
and weaker when the distance between the nodes in the 
taxonomy gets larger there are multiple ways to specify the 
distance between two taxonomy nodes besides the above 
requirement this function should lend itself to an efficient 
search of the ad space given a page we have to find the 
ad in a few milliseconds as this impacts the presentation to 
a waiting user this will be further discussed in the next 
section 
to capture both the generalization and efficiency 
requirements we define the taxscore function as follows 
taxscore pc ac 
x 
pc∈p c 
x 
ac∈ac 
idist lca pc ac ac ·cweight pc ·cweight ac 
in this function we consider every combination of page class 
and ad class for each combination we multiply the product 
of the class weights with the inverse distance function 
between the least common ancestor of the two classes lca 
and the ad class the inverse distance function idist c c 
takes two nodes on the same path in the class taxonomy 
and returns a number in the interval depending on the 
distance of the two class nodes it returns if the two nodes 
are the same and declines toward when lca pc ac or ac 
is the root of the taxonomy the rate of decline determines 
the weight of the generalization versus the other terms in 
the scoring formula 
to determine the rate of decline we consider the impact 
on the specificity of the match when we substitute a class 
with one of its ancestors in general the impact is topic 
dependent for example the node hobby in our taxonomy 
has tens of children each representing a different hobby two 
examples being sailing and knitting placing an ad 
about knitting on a page about sailing does not make 
lots of sense however in the winter sports example 
above in the absence of better alternative skiing ads could 
be put on snowboarding pages as they might promote the 
same venues equipment vendors etc such detailed analysis 
on a case by case basis is prohibitively expensive due to the 
size of the taxonomy 
one option is to use the confidences of the ancestor classes 
as given by the classifier however we found these 
numbers not suitable for this purpose as the magnitude of the 
confidences does not necessarily decrease when going up the 
tree another option is to use explore-exploit methods based 
on machine-learning from the click feedback as described 
in however for simplicity in this work we chose a 
simple heuristic to determine the cost of generalization from a 
child to a parent in this heuristic we look at the 
broadening of the scope when moving from a child to a parent we 
estimate the broadening by the density of ads classified in 
the parent nodes vs the child node the density is obtained 
by classifying a large set of ads in the taxonomy using the 
document classifier described above based on this let nc 
be the number of document classified into the subtree rooted 
at c then we define 
idist c p 
nc 
np 
where c represents the child node and p is the parent node 
this fraction can be viewed as a probability of an ad 
belonging to the parent topic being suitable for the child topic 
 searching the ad space 
in the previous section we discussed the choice of scoring 
function to estimate the match between an ad and a page 
the top-k ads with the highest score are offered by the 
system for placement on the publisher s page the process of 
score calculation and ad selection is to be done in real time 
and therefore must be very efficient as the ad collections 
are in the range of hundreds of millions of entries there is a 
need for indexed access to the ads 
inverted indexes provide scalable and low latency 
solutions for searching documents however these have been 
traditionally used to search based on keywords to be able 
to search the ads on a combination of keywords and classes 
we have mapped the classification match to term match and 
adapted the scoring function to be suitable for fast 
evaluation over inverted indexes in this section we overview the 
ad indexing and the ranking function of our prototype ad 
search system for matching ads and pages 
we used a standard inverted index framework where there 
is one posting list for each distinct term the ads are parsed 
into terms and each term is associated with a weight based 
on the section in which it appears weights from distinct 
occurrences of a term in an ad are added together so that 
the posting lists contain one entry per term ad combination 
the next challenge is how to index the ads so that the class 
information is preserved in the index a simple method is to 
create unique meta-terms for the classes and annotate each 
ad with one meta-term for each assigned class however 
this method does not allow for generalization since only the 
ads matching an exact label of the page would be selected 
therefore we annotated each ad with one meta-term for each 
ancestor of the assigned class the weights of meta-terms 
are assigned according to the value of the idist function 
defined in the previous section on the query side given the 
keywords and the class of a page we compose a keyword only 
query by inserting one class term for each ancestor of the 
classes assigned to the page 
the scoring function is adapted to the two part 
scoreone for the class meta-terms and another for the text term 
during the class score calculation for each class path we use 
only the lowest class meta-term ignoring the others for 
example if an ad belongs to the skiing class and is 
annotated with both skiing and its parent winter sports 
the index will contain the special class meta-terms for both 
skiing and winter sports and all their ancestors with 
the weights according to the product of the classifier 
confidence and the idist function when matching with a page 
classified into skiing the query will contain terms for class 
skiing and for each of its ancestors however when scoring 
an ad classified into skiing we will use the weight for the 
skiing class meta-term ads classified into 
snowboarding will be scored using the weight of the winter sports 
meta-term to make this check efficiently we keep a sorted 
list of all the class paths and at scoring time we search the 
paths bottom up for a meta-term appearing in the ad the 
first meta-term is used for scoring the rest are ignored 
at runtime we evaluate the query using a variant of the 
wand algorithm this is a document-at-a-time 
algorithm that uses a branch-and-bound approach to derive 
efficient moves for the cursors associated to the postings 
lists it finds the next cursor to be moved based on an 
upper bound of the score for the documents at which the 
cursors are currently positioned the algorithm keeps a heap of 
current best candidates documents with an upper bound 
smaller than the current minimum score among the 
candidate documents can be eliminated from further 
considerations and thus the cursors can skip over them to find the 
upper bound for a document the algorithm assumes that all 
cursors that are before it will hit this document i e the 
document contains all those terms represented by cursors before 
or at that document it has been shown that wand can 
be used with any function that is monotonic with respect to 
the number of matching terms in the document 
our scoring function is monotonic since the score can 
never decrease when more terms are found in the document 
in the special case when we add a cursor representing an 
ancestor of a class term already factored in the score the 
score simply does not change we add given these 
properties we use an adaptation of the wand algorithm where 
we change the calculation of the scoring function and the 
upper bound score calculation to reflect our scoring function 
the rest of the algorithm remains unchanged 
 experimental evaluation 
 data and methodology 
we quantify the effect of the semantic-syntactic matching 
using a set of pages this set of pages was selected 
by a random sample of a larger set of around million 
pages with contextual advertising ads for each of these 
pages have been selected from a larger pool of ads tens of 
millions by previous experiments conducted by yahoo us 
for other purposes each page-ad pair has been judged by 
three or more human judges on a to scale 
 relevant the ad is semantically directly related to 
the main subject of the page for example if the page 
is about the national football league and the ad is 
about tickets for nfl games it would be scored as 
 somewhat relevant the ad is related to the 
secondary subject of the page or is related to the main 
topic of the page in a general way in the nfl page 
example an ad about nfl branded products would 
be judged as 
 irrelevant the ad is unrelated to the page for 
example a mention of the nfl player john maytag triggers 
washing machine ads on a nfl page 
pages 
words per page 
judgments 
judg inter-editor agreement 
unique ads 
unique ads per page 
page classification precision 
ad classification precision 
table dataset statistics 
to obtain a score for a page-ad pair we average all the scores 
and then round to the closest integer we then used these 
judgments to evaluate how well our methods distinguish the 
positive and the negative ad assignments for each page the 
statistics of the page dataset is given in table 
the original experiments that paired the pages and the 
ads are loosely related to the syntactic keyword based 
matching and classification based assignment but used different 
taxonomies and keyword extraction techniques therefore 
we could not use standard pooling as an evaluation method 
since we did not control the way the pairs were selected and 
could not precisely establish the set of ads from which the 
placed ads were selected 
instead in our evaluation for each page we consider only 
those ads for which we have judgment each different method 
was applied to this set and the ads were ranked by the score 
the relative effectiveness of the algorithms were judged by 
comparing how well the methods separated the ads with 
positive judgment from the ads with negative judgment we 
present precision on various levels of recall within this set 
as the set of ads per page is relatively small the evaluation 
reports precision that is higher than it would be with a larger 
set of negative ads however these numbers still establish 
the relative performance of the algorithms and we can use 
it to evaluate performance at different score thresholds 
in addition to the precision-recall over the judged ads 
we also present kendall s τ rank correlation coefficient to 
establish how far from the perfect ordering are the orderings 
produced by our ranking algorithms for this test we ranked 
the judged ads by the scores assigned by the judges and then 
compared this order to the order assigned by our algorithms 
finally we also examined the precision at position and 
 
 results 
figure shows the precision recall curves for the 
syntactic matching keywords only used vs a syntactic-semantic 
matching with the optimal value of α judged by the 
 -point score in this figure we assume that the 
adpage pairs judged with or are positive examples and the 
 s are negative examples we also examined counting only 
the pairs judged with as positive examples and did not 
find a significant change in the relative performance of the 
tested methods overlaid are also results using phrases in 
the keyword match we see that these additional features 
do not change the relative performance of the algorithm 
the graphs show significant impact of the class 
information especially in the mid range of recall values in the 
low recall part of the chart the curves meet this indicates 
that when the keyword match is really strong i e when 
the ad is almost contained within the page the precision 
 
 
 
 
 
 
 
 
recall 
precision 
alpha no phrase alpha no phrase 
alpha phrase alpha phrase 
figure data set precision vs recall of 
syntactic match α vs syntactic-semantic match 
 α 
α kendall s τ 
α 
α 
α 
α 
α 
table kendall s τ for different α values 
of the syntactic keyword match is comparable with that of 
the semantic-syntactic match note however that the two 
methods might produce different ads and could be used as 
a complement at level of recall 
the semantic components provides largest lift in 
precision at the mid range of recall where improvement is 
achieved by using the class information for ad placement 
this means that when there is somewhat of a match 
between the ad and the page the restriction to the right classes 
provides a better scope for selecting the ads 
table shows the kendall s τ values for different values of 
α we calculated the values by ranking all the judged ads for 
each page and averaging the values over all the pages the 
ads with tied judgment were given the rank of the middle 
position the results show that when we take into account 
all the ad-page pairs the optimal value of α is around 
note that purely syntactic match α is by far the 
weakest method 
figure shows the effect of the parameter α in the scoring 
we see that in most cases precision grows or is flat when we 
increase α except at the low level of recall where due to 
small number of data points there is a bit of jitter in the 
results 
table shows the precision at positions and again 
the purely syntactic method has clearly the lowest score by 
individual positions and the total number of correctly placed 
ads the numbers are close due to the small number of the 
ads considered but there are still some noticeable trends 
for position the optimal α is in the range of to 
for positions and the optimum is at α this also 
indicates that for those ads that have a very high keyword 
score the semantic information is somewhat less important 
if almost all the words in an ad appear in the page this ad is 
precision vs alpha for different levels of recall 
 data set 
 
 
 
 
 
 
 
alpha 
precision 
 recall recall recall recall 
figure impact of α on precision for different levels 
of recall 
α sum 
α 
α 
α 
α 
α 
table precision at position and 
likely to be relevant for this page however when there is no 
such clear affinity the class information becomes a dominant 
factor 
 conclusion 
contextual advertising is the economic engine behind a 
large number of non-transactional sites on the web studies 
have shown that one of the main success factors for 
contextual ads is their relevance to the surrounding content all 
existing commercial contextual match solutions known to us 
evolved from search advertising solutions whereby a search 
query is matched to the bid phrase of the ads a natural 
extension of search advertising is to extract phrases from the 
page and match them to the bid phrase of the ads however 
individual phrases and words might have multiple meanings 
and or be unrelated to the overall topic of the page leading 
to miss-matched ads 
in this paper we proposed a novel way of matching 
advertisements to web pages that rely on a topical semantic 
match as a major component of the relevance score the 
semantic match relies on the classification of pages and ads 
into a nodes commercial advertising taxonomy to 
determine their topical distance as the classification relies 
on the full content of the page it is more robust than 
individual page phrases the semantic match is complemented 
with a syntactic match and the final score is a convex 
combination of the two sub-scores with the relative weight of 
each determined by a parameter α 
we evaluated the semantic-syntactic approach against a 
syntactic approach over a set of pages with different 
contextual advertising as shown in our experimental evaluation 
the optimal value of the parameter α depends on the precise 
objective of optimization precision at particular position 
precision at given recall however in all cases the optimal 
value of α is between and indicating significant effect 
of the semantic score component the effectiveness of the 
syntactic match depends on the quality of the pages used in 
lower quality pages we are more likely to make classification 
errors that will then negatively impact the matching we 
demonstrated that it is feasible to build a large scale 
classifier that has sufficient good precision for this application 
we are currently examining how to employ machine 
learning algorithms to learn the optimal value of α based on a 
collection of features of the input pages 
 references 
 r baeza-yates and b ribeiro-neto modern information 
retrieval acm 
 bernhard e boser isabelle guyon and vladimir vapnik 
a training algorithm for optimal margin classifiers in 
computational learning theory pages - 
 a z broder d carmel m herscovici a soffer and 
j zien efficient query evaluation using a two-level 
retrieval process in cikm proc of the twelfth intl 
conf on information and knowledge management pages 
 - new york ny acm 
 p chatterjee d l hoffman and t p novak modeling 
the clickstream implications for web-based advertising 
efforts marketing science - 
 d fain and j pedersen sponsored search a brief history 
in in proc of the second workshop on sponsored search 
auctions web publication 
 s c gates w teiken and k -shin f cheng taxonomies 
by the numbers building high-performance taxonomies in 
cikm proc of the th acm intl conf on 
information and knowledge management pages - 
new york ny acm 
 a lacerda m cristo m andre g w fan n ziviani 
and b ribeiro-neto learning to advertise in sigir 
proc of the th annual intl acm sigir conf pages 
 - new york ny acm 
 b ribeiro-neto m cristo p b golgher and e s 
de moura impedance coupling in content-targeted 
advertising in sigir proc of the th annual intl 
acm sigir conf pages - new york ny 
acm 
 j rocchio relevance feedback in information retrieval in 
the smart retrieval system experiments in automatic 
document processing pages - prenticehall 
 p sandeep d agarwal d chakrabarti and v josifovski 
bandits for taxonomies a model-based approach in in 
proc of the siam intl conf on data mining 
 t santner and d duffy the statistical analysis of 
discrete data springer-verlag 
 r stata k bharat and f maghoul the term vector 
database fast access to indexing terms for web pages 
computer networks - - 
 c wang p zhang r choi and m d eredita 
understanding consumers attitude toward advertising in 
eighth americas conf on information system pages 
 - 
 w yih j goodman and v r carvalho finding 
advertising keywords on web pages in www proc of 
the th intl conf on world wide web pages - 
new york ny acm 
context sensitive stemming for web search 
fuchun peng nawaaz ahmed xin li yumao lu 
yahoo inc 
 first avenue 
sunnyvale california 
{fuchun nawaaz xinli yumaol} yahoo-inc com 
abstract 
traditionally stemming has been applied to information 
retrieval tasks by transforming words in documents to the 
their root form before indexing and applying a similar 
transformation to query terms although it increases recall this 
naive strategy does not work well for web search since it 
lowers precision and requires a significant amount of 
additional computation 
in this paper we propose a context sensitive stemming 
method that addresses these two issues two unique 
properties make our approach feasible for web search first based 
on statistical language modeling we perform context 
sensitive analysis on the query side we accurately predict which 
of its morphological variants is useful to expand a query term 
with before submitting the query to the search engine this 
dramatically reduces the number of bad expansions which 
in turn reduces the cost of additional computation and 
improves the precision at the same time second our approach 
performs a context sensitive document matching for those 
expanded variants this conservative strategy serves as a 
safeguard against spurious stemming and it turns out to be 
very important for improving precision using word 
pluralization handling as an example of our stemming approach 
our experiments on a major web search engine show that 
stemming only of the query traffic we can improve 
relevance as measured by average discounted cumulative 
gain dcg by on these queries and over all 
query traffic 
categories and subject descriptors 
h information systems information storage and 
retrieval-query formulation 
general terms 
algorithms experimentation 
 introduction 
web search has now become a major tool in our daily lives 
for information seeking one of the important issues in web 
search is that user queries are often not best formulated to 
get optimal results for example running shoe is a query 
that occurs frequently in query logs however the query 
running shoes is much more likely to give better search 
results than the original query because documents matching 
the intent of this query usually contain the words running 
shoes 
correctly formulating a query requires the user to 
accurately predict which word form is used in the documents 
that best satisfy his or her information needs this is 
difficult even for experienced users and especially difficult for 
non-native speakers one traditional solution is to use 
stemming the process of transforming inflected or 
derived words to their root form so that a search term will 
match and retrieve documents containing all forms of the 
term thus the word run will match running ran 
runs and shoe will match shoes and shoeing 
stemming can be done either on the terms in a document 
during indexing and applying the same transformation to the 
query terms during query processing or by expanding the 
query with the variants during query processing stemming 
during indexing allows very little flexibility during query 
processing while stemming by query expansion allows 
handling each query differently and hence is preferred 
although traditional stemming increases recall by 
matching word variants it can reduce precision by retrieving 
too many documents that have been incorrectly matched 
when examining the results of applying stemming to a large 
number of queries one usually finds that nearly equal 
numbers of queries are helped and hurt by the technique in 
addition it reduces system performance because the search 
engine has to match all the word variants as we will show 
in the experiments this is true even if we simplify stemming 
to pluralization handling which is the process of converting 
a word from its plural to singular form or vice versa thus 
one needs to be very cautious when using stemming in web 
search engines 
one problem of traditional stemming is its blind 
transformation of all query terms that is it always performs 
the same transformation for the same query word without 
considering the context of the word for example the word 
book has four forms book books booking booked and 
store has four forms store stores storing stored for 
the query book store expanding both words to all of their 
variants significantly increases computation cost and hurts 
precision since not all of the variants are useful for this 
query transforming book store to match book stores 
is fine but matching book storing or booking store is 
not a weighting method that gives variant words smaller 
weights alleviates the problems to a certain extent if the 
weights accurately reflect the importance of the variant in 
this particular query however uniform weighting is not 
going to work and a query dependent weighting is still a 
challenging unsolved problem 
a second problem of traditional stemming is its blind 
matching of all occurrences in documents for the query 
book store a transformation that allows the variant stores 
to be matched will cause every occurrence of stores in the 
document to be treated equivalent to the query term store 
thus a document containing the fragment reading a book 
in coffee stores will be matched causing many wrong 
documents to be selected although we hope the ranking 
function can correctly handle these with many more candidates 
to rank the risk of making mistakes increases 
to alleviate these two problems we propose a context 
sensitive stemming approach for web search our solution 
consists of two context sensitive analysis one on the query 
side and the other on the document side on the query side 
we propose a statistical language modeling based approach 
to predict which word variants are better forms than the 
original word for search purpose and expanding the query 
with only those forms on the document side we propose a 
conservative context sensitive matching for the transformed 
word variants only matching document occurrences in the 
context of other terms in the query our model is simple yet 
effective and efficient making it feasible to be used in real 
commercial web search engines 
we use pluralization handling as a running example for 
our stemming approach the motivation for using 
pluralization handling as an example is to show that even such 
simple stemming if handled correctly can give significant 
benefits to search relevance as far as we know no 
previous research has systematically investigated the usage of 
pluralization in web search as we have to point out the 
method we propose is not limited to pluralization handling 
it is a general stemming technique and can also be applied 
to general query expansion experiments on general 
stemming yield additional significant improvements over 
pluralization handling for long queries although details will not 
be reported in this paper 
in the rest of the paper we first present the related work 
and distinguish our method from previous work in section 
we describe the details of the context sensitive stemming 
approach in section we then perform extensive 
experiments on a major web search engine to support our claims 
in section followed by discussions in section finally 
we conclude the paper in section 
 related work 
stemming is a long studied technology many stemmers 
have been developed such as the lovins stemmer and 
the porter stemmer the porter stemmer is widely used 
due to its simplicity and effectiveness in many applications 
however the porter stemming makes many mistakes 
because its simple rules cannot fully describe english 
morphology corpus analysis is used to improve porter stemmer 
by creating equivalence classes for words that are 
morphologically similar and occur in similar context as measured by 
expected mutual information we use a similar corpus 
based approach for stemming by computing the similarity 
between two words based on their distributional context 
features which can be more than just adjacent words and 
then only keep the morphologically similar words as 
candidates 
using stemming in information retrieval is also a well 
known technique however the effectiveness of 
stemming for english query systems was previously reported to 
be rather limited lennon et al compared the lovins 
and porter algorithms and found little improvement in 
retrieval performance later harman compares three 
general stemming techniques in text retrieval experiments 
including pluralization handing called s stemmer in the 
paper they also proposed selective stemming based on query 
length and term importance but no positive results were 
reported on the other hand krovetz performed 
comparisons over small numbers of documents from to k 
and showed dramatic precision improvement up to 
however due to the limited number of tested queries less 
than and the small size of the collection the results 
are hard to generalize to web search these mixed results 
mostly failures led early ir researchers to deem stemming 
irrelevant in general for english although recent research 
has shown stemming has greater benefits for retrieval in 
other languages we suspect the previous failures were 
mainly due to the two problems we mentioned in the 
introduction blind stemming or a simple query length based 
selective stemming as used in is not enough stemming 
has to be decided on case by case basis not only at the query 
level but also at the document level as we will show if 
handled correctly significant improvement can be achieved 
a more general problem related to stemming is query 
reformulation and query expansion which expands 
words not only with word variants to 
decide which expanded words to use people often use 
pseudorelevance feedback techniquesthat send the original query to 
a search engine and retrieve the top documents extract 
relevant words from these top documents as additional query 
words and resubmit the expanded query again this 
normally requires sending a query multiple times to search 
engine and it is not cost effective for processing the huge 
amount of queries involved in web search in addition 
query expansion including query reformulation has 
a high risk of changing the user intent called query drift 
since the expanded words may have different meanings adding 
them to the query could potentially change the intent of 
the original query thus query expansion based on 
pseudorelevance and query reformulation can provide suggestions 
to users for interactive refinement but can hardly be directly 
used for web search on the other hand stemming is much 
more conservative since most of the time stemming 
preserves the original search intent while most work on query 
expansion focuses on recall enhancement our work focuses 
on increasing both recall and precision the increase on 
recall is obvious with quality stemming good documents 
which were not selected before stemming will be pushed up 
and those low quality documents will be degraded 
on selective query expansion cronen-townsend et al 
proposed a method for selective query expansion based on 
comparing the kullback-leibler divergence of the results 
from the unexpanded query and the results from the 
expanded query this is similar to the relevance feedback in 
the sense that it requires multiple passes retrieval if a word 
can be expanded into several words it requires running this 
process multiple times to decide which expanded word is 
useful it is expensive to deploy this in production web 
search engines our method predicts the quality of 
expansion based on oﬄine information without sending the query 
to a search engine 
in summary we propose a novel approach to attack an old 
yet still important and challenging problem for web search 
- stemming our approach is unique in that it performs 
predictive stemming on a per query basis without relevance 
feedback from the web using the context of the variants in 
documents to preserve precision it s simple yet very 
efficient and effective making real time stemming feasible for 
web search our results will affirm researchers that 
stemming is indeed very important to large scale information 
retrieval 
 context sensitive stemming 
 overview 
our system has four components as illustrated in 
figure candidate generation query segmentation and head 
word detection context sensitive query stemming and 
context sensitive document matching candidate generation 
 component is performed oﬄine and generated candidates 
are stored in a dictionary for an input query we first 
segment query into concepts and detect the head word for each 
concept component we then use statistical language 
modeling to decide whether a particular variant is useful 
 component and finally for the expanded variants we 
perform context sensitive document matching component 
 below we discuss each of the components in more detail 
component context sensitive document matching 
input query 
and head word detection 
component segment component candidate generation 
comparisons − comparison 
component selective word expansion 
decision comparisons − comparison 
example hotel price comparisons 
output hotel comparisons 
hotel − hotels 
figure system components 
 expansion candidate generation 
one of the ways to generate candidates is using the porter 
stemmer the porter stemmer simply uses 
morphological rules to convert a word to its base form it has no 
knowledge of the semantic meaning of the words and sometimes 
makes serious mistakes such as executive to execution 
news to new and paste to past a more 
conservative way is based on using corpus analysis to improve the 
porter stemmer results the corpus analysis we do is 
based on word distributional similarity the rationale 
of using distributional word similarity is that true variants 
tend to be used in similar contexts in the distributional 
word similarity calculation each word is represented with a 
vector of features derived from the context of the word we 
use the bigrams to the left and right of the word as its 
context features by mining a huge web corpus the similarity 
between two words is the cosine similarity between the two 
corresponding feature vectors the top similar words to 
develop is shown in the following table 
rank candidate score rank candidate score 
 develop berts 
 developing wads 
 developed developer 
 incubator promoting 
 develops developmental 
 development reengineering 
 tutoring build 
 analyzing construct 
 developement educational 
 automation institute 
table top most similar candidates to word 
develop column score is the similarity score 
to determine the stemming candidates we apply a few 
porter stemmer morphological rules to the similarity 
list after applying these rules for the word develop 
the stemming candidates are developing developed 
develops development developement developer 
developmental for the pluralization handling purpose only the 
candidate develops is retained 
one thing we note from observing the distributionally 
similar words is that they are closely related semantically 
these words might serve as candidates for general query 
expansion a topic we will investigate in the future 
 segmentation and headword identification 
for long queries it is quite important to detect the 
concepts in the query and the most important words for those 
concepts we first break a query into segments each 
segment representing a concept which normally is a noun phrase 
for each of the noun phrases we then detect the most 
important word which we call the head word segmentation 
is also used in document sensitive matching section to 
enforce proximity 
to break a query into segments we have to define a 
criteria to measure the strength of the relation between words 
one effective method is to use mutual information as an 
indicator on whether or not to split two words we use 
a log of m queries and collect the bigram and unigram 
frequencies from it for every incoming query we compute 
the mutual information of two adjacent words if it passes 
a predefined threshold we do not split the query between 
those two words and move on to next word we continue 
this process until the mutual information between two words 
is below the threshold then create a concept boundary here 
table shows some examples of query segmentation 
the ideal way of finding the head word of a concept is to 
do syntactic parsing to determine the dependency structure 
of the query query parsing is more difficult than sentence 
 running shoe 
 best new york medical schools 
 pictures of white house 
 cookies in san francisco 
 hotel price comparison 
table query segmentation a segment is 
bracketed 
parsing since many queries are not grammatical and are very 
short applying a parser trained on sentences from 
documents to queries will have poor performance in our 
solution we just use simple heuristics rules and it works very 
well in practice for english for an english noun phrase 
the head word is typically the last nonstop word unless the 
phrase is of a particular pattern like xyz of in at from 
uvw in such cases the head word is typically the last 
nonstop word of xyz 
 context sensitive word expansion 
after detecting which words are the most important words 
to expand we have to decide whether the expansions will 
be useful 
our statistics show that about half of the queries can be 
transformed by pluralization via naive stemming among 
this half about of the queries improve relevance when 
transformed the majority about do not change their 
top results and the remaining perform worse thus 
it is extremely important to identify which queries should 
not be stemmed for the purpose of maximizing relevance 
improvement and minimizing stemming cost in addition 
for a query with multiple words that can be transformed 
or a word with multiple variants not all of the expansions 
are useful taking query hotel price comparison as an 
example we decide that hotel and price comparison are two 
concepts head words hotel and comparison can be 
expanded to hotels and comparisons are both 
transformations useful 
to test whether an expansion is useful we have to know 
whether the expanded query is likely to get more relevant 
documents from the web which can be quantified by the 
probability of the query occurring as a string on the web 
the more likely a query to occur on the web the more 
relevant documents this query is able to return now the 
whole problem becomes how to calculate the probability of 
query to occur on the web 
calculating the probability of string occurring in a 
corpus is a well known language modeling problem the goal 
of language modeling is to predict the probability of 
naturally occurring word sequences s w w wn or more 
simply to put high probability on word sequences that 
actually occur and low probability on word sequences that 
never occur the simplest and most successful approach to 
language modeling is still based on the n-gram model by 
the chain rule of probability one can write the probability 
of any word sequence as 
pr w w wn 
ny 
i 
pr wi w wi− 
an n-gram model approximates this probability by 
assuming that the only words relevant to predicting pr wi w wi− 
are the previous n − words i e 
pr wi w wi− pr wi wi−n wi− 
a straightforward maximum likelihood estimate of n-gram 
probabilities from a corpus is given by the observed 
frequency of each of the patterns 
pr wi wi−n wi− 
 wi−n wi 
 wi−n wi− 
 
where denotes the number of occurrences of a specified 
gram in the training corpus although one could attempt to 
use simple n-gram models to capture long range 
dependencies in language attempting to do so directly immediately 
creates sparse data problems using grams of length up to 
n entails estimating the probability of wn 
events where w 
is the size of the word vocabulary this quickly overwhelms 
modern computational and data resources for even modest 
choices of n beyond to also because of the heavy 
tailed nature of language i e zipf s law one is likely to 
encounter novel n-grams that were never witnessed during 
training in any test corpus and therefore some mechanism 
for assigning non-zero probability to novel n-grams is a 
central and unavoidable issue in statistical language modeling 
one standard approach to smoothing probability estimates 
to cope with sparse data problems and to cope with 
potentially missing n-grams is to use some sort of back-off 
estimator 
pr wi wi−n wi− 
 
 
 
 
ˆpr wi wi−n wi− 
if wi−n wi 
β wi−n wi− × pr wi wi−n wi− 
otherwise 
 
where 
ˆpr wi wi−n wi− 
discount wi−n wi 
 wi−n wi− 
 
is the discounted probability and β wi−n wi− is a 
normalization constant 
β wi−n wi− 
 − 
x 
x∈ wi−n wi− x 
ˆpr x wi−n wi− 
 − 
x 
x∈ wi−n wi− x 
ˆpr x wi−n wi− 
 
the discounted probability can be computed with 
different smoothing techniques including absolute smoothing 
good-turing smoothing linear smoothing and witten-bell 
smoothing we used absolute smoothing in our 
experiments 
since the likelihood of a string pr w w wn is a very 
small number and hard to interpret we use entropy as 
defined below to score the string 
entropy − 
 
n 
log pr w w wn 
now getting back to the example of the query hotel price 
comparisons there are four variants of this query and the 
entropy of these four candidates are shown in table we 
can see that all alternatives are less likely than the input 
query it is therefore not useful to make an expansion for this 
query on the other hand if the input query is hotel price 
comparisons which is the second alternative in the table 
then there is a better alternative than the input query and 
it should therefore be expanded to tolerate the variations 
in probability estimation we relax the selection criterion to 
those query alternatives if their scores are within a certain 
distance in our experiments to the best score 
query variations entropy 
hotel price comparison 
hotel price comparisons 
hotels price comparison 
hotels price comparisons 
table variations of query hotel price 
comparison ranked by entropy score with the original 
query in bold face 
 context sensitive document matching 
even after we know which word variants are likely to be 
useful we have to be conservative in document matching 
for the expanded variants for the query hotel price 
comparisons we decided that word comparisons is expanded 
to include comparison however not every occurrence of 
comparison in the document is of interest a page which 
is about comparing customer service can contain all of the 
words hotel price comparisons comparison this page is not 
a good page for the query 
if we accept matches of every occurrence of comparison 
it will hurt retrieval precision and this is one of the main 
reasons why most stemming approaches do not work well 
for information retrieval to address this problem we have 
a proximity constraint that considers the context around 
the expanded variant in the document a variant match 
is considered valid only if the variant occurs in the same 
context as the original word does the context is the left or 
the right non-stop segments 
of the original word taking 
the same query as an example the context of comparisons 
is price the expanded word comparison is only valid if 
it is in the same context of comparisons which is after the 
word price thus we should only match those occurrences 
of comparison in the document if they occur after the word 
price considering the fact that queries and documents 
may not represent the intent in exactly the same way we 
relax this proximity constraint to allow variant occurrences 
within a window of some fixed size if the expanded word 
comparison occurs within the context of price within 
a window it is considered valid the smaller the window 
size is the more restrictive the matching we use a window 
size of which typically captures contexts that include the 
containing and adjacent noun phrases 
 experimental evaluation 
 evaluation metrics 
we will measure both relevance improvement and the 
stemming cost required to achieve the relevance 
 
a context segment can not be a single stop word 
 relevance measurement 
we use a variant of the average discounted cumulative 
gain dcg a recently popularized scheme to measure search 
engine relevance given a query and a ranked list of k 
documents k is set to in our experiments the dcg k 
score for this query is calculated as follows 
dcg k 
kx 
k 
gk 
log k 
 
where gk is the weight for the document at rank k higher 
degree of relevance corresponds to a higher weight a page is 
graded into one of the five scales perfect excellent good 
fair bad with corresponding weights we use dcg to 
represent the average dcg over a set of test queries 
 stemming cost 
another metric is to measure the additional cost incurred 
by stemming given the same level of relevance 
improvement we prefer a stemming method that has less additional 
cost we measure this by the percentage of queries that are 
actually stemmed over all the queries that could possibly 
be stemmed 
 data preparation 
we randomly sample queries from a three month 
query log with from each month among all these 
queries we remove all misspelled queries since misspelled 
queries are not of interest to stemming we also remove all 
one word queries since stemming one word queries without 
context has a high risk of changing query intent especially 
for short words in the end we have correctly spelled 
queries with at least words 
 naive stemming for web search 
before explaining the experiments and results in detail 
we d like to describe the traditional way of using stemming 
for web search referred as the naive model this is to treat 
every word variant equivalent for all possible words in the 
query the query book store will be transformed into 
 book or books store or stores when limiting stemming 
to pluralization handling only where or is an operator that 
denotes the equivalence of the left and right arguments 
 experimental setup 
the baseline model is the model without stemming we 
first run the naive model to see how well it performs over 
the baseline then we improve the naive stemming model 
by document sensitive matching referred as document 
sensitive matching model this model makes the same stemming 
as the naive model on the query side but performs 
conservative matching on the document side using the strategy 
described in section the naive model and document 
sensitive matching model stem the most queries out of the 
 queries there are queries that they stem 
corresponding to query traffic out of a total of we 
then further improve the document sensitive matching model 
from the query side with selective word stemming based on 
statistical language modeling section referred as 
selective stemming model based on language modeling 
prediction this model stems only a subset of the queries 
stemmed by the document sensitive matching model we 
experiment with unigram language model and bigram 
language model since we only care how much we can improve 
the naive model we will only use these queries all the 
queries that are affected by the naive stemming model in 
the experiments 
to get a sense of how these models perform we also have 
an oracle model that gives the upper-bound performance a 
stemmer can achieve on this data the oracle model only 
expands a word if the stemming will give better results 
to analyze the pluralization handling influence on 
different query categories we divide queries into short queries 
and long queries among the queries stemmed by the 
naive model there are short queries with or words 
and long queries with at least words 
 results 
we summarize the overall results in table and present 
the results on short queries and long queries separately in 
table each row in table is a stemming strategy 
described in section the first column is the name of the 
strategy the second column is the number of queries 
affected by this strategy this column measures the stemming 
cost and the numbers should be low for the same level of 
dcg the third column is the average dcg score over all 
tested queries in this category including the ones that were 
not stemmed by the strategy the fourth column is the 
relative improvement over the baseline and the last column 
is the p-value of wilcoxon significance test 
there are several observations about the results we can 
see the naively stemming only obtains a statistically 
insignificant improvement of looking at table it gives an 
improvement of on short queries however it also 
hurts long queries by - overall the improvement is 
canceled out the reason that it improves short queries is 
that most short queries only have one word that can be 
stemmed thus blindly pluralizing short queries is 
relatively safe however for long queries most queries can have 
multiple words that can be pluralized expanding all of 
them without selection will significantly hurt precision 
document context sensitive stemming gives a significant 
lift to the performance from to for short queries 
and from - to - for long queries with an overall 
lift from to the improvement comes from the 
conservative context sensitive document matching an 
expanded word is valid only if it occurs within the context of 
original query in the document this reduces many spurious 
matches however we still notice that for long queries 
context sensitive stemming is not able to improve performance 
because it still selects too many documents and gives the 
ranking function a hard problem while the chosen window 
size of works the best amongst all the choices it still 
allows spurious matches it is possible that the window size 
needs to be chosen on a per query basis to ensure tighter 
proximity constraints for different types of noun phrases 
selective word pluralization further helps resolving the 
problem faced by document context sensitive stemming it 
does not stem every word that places all the burden on the 
ranking algorithm but tries to eliminate unnecessary 
stemming in the first place by predicting which word variants 
are going to be useful we can dramatically reduce the 
number of stemmed words thus improving both the recall and 
the precision with the unigram language model we can 
reduce the stemming cost by from to 
and lift the overall dcg improvement from to in 
particular it gives significant improvements on long queries 
the dcg gain is turned from negative to positive from − 
to this confirms our hypothesis that reducing 
unnecessary word expansion leads to precision improvement for 
short queries too we observe both dcg improvement and 
stemming cost reduction with the unigram language model 
the advantages of predictive word expansion with a 
language model is further boosted with a better bigram 
language model the overall dcg gain is lifted from 
to and stemming cost is dramatically reduced from 
 to corresponding to only of query 
traffic out of and an overall dcg 
improvement overall all query traffic for short queries bigram 
language model improves the dcg gain from to 
and reduces stemming cost from to for 
long queries bigram language model improves dcg gain from 
 to and reduces stemming cost from to 
 we observe that the bigram language model gives 
a larger lift for long queries this is because the uncertainty 
in long queries is larger and a more powerful language model 
is needed we hypothesize that a trigram language model 
would give a further lift for long queries and leave this for 
future investigation 
considering the tight upper-bound 
on the improvement 
to be gained from pluralization handling via the oracle 
model the current performance on short queries is very 
satisfying for short queries the dcg gain upper-bound is 
for perfect pluralization handling our current gain is 
with a bigram language model for long queries the dcg 
gain upper-bound is for perfect pluralization handling 
our current gain is with a bigram language model we 
may gain additional benefit with a more powerful language 
model for long queries however the difficulties of long 
queries come from many other aspects including the 
proximity and the segmentation problem these problems have 
to be addressed separately looking at the the upper-bound 
of overhead reduction for oracle stemming 
of the naive stemmings are wasteful we currently capture 
about half of them further reduction of the overhead 
requires sacrificing the dcg gain 
now we can compare the stemming strategies from a 
different aspect instead of looking at the influence over all 
queries as we described above table summarizes the dcg 
improvements over the affected queries only we can see 
that the number of affected queries decreases as the 
stemming strategy becomes more accurate dcg improvement 
for the bigram language model over the stemmed 
queries the dcg improvement is an interesting 
observation is the average dcg decreases with a better model 
which indicates a better stemming strategy stems more 
difficult queries low dcg queries 
 discussions 
 language models from query vs from web 
as we mentioned in section we are trying to predict 
the probability of a string occurring on the web the 
language model should describe the occurrence of the string on 
the web however the query log is also a good resource 
 
note that this upperbound is for pluralization handling 
only not for general stemming general stemming gives a 
 upperbound which is quite substantial in terms of our 
metrics 
affected queries dcg dcg improvement p-value 
baseline n a n a 
naive model 
document context sensitive model 
selective model unigram lm 
selective model bigram lm 
oracle model 
table results comparison of different stemming strategies over all queries affected by naive stemming 
short query results 
affected queries dcg improvement p-value 
baseline n a n a 
naive model 
document context sensitive model 
selective model unigram lm 
selective model bigram lm 
oracle model 
long query results 
affected queries dcg improvement p-value 
baseline n a n a 
naive model - 
document context sensitive model - 
selective model unigram lm 
selective model bigram lm 
oracle model 
table results comparison of different stemming strategies overall short queries and long queries 
users reformulate a query using many different variants to 
get good results 
to test the hypothesis that we can learn reliable 
transformation probabilities from the query log we trained a 
language model from the same query top m queries as used 
to learn segmentation and use that for prediction we 
observed a slight performance decrease compared to the model 
trained on web frequencies in particular the performance 
for unigram lm was not affected but the dcg gain for bigram 
lm changed from to for short queries thus the 
query log can serve as a good approximation of the web 
frequencies 
 how linguistics helps 
some linguistic knowledge is useful in stemming for the 
pluralization handling case pluralization and de-pluralization 
is not symmetric a plural word used in a query indicates 
a special intent for example the query new york hotels 
is looking for a list of hotels in new york not the specific 
new york hotel which might be a hotel located in 
california a simple equivalence of hotel to hotels might boost 
a particular page about new york hotel to top rank to 
capture this intent we have to make sure the document is a 
general page about hotels in new york we do this by 
requiring that the plural word hotels appears in the document 
on the other hand converting a singular word to plural is 
safer since a general purpose page normally contains 
specific information we observed a slight overall dcg decrease 
although not statistically significant for document context 
sensitive stemming if we do not consider this asymmetric 
property 
 error analysis 
one type of mistakes we noticed though rare but 
seriously hurting relevance is the search intent change after 
stemming generally speaking pluralization or 
depluralization keeps the original intent however the intent could 
change in a few cases for one example of such a query 
job at apple we pluralize job to jobs this 
stemming makes the original query ambiguous the query job 
or jobs at apple has two intents one is the employment 
opportunities at apple and another is a person working at 
apple steve jobs who is the ceo and co-founder of the 
company thus the results after query stemming returns 
steve jobs as one of the results in top one solution is 
performing results set based analysis to check if the intent is 
changed this is similar to relevance feedback and requires 
second phase ranking 
a second type of mistakes is the entity concept 
recognition problem these include two kinds one is that the 
stemmed word variant now matches part of an entity or 
concept for example query cookies in san francisco is 
pluralized to cookies or cookie in san francisco the 
results will match cookie jar in san francisco although 
cookie still means the same thing as cookies cookie 
jar is a different concept another kind is the unstemmed 
word matches an entity or concept because of the stemming 
of the other words for example quote ice is 
pluralized to quote or quotes ice the original intent for this 
query is searching for stock quote for ticker ice however 
we noticed that among the top results one of the results 
is food quotes ice cream this is matched because of 
affected queries old dcg new dcg dcg improvement 
naive model 
document context sensitive model 
selective model unigram lm 
selective model bigram lm 
table results comparison over the stemmed queries only column old new dcg is the dcg score over the 
affected queries before after applying stemming 
the pluralized word quotes the unchanged word ice 
matches part of the noun phrase ice cream here to solve 
this kind of problem we have to analyze the documents and 
recognize cookie jar and ice cream as concepts instead 
of two independent words 
a third type of mistakes occurs in long queries for the 
query bar code reader software two words are pluralized 
code to codes and reader to readers in fact bar 
code reader in the original query is a strong concept and 
the internal words should not be changed this is the 
segmentation and entity and noun phrase detection problem in 
queries which we actively are attacking for long queries 
we should correctly identify the concepts in the query and 
boost the proximity for the words within a concept 
 conclusions and future work 
we have presented a simple yet elegant way of stemming 
for web search it improves naive stemming in two aspects 
selective word expansion on the query side and 
conservative word occurrence matching on the document side using 
pluralization handling as an example experiments on a 
major web search engine data show it significantly improves 
the web relevance and reduces the stemming cost it also 
significantly improves web click through rate details not 
reported in the paper 
for the future work we are investigating the problems 
we identified in the error analysis section these include 
entity and noun phrase matching mistakes and improved 
segmentation 
 references 
 e agichtein e brill and s t dumais improving 
web search ranking by incorporating user behavior 
information in sigir 
 e airio word normalization and decompounding in 
mono- and bilingual ir information retrieval 
 - 
 p anick using terminological feedback for web 
search refinement a log-based study in sigir 
 
 r baeza-yates and b ribeiro-neto modern 
information retrieval acm press addison wesley 
 
 s chen and j goodman an empirical study of 
smoothing techniques for language modeling 
technical report tr- - harvard university 
 s cronen-townsend y zhou and b croft a 
framework for selective query expansion in cikm 
 
 h fang and c zhai semantic term matching in 
axiomatic approaches to information retrieval in 
sigir 
 w b frakes term conflation for information 
retrieval in c j rijsbergen editor research and 
development in information retrieval pages - 
cambridge university press 
 d harman how effective is suffixing jasis 
 - 
 d hull stemming algorithms - a case study for 
detailed evaluation jasis - 
 k jarvelin and j kekalainen cumulated gain-based 
evaluation evaluation of ir techniques acm tois 
 - 
 r jones b rey o madani and w greiner 
generating query substitutions in www 
 w kraaij and r pohlmann viewing stemming as 
recall enhancement in sigir 
 r krovetz viewing morphology as an inference 
process in sigir 
 d lin automatic retrieval and clustering of similar 
words in coling-acl 
 j b lovins development of a stemming algorithm 
mechanical translation and computational 
linguistics ii - 
 m lennon and d peirce and b tarry and p willett 
an evaluation of some conflation algorithms for 
information retrieval journal of information science 
 - 
 m porter an algorithm for suffix stripping 
program - 
 k m risvik t mikolajewski and p boros query 
segmentation for web search in www 
 s e robertson on term selection for query 
expansion journal of documentation - 
 
 g salton and c buckley improving retrieval 
performance by relevance feedback jasis 
- 
 r sun c -h ong and t -s chua mining 
dependency relations for query expansion in 
passage retrieval in sigir 
 c van rijsbergen information retrieval 
butterworths second version 
 b v´elez r weiss m a sheldon and d k gifford 
fast and effective query refinement in sigir 
 j xu and b croft query expansion using local and 
global document analysis in sigir 
 j xu and b croft corpus-based stemming using 
cooccurrence of word variants acm tois 
 - 
improving web search ranking by incorporating 
user behavior information 
eugene agichtein 
microsoft research 
eugeneag microsoft com 
eric brill 
microsoft research 
brill microsoft com 
susan dumais 
microsoft research 
sdumais microsoft com 
abstract 
we show that incorporating user behavior data can significantly 
improve ordering of top results in real web search setting we 
examine alternatives for incorporating feedback into the ranking 
process and explore the contributions of user feedback compared 
to other common web search features we report results of a large 
scale evaluation over queries and million user 
interactions with a popular web search engine we show that 
incorporating implicit feedback can augment other features 
improving the accuracy of a competitive web search ranking 
algorithms by as much as relative to the original 
performance 
categories and subject descriptors 
h information search and retrieval - relevance feedback 
search process h online information services - web-based 
services 
general terms 
algorithms measurement experimentation 
 introduction 
millions of users interact with search engines daily they issue 
queries follow some of the links in the results click on ads spend 
time on pages reformulate their queries and perform other 
actions these interactions can serve as a valuable source of 
information for tuning and improving web search result ranking 
and can compliment more costly explicit judgments 
implicit relevance feedback for ranking and personalization has 
become an active area of research recent work by joachims and 
others exploring implicit feedback in controlled environments 
have shown the value of incorporating implicit feedback into the 
ranking process our motivation for this work is to understand 
how implicit feedback can be used in a large-scale operational 
environment to improve retrieval how does it compare to and 
compliment evidence from page content anchor text or link-based 
features such as inlinks or pagerank while it is intuitive that 
user interactions with the web search engine should reveal at least 
some information that could be used for ranking estimating user 
preferences in real web search settings is a challenging problem 
since real user interactions tend to be more noisy than 
commonly assumed in the controlled settings of previous studies 
our paper explores whether implicit feedback can be helpful in 
realistic environments where user feedback can be noisy or 
adversarial and a web search engine already uses hundreds of 
features and is heavily tuned to this end we explore different 
approaches for ranking web search results using real user behavior 
obtained as part of normal interactions with the web search 
engine 
the specific contributions of this paper include 
 analysis of alternatives for incorporating user behavior 
into web search ranking section 
 an application of a robust implicit feedback model 
derived from mining millions of user interactions with a 
major web search engine section 
 a large scale evaluation over real user queries and search 
results showing significant improvements derived from 
incorporating user feedback section 
we summarize our findings and discuss extensions to the current 
work in section which concludes the paper 
 background and related work 
ranking search results is a fundamental problem in information 
retrieval most common approaches primarily focus on similarity 
of query and a page as well as the overall page quality 
however with increasing popularity of search engines implicit 
feedback i e the actions users take when interacting with the 
search engine can be used to improve the rankings 
implicit relevance measures have been studied by several research 
groups an overview of implicit measures is compiled in kelly and 
teevan this research while developing valuable insights 
into implicit relevance measures was not applied to improve the 
ranking of web search results in realistic settings 
closely related to our work joachims collected implicit 
measures in place of explicit measures introducing a technique 
based entirely on clickthrough data to learn ranking functions fox 
et al explored the relationship between implicit and explicit 
measures in web search and developed bayesian models to 
correlate implicit measures and explicit relevance judgments for 
both individual queries and search sessions this work considered 
a wide range of user behaviors e g dwell time scroll time 
reformulation patterns in addition to the popular clickthrough 
behavior however the modeling effort was aimed at predicting 
explicit relevance judgments from implicit user actions and not 
specifically at learning ranking functions other studies of user 
behavior in web search include pharo and järvelin but were 
not directly applied to improve ranking 
more recently joachims et al presented an empirical 
evaluation of interpreting clickthrough evidence by performing 
eye tracking studies and correlating predictions of their strategies 
with explicit ratings the authors showed that it is possible to 
accurately interpret clickthroughs in a controlled laboratory 
setting unfortunately the extent to which previous research 
applies to real-world web search is unclear at the same time 
while recent work e g on using clickthrough information 
for improving web search ranking is promising it captures only 
one aspect of the user interactions with web search engines 
we build on existing research to develop robust user behavior 
interpretation techniques for the real web search setting instead of 
treating each user as a reliable expert we aggregate information 
from multiple unreliable user search session traces as we 
describe in the next two sections 
 incorporating implicit 
feedback 
we consider two complementary approaches to ranking with 
implicit feedback treating implicit feedback as independent 
evidence for ranking results and integrating implicit feedback 
features directly into the ranking algorithm we describe the two 
general ranking approaches next the specific implicit feedback 
features are described in section and the algorithms for 
interpreting and incorporating implicit feedback are described in 
section 
 implicit feedback as independent 
evidence 
the general approach is to re-rank the results obtained by a web 
search engine according to observed clickthrough and other user 
interactions for the query in previous search sessions each result 
is assigned a score according to expected relevance user 
satisfaction based on previous interactions resulting in some 
preference ordering based on user interactions alone 
while there has been significant work on merging multiple 
rankings we adapt a simple and robust approach of ignoring the 
original rankers scores and instead simply merge the rank orders 
the main reason for ignoring the original scores is that since the 
feature spaces and learning algorithms are different the scores are 
not directly comparable and re-normalization tends to remove the 
benefit of incorporating classifier scores 
we experimented with a variety of merging functions on the 
development set of queries and using a set of interactions from a 
different time period from final evaluation sets we found that a 
simple rank merging heuristic combination works well and is 
robust to variations in score values from original rankers for a 
given query q the implicit score isd is computed for each result d 
from available user interaction features resulting in the implicit 
rank id for each result we compute a merged score sm d for d by 
combining the ranks obtained from implicit feedback id with the 
original rank of d od 
 
 
 
 
 
¢ 
£ 
 
 
 
 
 
otherwise 
o 
dforexistsfeedbackimplicitif 
oi 
w 
woids 
d 
dd 
i 
iddm 
 
 
 
 
 
 
 
where the weight wi is a heuristically tuned scaling factor 
representing the relative importance of the implicit feedback 
the query results are ordered in by decreasing values of sm to 
produce the final ranking one special case of this model arises 
when setting wi to a very large value effectively forcing clicked 
results to be ranked higher than un-clicked results - an intuitive 
and effective heuristic that we will use as a baseline applying 
more sophisticated classifier and ranker combination algorithms 
may result in additional improvements and is a promising 
direction for future work 
the approach above assumes that there are no interactions 
between the underlying features producing the original web search 
ranking and the implicit feedback features we now relax this 
assumption by integrating implicit feedback features directly into 
the ranking process 
 ranking with implicit feedback features 
modern web search engines rank results based on a large number 
of features including content-based features i e how closely a 
query matches the text or title or anchor text of the document and 
query-independent page quality features e g pagerank of the 
document or the domain in most cases automatic or 
semiautomatic methods are developed for tuning the specific ranking 
function that combines these feature values 
hence a natural approach is to incorporate implicit feedback 
features directly as features for the ranking algorithm during 
training or tuning the ranker can be tuned as before but with 
additional features at runtime the search engine would fetch the 
implicit feedback features associated with each query-result url 
pair this model requires a ranking algorithm to be robust to 
missing values more than of queries to web search engines 
are unique with no previous implicit feedback available we now 
describe such a ranker that we used to learn over the combined 
feature sets including implicit feedback 
 learning to rank web search results 
a key aspect of our approach is exploiting recent advances in 
machine learning namely trainable ranking algorithms for web 
search and information retrieval e g and classical results 
reviewed in in our setting explicit human relevance 
judgments labels are available for a set of web search queries 
and results hence an attractive choice to use is a supervised 
machine learning technique to learn a ranking function that best 
predicts relevance judgments 
ranknet is one such algorithm it is a neural net tuning algorithm 
that optimizes feature weights to best match explicitly provided 
pairwise user preferences while the specific training algorithms 
used by ranknet are beyond the scope of this paper it is 
described in detail in and includes extensive evaluation and 
comparison with other ranking methods an attractive feature of 
ranknet is both train- and run-time efficiency - runtime ranking 
can be quickly computed and can scale to the web and training 
can be done over thousands of queries and associated judged 
results 
we use a -layer implementation of ranknet in order to model 
non-linear relationships between features furthermore ranknet 
can learn with many differentiable cost functions and hence can 
automatically learn a ranking function from human-provided 
labels an attractive alternative to heuristic feature combination 
techniques hence we will also use ranknet as a generic ranker 
to explore the contribution of implicit feedback for different 
ranking alternatives 
 implicit user feedback model 
our goal is to accurately interpret noisy user feedback obtained as 
by tracing user interactions with the search engine interpreting 
implicit feedback in real web search setting is not an easy task 
we characterize this problem in detail in where we motivate 
and evaluate a wide variety of models of implicit user activities 
the general approach is to represent user actions for each search 
result as a vector of features and then train a ranker on these 
features to discover feature values indicative of relevant and 
nonrelevant search results we first briefly summarize our features 
and model and the learning approach section in order to 
provide sufficient information to replicate our ranking methods 
and the subsequent experiments 
 representing user actions as features 
we model observed web search behaviors as a combination of a 
``background component i e query- and relevance-independent 
noise in user behavior including positional biases with result 
interactions and a ``relevance component i e query-specific 
behavior indicative of relevance of a result to a query we design 
our features to take advantage of aggregated user behavior the 
feature set is comprised of directly observed features computed 
directly from observations for each query as well as 
queryspecific derived features computed as the deviation from the 
overall query-independent distribution of values for the 
corresponding directly observed feature values 
the features used to represent user interactions with web search 
results are summarized in table this information was 
obtained via opt-in client-side instrumentation from users of a 
major web search engine 
we include the traditional implicit feedback features such as 
clickthrough counts for the results as well as our novel derived 
features such as the deviation of the observed clickthrough number 
for a given query-url pair from the expected number of clicks on 
a result in the given position we also model the browsing 
behavior after a result was clicked - e g the average page dwell 
time for a given query-url pair as well as its deviation from the 
expected average dwell time furthermore the feature set was 
designed to provide essential information about the user 
experience to make feedback interpretation robust for example 
web search users can often determine whether a result is relevant 
by looking at the result title url and summary - in many cases 
looking at the original document is not necessary to model this 
aspect of user experience we include features such as overlap in 
words in title and words in query titleoverlap and the fraction 
of words shared by the query and the result summary 
clickthrough features 
position position of the url in current ranking 
clickfrequency number of clicks for this query url pair 
clickprobability probability of a click for this query and url 
clickdeviation deviation from expected click probability 
isnextclicked if clicked on next position otherwise 
ispreviousclicked if clicked on previous position otherwise 
isclickabove if there is a click above otherwise 
isclickbelow if there is click below otherwise 
browsing features 
timeonpage page dwell time 
cumulativetimeonpage 
cumulative time for all subsequent pages after 
search 
timeondomain cumulative dwell time for this domain 
timeonshorturl cumulative time on url prefix no parameters 
isfollowedlink if followed link to result otherwise 
isexacturlmatch if aggressive normalization used otherwise 
isredirected if initial url same as final url otherwise 
ispathfromsearch if only followed links after query otherwise 
clicksfromsearch number of hops to reach page from query 
averagedwelltime average time on page for this query 
dwelltimedeviation deviation from average dwell time on page 
cumulativedeviation deviation from average cumulative dwell time 
domaindeviation deviation from average dwell time on domain 
query-text features 
titleoverlap words shared between query and title 
summaryoverlap words shared between query and snippet 
queryurloverlap words shared between query and url 
querydomainoverlap words shared between query and url domain 
querylength number of tokens in query 
querynextoverlap fraction of words shared with next query 
table some features used to represent post-search 
navigation history for a given query and search result url 
having described our feature set we briefly review our general 
method for deriving a user behavior model 
 deriving a user feedback model 
to learn to interpret the observed user behavior we correlate user 
actions i e the features in table representing the actions 
with the explicit user judgments for a set of training queries we 
find all the instances in our session logs where these queries were 
submitted to the search engine and aggregate the user behavior 
features for all search sessions involving these queries 
each observed query-url pair is represented by the features in 
table with values averaged over all search sessions and 
assigned one of six possible relevance labels ranging from 
perfect to bad as assigned by explicit relevance judgments 
these labeled feature vectors are used as input to the ranknet 
training algorithm section which produces a trained user 
behavior model this approach is particularly attractive as it does 
not require heuristics beyond feature engineering the resulting 
user behavior model is used to help rank web search 
resultseither directly or in combination with other features as described 
below 
 experimental setup 
the ultimate goal of incorporating implicit feedback into ranking 
is to improve the relevance of the returned web search results 
hence we compare the ranking methods over a large set of judged 
queries with explicit relevance labels provided by human judges 
in order for the evaluation to be realistic we obtained a random 
sample of queries from web search logs of a major search engine 
with associated results and traces for user actions we describe 
this dataset in detail next our metrics are described in section 
that we use to evaluate the ranking alternatives listed in section 
 in the experiments of section 
 datasets 
we compared our ranking methods over a random sample of 
queries from the search engine query logs the queries were 
drawn from the logs uniformly at random by token without 
replacement resulting in a query sample representative of the 
overall query distribution on average results were explicitly 
labeled by human judges using a six point scale ranging from 
perfect down to bad overall there were over results 
with explicit relevance judgments in order to compute various 
statistics documents with label good or better will be 
considered relevant and with lower labels to be non-relevant 
note that the experiments were performed over the results already 
highly ranked by a web search engine which corresponds to a 
typical user experience which is limited to the small number of the 
highly ranked results for a typical web search query 
the user interactions were collected over a period of weeks 
using voluntary opt-in information in total over million 
unique queries were instrumented resulting in over million 
individual interactions with the search engine the data consisted 
of user interactions with the web search engine e g clicking on a 
result link going back to search results etc performed after a 
query was submitted these actions were aggregated across users 
and search sessions and converted to features in table 
to create the training validation and test query sets we created 
three different random splits of training validation and 
 test queries the splits were done randomly by query so that 
there was no overlap in training validation and test queries 
 evaluation metrics 
we evaluate the ranking algorithms over a range of accepted 
information retrieval metrics namely precision at k p k 
normalized discounted cumulative gain ndcg and mean 
average precision map each metric focuses on a deferent 
aspect of system performance as we describe below 
 precision at k as the most intuitive metric p k reports the 
fraction of documents ranked in the top k results that are 
labeled as relevant in our setting we require a relevant 
document to be labeled good or higher the position of 
relevant documents within the top k is irrelevant and hence 
this metric measure overall user satisfaction with the top k 
results 
 ndcg at k ndcg is a retrieval measure devised specifically 
for web search evaluation for a given query q the ranked 
results are examined from the top ranked down and the ndcg 
computed as 
 
 
 − 
k 
j 
jr 
qq jmn 
 
 
 log 
where mq is a normalization constant calculated so that a 
perfect ordering would obtain ndcg of and each r j is an 
integer relevance label bad and perfect of result 
returned at position j note that unlabeled and bad documents 
do not contribute to the sum but will reduce ndcg for the 
query pushing down the relevant labeled documents reducing 
their contributions ndcg is well suited to web search 
evaluation as it rewards relevant documents in the top ranked 
results more heavily than those ranked lower 
 map average precision for each query is defined as the mean 
of the precision at k values computed after each relevant 
document was retrieved the final map value is defined as the 
mean of average precisions of all queries in the test set this 
metric is the most commonly used single-value summary of a 
run over a set of queries 
 ranking methods compared 
recall that our goal is to quantify the effectiveness of implicit 
behavior for real web search one dimension is to compare the 
utility of implicit feedback with other information available to a 
web search engine specifically we compare effectiveness of 
implicit user behaviors with content-based matching static page 
quality features and combinations of all features 
 bm f as a strong web search baseline we used the bm f 
scoring which was used in one of the best performing systems 
in the trec web track bm f and its variants 
have been extensively described and evaluated in ir literature 
and hence serve as a strong reproducible baseline the bm f 
variant we used for our experiments computes separate match 
scores for each field for a result document e g body text 
title and anchor text and incorporates query-independent 
linkbased information e g pagerank clickdistance and url 
depth the scoring function and field-specific tuning is 
described in detail in note that bm f does not directly 
consider explicit or implicit feedback for tuning 
 rn the ranking produced by a neural net ranker ranknet 
described in section that learns to rank web search results 
by incorporating bm f and a large number of additional static 
and dynamic features describing each search result this system 
automatically learns weights for all features including the 
bm f score for a document based on explicit human labels 
for a large set of queries a system incorporating an 
implementation of ranknet is currently in use by a major 
search engine and can be considered representative of the state 
of the art in web search 
 bm f-rerankct the ranking produced by incorporating 
clickthrough statistics to reorder web search results ranked by 
bm f above clickthrough is a particularly important special 
case of implicit feedback and has been shown to correlate with 
result relevance this is a special case of the ranking method in 
section with the weight wi set to and the ranking id 
is simply the number of clicks on the result corresponding to d 
in effect this ranking brings to the top all returned web search 
results with at least one click and orders them in decreasing 
order by number of clicks the relative ranking of the 
remainder of results is unchanged and they are inserted below 
all clicked results this method serves as our baseline implicit 
feedback reranking method 
bm f-rerankall the ranking produced by reordering the 
bm f results using all user behavior features section 
this method learns a model of user preferences by correlating 
feature values with explicit relevance labels using the ranknet 
neural net algorithm section at runtime for a given 
query the implicit score ir is computed for each result r with 
available user interaction features and the implicit ranking is 
produced the merged ranking is computed as described in 
section based on the experiments over the development set 
we fix the value of wi to the effect of the wi parameter for 
this ranker turned out to be negligible 
 bm f all ranking derived by training the ranknet 
 section learner over the features set of the bm f score 
as well as all implicit feedback features section we used 
the -layer implementation of ranknet trained on the 
queries and labels in the training and validation sets 
 rn all ranking derived by training the -layer ranknet 
ranking algorithm section over the union of all content 
dynamic and implicit feedback features i e all of the features 
described above as well as all of the new implicit feedback 
features we introduced 
the ranking methods above span the range of the information used 
for ranking from not using the implicit or explicit feedback at all 
 i e bm f to a modern web search engine using hundreds of 
features and tuned on explicit judgments rn as we will show 
next incorporating user behavior into these ranking systems 
dramatically improves the relevance of the returned documents 
 experimental results 
implicit feedback for web search ranking can be exploited in a 
number of ways we compare alternative methods of exploiting 
implicit feedback both by re-ranking the top results i e the 
bm f-rerankct and bm f-rerankall methods that reorder 
bm f results as well as by integrating the implicit features 
directly into the ranking process i e the rn all and 
bm f all methods which learn to rank results over the implicit 
feedback and other features we compare our methods over strong 
baselines bm f and rn over the ndcg precision at k and 
map measures defined in section the results were averaged 
over three random splits of the overall dataset each split 
contained training validation and test queries all 
query sets disjoint we first present the results over all test 
queries i e including queries for which there are no implicit 
measures so we use the original web rankings we then drill 
down to examine the effects on reranking for the attempted 
queries in more detail analyzing where implicit feedback proved 
most beneficial 
we first experimented with different methods of re-ranking the 
output of the bm f search results figures and report 
ndcg and precision for bm f as well as for the strategies 
reranking results with user feedback section incorporating 
all user feedback either in reranking framework or as features to 
the learner directly results in significant improvements using 
two-tailed t-test with p over both the original bm f 
ranking as well as over reranking with clickthrough alone the 
improvement is consistent across the top results and largest for 
the top result ndcg at for bm f all is compared to 
 of the original results and precision at similarly increases 
from to based on these results we will use the direct 
feature combination i e bm f all ranker for subsequent 
comparisons involving implicit feedback 
 
 
 
 
 
 
 
 
 
 
 k 
ndcg 
bm 
bm -rerank-ct 
bm -rerank-all 
bm all 
figure ndcg at k for bm f bm f-rerankct 
bm f-rerank-all and bm f all for varying k 
 
 
 
 
 
 
 
 
k 
precision 
bm 
bm -rerank-ct 
bm -rerank-all 
bm all 
figure precision at k for bm f bm f-rerankct 
bm f-rerank-all and bm f all for varying k 
interestingly using clickthrough alone while giving significant 
benefit over the original bm f ranking is not as effective as 
considering the full set of features in table while we analyze 
user behavior and most effective component features in a 
separate paper it is worthwhile to give a concrete example of 
the kind of noise inherent in real user feedback in web search 
setting 
 
 
 
 
 
 
 
 
 
 
 
 
result position 
relativeclickfrequency 
ptr 
ptr 
ptr 
figure relative clickthrough frequency for queries with 
varying position of top relevant result ptr 
if users considered only the relevance of a result to their query 
they would click on the topmost relevant results unfortunately as 
joachims and others have shown presentation also influences 
which results users click on quite dramatically users often click 
on results above the relevant one presumably because the short 
summaries do not provide enough information to make accurate 
relevance assessments and they have learned that on average 
topranked items are relevant figure shows relative clickthrough 
frequencies for queries with known relevant items at positions 
other than the first position the position of the top relevant result 
 ptr ranges from - in the figure for example for queries 
with first relevant result at position ptr there are more 
clicks on the non-relevant results in higher ranked positions than 
on the first relevant result at position as we will see learning 
over a richer behavior feature set results in substantial accuracy 
improvement over clickthrough alone 
we now consider incorporating user behavior into a much richer 
feature set rn section used by a major web search engine 
rn incorporates bm f link-based features and hundreds of 
other features figure reports ndcg at k and figure 
reports precision at k interestingly while the original rn 
rankings are significantly more accurate than bm f alone 
incorporating implicit feedback features bm f all results in 
ranking that significantly outperforms the original rn rankings in 
other words implicit feedback incorporates sufficient information 
to replace the hundreds of other features available to the ranknet 
learner trained on the rn feature set 
 
 
 
 
 
 
 
 
 
 
 
 k 
ndcg 
rn 
rn all 
bm 
bm all 
figure ndcg at k for bm f bm f all rn and 
rn all for varying k 
furthermore enriching the rn features with implicit feedback set 
exhibits significant gain on all measures allowing rn all to 
outperform all other methods this demonstrates the 
complementary nature of implicit feedback with other features 
available to a state of the art web search engine 
 
 
 
 
 
 
 
k 
precision 
rn 
rn all 
bm 
bm all 
figure precision at k for bm f bm f all rn and 
rn all for varying k 
we summarize the performance of the different ranking methods 
in table we report the mean average precision map score 
for each system while not intuitive to interpret map allows 
quantitative comparison on a single metric the gains marked with 
 are significant at p level using two tailed t-test 
map gain p gain 
bm f - 
 bm f-rerank-ct 
bm f-rerankimplicit 
bm f implicit 
rn - 
 rn all 
table mean average precision map for all strategies 
so far we reported results averaged across all queries in the test 
set unfortunately less than half had sufficient interactions to 
attempt reranking out of the queries in test between 
and depending on the train-test split had sufficient 
interaction information to make predictions i e there was at least 
 search session in which at least result url was clicked on by 
the user this is not surprising web search is heavy-tailed and 
there are many unique queries we now consider the performance 
on the queries for which user interactions were available figure 
 reports ndcg for the subset of the test queries with the 
implicit feedback features the gains at top are dramatic the 
ndcg at of bm f all increases from to a 
relative gain achieving performance comparable to rn all 
operating over a much richer feature set 
 
 
 
 
 
 k 
ndcg 
rn rn all 
bm bm all 
figure ndcg at k for bm f bm f all rn and 
rn all on test queries with user interactions 
similarly gains on precision at top are substantial figure 
and are likely to be apparent to web search users when implicit 
feedback is available the bm f all system returns relevant 
document at top almost of the time compared of the 
time when implicit feedback is not considered by the original 
bm f system 
 
 
 
 
 
 
 k 
precision 
rn 
rn all 
bm 
bm all 
figure precision at k ndcg at k for bm f 
bm f all rn and rn all on test queries with user 
interactions 
we summarize the results on the map measure for attempted 
queries in table map improvements are both substantial and 
significant with improvements over the bm f ranker most 
pronounced 
method map gain p gain 
rn 
rn all 
bm f 
bm f all 
table mean average precision map on attempted 
queries for best performing methods 
we now analyze the cases where implicit feedback was shown 
most helpful figure reports the map improvements over the 
baseline bm f run for each query with map under note 
that most of the improvement is for poorly performing queries 
 i e map interestingly incorporating user behavior 
information degrades accuracy for queries with high original map 
score one possible explanation is that these easy queries tend 
to be navigational i e having a single highly-ranked most 
appropriate answer and user interactions with lower-ranked 
results may indicate divergent information needs that are better 
served by the less popular results with correspondingly poor 
overall relevance ratings 
 
 
 
 
 
 
 
 
 
- 
- 
- 
- 
- 
- 
- 
- 
 
 
 
 
 
frequency average gain 
figure gain of bm f all over original bm f ranking 
to summarize our experimental results incorporating implicit 
feedback in real web search setting resulted in significant 
improvements over the original rankings using both bm f and 
rn baselines our rich set of implicit features such as time on 
page and deviations from the average behavior provides 
advantages over using clickthrough alone as an indicator of 
interest furthermore incorporating implicit feedback features 
directly into the learned ranking function is more effective than 
using implicit feedback for reranking the improvements observed 
over large test sets of queries total between and 
with implicit feedback available are both substantial and 
statistically significant 
 conclusions and future work 
in this paper we explored the utility of incorporating noisy implicit 
feedback obtained in a real web search setting to improve web 
search ranking we performed a large-scale evaluation over 
queries and more than million user interactions with a major 
search engine establishing the utility of incorporating noisy 
implicit feedback to improve web search relevance 
we compared two alternatives of incorporating implicit feedback 
into the search process namely reranking with implicit feedback 
and incorporating implicit feedback features directly into the 
trained ranking function our experiments showed significant 
improvement over methods that do not consider implicit feedback 
the gains are particularly dramatic for the top k result in the 
final ranking with precision improvements as high as and 
the gains are substantial for all values of k our experiments 
showed that implicit user feedback can further improve web 
search performance when incorporated directly with popular 
content- and link-based features 
interestingly implicit feedback is particularly valuable for queries 
with poor original ranking of results e g map lower than 
one promising direction for future work is to apply recent research 
on automatically predicting query difficulty and only attempt to 
incorporate implicit feedback for the difficult queries as 
another research direction we are exploring methods for extending 
our predictions to the previously unseen queries e g query 
clustering which should further improve the web search 
experience of users 
acknowledgments 
we thank chris burges and matt richardson for an 
implementation of ranknet for our experiments we also thank 
robert ragno for his valuable suggestions and many discussions 
 references 
 e agichtein e brill s dumais and r ragno learning 
user interaction models for predicting web search result 
preferences in proceedings of the acm conference on 
research and development on information retrieval sigir 
 
 j allan hard track overview in trec high 
accuracy retrieval from documents 
 r baeza-yates and b ribeiro-neto modern information 
retrieval addison-wesley 
 s brin and l page the anatomy of a large-scale 
hypertextual web search engine in proceedings of www 
 
 c j c burges t shaked e renshaw a lazier m deeds 
n hamilton g hullender learning to rank using gradient 
descent in proceedings of the international conference on 
machine learning 
 d m chickering the winmine toolkit microsoft technical 
report msr-tr- - 
 m claypool d brown p lee and m waseda inferring 
user interest ieee internet computing 
 s fox k karnawat m mydland s t dumais and t 
white evaluating implicit measures to improve the search 
experience in acm transactions on information systems 
 
 j goecks and j shavlick learning users interests by 
unobtrusively observing their normal behavior in 
proceedings of the ijcai workshop on machine learning for 
information filtering 
 k jarvelin and j kekalainen ir evaluation methods for 
retrieving highly relevant documents in proceedings of the 
acm conference on research and development on 
information retrieval sigir 
 t joachims optimizing search engines using clickthrough 
data in proceedings of the acm conference on knowledge 
discovery and datamining sigkdd 
 t joachims l granka b pang h hembrooke and g gay 
accurately interpreting clickthrough data as implicit 
feedback proceedings of the acm conference on research 
and development on information retrieval sigir 
 t joachims making large-scale svm learning practical 
advances in kernel methods in support vector learning 
mit press 
 d kelly and j teevan implicit feedback for inferring user 
preference a bibliography in sigir forum 
 j konstan b miller d maltz j herlocker l gordon and 
j riedl grouplens applying collaborative filtering to 
usenet news in communications of acm 
 m morita and y shinoda information filtering based on 
user behavior analysis and best match text retrieval 
proceedings of the acm conference on research and 
development on information retrieval sigir 
 d oard and j kim implicit feedback for recommender 
systems in proceedings of the aaai workshop on 
recommender systems 
 d oard and j kim modeling information content using 
observable behavior in proceedings of the th annual 
meeting of the american society for information science and 
technology 
 n pharo n and k järvelin the sst method a tool for 
analyzing web information search processes in information 
processing management 
 p pirolli the use of proximal information scent to forage 
for distal content on the world wide web in working with 
technology in mind brunswikian resources for cognitive 
science and engineering oxford university press 
 f radlinski and t joachims query chains learning to 
rank from implicit feedback in proceedings of the acm 
conference on knowledge discovery and data mining 
 sigkdd 
 f radlinski and t joachims evaluating the robustness of 
learning from implicit feedback in proceedings of the icml 
workshop on learning in web search 
 s e robertson h zaragoza and m taylor simple bm 
extension to multiple weighted fields in proceedings of the 
conference on information and knowledge management 
 cikm 
 g salton m mcgill introduction to modern information 
retrieval mcgraw-hill 
 e m voorhees d harman overview of trec 
 g r xue h j zeng z chen y yu w y ma w s xi 
and w g fan optimizing web search using web 
clickthrough data in proceedings of the conference on 
information and knowledge management cikm 
 h zaragoza n craswell m taylor s saria and s 
robertson microsoft cambridge at trec web and hard 
tracks in proceedings of trec 
adarank a boosting algorithm for information retrieval 
jun xu 
microsoft research asia 
no zhichun road haidian distinct 
beijing china 
junxu microsoft com 
hang li 
microsoft research asia 
no zhichun road haidian distinct 
beijing china 
hangli microsoft com 
abstract 
in this paper we address the issue of learning to rank for document 
retrieval in the task a model is automatically created with some 
training data and then is utilized for ranking of documents the 
goodness of a model is usually evaluated with performance 
measures such as map mean average precision and ndcg 
 normalized discounted cumulative gain ideally a learning 
algorithm would train a ranking model that could directly optimize the 
performance measures with respect to the training data existing 
methods however are only able to train ranking models by 
minimizing loss functions loosely related to the performance measures 
for example ranking svm and rankboost train ranking 
models by minimizing classification errors on instance pairs to deal 
with the problem we propose a novel learning algorithm within 
the framework of boosting which can minimize a loss function 
directly defined on the performance measures our algorithm 
referred to as adarank repeatedly constructs  weak rankers on the 
basis of re-weighted training data and finally linearly combines the 
weak rankers for making ranking predictions we prove that the 
training process of adarank is exactly that of enhancing the 
performance measure used experimental results on four benchmark 
datasets show that adarank significantly outperforms the baseline 
methods of bm ranking svm and rankboost 
categories and subject descriptors 
h information search and retrieval retrieval models 
general terms 
algorithms experimentation theory 
 introduction 
recently  learning to rank has gained increasing attention in 
both the fields of information retrieval and machine learning when 
applied to document retrieval learning to rank becomes a task as 
follows in training a ranking model is constructed with data 
consisting of queries their corresponding retrieved documents and 
relevance levels given by humans in ranking given a new query the 
corresponding retrieved documents are sorted by using the trained 
ranking model in document retrieval usually ranking results are 
evaluated in terms of performance measures such as map mean 
average precision and ndcg normalized discounted 
cumulative gain ideally the ranking function is created so that the 
accuracy of ranking in terms of one of the measures with respect to 
the training data is maximized 
several methods for learning to rank have been developed and 
applied to document retrieval for example herbrich et al 
propose a learning algorithm for ranking on the basis of support 
vector machines called ranking svm freund et al take a 
similar approach and perform the learning by using boosting 
referred to as rankboost all the existing methods used for 
document retrieval are designed to optimize loss 
functions loosely related to the ir performance measures not loss 
functions directly based on the measures for example ranking 
svm and rankboost train ranking models by minimizing 
classification errors on instance pairs 
in this paper we aim to develop a new learning algorithm that 
can directly optimize any performance measure used in document 
retrieval inspired by the work of adaboost for classification 
we propose to develop a boosting algorithm for information 
retrieval referred to as adarank adarank utilizes a linear 
combination of  weak rankers as its model in learning it repeats the 
process of re-weighting the training sample creating a weak ranker 
and calculating a weight for the ranker 
we show that adarank algorithm can iteratively optimize an 
exponential loss function based on any of ir performance measures 
a lower bound of the performance on training data is given which 
indicates that the ranking accuracy in terms of the performance 
measure can be continuously improved during the training process 
adarank offers several advantages ease in implementation 
theoretical soundness efficiency in training and high accuracy in ranking 
experimental results indicate that adarank can outperform the 
baseline methods of bm ranking svm and rankboost on four 
benchmark datasets including ohsumed wsj ap and gov 
tuning ranking models using certain training data and a 
performance measure is a common practice in ir as the number of 
features in the ranking model gets larger and the amount of 
training data gets larger the tuning becomes harder from the viewpoint 
of ir adarank can be viewed as a machine learning method for 
ranking model tuning 
recently direct optimization of performance measures in 
learning has become a hot research topic several methods for 
classification and ranking have been proposed adarank can 
be viewed as a machine learning method for direct optimization of 
performance measures based on a different approach 
the rest of the paper is organized as follows after a summary 
of related work in section we describe the proposed adarank 
algorithm in details in section experimental results and 
discussions are given in section section concludes this paper and 
gives future work 
 related work 
 information retrieval 
the key problem for document retrieval is ranking specifically 
how to create the ranking model function that can sort documents 
based on their relevance to the given query it is a common practice 
in ir to tune the parameters of a ranking model using some labeled 
data and one performance measure for example the 
state-ofthe-art methods of bm and lmir language models for 
information retrieval all have parameters to tune as 
the ranking models become more sophisticated more features are 
used and more labeled data become available how to tune or train 
ranking models turns out to be a challenging issue 
recently methods of  learning to rank have been applied to 
ranking model construction and some promising results have been 
obtained for example joachims applies ranking svm to 
document retrieval he utilizes click-through data to deduce 
training data for the model creation cao et al adapt ranking 
svm to document retrieval by modifying the hinge loss function 
to better meet the requirements of ir specifically they introduce 
a hinge loss function that heavily penalizes errors on the tops of 
ranking lists and errors from queries with fewer retrieved 
documents burges et al employ relative entropy as a loss function 
and gradient descent as an algorithm to train a neural network 
model for ranking in document retrieval the method is referred to 
as  ranknet 
 machine learning 
there are three topics in machine learning which are related to 
our current work they are  learning to rank boosting and direct 
optimization of performance measures 
learning to rank is to automatically create a ranking function 
that assigns scores to instances and then rank the instances by 
using the scores several approaches have been proposed to tackle 
the problem one major approach to learning to rank is that of 
transforming it into binary classification on instance pairs this 
 pair-wise approach fits well with information retrieval and thus is 
widely used in ir typical methods of the approach include 
ranking svm rankboost and ranknet for other 
approaches to learning to rank refer to 
in the pair-wise approach to ranking the learning task is 
formalized as a problem of classifying instance pairs into two categories 
 correctly ranked and incorrectly ranked actually it is known 
that reducing classification errors on instance pairs is equivalent to 
maximizing a lower bound of map in that sense the 
existing methods of ranking svm rankboost and ranknet are only 
able to minimize loss functions that are loosely related to the ir 
performance measures 
boosting is a general technique for improving the accuracies of 
machine learning algorithms the basic idea of boosting is to 
repeatedly construct  weak learners by re-weighting training data 
and form an ensemble of weak learners such that the total 
performance of the ensemble is  boosted freund and schapire have 
proposed the first well-known boosting algorithm called adaboost 
 adaptive boosting which is designed for binary 
classification - prediction later schapire singer have introduced a 
generalized version of adaboost in which weak learners can give 
confidence scores in their predictions rather than make - 
decisions extensions have been made to deal with the problems 
of multi-class classification regression and ranking 
 in fact adaboost is an algorithm that ingeniously constructs 
a linear model by minimizing the  exponential loss function with 
respect to the training data our work in this paper can be 
viewed as a boosting method developed for ranking particularly 
for ranking in ir 
recently a number of authors have proposed conducting direct 
optimization of multivariate performance measures in learning for 
instance joachims presents an svm method to directly 
optimize nonlinear multivariate performance measures like the f 
measure for classification cossock zhang find a way to 
approximately optimize the ranking performance measure dcg 
metzler et al also propose a method of directly maximizing 
rank-based metrics for ranking on the basis of manifold learning 
adarank is also one that tries to directly optimize multivariate 
performance measures but is based on a different approach adarank 
is unique in that it employs an exponential loss function based on 
ir performance measures and a boosting technique 
 our method adarank 
 general framework 
we first describe the general framework of learning to rank for 
document retrieval in retrieval testing given a query the system 
returns a ranking list of documents in descending order of the 
relevance scores the relevance scores are calculated with a ranking 
function model in learning training a number of queries and 
their corresponding retrieved documents are given furthermore 
the relevance levels of the documents with respect to the queries are 
also provided the relevance levels are represented as ranks i e 
categories in a total order the objective of learning is to construct 
a ranking function which achieves the best results in ranking of the 
training data in the sense of minimization of a loss function ideally 
the loss function is defined on the basis of the performance measure 
used in testing 
suppose that y {r r · · · r } is a set of ranks where denotes 
the number of ranks there exists a total order between the ranks 
r r − · · · r where   denotes a preference relationship 
in training a set of queries q {q q · · · qm} is given each 
query qi is associated with a list of retrieved documents di {di di 
· · · di n qi } and a list of labels yi {yi yi · · · yi n qi } where n qi 
denotes the sizes of lists di and yi dij denotes the jth 
document in 
di and yij ∈ y denotes the rank of document di j a feature 
vector xij ψ qi di j ∈ x is created from each query-document pair 
 qi di j i · · · m j · · · n qi thus the training set 
can be represented as s { qi di yi }m 
i 
the objective of learning is to create a ranking function f x → 
 such that for each query the elements in its corresponding 
document list can be assigned relevance scores using the function and 
then be ranked according to the scores specifically we create a 
permutation of integers π qi di f for query qi the 
corresponding list of documents di and the ranking function f let di 
{di di · · · di n qi } be identified by the list of integers { · · · n qi } 
then permutation π qi di f is defined as a bijection from { · · · 
n qi } to itself we use π j to denote the position of item j i e 
di j the learning process turns out to be that of minimizing the 
loss function which represents the disagreement between the 
permutation π qi di f and the list of ranks yi for all of the queries 
table notations and explanations 
notations explanations 
qi ∈ q ith 
query 
di {di di · · · di n qi } list of documents for qi 
yi j ∈ {r r · · · r } rank of di j w r t qi 
yi {yi yi · · · yi n qi } list of ranks for qi 
s { qi di yi }m 
i training set 
xij ψ qi dij ∈ x feature vector for qi di j 
f xij ∈ ranking model 
π qi di f permutation for qi di and f 
ht xi j ∈ tth 
weak ranker 
e π qi di f yi ∈ − performance measure function 
in the paper we define the rank model as a linear combination of 
weak rankers f x t 
t αtht x where ht x is a weak ranker αt 
is its weight and t is the number of weak rankers 
in information retrieval query-based performance measures are 
used to evaluate the  goodness of a ranking function by query 
based measure we mean a measure defined over a ranking list 
of documents with respect to a query these measures include 
map ndcg mrr mean reciprocal rank wta winners take 
all and precision n we utilize a general function 
e π qi di f yi ∈ − to represent the performance 
measures the first argument of e is the permutation π created using 
the ranking function f on di the second argument is the list of 
ranks yi given by humans e measures the agreement between π 
and yi table gives a summary of notations described above 
next as examples of performance measures we present the 
definitions of map and ndcg given a query qi the corresponding 
list of ranks yi and a permutation πi on di average precision for qi 
is defined as 
avgpi 
n qi 
j pi j · yij 
n qi 
j yij 
 
where yij takes on and as values representing being relevant or 
irrelevant and pi j is defined as precision at the position of dij 
pi j 
k πi k ≤πi j yik 
πi j 
 
where πi j denotes the position of di j 
given a query qi the list of ranks yi and a permutation πi on di 
ndcg at position m for qi is defined as 
ni ni · 
j πi j ≤m 
 yi j − 
log πi j 
 
where yij takes on ranks as values and ni is a normalization 
constant ni is chosen so that a perfect ranking π∗ 
i s ndcg score at 
position m is 
 algorithm 
inspired by the adaboost algorithm for classification we have 
devised a novel algorithm which can optimize a loss function based 
on the ir performance measures the algorithm is referred to as 
 adarank and is shown in figure 
adarank takes a training set s { qi di yi }m 
i as input and 
takes the performance measure function e and the number of 
iterations t as parameters adarank runs t rounds and at each round it 
creates a weak ranker ht t · · · t finally it outputs a ranking 
model f by linearly combining the weak rankers 
at each round adarank maintains a distribution of weights over 
the queries in the training data we denote the distribution of weights 
input s { qi di yi }m 
i and parameters e and t 
initialize p i m 
for t · · · t 
 create weak ranker ht with weighted distribution pt on 
training data s 
 choose αt 
αt 
 
 
· ln 
m 
i pt i { e π qi di ht yi } 
m 
i pt i { − e π qi di ht yi } 
 
 create ft 
ft x 
t 
k 
αkhk x 
 update pt 
pt i 
exp{−e π qi di ft yi } 
m 
j exp{−e π qj dj ft yj } 
 
end for 
output ranking model f x ft x 
figure the adarank algorithm 
at round t as pt and the weight on the ith 
training query qi at round 
t as pt i initially adarank sets equal weights to the queries at 
each round it increases the weights of those queries that are not 
ranked well by ft the model created so far as a result the learning 
at the next round will be focused on the creation of a weak ranker 
that can work on the ranking of those  hard queries 
at each round a weak ranker ht is constructed based on training 
data with weight distribution pt the goodness of a weak ranker is 
measured by the performance measure e weighted by pt 
m 
i 
pt i e π qi di ht yi 
several methods for weak ranker construction can be considered 
for example a weak ranker can be created by using a subset of 
queries together with their document list and label list sampled 
according to the distribution pt in this paper we use single features 
as weak rankers as will be explained in section 
once a weak ranker ht is built adarank chooses a weight αt 
for the weak ranker intuitively αt measures the importance of ht 
a ranking model ft is created at each round by linearly 
combining the weak rankers constructed so far h · · · ht with weights 
α · · · αt ft is then used for updating the distribution pt 
 theoretical analysis 
the existing learning algorithms for ranking attempt to minimize 
a loss function based on instance pairs document pairs in 
contrast adarank tries to optimize a loss function based on queries 
furthermore the loss function in adarank is defined on the basis 
of general ir performance measures the measures can be map 
ndcg wta mrr or any other measures whose range is within 
 − we next explain why this is the case 
ideally we want to maximize the ranking accuracy in terms of a 
performance measure on the training data 
max 
f∈f 
m 
i 
e π qi di f yi 
where f is the set of possible ranking functions this is equivalent 
to minimizing the loss on the training data 
min 
f∈f 
m 
i 
 − e π qi di f yi 
it is difficult to directly optimize the loss because e is a 
noncontinuous function and thus may be difficult to handle we instead 
attempt to minimize an upper bound of the loss in 
min 
f∈f 
m 
i 
exp{−e π qi di f yi } 
because e−x 
≥ − x holds for any x ∈ we consider the use of a 
linear combination of weak rankers as our ranking model 
f x 
t 
t 
αtht x 
the minimization in then turns out to be 
min 
ht∈h αt∈ 
l ht αt 
m 
i 
exp{−e π qi di ft− αtht yi } 
where h is the set of possible weak rankers αt is a positive weight 
and ft− αtht x ft− x αtht x several ways of computing 
coefficients αt and weak rankers ht may be considered following 
the idea of adaboost in adarank we take the approach of  forward 
stage-wise additive modeling and get the algorithm in figure 
 it can be proved that there exists a lower bound on the ranking 
accuracy for adarank on training data as presented in theorem 
t the following bound holds on the ranking 
accuracy of the adarank algorithm on training data 
 
m 
m 
i 
e π qi di ft yi ≥ − 
t 
t 
e−δt 
min − ϕ t 
where ϕ t m 
i pt i e π qi di ht yi δt 
min mini ··· m δt 
i and 
δt 
i e π qi di ft− αtht yi − e π qi di ft− yi 
−αte π qi di ht yi 
for all i · · · m and t · · · t 
a proof of the theorem can be found in appendix the theorem 
implies that the ranking accuracy in terms of the performance 
measure can be continuously improved as long as e−δt 
min − ϕ t 
holds 
 advantages 
adarank is a simple yet powerful method more importantly it 
is a method that can be justified from the theoretical viewpoint as 
discussed above in addition adarank has several other advantages 
when compared with the existing learning to rank methods such as 
ranking svm rankboost and ranknet 
first adarank can incorporate any performance measure 
provided that the measure is query based and in the range of − 
notice that the major ir measures meet this requirement in 
contrast the existing methods only minimize loss functions that are 
loosely related to the ir measures 
second the learning process of adarank is more efficient than 
those of the existing learning algorithms the time complexity of 
adarank is of order o k t ·m·n log n where k denotes the 
number of features t the number of rounds m the number of queries 
in training data and n is the maximum number of documents for 
queries in training data the time complexity of rankboost for 
example is of order o t · m · n 
 
third adarank employs a more reasonable framework for 
performing the ranking task than the existing methods specifically in 
adarank the instances correspond to queries while in the existing 
methods the instances correspond to document pairs as a result 
adarank does not have the following shortcomings that plague the 
existing methods a the existing methods have to make a strong 
assumption that the document pairs from the same query are 
independently distributed in reality this is clearly not the case and this 
problem does not exist for adarank b ranking the most relevant 
documents on the tops of document lists is crucial for document 
retrieval the existing methods cannot focus on the training on the 
tops as indicated in several methods for rectifying the problem 
have been proposed e g however they do not seem to 
fundamentally solve the problem in contrast adarank can naturally 
focus on training on the tops of document lists because the 
performance measures used favor rankings for which relevant documents 
are on the tops c in the existing methods the numbers of 
document pairs vary from query to query resulting in creating models 
biased toward queries with more document pairs as pointed out in 
 adarank does not have this drawback because it treats queries 
rather than document pairs as basic units in learning 
 differences from adaboost 
adarank is a boosting algorithm in that sense it is similar to 
adaboost but it also has several striking differences from adaboost 
first the types of instances are different adarank makes use of 
queries and their corresponding document lists as instances the 
labels in training data are lists of ranks relevance levels adaboost 
makes use of feature vectors as instances the labels in training 
data are simply and − 
second the performance measures are different in adarank 
the performance measure is a generic measure defined on the 
document list and the rank list of a query in adaboost the 
corresponding performance measure is a specific measure for binary 
classification also referred to as  margin 
third the ways of updating weights are also different in 
adaboost the distribution of weights on training instances is 
calculated according to the current distribution and the performance of 
the current weak learner in adarank in contrast it is calculated 
according to the performance of the ranking model created so far 
as shown in figure note that adaboost can also adopt the weight 
updating method used in adarank for adaboost they are 
equivalent cf page however this is not true for adarank 
 construction of weak ranker 
we consider an efficient implementation for weak ranker 
construction which is also used in our experiments in the 
implementation as weak ranker we choose the feature that has the optimal 
weighted performance among all of the features 
max 
k 
m 
i 
pt i e π qi di xk yi 
creating weak rankers in this way the learning process turns out 
to be that of repeatedly selecting features and linearly combining 
the selected features note that features which are not selected in 
the training phase will have a weight of zero 
 experimental results 
we conducted experiments to test the performances of adarank 
using four benchmark datasets ohsumed wsj ap and gov 
table features used in the experiments on ohsumed 
wsj and ap datasets c w d represents frequency of word 
w in document d c represents the entire collection n denotes 
number of terms in query · denotes the size function and 
id f · denotes inverse document frequency 
 wi∈q d ln c wi d wi∈q d ln c 
c wi c 
 
 wi∈q d ln id f wi wi∈q d ln c wi d 
 d 
 
 wi∈q d ln c wi d 
 d 
· id f wi wi∈q d ln c wi d · c 
 d ·c wi c 
 
 ln bm score 
 
 
 
 
 
map ndcg  ndcg  ndcg  ndcg  
bm 
ranking svm 
rarnkboost 
adarank map 
adarank ndcg 
figure ranking accuracies on ohsumed data 
 experiment setting 
ranking svm and rankboost were selected as 
baselines in the experiments because they are the state-of-the-art 
learning to rank methods furthermore bm was used as a 
baseline representing the state-of-the-arts ir method we actually used 
the tool lemur 
 
for adarank the parameter t was determined automatically 
during each experiment specifically when there is no 
improvement in ranking accuracy in terms of the performance measure the 
iteration stops and t is determined as the measure e map and 
ndcg  were utilized the results for adarank using map and 
ndcg  as measures in training are represented as adarank map 
and adarank ndcg respectively 
 experiment with ohsumed data 
in this experiment we made use of the ohsumed dataset 
to test the performances of adarank the ohsumed dataset 
consists of documents and queries there are in total 
 query-document pairs upon which relevance judgments are 
made the relevance judgments are either  d definitely relevant 
 p possibly relevant or  n not relevant the data have been 
used in many experiments in ir for example 
as features we adopted those used in document retrieval 
table shows the features for example tf term frequency idf 
 inverse document frequency dl document length and 
combinations of them are defined as features bm score itself is also a 
feature stop words were removed and stemming was conducted in 
the data 
we randomly divided queries into four even subsets and 
conducted -fold cross-validation experiments we tuned the 
parameters for bm during one of the trials and applied them to the other 
trials the results reported in figure are those averaged over four 
trials in map calculation we define the rank  d as relevant and 
 
http www lemurproject com 
table statistics on wsj and ap datasets 
dataset queries retrieved docs docs per query 
ap 
wsj 
 
 
 
 
 
map ndcg  ndcg  ndcg  ndcg  
bm 
ranking svm 
rankboost 
adarank map 
adarank ndcg 
figure ranking accuracies on wsj dataset 
the other two ranks as irrelevant from figure we see that both 
adarank map and adarank ndcg outperform bm ranking 
svm and rankboost in terms of all measures we conducted 
significant tests t-test on the improvements of adarank map over 
bm ranking svm and rankboost in terms of map the 
results indicate that all the improvements are statistically significant 
 p-value we also conducted t-test on the improvements 
of adarank ndcg over bm ranking svm and rankboost 
in terms of ndcg  the improvements are also statistically 
significant 
 experiment with wsj and ap data 
in this experiment we made use of the wsj and ap datasets 
from the trec ad-hoc retrieval track to test the performances of 
adarank wsj contains articles of wall street journals 
from to and ap contains articles of 
associated press in and queries are selected from the 
trec topics no ∼ no each query has a number of 
documents associated and they are labeled as  relevant or  irrelevant 
 to the query following the practice in the queries that have 
less than relevant documents were discarded table shows the 
statistics on the two datasets 
in the same way as in section we adopted the features listed 
in table for ranking we also conducted -fold cross-validation 
experiments the results reported in figure and are those 
averaged over four trials on wsj and ap datasets respectively from 
figure and we can see that adarank map and adarank ndcg 
outperform bm ranking svm and rankboost in terms of all 
measures on both wsj and ap we conducted t-tests on the 
improvements of adarank map and adarank ndcg over bm 
ranking svm and rankboost on wsj and ap the results 
indicate that all the improvements in terms of map are statistically 
significant p-value however only some of the improvements 
in terms of ndcg  are statistically significant although overall 
the improvements on ndcg scores are quite high - points 
 experiment with gov data 
in this experiment we further made use of the trec gov data 
to test the performance of adarank for the task of web retrieval 
the corpus is a crawl from the gov domain in early and 
has been used at trec web track since there are a total 
 
 
 
 
map ndcg  ndcg  ndcg  ndcg  
bm 
ranking svm 
rankboost 
adarank map 
adarank ndcg 
figure ranking accuracies on ap dataset 
 
 
 
 
 
 
 
map ndcg  ndcg  ndcg  ndcg  
bm 
ranking svm 
rankboost 
adarank map 
adarank ndcg 
figure ranking accuracies on gov dataset 
table features used in the experiments on gov dataset 
 bm msra 
 pagerank hostrank 
 relevance propagation features 
of web pages with hyperlinks in the data 
the queries in the topic distillation task in the web track of 
trec were used the ground truths for the queries are 
provided by the trec committee with binary judgment relevant 
or irrelevant the number of relevant pages vary from query to 
query from to 
we extracted features from each query-document pair 
table gives a list of the features they are the outputs of some 
well-known algorithms systems these features are different from 
those in table because the task is different 
again we conducted -fold cross-validation experiments the 
results averaged over four trials are reported in figure from the 
results we can see that adarank map and adarank ndcg 
outperform all the baselines in terms of all measures we conducted 
ttests on the improvements of adarank map and adarank ndcg 
over bm ranking svm and rankboost some of the 
improvements are not statistically significant this is because we have only 
 queries used in the experiments and the number of queries is 
too small 
 discussions 
we investigated the reasons that adarank outperforms the 
baseline methods using the results of the ohsumed dataset as examples 
first we examined the reason that adarank has higher 
performances than ranking svm and rankboost specifically we 
com 
 
 
 
 
 
d-n d-p p-n 
accuracy 
pair type 
ranking svm 
rankboost 
adarank map 
adarank ndcg 
figure accuracy on ranking document pairs with 
ohsumed dataset 
 
 
 
 
 
 
 
numberofqueries 
number of document pairs per query 
figure distribution of queries with different number of 
document pairs in training data of trial 
pared the error rates between different rank pairs made by 
ranking svm rankboost adarank map and adarank ndcg on the 
test data the results averaged over four trials in the -fold cross 
validation are shown in figure we use  d-n to stand for the pairs 
between  definitely relevant and  not relevant  d-p the pairs 
between  definitely relevant and  partially relevant and  p-n the 
pairs between  partially relevant and  not relevant from 
figure we can see that adarank map and adarank ndcg make 
fewer errors for  d-n and  d-p which are related to the tops of 
rankings and are important this is because adarank map and 
adarank ndcg can naturally focus upon the training on the tops 
by optimizing map and ndcg  respectively 
we also made statistics on the number of document pairs per 
query in the training data for trial the queries are clustered into 
different groups based on the the number of their associated 
document pairs figure shows the distribution of the query groups in 
the figure for example   - k is the group of queries whose 
number of document pairs are between and we can see that the 
numbers of document pairs really vary from query to query next 
we evaluated the accuracies of adarank map and rankboost in 
terms of map for each of the query group the results are reported 
in figure we found that the average map of adarank map 
over the groups is two points higher than rankboost furthermore 
it is interesting to see that adarank map performs particularly 
better than rankboost for queries with small numbers of document 
pairs e g   - k   k- k and   k- k the results indicate that 
adarank map can effectively avoid creating a model biased 
towards queries with more document pairs for adarank ndcg 
similar results can be observed 
 
 
 
 
map 
query group 
rankboost 
adarank map 
figure differences in map for different query groups 
 
 
 
 
 
trial trial trial trial 
map 
adarank map 
adarank ndcg 
figure map on training set when model is trained with map 
or ndcg  
we further conducted an experiment to see whether adarank has 
the ability to improve the ranking accuracy in terms of a measure 
by using the measure in training specifically we trained ranking 
models using adarank map and adarank ndcg and evaluated 
their accuracies on the training dataset in terms of both map and 
ndcg  the experiment was conducted for each trial figure 
 and figure show the results in terms of map and ndcg  
respectively we can see that adarank map trained with map 
performs better in terms of map while adarank ndcg trained 
with ndcg  performs better in terms of ndcg  the results 
indicate that adarank can indeed enhance ranking performance in 
terms of a measure by using the measure in training 
finally we tried to verify the correctness of theorem that is 
the ranking accuracy in terms of the performance measure can be 
continuously improved as long as e−δt 
min − ϕ t holds as 
an example figure shows the learning curve of adarank map 
in terms of map during the training phase in one trial of the cross 
validation from the figure we can see that the ranking accuracy 
of adarank map steadily improves as the training goes on until 
it reaches to the peak the result agrees well with theorem 
 conclusion and future work 
in this paper we have proposed a novel algorithm for learning 
ranking models in document retrieval referred to as adarank in 
contrast to existing methods adarank optimizes a loss function 
that is directly defined on the performance measures it employs 
a boosting technique in ranking model learning adarank offers 
several advantages ease of implementation theoretical soundness 
efficiency in training and high accuracy in ranking experimental 
results based on four benchmark datasets show that adarank can 
significantly outperform the baseline methods of bm ranking 
svm and rankboost 
 
 
 
 
 
trial trial trial trial 
ndcg  
adarank map 
adarank ndcg 
figure ndcg  on training set when model is trained 
with map or ndcg  
 
 
 
 
 
map 
number of rounds 
figure learning curve of adarank 
future work includes theoretical analysis on the generalization 
error and other properties of the adarank algorithm and further 
empirical evaluations of the algorithm including comparisons with 
other algorithms that can directly optimize performance measures 
 acknowledgments 
we thank harry shum wei-ying ma tie-yan liu gu xu bin 
gao robert schapire and andrew arnold for their valuable 
comments and suggestions to this paper 
 references 
 r baeza-yates and b ribeiro-neto modern information 
retrieval addison wesley may 
 c burges r ragno and q le learning to rank with 
nonsmooth cost functions in advances in neural 
information processing systems pages - mit 
press cambridge ma 
 c burges t shaked e renshaw a lazier m deeds 
n hamilton and g hullender learning to rank using 
gradient descent in icml pages - 
 y cao j xu t -y liu h li y huang and h -w hon 
adapting ranking svm to document retrieval in sigir 
pages - 
 d cossock and t zhang subset ranking using regression 
in colt pages - 
 n craswell d hawking r wilkinson and m wu 
overview of the trec web track in trec pages 
 - 
 n duffy and d helmbold boosting methods for regression 
mach learn - - 
 y freund r d iyer r e schapire and y singer an 
efficient boosting algorithm for combining preferences 
journal of machine learning research - 
 y freund and r e schapire a decision-theoretic 
generalization of on-line learning and an application to 
boosting j comput syst sci - 
 j friedman t hastie and r tibshirani additive logistic 
regression a statistical view of boosting the annals of 
statistics - 
 g fung r rosales and b krishnapuram learning 
rankings via convex hull separation in advances in neural 
information processing systems pages - mit 
press cambridge ma 
 t hastie r tibshirani and j h friedman the elements of 
statistical learning springer august 
 r herbrich t graepel and k obermayer large margin 
rank boundaries for ordinal regression mit press 
cambridge ma 
 w hersh c buckley t j leone and d hickam 
ohsumed an interactive retrieval evaluation and new large 
test collection for research in sigir pages - 
 k jarvelin and j kekalainen ir evaluation methods for 
retrieving highly relevant documents in sigir pages 
 - 
 t joachims optimizing search engines using clickthrough 
data in sigkdd pages - 
 t joachims a support vector method for multivariate 
performance measures in icml pages - 
 j lafferty and c zhai document language models query 
models and risk minimization for information retrieval in 
sigir pages - 
 d a metzler w b croft and a mccallum direct 
maximization of rank-based metrics for information 
retrieval technical report ciir 
 r nallapati discriminative models for information retrieval 
in sigir pages - 
 l page s brin r motwani and t winograd the 
pagerank citation ranking bringing order to the web 
technical report stanford digital library technologies 
project 
 j m ponte and w b croft a language modeling approach 
to information retrieval in sigir pages - 
 t qin t -y liu x -d zhang z chen and w -y ma a 
study of relevance propagation for web search in sigir 
pages - 
 s e robertson and d a hull the trec- filtering track 
final report in trec pages - 
 r e schapire y freund p barlett and w s lee boosting 
the margin a new explanation for the effectiveness of voting 
methods in icml pages - 
 r e schapire and y singer improved boosting algorithms 
using confidence-rated predictions mach learn 
 - 
 r song j wen s shi g xin t yan liu t qin x zheng 
j zhang g xue and w -y ma microsoft research asia at 
web track and terabyte track of trec in trec 
 a trotman learning to rank inf retr - 
 j xu y cao h li and y huang cost-sensitive learning 
of svm for ranking in ecml pages - 
 g -r xue q yang h -j zeng y yu and z chen 
exploiting the hierarchical structure for link analysis in 
sigir pages - 
 h yu svm selective sampling for ranking with application 
to data retrieval in sigkdd pages - 
appendix 
here we give the proof of theorem 
p set zt m 
i exp {−e π qi di ft yi } and φ t 
 
 
ϕ t according to the definition of αt we know that eαt φ t 
 −φ t 
 
zt 
m 
i 
exp {−e π qi di ft− αt ht yi } 
 
m 
i 
exp −e π qi di ft− yi − αt e π qi di ht yi − δt 
i 
≤ 
m 
i 
exp {−e π qi di ft− yi } exp {−αt e π qi di ht yi } e−δt 
min 
 e−δt 
min zt− 
m 
i 
exp {−e π qi di ft− yi } 
zt− 
exp{−αt e π qi di ht yi } 
 e−δt 
min zt− 
m 
i 
pt i exp{−αt e π qi di ht yi } 
moreover if e π qi di ht yi ∈ − then 
zt ≤ e−δt 
minzt− 
m 
i 
pt i 
 e π qi di ht yi 
 
e−αt 
 
 −e π qi di ht yi 
 
eαt 
 e−δt 
min zt− 
 
φ t 
 − φ t 
φ t 
 − φ t 
φ t 
 − φ t 
 
 
 zt− e−δt 
min φ t − φ t 
≤ zt− 
t 
t t− 
e−δt 
min φ t − φ t 
≤ z 
t 
t 
e−δt 
min φ t − φ t 
 m 
m 
i 
 
m 
exp{−e π qi di α h yi } 
t 
t 
e−δt 
min φ t − φ t 
 m 
m 
i 
 
m 
exp{−α e π qi di h yi − δ 
i } 
t 
t 
e−δt 
min φ t − φ t 
≤ me−δ 
min 
m 
i 
 
m 
exp{−α e π qi di h yi } 
t 
t 
e−δt 
min φ t − φ t 
≤ m e−δ 
min φ − φ 
t 
t 
e−δt 
min φ t − φ t 
 m 
t 
t 
e−δt 
min − ϕ t 
∴ 
 
m 
m 
i 
e π qi di ft yi ≥ 
 
m 
m 
i 
{ − exp −e π qi di ft yi } 
≥ − 
t 
t 
e−δt 
min − ϕ t 
hits hits 
trecexploring ir evaluation results with network analysis 
stefano mizzaro 
dept of mathematics and computer science 
university of udine 
via delle scienze - udine italy 
mizzaro dimi uniud it 
stephen robertson 
microsoft research 
 jj thomson avenue 
cambridge cb fb uk 
ser microsoft com 
abstract 
we propose a novel method of analysing data gathered from 
trec or similar information retrieval evaluation 
experiments we define two normalized versions of average 
precision that we use to construct a weighted bipartite graph 
of trec systems and topics we analyze the meaning of 
well known - and somewhat generalized - indicators from 
social network analysis on the systems-topics graph we 
apply this method to an analysis of trec data among 
the results we find that authority measures systems 
performance that hubness of topics reveals that some topics are 
better than others at distinguishing more or less effective 
systems that with current measures a system that wants to 
be effective in trec needs to be effective on easy topics 
and that by using different effectiveness measures this is no 
longer the case 
categories and subject descriptors 
h information storage and retrieval information 
search and retrieval 
general terms 
measurement experimentation 
 introduction 
evaluation is a primary concern in the information 
retrieval ir field trec text retrieval conference 
 is an annual benchmarking exercise that has become a 
de facto standard in ir evaluation before the actual 
conference trec provides to participants a collection of 
documents and a set of topics representations of information 
needs participants use their systems to retrieve and 
submit to trec a list of documents for each topic after the 
lists have been submitted and pooled the trec organizers 
employ human assessors to provide relevance judgements on 
the pooled set this defines a set of relevant documents for 
each topic system effectiveness is then measured by well 
established metrics mean average precision being the most 
used other conferences such as ntcir inex clef 
provide comparable data 
network analysis is a discipline that studies features and 
properties of usually large networks or graphs of 
particular importance is social network analysis that studies 
networks made up by links among humans friendship 
acquaintance co-authorship bibliographic citation etc 
network analysis and ir fruitfully meet in web search 
engine implementation as is already described in textbooks 
 current search engines use link analysis techniques to 
help rank the retrieved documents some indicators and 
the corresponding algorithms that compute them have been 
found useful in this respect and are nowadays well known 
inlinks the number of links to a web page pagerank 
and hits hyperlink-induced topic search several 
extensions to these algorithms have been and are being 
proposed these indicators and algorithms might be quite 
general in nature and can be used for applications which are 
very different from search result ranking one example is 
using hits for stemming as described by agosti et al 
in this paper we propose and demonstrate a method 
for constructing a network specifically a weighted complete 
bidirectional directed bipartite graph on a set of trec 
topics and participating systems links represent effectiveness 
measurements on system-topic pairs we then apply 
analysis methods originally developed for search applications to 
the resulting network this reveals phenomena previously 
hidden in trec data in passing we also provide a small 
generalization to kleinberg s hits algorithm as well as to 
inlinks and pagerank 
the paper is organized as follows sect gives some 
motivations for the work sect presents the basic ideas of 
normalizing average precision and of constructing a 
systemstopics graph whose properties are analyzed in sect sect 
presents some experiments on trec data sect 
discusses some issues and sect closes the paper 
 motivations 
we are interested in the following hypotheses 
 some systems are more effective than others 
t · · · tn map 
s ap s t · · · ap s tn map s 
 
 
 
sm ap sm t · · · ap sm tn map sm 
aap aap t · · · aap tn 
 a 
t t · · · map 
s · · · 
s · · · · · · 
 
 
 
 
aap · · · 
 b 
table ap map and aap 
 some topics are easier than others 
 some systems are better than others at distinguishing 
easy and difficult topics 
 some topics are better than others at distinguishing 
more or less effective systems 
the first of these hypotheses needs no further justification 
- every reported significant difference between any two 
systems supports it there is now also quite a lot of evidence 
for the second centered on the trec robust track 
our primary interest is in the third and fourth the third 
might be regarded as being of purely academic interest 
however the fourth has the potential for being of major 
practical importance in evaluation studies if we could identify 
a relatively small number of topics which were really good 
at distinguishing effective and ineffective systems we could 
save considerable effort in evaluating systems 
one possible direction from this point would be to attempt 
direct identification of such small sets of topics however in 
the present paper we seek instead to explore the 
relationships suggested by the hypotheses between what different 
topics tell us about systems and what different systems tell 
us about topics we seek methods of building and analysing 
a matrix of system-topic normalised performances with a 
view to giving insight into the issue and confirming or 
refuting the third and fourth hypotheses it turns out that 
the obvious symmetry implied by the above formulation of 
the hypotheses is a property worth investigating and the 
investigation does indeed give us valuable insights 
 the idea 
 st step average precision table 
from trec results one can produce an average 
precision ap table see tab a each ap si tj value 
measures the ap of system si on topic tj 
besides ap values the table shows mean average 
precision map values i e the mean of the ap values for a 
single system over all topics and what we call average 
average precision aap values i e the average of the ap 
values for a single topic over all systems 
map si 
 
n 
nx 
j 
ap si tj 
aap tj 
 
m 
mx 
i 
ap si tj 
maps are indicators of systems performance higher map 
means good system aap are indicators of the performance 
on a topic higher aap means easy topic - a topic on which 
all or most systems have good performance 
 critique of pure ap 
map is a standard well known and widely used ir 
effectiveness measure single ap values are used too e g 
in ap histograms topic difficulty is often discussed e g 
in trec robust track although aap values are not 
used and to the best of our knowledge have never been 
proposed the median not the average of ap on a topic 
is used to produce trec ap histograms however 
the ap values in tab present two limitations which are 
symmetric in some respect 
 problem they are not reliable to compare the 
effectiveness of a system on different topics relative 
to the other systems if for example ap s t 
ap s t can we infer that s is a good system i e 
has a good performance on t and a bad system on 
t the answer is no t might be an easy topic with 
high aap and t a difficult one low aap see an 
example in tab b s is outperformed on average 
by the other systems on t and it outperforms the 
other systems on t 
 problem conversely if for example ap s t 
ap s t can we infer that t is considered easier 
by s than by s no we cannot s might be a good 
system with high map and s a bad one low map 
see an example in tab b 
these two problems are a sort of breakdown of the well 
known high influence of topics on ir evaluation again our 
formulation makes explicit the topics systems symmetry 
 nd step normalizations 
to avoid these two problems we can normalize the ap 
table in two ways the first normalization removes the 
influence of the single topic ease on system performance each 
ap si tj value in the table depends on both system 
goodness and topic ease the value will increase if a system is 
good and or the topic is easy by subtracting from each 
ap si tj the aap tj value we obtain normalized ap 
values apa si tj normalized ap according to aap 
apa si tj ap si tj − aap tj 
that depend on system performance only the value will 
increase only if system performance is good see tab a 
the second normalization removes the influence of the 
single system effectiveness on topic ease by subtracting from 
each ap si tj the map si value we obtain normalized 
ap values apm si tj normalized ap according to map 
apm si tj ap si tj − map si 
that depend on topic ease only the value will increase only 
if the topic is easy i e all systems perform well on that 
topic see tab b 
in other words apa avoids problem apa s t values 
measure the performance of system s on topic t normalized 
t · · · tn map 
s apa s t · · · apa s tn map s 
 
 
 
sm apa sm t · · · apa sm tn map sm 
 · · · 
 a 
t · · · tn 
s apm s t · · · apm s tn 
 
 
 
sm apm sm t · · · apm sm tn 
aap aap t · · · aap tn 
 b 
t t · · · map 
s − · · · 
s · · · · · · 
 
 
 
 · · · 
t t · · · 
s − − · · · 
s · · · · · · 
 
 
 
aap · · · 
 c d 
table normalizations apa and map normalized 
ap apa and map map a normalized ap apm 
and aap aap b a numeric example c and d 
according to the ease of the topic easy topics will not have 
higher apa values now if for example apa s t 
apa s t we can infer that s is a good system on t and 
a bad system on t see tab c vice versa apm avoids 
problem apm s t values measure the ease of topic t 
according to system s normalized according to goodness 
of the system good systems will not lead to higher apm 
values if for example apm s t apm s t we 
can infer that t is considered easier by s than by s see 
tab d 
on the basis of tables a and b we can also define two 
new measures of system effectiveness and topic ease i e a 
normalized map map obtained by averaging the apa 
values on one row in tab a and a normalized aap aap 
obtained by averaging the apm values on one column in 
tab b 
map si 
 
n 
nx 
j 
apa si tj 
aap tj 
 
m 
mx 
i 
apm si tj 
thus overall system performance can be measured 
besides by means of map also by means of map moreover 
map is equivalent to map as can be immediately proved 
by using eqs and 
map si 
 
n 
nx 
j 
 ap si tj − aap tj 
 map si − 
 
n 
nx 
j 
aap tj 
 and 
n 
pn 
j aap tj is the same for all systems and 
conversely overall topic ease can be measured besides by 
t · · · tn 
s 
 apm 
sm 
t · · · tn 
s 
 apa 
sm 
s · · · sm t · · · tn 
s 
 apm 
sm 
t 
 apa 
t 
 
tn 
map aap 
figure construction of the adjacency matrix 
apa 
t 
is the transpose of apa 
means of aap also by means of aap and this is equivalent 
 the proof is analogous and relies on eqs and 
the two tables a and b are interesting per se and can 
be analyzed in several different ways in the following we 
propose an analysis based on network analysis techniques 
mainly kleinberg s hits algorithm there is a little further 
discussion of these normalizations in sect 
 rd step systems-topics graph 
the two tables a and b can be merged into a single one 
with the procedure shown in fig the obtained matrix 
can be interpreted as the adjacency matrix of a complete 
weighted bipartite graph that we call systems-topics graph 
arcs and weights in the graph can be interpreted as follows 
 weight on arc s → t how much the system s thinks 
that the topic t is easy - assuming that a system has 
no knowledge of the other systems or in other words 
how easy we might think the topic is knowing only 
the results for this one system this corresponds to 
apm values i e to normalized topic ease fig a 
 weight on arc s ← t how much the topic t thinks 
that the system s is good - assuming that a topic has 
no knowledge of the other topics or in other words 
how good we might think the system is knowing only 
the results for this one topic this corresponds to 
apa normalized system effectiveness fig b 
figs c and d show the systems-topics complete weighted 
bipartite graph on a toy example with systems and 
topics the graph is split in two parts to have an understandable 
graphical representation arcs in fig c are labeled with 
apm values arcs in fig d are labeled with apa values 
 analysis of the graph 
 weighted inlinks outlinks pagerank 
the sum of weighted outlinks i e the sum of the weights 
on the outgoing arcs from each node is always zero 
 the outlinks on each node corresponding to a system 
s fig c is the sum of all the corresponding apm 
values on one row of the matrix in tab b 
 the outlinks on each node corresponding to a topic 
t fig d is the sum of all the corresponding apa 
 a b 
 c d 
figure the relationships between systems and 
topics a and b and the systems-topics graph for 
a toy example c and d dashed arcs correspond 
to negative values 
h 
 a 
s 
 
sm 
t 
 
tn 
 
s · · · sm t · · · tn 
s 
 apm 
 apa 
sm 
t 
 apa 
t 
 
tn apm 
t 
 
· 
a 
 h 
s 
 
sm 
t 
 
tn 
figure hub and authority computation 
values on one row of the transpose of the matrix in 
tab a 
the average 
of weighted inlinks is 
 map for each node corresponding to a system s this 
corresponds to the average of all the corresponding 
apa values on one column of the apa 
t 
part of the 
adjacency matrix see fig 
 aap for each node corresponding to a topic t this 
corresponds to the average of all the corresponding 
apm values on one column of the apm part of the 
adjacency matrix see fig 
therefore weighted inlinks measure either system 
effectiveness or topic ease weighted outlinks are not meaningful we 
could also apply the pagerank algorithm to the network 
the meaning of the pagerank of a node is not quite so 
obvious as inlinks and outlinks but it also seems a sensible 
measure of either system effectiveness or topic ease if a 
system is effective it will have several incoming links with high 
 
usually the sum of the weights on the incoming arcs to 
each node is used in place of the average since the graph is 
complete it makes no difference 
weights apa if a topic is easy it will have high weights 
 apm on the incoming links too we will see experimental 
confirmation in the following 
 hubs and authorities 
let us now turn to more sophisticated indicators 
kleinberg s hits algorithm defines for a directed graph two 
indicators hubness and authority we reiterate here some of 
the basic details of the hits algorithm in order to 
emphasize both the nature of our generalization and the 
interpretation of the hits concepts in this context usually hubness 
and authority are defined as h x 
p 
x→y a y and a x 
p 
y→x h y and described intuitively as a good hub links 
many good authorities a good authority is linked from many 
good hubs as it is well known an equivalent formulation 
in linear algebra terms is see also fig 
h aa and a at 
h 
 where h is the hubness vector with the hub values for all 
the nodes a is the authority vector a is the adjacency 
matrix of the graph and at 
its transpose usually a 
contains s and s only corresponding to presence and absence 
of unweighted directed arcs but eq can be immediately 
generalized to in fact it is already valid for a containing 
any real value i e to weighted graphs 
therefore we can have a generalized version or rather 
a generalized interpretation since the formulation is still 
the original one of hubness and authority for all nodes in 
a graph an intuitive formulation of this generalized hits 
is still available although slightly more complex a good 
hub links by means of arcs having high weights many good 
authorities a good authority is linked by means of arcs 
having high weights from many good hubs since arc weights 
can be in general negative hub and authority values can be 
negative and one could speak of unhubness and unauthority 
the intuitive formulation could be completed by adding that 
a good hub links good unauthorities by means of links with 
highly negative weights a good authority is linked by good 
unhubs by means of links with highly negative weights 
and also a good unhub links positively good 
unauthorities and negatively good authorities a good unauthority 
is linked positively from good unhubs and negatively from 
good hubs 
let us now apply generalized hits to our systems-topics 
graph we compute a s h s a t and h t intuitively 
we expect that a s is somehow similar to inlinks so it 
should be a measure of either systems effectiveness or topic 
ease similarly hubness should be more similar to outlinks 
thus less meaningful although the interplay between hub 
and authority might lead to the discovery of something 
different let us start by remarking that authority of topics 
and hubness of systems depend only on each other similarly 
hubness of topics and authority of systems depend only on 
each other see figs c d and 
thus the two graphs in figs c and d can be analyzed 
independently in fact the entire hits analysis could be 
done in one direction only with just apm s t values or 
alternatively with just apa s t as discussed below 
probably most interest resides in the hubness of topics and the 
authority of systems so the latter makes sense however in 
this paper we pursue both analyses together because the 
symmetry itself is interesting 
considering fig c we can state that 
 authority a t of a topic node t increases when 
- if h si apm si t increases 
 or if apm si t h si increases 
- if h si apm si t decreases 
 or if apm si t h si decreases 
 hubness h s of a system node s increases when 
- if a tj apm s tj increases 
 or if apm s tj a tj increases 
- if a tj apm s tj decreases 
 or if apm s tj a tj decreases 
we can summarize this as a t is high if apm s t is high 
for those systems with high h s h s is high if apm s t 
is high for those topics with high a t intuitively authority 
a t of a topic measures topic ease hubness h s of a system 
measures system s capability to recognize easy topics a 
system with high unhubness negative hubness would tend 
to regard easy topics as hard and hard ones as easy 
the situation for fig d i e for a s and h t is 
analogous authority a s of a system node s measures system 
effectiveness it increases with the weight on the arc i e 
apa s tj and the hubness of the incoming topic nodes tj 
hubness h t of a topic node t measures topic capability to 
recognize effective systems if h t it increases further 
if apa s tj increases if h t it increases if apa s tj 
decreases 
intuitively we can state that a system has a higher 
authority if it is more effective on topics with high hubness 
and a topic has a higher hubness if it is easier for those 
systems which are more effective in general conversely for 
system hubness and topic authority a topic has a higher 
authority if it is easier on systems with high hubness and 
a system has a higher hubness if it is more effective for 
those topics which are easier in general 
therefore for each system we have two indicators 
authority a s measuring system effectiveness and hubness 
 h s measuring system capability to estimate topic ease 
and for each topic we have two indicators authority a t 
measuring topic ease and hubness h t measuring topic 
capability to estimate systems effectiveness we can define 
them formally as 
a s 
x 
t 
h t · apa s t h t 
x 
s 
a s · apa s t 
a t 
x 
s 
h s · apm s t h s 
x 
t 
a t · apm s t 
we observe that the hubness of topics may be of particular 
interest for evaluation studies it may be that we can 
evaluate the effectiveness of systems efficiently by using relatively 
few high-hubness topics 
 experiments 
we now turn to discuss if these indicators are meaningful 
and useful in practice and how they correlate with standard 
measures used in trec we have built the systems-topics 
graph for trec data featuring 
systems - actually 
 
actually trec data features systems due to 
some bug in our scripts we did not include one system 
 manext d n but the results should not be affected 
 
 
 
 
- - 
napm 
napa 
ap 
figure distributions of ap apa and apm values 
in trec data 
map in pr h a 
map 
inlinks 
pagerank 
hub 
 a 
aap in pr h a 
aap 
inlinks 
pagerank 
hub 
 b 
table correlations between network analysis 
measures and map a and aap b 
runs - on topics this section illustrates the results 
obtained mining these data according to the method presented 
in previous sections 
fig shows the distributions of ap apa and apm 
whereas ap is very skewed both apa and apm are much 
more symmetric as it should be since they are constructed 
by subtracting the mean tables a and b show the 
pearson s correlation values between inlinks pagerank hub 
authority and respectively map or aap outlinks 
values are not shown since they are always zero as seen in 
sect as expected inlinks and pagerank have a perfect 
correlation with map and aap authority has a very high 
correlation too with map and aap hub assumes slightly 
lower values 
let us analyze the correlations more in detail the 
correlations chart in figs a and b demonstrate the high 
correlation between authority and map or aap hubness 
presents interesting phenomena both fig c correlation 
with map and fig d correlation with aap show that 
correlation is not exact but neither is it random this given 
the meaning of hubness capability in estimating topic ease 
and system effectiveness means two things i more 
effective systems are better at estimating topic ease and ii 
easier topics are better at estimating system effectiveness 
whereas the first statement is fine there is nothing against 
it the second is a bit worrying it means that system 
effectiveness in trec is affected more by easy topics than by 
difficult topics which is rather undesirable for quite obvious 
reasons a system capable of performing well on a difficult 
topic i e on a topic on which the other systems perform 
badly would be an important result for ir effectiveness 
con- e- 
- e- 
- e- 
- e- 
 e 
 e- 
 e- 
 e- 
 
- e- 
- e- 
- e- 
 e 
 e- 
 e- 
 e- 
 e- 
 e- 
 
 a b 
 e 
 e- 
 e- 
 e- 
 e- 
 e- 
 e- 
 e- 
 
 e 
 e- 
 e- 
 e- 
 e- 
 e- 
 e- 
 e- 
 
 c d 
figure correlations map x axis and authority y axis of systems a aap and authority of topics 
 b map and hub of systems c and aap and hub of topics d 
versely a system capable of performing well on easy topics 
is just a confirmation of the state of the art indeed the 
correlation between hubness and aap statement i above is 
higher than the correlation between hubness and map 
 corresponding to statement ii vs however this 
phenomenon is quite strong this is also confirmed by the 
work being done on the trec robust track 
in this respect it is interesting to see what happens if we 
use a different measure from map and aap the gmap 
 geometric map metric is defined as the geometric mean of 
ap values or equivalently as the arithmetic mean of the 
logarithms of ap values gmap has the property of giving 
more weight to the low end of the ap scale i e to low ap 
values and this seems reasonable since intuitively a 
performance increase in map values from to should 
be more important than an increase from to to 
use gmap in place of map and aap we only need to take 
the logarithms of initial ap values i e those in tab a 
 zero values are modified into ε we then repeat 
the same normalization process with gmap and gaap 
- geometric aap - replacing map and aap whereas 
authority values still perfectly correlate with gmap 
and gaap the correlation with hubness largely 
disappears values are − and − - slightly negative but 
not enough to concern us 
this is yet another confirmation that trec effectiveness 
as measured by map depends mainly on easy topics gmap 
appears to be a more balanced measure note that 
perhaps surprisingly gmap is indeed fairly well balanced not 
biased in the opposite direction - that is it does not 
overemphasize the difficult topics 
in sect below we discuss another transformation 
replacing the log function used in gmap with logit this has 
a similar effect the correlation of mean logitap and average 
logitap with hubness are now small positive numbers 
and respectively still comfortably away from the high 
correlations with regular map and aap i e not presenting 
the problematic phenomenon ii above over-dependency on 
easy topics 
we also observe that hub values are positive whereas 
authority assumes as predicted both positive and negative 
values an intuitive justification is that negative hubness 
would indicate a node which disagrees with the other nodes 
e g a system which does better on difficult topics or a 
topic on which bad systems do better such systems and 
topics would be quite strange and probably do not appear 
in trec finally although one might think that topics with 
several relevant documents are more important and difficult 
this is not the case there is no correlation between hub or 
any other indicator and the number of documents relevant 
to a topic 
 discussion 
 related work 
there has been considerable interest in recent years in 
questions of statistical significance of effectiveness 
comparisons between systems e g and related questions of 
how many topics might be needed to establish differences 
 e g we regard some results of the present study as 
in some way complementary to this work in that we make a 
step towards answering the question which topics are best 
for establishing differences 
the results on evaluation without relevance judgements 
such as show that to some extent good systems agree 
on which are the good documents we have not addressed 
the question of individual documents in the present analysis 
but this effect is certainly analogous to our results 
 are normalizations necessary 
at this point it is also worthwhile to analyze what would 
happen without the map- and aap-normalizations defined 
in sect indeed the process of graph construction 
 sect is still valid both the apm and apa matrices 
are replaced by the ap one and then everything goes on as 
above therefore one might think that the normalizations 
are unuseful in this setting 
this is not the case from the theoretical point of view 
the ap-only graph does not present the interesting 
properties above discussed since the ap-only graph is 
symmetrical the weight on each incoming link is equal to the weight 
on the corresponding outgoing link inlinks and outlinks 
assume the same values there is symmetry also in 
computing hub and authority that assume the same value for each 
node since the weights on the incoming and outgoing arcs 
are the same this could be stated in more precise and 
formal terms but one might still wonder if on the overall graph 
there are some sort of counterbalancing effects it is 
therefore easier to look at experimental data which confirm that 
the normalizations are needed the correlations between ap 
inlinks outlinks hub and or authority are all very close 
to one none of them is below 
 are these normalizations sufficient 
it might be argued that in the case of apa for example 
the amount we have subtracted from each ap value is 
topicdependent therefore the range of the resulting apa value 
is also topic-dependent e g the maximum is − aap tj 
and the minimum is − aap tj this suggests that the 
cross-topic comparisons of these values suggested in sect 
may not be reliable a similar issue arises for apm and 
comparisons across systems 
one possible way to overcome this would be to use an 
unconstrained measure whose range is the full real line note 
that in applying the method to gmap by using log ap we 
avoid the problem with the lower limit but retain it for the 
upper limit one way to achieve an unconstrainted range 
would be to use the logit function rather than the log 
we have also run this variant as already reported in 
sect above and it appears to provide very similar 
results to the gmap results already given this is not 
surprising since in practice the two functions are very similar 
over most of the operative range the normalizations thus 
seem reliable 
 on aat 
and at 
a 
it is well known that h and a vectors are the principal 
left eigenvectors of aat 
and at 
a respectively this can 
be easily derived from eqs and that in the case of 
citation graphs aat 
and at 
a represent respectively 
bibliographic coupling and co-citations what is the meaning 
if any of aat 
and at 
a in our systems-topics graph it 
is easy to derive that 
aat 
 i j 
 
 
 
 
 
if i ∈ s ∧ j ∈ t 
or i ∈ t ∧ j ∈ s 
p 
k a i k · a j k otherwise 
at 
a i j 
 
 
 
 
 
if i ∈ s ∧ j ∈ t 
or i ∈ t ∧ j ∈ s 
p 
k a k i · a k j otherwise 
 where s is the set of indices corresponding to systems and t 
the set of indices corresponding to topics thus aat 
and 
at 
a are block diagonal matrices with two blocks each one 
relative to systems and one relative to topics 
 a if i j ∈ s then aat 
 i j 
p 
k∈t apm i k ·apm j k 
measures how much the two systems i and j agree in 
estimating topics ease apm high values mean that 
the two systems agree on topics ease 
 b if i j ∈ t then aat 
 i j 
p 
k∈s apa k i ·apa k j 
measures how much the two topics i and j agree in 
estimating systems effectiveness apa high values mean 
that the two topics agree on systems effectiveness and 
that trec results would not change by leaving out one 
of the two topics 
 c if i j ∈ s then at 
a i j 
p 
k∈t apa i k · apa j k 
measures how much agreement on the effectiveness of 
two systems i and j there is over all topics high 
values mean that many topics quite agree on the two 
systems effectiveness low values single out systems that 
are somehow controversial and that need several topics 
to have a correct effectiveness assessment 
 d if i j ∈ t then at 
a i j 
p 
k∈s apm k i ·apm k j 
measures how much agreement on the ease of the two 
topics i and j there is over all systems high values mean 
that many systems quite agree on the two topics ease 
therefore these matrices are meaningful and somehow 
interesting for instance the submatrix b corresponds to 
a weighted undirected complete graph whose nodes are the 
topics and whose arc weights are a measure of how much 
two topics agree on systems effectiveness two topics that 
are very close on this graph give the same information and 
therefore one of them could be discarded without changes in 
trec results it would be interesting to cluster the topics 
on this graph furthermore the matrix graph a could be 
useful in trec pool formation systems that do not agree 
on topic ease would probably find different relevant 
documents and should therefore be complementary in pool 
formation note that no notion of single documents is involved 
in the above analysis 
 insights 
as indicated the primary contribution of this paper has 
been a method of analysis however in the course of 
applying this method to one set of trec results we have 
achieved some insights relating to the hypotheses formulated 
in sect 
 we confirm hypothesis above that some topics are 
easier than others 
 differences in the hubness of systems reveal that some 
systems are better than others at distinguishing easy 
and difficult topics thus we have some confirmation of 
hypothesis 
 there are some relatively idiosyncratic systems which 
do badly on some topics generally considered easy but 
well on some hard topics however on the whole the 
more effective systems are better at distinguishing easy 
and difficult topics this is to be expected a really 
bad system will do badly on everything while even a 
good system may have difficulty with some topics 
 differences in the hubness of topics reveal that some 
topics are better than others at distinguising more or 
less effective systems thus we have some confirmation 
of hypothesis 
 if we use map as the measure of effectiveness it is 
also true that the easiest topics are better at 
distinguishing more or less effective systems as argued in 
sect this is an undesirable property gmap is more 
balanced 
clearly these ideas need to be tested on other data sets 
however they reveal that the method of analysis proposed 
in this paper can provide valuable information 
 selecting topics 
the confirmation of hypothesis leads as indicated to 
the idea that we could do reliable system evaluation on a 
much smaller set of topics provided we could select such an 
appropriate set this selection may not be straightforward 
however it is possible that simply selecting the high 
hubness topics will achieve this end however it is also possible 
that there are significant interactions between topics which 
would render such a simple rule ineffective this 
investigation would therefore require serious experimentation for 
this reason we have not attempted in this paper to point to 
the specific high hubness topics as being good for evaluation 
this is left for future work 
 conclusions and future 
developments 
the contribution of this paper is threefold 
 we propose a novel way of normalizing ap values 
 we propose a novel method to analyse trec data 
 the method applied on trec data does indeed reveal 
some hidden properties 
more particularly we propose average average precision 
 aap a measure of topic ease and a novel way of 
normalizing the average precision measure in trec on the basis 
of both map mean average precision and aap the 
normalized measures apm and apa are used to build a 
bipartite weighted systems-topics graph that is then 
analyzed by means of network analysis indicators widely known 
in the social network analysis field but somewhat 
generalised we note that no such approach to trec data 
analysis has been proposed so far the analysis shows that 
with current measures a system that wants to be effective 
in trec needs to be effective on easy topics also it is 
suggested that a cluster analysis on topic similarity can lead to 
relying on a lower number of topics 
our method of analysis as described in this paper can 
be applied only a posteriori i e once we have all the 
topics and all the systems available adding removing a new 
system topic would mean re-computing hubness and 
authority indicators moreover we are not explicitly proposing 
a change to current trec methodology although this could 
be a by-product of these - and further - analyses 
this is an initial work and further analyses could be 
performed for instance other effectiveness metrics could be 
used in place of ap other centrality indicators widely 
used in social network analysis could be computed although 
probably with similar results to pagerank it would be 
interesting to compute the higher-order eigenvectors of at 
a 
and aat 
 the same kind of analysis could be performed at 
the document level measuring document ease hopefully 
further analyses of the graph defined in this paper 
according to the approach described can be insightful for a better 
understanding of trec or similar data 
acknowledgments 
we would like to thank nick craswell for insightful 
discussions and the anonymous referees for useful remarks part 
of this research has been carried on while the first author 
was visiting microsoft research cambridge whose financial 
support is acknowledged 
 references 
 m agosti m bacchin n ferro and m melucci 
improving the automatic retrieval of text documents 
in proceedings of the rd clef workshop volume 
 of lncs pages - 
 c buckley and e voorhees evaluating evaluation 
measure stability in rd sigir pages - 
 s chakrabarti mining the web morgan kaufmann 
 
 g v cormack and t r lynam statistical precision 
of information retrieval evaluation in th sigir 
pages - 
 j kleinberg authoritative sources in a hyperlinked 
environment j of the acm - 
 m levene an introduction to search engines and 
web navigation addison wesley 
 l page s brin r motwani and t winograd the 
pagerank citation ranking bringing order to the 
web 
http dbpubs stanford edu pub - 
 s robertson on gmap - and other transformations 
in th cikm pages - 
 m sanderson and j zobel information retrieval 
system evaluation effort sensitivity and reliability in 
 th sigir pages - 
http doi acm org 
 i soboroff c nicholas and p cahan ranking 
retrieval systems without relevance judgments in th 
sigir pages - 
 trec common evaluation measures 
http trec nist gov pubs trec appendices 
ce measures pdf last visit jan 
 text retrieval conference trec 
http trec nist gov last visit jan 
 e voorhees and c buckley the effect of topic set 
size on retrieval experiment error in th sigir 
pages - 
 e m voorhees overview of the trec robust 
retrieval track in trec proceedings 
 e m voorhees and d k harman 
trecexperiment and evaluation in information retrieval 
mit press 
 s wasserman and k faust social network analysis 
cambridge university press cambridge uk 
diffusionrank a possible penicillin for web spamming 
haixuan yang irwin king and michael r lyu 
dept of computer science and engineering 
the chinese university of hong kong 
shatin nt hong kong 
{hxyang king lyu} cse cuhk edu hk 
abstract 
while the pagerank algorithm has proven to be very 
effective for ranking web pages the rank scores of web pages 
can be manipulated to handle the manipulation problem 
and to cast a new insight on the web structure we propose 
a ranking algorithm called diffusionrank diffusionrank is 
motivated by the heat diffusion phenomena which can be 
connected to web ranking because the activities flow on the 
web can be imagined as heat flow the link from a page to 
another can be treated as the pipe of an air-conditioner and 
heat flow can embody the structure of the underlying web 
graph theoretically we show that diffusionrank can serve 
as a generalization of pagerank when the heat diffusion 
coefficient γ tends to infinity in such a case γ 
diffusionrank pagerank has low ability of anti-manipulation 
when γ diffusionrank obtains the highest ability of 
anti-manipulation but in such a case the web structure is 
completely ignored consequently γ is an interesting factor 
that can control the balance between the ability of 
preserving the original web and the ability of reducing the effect 
of manipulation it is found empirically that when γ 
diffusionrank has a penicillin-like effect on the link 
manipulation moreover diffusionrank can be employed to 
find group-to-group relations on the web to divide the web 
graph into several parts and to find link communities 
experimental results show that the diffusionrank algorithm 
achieves the above mentioned advantages as expected 
categories and subject descriptors h 
 information systems information search and retrieval g 
 discrete mathematics graph theory 
general terms algorithms 
 introduction 
while the pagerank algorithm has proven to be very 
effective for ranking web pages inaccurate pagerank 
results are induced because of web page manipulations by 
people for commercial interests the manipulation problem is 
also called the web spam which refers to hyperlinked pages 
on the world wide web that are created with the intention 
of misleading search engines it is reported that 
approximately of all pages in the biz domain and about 
of the pages in the us domain belong to the spam category 
 the reason for the increasing amount of web spam is 
explained in some web site operators try to influence 
the positioning of their pages within search results because 
of the large fraction of web traffic originating from searches 
and the high potential monetary value of this traffic 
from the viewpoint of the web site operators who want 
to increase the ranking value of a particular page for search 
engines keyword stuffing and link stuffing are being used 
widely from the viewpoint of the search engine 
managers the web spam is very harmful to the users evaluations 
and thus their preference to choosing search engines because 
people believe that a good search engine should not return 
irrelevant or low-quality results there are two methods 
being employed to combat the web spam problem machine 
learning methods are employed to handle the keyword 
stuffing to successfully apply machine learning methods we 
need to dig out some useful textual features for web pages 
to mark part of the web pages as either spam or non-spam 
then to apply supervised learning techniques to mark other 
pages for example see link analysis methods are 
also employed to handle the link stuffing problem one 
example is the trustrank a link-based method in which 
the link structure is utilized so that human labelled trusted 
pages can propagate their trust scores trough their links 
this paper focuses on the link-based method 
the rest of the materials are organized as follows in the 
next section we give a brief literature review on various 
related ranking techniques we establish the heat diffusion 
model hdm on various cases in section and propose 
diffusionrank in section in section we describe the 
data sets that we worked on and the experimental results 
finally we draw conclusions in section 
 literature review 
the importance of a web page is determined by either 
the textual content of pages or the hyperlink structure or 
both as in previous work we focus on ranking 
methods solely determined by hyperlink structure of the 
web graph all the mentioned ranking algorithms are 
established on a graph for our convenience we first give some 
notations denote a static graph by g v e where v 
{v v vn} e { vi vj there is an edge from vi to 
vj} ii and di denote the in-degree and the out-degree of 
page i respectively 
 pagerank 
the importance of a web page is an inherently subjective 
matter which depends on the reader s interests knowledge 
and attitudes however the average importance of all 
readers can be considered as an objective matter pagerank 
tries to find such average importance based on the web link 
structure which is considered to contain a large amount of 
statistical data the web is modelled by a directed graph g 
in the pagerank algorithms and the rank or importance 
xi for page vi ∈ v is defined recursively in terms of pages 
which point to it xi j i ∈e aijxj where aij is assumed 
to be dj if there is a link from j to i and otherwise or 
in matrix terms x ax when the concept of random 
jump is introduced the matrix form is changed to 
x − α g t 
 αa x 
where α is the probability of following the actual link from a 
page − α is the probability of taking a random jump 
and g is a stochastic vector i e t 
g typically α 
 and g 
n 
 is one of the standard settings where is 
the vector of all ones 
 trustrank 
trustrank is composed of two parts the first part 
is the seed selection algorithm in which the inverse 
pagerank was proposed to help an expert of determining a good 
node the second part is to utilize the biased pagerank 
in which the stochastic distribution g is set to be shared by 
all the trusted pages found in the first part moreover the 
initial input of x is also set to be g the justification for 
the inverse pagerank and the solid experiments support its 
advantage in combating the web spam although there are 
many variations of pagerank e g a family of link-based 
ranking algorithms in trustrank is especially chosen for 
comparisons for three reasonss it is designed for 
combatting spamming its fixed parameters make a 
comparison easy and it has a strong theoretical relations with 
pagerank and diffusionrank 
 manifold ranking 
in the idea of ranking on the data manifolds was 
proposed the data points represented as vectors in euclidean 
space are considered to be drawn from a manifold from 
the data points on such a manifold an undirected weighted 
graph is created then the weight matrix is given by the 
gaussian kernel smoothing while the manifold ranking 
algorithm achieves an impressive result on ranking images 
the biased vector g and the parameter k in the general 
personalized pagerank in are unknown in the web graph 
setting therefore we do not include it in the comparisons 
 heat diffusion 
heat diffusion is a physical phenomena in a medium 
heat always flow from position with high temperature to 
position with low temperature heat kernel is used to 
describe the amount of heat that one point receives from 
another point recently the idea of heat kernel on a manifold 
is borrowed in applications such as dimension reduction 
and classification in these work the input data 
is considered to lie in a special structure 
all the above topics are related to our work the readers 
can find that our model is a generalization of pagerank in 
order to resist web manipulation that we inherit the first 
part of trustrank that we borrow the concept of ranking on 
the manifold to introduce our model and that heat diffusion 
is a main scheme in this paper 
 heat diffusion model 
heat diffusion provides us with another perspective about 
how we can view the web and also a way to calculate 
ranking values in this paper the web pages are considered to 
be drawn from an unknown manifold and the link structure 
forms a directed graph which is considered as an 
approximation to the unknown manifold the heat kernel established 
on the web graph is considered as the representation of the 
relationship between web pages the temperature 
distribution after a fixed time period induced by a special initial 
temperature distribution is considered as the rank scores on 
the web pages before establishing the proposed models we 
first show our motivations 
 motivations 
there are two points to explain that pagerank is 
susceptible to web spam 
 over-democratic there is a belief behind 
pagerank-all pages are born equal this can be seen from 
the equal voting ability of one page the sum of each 
column is equal to one this equal voting ability of all 
pages gives the chance for a web site operator to 
increase a manipulated page by creating a large number 
of new pages pointing to this page since all the newly 
created pages can obtain an equal voting right 
 input-independent for any given non-zero initial 
input the iteration will converge to the same stable 
distribution corresponding to the maximum eigenvalue 
 of the transition matrix this input-independent 
property makes it impossible to set a special initial 
input larger values for trusted pages and less values even 
negative values for spam pages to avoid web spam 
the input-independent feature of pagerank can be further 
explained as follows p − α g t 
 αa is a positive 
stochastic matrix if g is set to be a positive stochastic vector 
 the uniform distribution is one of such settings and so the 
largest eigenvalue is and no other eigenvalue whose 
absolute value is equal to which is guaranteed by the perron 
theorem let y be the eigenvector corresponding to 
then we have py y let {xk} be the sequence generated 
from the iterations xk pxk and x is the initial input 
if {xk} converges to x then xk pxk implies that x 
must satisfy px x since the only maximum eigenvalue 
is we have x cy where c is a constant and if both x 
and y are normalized by their sums then c the above 
discussions show that pagerank is independent of the initial 
input x 
in our opinion g and α are objective parameters 
determined by the users behaviors and preferences a α and 
g are the true web structure while a is obtained by a 
crawler and the setting α is accepted by the people 
we think that g should be determined by a user behavior 
investigation something like without any prior 
knowledge g has to be set as g 
n 
 
trustrank model does not follow the true web structure 
by setting a biased g but the effects of combatting 
spamming are achieved in pagerank is on the contrary in 
some ways we expect a ranking algorithm that has an 
effect of anti-manipulation as trustrank while respecting the 
true web structure as pagerank 
we observe that the heat diffusion model is a natural way 
to avoid the over-democratic and input-independent feature 
of pagerank since heat always flows from a position with 
higher temperatures to one with lower temperatures points 
are not equal as some points are born with high 
temperatures while others are born with low temperatures on the 
other hand different initial temperature distributions will 
give rise to different temperature distributions after a fixed 
time period based on these considerations we propose the 
novel diffusionrank this ranking algorithm is also 
motivated by the viewpoint for the web structure we view 
all the web pages as points drawn from a highly complex 
geometric structure like a manifold in a high dimensional 
space on a manifold heat can flow from one point to 
another through the underlying geometric structure in a given 
time period different geometric structures determine 
different heat diffusion behaviors and conversely the diffusion 
behavior can reflect the geometric structure more 
specifically on the manifold the heat flows from one point to 
another point and in a given time period if one point x 
receives a large amount of heat from another point y we 
can say x and y are well connected and thus x and y have 
a high similarity in the sense of a high mutual connection 
we note that on a point with unit mass the temperature 
and the heat of this point are equivalent and these two terms 
are interchangeable in this paper in the following we first 
show the hdm on a manifold which is the origin of hdm 
but cannot be employed to the world wide web directly 
and so is considered as the ideal case to connect the ideal 
case and the practical case we then establish hdm on a 
graph as an intermediate case to model the real world 
problem we further build hdm on a random graph as a 
practical case finally we demonstrate the diffusionrank 
which is derived from the hdm on a random graph 
 heat flow on a known manifold 
if the underlying manifold is known the heat flow 
throughout a geometric manifold with initial conditions can be 
described by the following second order differential equation 
∂f x t 
∂t 
− ∆f x t where f x t is the heat at location x 
at time t and ∆f is the laplace-beltrami operator on a 
function f the heat diffusion kernel kt x y is a special 
solution to the heat equation with a special initial condition-a 
unit heat source at position y when there is no heat in other 
positions based on this the heat kernel kt x y describes 
the heat distribution at time t diffusing from the initial unit 
heat source at position y and thus describes the 
connectivity which is considered as a kind of similarity between x 
and y however it is very difficult to represent the world 
wide web as a regular geometry with a known dimension 
even the underlying is known it is very difficult to find the 
heat kernel kt x y which involves solving the heat 
equation with the delta function as the initial condition this 
motivates us to investigate the heat flow on a graph the 
graph is considered as an approximation to the underlying 
manifold and so the heat flow on the graph is considered as 
an approximation to the heat flow on the manifold 
 on an undirected graph 
on an undirected graph g the edge vi vj is considered 
as a pipe that connects nodes vi and vj the value fi t 
describes the heat at node vi at time t beginning from an 
initial distribution of heat given by fi at time zero f t 
 f denotes the vector consisting of fi t fi 
we construct our model as follows suppose at time t 
each node i receives m i j t ∆t amount of heat from its 
neighbor j during a period of ∆t the heat m i j t ∆t 
should be proportional to the time period ∆t and the heat 
difference fj t − fi t moreover the heat flows from node 
j to node i through the pipe that connects nodes i and j 
based on this consideration we assume that m i j t ∆t 
γ fj t − fi t ∆t as a result the heat difference at node 
i between time t ∆t and time t will be equal to the sum 
of the heat that it receives from all its neighbors this is 
formulated as 
fi t ∆t − fi t 
j j i ∈e 
γ fj t − fi t ∆t 
where e is the set of edges to find a closed form solution 
to eq we express it in a matrix form f t ∆t − 
f t ∆t γhf t where d v denotes the degree of the 
node v in the limit ∆t → it becomes d 
dt 
f t γhf t 
solving it we obtain f t eγth 
f especially we have 
f eγh 
f hij 
 
 
 
−d vj j i 
 vj vi ∈ e 
 otherwise 
 
where eγh 
is defined as eγh 
 i γh γ 
 
h 
 γ 
 
h 
 · · · 
 on a directed graph 
the above heat diffusion model must be modified to fit the 
situation where the links between web pages are directed 
on one web page when the page-maker creates a link a b 
to another page b he actually forces the energy flow for 
example people s click-through activities to that page and 
so there is added energy imposed on the link as a result 
heat flows in a one-way manner only from a to b not from 
b to a based on such consideration we modified the heat 
diffusion model on an undirected graph as follows 
on a directed graph g the pipe vi vj is forced by added 
energy such that heat flows only from vi to vj suppose at 
time t each node vi receives rh rh i j t ∆t amount of 
heat from vj during a period of ∆t we have three 
assumptions rh should be proportional to the time period ∆t 
 rh should be proportional to the the heat at node vj 
and rh is zero if there is no link from vj to vi as a 
result vi will receive j vj vi ∈e σjfj t ∆t amount of heat 
from all its neighbors that points to it 
on the other hand node vi diffuses dh i t ∆t amount 
of heat to its subsequent nodes we assume that the 
heat dh i t ∆t should be proportional to the time period 
∆t the heat dh i t ∆t should be proportional to the 
the heat at node vi each node has the same ability of 
diffusing heat this fits the intuition that a web surfer only 
has one choice to find the next page that he wants to browse 
 the heat dh i t ∆t should be uniformly distributed 
to its subsequent nodes the real situation is more complex 
than what we assume but we have to make this simple 
assumption in order to make our model concise as a result 
node vi will diffuse γfi t ∆t di amount of heat to any of its 
subsequent nodes and each of its subsequent node should 
receive γfi t ∆t di amount of heat therefore σj γ dj 
to sum up the heat difference at node vi between time 
t ∆t and time t will be equal to the sum of the heat that it 
receives deducted by what it diffuses this is formulated as 
fi t ∆t − fi t −γfi t ∆t j vj vi ∈e γ djfj t ∆t 
similarly we obtain 
f eγh 
f hij 
 
 
 
− j i 
 dj vj vi ∈ e 
 otherwise 
 
 on a random directed graph 
for real world applications we have to consider random 
edges this can be seen in two viewpoints the first one 
is that in eq the web graph is actually modelled as 
a random graph there is an edge from node vi to node vj 
with a probability of − α gj see the item − α g t 
 
and that the web graph is predicted by a random graph 
 the second one is that the web structure is a 
random graph in essence if we consider the content similarity 
between two pages though this is not done in this paper 
for these reasons the model would become more flexible if 
we extend it to random graphs the definition of a random 
graph is given below 
definition a random graph rg v p pij is 
defined as a graph with a vertex set v in which the edges are 
chosen independently and for ≤ i j ≤ v the probability 
of vi vj being an edge is exactly pij 
the original definition of random graphs in is changed 
slightly to consider the situation of directed graphs note 
that every static graph can be considered as a special 
random graph in the sense that pij can only be or 
on a random graph rg v p where p pij is 
the probability of the edge vi vj exists in such a random 
graph the expected heat difference at node i between time 
t ∆t and time t will be equal to the sum of the expected 
heat that it receives from all its antecedents deducted by 
the expected heat that it diffuses 
since the probability of the link vj vi is pji the 
expected heat flow from node j to node i should be multiplied 
by pji and so we have fi t ∆t − fi t −γ fi t ∆t 
j vj vi ∈e γpjifj t ∆t rd 
 vj where rd 
 vi is the 
expected out-degree of node vi it is defined as k pik 
similarly we have 
f eγr 
f rij 
 
 
 
− j i 
pji rd 
 vj j i 
 
when the graph is large a direct computation of eγr 
is 
time-consuming and we adopt its discrete approximation 
f i 
γ 
n 
r n 
f 
the matrix i γ 
n 
r n 
in eq and matrix eγr 
in eq 
are called discrete diffusion kernel and the continuous 
diffusion kernel respectively based on the heat diffusion 
models and their solutions diffusionrank can be 
established on undirected graphs directed graphs and random 
graphs in the next section we mainly focus on 
diffusionrank in the random graph setting 
 diffusionrank 
for a random graph the matrix i γ 
n 
r n 
or eγr 
can 
measure the similarity relationship between nodes let fi 
 fj if j i then the vector f represent the unit 
heat at node vi while all other nodes has zero heat for such 
f in a random graph we can find the heat distribution 
at time by using eq or eq the heat 
distribution is exactly the i−th row of the matrix of i γ 
n 
r n 
or 
eγr 
 so the ith-row jth-column element hij in the matrix 
 i γ∆tr n 
or eγr 
means the amount of heat that vi can 
receive from vj from time to thus the value hij can be 
used to measure the similarity from vj to vi for a static 
graph similarly the matrix i γ 
n 
h n 
or eγh 
can measure 
the similarity relationship between nodes 
the intuition behind is that the amount h i j of heat 
that a page vi receives from a unit heat in a page vj in a 
unit time embodies the extent of the link connections from 
page vj to page vi roughly speaking when there are more 
uncrossed paths from vj to vi vi will receive more heat from 
vj when the path length from vj to vi is shorter vi will 
receive more heat from vj and when the pipe connecting 
vj and vi is wide the heat will flow quickly the final heat 
that vi receives will depend on various paths from vj to vi 
their length and the width of the pipes 
algorithm diffusionrank function 
input the transition matrix a the inverse transition 
matrix u the decay factor αi for the inverse pagerank the 
decay factor αb for pagerank number of iterations mi for 
the inverse pagerank the number of trusted pages l the 
thermal conductivity coefficient γ 
output diffusionrank score vector h 
 s 
 for i to mi do 
 s αi · u · s − αi · 
n 
· 
 end for 
 sort s in a decreasing order π rank { n} s 
 d count i 
 while count ≤ l do 
 if π i is evaluated as a trusted page then 
 d π i count 
 end if 
 i 
 end while 
 d d d 
 h d 
 find the iteration number mb according to λ 
 for i to mb do 
 h − γ 
mb 
 h γ 
mb 
 αb · a · h − αb · 
n 
· 
 end for 
 return h 
 algorithm 
for the ranking task we adopt the heat kernel on a 
random graph formally the diffusionrank is described in 
algorithm in which the element uij in the inverse transition 
matrix u is defined to be ij if there is a link from i to j 
and otherwise this trusted pages selection procedure by 
inverse pagerank is completely borrowed from trustrank 
 except for a fix number of the size of the trusted set 
although the inverse pagerank is not perfect in its 
ability of determining the maximum coverage it is appealing 
because of its polynomial execution time and its 
reasonable intuition-we actually inverse the original link when 
we try to build the seed set from those pages that point 
to many pages that in turn point to many pages and so 
on in the algorithm the underlying random graph is set as 
p αb · a − αb · 
n 
· n×n which is induced by the 
web graph as a result r −i p 
in fact the more general setting for diffusionrank is p 
αb ·a −αb · 
n 
·g· t 
 by such a setting diffusionrank 
is a generalization of trustrank when γ tends to infinity 
and when g is set in the same way as trustrank however 
the second part of trustrank is not adopted by us in our 
model g should be the true teleportation determined by 
the user s browse habits popularity distribution over all the 
web pages and so on p should be the true model of the 
random nature of the world wide web setting g according 
to the trusted pages will not be consistent with the basic idea 
of heat diffusion on a random graph we simply set g 
only because we cannot find it without any priori knowledge 
remark in a social network interpretation 
diffusionrank first recognizes a group of trusted people who may 
not be highly ranked but they know many other people 
the initially trusted people are endowed with the power to 
decide who can be further trusted but cannot decide the 
final voting results and so they are not dictators 
 advantages 
next we show the four advantages for diffusionrank 
 two closed forms 
first its solutions have two forms both of which are 
closed form one takes the discrete form and has the 
advantage of fast computing while the other takes the continuous 
form and has the advantage of being easily analyzed in 
theoretical aspects the theoretical advantage has been shown 
in the proof of theorem in the next section 
 a group to group relations b an undirected graph 
figure two graphs 
 group-group relations 
second it can be naturally employed to detect the 
groupgroup relation for example let g and g denote two 
groups containing pages j j js and i i it 
respectively then u v hiu jv is the total amounts of heat 
that g receives from g where hiu jv is the iu−th row 
jv−th column element of the heat kernel more specifically 
we need to first set f for such an application as follows 
in f f f fn t 
 if i ∈ {j j js} 
then fi and otherwise then we employ eq 
to calculate f f f fn t 
 finally we sum 
those fj where j ∈ {i i it} fig a shows the 
results generated by the diffusionrank we consider five 
groups-five departments in our engineering faculty cse 
mae ee ie and se γ is set to be the numbers in 
fig a are the amount of heat that they diffuse to each 
other these results are normalized by the total number of 
each group and the edges are ignored if the values are less 
than the group-to-group relations are therefore 
detected for example we can see that the most strong 
overall tie is from ee to ie while it is a natural application 
for diffusionrank because of the easy interpretation by the 
amount heat from one group to another group it is difficult 
to apply other ranking techniques to such an application 
because they lack such a physical meaning 
 graph cut 
third it can be used to partition the web graph into 
several parts a quick example is shown below the graph 
in fig b is an undirected graph and so we employ the 
eq if we know that node belongs to one 
community and that node belongs to another community then 
we can put one unit positive heat source on node and 
one unit negative heat source on node after time if 
we set γ the heat distribution is 
 - - - - - and if 
we set γ it will be 
 - - - - - in both settings we 
can easily divide the graph into two parts { } 
with positive temperatures and { } with 
negative temperatures for directed graphs and random graphs 
similarly we can cut them by employing corresponding heat 
solution 
 anti-manipulation 
fourth it can be used to combat manipulation let g 
contain trusted web pages j j js then for each page 
i v hi jv is the heat that page i receives from g and can 
be computed by the discrete approximation of eq in 
the case of a static graph or eq in the case of a random 
graph in which f is set to be a special initial heat 
distribution so that the trusted web pages have unit heat while 
all the others have zero heat in doing so manipulated web 
page will get a lower rank unless it has strong in-links from 
the trusted web pages directly or indirectly the situation 
is quite different for pagerank because pagerank is 
inputindependent as we have shown in section based on the 
fact that the connection from a trusted page to a bad page 
should be weak-less uncross paths longer distance and 
narrower pipe we can say diffusionrank can resist web spam if 
we can select trusted pages it is fortunate that the trusted 
pages selection method in -the first part of trustrank can 
help us to fulfill this task for such an application of 
diffusionrank the computation complexity for discrete 
diffusion kernel is the same as that for pagerank in cases of 
both a static graph and a random graph this can be seen 
in eq by which we need n iterations and for each 
iteration we need a multiplication operation between a matrix 
and a vector while in eq we also need a multiplication 
operation between a matrix and a vector for each iteration 
 the physical meaning of γ 
γ plays an important role in the anti-manipulation effect 
of diffusionrank γ is the thermal conductivity-the heat 
diffusion coefficient if it has a high value heat will 
diffuse very quickly conversely if it is small heat will diffuse 
slowly in the extreme case if it is infinitely large then heat 
will diffuse from one node to other nodes immediately and 
this is exactly the case corresponding to pagerank next 
we will interpret it mathematically 
theorem when γ tends to infinity and f is not the 
zero vector eγr 
f is proportional to the stable distribution 
produced by pagerank 
let g 
n 
 by the perron theorem we have shown 
that is the largest eigenvalue of p − α g t 
 αa 
and that no other eigenvalue whose absolute value is equal 
to let x be the stable distribution and so px x x is 
the eigenvector corresponding to the eigenvalue assume 
the n − other eigenvalues of p are λ λn 
we can find an invertible matrix s x s such that 
s− 
ps 
 
 
 
 
 
 ∗ ∗ ∗ 
 λ ∗ ∗ 
 
 ∗ 
 λn 
 
 
 
 
 
 
since eγr 
 eγ −i p 
 
s− 
 
 
 
 
 
 ∗ ∗ ∗ 
 eγ λ − 
∗ ∗ 
 
 ∗ 
 eγ λn− 
 
 
 
 
 
s 
all eigenvalues of the matrix eγr 
are eγ λ − 
 eγ λn− 
 
when γ → ∞ they become which means that is 
the only nonzero eigenvalue of eγr 
when γ → ∞ we can see 
that when γ → ∞ eγr 
eγr 
f eγr 
f and so eγr 
f 
is an eigenvector of eγr 
when γ → ∞ on the other hand 
eγr 
x i γr γ 
 
r 
 γ 
 
r 
 x ix γrx γ 
 
r 
x 
γ 
 
r 
x x since rx −i p x −x x 
and hence x is the eigenvector of eγr 
for any γ therefore 
both x and eγr 
f are the eigenvectors corresponding the 
unique eigenvalue of eγr 
when γ → ∞ and consequently 
x ceγr 
f 
by this theorem we see that diffusionrank is a 
generalization of pagerank when γ the ranking value is 
most robust to manipulation since no heat is diffused and 
the system is unchangeable but the web structure is 
completely ignored since eγr 
f e r 
f if f 
when γ ∞ diffusionrank becomes pagerank it can be 
manipulated easily we expect an appropriate setting of 
γ that can balance both for this we have no theoretical 
result but in practice we find that γ works well in 
section next we discuss how to determine the number of 
iterations if we employ the discrete heat kernel 
 the number of iterations 
while we enjoy the advantage of the concise form of the 
exponential heat kernel it is better for us to calculate 
diffusionrank by employing eq in an iterative way then 
the problem about determining n-the number of iterations 
arises 
for a given threshold find n such that i γ 
n 
r n 
− 
eγr 
 f for any f whose sum is one 
since it is difficult to solve this problem we propose a 
heuristic motivated by the following observations when 
r −i p by eq we have i γ 
n 
r n 
 i γ 
n 
 −i 
p n 
 
s− 
 
 
 
 
 
 ∗ ∗ ∗ 
 γ λ − 
n 
 n 
∗ ∗ 
 
 ∗ 
 γ λn− 
n 
 n 
 
 
 
 
 
s 
comparing eq and eq we observe that the 
eigenvalues of i γ 
n 
r n 
− eγr 
are γ λn− 
n 
 n 
− eγ λn− 
 
we propose a heuristic method to determine n so that the 
difference between the eigenvalues are less than a threshold 
for only positive λs 
we also observe that if γ λ then γ λ− 
n 
 n 
− 
eγ λ− 
 if n ≥ and γ λ− 
n 
 n 
−eγ λ− 
 
 if n ≥ so we can set n or n or others 
according to different accuracy requirements in this paper 
we use the relatively accurate setting n to make the 
real eigenvalues in i γ 
n 
r n 
− eγr 
less than 
 experiments 
in this section we show the experimental data the 
methodology the setting and the results 
 data preparation 
our input data consist of a toy graph a middle-size 
realworld graph and a large-size real-world graph the toy 
graph is shown in fig a the graph below it shows node 
 is being manipulated by adding new nodes a b c 
such that they all point to node and node points to 
them all the data of two real web graph were obtained 
from the domain in our institute in october the 
total number of pages found are in the middle-size 
graph and in the large-size graph respectively the 
middle-size graph is a subgraph of the large-size graph and 
they were obtained by the same crawler one is recorded 
by the crawler in its earlier time and the other is obtained 
when the crawler stopped 
 methodology 
the algorithms we run include pagerank trustrank and 
diffusionrank all the rank values are multiplied by the 
number of nodes so that the sum of the rank values is equal 
to the number of nodes by this normalization we can 
compare the results on graphs with different sizes since the 
average rank value is one for any graph after such normalization 
we will need value difference and pairwise order difference as 
comparison measures their definitions are listed as follows 
value difference the value difference between a 
{ai}n 
i and b {bi}n 
i is measured as n 
i ai − bi 
pairwise order difference the order difference between 
a and b is measured as the number of significant order 
differences between a and b the pair a i a j and 
 b i b j is considered as a significant order difference if 
one of the following cases happens both a i a j 
and b i ≤ ≥ a j both a i ≤ ≥ a j and b i 
 a j 
a 
 
b 
c 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
gamma 
valuedifference 
trust set { } 
trust set { } 
trust set { } 
trust set { } 
trust set { } 
trust set { } 
 a b 
figure a the toy graph consisting of six nodes 
and node is being manipulated by adding new 
nodes a b c b the approximation tendency to 
pagerank by diffusionrank 
 experimental set-up 
the experiments on the middle-size graph and the 
largesize graphs are conducted on the workstation whose 
hardware model is nix dual intel xeon ghz with gb ram 
and a linux kernel - smp redhat in 
calculating diffusionrank we employ eq and the discrete 
approximation of eq for such graphs the related tasks 
are implemented using c language while in the toy graph 
we employ the continuous diffusion kernel in eq and 
eq and implement related tasks using matlab 
for nodes that have zero out-degree dangling nodes we 
employ the method in the modified pagerank algorithm 
in which dangling nodes of are considered to have random 
links uniformly to each node we set α αi αb in 
all algorithms we also set g to be the uniform distribution 
in both pagerank and diffusionrank for diffusionrank 
we set γ according to the discussions in section and 
section we set the iteration number to be mb in 
diffusionrank and for accuracy consideration the iteration 
number in all the algorithms is set to be 
 approximation of pagerank 
we show that when γ tends to infinity the value 
differences between diffusionrank and pagerank tend to zero 
fig b shows the approximation property of 
diffusionrank as proved in theorem on the toy graph the 
horizontal axis of fig b marks the γ value and vertical axis 
corresponds to the value difference between diffusionrank 
and pagerank all the possible trusted sets with l 
are considered for l the results should be the linear 
combination of some of these curves because of the 
linearity of the solutions to heat equations on other graphs the 
situations are similar 
 results of anti-manipulation 
in this section we show how the rank values change as the 
intensity of manipulation increases we measure the 
intensity of manipulation by the number of newly added points 
that point to the manipulated point the horizontal axes 
of fig stand for the numbers of newly added points and 
vertical axes show the corresponding rank values of the 
manipulated nodes to be clear we consider all six situations 
every node in fig a is manipulated respectively and its 
 
 
 
 
 
 
 
rankofthemanipulatdnode− 
diffusionrank−trust 
pagerank 
trustranl−trust 
 
 
 
 
 
 
 
rankofthemanipulatdnode− 
diffusionrank−trust 
pagerank 
trustranl−trust 
 
 
 
 
 
 
 
rankofthemanipulatdnode− 
diffusionrank−trust 
pagerank 
trustranl−trust 
 
 
 
 
 
 
 
number of new added nodes 
rankofthemanipulatdnode− 
diffusionrank−trust 
pagerank 
trustranl−trust 
 
 
 
 
 
 
 
number of new added nodes 
rankofthemanipulatdnode− 
diffusionrank−trust 
pagerank 
trustranl−trust 
 
 
 
 
 
 
 
number of new added nodes 
rankofthemanipulatdnode− 
diffusionrank−trust 
pagerank 
trustranl−trust 
figure the rank values of the manipulated nodes 
on the toy graph 
 
 
 
 
 
 
 
 
 
 
number of new added points 
rankofthemanipulatdnode 
pagerank 
diffusionrank−uniform 
diffusionrank 
diffusionrank 
diffusionrank 
diffusionrank 
trustrank 
trustrank 
trustrank 
trustrank 
 
 
 
 
 
 
 
 
 
 
 
number of new added points 
rankofthemanipulatdnode 
pagerank 
diffusionrank 
trustrank 
diffusionrank−uniform 
 a b 
figure a the rank values of the manipulated 
nodes on the middle-size graph b the rank values 
of the manipulated nodes on the large-size graph 
corresponding values for pagerank trustrank tr 
diffusionrank dr are shown in the one of six sub-figures in 
fig the vertical axes show which node is being 
manipulated in each sub-figure the trusted sets are 
computed below since the inverse pagerank yields the results 
 let l if the 
manipulated node is not then the trusted set is { } and 
otherwise { } we observe that in all the cases rank values 
of the manipulated node for diffusionrank grow slowest as 
the number of the newly added nodes increases on the 
middle-size graph and the large-size graph this conclusion 
is also true see fig note that in fig a we choose 
four trusted sets l on which we test diffusionrank 
and trustrank the results are denoted by diffusionranki 
and trustranki i denotes the four trusted set 
in fig b we choose one trusted set l moreover 
in both fig a and fig b we show the results for 
diffusionrank when we have no trusted set and we trust 
all the pages before some of them are manipulated 
we also test the order difference between the ranking 
order a before the page is manipulated and the ranking order 
pa after the page is manipulated because after 
manipulation the number of pages changes we only compare the 
common part of a and pa this experiment is used to test 
the stability of all these algorithms the less the order 
difference the stabler the algorithm in the sense that only a 
smaller part of the order relations is affected by the 
manipulation figure a shows that the order difference values 
change when we add new nodes that point to the 
manipulated node we give several γ settings we find that when 
γ the least order difference is achieved by 
diffusionrank it is interesting to point out that as γ increases the 
order difference will increase first after reaching a maximum 
value it will decrease and finally it tends to the pagerank 
results we show this tendency in fig b in which we 
choose three different settings-the number of manipulated 
nodes are and respectively from these 
figures we can see that when γ the values are less than 
those for pagerank and that when γ the difference 
between pagerank and diffusionrank is very small 
after these investigations we find that in all the graphs we 
tested diffusionrank when γ is most robust to 
manipulation both in value difference and order difference the 
trust set selection algorithm proposed in is effective for 
both trustrank and diffusionrank 
 
 
 
 
 
 
 
 
x 
 
number of new added points 
pairwiseorderdifference 
pagerank 
diffusionrank−gamma 
diffusionrank−gamma 
diffusionrank−gamma 
diffusionrank−gamma 
diffusionrank−gamma 
diffusionrank−gamma 
trustrank 
 
 
 
 
 
 
 
x 
 
gamma 
pairwiseorderdifference 
diffusionrank when added nodes 
diffusionrank when added nodes 
diffusionrank when added nodes 
pagerank 
 a b 
figure a pairwise order difference on the 
middle-size graph the least it is the more stable 
the algorithm b the tendency of varying γ 
 conclusions 
we conclude that diffusionrank is a generalization of 
pagerank which is interesting in that the heat diffusion 
coefficient γ can balance the extent that we want to model the 
original web graph and the extent that we want to reduce 
the effect of link manipulations the experimental results 
show that we can actually achieve such a balance by 
setting γ although the best setting including varying γi 
is still under further investigation this anti-manipulation 
feature enables diffusionrank to be a candidate as a 
penicillin for web spamming moreover diffusionrank can be 
employed to find group-group relations and to partition web 
graph into small communities all these advantages can be 
achieved in the same computational complexity as 
pagerank for the special application of anti-manipulation 
diffusionrank performs best both in reduction effects and in 
its stability among all the three algorithms 
 acknowledgments 
we thank patrick lau zhenjiang lin and zenglin xu 
for their help this work is fully supported by two grants 
from the research grants council of the hong kong special 
administrative region china project no cuhk e 
and project no cuhk e 
 references 
 e agichtein e brill and s t dumais improving web search 
ranking by incorporating user behavior information in e n 
efthimiadis s t dumais d hawking and k j¨arvelin 
editors proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval sigir pages - 
 r a baeza-yates p boldi and c castillo generalizing 
pagerank damping functions for link-based ranking 
algorithms in e n efthimiadis s t dumais d hawking 
and k j¨arvelin editors proceedings of the th annual 
international acm sigir conference on research and 
development in information retrieval sigir pages 
 - 
 m belkin and p niyogi laplacian eigenmaps for 
dimensionality reduction and data representation neural 
computation - jun 
 b bollob´as random graphs academic press inc london 
 
 c burges t shaked e renshaw a lazier m deeds 
n hamilton and g hullender learning to rank using 
gradient descent in proceedings of the nd international 
conference on machine learning icml pages - 
 n eiron k s mccurley and j a tomlin ranking the web 
frontier in proceeding of the th world wide web 
conference www pages - 
 z gy¨ongyi h garcia-molina and j pedersen combating 
web spam with trustrank in m a nascimento m t ¨ozsu 
d kossmann r j miller j a blakeley and k b schiefer 
editors proceedings of the thirtieth international conference 
on very large data bases vldb pages - 
 s d kamvar t h haveliwala c d manning and g h 
golub exploiting the block structure of the web for computing 
pagerank technical report stanford university 
 r i kondor and j d lafferty diffusion kernels on graphs 
and other discrete input spaces in c sammut and a g 
hoffmann editors proceedings of the nineteenth 
international conference on machine learning icml 
pages - 
 j lafferty and g lebanon diffusion kernels on statistical 
manifolds journal of machine learning research - 
jan 
 c r maccluer the many proofs and applications of perron s 
theorem siam review - 
 a ntoulas m najork m manasse and d fetterly detecting 
spam web pages through content analysis in proceedings of 
the th international conference on world wide web 
 www pages - 
 l page s brin r motwani and t winograd the pagerank 
citation ranking bringing order to the web technical report 
paper sidl-wp- - version of stanford 
digital library technologies project 
 h yang i king and m r lyu nhdc and phdc 
non-propagating and propagating heat diffusion classifiers in 
proceedings of the th international conference on neural 
information processing iconip pages - 
 h yang i king and m r lyu predictive ranking a novel 
page ranking approach by estimating the web structure in 
proceedings of the th international conference on world 
wide web www - special interest tracks and posters 
pages - 
 h yang i king and m r lyu predictive random graph 
ranking on the web in proceedings of the ieee world 
congress on computational intelligence wcci pages 
 - 
 d zhou j weston a gretton o bousquet and 
b sch¨olkopf ranking on data manifolds in s thrun l saul 
and b sch¨olkopf editors advances in neural information 
processing systems nips 
location based indexing scheme for days 
debopam acharya and vijay kumar 
 
computer science and informatics 
university of missouri-kansas city 
kansas city mo 
dargc kumarv  umkc edu 
abstract 
data dissemination through wireless channels for broadcasting 
information to consumers is becoming quite common many 
dissemination schemes have been proposed but most of them push 
data to wireless channels for general consumption push based 
broadcast is essentially asymmetric i e the volume of data 
being higher from the server to the users than from the users back 
to the server push based scheme requires some indexing which 
indicates when the data will be broadcast and its position in the 
broadcast access latency and tuning time are the two main 
parameters which may be used to evaluate an indexing scheme 
two of the important indexing schemes proposed earlier were tree 
based and the exponential indexing schemes none of these 
schemes were able to address the requirements of location 
dependent data ldd which is highly desirable feature of data 
dissemination in this paper we discuss the broadcast of ldd in 
our project data in your space days and propose a scheme 
for indexing ldd we argue that this scheme when applied to 
ldd significantly improves performance in terms of tuning time 
over the above mentioned schemes we prove our argument with 
the help of simulation results 
categories and subject descriptors 
h information systems information storage and retrieval 
- content analysis and indexing h information systems 
information storage and retrieval - information search and 
retrieval 
general terms 
algorithms performance experimentation 
 introduction 
wireless data dissemination is an economical and efficient 
way to make desired data available to a large number of mobile or 
static users the mode of data transfer is essentially asymmetric 
that is the capacity of the transfer of data downstream 
communication from the server to the client mobile user is 
significantly larger than the client or mobile user to the server 
 upstream communication the effectiveness of a data 
dissemination system is judged by its ability to provide user the 
required data at anywhere and at anytime one of the best ways to 
accomplish this is through the dissemination of highly 
personalized location based services lbs which allows users 
to access personalized location dependent data an example 
would be someone using their mobile device to search for a 
vegetarian restaurant the lbs application would interact with 
other location technology components or use the mobile user s 
input to determine the user s location and download the 
information about the restaurants in proximity to the user by 
tuning into the wireless channel which is disseminating ldd 
we see a limited deployment of lbs by some service 
providers but there are every indications that with time some of 
the complex technical problems such as uniform location 
framework calculating and tracking locations in all types of 
places positioning in various environments innovative location 
applications etc will be resolved and lbs will become a 
common facility and will help to improve market productivity and 
customer comfort in our project called days we use wireless 
data broadcast mechanism to push ldd to users and mobile users 
monitor and tune the channel to find and download the required 
data a simple broadcast however is likely to cause significant 
performance degradation in the energy constrained mobile devices 
and a common solution to this problem is the use of efficient air 
indexing the indexing approach stores control information which 
tells the user about the data location in the broadcast and how and 
when he could access it a mobile user thus has some free time 
to go into the doze mode which conserves valuable power it also 
allows the user to personalize his own mobile device by 
selectively tuning to the information of his choice 
access efficiency and energy conservation are the two issues 
which are significant for data broadcast systems access efficiency 
refers to the latency experienced when a request is initiated till the 
response is received energy conservation refers to the 
efficient use of the limited energy of the mobile device in 
accessing broadcast data two parameters that affect these are the 
tuning time and the access latency tuning time refers to the time 
during which the mobile unit mu remains in active state to tune 
the channel and download its required data it can also be defined 
as the number of buckets tuned by the mobile device in active 
state to get its required data access latency may be defined as the 
time elapsed since a request has been issued till the response has 
been received 
 
this research was supported by a grant from nsf iis- 
several indexing schemes have been proposed in the past and 
the prominent among them are the tree based and the exponential 
indexing schemes the main disadvantages of the tree based 
schemes are that they are based on centralized tree structures to 
start a search the mu has to wait until it reaches the root of the 
next broadcast tree this significantly affects the tuning time of 
the mobile unit the exponential schemes facilitate index 
replication by sharing links in different search trees for 
broadcasts with large number of pages the exponential scheme 
has been shown to perform similarly as the tree based schemes in 
terms of access latency also the average length of broadcast 
increases due to the index replication and this may cause 
significant increase in the access latency none of the above 
indexing schemes is equally effective in broadcasting location 
dependent data in addition to providing low latency they lack 
properties which are used to address ldd issues we propose an 
indexing scheme in days which takes care of some these 
problems we show with simulation results that our scheme 
outperforms some of the earlier indexing schemes for 
broadcasting ldd in terms of tuning time 
the rest of the paper is presented as follows in section we 
discuss previous work related to indexing of broadcast data 
section describes our days architecture location dependent 
data its generation and subsequent broadcast is presented in 
section section discusses our indexing scheme in detail 
simulation of our scheme and its performance evaluation is 
presented in section section concludes the paper and 
mentions future related work 
 previous work 
several disk-based indexing techniques have been used for air 
indexing imielinski et al applied the b index tree where 
the leaf nodes store the arrival times of the data items the 
distributed indexing method was proposed to efficiently replicate 
and distribute the index tree in a broadcast specifically the index 
tree is divided into a replicated part and a non replicated part 
each broadcast consists of the replicated part and the 
nonreplicated part that indexes the data items immediately following 
it as such each node in the non-replicated part appears only once 
in a broadcast and hence reduces the replication cost and access 
latency while achieving a good tuning time chen et al and 
shivakumar et al considered unbalanced tree structures to 
optimize energy consumption for non-uniform data access these 
structures minimize the average index search cost by reducing the 
number of index searches for hot data at the expense of spending 
more on cold data tan and yu discussed data and index 
organization under skewed broadcast hashing and signature 
methods have also been suggested for wireless broadcast that 
supports equality queries a flexible indexing method was 
proposed in the flexible index first sorts the data items in 
ascending or descending order of the search key values and then 
divides them into p segments the first bucket in each data 
segment contains a control index which is a binary index 
mapping a given key value to the segment containing that key 
and a local index which is an m-entry index mapping a given key 
value to the buckets within the current segment by tuning the 
parameters of p and m mobile clients can achieve either a good 
tuning time or good access latency another indexing technique 
proposed is the exponential indexing scheme in this scheme 
a parameterized index called the exponential index is used to 
optimize the access latency or the tuning time it facilitates index 
replication by linking different search trees all of the above 
mentioned schemes have been applied to data which are non 
related to each other these non related data may be clustered or 
non clustered however none of them has specifically addressed 
the requirements of ldd location dependent data are data which 
are associated with a location presently there are several 
applications that deal with ldd almost all of them 
depict ldd with the help of hierarchical structures this is 
based on the containment property of location dependent data 
the containment property helps determining relative position of 
an object by defining or identifying locations that contains those 
objects the subordinate locations are hierarchically related to 
each other thus containment property limits the range of 
availability or operation of a service we use this containment 
property in our indexing scheme to index ldd 
 days architecture 
days has been conceptualized to disseminate topical and 
nontopical data to users in a local broadcast space and to accept 
queries from individual users globally topical data for example 
weather information traffic information stock information etc 
constantly changes over time non topical data such as hotel 
restaurant real estate prices etc do not change so often thus 
we envision the presence of two types of data distribution in the 
first case server pushes data to local users through wireless 
channels the other case deals with the server sending results of 
user queries through downlink wireless channels technically we 
see the presence of two types of queues in the pull based data 
access one is a heavily loaded queue containing globally 
uploaded queries the other is a comparatively lightly loaded 
queue consisting of locally uploaded queries the days 
architecture as shown in figure consists of a data server 
broadcast scheduler days coordinator network of leo 
satellites for global data delivery and a local broadcast space 
data is pushed into the local broadcast space so that users may 
tune into the wireless channels to access the data the local 
broadcast space consists of a broadcast tower mobile units and a 
network of data staging machines called the surrogates data 
staging in surrogates has been earlier investigated as a successful 
technique to cache users related data we believe that 
data staging can be used to drastically reduce the latency time for 
both the local broadcast data as well as global responses query 
request in the surrogates may subsequently be used to generate the 
popularity patterns which ultimately decide the broadcast 
schedule 
 
popularity 
feedback from 
surrogates for 
broadcast 
scheduler 
local broadcast space 
broadcast tower 
surrogatemu 
mu 
mu 
mu 
data serverbroadcast schedulerdays coordinator 
local downlink 
channel 
global downlink 
channel 
pull request queue 
global request queue 
local request queue location based index 
starbucks 
plaza 
kansas 
city 
figure days architecture figure location structure ofstarbucks plaza 
 location dependent data ldd 
we argue that incorporating location information in wireless data 
broadcast can significantly decrease the access latency this 
property becomes highly useful for mobile unit which has limited 
storage and processing capability there are a variety of 
applications to obtain information about traffic restaurant and 
hotel booking fast food gas stations post office grocery stores 
etc if these applications are coupled with location information 
then the search will be fast and highly cost effective an important 
property of the locations is containment which helps to determine 
the relative location of an object with respect to its parent that 
contains the object thus containment limits the range of 
availability of a data we use this property in our indexing 
scheme the database contains the broadcast contents which are 
converted into ldd by associating them with respective 
locations so that it can be broadcasted in a clustered manner the 
clustering of ldd helps the user to locate information efficiently 
and supports containment property we present an example to 
justify our proposition 
example suppose a user issues query starbucks coffee in 
plaza please to access information about the plaza branch of 
starbucks coffee in kansas city in the case of location 
independent set up the system will list all starbucks coffee shops 
in kansas city area it is obvious that such responses will 
increase access latency and are not desirable these can be 
managed efficiently if the server has location dependent data i e 
a mapping between a starbucks coffee shop data and its physical 
location also for a query including range of locations of 
starbucks a single query requesting locations for the entire 
region of kansas city as shown in figure will suffice this 
will save enormous amount of bandwidth by decreasing the 
number of messages and at the same time will be helpful in 
preventing the scalability bottleneck in highly populated area 
 mapping function for ldd 
the example justifies the need for a mapping function to process 
location dependent queries this will be especially important for 
pull based queries across the globe for which the reply could be 
composed for different parts of the world the mapping function 
is necessary to construct the broadcast schedule 
we define global property set gps information content 
 ic set and location hierarchy lh set where ic ⊆ gps and 
lh ⊆ gps to develop a mapping function lh {l l l   lk} 
where li represent locations in the location tree and ic {ic ic 
ic   icn} where ici represent information type for example if 
we have traffic weather and stock information are in broadcast 
then ic {ictraffic icweather and icstock} the mapping scheme must 
be able to identify and select an ic member and a lh node for a 
correct association b granularity match c and termination 
condition for example weather ∈ ic could be associated with a 
country or a state or a city or a town of lh the granularity match 
between the weather and a lh node is as per user requirement 
thus with a coarse granularity weather information is associated 
with a country to get country s weather and with town in a finer 
granularity if a town is the finest granularity then it defines the 
terminal condition for association between ic and lh for weather 
this means that a user cannot get weather information about 
subdivision of a town in reality weather of a subdivision does 
not make any sense 
we develop a simple heuristic mapping approach scheme based 
on user requirement let ic {m m m mk} where mi 
represent its element and let lh {n n n nl} where ni 
represents lh s member we define gps for ic gpsic ⊆ gps 
and for lh gpslh ⊆ gps as gpsic {p p   pn} where 
p p p   pn are properties of its members and gpslh {q 
q   qm} where q q   qm are properties of its members 
the properties of a particular member of ic are a subset of 
gpsic it is generally true that property set mi∈ ic ∪ property 
set mj∈ ic ≠ ∅ however there may be cases where the 
intersection is not null for example stock ∈ ic and movie ∈ ic 
rating do not have any property in common we assume that any 
two or more members of ic have at least one common 
geographical property i e location because days broadcasts 
information about those categories which are closely tied with a 
location for example stock of a company is related to a country 
weather is related to a city or state etc 
we define the property subset of mi∈ ic as psm 
i 
∀ mi ∈ ic and 
psm 
i 
 {p p pr} where r ≤ n ∀ pr {pr ∈ psm 
i 
→ pr∈ 
gpsic} which implies that ∀ i psm 
i 
⊆ gpsic the geographical 
properties of this set are indicative of whether mi ∈ ic can be 
mapped to only a single granularity level i e a single location in 
lh or a multiple granularity levels i e more than one nodes in 
 
the hierarchy in lh how many and which granularity levels 
should a mi map to depends upon the level at which the service 
provider wants to provide information about the mi in question 
similarly we define a property subset of lh members as psn 
j 
∀ nj 
∈ lh which can be written as psn 
j 
 {q q q   qs} where s ≤ 
m in addition ∀ qs {qs∈ psn 
j 
→ qs∈ gpslh} which implies that 
∀j psn 
j 
⊆ gpslh 
the process of mapping from ic to lh is then identifying for 
some mx∈ ic one or more ny∈ lh such that psmx ∩ psnv ≠ φ 
this means that when mx maps to ny and all children of ny if mx 
can map to multiple granularity levels or mx maps only to ny if mx 
can map to a single granularity level 
we assume that new members can join and old member can leave 
ic or lh any time the deletion of members from the ic space is 
simple but addition of members to the ic space is more restrictive 
if we want to add a new member to the ic space then we first 
define a property set for the new member psmnew m {p p p 
  pt} and add it to the ic only if the condition ∀ pw {pw∈ 
pspnew m → pw∈ gpsic} is satisfied this scheme has an 
additional benefit of allowing the information service providers to 
have a control over what kind of information they wish to provide 
to the users we present the following example to illustrate the 
mapping concept 
ic {traffic stock restaurant weather important history 
dates road conditions} 
lh {country state city zip-code major-roads} 
gpsic {surface-mobility roads high low italian-food 
statename temp cityname seat-availability zip traffic-jams 
stock-price countryname majorroadname wars discoveries 
world} 
gpslh {country countrysize statename cityname zip 
majorroadname} 
ps icstock {stock-price countryname high low} 
ps ictraffic {surface-mobility roads high low traffic-jams 
cityname} 
ps icimportant dates in history {world wars discoveries} 
ps icroad conditions {precipitation statename cityname} 
ps icrestaurant {italian-food zip code} 
ps icweather {statename cityname precipitation 
temperature} 
ps lhcountry {countryname countrysize} 
ps lhstate {statename state size} 
ps lhcity {cityname city size} 
ps lhzipcode {zipcodenum } 
ps lhmajor roads {majorroadname} 
now only ps icstock ∩ pscountry ≠φ in addition ps icstock 
indicated that stock can map to only a single location country 
when we consider the member traffic of ic space only 
ps ictraffic ∩ pscity ≠ φ as ps ictraffic indicates that traffic can 
map to only a single location it maps only to city and none of its 
children now unlike stock mapping of traffic with major roads 
which is a child of city is meaningful however service providers 
have right to control the granularity levels at which they want to 
provide information about a member of ic space 
ps icroad conditions ∩ psstate ≠φ and ps icroad conditions ∩ pscity≠φ 
so road conditions maps to state as well as city as ps icroad 
conditions indicates that road conditions can map to multiple 
granularity levels road conditions will also map to zip code and 
major roads which are the children of state and city similarly 
restaurant maps only to zip code and weather maps to state 
city and their children major roads and zip code 
 location based indexing scheme 
this section discusses our location based indexing scheme 
 lbis the scheme is designed to conform to the ldd broadcast 
in our project days as discussed earlier we use the 
containment property of ldd in the indexing scheme this 
significantly limits the search of our required data to a particular 
portion of broadcast thus we argue that the scheme provides 
bounded tuning time 
we describe the architecture of our indexing scheme our scheme 
contains separate data buckets and index buckets the index 
buckets are of two types the first type is called the major index 
the major index provides information about the types of data 
broadcasted for example if we intend to broadcast information 
like entertainment weather traffic etc then the major index 
points to either these major types of information and or their main 
subtypes of information the number of main subtypes varying 
from one information to another this strictly limits number of 
accesses to a major index the major index never points to the 
original data it points to the sub indexes called the minor index 
the minor indexes are the indexes which actually points to the 
original data we called these minor index pointers as location 
pointers as they points to the data which are associated with a 
location thus our search for a data includes accessing of a major 
index and some minor indexes the number of minor index 
varying depending on the type of information 
thus our indexing scheme takes into account the hierarchical 
nature of the ldd the containment property and requires our 
broadcast schedule to be clustered based on data type and 
location the structure of the location hierarchy requires the use 
of different types of index at different levels the structure and 
positions of index strictly depend on the location hierarchy as 
described in our mapping scheme earlier we illustrate the 
implementation of our scheme with an example the rules for 
framing the index are mentioned subsequently 
 
a 
entertainment 
resturant 
movie 
a 
a 
a 
r 
r 
r 
r 
r 
r 
r 
r 
weather 
kc 
sl 
jc 
sf 
entertainment 
r r r r r r r r kc sl jc sf 
 a r next 
 r 
 r 
type s l 
er 
w 
e 
em 
 
 
 
 
type s l 
w 
e 
em 
er 
 
 
 
 
type s l 
e 
em 
er 
w 
 
 
 
 
a a a a 
movie resturant weather 
 
major index major index major index 
minor index 
major index minor index 
figure location mapped information for broadcast figure data coupled with location based index 
example let us suppose that our broadcast content contains 
icentertainment and icweather which is represented as shown in fig 
ai represents areas of city and ri represents roads in a certain 
area the leaves of weather structure represent four cities the 
index structure is given in fig which shows the position of 
major and minor index and data in the broadcast schedule 
we propose the following rules for the creation of the air indexed 
broadcast schedule 
 the major index and the minor index are created 
 the major index contains the position and range of different 
types of data items weather and entertainment figure 
and their categories the sub categories of entertainment 
movie and restaurant are also in the index thus the major 
index contains entertainment e entertainment-movie 
 em entertainment-restaurant er and weather w the 
tuple s l represents the starting position s of the data 
item and l represents the range of the item in terms of 
number of data buckets 
 the minor index contains the variables a r and a pointer 
next in our example figure road r represents the first 
node of area a the minor index is used to point to actual 
data buckets present at the lowest levels of the hierarchy in 
contrast the major index points to a broader range of 
locations and so it contains information about main and sub 
categories of data 
 index information is not incorporated in the data buckets 
index buckets are separate containing only the control 
information 
 the number of major index buckets m ic ic {ic ic 
ic   icn} where ici represent information type and 
represents the cardinality of the information content set ic 
in this example ic {icmovie icweather icrestaurant} and so 
 ic hence the number of major index buckets is 
 mechanism to resolve the query is present in the java based 
coordinator in mu for example if a query q is presented as 
q entertainment movie road then the resultant search 
will be for the em information in the major index we say 
q em 
our proposed index works as follows let us suppose that an mu 
issues a query which is represented by java coordinator present in 
the mu as restaurant information on road this is resolved 
by the coordinator as q er this means one has to search for 
er unit of index in the major index let us suppose that the mu 
logs into the channel at r the first index it receives is a minor 
index after r in this index value of next variable which 
means that the next major index is present after bucket the mu 
may go into doze mode it becomes active after bucket and 
receives the major index it searches for er information which is 
the first entry in this index it is now certain that the mu will get 
the position of the data bucket in the adjoining minor index the 
second unit in the minor index depicts the position of the required 
data r it tells that the data bucket is the first bucket in area 
the mu goes into doze mode again and becomes active after 
bucket it gets the required data in the next bucket we present 
the algorithm for searching the location based index 
algorithm location based index search in days 
 scan broadcast for the next index bucket found false 
 while not found do 
 if bucket is major index then 
 find the type tuple s l 
 if s is greater than go into doze mode for s seconds 
 end if 
 wake up at the sth 
bucket and observe the minor index 
 end if 
 if bucket is minor index then 
 if typerequested not equal to typefound and a r request not 
equal to a r found then 
 go into doze mode till next repeat from step 
 end if 
 else find entry in minor index which points to data 
 compute time of arrival t of data bucket 
 go into doze mode till t 
 wake up at t and access data found true 
 end else 
 end if 
 end while 
 
 performance evaluation 
conservation of energy is the main concern when we try to access 
data from wireless broadcast an efficient scheme should allow 
the mobile device to access its required data by staying active for 
a minimum amount of time this would save considerable amount 
of energy since items are distributed based on types and are 
mapped to suitable locations we argue that our broadcast deals 
with clustered data types the mobile unit has to access a larger 
major index and a relatively much smaller minor index to get 
information about the time of arrival of data this is in contrast to 
the exponential scheme where the indexes are of equal sizes the 
example discussed and algorithm reveals that to access any 
data we need to access the major index only once followed by 
one or more accesses to the minor index the number of minor 
index access depends on the number of internal locations as the 
number of internal locations vary for item to item for example 
weather is generally associated with a city whereas traffic is 
granulated up to major and minor roads of a city we argue that 
the structure of the location mapped information may be 
visualized as a forest which is a collection of general trees the 
number of general trees depending on the types of information 
broadcasted and depth of a tree depending on the granularity of 
the location information associated with the information 
for our experiments we assume the forest as a collection of 
balanced m-ary trees we further assume the m-ary trees to be 
full by assuming the presence of dummy nodes in different levels 
of a tree 
thus if the number of data items is d and the height of the tree is 
m then 
n m d- m- where n is the number of vertices in the tree and 
i d- m- where i is the number of internal vertices 
tuning time for a data item involves unit of time required to 
access the major index plus time required to access the data items 
present in the leaves of the tree 
thus tuning time with d data items is t logmd 
we can say that tuning time is bounded by o logmd 
we compare our scheme with the distributed indexing and 
exponential scheme we assume a flat broadcast and number of 
pages varying from to the various simulation 
parameters are shown in table 
figure - shows the relative tuning times of three indexing 
algorithms ie the lbis exponential scheme and the distributed 
tree scheme figure shows the result for number of internal 
location nodes we can see that lbis significantly 
outperforms both the other schemes the tuning time in lbis 
ranges from approx to this large tuning time is due to the 
fact that after reaching the lowest minor index the mu may have 
to access few buckets sequentially to get the required data bucket 
we can see that the tuning time tends to become stable as the 
length of broadcast increases in figure we consider m here 
we can see that the exponential and the distributed perform almost 
similarly though the former seems to perform slightly better as 
the broadcast length increases a very interesting pattern is visible 
in figure for smaller broadcast size the lbis seems to have 
larger tuning time than the other two schemes but as the length of 
broadcast increases it is clearly visible the lbis outperforms the 
other two schemes the distributed tree indexing shows similar 
behavior like the lbis the tuning time in lbis remains low 
because the algorithm allows the mu to skip some intermediate 
minor indexes this allows the mu to move into lower levels 
directly after coming into active mode thus saving valuable 
energy this action is not possible in the distributed tree indexing 
and hence we can observe that its tuning time is more than the 
lbis scheme although it performs better than the exponential 
scheme figure in contrast shows us that the tuning time in 
lbis though less than the other two schemes tends to increase 
sharply as the broadcast length becomes greater than the 
pages this may be attributed both due to increase in time 
required to scan the intermediate minor indexes and the length of 
the broadcast but we can observe that the slope of the lbis 
curve is significantly less than the other two curves 
table simulation parameters 
p definition values 
n number of data items - 
m number of internal location nodes 
b capacity of bucket without index for 
exponential index 
 
i index base for exponential index 
k index size for distributed tree bytes 
the simulation results establish some facts about our 
location based indexing scheme the scheme performs 
better than the other two schemes in terms of tuning time in 
most of the cases as the length of the broadcast increases after a 
certain point though the tuning time increases as a result of 
factors which we have described before the scheme always 
performs better than the other two schemes due to the prescribed 
limit of the number of pages in the paper we are unable to show 
more results but these omitted results show similar trend as the 
results depicted in figure - 
 conclusion and future work 
in this paper we have presented a scheme for mapping of wireless 
broadcast data with their locations we have presented an example 
to show how the hierarchical structure of the location tree maps 
with the data to create ldd we have presented a scheme called 
lbis to index this ldd we have used the containment property 
of ldd in the scheme that limits the search to a narrow range of 
data in the broadcast thus saving valuable energy in the device 
the mapping of data with locations and the indexing scheme will 
be used in our days project to create the push based 
architecture the lbis has been compared with two other 
prominent indexing schemes i e the distributed tree indexing 
scheme and the exponential indexing scheme we showed in our 
simulations that the lbis scheme has the lowest tuning time for 
broadcasts having large number of pages thus saving valuable 
battery power in the mu 
 
in the future work we try to incorporate pull based architecture in 
our days project data from the server is available for access by 
the global users this may be done by putting a request to the 
source server the query in this case is a global query it is 
transferred from the user s source server to the destination server 
through the use of leo satellites we intend to use our ldd 
scheme and data staging architecture in the pull based architecture 
we will show that the ldd scheme together with the data staging 
architecture significantly improves the latency for global as well as 
local query 
 references 
 acharya s alonso r franklin m and zdonik s broadcast 
disk data management for asymmetric communications 
environments in proceedings of acm sigmod conference 
on management of data pages - san jose ca may 
 
 chen m s wu k l and yu p s optimizing index 
allocation for sequential data broadcasting in wireless mobile 
computing ieee transactions on knowledge and data 
engineering tkde - january february 
figure broadcast size buckets 
dist tree 
expo 
lbis 
figure broadcast size buckets 
dist tree 
expo 
lbis 
figure broadcast size buckets 
dist tree 
expo 
lbis 
figure broadcast size buckets 
dist tree 
expo 
lbis 
averagetuningtime 
averagetuningtime 
averagetuningtime 
averagetuningtime 
 
 hu q l lee d l and lee w c performance evaluation 
of a wireless hierarchical data dissemination system in 
proceedings of the th 
annual acm international conference 
on mobile computing and networking mobicom pages 
 - seattle wa august 
 hu q l lee w c and lee d l power conservative 
multi-attribute queries on data broadcast in proceedings of 
the th international conference on data engineering 
 icde pages - san diego ca february 
 imielinski t viswanathan s and badrinath b r power 
efficient filtering of data on air in proceedings of the th 
international conference on extending database technology 
 edbt pages - cambridge uk march 
 imielinski t viswanathan s and badrinath b r data on 
air - organization and access ieee transactions on 
knowledge and data engineering tkde - 
may june 
 shih e bahl p and sinclair m j wake on wireless an 
event driven energy saving strategy for battery operated 
devices in proceedings of the th annual acm international 
conference on mobile computing and networking 
 mobicom pages - atlanta ga september 
 
 shivakumar n and venkatasubramanian s energy-efficient 
indexing for information dissemination in wireless systems 
acm baltzer journal of mobile networks and applications 
 monet - december 
 tan k l and yu j x energy efficient filtering of non 
uniform broadcast in proceedings of the th international 
conference on distributed computing systems icdcs 
pages - hong kong may 
 viredaz m a brakmo l s and hamburgen w r energy 
management on handheld devices acm queue - 
october 
 garg n kumar v dunham m h information mapping 
and indexing in days th international workshop on 
mobility in databases and distributed systems in 
conjunction with the th international conference on 
database and expert systems applications september - 
prague czech republic 
 acharya d kumar v dunham m h infospace hybrid 
and adaptive public data dissemination system for 
ubiquitous computing accepted for publication in the 
special issue of pervasive computing wiley journal for 
wireless communications and mobile computing 
 acharya d kumar v prabhu n discovering and using 
web services in m-commerce proceedings for th vldb 
workshop on technologies for e-services toronto 
canada 
 acharya d kumar v indexing location dependent data in 
broadcast environment accepted for publication jdim 
special issue on distributed data management 
 flinn j sinnamohideen s satyanarayan m data 
staging on untrusted surrogates intel research pittsburg 
unpublished report 
 seydim a y dunham m h kumar v location 
dependent query processing proceedings of the nd acm 
international workshop on data engineering for wireless and 
mobile access p - santa barbara california usa 
 
 xu j lee w c tang x exponential index a 
parameterized distributed indexing scheme for data on air 
in proceedings of the nd acm usenix international 
conference on mobile systems applications and services 
 mobisys boston ma june 
 
automatic extraction of titles from general documents 
using machine learning 
yunhua hu 
computer science department 
xi an jiaotong university 
no xianning west road 
xi an china 
yunhuahu mail xjtu edu cn 
hang li yunbo cao 
microsoft research asia 
 f sigma center 
no zhichun road haidian 
beijing china 
{hangli yucao} microsoft com 
qinghua zheng 
computer science department 
xi an jiaotong university 
no xianning west road 
xi an china 
qhzheng mail xjtu edu cn 
dmitriy meyerzon 
microsoft corporation 
one microsoft way 
redmond wa 
usa 
dmitriym microsoft com 
abstract 
in this paper we propose a machine learning approach to title 
extraction from general documents by general documents we 
mean documents that can belong to any one of a number of 
specific genres including presentations book chapters technical 
papers brochures reports and letters previously methods have 
been proposed mainly for title extraction from research papers it 
has not been clear whether it could be possible to conduct 
automatic title extraction from general documents as a case study 
we consider extraction from office including word and 
powerpoint in our approach we annotate titles in sample 
documents for word and powerpoint respectively and take them 
as training data train machine learning models and perform title 
extraction using the trained models our method is unique in that 
we mainly utilize formatting information such as font size as 
features in the models it turns out that the use of formatting 
information can lead to quite accurate extraction from general 
documents precision and recall for title extraction from word is 
 and respectively and precision and recall for title 
extraction from powerpoint is and respectively in an 
experiment on intranet data other important new findings in this 
work include that we can train models in one domain and apply 
them to another domain and more surprisingly we can even train 
models in one language and apply them to another language 
moreover we can significantly improve search ranking results in 
document retrieval by using the extracted titles 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval - search process h information systems 
applications office automation - word processing d 
 software engineering metrics - complexity measures 
performance measures 
general terms 
algorithms experimentation performance 
 introduction 
metadata of documents is useful for many kinds of document 
processing such as search browsing and filtering ideally 
metadata is defined by the authors of documents and is then used 
by various systems however people seldom define document 
metadata by themselves even when they have convenient 
metadata definition tools thus how to automatically extract 
metadata from the bodies of documents turns out to be an 
important research issue 
methods for performing the task have been proposed however 
the focus was mainly on extraction from research papers for 
instance han et al proposed a machine learning based 
method to conduct extraction from research papers they 
formalized the problem as that of classification and employed 
support vector machines as the classifier they mainly used 
linguistic features in the model 
in this paper we consider metadata extraction from general 
documents by general documents we mean documents that may 
belong to any one of a number of specific genres general 
documents are more widely available in digital libraries intranets 
and the internet and thus investigation on extraction from them is 
sorely needed research papers usually have well-formed styles 
and noticeable characteristics in contrast the styles of general 
documents can vary greatly it has not been clarified whether a 
machine learning based approach can work well for this task 
there are many types of metadata title author date of creation 
etc as a case study we consider title extraction in this paper 
general documents can be in many different file formats 
microsoft office pdf ps etc as a case study we consider 
extraction from office including word and powerpoint 
we take a machine learning approach we annotate titles in 
sample documents for word and powerpoint respectively and 
take them as training data to train several types of models and 
perform title extraction using any one type of the trained models 
in the models we mainly utilize formatting information such as 
font size as features we employ the following models maximum 
entropy model perceptron with uneven margins maximum 
entropy markov model and voted perceptron 
in this paper we also investigate the following three problems 
which did not seem to have been examined previously 
 comparison between models among the models above which 
model performs best for title extraction 
 generality of model whether it is possible to train a model on 
one domain and apply it to another domain and whether it is 
possible to train a model in one language and apply it to another 
language 
 usefulness of extracted titles whether extracted titles can 
improve document processing such as search 
experimental results indicate that our approach works well for 
title extraction from general documents our method can 
significantly outperform the baselines one that always uses the 
first lines as titles and the other that always uses the lines in the 
largest font sizes as titles precision and recall for title extraction 
from word are and respectively and precision and 
recall for title extraction from powerpoint are and 
respectively it turns out that the use of format features is the key 
to successful title extraction 
 we have observed that perceptron based models perform 
better in terms of extraction accuracies we have empirically 
verified that the models trained with our approach are generic in 
the sense that they can be trained on one domain and applied to 
another and they can be trained in one language and applied to 
another we have found that using the extracted titles we can 
significantly improve precision of document retrieval by 
we conclude that we can indeed conduct reliable title extraction 
from general documents and use the extracted results to improve 
real applications 
the rest of the paper is organized as follows in section we 
introduce related work and in section we explain the 
motivation and problem setting of our work in section we 
describe our method of title extraction and in section we 
describe our method of document retrieval using extracted titles 
section gives our experimental results we make concluding 
remarks in section 
 related work 
 document metadata extraction 
methods have been proposed for performing automatic metadata 
extraction from documents however the main focus was on 
extraction from research papers 
the proposed methods fall into two categories the rule based 
approach and the machine learning based approach 
giuffrida et al for instance developed a rule-based system for 
automatically extracting metadata from research papers in 
postscript they used rules like titles are usually located on the 
upper portions of the first pages and they are usually in the largest 
font sizes liddy et al and yilmazel el al performed 
metadata extraction from educational materials using rule-based 
natural language processing technologies mao et al also 
conducted automatic metadata extraction from research papers 
using rules on formatting information 
the rule-based approach can achieve high performance however 
it also has disadvantages it is less adaptive and robust when 
compared with the machine learning approach 
han et al for instance conducted metadata extraction with 
the machine learning approach they viewed the problem as that 
of classifying the lines in a document into the categories of 
metadata and proposed using support vector machines as the 
classifier they mainly used linguistic information as features 
they reported high extraction accuracy from research papers in 
terms of precision and recall 
 information extraction 
metadata extraction can be viewed as an application of 
information extraction in which given a sequence of instances we 
identify a subsequence that represents information in which we 
are interested hidden markov model maximum entropy 
model maximum entropy markov model support 
vector machines conditional random field and voted 
perceptron are widely used information extraction models 
information extraction has been applied for instance to 
part-ofspeech tagging named entity recognition and table 
extraction 
 search using title information 
title information is useful for document retrieval 
in the system citeseer for instance giles et al managed to 
extract titles from research papers and make use of the extracted 
titles in metadata search of papers 
in web search the title fields i e file properties and anchor texts 
of web pages html documents can be viewed as  titles of the 
pages many search engines seem to utilize them for web page 
retrieval zhang et al found that web pages with 
well-defined metadata are more easily retrieved than those without 
well-defined metadata 
to the best of our knowledge no research has been conducted on 
using extracted titles from general documents e g office 
documents for search of the documents 
 
 motivation and problem 
setting 
we consider the issue of automatically extracting titles from 
general documents 
by general documents we mean documents that belong to one of 
any number of specific genres the documents can be 
presentations books book chapters technical papers brochures 
reports memos specifications letters announcements or resumes 
general documents are more widely available in digital libraries 
intranets and internet and thus investigation on title extraction 
from them is sorely needed 
figure shows an estimate on distributions of file formats on 
intranet and internet office and pdf are the main file 
formats on the intranet even on the internet the documents in the 
formats are still not negligible given its extremely large size in 
this paper without loss of generality we take office documents as 
an example 
figure distributions of file formats in internet and intranet 
for office documents users can define titles as file properties 
using a feature provided by office we found in an experiment 
however that users seldom use the feature and thus titles in file 
properties are usually very inaccurate that is to say titles in file 
properties are usually inconsistent with the  true titles in the file 
bodies that are created by the authors and are visible to readers 
we collected word and powerpoint documents from 
an intranet and the internet and examined how many titles in the 
file properties are correct we found that surprisingly the accuracy 
was only cf section for details a number of reasons 
can be considered for example if one creates a new file by 
copying an old file then the file property of the new file will also 
be copied from the old file 
in another experiment we found that google uses the titles in file 
properties of office documents in search and browsing but the 
titles are not very accurate we created queries to search word 
and powerpoint documents and examined the top results of 
each query returned by google we found that nearly all the titles 
presented in the search results were from the file properties of the 
documents however only of them were correct 
actually  true titles usually exist at the beginnings of the bodies 
of documents if we can accurately extract the titles from the 
bodies of documents then we can exploit reliable title information 
in document processing this is exactly the problem we address in 
this paper 
more specifically given a word document we are to extract the 
title from the top region of the first page given a powerpoint 
document we are to extract the title from the first slide a title 
sometimes consists of a main title and one or two subtitles we 
only consider extraction of the main title 
as baselines for title extraction we use that of always using the 
first lines as titles and that of always using the lines with largest 
font sizes as titles 
figure title extraction from word document 
figure title extraction from powerpoint document 
next we define a  specification for human judgments in title data 
annotation the annotated data will be used in training and testing 
of the title extraction methods 
summary of the specification the title of a document should be 
identified on the basis of common sense if there is no difficulty in 
the identification however there are many cases in which the 
identification is not easy there are some rules defined in the 
specification that guide identification for such cases the rules 
include a title is usually in consecutive lines in the same format 
a document can have no title titles in images are not 
considered a title should not contain words like  draft 
 
 whitepaper etc if it is difficult to determine which is the title 
select the one in the largest font size and if it is still difficult to 
determine which is the title select the first candidate the 
specification covers all the cases we have encountered in data 
annotation 
figures and show examples of office documents from which 
we conduct title extraction in figure  differences in win 
api implementations among windows operating systems is the 
title of the word document  microsoft windows on the top of 
this page is a picture and thus is ignored in figure  building 
competitive advantages through an agile infrastructure is the 
title of the powerpoint document 
we have developed a tool for annotation of titles by human 
annotators figure shows a snapshot of the tool 
figure title annotation tool 
 title extraction method 
 outline 
title extraction based on machine learning consists of training and 
extraction the same pre-processing step occurs before training 
and extraction 
during pre-processing from the top region of the first page of a 
word document or the first slide of a powerpoint document a 
number of units for processing are extracted if a line lines are 
separated by  return symbols only has a single format then the 
line will become a unit if a line has several parts and each of 
them has its own format then each part will become a unit each 
unit will be treated as an instance in learning a unit contains not 
only content information linguistic information but also 
formatting information the input to pre-processing is a document 
and the output of pre-processing is a sequence of units instances 
figure shows the units obtained from the document in figure 
figure example of units 
in learning the input is sequences of units where each sequence 
corresponds to a document we take labeled units labeled as 
title begin title end or other in the sequences as training data 
and construct models for identifying whether a unit is title begin 
title end or other we employ four types of models perceptron 
maximum entropy me perceptron markov model pmm and 
maximum entropy markov model memm 
in extraction the input is a sequence of units from one document 
we employ one type of model to identify whether a unit is 
title begin title end or other we then extract units from the unit 
labeled with  title begin to the unit labeled with  title end the 
result is the extracted title of the document 
the unique characteristic of our approach is that we mainly utilize 
formatting information for title extraction our assumption is that 
although general documents vary in styles their formats have 
certain patterns and we can learn and utilize the patterns for title 
extraction this is in contrast to the work by han et al in which 
only linguistic features are used for extraction from research 
papers 
 models 
the four models actually can be considered in the same metadata 
extraction framework that is why we apply them together to our 
current problem 
each input is a sequence of instances kxxx l together with a 
sequence of labels kyyy l ix and iy represents an instance 
and its label respectively ki l recall that an instance 
here represents a unit a label represents title begin title end or 
other here k is the number of units in a document 
in learning we train a model which can be generally denoted as a 
conditional probability distribution kk xxyyp ll where 
ix and iy denote random variables taking instance ix and label 
iy as values respectively ki l 
learning tool 
extraction tool 
 
 
 
nknnknn 
kk 
kk 
yyyxxx 
yyyxxx 
yyyxxx 
ll 
ll 
ll 
ll 
→ 
→ 
→ 
 maxarg mkmmkm xxyyp ll 
 kk xxyyp ll 
conditional 
distribution 
mkmm xxx l 
figure metadata extraction model 
we can make assumptions about the general model in order to 
make it simple enough for training 
 
for example we can assume that kyy l are independent of 
each other given kxx l thus we have 
 
 
 
 
kk 
kk 
xypxyp 
xxyyp 
l 
ll 
 
in this way we decompose the model into a number of classifiers 
we train the classifiers locally using the labeled data as the 
classifier we employ the perceptron or maximum entropy model 
we can also assume that the first order markov property holds for 
kyy l given kxx l thus we have 
 
 
 
 
kkk 
kk 
xyypxyp 
xxyyp 
− l 
ll 
again we obtain a number of classifiers however the classifiers 
are conditioned on the previous label when we employ the 
percepton or maximum entropy model as a classifier the models 
become a percepton markov model or maximum entropy markov 
model respectively that is to say the two models are more 
precise 
in extraction given a new sequence of instances we resort to one 
of the constructed models to assign a sequence of labels to the 
sequence of instances i e perform extraction 
for perceptron and me we assign labels locally and combine the 
results globally later using heuristics specifically we first 
identify the most likely title begin then we find the most likely 
title end within three units after the title begin finally we 
extract as a title the units between the title begin and the title end 
for pmm and memm we employ the viterbi algorithm to find 
the globally optimal label sequence 
in this paper for perceptron we actually employ an improved 
variant of it called perceptron with uneven margin this 
version of perceptron can work well especially when the number 
of positive instances and the number of negative instances differ 
greatly which is exactly the case in our problem 
we also employ an improved version of perceptron markov 
model in which the perceptron model is the so-called voted 
perceptron in addition in training the parameters of the 
model are updated globally rather than locally 
 features 
there are two types of features format features and linguistic 
features we mainly use the former the features are used for both 
the title-begin and the title-end classifiers 
 format features 
font size there are four binary features that represent the 
normalized font size of the unit recall that a unit has only one 
type of font 
if the font size of the unit is the largest in the document then the 
first feature will be otherwise if the font size is the smallest 
in the document then the fourth feature will be otherwise if 
the font size is above the average font size and not the largest in 
the document then the second feature will be otherwise if the 
font size is below the average font size and not the smallest the 
third feature will be otherwise 
it is necessary to conduct normalization on font sizes for 
example in one document the largest font size might be   pt 
while in another the smallest one might be   pt 
boldface this binary feature represents whether or not the 
current unit is in boldface 
alignment there are four binary features that respectively 
represent the location of the current unit  left  center  right 
and  unknown alignment 
the following format features with respect to  context play an 
important role in title extraction 
empty neighboring unit there are two binary features that 
represent respectively whether or not the previous unit and the 
current unit are blank lines 
font size change there are two binary features that represent 
respectively whether or not the font size of the previous unit and 
the font size of the next unit differ from that of the current unit 
alignment change there are two binary features that represent 
respectively whether or not the alignment of the previous unit and 
the alignment of the next unit differ from that of the current one 
same paragraph there are two binary features that represent 
respectively whether or not the previous unit and the next unit are 
in the same paragraph as the current unit 
 linguistic features 
the linguistic features are based on key words 
positive word this binary feature represents whether or not the 
current unit begins with one of the positive words the positive 
words include  title  subject  subject line for example in 
some documents the lines of titles and authors have the same 
formats however if lines begin with one of the positive words 
then it is likely that they are title lines 
negative word this binary feature represents whether or not the 
current unit begins with one of the negative words the negative 
words include  to  by  created by  updated by etc 
there are more negative words than positive words the above 
linguistic features are language dependent 
word count a title should not be too long we heuristically 
create four intervals and ∞ and define one 
feature for each interval if the number of words in a title falls into 
an interval then the corresponding feature will be otherwise 
ending character this feature represents whether the unit ends 
with    - or other special characters a title usually does not 
end with such a character 
 document retrieval method 
we describe our method of document retrieval using extracted 
titles 
typically in information retrieval a document is split into a 
number of fields including body title and anchor text a ranking 
function in search can use different weights for different fields of 
 
the document also titles are typically assigned high weights 
indicating that they are important for document retrieval as 
explained previously our experiment has shown that a significant 
number of documents actually have incorrect titles in the file 
properties and thus in addition of using them we use the extracted 
titles as one more field of the document by doing this we attempt 
to improve the overall precision 
in this paper we employ a modification of bm that allows field 
weighting as fields we make use of body title extracted 
title and anchor first for each term in the query we count the 
term frequency in each field of the document each field 
frequency is then weighted according to the corresponding weight 
parameter 
∑ 
f 
tfft tfwwtf 
similarly we compute the document length as a weighted sum of 
lengths of each field average document length in the corpus 
becomes the average of all weighted document lengths 
∑ 
f 
ff dlwwdl 
in our experiments we used bk weight for content 
was title was anchor was and extracted title was 
 
 experimental results 
 data sets and evaluation measures 
we used two data sets in our experiments 
first we downloaded and randomly selected word 
documents and powerpoint documents from an intranet of 
microsoft we call it ms hereafter 
second we downloaded and randomly selected word and 
powerpoint documents from the dotgov and dotcom domains on 
the internet respectively 
figure shows the distributions of the genres of the documents 
we see that the documents are indeed  general documents as we 
define them 
figure distributions of document genres 
third a data set in chinese was also downloaded from the internet 
it includes word documents and powerpoint documents 
in chinese 
we manually labeled the titles of all the documents on the basis 
of our specification 
not all the documents in the two data sets have titles table 
shows the percentages of the documents having titles we see that 
dotcom and dotgov have more powerpoint documents with titles 
than ms this might be because powerpoint documents published 
on the internet are more formal than those on the intranet 
table the portion of documents with titles 
domain 
type 
ms dotcom dotgov 
word 
powerpoint 
in our experiments we conducted evaluations on title extraction in 
terms of precision recall and f-measure the evaluation 
measures are defined as follows 
precision p a a b 
recall r a a c 
f-measure f pr p r 
here a b c and d are numbers of documents as those defined 
in table 
table contingence table with regard to title extraction 
is title is not title 
extracted a b 
not extracted c d 
 baselines 
we test the accuracies of the two baselines described in section 
 they are denoted as  largest font size and  first line 
respectively 
 accuracy of titles in file properties 
we investigate how many titles in the file properties of the 
documents are reliable we view the titles annotated by humans as 
true titles and test how many titles in the file properties can 
approximately match with the true titles we use edit distance to 
conduct the approximate match approximate match is only used 
in this evaluation this is because sometimes human annotated 
titles can be slightly different from the titles in file properties on 
the surface e g contain extra spaces 
given string a and string b 
if d or d la lb θ then string a string b 
d edit distance between string a and string b 
la length of string a 
lb length of string b 
θ 
∑ × 
 − 
 
 
t 
t 
n 
n 
wtf 
avwdl 
wdl 
bbk 
kwtf 
fbm log 
 
 
 
 
 
 
table accuracies of titles in file properties 
file type domain precision recall f 
ms 
dotcom word 
dotgov 
ms 
dotcom powerpoint 
dotgov 
 comparison with baselines 
we conducted title extraction from the first data set word and 
powerpoint in ms as the model we used perceptron 
we conduct -fold cross validation thus all the results reported 
here are those averaged over trials tables and show the 
results we see that perceptron significantly outperforms the 
baselines in the evaluation we use exact matching between the 
true titles annotated by humans and the extracted titles 
table accuracies of title extraction with word 
precision recall f 
model perceptron 
largest font size 
baselines 
first line 
table accuracies of title extraction with powerpoint 
precision recall f 
model perceptron 
largest font size 
baselines 
first line 
we see that the machine learning approach can achieve good 
performance in title extraction for word documents both 
precision and recall of the approach are percent higher than 
those of the baselines for powerpoint both precision and recall of 
the approach are percent higher than those of the baselines 
we conduct significance tests the results are shown in table 
here  largest denotes the baseline of using the largest font size 
 first denotes the baseline of using the first line the results 
indicate that the improvements of machine learning over baselines 
are statistically significant in the sense p-value 
table sign test results 
documents type sign test between p-value 
perceptron vs largest e- 
word 
perceptron vs first e- 
perceptron vs largest 
powerpoint 
perceptron vs first e- 
we see from the results that the two baselines can work well for 
title extraction suggesting that font size and position information 
are most useful features for title extraction however it is also 
obvious that using only these two features is not enough there 
are cases in which all the lines have the same font size i e the 
largest font size or cases in which the lines with the largest font 
size only contain general descriptions like  confidential  white 
paper etc for those cases the  largest font size method cannot 
work well for similar reasons the  first line method alone 
cannot work well either with the combination of different 
features evidence in title judgment perceptron can outperform 
largest and first 
we investigate the performance of solely using linguistic features 
we found that it does not work well it seems that the format 
features play important roles and the linguistic features are 
supplements 
figure an example word document 
figure an example powerpoint document 
we conducted an error analysis on the results of perceptron we 
found that the errors fell into three categories about one third 
of the errors were related to  hard cases in these documents the 
layouts of the first pages were difficult to understand even for 
humans figure and shows examples nearly one fourth of 
the errors were from the documents which do not have true titles 
but only contain bullets since we conduct extraction from the top 
regions it is difficult to get rid of these errors with the current 
approach confusions between main titles and subtitles were 
another type of error since we only labeled the main titles as 
titles the extractions of both titles were considered incorrect this 
type of error does little harm to document processing like search 
however 
 comparison between models 
to compare the performance of different machine learning models 
we conducted another experiment again we perform -fold cross 
 
validation on the first data set ms table shows the results 
of all the four models 
it turns out that perceptron and pmm perform the best followed 
by memm and me performs the worst in general the 
markovian models perform better than or as well as their classifier 
counterparts this seems to be because the markovian models are 
trained globally while the classifiers are trained locally the 
perceptron based models perform better than the me based 
counterparts this seems to be because the perceptron based 
models are created to make better classifications while me 
models are constructed for better prediction 
table comparison between different learning models for 
title extraction with word 
model precision recall f 
perceptron 
memm 
pmm 
me 
table comparison between different learning models for 
title extraction with powerpoint 
model precision recall f 
perceptron 
memm 
pmm 
me 
 domain adaptation 
we apply the model trained with the first data set ms to the 
second data set dotcom and dotgov tables - show the 
results 
table accuracies of title extraction with word in dotgov 
precision recall f 
model perceptron 
largest font size baselines 
first line 
table accuracies of title extraction with powerpoint in 
dotgov 
precision recall f 
model perceptron 
largest font size baselines 
first line 
table accuracies of title extraction with word in dotcom 
precisio 
n 
recall f 
model perceptron 
largest font size baselines 
first line 
table performance of powerpoint document title 
extraction in dotcom 
precisio 
n 
recall f 
model perceptron 
largest font size baselines 
first line 
from the results we see that the models can be adapted to 
different domains well there is almost no drop in accuracy the 
results indicate that the patterns of title formats exist across 
different domains and it is possible to construct a domain 
independent model by mainly using formatting information 
 language adaptation 
we apply the model trained with the data in english ms to the 
data set in chinese 
tables - show the results 
table accuracies of title extraction with word in chinese 
precision recall f 
model perceptron 
largest font size baselines 
first line 
table accuracies of title extraction with powerpoint in 
chinese 
precision recall f 
model perceptron 
largest font size baselines 
first line 
we see that the models can be adapted to a different language 
there are only small drops in accuracy obviously the linguistic 
features do not work for chinese but the effect of not using them 
is negligible the results indicate that the patterns of title formats 
exist across different languages 
from the domain adaptation and language adaptation results we 
conclude that the use of formatting information is the key to a 
successful extraction from general documents 
 search with extracted titles 
we performed experiments on using title extraction for document 
retrieval as a baseline we employed bm without using 
extracted titles the ranking mechanism was as described in 
section the weights were heuristically set we did not conduct 
optimization on the weights 
the evaluation was conducted on a corpus of m documents 
crawled from the intranet of microsoft using evaluation 
queries obtained from this intranet s search engine query logs 
queries were from the most popular set while queries other 
were chosen randomly users were asked to provide judgments of 
the degree of document relevance from a scale of to 
meaning detrimental - bad - fair - good and - excellent 
 
figure shows the results in the chart two sets of precision 
results were obtained by either considering good or excellent 
documents as relevant left bars with relevance threshold or 
by considering only excellent documents as relevant right bars 
with relevance threshold 
 
 
 
 
 
 
 
 
 
 
p  p  reciprocal p  p  reciprocal 
 
bm anchor title body 
bm anchor title body extractedtitle 
name all 
relevancethreshold data 
description 
figure search ranking results 
figure shows different document retrieval results with different 
ranking functions in terms of precision   precision   and 
reciprocal rank 
 blue bar - bm including the fields body title file 
property and anchor text 
 purple bar - bm including the fields body title file 
property anchor text and extracted title 
with the additional field of extracted title included in bm the 
precision   increased from to or by   thus 
it is safe to say that the use of extracted title can indeed improve 
the precision of document retrieval 
 conclusion 
in this paper we have investigated the problem of automatically 
extracting titles from general documents we have tried using a 
machine learning approach to address the problem 
previous work showed that the machine learning approach can 
work well for metadata extraction from research papers in this 
paper we showed that the approach can work for extraction from 
general documents as well our experimental results indicated that 
the machine learning approach can work significantly better than 
the baselines in title extraction from office documents previous 
work on metadata extraction mainly used linguistic features in 
documents while we mainly used formatting information it 
appeared that using formatting information is a key for 
successfully conducting title extraction from general documents 
we tried different machine learning models including perceptron 
maximum entropy maximum entropy markov model and voted 
perceptron we found that the performance of the perceptorn 
models was the best we applied models constructed in one 
domain to another domain and applied models trained in one 
language to another language we found that the accuracies did 
not drop substantially across different domains and across 
different languages indicating that the models were generic we 
also attempted to use the extracted titles in document retrieval we 
observed a significant improvement in document ranking 
performance for search when using extracted title information all 
the above investigations were not conducted in previous work and 
through our investigations we verified the generality and the 
significance of the title extraction approach 
 acknowledgements 
we thank chunyu wei and bojuan zhao for their work on data 
annotation we acknowledge jinzhu li for his assistance in 
conducting the experiments we thank ming zhou john chen 
jun xu and the anonymous reviewers of jcdl for their 
valuable comments on this paper 
 references 
 berger a l della pietra s a and della pietra v j a 
maximum entropy approach to natural language processing 
computational linguistics - 
 collins m discriminative training methods for hidden 
markov models theory and experiments with perceptron 
algorithms in proceedings of conference on empirical 
methods in natural language processing - 
 cortes c and vapnik v support-vector networks machine 
learning - 
 chieu h l and ng h t a maximum entropy approach to 
information extraction from semi-structured and free text in 
proceedings of the eighteenth national conference on 
artificial intelligence - 
 evans d k klavans j l and mckeown k r columbia 
newsblaster multilingual news summarization on the web 
in proceedings of human language technology conference 
north american chapter of the association for 
computational linguistics annual meeting - 
 ghahramani z and jordan m i factorial hidden markov 
models machine learning - 
 gheel j and anderson t data and metadata for finding and 
reminding in proceedings of the international 
conference on information visualization - 
 giles c l petinot y teregowda p b han h 
lawrence s rangaswamy a and pal n ebizsearch a 
niche search engine for e-business in proceedings of the 
 th annual international acm sigir conference on 
research and development in information retrieval 
 
 giuffrida g shek e c and yang j knowledge-based 
metadata extraction from postscript files in proceedings of 
the fifth acm conference on digital libraries - 
 han h giles c l manavoglu e zha h zhang z and 
fox e a automatic document metadata extraction using 
support vector machines in proceedings of the third 
acm ieee-cs joint conference on digital libraries - 
 
 kobayashi m and takeda k information retrieval on the 
web acm computing surveys - 
 lafferty j mccallum a and pereira f conditional 
random fields probabilistic models for segmenting and 
 
labeling sequence data in proceedings of the eighteenth 
international conference on machine learning - 
 
 li y zaragoza h herbrich r shawe-taylor j and 
kandola j s the perceptron algorithm with uneven margins 
in proceedings of the nineteenth international conference 
on machine learning - 
 liddy e d sutton s allen e harwell s corieri s 
yilmazel o ozgencil n e diekema a mccracken n 
and silverstein j automatic metadata generation 
evaluation in proceedings of the th annual international 
acm sigir conference on research and development in 
information retrieval - 
 littlefield a effective enterprise information retrieval 
across new content formats in proceedings of the seventh 
search engine conference 
http www infonortics com searchengines sh prog html 
 
 mao s kim j w and thoma g r a dynamic feature 
generation system for automated metadata extraction in 
preservation of digital materials in proceedings of the first 
international workshop on document image analysis for 
libraries - 
 mccallum a freitag d and pereira f maximum entropy 
markov models for information extraction and segmentation 
in proceedings of the seventeenth international conference 
on machine learning - 
 murphy l d digital document metadata in organizations 
roles analytical approaches and future research directions 
in proceedings of the thirty-first annual hawaii 
international conference on system sciences - 
 pinto d mccallum a wei x and croft w b table 
extraction using conditional random fields in proceedings of 
the th annual international acm sigir conference on 
research and development in information retrieval 
 
 ratnaparkhi a unsupervised statistical models for 
prepositional phrase attachment in proceedings of the 
seventeenth international conference on computational 
linguistics - 
 robertson s zaragoza h and taylor m simple bm 
extension to multiple weighted fields in proceedings of 
acm thirteenth conference on information and knowledge 
management - 
 yi j and sundaresan n metadata based web mining for 
relevance in proceedings of the international 
symposium on database engineering applications 
 
 yilmazel o finneran c m and liddy e d metaextract 
an nlp system to automatically assign metadata in 
proceedings of the joint acm ieee conference on 
digital libraries - 
 zhang j and dimitroff a internet search engines response 
to metadata dublin core implementation journal of 
information science - 
 zhang l pan y and zhang t recognising and using 
named entities focused named entity recognition using 
machine learning in proceedings of the th annual 
international acm sigir conference on research and 
development in information retrieval - 
 http dublincore org groups corporate seattle 
 
performance prediction using spatial autocorrelation 
fernando diaz 
center for intelligent information retrieval 
department of computer science 
university of massachusetts 
amherst ma 
fdiaz cs umass edu 
abstract 
evaluation of information retrieval systems is one of the core 
tasks in information retrieval problems include the 
inability to exhaustively label all documents for a topic 
nongeneralizability from a small number of topics and 
incorporating the variability of retrieval systems previous work 
addresses the evaluation of systems the ranking of queries 
by difficulty and the ranking of individual retrievals by 
performance approaches exist for the case of few and even no 
relevance judgments our focus is on zero-judgment 
performance prediction of individual retrievals 
one common shortcoming of previous techniques is the 
assumption of uncorrelated document scores and judgments 
if documents are embedded in a high-dimensional space as 
they often are we can apply techniques from spatial data 
analysis to detect correlations between document scores 
we find that the low correlation between scores of 
topically close documents often implies a poor retrieval 
performance when compared to a state of the art baseline 
we demonstrate that the spatial analysis of retrieval scores 
provides significantly better prediction performance these 
new predictors can also be incorporated with classic 
predictors to improve performance further we also describe 
the first large-scale experiment to evaluate zero-judgment 
performance prediction for a massive number of retrieval 
systems over a variety of collections in several languages 
categories and subject descriptors 
h information search and retrieval retrieval 
models h systems and software performance 
evaluation efficiency and effectiveness 
general terms 
performance design reliability experimentation 
 introduction 
in information retrieval a user poses a query to a system 
the system retrieves n documents each receiving a 
realvalued score indicating the predicted degree of relevance 
if we randomly select pairs of documents from this set we 
expect some pairs to share the same topic and other pairs to 
not share the same topic take two topically-related 
documents from the set and call them a and b if the scores of a 
and b are very different we may be concerned about the 
performance of our system that is if a and b are both on the 
topic of the query we would like them both to receive a high 
score if a and b are not on the topic of the query we would 
like them both to receive a low score we might become more 
worried as we find more differences between scores of related 
documents we would be more comfortable with a retrieval 
where scores are consistent between related documents 
our paper studies the quantification of this inconsistency 
in a retrieval from a spatial perspective spatial analysis is 
appropriate since many retrieval models embed documents 
in some vector space if documents are embedded in a space 
proximity correlates with topical relationships score 
consistency can be measured by the spatial version of 
autocorrelation known as the moran coefficient or im in 
this paper we demonstrate a strong correlation between im 
and retrieval performance 
the discussion up to this point is reminiscent of the 
cluster hypothesis the cluster hypothesis states closely-related 
documents tend to be relevant to the same request as we 
shall see a retrieval function s spatial autocorrelation 
measures the degree to which closely-related documents receive 
similar scores because of this we interpret 
autocorrelation as measuring the degree to which a retrieval function 
satisfies the clustering hypothesis if this connection is 
reasonable in section we present evidence that failure to 
satisfy the cluster hypothesis correlates strongly with poor 
performance 
in this work we provide the following contributions 
 a general robust method for predicting the 
performance of retrievals with zero relevance judgments 
 section 
 a theoretical treatment of the similarities and 
motivations behind several state-of-the-art performance 
prediction techniques section 
 the first large-scale experiments of zero-judgment 
single run performance prediction sections and 
 problem definition 
given a query an information retrieval system produces 
a ranking of documents in the collection encoded as a set 
of scores associated with documents we refer to the set 
of scores for a particular query-system combination as a 
retrieval we would like to predict the performance of this 
retrieval with respect to some evaluation measure eg mean 
average precision in this paper we present results for 
ranking retrievals from arbitrary systems we would like 
this ranking to approximate the ranking of retrievals by the 
evaluation measure this is different from ranking queries 
by the average performance on each query it is also 
different from ranking systems by the average performance on a 
set of queries 
scores are often only computed for the top n documents 
from the collection we place these scores in the length 
n vector y where yi refers to the score of the ith-ranked 
document we adjust scores to have zero mean and unit 
variance we use this method because of its simplicity and 
its success in previous work 
 spatial correlation 
in information retrieval we often assume that the 
representations of documents exist in some high-dimensional 
vector space for example given a vocabulary v this vector 
space may be an arbitrary v -dimensional space with cosine 
inner-product or a multinomial simplex with a 
distributionbased distance measure an embedding space is often 
selected to respect topical proximity if two documents are 
near they are more likely to share a topic 
because of the prevalence and success of spatial models 
of information retrieval we believe that the application of 
spatial data analysis techniques are appropriate whereas 
in information retrieval we are concerned with the score at 
a point in a space in spatial data analysis we are concerned 
with the value of a function at a point or location in a space 
we use the term function here to mean a mapping from a 
location to a real value for example we might be interested 
in the prevalence of a disease in the neighborhood of some 
city the function would map the location of a neighborhood 
to an infection rate 
if we want to quantify the spatial dependencies of a 
function we would employ a measure referred to as the spatial 
autocorrelation high spatial autocorrelation suggests 
that knowing the value of a function at location a will tell 
us a great deal about the value at a neighboring location 
b there is a high spatial autocorrelation for a function 
representing the temperature of a location since knowing 
the temperature at a location a will tell us a lot about the 
temperature at a neighboring location b low spatial 
autocorrelation suggests that knowing the value of a function 
at location a tells us little about the value at a neighboring 
location b there is low spatial autocorrelation in a function 
measuring the outcome of a coin toss at a and b 
in this section we will begin by describing what we mean 
by spatial proximity for documents and then define a 
measure of spatial autocorrelation we conclude by extending 
this model to include information from multiple retrievals 
from multiple systems for a single query 
 spatial representation of documents 
our work does not focus on improving a specific similarity 
measure or defining a novel vector space instead we choose 
an inner product known to be effective at detecting 
interdocument topical relationships specifically we adopt tf idf 
document vectors 
˜di di log 
„ 
 n − ci 
 ci 
 
 
where d is a vector of term frequencies c is the length- v 
document frequency vector we use this weighting scheme 
due to its success for topical link detection in the context 
of topic detection and tracking tdt evaluations 
assuming vectors are scaled by their l norm we use the inner 
product ˜di ˜dj to define similarity 
given documents and some similarity measure we can 
construct a matrix which encodes the similarity between 
pairs of documents recall that we are given the top n 
documents retrieved in y we can compute an n × n 
similarity matrix w an element of this matrix wij represents 
the similarity between documents ranked i and j in 
practice we only include the affinities for a document s k-nearest 
neighbors in all of our experiments we have fixed k to 
we leave exploration of parameter sensitivity to future work 
we also row normalize the matrix so that 
pn 
j wij for 
all i 
 spatial autocorrelation of a retrieval 
recall that we are interested in measuring the similarity 
between the scores of spatially-close documents one such 
suitable measure is the moran coefficient of spatial 
autocorrelation assuming the function y over n locations this is 
defined as 
˜im 
n 
etwe 
p 
i j wijyiyj 
p 
i y 
i 
 
n 
etwe 
yt 
wy 
yty 
 
where et 
we 
p 
ij wij 
we would like to compare autocorrelation values for 
different retrievals unfortunately the bound for equation 
is not consistent for different w and y therefore we use 
the cauchy-schwartz inequality to establish a bound 
˜im ≤ 
n 
etwe 
s 
ytwtwy 
yty 
and we define the normalized spatial autocorrelation as 
im 
yt 
wy 
p 
yty × ytwtwy 
notice that if we let ˜y wy then we can write this formula 
as 
im 
yt 
˜y 
y ˜y 
 
which can be interpreted as the correlation between the 
original retrieval scores and a set of retrieval scores diffused 
in the space 
we present some examples of autocorrelations of functions 
on a grid in figure 
 correlation with other retrievals 
sometimes we are interested in the performance of a single 
retrieval but have access to scores from multiple systems for 
 a im b im c im 
figure the moran coefficient im for a several 
binary functions on a grid the moran coefficient 
is a local measure of function consistency from the 
perspective of information retrieval each of these 
grid spaces would represent a document and 
documents would be organized so that they lay next to 
topically-related documents binary retrieval scores 
would define a pattern on this grid notice that 
as the moran coefficient increases neighboring cells 
tend to have similar values 
the same query in this situation we can use combined 
information from these scores to construct a surrogate for 
a high-quality ranking we can treat the correlation 
between the retrieval we are interested in and the combined 
scores as a predictor of performance 
assume that we are given m score functions yi for the 
same n documents we will represent the mean of these 
vectors as yµ 
pm 
i yi we use the mean vector as an 
approximation to relevance since we use zero mean and unit 
variance normalization work in metasearch suggests that 
this assumption is justified because yµ represents a 
very good retrieval we hypothesize that a strong similarity 
between yµ and y will correlate positively with system 
performance we use pearson s product-moment correlation to 
measure the similarity between these vectors 
ρ y yµ 
yt 
yµ 
y yµ 
 
we will comment on the similarity between equation and 
 in section 
of course we can combine ρ y ˜y and ρ y yµ if we 
assume that they capture different factors in the prediction 
one way to accomplish this is to combine these predictors 
as independent variables in a linear regression an 
alternative means of combination is suggested by the mathematical 
form of our predictors since ˜y encodes the spatial 
dependencies in y and yµ encodes the spatial properties of the 
multiple runs we can compute a third correlation between 
these two vectors 
ρ ˜y yµ 
˜yt 
yµ 
˜y yµ 
 
we can interpret equation as measuring the correlation 
between a high quality ranking yµ and a spatially smoothed 
version of the retrieval ˜y 
 relationship with other 
predictors 
one way to predict the effectiveness of a retrieval is to 
look at the shared vocabulary of the top n retrieved 
documents if we computed the most frequent content words 
in this set we would hope that they would be consistent 
with our topic in fact we might believe that a bad 
retrieval would include documents on many disparate topics 
resulting in an overlap of terminological noise the clarity 
of a query attempts to quantify exactly this specifically 
clarity measures the similarity of the words most frequently 
used in retrieved documents to those most frequently used 
in the whole corpus the conjecture is that a good retrieval 
will use language distinct from general text the overlapping 
language in a bad retrieval will tend to be more similar to 
general text mathematically we can compute a 
representation of the language used in the initial retrieval as a weighted 
combination of document language models 
p w θq 
nx 
i 
p w θi 
p q θi 
z 
 
where θi is the language model of the ith-ranked 
document p q θi is the query likelihood score of the ith-ranked 
document and z 
pn 
i p q θi is a normalization 
constant the similarity between the multinomial p w θq 
and a model of general text can be computed using the 
kullback-leibler divergence dv 
kl θq θc here the 
distribution p w θc is our model of general text which can be 
computed using term frequencies in the corpus in figure 
 a we present clarity as measuring the distance between the 
weighted center of mass of the retrieval labeled y and the 
unweighted center of mass of the collection labeled o 
clarity reaches a minimum when a retrieval assigns every 
document the same score 
let s again assume we have a set of n documents retrieved 
for our query another way to quantify the dispersion of a 
set of documents is to look at how clustered they are we 
may hypothesize that a good retrieval will return a single 
tight cluster a poorly performing retrieval will return a 
loosely related set of documents covering many topics one 
proposed method of quantifying this dispersion is to 
measure the distance from a random document a to it s nearest 
neighbor b a retrieval which is tightly clustered will on 
average have a low distance between a and b a retrieval 
which is less tightly-closed will on average have high 
distances between a and b this average corresponds to using 
the cox-lewis statistic to measure the randomness of the 
top n documents retrieved from a system in figure 
 a this is roughly equivalent to measuring the area of the 
set n notice that we are throwing away information about 
the retrieval function y therefore the cox-lewis statistic 
is highly dependent on selecting the top n documents 
remember that we have n documents and a set of scores 
let s assume that we have access to the system which 
provided the original scores and that we can also request scores 
for new documents this suggests a third method for 
predicting performance take some document a from the 
retrieved set and arbitrarily add or remove words at random 
to create a new document ˜a now we can ask our system 
to score ˜a with respect to our query if on average over 
the n documents the scores of a and ˜a tend to be very 
different we might suspect that the system is failing on this 
query so an alternative approach is to measure the 
simi 
the authors have suggested coupling the query with the 
distance measure the information introduced by the 
query though is retrieval-independent so that if two 
retrievals return the same set of documents the approximate 
cox-lewis statistic will be the same regardless of the 
retrieval scores 
yoy 
 a global divergence 
µ y ˜y 
y 
 b score perturbation 
µ y 
y 
 c multirun averaging 
figure representation of several performance predictors on a grid in figure a we depict predictors 
which measure the divergence between the center of mass of a retrieval and the center of the embedding 
space in figure b we depict predictors which compare the original retrieval y to a perturbed version of 
the retrieval ˜y our approach uses a particular type of perturbation based on score diffusion finally in 
figure c we depict prediction when given retrievals from several other systems on the same query here 
we can consider the fusion of these retrieval as a surrogate for relevance 
larity between the retrieval and a perturbed version of that 
retrieval this can be accomplished by either 
perturbing the documents or queries the similarity between 
the two retrievals can be measured using some correlation 
measure this is depicted in figure b the upper grid 
represents the original retrieval y while the lower grid 
represents the function after having been perturbed ˜y the 
nature of the perturbation process requires additional 
scorings or retrievals our predictor does not require access to 
the original scoring function or additional retrievals so 
although our method is similar to other perturbation methods 
in spirit it can be applied in situations when the retrieval 
system is inaccessible or costly to access 
finally assume that we have in addition to the retrieval 
we want to evaluate m retrievals from a variety of 
different systems in this case we might take a document a 
compare its rank in the retrieval to its average rank in the 
m retrievals if we believe that the m retrievals provide a 
satisfactory approximation to relevance then a very large 
difference in rank would suggest that our retrieval is 
misranking a if this difference is large on average over all 
n documents then we might predict that the retrieval is 
bad if on the other hand the retrieval is very consistent 
with the m retrievals then we might predict that the 
retrieval is good the similarity between the retrieval and 
the combined retrieval may be computed using some 
correlation measure this is depicted in figure c in previous 
work the kullback-leibler divergence between the 
normalized scores of the retrieval and the normalized scores of the 
combined retrieval provides the similarity 
 experiments 
our experiments focus on testing the predictive power of 
each of our predictors ρ y ˜y ρ y yµ and ρ ˜y yµ as 
stated in section we are interested in predicting the 
performance of the retrieval generated by an arbitrary system 
our methodology is consistent with previous research in that 
we predict the relative performance of a retrieval by 
comparing a ranking based on our predictor to a ranking based on 
average precision 
we present results for two sets of experiments the first 
set of experiments presents detailed comparisons of our 
predictors to previously-proposed predictors using identical data 
sets our second set of experiments demonstrates the 
generalizability of our approach to arbitrary retrieval methods 
corpus types and corpus languages 
 detailed experiments 
in these experiments we will predict the performance of 
language modeling scores using our autocorrelation 
predictor ρ y ˜y we do not consider ρ y yµ or ρ ˜y yµ 
because in these detailed experiments we focus on ranking 
the retrievals from a single system we use retrievals values 
for baseline predictors and evaluation measures reported in 
previous work 
 topics and collections 
these performance prediction experiments use language 
model retrievals performed for queries associated with 
collections in the trec corpora using trec collections 
allows us to confidently associate an average precision with a 
retrieval in these experiments we use the following topic 
collections trec ad-hoc trec ad-hoc robust 
terabyte and terabyte 
 baselines 
we provide two baselines our first baseline is the 
classic clarity predictor presented in equation clarity is 
designed to be used with language modeling systems our 
second baseline is zhou and croft s ranking robustness 
predictor this predictor corrupts the top k documents 
from retrieval and re-computes the language model scores 
for these corrupted documents the value of the predictor 
is the spearman rank correlation between the original 
ranking and the corrupted ranking in our tables we will label 
results for clarity using dv 
kl and the ranking robustness 
predictor using p 
 generalizability experiments 
our predictors do not require a particular baseline 
retrieval system the predictors can be computed for an 
arbitrary retrieval regardless of how scores were generated we 
believe that that is one of the most attractive aspects of our 
algorithm therefore in a second set of experiments we 
demonstrate the ability of our techniques to generalize to a 
variety of collections topics and retrieval systems 
 topics and collections 
we gathered a diverse set of collections from all possible 
trec corpora we cast a wide net in order to locate 
collections where our predictors might fail our hypothesis is that 
documents with high topical similarity should have 
correlated scores therefore we avoided collections where scores 
were unlikely to be correlated eg question-answering or 
were likely to be negatively correlated eg novelty 
nevertheless our collections include corpora where correlations 
are weakly justified eg non-english corpora or not 
justified at all eg expert search we use the ad-hoc tracks from 
trec - trec robust - trec terabyte 
 trec - spanish trec - chinese and trec 
enterprise expert search in all cases we use only the 
automatic runs for ad-hoc tracks submitted to nist 
for all english and spanish corpora we construct the 
matrix w according to the process described in section for 
chinese corpora we use na¨ıve character-based tf idf vectors 
for entities entries in w are proportional to the number of 
documents in which two entities cooccur 
 baselines 
in our detailed experiments we used the clarity measure 
as a baseline since we are predicting the performance of 
retrievals which are not based on language modeling we 
use a version of clarity referred to as ranked-list clarity 
 ranked-list clarity converts document ranks to p q θi 
values this conversion begins by replacing all of the scores 
in y with the respective ranks our estimation of p q θi 
from the ranks then is 
p q θi 
 
 c −yi 
c c 
if yi ≤ c 
 otherwise 
 
where c is a cutoff parameter as suggested by the authors 
we fix the algorithm parameters c and λ so that c 
and λ we use equation to estimate p w θq and 
dv 
kl θq θc to compute the value of the predictor we 
will refer to this predictor as dv 
kl superscripted by v to 
indicate that the kullback-leibler divergence is with respect 
to the term embedding space 
when information from multiple runs on the same query is 
available we use aslam and pavlu s document-space 
multinomial divergence as a baseline this rank-based method 
first normalizes the scores in a retrieval as an n-dimensional 
multinomial as with ranked-list clarity we begin by 
replacing all of the scores in y with their respective ranks 
then we adjust the elements of y in the following way 
ˆyi 
 
 n 
 
  
nx 
k yi 
 
k 
 
a 
in our multirun experiments we only use the top 
documents from each retrieval n this is within the range 
of parameter values suggested by the authors however we 
admit not tuning this parameter for either our system or the 
baseline the predictor is the divergence between the 
candidate distribution y and the mean distribution yµ with 
the uniform linear combination of these m retrievals 
represented as yµ we can compute the divergence as dn 
kl ˆy ˆyµ 
where we use the superscript n to indicate that the 
summation is over the set of n documents this baseline was 
developed in the context of predicting query difficulty but 
we adopt it as a reasonable baseline for predicting retrieval 
performance 
 parameter settings 
when given multiple retrievals we use documents in the 
union of the top k documents from each of the m 
retrievals for that query if the size of this union is ˜n then 
yµ and each yi is of length ˜n in some cases a system 
did not score a document in the union since we are 
making a gaussian assumption about our scores we can sample 
scores for these unseen documents from the negative tail 
of the distribution specifically we sample from the part 
of the distribution lower than the minimum value of in the 
normalized retrieval this introduces randomness into our 
algorithm but we believe it is more appropriate than 
assigning an arbitrary fixed value 
we optimized the linear regression using the square root 
of each predictor we found that this substantially improved 
fits for all predictors including the baselines we considered 
linear combinations of pairs of predictors labeled by the 
components and all predictors labeled as β 
 evaluation 
given a set of retrievals potentially from a combination 
of queries and systems we measure the correlation of the 
rank ordering of this set by the predictor and by the 
performance metric in order to ensure comparability with 
previous results we present kendall s τ correlation between the 
predictor s ranking and ranking based on average precision 
of the retrieval unless explicitly noted all correlations are 
significant with p 
predictors can sometimes perform better when linearly 
combined although previous work has presented 
the coefficient of determination r 
 to measure the quality 
of the regression this measure cannot be reliably used when 
comparing slight improvements from combining predictors 
therefore we adopt the adjusted coefficient of 
determination which penalizes models with more variables the 
adjusted r 
allows us to evaluate the improvement in 
prediction achieved by adding a parameter but loses the statistical 
interpretation of r 
 we will use kendall s τ to evaluate the 
magnitude of the correlation and the adjusted r 
to 
evaluate the combination of variables 
 results 
we present results for our detailed experiments comparing 
the prediction of language model scores in table although 
the clarity measure is theoretically designed for language 
model scores it consistently underperforms our system-agnostic 
predictor ranking robustness was presented as an 
improvement to clarity for web collections represented in our 
experiments by the terabyte and terabyte collections 
shifting the τ correlation from to for terabyte and 
 to for terabyte however these improvements 
are slight compared to the performance of autocorrelation 
on these collections our predictor achieves a τ correlation 
of for terabyte and for terabyte though 
not always the strongest autocorrelation achieves 
correlations competitive with baseline predictors when 
examining the performance of linear combinations of predictors we 
note that in every case autocorrelation factors as a 
necessary component of a strong predictor we also note that the 
adjusted r 
for individual baselines are always significantly 
improved by incorporating autocorrelation 
we present our generalizability results in table we 
begin by examining the situation in column a where we 
are presented with a single retrieval and no information 
from additional retrievals for every collection except one 
we achieve significantly better correlations than ranked-list 
clarity surprisingly we achieve relatively strong 
correlations for spanish and chinese collections despite our na¨ıve 
processing we do not have a ranked-list clarity correlation 
for ent because entity modeling is itself an open research 
question however our autocorrelation measure does not 
achieve high correlations perhaps because relevance for 
entity retrieval does not propagate according to the 
cooccurrence links we use 
as noted above the poor clarity performance on web 
data is consistent with our findings in the detailed 
experiments clarity also notably underperforms for several news 
corpora trec trec and robust on the other hand 
autocorrelation seems robust to the changes between different 
corpora 
next we turn to the introduction of information from 
multiple retrievals we compare the correlations between 
those predictors which do not use this information in column 
 a and those which do in column b for every collection 
the predictors in column b outperform the predictors in 
column a indicating that the information from additional 
runs can be critical to making good predictions 
inspecting the predictors in column b we only draw 
weak conclusions our new predictors tend to perform 
better on news corpora and between our new predictors the 
hybrid ρ ˜y yµ predictor tends to perform better recall 
that our ρ ˜y yµ measure incorporates both spatial and 
multiple retrieval information therefore we believe that 
the improvement in correlation is the result of 
incorporating information from spatial behavior 
in column c we can investigate the utility of 
incorporating spatial information with information from 
multiple retrievals notice that in the cases where 
autocorrelation ρ y ˜y alone performs well trec trec -spanish and 
trec -chinese it is substantially improved by 
incorporating multiple-retrieval information from ρ y yµ in the 
linear regression β in the cases where ρ y yµ performs well 
incorporating autocorrelation rarely results in a significant 
improvement in performance in fact in every case where 
our predictor outperforms the baseline it includes 
information from multiple runs 
 discussion 
the most important result from our experiments involves 
prediction when no information is available from multiple 
runs tables and a this situation arises often in system 
design for example a system may need to at retrieval 
time assess its performance before deciding to conduct more 
intensive processing such as pseudo-relevance feedback or 
interaction assuming the presence of multiple retrievals is 
unrealistic in this case 
we believe that autocorrelation is like multiple-retrieval 
algorithms approximating a good ranking in this case by 
diffusing scores why is ˜y a reasonable surrogate we know 
that diffusion of scores on the web graph and language model 
graphs improves performance therefore if score 
diffusion tends to in general improve performance then 
diffused scores will in general provide a good surrogate for 
relevance our results demonstrate that this approximation 
is not as powerful as information from multiple retrievals 
nevertheless in situations where this information is lacking 
autocorrelation provides substantial information 
the success of autocorrelation as a predictor may also 
have roots in the clustering hypothesis recall that we 
regard autocorrelation as the degree to which a retrieval 
satisfies the clustering hypothesis our experiments then 
demonstrate that a failure to respect the clustering 
hypothesis correlates with poor performance why might systems 
fail to conform to the cluster hypothesis query-based 
information retrieval systems often score documents 
independently the score of document a may be computed by 
examining query term or phrase matches the document length 
and perhaps global collection statistics once computed 
a system rarely compares the score of a to the score of a 
topically-related document b with some exceptions the 
correlation of document scores has largely been ignored 
we should make it clear that we have selected tasks where 
topical autocorrelation is appropriate there are certainly 
cases where there is no reason to believe that retrieval scores 
will have topical autocorrelation for example ranked lists 
which incorporate document novelty should not exhibit 
spatial autocorrelation if anything autocorrelation should be 
negative for this task similarly answer candidates in a 
question-answering task may or may not exhibit 
autocorrelation in this case the semantics of links is questionable 
too it is important before applying this measure to confirm 
that given the semantics for some link between two retrieved 
items we should expect a correlation between scores 
 related work 
in this section we draw more general comparisons to other 
work in performance prediction and spatial data analysis 
there is a growing body of work which attempts to predict 
the performance of individual retrievals we 
have attempted to place our work in the context of much of 
this work in section however a complete comparison is 
beyond the scope of this paper we note though that our 
experiments cover a larger and more diverse set of retrievals 
collections and topics than previously examined 
much previous work-particularly in the context of 
trecfocuses on predicting the performance of systems here 
each system generates k retrievals the task is given these 
retrievals to predict the ranking of systems according to 
some performance measure several papers attempt to 
address this task under the constraint of few judgments 
some work even attempts to use zero judgments by 
leveraging multiple retrievals for the same query our task 
differs because we focus on ranking retrievals independent 
of the generating system the task here is not to test the 
hypothesis system a is superior to system b but to test 
the hypothesis retrieval a is superior to retrieval b 
autocorrelation manifests itself in many classification tasks 
neville and jensen define relational autocorrelation for 
relational learning problems and demonstrate that many 
classification tasks manifest autocorrelation temporal 
autocorrelation of initial retrievals has also been used to predict 
performance however temporal autocorrelation is 
performed by projecting the retrieval function into the temporal 
embedding space in our work we focus on the behavior of 
the function over the relationships between documents 
τ adjusted r 
dv 
kl p ρ y ˜y dv 
kl p ρ y ˜y dv 
kl p dv 
kl ρ y ˜y pρ y ˜y β 
trec 
trec 
robust 
terabyte 
terabyte 
table comparison to robustness and clarity measures for language model scores evaluation replicates 
experiments from we present correlations between the classic clarity measure dv 
kl the ranking 
robustness measure p and autocorrelation ρ y ˜y each with mean average precision in terms of kendall s 
τ the adjusted coefficient of determination is presented to measure the effectiveness of combining predictors 
measures in bold represent the strongest correlation for that test collection pair 
multiple run 
 a b c 
τ τ adjusted r 
dkl ρ y ˜y dn 
kl ρ y yµ ρ ˜y yµ dn 
kl ρ y ˜y ρ y yµ ρ ˜y yµ β 
trec 
trec 
trec 
trec 
trec 
trec 
robust 
robust 
robust 
terabyte 
terabyte 
trec -spanish 
trec -spanish 
trec -chinese 
trec -chinese 
ent - 
table large scale prediction experiments we predict the ranking of large sets of retrievals for various 
collections and retrieval systems kendall s τ correlations are computed between the predicted ranking and 
a ranking based on the retrieval s average precision in column a we have predictors which do not use 
information from other retrievals for the same query in columns b and c we present performance for 
predictors which incorporate information from multiple retrievals the adjusted coefficient of determination 
is computed to determine effectiveness of combining predictors measures in bold represent the strongest 
correlation for that test collection pair 
finally regularization-based re-ranking processes are also 
closely-related to our work these techniques seek to 
maximize the agreement between scores of related 
documents by solving a constrained optimization problem the 
maximization of consistency is equivalent to maximizing the 
moran autocorrelation therefore we believe that our work 
provides explanation for why regularization-based re-ranking 
works 
 conclusion 
we have presented a new method for predicting the 
performance of a retrieval ranking without any relevance 
judgments we consider two cases first when making 
predictions in the absence of retrievals from other systems our 
predictors demonstrate robust strong correlations with 
average precision this performance combined with a simple 
implementation makes our predictors in particular very 
attractive we have demonstrated this improvement for many 
diverse settings to our knowledge this is the first large 
scale examination of zero-judgment single-retrieval 
performance prediction second when provided retrievals from 
other systems our extended methods demonstrate 
competitive performance with state of the art baselines our 
experiments also demonstrate the limits of the usefulness of our 
predictors when information from multiple runs is provided 
our results suggest two conclusions first our results 
could affect retrieval algorithm design retrieval algorithms 
designed to consider spatial autocorrelation will conform to 
the cluster hypothesis and improve performance second 
our results could affect the design of minimal test collection 
algorithms much of the recent work in ranking systems 
sometimes ignores correlations between document labels and 
scores we believe that these two directions could be 
rewarding given the theoretical and experimental evidence in this 
paper 
 acknowledgments 
this work was supported in part by the center for 
intelligent information retrieval and in part by the defense 
advanced research projects agency darpa under 
contract number hr - -c- any opinions findings 
and conclusions or recommendations expressed in this 
material are the author s and do not necessarily reflect those 
of the sponsor we thank yun zhou and desislava petkova 
for providing data and andre gauthier for technical 
assistance 
 references 
 j aslam and v pavlu query hardness estimation using 
jensen-shannon divergence among multiple scoring 
functions in ecir proceedings of the th european 
conference on information retrieval 
 j a aslam v pavlu and e yilmaz a statistical method 
for system evaluation using incomplete judgments in 
s dumais e n efthimiadis d hawking and k jarvelin 
editors proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - acm press august 
 
 d carmel e yom-tov a darlow and d pelleg what 
makes a query difficult in sigir proceedings of the 
 th annual international acm sigir conference on 
research and development in information retrieval pages 
 - new york ny usa acm press 
 b carterette j allan and r sitaraman minimal test 
collections for retrieval evaluation in sigir 
proceedings of the th annual international acm sigir 
conference on research and development in information 
retrieval pages - new york ny usa acm 
press 
 a d cliff and j k ord spatial autocorrelation pion 
ltd 
 m connell a feng g kumaran h raghavan c shah 
and j allan umass at tdt technical report ciir 
technical report ir - department of computer 
science university of massachusetts 
 s cronen-townsend y zhou and w b croft precision 
prediction based on ranked list coherence inf retr 
 - 
 f diaz regularizing ad-hoc retrieval scores in cikm 
proceedings of the th acm international conference on 
information and knowledge management pages - 
new york ny usa acm press 
 f diaz and r jones using temporal profiles of queries for 
precision prediction in sigir proceedings of the th 
annual international acm sigir conference on research 
and development in information retrieval pages - 
new york ny usa acm press 
 d a griffith spatial autocorrelation and spatial 
filtering springer verlag 
 b he and i ounis inferring query performance using 
pre-retrieval predictors in the eleventh symposium on 
string processing and information retrieval spire 
 
 n jardine and c j v rijsbergen the use of hierarchic 
clustering in information retrieval information storage and 
retrieval - 
 d jensen and j neville linkage and autocorrelation cause 
feature selection bias in relational learning in icml 
proceedings of the nineteenth international conference on 
machine learning pages - san francisco ca 
usa morgan kaufmann publishers inc 
 o kurland and l lee corpus structure language models 
and ad-hoc information retrieval in sigir 
proceedings of the th annual international conference on 
research and development in information retrieval pages 
 - new york ny usa acm press 
 m montague and j a aslam relevance score 
normalization for metasearch in cikm proceedings of 
the tenth international conference on information and 
knowledge management pages - new york ny 
usa acm press 
 t qin t -y liu x -d zhang z chen and w -y ma a 
study of relevance propagation for web search in sigir 
 proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - new york ny usa 
 acm press 
 i soboroff c nicholas and p cahan ranking retrieval 
systems without relevance judgments in sigir 
proceedings of the th annual international acm sigir 
conference on research and development in information 
retrieval pages - new york ny usa acm 
press 
 v vinay i j cox n milic-frayling and k wood on 
ranking the effectiveness of searches in sigir 
proceedings of the th annual international acm sigir 
conference on research and development in information 
retrieval pages - new york ny usa acm 
press 
 y zhou and w b croft ranking robustness a novel 
framework to predict query performance in cikm 
proceedings of the th acm international conference on 
information and knowledge management pages - 
new york ny usa acm press 
an outranking approach for rank aggregation in 
information retrieval 
mohamed farah 
lamsade paris dauphine university 
place du mal de lattre de tassigny 
 paris cedex france 
farah lamsade dauphine fr 
daniel vanderpooten 
lamsade paris dauphine university 
place du mal de lattre de tassigny 
 paris cedex france 
vdp lamsade dauphine fr 
abstract 
research in information retrieval usually shows performance 
improvement when many sources of evidence are combined 
to produce a ranking of documents e g texts pictures 
sounds etc in this paper we focus on the rank aggregation 
problem also called data fusion problem where rankings of 
documents searched into the same collection and provided 
by multiple methods are combined in order to produce a 
new ranking in this context we propose a rank aggregation 
method within a multiple criteria framework using 
aggregation mechanisms based on decision rules identifying positive 
and negative reasons for judging whether a document should 
get a better rank than another we show that the proposed 
method deals well with the information retrieval distinctive 
features experimental results are reported showing that 
the suggested method performs better than the well-known 
combsum and combmnz operators 
categories and subject descriptors h 
 information systems information search and retrieval - 
retrieval models 
general terms algorithms measurement 
experimentation performance theory 
 introduction 
a wide range of current information retrieval ir 
approaches are based on various search models boolean 
vector space probabilistic language etc in order to 
retrieve relevant documents in response to a user request the 
result lists produced by these approaches depend on the 
exact definition of the relevance concept 
rank aggregation approaches also called data fusion 
approaches consist in combining these result lists in order 
to produce a new and hopefully better ranking such 
approaches give rise to metasearch engines in the web context 
we consider in the following cases where only ranks are 
available and no other additional information is provided 
such as the relevance scores this corresponds indeed to the 
reality where only ordinal information is available 
data fusion is also relevant in other contexts such as when 
the user writes several queries of his her information need 
 e g a boolean query and a natural language query or 
when many document surrogates are available 
several studies argued that rank aggregation has the 
potential of combining effectively all the various sources of 
evidence considered in various input methods for instance 
experiments carried out in and showed that 
documents which appear in the lists of the majority of the 
input methods are more likely to be relevant moreover lee 
 and vogt and cottrell found that various retrieval 
approaches often return very different irrelevant documents 
but many of the same relevant documents bartell et al 
 also found that rank aggregation methods improve the 
performances w r t those of the input methods even when 
some of them have weak individual performances these 
methods also tend to smooth out biases of the input 
methods according to montague and aslam data fusion has 
recently been proved to improve performances for both the 
ad-hoc retrieval and categorization tasks within the trec 
genomics track in 
the rank aggregation problem was addressed in various 
fields such as i in social choice theory which studies 
voting algorithms which specify winners of elections or winners 
of competitions in tournaments ii in statistics when 
studying correlation between rankings iii in distributed 
databases when results from different databases must be 
combined and iv in collaborative filtering 
most current rank aggregation methods consider each 
input ranking as a permutation over the same set of items 
they also give rigid interpretation to the exact ranking of 
the items both of these assumptions are rather not valid in 
the ir context as will be shown in the following sections 
the remaining of the paper is organized as follows we 
first review current rank aggregation methods in section 
then we outline the specificities of the data fusion problem 
in the ir context section in section we present a 
new aggregation method which is proven to best fit the ir 
context experimental results are presented in section and 
conclusions are provided in a final section 
 related work 
as pointed out by riker we can distinguish two 
families of rank aggregation methods positional methods which 
assign scores to items to be ranked according to the ranks 
they receive and majoritarian methods which are based on 
pairwise comparisons of items to be ranked these two 
families of methods find their roots in the pioneering works of 
borda and condorcet respectively in the social choice 
literature 
 preliminaries 
we first introduce some basic notations to present the 
rank aggregation methods in a uniform way let d 
{d d dnd } be a set of nd documents a list or a 
ranking j is an ordering defined on dj ⊆ d j n 
thus di j di means di  is ranked better than di in j 
when dj d j is said to be a full list otherwise it 
is a partial list if di belongs to dj rj 
i denotes the rank 
or position of di in j we assume that the best answer 
 document is assigned the position and the worst one is 
assigned the position dj let d be the set of all 
permutations on d or all subsets of d a profile is a n-tuple 
of rankings pr n restricting pr to the 
rankings containing document di defines pri we also call 
the number of rankings which contain document di the rank 
hits of di 
the rank aggregation or data fusion problem consists of 
finding a ranking function or mechanism ψ also called a 
social welfare function in the social choice theory terminology 
defined by 
ψ 
n 
d → d 
pr n → σ ψ pr 
where σ is called a consensus ranking 
 positional methods 
 borda count 
this method first assigns a score n 
j rj 
i to each 
document di documents are then ranked by increasing order 
of this score breaking ties if any arbitrarily 
 linear combination methods 
this family of methods basically combine scores of 
documents when used for the rank aggregation problem ranks 
are assumed to be scores or performances to be combined 
using aggregation operators such as the weighted sum or 
some variation of it 
for instance callan et al used the inference 
networks model to combine rankings fox and shaw 
proposed several combination strategies which are 
combsum combmin combmax combanz and combmnz 
the first three operators correspond to the sum min and 
max operators respectively combanz and combmnz 
respectively divides and multiplies the combsum score by 
the rank hits it is shown in that the combsum and 
combmnz operators perform better than the others 
metasearch engines such as savvysearch and metacrawler use 
the combsum strategy to fuse rankings 
 footrule optimal aggregation 
in this method a consensus ranking minimizes the 
spearman footrule distance from the input rankings 
formally given two full lists j and j this distance is given 
by f j j nd 
i rj 
i − rj 
i it extends to several lists 
as follows given a profile pr and a consensus ranking 
σ the spearman footrule distance of σ to pr is given by 
f σ pr n 
j f σ j 
cook and kress proposed a similar method which 
consists in optimizing the distance d j j 
 
nd 
i i rj 
i i − 
rj 
i i where rj 
i i rj 
i −rj 
i this formulation has the 
advantage that it considers the intensity of preferences 
 probabilistic methods 
this kind of methods assume that the performance of the 
input methods on a number of training queries is indicative 
of their future performance during the training process 
probabilities of relevance are calculated for subsequent 
queries documents are ranked based on these probabilities 
for instance in each input ranking j is divided into a 
number of segments and the conditional probability of 
relevance r of each document di depending on the segment 
k it occurs in is computed i e prob r di k j for 
subsequent queries the score of each document di is given by 
n 
j 
prob r di k j 
k 
 le calve and savoy suggest using 
a logistic regression approach for combining scores training 
data is needed to infer the model parameters 
 majoritarian methods 
 condorcet procedure 
the original condorcet rule specifies that a winner of 
the election is any item that beats or ties with every other 
item in a pairwise contest formally let c diσdi { j∈ 
pr di j di } be the coalition of rankings that are 
concordant with establishing diσdi i e with the proposition 
di  should be ranked better than di in the final ranking σ 
di beats or ties with di iff c diσdi ≥ c di σdi 
the repetitive application of the condorcet algorithm can 
produce a ranking of items in a natural way select the 
condorcet winner remove it from the lists and repeat the 
previous two steps until there are no more documents to rank 
since there is not always condorcet winners variations of 
the condorcet procedure have been developed within the 
multiple criteria decision aid theory with methods such as 
electre 
 kemeny optimal aggregation 
as in section a consensus ranking minimizes a 
geometric distance from the input rankings where the kendall 
tau distance is used instead of the spearman footrule 
distance formally given two full lists j and j the kendall 
tau distance is given by k j j { di di i i rj 
i 
rj 
i rj 
i rj 
i } i e the number of pairwise disagreements 
between the two lists it is easy to show that the consensus 
ranking corresponds to the geometric median of the input 
rankings and that the kemeny optimal aggregation problem 
corresponds to the minimum feedback edge set problem 
 markov chain methods 
markov chains mcs have been used by dwork et al 
as a  natural method to obtain a consensus ranking where 
states correspond to the documents to be ranked and the 
transition probabilities vary depending on the interpretation 
of the transition event in the same reference the authors 
proposed four specific mcs and experimental testing had 
shown that the following mc is the best performing one 
 see also 
 mc move from the current state di to the next state 
di by first choosing a document di uniformly from d 
if for the majority of the rankings we have rj 
i ≤ rj 
i 
then move to di else stay in di 
the consensus ranking corresponds to the stationary 
distribution of mc 
 specificities of the rank 
aggregation problem in the ir context 
 limited significance of the rankings 
the exact positions of documents in one input ranking 
have limited significance and should not be overemphasized 
for instance having three relevant documents in the first 
three positions any perturbation of these three items will 
have the same value indeed in the ir context the complete 
order provided by an input method may hide ties in this 
case we call such rankings semi orders this was outlined in 
 as the problem of aggregation with ties it is therefore 
important to build the consensus ranking based on robust 
information 
 documents with near positions in j are more likely 
to have similar interest or relevance thus a slight 
perturbation of the initial ranking is meaningless 
 assuming that document di is better ranked than 
document di in a ranking j di is more likely to be 
definitively more relevant than di in j when the number 
of intermediate positions between di and di increases 
 partial lists 
in real world applications such as metasearch engines 
rankings provided by the input methods are often partial 
lists this was outlined in as the problem of having to 
merge top-k results from various input lists for instance 
in the experiments carried out by dwork et al authors 
found that among the top best documents of input 
search engines of the documents were present in only 
one search engine whereas less than two documents were 
present in all the search engines 
rank aggregation of partial lists raises four major 
difficulties which we state hereafter proposing for each of them 
various working assumptions 
 partial lists can have various lengths which can favour 
long lists we thus consider the following two working 
hypotheses 
h 
k we only consider the top k best documents from 
each input ranking 
h 
all we consider all the documents from each input 
ranking 
 since there are different documents in the input 
rankings we must decide which documents should be kept 
in the consensus ranking two working hypotheses are 
therefore considered 
h 
k we only consider documents which are present in 
at least k input rankings k 
h 
all we consider all the documents which are ranked 
in at least one input ranking 
hereafter we call documents which will be retained 
in the consensus ranking candidate documents and 
documents that will be excluded from the consensus 
ranking excluded documents we also call a candidate 
document which is missing in one or more rankings a 
missing document 
 some candidate documents are missing documents in 
some input rankings main reasons for a missing 
document are that it was not indexed or it was indexed 
but deemed irrelevant usually this information is not 
available we consider the following two working 
hypotheses 
h 
yes each missing document in each j is assigned 
a position 
h 
no no assumption is made that is each missing 
document is considered neither better nor worse than any 
other document 
 when assumption h 
k holds each input ranking may 
contain documents which will not be considered in the 
consensus ranking regarding the positions of the 
candidate documents we can consider the following 
working hypotheses 
h 
init the initial positions of candidate documents 
are kept in each input ranking 
h 
new candidate documents receive new positions in 
each input ranking after discarding excluded ones 
in the ir context rank aggregation methods need to 
decide more or less explicitly which assumptions to retain 
w r t the above-mentioned difficulties 
 outranking approach for rank 
aggregation 
 presentation 
positional methods consider implicitly that the positions 
of the documents in the input rankings are scores giving thus 
a cardinal meaning to an ordinal information this 
constitutes a strong assumption that is questionable especially 
when the input rankings have different lengths moreover 
for positional methods assumptions h 
and h 
 which are 
often arbitrary have a strong impact on the results for 
instance let us consider an input ranking of documents 
out of candidate documents whether we assign to 
each of the missing documents the position or 
 -corresponding to variations of h 
yes- will give rise to 
very contrasted results especially regarding the top of the 
consensus ranking 
majoritarian methods do not suffer from the 
above-mentioned drawbacks of the positional methods since they build 
consensus rankings exploiting only ordinal information 
contained in the input rankings nevertheless they suppose 
that such rankings are complete orders ignoring that they 
may hide ties therefore majoritarian methods base 
consensus rankings on illusory discriminant information rather 
than less discriminant but more robust information 
trying to overcome the limits of current rank aggregation 
methods we found that outranking approaches which were 
initially used for multiple criteria aggregation problems 
can also be used for the rank aggregation purpose where 
each ranking plays the role of a criterion therefore in 
order to decide whether a document di should be ranked 
better than di in the consensus ranking σ the two following 
conditions should be met 
 a concordance condition which ensures that a 
majority of the input rankings are concordant with diσdi 
 majority principle 
 a discordance condition which ensures that none of the 
discordant input rankings strongly refutes dσd 
 respect of minorities principle 
formally the concordance coalition with diσdi is 
csp diσdi { j∈ pr rj 
i ≤ rj 
i − sp} 
where sp is a preference threshold which is the variation 
of document positions -whether it is absolute or relative to 
the ranking length- which draws the boundaries between an 
indifference and a preference situation between documents 
the discordance coalition with diσdi is 
dsv diσdi { j∈ pr rj 
i ≥ rj 
i sv} 
where sv is a veto threshold which is the variation of 
document positions -whether it is absolute or relative to the 
ranking length- which draws the boundaries between a weak 
and a strong opposition to diσdi 
depending on the exact definition of the preceding 
concordance and discordance coalitions leading to the definition 
of some decision rules several outranking relations can be 
defined they can be more or less demanding depending on 
i the values of the thresholds sp and sv ii the importance 
or minimal size cmin required for the concordance coalition 
and iii the importance or maximum size dmax of the 
discordance coalition 
a generic outranking relation can thus be defined as 
follows 
dis sp sv cmin dmax di ⇔ csp diσdi ≥ cmin 
and dsv diσdi ≤ dmax 
this expression defines a family of nested outranking 
relations since s sp sv cmin dmax ⊆ s sp sv cmin dmax when 
cmin ≥ cmin and or dmax ≤ dmax and or sp ≥ sp and or 
sv ≤ sv this expression also generalizes the majority rule 
which corresponds to the particular relation s ∞ n 
 
 n it 
also satisfies important properties of rank aggregation 
methods called neutrality pareto-optimality condorcet 
property and extended condorcet property in the social choice 
literature 
outranking relations are not necessarily transitive and do 
not necessarily correspond to rankings since directed cycles 
may exist therefore we need specific procedures in order to 
derive a consensus ranking we propose the following 
procedure which finds its roots in it consists in partitioning 
the set of documents into r ranked classes 
each class ch contains documents with the same relevance 
and results from the application of all relations if possible 
to the set of documents remaining after previous classes are 
computed documents within the same equivalence class are 
ranked arbitrarily 
formally let 
 r be the set of candidate documents for a query 
 s 
 s 
 be a family of nested outranking relations 
 fk di e {di ∈ e disk 
di } be the number of 
documents in e e ⊆ r that could be considered 
 worse than di according to relation sk 
 
 fk di e {di ∈ e di sk 
di} be the number of 
documents in e that could be considered  better than 
di according to sk 
 
 sk di e fk di e − fk di e be the qualification 
of di in e according to sk 
 
each class ch results from a distillation process it 
corresponds to the last distillate of a series of sets e ⊇ e ⊇ 
where e r \ c ∪ ∪ ch− and ek is a reduced 
subset of ek− resulting from the application of the following 
procedure 
 compute for each di ∈ ek− its qualification according 
to sk 
 i e sk di ek− 
 define smax maxdi∈ek− {sk di ek− } then 
 ek {di ∈ ek− sk di ek− smax} 
when one outranking relation is used the distillation 
process stops after the first application of the previous 
procedure i e ch corresponds to distillate e when different 
outranking relations are used the distillation process stops 
when all the pre-defined outranking relations have been used 
or when ek 
 illustrative example 
this section illustrates the concepts and procedures of 
section let us consider a set of candidate documents 
r {d d d d d } the following table gives a profile 
pr of different rankings of the documents of r pr 
 
table rankings of documents 
rj 
i 
d 
d 
d 
d 
d 
let us suppose that the preference and veto thresholds 
are set to values and respectively and that the 
concordance and discordance thresholds are set to values and 
respectively the following tables give the concordance 
discordance and outranking matrices each entry csp di di 
 dsv di di in the concordance discordance matrix gives 
the number of rankings that are concordant discordant 
with diσdi i e csp di di csp diσdi and dsv di di 
 dsv diσdi 
table computation of the outranking relation 
d d d d d 
d - 
d - 
d - 
d - 
d 
 concordance matrix 
d d d d d 
d - 
d - 
d - 
d - 
d 
 discordance matrix 
d d d d d 
d - 
d - 
d - 
d - 
d 
 outranking matrix s 
for instance the concordance coalition for the assertion 
d σd is c d σd { } and the discordance 
coalition for the same assertion is d d σd ∅ 
therefore c d d d d d and d s 
d holds 
notice that fk di r fk di r is given by summing the 
values of the ith 
row column of the outranking matrix the 
consensus ranking is obtained as follows to get the first class 
c we compute the qualifications of all the documents of 
e r with respect to s 
 they are respectively - 
and - therefore smax equals and c e {d d d } 
observe that if we had used a second outranking relation 
s ⊇ s these three documents could have been 
possibly discriminated at this stage we remove documents of 
c from the outranking matrix and compute the next class 
c we compute the new qualifications of the documents of 
e r \ c {d d } they are respectively and - so 
c e {d } the last document d is the only 
document of the last class c thus the consensus ranking is 
{d d d } → {d } → {d } 
 experiments and results 
 test setting 
to facilitate empirical investigation of the proposed 
methodology we developed a prototype metasearch engine that 
implements a version of our outranking approach for rank 
aggregation in this paper we apply our approach to the 
topic distillation td task of trec- web track 
in this task there are topics where only a short 
description of each is given for each query we retained the 
rankings of the best runs of the td task which are provided 
by trec- participating teams the performances of 
these runs are reported in table 
table performances of the best runs of the td 
task of trec- 
run id map p  s  s  s  
uogwebcau 
msramixed 
msrc c 
humw rdpl 
thuirmix 
uamst mwscb 
ict ciis at 
sjtuincmix 
mu web 
meijihilw 
average 
for each query each run provides a ranking of about 
documents the number of documents retrieved by all these 
runs ranges from to their average median 
number is it is worth noting that we found similar 
distributions of the documents among the rankings as in 
 
for evaluation we used the  trec eval standard tool which 
is used by the trec community to calculate the standard 
measures of system effectiveness which are mean average 
precision map and success n s n for n and 
our approach effectiveness is compared against some high 
performing official results from trec- as well as against 
some standard rank aggregation algorithms in the 
experiments significance testing is mainly based on the t-student 
statistic which is computed on the basis of the map values of 
the compared runs in the tables of the following section 
statistically significant differences are marked with an 
asterisk values between brackets of the first column of each 
table indicate the parameter value of the corresponding run 
 results 
we carried out several series of runs in order to i study 
performance variations of the outranking approach when 
tuning the parameters and working assumptions ii 
compare performances of the outranking approach vs standard 
rank aggregation strategies and iii check whether rank 
aggregation performs better than the best input rankings 
we set our basic run mcm with the following parameters 
we considered that each input ranking is a complete 
order sp and that an input ranking strongly refutes 
diσdi when the difference of both document positions is 
large enough sv preference and veto thresholds 
are computed proportionally to the number of documents 
retained in each input ranking they consequently may vary 
from one ranking to another in addition to accept the 
assertion diσdi we supposed that the majority of the 
rankings must be concordant cmin and that every input 
ranking can impose its veto dmax concordance and 
discordance thresholds are computed for each tuple di di 
as the percentage of the input rankings of pri ∩pri thus 
our choice of parameters leads to the definition of the 
outranking relation s 
to test the run mcm we had chosen the following 
assumptions we retained the top best documents from each 
input ranking h 
 only considered documents which are 
present in at least half of the input rankings h 
 and 
assumed h 
no and h 
new in these conditions the number of 
successful documents was about on average and the 
computation time per query was less than one second 
obviously modifying the working assumptions should have 
deeper impact on the performances than tuning our model 
parameters this was validated by preliminary experiments 
thus we hereafter begin by studying performance variation 
when different sets of assumptions are considered 
afterwards we study the impact of tuning parameters finally 
we compare our model performances w r t the input 
rankings as well as some standard data fusion algorithms 
 impact of the working assumptions 
table summarizes the performance variation of the 
outranking approach under different working hypotheses in 
table impact of the working assumptions 
run id map s  s  s  
mcm 
mcm h 
yes - 
mcm h 
init - 
mcm h 
all 
mcm h 
all 
this table we first show that run mcm in which missing 
documents are all put in the same last position of each input 
ranking leads to performance drop w r t run mcm 
moreover s  moves from to - this 
shows that several relevant documents which were initially 
put at the first position of the consensus ranking in mcm lose 
this first position but remain ranked in the top documents 
since s  did not change we also conclude that documents 
which have rather good positions in some input rankings are 
more likely to be relevant even though they are missing in 
some other rankings consequently when they are missing 
in some rankings assigning worse ranks to these documents 
is harmful for performance 
also from table we found that the performances of 
runs mcm and mcm are similar therefore the outranking 
approach is not sensitive to keeping the initial positions of 
candidate documents or recomputing them by discarding 
excluded ones 
from the same table performance of the outranking 
approach increases significantly for runs mcm and mcm 
therefore whether we consider all the documents which are 
present in half of the rankings mcm or we consider all 
the documents which are ranked in the first positions in 
one or more rankings mcm increases performances this 
result was predictable since in both cases we have more 
detailed information on the relative importance of documents 
tables and confirm this evidence table where 
values between brackets of the first column give the number 
of documents which are retained from each input ranking 
shows that selecting more documents from each input 
ranking leads to performance increase it is worth mentioning 
that selecting more than documents from each input 
ranking does not improve performance 
table impact of the number of retained 
documents 
run id map s  s  s  
mcm 
mcm - 
mcm - 
mcm - 
mcm - 
mcm 
table reports runs corresponding to variations of h 
k 
values between brackets are rank hits for instance in 
the run mcm only documents which are present in or 
more input rankings were considered successful this 
table shows that performance is significantly better when rare 
documents are considered whereas it decreases significantly 
when these documents are discarded therefore we 
conclude that many of the relevant documents are retrieved by 
a rather small set of ir models 
table performance considering different rank hits 
run id map s  s  s  
mcm 
mcm 
mcm 
mcm - 
mcm - 
mcm - 
for both runs mcm and mcm the number of successful 
documents was about and therefore the computation 
time per query increased and became around seconds 
 impact of the variation of the parameters 
table shows performance variation of the outranking 
approach when different preference thresholds are considered 
we found performance improvement up to threshold values 
of about then there is a decrease in the performance 
which becomes significant for threshold values greater than 
 moreover s  improves from to when 
preference threshold changes from to we can thus 
conclude that the input rankings are semi orders rather than 
complete orders 
table shows the evolution of the performance measures 
w r t the concordance threshold we can conclude that in 
order to put document di before di in the consensus ranking 
table impact of the variation of the preference 
threshold from to 
run id map s  s  s  
mcm 
mcm 
mcm 
mcm 
mcm - 
mcm - 
mcm b - 
at least half of the input rankings of pri ∩ pri should be 
concordant performance drops significantly for very low 
and very high values of the concordance threshold in fact 
for such values the concordance condition is either fulfilled 
rather always by too many document pairs or not fulfilled at 
all respectively therefore the outranking relation becomes 
either too weak or too strong respectively 
table impact of the variation of cmin 
run id map s  s  s  
mcm - 
mcm - 
mcm 
mcm - 
mcm - 
mcm - 
in the experiments varying the veto threshold as well as 
the discordance threshold within reasonable intervals does 
not have significant impact on performance measures in 
fact runs with different veto thresholds sv ∈ 
had similar performances even though there is a slight 
advantage for runs with high threshold values which means 
that it is better not to allow the input rankings to put their 
veto easily also tuning the discordance threshold was 
carried out for values and of the veto threshold for 
these runs we did not get any noticeable performance 
variation although for low discordance thresholds dmax 
performance slightly decreased 
 impact of the variation of the number of input 
rankings 
to study performance evolution when different sets of 
input rankings are considered we carried three more runs 
where and of the best performing sets of the 
input rankings are considered results reported in table 
are seemingly counter-intuitive and also do not support 
previous findings regarding rank aggregation research 
nevertheless this result shows that low performing rankings 
bring more noise than information to the establishment of 
the consensus ranking therefore when they are considered 
performance decreases 
table performance considering different best 
performing sets of input rankings 
run id map s  s  s  
mcm 
mcm 
mcm 
mcm - 
 comparison of the performance of different 
rank aggregation methods 
in this set of runs we compare the outranking approach 
with some standard rank aggregation methods which were 
proven to have acceptable performance in previous studies 
we considered two positional methods which are the 
combsum and the combmnz strategies we also examined 
the performance of one majoritarian method which is the 
markov chain method mc for the comparisons we 
considered a specific outranking relation s∗ 
 s 
which results in good overall performances when tuning all 
the parameters 
the first row of table gives performances of the rank 
aggregation methods w r t a basic assumption set a 
 h 
 h 
 h 
new we only consider the first documents 
from each ranking then retain documents present in or 
more rankings and update ranks of successful documents 
for positional methods we place missing documents at the 
queue of the ranking h 
yes whereas for our method as well 
as for mc we retained hypothesis h 
no the three 
following rows of table report performances when changing 
one element from the basic assumption set the second row 
corresponds to the assumption set a h 
 h 
 h 
new 
i e changing the number of retained documents from 
to the third row corresponds to the assumption set 
a h 
 h 
all h 
new i e considering the documents 
present in at least one ranking the fourth row corresponds 
to the assumption set a h 
 h 
 h 
init i e keeping 
the original ranks of successful documents 
the fifth row of table labeled a gives performance 
when all the queries of the web track of trec- are 
considered obviously performance level cannot be 
compared with previous lines since the additional queries are 
different from the td queries and correspond to other tasks 
 home page and named page tasks of trec- 
web track this set of runs aims to show whether relative 
performance of the various methods is task-dependent 
the last row of table labeled a reports performance 
of the various methods considering the td task of 
trec instead of trec- we fused the results of input 
rankings of the best official runs for each of the td 
queries considering the set of assumptions a of the first 
row this aims to show whether relative performance of the 
various methods changes from year to year 
values between brackets of table are variations of 
performance of each rank aggregation method w r t 
performance of the outranking approach 
table performance map of different rank 
aggregation methods under different test collections 
mcm combsum combmnz markov 
a - - - 
a - - - 
a - - - 
a - - - 
a - - - 
a - - - 
from the analysis of table the following can be 
established 
 for all the runs considering all the documents in each 
input ranking a significantly improves performance 
 map increases by on average this is 
predictable since some initially unreported relevant 
documents would receive better positions in the consensus 
ranking 
 for all the runs considering documents even those 
present in only one input ranking a significantly 
improves performance for mcm combsum and combmnz 
performance improvement is more important map 
increases by on average than for the markov run 
 map increases by 
 preserving the initial positions of documents a or 
recomputing them a does not have a noticeable 
influence on performance for both positional and 
majoritarian methods 
 considering all the queries of the web track of 
trec a as well as the td queries of the web track 
of trec- a does not alter the relative 
performance of the different data fusion methods 
 considering the td queries of the web track of 
trec performances of all the data fusion methods are 
lower than that of the best performing input ranking 
for which the map value equals this is because 
most of the fused input rankings have very low 
performances compared to the best one which brings more 
noise to the consensus ranking 
 performances of the data fusion methods mcm and markov 
are significantly better than that of the best input 
ranking uogwebcau this remains true for runs 
combsum and combmnz only under assumptions h 
all or 
h 
all this shows that majoritarian methods are less 
sensitive to assumptions than positional methods 
 outranking approach always performs significantly 
better than positional methods combsum and combmnz it 
has also better performances than the markov chain 
method especially under assumption h 
all where 
difference of performances becomes significant 
 conclusions 
in this paper we address the rank aggregation problem 
where different but not disjoint lists of documents are to 
be fused we noticed that the input rankings can hide ties 
so they should not be considered as complete orders only 
robust information should be used from each input ranking 
current rank aggregation methods and especially 
positional methods e g combsum are not initially 
designed to work with such rankings they should be adapted 
by considering specific working assumptions 
we propose a new outranking method for rank 
aggregation which is well adapted to the ir context indeed it 
ranks two documents w r t the intensity of their positions 
difference in each input ranking and also considering the 
number of the input rankings that are concordant and 
discordant in favor of a specific document there is also no 
need to make specific assumptions on the positions of the 
missing documents this is an important feature since the 
absence of a document from a ranking should not be 
necessarily interpreted negatively 
experimental results show that the outranking method 
significantly out-performs popular classical positional data 
fusion methods like combsum and combmnz strategies it 
also out-performs a good performing majoritarian methods 
which is the markov chain method these results are tested 
against different test collections and queries from the 
experiments we can also conclude that in order to improve the 
performances we should fuse result lists of well performing 
ir models and that majoritarian data fusion methods 
perform better than positional methods 
the proposed method can have a real impact on web 
metasearch performances since only ranks are available from 
most primary search engines whereas most of the current 
approaches need scores to merge result lists into one single 
list 
further work involves investigating whether the 
outranking approach performs well in various other contexts e g 
using the document scores or some combination of 
document ranks and scores 
acknowledgments 
the authors would like to thank jacques savoy for his 
valuable comments on a preliminary version of this paper 
 references 
 a aronson d demner-fushman s humphrey 
j lin h liu p ruch m ruiz l smith l tanabe 
and w wilbur fusion of knowledge-intensive and 
statistical approaches for retrieving and annotating 
textual genomics documents in proceedings 
trec nist publication 
 r a baeza-yates and b a ribeiro-neto modern 
information retrieval acm press 
 b t bartell g w cottrell and r k belew 
automatic combination of multiple ranked retrieval 
systems in proceedings acm-sigir pages 
 - springer-verlag 
 n j belkin p kantor e a fox and j a shaw 
combining evidence of multiple query representations 
for information retrieval ipm - 
 j borda m´emoire sur les ´elections au scrutin 
histoire de l acad´emie des sciences 
 j p callan z lu and w b croft searching 
distributed collections with inference networks in 
proceedings acm-sigir pages - 
 m condorcet essai sur l application de l analyse `a la 
probabilit´e des d´ecisions rendues `a la pluralit´e des 
voix imprimerie royale paris 
 w d cook and m kress ordinal ranking with 
intensity of preference management science 
 - 
 n craswell and d hawking overview of the 
trec- web track in proceedings trec 
nist publication 
 n craswell and d hawking overview of the 
trec- web track in proceedings of 
trec nist publication 
 c dwork s r kumar m naor and d sivakumar 
rank aggregation methods for the web in 
proceedings www pages - 
 r fagin combining fuzzy information from multiple 
systems jcss - 
 r fagin r kumar m mahdian d sivakumar and 
e vee comparing and aggregating rankings with 
ties in pods pages - 
 r fagin r kumar and d sivakumar comparing 
top k lists siam j on discrete mathematics 
 - 
 e a fox and j a shaw combination of multiple 
searches in proceedings of trec nist 
publication 
 j katzer m mcgill j tessier w frakes and 
p dasgupta a study of the overlap among document 
representations information technology research 
and development - 
 l s larkey m e connell and j callan collection 
selection and results merging with topically organized 
u s patents and trec data in proceedings 
acm-cikm pages - acm press 
 a le calv´e and j savoy database merging strategy 
based on logistic regression ipm - 
 j h lee analyses of multiple evidence combination 
in proceedings acm-sigir pages - 
 d lillis f toolan r collier and j dunnion 
probfuse a probabilistic approach to data fusion in 
proceedings acm-sigir pages - acm 
press 
 j i marden analyzing and modeling rank data 
number in monographs on statistics and applied 
probability chapman hall 
 m montague and j a aslam metasearch 
consistency in proceedings acm-sigir pages 
 - acm press 
 d m pennock and e horvitz analysis of the 
axiomatic foundations of collaborative filtering in 
workshop on ai for electronic commerce at the th 
national conference on artificial intelligence 
 m e renda and u straccia web metasearch rank 
vs score based rank aggregation methods in 
proceedings acm-sac pages - acm 
press 
 w h riker liberalism against populism waveland 
press 
 b roy the outranking approach and the foundations 
of electre methods theory and decision 
 - 
 b roy and j hugonnard ranking of suburban line 
extension projects on the paris metro system by a 
multicriteria method transportation research 
 a - 
 l si and j callan using sampled data and regression 
to merge search engine results in proceedings 
acm-sigir pages - acm press 
 m truchon an extension of the condorcet criterion 
and kemeny orders cahier centre de recherche 
en economie et finance appliqu´ees oct 
 h turtle and w b croft inference networks for 
document retrieval in proceedings of acm-sigir 
pages - acm press 
 c c vogt and g w cottrell fusion via a linear 
combination of scores information retrieval 
 - 
a time machine for text search 
klaus berberich srikanta bedathur thomas neumann gerhard weikum 
max-planck institute for informatics 
saarbr¨ucken germany 
{kberberi bedathur neumann weikum} mpi-inf mpg de 
abstract 
text search over temporally versioned document collections 
such as web archives has received little attention as a 
research problem as a consequence there is no scalable and 
principled solution to search such a collection as of a 
specified time t in this work we address this shortcoming and 
propose an efficient solution for time-travel text search by 
extending the inverted file index to make it ready for 
temporal search we introduce approximate temporal coalescing 
as a tunable method to reduce the index size without 
significantly affecting the quality of results in order to further 
improve the performance of time-travel queries we 
introduce two principled techniques to trade off index size for 
its performance these techniques can be formulated as 
optimization problems that can be solved to near-optimality 
finally our approach is evaluated in a comprehensive 
series of experiments on two large-scale real-world datasets 
results unequivocally show that our methods make it 
possible to build an efficient time machine scalable to large 
versioned text collections 
categories and subject descriptors 
h content analysis and indexing indexing 
methods h information search and retrieval 
retrieval models search process 
general terms 
algorithms experimentation performance 
 introduction 
in this work we address time-travel text search over 
temporally versioned document collections given a keyword 
query q and a time t our goal is to identify and rank 
relevant documents as if the collection was in its state as of 
time t 
an increasing number of such versioned document 
collections is available today including web archives 
collaborative authoring environments like wikis or timestamped 
information feeds text search on these collections 
however is mostly time-ignorant while the searched collection 
changes over time often only the most recent version of 
a documents is indexed or versions are indexed 
independently and treated as separate documents even worse for 
some collections in particular web archives like the 
internet archive a comprehensive text-search functionality 
is often completely missing 
time-travel text search as we develop it in this paper 
is a crucial tool to explore these collections and to unfold 
their full potential as the following example demonstrates 
for a documentary about a past political scandal a 
journalist needs to research early opinions and statements made 
by the involved politicians sending an appropriate query 
to a major web search-engine the majority of returned 
results contains only recent coverage since many of the early 
web pages have disappeared and are only preserved in web 
archives if the query could be enriched with a time point 
say august th as the day after the scandal got 
revealed and be issued against a web archive only pages that 
existed specifically at that time could be retrieved thus better 
satisfying the journalist s information need 
document collections like the web or wikipedia as 
we target them here are already large if only a single 
snapshot is considered looking at their evolutionary history we 
are faced with even larger data volumes as a consequence 
na¨ıve approaches to time-travel text search fail and viable 
approaches must scale-up well to such large data volumes 
this paper presents an efficient solution to time-travel 
text search by making the following key contributions 
 the popular well-studied inverted file index is 
transparently extended to enable time-travel text search 
 temporal coalescing is introduced to avoid an 
indexsize explosion while keeping results highly accurate 
 we develop two sublist materialization techniques to 
improve index performance that allow trading off space 
vs performance 
 in a comprehensive experimental evaluation our 
approach is evaluated on the english wikipedia and parts 
of the internet archive as two large-scale real-world 
datasets with versioned documents 
the remainder of this paper is organized as follows the 
presented work is put in context with related work in 
section we delineate our model of a temporally versioned 
document collection in section we present our time-travel 
inverted index in section building on it temporal 
coalescing is described in section in section we describe 
principled techniques to improve index performance before 
presenting the results of our experimental evaluation in 
section 
 related work 
we can classify the related work mainly into the following 
two categories i methods that deal explicitly with 
collections of versioned documents or temporal databases and 
 ii methods for reducing the index size by exploiting either 
the document-content overlap or by pruning portions of the 
index we briefly review work under these categories here 
to the best of our knowledge there is very little prior work 
dealing with historical search over temporally versioned 
documents anick and flynn while pioneering this research 
describe a help-desk system that supports historical queries 
access costs are optimized for accesses to the most recent 
versions and increase as one moves farther into the past 
burrows and hisgen in a patent description delineate 
a method for indexing range-based values and mention its 
potential use for searching based on dates associated with 
documents recent work by nørv˚ag and nybø and 
their earlier proposals concentrate on the relatively simpler 
problem of supporting text-containment queries only and 
neglect the relevance scoring of results stack reports 
practical experiences made when adapting the open source 
search-engine nutch to search web archives this 
adaptation however does not provide the intended time-travel 
text search functionality in contrast research in temporal 
databases has produced several index structures tailored for 
time-evolving databases a comprehensive overview of the 
state-of-art is available in unlike the inverted file 
index their applicability to text search is not well understood 
moving on to the second category of related work broder 
et al describe a technique that exploits large content 
overlaps between documents to achieve a reduction in index 
size their technique makes strong assumptions about the 
structure of document overlaps rendering it inapplicable to 
our context more recent approaches by hersovici et al 
and zhang and suel exploit arbitrary content overlaps 
between documents to reduce index size none of the 
approaches however considers time explicitly or provides the 
desired time-travel text search functionality static 
indexpruning techniques aim to reduce the effective index 
size by removing portions of the index that are expected 
to have low impact on the query result they also do not 
consider temporal aspects of documents and thus are 
technically quite different from our proposal despite having a 
shared goal of index-size reduction it should be noted that 
index-pruning techniques can be adapted to work along with 
the temporal text index we propose here 
 model 
in the present work we deal with a temporally versioned 
document collection d that is modeled as described in the 
following each document d ∈ d is a sequence of its versions 
d dt 
 dt 
 
each version dti 
has an associated timestamp ti reflecting 
when the version was created each version is a vector of 
searchable terms or features any modification to a 
document version results in the insertion of a new version with 
corresponding timestamp we employ a discrete definition 
of time so that timestamps are non-negative integers the 
deletion of a document at time ti i e its disappearance 
from the current state of the collection is modeled as the 
insertion of a special tombstone version ⊥ the validity 
time-interval val dti 
 of a version dti 
is ti ti if a newer 
version with associated timestamp ti exists and ti now 
otherwise where now points to the greatest possible value of 
a timestamp i e ∀t t now 
putting all this together we define the state dt 
of the 
collection at time t i e the set of versions valid at t that 
are not deletions as 
dt 
 
 
d∈d 
{dti 
∈ d t ∈ val dti 
 ∧ dti 
 ⊥} 
as mentioned earlier we want to enrich a keyword query 
q with a timestamp t so that q be evaluated over dt 
 i e 
the state of the collection at time t the enriched time-travel 
query is written as q t 
for brevity 
as a retrieval model in this work we adopt okapi bm 
but note that the proposed techniques are not dependent on 
this choice and are applicable to other retrieval models like 
tf-idf or language models as well for our considered 
setting we slightly adapt okapi bm as 
w q t 
 dti 
 
x 
v∈q 
wtf v dti 
 · widf v t 
in the above formula the relevance w q t 
 dti 
 of a 
document version dti 
to the time-travel query q t 
is defined 
we reiterate that q t 
is evaluated over dt 
so that only the 
version dti 
valid at time t is considered the first factor 
wtf v dti 
 in the summation further referred to as the 
tfscore is defined as 
wtf v dti 
 
 k · tf v dti 
 
k · − b b · dl d ti 
avdl ti 
 tf v dti 
 
it considers the plain term frequency tf v dti 
 of term v 
in version dti 
normalizing it taking into account both the 
length dl dti 
 of the version and the average document length 
avdl ti in the collection at time ti the length-normalization 
parameter b and the tf-saturation parameter k are inherited 
from the original okapi bm and are commonly set to 
values and respectively the second factor widf v t 
which we refer to as the idf-score in the remainder conveys 
the inverse document frequency of term v in the collection 
at time t and is defined as 
widf v t log 
n t − df v t 
df v t 
where n t dt 
 is the collection size at time t and df v t 
gives the number of documents in the collection that contain 
the term v at time t while the idf-score depends on the 
whole corpus as of the query time t the tf-score is specific 
to each version 
 time-travelinvertedfileindex 
the inverted file index is a standard technique for text 
indexing deployed in many systems in this section we 
briefly review this technique and present our extensions to 
the inverted file index that make it ready for time-travel text 
search 
 inverted file index 
an inverted file index consists of a vocabulary commonly 
organized as a b -tree that maps each term to its 
idfscore and inverted list the index list lv belonging to term 
v contains postings of the form 
 d p 
where d is a document-identifier and p is the so-called 
payload the payload p contains information about the term 
frequency of v in d but may also include positional 
information about where the term appears in the document 
the sort-order of index lists depends on which queries are 
to be supported efficiently for boolean queries it is 
favorable to sort index lists in document-order 
frequencyorder and impact-order sorted index lists are beneficial for 
ranked queries and enable optimized query processing that 
stops early after having identified the k most relevant 
documents a variety of compression techniques 
such as encoding document identifiers more compactly have 
been proposed to reduce the size of index lists for 
an excellent recent survey about inverted file indexes we 
refer to 
 time-travel inverted file index 
in order to prepare an inverted file index for time travel 
we extend both inverted lists and the vocabulary structure 
by explicitly incorporating temporal information the main 
idea for inverted lists is that we include a validity 
timeinterval tb te in postings to denote when the payload 
information was valid the postings in our time-travel inverted 
file index are thus of the form 
 d p tb te 
where d and p are defined as in the standard inverted file 
index above and tb te is the validity time-interval 
as a concrete example in our implementation for a 
version dti 
having the okapi bm tf-score wtf v dti 
 for term 
v the index list lv contains the posting 
 d wtf v dti 
 ti ti 
similarly the extended vocabulary structure maintains 
for each term a time-series of idf-scores organized as a 
b tree unlike the tf-score the idf-score of every term could 
vary with every change in the corpus therefore we take 
a simplified approach to idf-score maintenance by 
computing idf-scores for all terms in the corpus at specific possibly 
periodic times 
 query processing 
during processing of a time-travel query q t 
 for each query 
term the corresponding idf-score valid at time t is retrieved 
from the extended vocabulary then index lists are 
sequentially read from disk thereby accumulating the information 
contained in the postings we transparently extend the 
sequential reading which is - to the best of our 
knowledgecommon to all query processing techniques on inverted file 
indexes thus making them suitable for time-travel 
queryprocessing to this end sequential reading is extended by 
skipping all postings whose validity time-interval does not 
contain t i e t ∈ tb te whether a posting can be 
skipped can only be decided after the posting has been 
transferred from disk into memory and therefore still incurs 
significant i o cost as a remedy we propose index organization 
techniques in section that aim to reduce the i o overhead 
significantly 
we note that our proposed extension of the inverted file 
index makes no assumptions about the sort-order of 
index lists as a consequence existing query-processing 
techniques and most optimizations e g compression techniques 
remain equally applicable 
 temporal coalescing 
if we employ the time-travel inverted index as described 
in the previous section to a versioned document collection 
we obtain one posting per term per document version for 
frequent terms and large highly-dynamic collections this 
time 
score 
non-coalesced 
coalesced 
figure approximate temporal coalescing 
leads to extremely long index lists with very poor 
queryprocessing performance 
the approximate temporal coalescing technique that we 
propose in this section counters this blowup in index-list 
size it builds on the observation that most changes in a 
versioned document collection are minor leaving large parts 
of the document untouched as a consequence the payload 
of many postings belonging to temporally adjacent versions 
will differ only slightly or not at all approximate temporal 
coalescing reduces the number of postings in an index list by 
merging such a sequence of postings that have almost equal 
payloads while keeping the maximal error bounded this 
idea is illustrated in figure which plots non-coalesced and 
coalesced scores of postings belonging to a single document 
approximate temporal coalescing is greatly effective given 
such fluctuating payloads and reduces the number of 
postings from to in the example the notion of temporal 
coalescing was originally introduced in temporal database 
research by b¨ohlen et al where the simpler problem of 
coalescing only equal information was considered 
we next formally state the problem dealt with in 
approximate temporal coalescing and discuss the computation of 
optimal and approximate solutions note that the technique 
is applied to each index list separately so that the following 
explanations assume a fixed term v and index list lv 
as an input we are given a sequence of temporally 
adjacent postings 
i d pi ti ti d pn− tn− tn 
each sequence represents a contiguous time period during 
which the term was present in a single document d if a term 
disappears from d but reappears later we obtain multiple 
input sequences that are dealt with separately we seek to 
generate the minimal length output sequence of postings 
o d pj tj tj d pm− tm− tm 
that adheres to the following constraints first o and i 
must cover the same time-range i e ti tj and tn tm 
second when coalescing a subsequence of postings of the 
input into a single posting of the output we want the 
approximation error to be below a threshold in other words if 
 d pi ti ti and d pj tj tj are postings of i and 
o respectively then the following must hold for a chosen 
error function and a threshold 
tj ≤ ti ∧ ti ≤ tj ⇒ error pi pj ≤ 
in this paper as an error function we employ the relative 
error between payloads i e tf-scores of a document in i 
and o defined as 
errrel pi pj pi − pj pi 
finding an optimal output sequence of postings can be 
cast into finding a piecewise-constant representation for the 
points ti pi that uses a minimal number of segments while 
retaining the above approximation guarantee similar 
problems occur in time-series segmentation and histogram 
construction typically dynamic programming is 
applied to obtain an optimal solution in o n 
m∗ 
 
time with m∗ 
being the number of segments in an optimal 
sequence in our setting as a key difference only a 
guarantee on the local error is retained - in contrast to a guarantee 
on the global error in the aforementioned settings 
exploiting this fact an optimal solution is computable by means of 
induction in o n 
 time details of the optimal 
algorithm are omitted here but can be found in the 
accompanying technical report 
the quadratic complexity of the optimal algorithm makes 
it inappropriate for the large datasets encountered in this 
work as an alternative we introduce a linear-time 
approximate algorithm that is based on the sliding-window 
algorithm given in this algorithm produces nearly-optimal 
output sequences that retain the bound on the relative error 
but possibly require a few additional segments more than an 
optimal solution 
algorithm temporal coalescing approximate 
 i d pi ti ti o 
 pmin pi pmax pi p pi tb ti te ti 
 for d pj tj tj ∈ i do 
 pmin min pmin pj pmax max pmax pj 
 p optrep pmin pmax 
 if errrel pmin p ≤ ∧ errrel pmax p ≤ then 
 pmin pmin pmax pmax p p te tj 
 else 
 o o ∪ d p tb te 
 pmin pj pmax pj p pj tb tj te tj 
 end if 
 end for 
 o o ∪ d p tb te 
algorithm makes one pass over the input sequence i 
while doing so it coalesces sequences of postings having 
maximal length the optimal representative for a sequence 
of postings depends only on their minimal and maximal 
payload pmin and pmax and can be looked up using optrep in 
o see for details when reading the next 
posting the algorithm tries to add it to the current sequence of 
postings it computes the hypothetical new representative 
p and checks whether it would retain the approximation 
guarantee if this test fails a coalesced posting bearing the 
old representative is added to the output sequence o and 
following that the bookkeeping is reinitialized the time 
complexity of the algorithm is in o n 
note that since we make no assumptions about the sort 
order of index lists temporal-coalescing algorithms have an 
additional preprocessing cost in o lv log lv for sorting 
the index list and chopping it up into subsequences for each 
document 
 sublist materialization 
efficiency of processing a query q t 
on our time-travel 
inverted index is influenced adversely by the wasted i o due 
to read but skipped postings temporal coalescing 
implicitly addresses this problem by reducing the overall index list 
size but still a significant overhead remains in this section 
we tackle this problem by proposing the idea of materializing 
sublists each of which corresponds to a contiguous 
subinterval of time spanned by the full index each of these sublists 
contains all coalesced postings that overlap with the 
corresponding time interval of the sublist note that all those 
postings whose validity time-interval spans across the 
temporal boundaries of several sublists are replicated in each of 
the spanned sublists thus in order to process the query q t 
time 
t t t t t t t t t t 
d 
d 
d 
document 
 
 
 
figure sublist materialization 
it is sufficient to scan any materialized sublist whose 
timeinterval contains t 
we illustrate the idea of sublist materialization using an 
example shown in figure the index list lv visualized in 
the figure contains a total of postings from three 
documents d d and d for ease of description we have 
numbered boundaries of validity time-intervals in increasing 
time-order as t t and numbered the postings 
themselves as now consider the processing of a query 
q t 
with t ∈ t t using this inverted list although only 
three postings postings and are valid at time t the 
whole inverted list has to be read in the worst case suppose 
that we split the time axis of the list at time t forming two 
sublists with postings { } and { 
 } respectively then we can process the above query with 
optimal cost by reading only those postings that existed at 
this t 
at a first glance it may seem counterintuitive to reduce 
index size in the first step using temporal coalescing and 
then to increase it again using the sublist materialization 
techniques presented in this section however we reiterate 
that our main objective is to improve the efficiency of 
processing queries not to reduce the index size alone the use 
of temporal coalescing improves the performance by 
reducing the index size while the sublist materialization improves 
performance by judiciously replicating entries further the 
two techniques can be applied separately and are 
independent if applied in conjunction though there is a synergetic 
effect - sublists that are materialized from a temporally 
coalesced index are generally smaller 
we employ the notation lv ti tj to refer to the 
materialized sublist for the time interval ti tj that is formally 
defined as 
lv ti tj { d p tb te ∈ lv tb tj ∧ te ti} 
to aid the presentation in the rest of the paper we first 
provide some definitions let t t tn be the sorted 
sequence of all unique time-interval boundaries of an 
inverted list lv then we define 
e { ti ti ≤ i n} 
to be the set of elementary time intervals we refer to the 
set of time intervals for which sublists are materialized as 
m ⊆ { ti tj ≤ i j ≤ n } 
and demand 
∀ t ∈ t tn ∃ m ∈ m t ∈ m 
i e the time intervals in m must completely cover the 
time interval t tn so that time-travel queries q t 
for all 
t ∈ t tn can be processed we also assume that 
intervals in m are disjoint we can make this assumption 
without ruling out any optimal solution with regard to space 
or performance defined below the space required for the 
materialization of sublists in a set m is defined as 
s m 
x 
m∈m 
 lv m 
i e the total length of all lists in m given a set m we let 
π ti ti tj tk ∈ m ti ti ⊆ tj tk 
denote the time interval that is used to process queries q t 
with t ∈ ti ti the performance of processing queries 
q t 
for t ∈ ti ti inversely depends on its processing cost 
pc ti ti lv π ti ti 
which is assumed to be proportional to the length of the 
list lv π ti ti thus in order to optimize the 
performance of processing queries we minimize their processing 
costs 
 performance space-optimal approaches 
one strategy to eliminate the problem of skipped 
postings is to eagerly materialize sublists for all elementary time 
intervals i e to choose m e in doing so for every 
query q t 
only postings valid at time t are read and thus the 
best possible performance is achieved therefore we will 
refer to this approach as popt in the remainder the initial 
approach described above that keeps only the full list lv 
and thus picks m { t tn } is referred to as sopt in the 
remainder this approach requires minimal space since it 
keeps each posting exactly once 
popt and sopt are extremes the former provides the best 
possible performance but is not space-efficient the latter 
requires minimal space but does not provide good 
performance the two approaches presented in the rest of this 
section allow mutually trading off space and performance 
and can thus be thought of as means to explore the 
configuration spectrum between the popt and the sopt approach 
 performance-guarantee approach 
the popt approach clearly wastes a lot of space 
materializing many nearly-identical sublists in the example 
illustrated in figure materialized sublists for t t and 
 t t differ only by one posting if the sublist for t t 
was materialized instead one could save significant space 
while incurring only an overhead of one skipped posting for 
all t ∈ t t the technique presented next is driven by 
the idea that significant space savings over popt are 
achievable if an upper-bounded loss on the performance can be 
tolerated or to put it differently if a performance 
guarantee relative to the optimum is to be retained in detail 
the technique which we refer to as pg performance 
guarantee in the remainder finds a set m that has minimal 
required space but guarantees for any elementary time 
interval ti ti and thus for any query q t 
with t ∈ ti ti 
that performance is worse than optimal by at most a factor 
of γ ≥ formally this problem can be stated as 
argmin 
m 
s m s t 
∀ ti ti ∈ e pc ti ti ≤ γ · lv ti ti 
an optimal solution to the problem can be computed by 
means of induction using the recurrence 
c t tk 
min {c t tj lv tj tk ≤ j ≤ k ∧ condition} 
where c t tj is the optimal cost i e the space 
required for the prefix subproblem 
{ ti ti ∈ e ti ti ⊆ t tj } 
and condition stands for 
∀ ti ti ∈ e ti ti ⊆ tj tk 
⇒ lv tj tk ≤ γ · lv ti ti 
 
intuitively the recurrence states that an optimal solution 
for t tk be combined from an optimal solution to a 
prefix subproblem c t tj and a time interval tj tk 
that can be materialized without violating the performance 
guarantee 
pseudocode of the algorithm is omitted for space reasons 
but can be found in the accompanying technical report 
the time complexity of the algorithm is in o n 
 - for each 
prefix subproblem the above recurrence must be evaluated 
which is possible in linear time if list sizes l ti tj are 
precomputed the space complexity is in o n 
 - the cost 
of keeping the precomputed sublist lengths and memoizing 
optimal solutions to prefix subproblems 
 space-bound approach 
so far we considered the problem of materializing sublists 
that give a guarantee on performance while requiring 
minimal space in many situations though the storage space is 
at a premium and the aim would be to materialize a set of 
sublists that optimizes expected performance while not 
exceeding a given space limit the technique presented next 
which is named sb tackles this very problem the space 
restriction is modeled by means of a user-specified 
parameter κ ≥ that limits the maximum allowed blowup in index 
size from the space-optimal solution provided by sopt the 
sb technique seeks to find a set m that adheres to this 
space limit but minimizes the expected processing cost and 
thus optimizes the expected performance in the definition 
of the expected processing cost p ti ti denotes the 
probability of a query time-point being in ti ti 
formally this space-bound sublist-materialization problem can 
be stated as 
argmin 
m 
x 
 ti ti ∈ e 
p ti ti · pc ti ti s t 
x 
m∈m 
 lv m ≤ κ lv 
the problem can be solved by using dynamic 
programming over an increasing number of time intervals at each 
time interval in e the algorithms decides whether to start a 
new materialization time-interval using the known best 
materialization decision from the previous time intervals and 
keeping track of the required space consumption for 
materialization a detailed description of the algorithm is omitted 
here but can be found in the accompanying technical 
report unfortunately the algorithm has time complexity 
in o n 
 lv and its space complexity is in o n 
 lv which 
is not practical for large data sets 
we obtain an approximate solution to the problem 
using simulated annealing simulated annealing takes 
a fixed number r of rounds to explore the solution space 
in each round a random successor of the current solution 
is looked at if the successor does not adhere to the space 
limit it is always rejected i e the current solution is kept 
a successor adhering to the space limit is always accepted if 
it achieves lower expected processing cost than the current 
solution if it achieves higher expected processing cost it is 
randomly accepted with probability e−∆ r 
where ∆ is the 
increase in expected processing cost and r ≥ r ≥ denotes 
the number of remaining rounds in addition throughout 
all rounds the method keeps track of the best solution seen 
so far the solution space for the problem at hand can be 
efficiently explored as we argued above we solely have 
to look at sets m that completely cover the time interval 
 t tn and do not contain overlapping time intervals we 
represent such a set m as an array of n boolean variables 
b bn that convey the boundaries of time intervals in the 
set note that b and bn are always set to true initially 
all n − intermediate variables assume false which 
corresponds to the set m { t tn } a random successor 
can now be easily generated by switching the value of one 
of the n − intermediate variables the time complexity of 
the method is in o n 
 - the expected processing cost must 
be computed in each round its space complexity is in o n 
- for keeping the n boolean variables 
as a side remark note that for κ the sb method 
does not necessarily produce the solution that is obtained 
from sopt but may produce a solution that requires the 
same amount of space while achieving better expected 
performance 
 experimental evaluation 
we conducted a comprehensive series of experiments on 
two real-world datasets to evaluate the techniques proposed 
in this paper 
 setup and datasets 
the techniques described in this paper were implemented 
in a prototype system using java jdk all 
experiments described below were run on a single sun v z 
machine having four amd opteron cpus gb ram a large 
network-attached raid- disk array and running microsoft 
windows server all data and indexes are kept in an 
oracle g database that runs on the same machine for 
our experiments we used two different datasets 
the english wikipedia revision history referred to as 
wiki in the remainder is available for free download as 
a single xml file this large dataset totaling tbytes 
contains the full editing history of the english wikipedia 
from january to december the time of our 
download we indexed all encyclopedia articles excluding 
versions that were marked as the result of a minor edit e g 
the correction of spelling errors etc this yielded a total of 
 documents with versions having a mean 
 µ of versions per document at standard deviation σ 
of we built a time-travel query workload using the 
query log temporarily made available recently by aol 
research as follows - we first extracted the most frequent 
keyword queries that yielded a result click on a wikipedia 
article for e g french revolution hurricane season 
da vinci code etc the thus extracted queries contained 
a total of distinct terms for each extracted query we 
randomly picked a time point for each month covered by 
the dataset this resulted in a total of × 
time-travel queries 
the second dataset used in our experiments was based 
on a subset of the european archive containing weekly 
crawls of gov uk websites throughout the years and 
 amounting close to tbytes of raw data we filtered 
out documents not belonging to mime-types text plain 
and text html to obtain a dataset that totals tbytes 
and which we refer to as ukgov in rest of the paper this 
included a total of documents with 
versions µ and σ we built a corresponding 
query workload as mentioned before this time choosing 
keyword queries that led to a site in the gov uk domain e g 
minimum wage inheritance tax citizenship ceremony 
dates etc and randomly sampling a time point for every 
month within the two year period spanned by the dataset 
thus we obtained a total of × time-travel 
queries for the ukgov dataset in total terms appear 
in the extracted queries 
the collection statistics i e n and avdl and term 
statistics i e df were computed at monthly granularity for 
both datasets 
 impact of temporal coalescing 
our first set of experiments is aimed at evaluating the 
approximate temporal coalescing technique described in 
section in terms of index-size reduction and its effect on the 
result quality for both the wiki and ukgov datasets we 
compare temporally coalesced indexes for different values of 
the error threshold computed using algorithm with the 
non-coalesced index as a baseline 
wiki ukgov 
 postings ratio postings ratio 
- 
 
 
 
 
 
 
table index sizes for non-coalesced index - and 
coalesced indexes for different values of 
table summarizes the index sizes measured as the total 
number of postings as these results demonstrate 
approximate temporal coalescing is highly effective in reducing 
index size even a small threshold value e g has a 
considerable effect by reducing the index size almost by an 
order of magnitude note that on the ukgov dataset even 
accurate coalescing manages to reduce the index size 
to less than of the original size index size continues to 
reduce on both datasets as we increase the value of 
how does the reduction in index size affect the query 
results in order to evaluate this aspect we compared the 
top-k results computed using a coalesced index against the 
ground-truth result obtained from the original index for 
different cutoff levels k let gk and ck be the top-k documents 
from the ground-truth result and from the coalesced index 
respectively we used the following two measures for 
comparison i relative recall at cutoff level k rr k that 
measures the overlap between gk and ck which ranges in 
 and is defined as 
rr k gk ∩ ck k 
 ii kendall s τ see for a detailed definition at 
cutoff level k kt k measuring the agreement between two 
results in the relative order of items in gk ∩ ck with value 
 or - indicating total agreement or disagreement 
figure plots for cutoff levels and the mean of 
rr k and kt k along with and percentiles for 
different values of the threshold starting from note 
that for results coincide with those obtained by the 
original index and hence are omitted from the graph 
it is reassuring to see from these results that approximate 
temporal coalescing induces minimal disruption to the query 
results since rr k and kt k are within reasonable limits 
for the smallest value of in our experiments 
rr  for wiki is indicating that the results are 
- 
- 
 
 
 
ε 
 
relative recall   wiki 
kendall s τ   wiki 
relative recall   ukgov 
kendall s τ   ukgov 
 a   
- 
- 
 
 
 
ε 
 
relative recall   wiki 
kendall s τ   wiki 
relative recall   ukgov 
kendall s τ   ukgov 
 b   
figure relative recall and kendall s τ observed on coalesced indexes for different values of 
almost indistinguishable from those obtained through the 
original index even the relative order of these common 
results is quite high as the mean kt  is close to 
for the extreme value of which results in an index 
size of just of the original the rr  and kt  
are about and respectively on the relatively less 
dynamic ukgov dataset as can be seen from the σ values 
above results were even better with high values of rr and 
kt seen throughout the spectrum of values for both cutoff 
values 
 sublist materialization 
we now turn our attention towards evaluating the 
sublist materialization techniques introduced in section for 
both datasets we started with the coalesced index produced 
by a moderate threshold setting of in order to 
reduce the computational effort boundaries of elementary 
time intervals were rounded to day granularity before 
computing the sublist materializations however note that the 
postings in the materialized sublists still retain their 
original timestamps for a comparative evaluation of the four 
approaches - popt sopt pg and sb - we measure space 
and performance as follows the required space s m as 
defined earlier is equal to the total number of postings in 
the materialized sublists to assess performance we 
compute the expected processing cost epc for all terms in 
the respective query workload assuming a uniform 
probability distribution among query time-points we report the 
mean epc as well as the - and -percentile in other 
words the mean epc reflects the expected length of the 
index list in terms of index postings that needs to be scanned 
for a random time point and a random term from the query 
workload 
the sopt and popt approaches are by their definition 
parameter-free for the pg approach we varied its 
parameter γ which limits the maximal performance degradation 
between and analogously for the sb approach 
the parameter κ as an upper-bound on the allowed space 
blowup was varied between and solutions for the 
sb approach were obtained running simulated annealing for 
r rounds 
table lists the obtained space and performance figures 
note that epc values are smaller on wiki than on 
ukgov since terms in the query workload employed for wiki 
are relatively rarer in the corpus based on the depicted 
results we make the following key observations i as 
expected popt achieves optimal performance at the cost of an 
enormous space consumption sopt to the contrary while 
consuming an optimal amount of space provides only poor 
expected processing cost the pg and sb methods for 
different values of their respective parameter produce 
solutions whose space and performance lie in between the 
extremes that popt and sopt represent ii for the pg method 
we see that for an acceptable performance degradation of 
only i e γ the required space drops by more 
than one order of magnitude in comparison to popt on both 
datasets iii the sb approach achieves close-to-optimal 
performance on both datasets if allowed to consume at most 
three times the optimal amount of space i e κ 
which on our datasets still corresponds to a space reduction 
over popt by more than one order of magnitude 
we also measured wall-clock times on a sample of the 
queries with results indicating improvements in execution 
time by up to a factor of 
 conclusions 
in this work we have developed an efficient solution for 
time-travel text search over temporally versioned document 
collections experiments on two real-world datasets showed 
that a combination of the proposed techniques can reduce 
index size by up to an order of magnitude while achieving 
nearly optimal performance and highly accurate results 
the present work opens up many interesting questions 
for future research e g how can we even further improve 
performance by applying and possibly extending encoding 
compression and skipping techniques how can we 
extend the approach for queries q tb te 
specifying a time 
interval instead of a time point how can the described 
time-travel text search functionality enable or speed up text 
mining along the time axis e g tracking sentiment changes 
in customer opinions 
 acknowledgments 
we are grateful to the anonymous reviewers for their 
valuable comments - in particular to the reviewer who pointed 
out the opportunity for algorithmic improvements in 
section and section 
 references 
 v n anh and a moffat pruned query evaluation 
using pre-computed impacts in sigir 
 v n anh and a moffat pruning strategies for 
mixed-mode querying in cikm 
wiki ukgov 
s m epc s m epc 
 mean mean 
popt 
sopt 
pg γ 
pg γ 
pg γ 
pg γ 
pg γ 
pg γ 
pg γ 
sb κ 
sb κ 
sb κ 
sb κ 
sb κ 
sb κ 
sb κ 
table required space and expected processing cost in postings observed on coalesced indexes 
 p g anick and r a flynn versioning a full-text 
information retrieval system in sigir 
 r a baeza-yates and b ribeiro-neto modern 
information retrieval addison-wesley 
 k berberich s bedathur t neumann and 
g weikum a time machine for text search 
technical report mpi-i- - - max-planck 
institute for informatics 
 m h b¨ohlen r t snodgrass and m d soo 
coalescing in temporal databases in vldb 
 p boldi m santini and s vigna do your worst to 
make the best paradoxical effects in pagerank 
incremental computations in waw 
 a z broder n eiron m fontoura m herscovici 
r lempel j mcpherson r qi and e j shekita 
indexing shared content in information retrieval 
systems in edbt 
 c buckley and a f lewit optimization of inverted 
vector searches in sigir 
 m burrows and a l hisgen method and apparatus 
for generating and searching range-based index of 
word locations u s patent 
 s b¨uttcher and c l a clarke a document-centric 
approach to static index pruning in text retrieval 
systems in cikm 
 d carmel d cohen r fagin e farchi 
m herscovici y s maarek and a soffer static 
index pruning for information retrieval systems in 
sigir 
 http www europarchive org 
 r fagin r kumar and d sivakumar comparing 
top k lists siam j discrete math - 
 
 r fagin a lotem and m naor optimal 
aggregation algorithms for middleware j comput 
syst sci - 
 s guha k shim and j woo rehist relative 
error histogram construction algorithms in vldb 
 
 m hersovici r lempel and s yogev efficient 
indexing of versioned document sequences in ecir 
 
 http www archive org 
 y e ioannidis and v poosala balancing histogram 
optimality and practicality for query result size 
estimation in sigmod 
 h v jagadish n koudas s muthukrishnan 
v poosala k c sevcik and t suel optimal 
histograms with quality guarantees in vldb 
 e j keogh s chu d hart and m j pazzani an 
online algorithm for segmenting time series in 
icdm 
 s kirkpatrick d g jr and m p vecchi 
optimization by simulated annealing science 
 - 
 j kleinberg and e tardos algorithm design 
addison-wesley 
 u manber introduction to algorithms a creative 
approach addison-wesley 
 k nørv˚ag and a o n nybø dyst dynamic and 
scalable temporal text indexing in time 
 j m ponte and w b croft a language modeling 
approach to information retrieval in sigir 
 s e robertson and s walker okapi keenbow at 
trec- in trec 
 b salzberg and v j tsotras comparison of access 
methods for time-evolving data acm comput 
surv - 
 m stack full text search of web archive 
collections in iwaw 
 e terzi and p tsaparas efficient algorithms for 
sequence segmentation in siam-dm 
 m theobald g weikum and r schenkel top-k 
query evaluation with probabilistic guarantees in 
vldb 
 http www wikipedia org 
 i h witten a moffat and t c bell managing 
gigabytes compressing and indexing documents and 
images morgan kaufmann publishers inc 
 j zhang and t suel efficient search in large 
textual collections with redundancy in www 
 
 j zobel and a moffat inverted files for text search 
engines acm comput surv 
robustness of adaptive filtering methods 
in a cross-benchmark evaluation 
yiming yang shinjae yoo jian zhang bryan kisiel 
school of computer science carnegie mellon university 
 forbes avenue pittsburgh pa usa 
abstract 
this paper reports a cross-benchmark evaluation of regularized 
logistic regression lr and incremental rocchio for adaptive 
filtering using four corpora from the topic detection and 
tracking tdt forum and the text retrieval conferences 
 trec we evaluated these methods with non-stationary topics 
at various granularity levels and measured performance with 
different utility settings we found that lr performs strongly 
and robustly in optimizing t su a trec utility function 
while rocchio is better for optimizing ctrk the tdt tracking 
cost a high-recall oriented objective function using systematic 
cross-corpus parameter optimization with both methods we 
obtained the best results ever reported on tdt trec and 
trec relevance feedback on a small portion   
of the tdt test documents yielded significant performance 
improvements measuring up to a reduction in ctrk and a 
 increase in t su with β compared to the results 
of the top-performing system in tdt without relevance 
feedback information 
categories and subject descriptors 
h information search and retrieval information 
filtering relevance feedback retrieval models selection 
process i design methodology classifier design and 
evaluation 
general terms 
algorithms measurement performance experimentation 
 introduction 
adaptive filtering af has been a challenging research topic in 
information retrieval the task is for the system to make an 
online topic membership decision yes or no for every 
document as soon as it arrives with respect to each pre-defined 
topic of interest starting from in the topic detection and 
tracking tdt area and in the text retrieval 
conferences trec benchmark evaluations have been 
conducted by nist under the following 
conditions 
 a very small number to of positive training examples 
was provided for each topic at the starting point 
 relevance feedback was available but only for the 
systemaccepted documents with a yes decision in the trec 
evaluations for af 
 relevance feedback rf was not allowed in the tdt 
evaluations for af or topic tracking in the tdt 
terminology until 
 tdt was the first time that trec and tdt metrics 
were jointly used in evaluating af methods on the same 
benchmark the tdt corpus where non-stationary topics 
dominate 
the above conditions attempt to mimic realistic situations where 
an af system would be used that is the user would be willing 
to provide a few positive examples for each topic of interest at 
the start and might or might not be able to provide additional 
labeling on a small portion of incoming documents through 
relevance feedback furthermore topics of interest might 
change over time with new topics appearing and growing and 
old topics shrinking and diminishing these conditions make 
adaptive filtering a difficult task in statistical learning online 
classification for the following reasons 
 it is difficult to learn accurate models for prediction based 
on extremely sparse training data 
 it is not obvious how to correct the sampling bias i e 
relevance feedback on system-accepted documents only 
during the adaptation process 
 it is not well understood how to effectively tune parameters 
in af methods using cross-corpus validation where the 
validation and evaluation topics do not overlap and the 
documents may be from different sources or different 
epochs 
none of these problems is addressed in the literature of 
statistical learning for batch classification where all the training 
data are given at once the first two problems have been 
studied in the adaptive filtering literature including topic profile 
adaptation using incremental rocchio gaussian-exponential 
density models logistic regression in a bayesian framework 
etc and threshold optimization strategies using probabilistic 
calibration or local fitting techniques 
although these works provide valuable insights for 
understanding the problems and possible solutions it is difficult 
to draw conclusions regarding the effectiveness and robustness 
of current methods because the third problem has not been 
thoroughly investigated addressing the third issue is the main 
focus in this paper 
we argue that robustness is an important measure for evaluating 
and comparing af methods by robust we mean consistent 
and strong performance across benchmark corpora with a 
systematic method for parameter tuning across multiple corpora 
most af methods have pre-specified parameters that may 
influence the performance significantly and that must be 
determined before the test process starts available training 
examples on the other hand are often insufficient for tuning the 
parameters in tdt for example there is only one labeled 
training example per topic at the start parameter optimization 
on such training data is doomed to be ineffective 
this leaves only one option assuming tuning on the test set is 
not an alternative that is choosing an external corpus as the 
validation set notice that the validation-set topics often do not 
overlap with the test-set topics thus the parameter optimization 
is performed under the tough condition that the validation data 
and the test data may be quite different from each other now 
the important question is which methods if any are robust 
under the condition of using cross-corpus validation to tune 
parameters current literature does not offer an answer because 
no thorough investigation on the robustness of af methods has 
been reported 
in this paper we address the above question by conducting a 
cross-benchmark evaluation with two effective approaches in 
af incremental rocchio and regularized logistic regression 
 lr rocchio-style classifiers have been popular in af with 
good performance in benchmark evaluations trec and tdt 
if appropriate parameters were used and if combined with an 
effective threshold calibration strategy 
logistic regression is a classical method in statistical learning 
and one of the best in batch-mode text categorization it 
was recently evaluated in adaptive filtering and was found to 
have relatively strong performance section furthermore a 
recent paper reported that the joint use of rocchio and lr 
in a bayesian framework outperformed the results of using each 
method alone on the trec corpus stimulated by those 
findings we decided to include rocchio and lr in our 
crossbenchmark evaluation for robustness testing specifically we 
focus on how much the performance of these methods depends 
on parameter tuning what the most influential parameters are in 
these methods how difficult or how easy to optimize these 
influential parameters using cross-corpus validation how strong 
these methods perform on multiple benchmarks with the 
systematic tuning of parameters on other corpora and how 
efficient these methods are in running af on large benchmark 
corpora 
the organization of the paper is as follows section introduces 
the four benchmark corpora trec and trec tdt and 
tdt used in this study section analyzes the differences 
among the trec and tdt metrics utilities and tracking cost 
and the potential implications of those differences section 
outlines the rocchio and lr approaches to af respectively 
section reports the experiments and results section 
concludes the main findings in this study 
 benchmark corpora 
we used four benchmark corpora in our study table shows 
the statistics about these data sets 
trec was the evaluation benchmark for adaptive filtering in 
trec consisting of roughly reuters news stories 
from august to august with topic labels subject 
categories the first two weeks august th 
to st 
 of 
documents is the training set and the remaining ½ months 
 from september st 
 to august th 
 is the test set 
trec was the evaluation benchmark for adaptive filtering in 
trec consisting of the same set of documents as those in 
trec but with a slightly different splitting point for the 
training and test sets the trec topics are quite 
different from those in trec they are queries for retrieval 
with relevance judgments by nist assessors 
tdt was the evaluation benchmark in the tdt dry run 
 
the tracking part of the corpus consists of news stories 
from multiple sources in english and mandarin ap nyt 
cnn abc nbc msnbc xinhua zaobao voice of america 
and pri the world in the period of october to december 
machine-translated versions of the non-english stories xinhua 
zaobao and voa mandarin are provided as well the splitting 
point for training-test sets is different for each topic in tdt 
tdt was the evaluation benchmark in tdt the 
tracking part of the corpus consists of news stories in 
the period of april to september from news agents or 
broadcast sources in english arabic and mandarin with 
machine-translated versions of the non-english stories we only 
used the english versions of those documents in our 
experiments for this paper 
the tdt topics differ from trec topics both conceptually 
and statistically instead of generic ever-lasting subject 
categories as those in trec tdt topics are defined at a finer 
level of granularity for events that happen at certain times and 
locations and that are born and die typically associated 
with a bursty distribution over chronologically ordered news 
stories the average size of tdt topics events is two orders 
of magnitude smaller than that of the trec topics figure 
compares the document densities of a trec topic civil 
wars and two tdt topics gunshot and apec summit 
meeting respectively over a -month time period where the 
area under each curve is normalized to one 
the granularity differences among topics and the corresponding 
non-stationary distributions make the cross-benchmark 
evaluation interesting for example algorithms favoring large 
and stable topics may not work well for short-lasting and 
nonstationary topics and vice versa cross-benchmark evaluations 
allow us to test this hypothesis and possibly identify the 
weaknesses in current approaches to adaptive filtering in 
tracking the drifting trends of topics 
 
http www ldc upenn edu projects tdt topics html 
table statistics of benchmark corpora for adaptive filtering evaluations 
n tr is the number of the initial training documents n ts is the number of the test documents 
n is the number of positive examples of a predefined topic is an average over all the topics 
 
 
 
 
 
 
 
 
 
 
 
week 
p topic week 
gunshot tdt 
apec summit meeting tdt 
civil war trec 
figure the temporal nature of topics 
 metrics 
to make our results comparable to the literature we decided to 
use both trec-conventional and tdt-conventional metrics in 
our evaluation 
 trec metrics 
let a b c and d be respectively the numbers of true 
positives false alarms misses and true negatives for a specific 
topic and dcban be the total number of test 
documents the trec-conventional metrics are defined as 
precision baa recall caa 
 
 
caba 
a 
f 
 
 
 
β 
β 
β 
 
η 
ηηβ 
ηβ 
− 
− − 
 
 
 max 
 
caba 
sut 
where parameters β and η were set to and - respectively 
in trec and trec for evaluating the 
performance of a system the performance scores are computed 
for individual topics first and then averaged over topics 
 macroaveraging 
 tdt metrics 
the tdt-conventional metric for topic tracking is defined as 
famisstrk ptpwptpwtc − 
where p t is the percentage of documents on topic t missp is 
the miss rate by the system on that topic fap is the false alarm 
rate and w and w are the costs pre-specified constants for a 
miss and a false alarm respectively the tdt benchmark 
evaluations since have used the settings 
of w w and tp for all topics for evaluating 
the performance of a system ctrk is computed for each topic 
first and then the resulting scores are averaged for a single 
measure the topic-weighted ctrk 
to make the intuition behind this measure transparent we 
substitute the terms in the definition of ctrk as follows 
n 
ca 
tp 
 
 
n 
db 
tp 
 
 − 
ca 
c 
pmiss 
 
 
db 
b 
pfa 
 
 
 
 
 
 
 
bwcw 
n 
db 
b 
n 
db 
w 
ca 
c 
n 
ca 
wtctrk 
 ⋅ 
 
⋅ 
 
⋅ 
 
⋅ 
 
⋅ 
clearly trkc is the average cost per error on topic t with w 
and w controlling the penalty ratio for misses vs false alarms 
in addition to trkc tdt also employed βsut as a 
utility metric to distinguish this from the βsut in 
trec we call former tdt su in the rest of this paper 
corpus topics n tr n ts avg 
n tr 
avg 
n ts 
max 
n ts 
min 
n ts 
 topics per 
doc ts 
trec 
trec 
tdt 
tdt 
 the correlations and the differences 
from an optimization point of view tdt su and t su are 
both utility functions while ctrk is a cost function our objective 
is to maximize the former or to minimize the latter on test 
documents the differences and correlations among these 
objective functions can be analyzed through the shared counts 
of a b c and d in their definitions for example both 
tdt su and t su are positively correlated to the values of a 
and d and negatively correlated to the values of b and c the 
only difference between them is in their penalty ratios for 
misses vs false alarms i e in tdt su and in t su 
the ctrk function on the other hand is positively correlated to 
the values of c and b and negatively correlated to the values of 
a and d hence it is negatively correlated to t su and 
tdt su 
more importantly there is a subtle and major difference 
between ctrk and the utility functions t su and tdt su 
that is ctrk has a very different penalty ratio for misses vs 
false alarms it favors recall-oriented systems to an extreme at 
first glance one would think that the penalty ratio in ctrk is 
 since w and w however this is not true if 
 tp is an inaccurate estimate of the on-topic documents 
on average for the test corpus using tdt as an example the 
true percentage is 
 
 
 
 ≈ 
 
 
n 
n 
tp 
where n is the average size of the test sets in tdt and n is 
the average number of positive examples per topic in the test 
sets using ˆ tp as an inaccurate estimate of 
enlarges the intended penalty ratio of to roughly 
speaking to wit 
 
 
 
 
 
 
 
 
 
bc 
nn 
b 
n 
c 
b 
n 
c 
db 
b 
n 
ca 
n 
c 
faptpwmissptpw 
fapwmisspwttrkc 
× × × ×≈ 
− 
×−× ×× 
 
 
×−× ×× 
×−⋅ ×× 
−× ×× 
⎟ 
⎠ 
⎞ 
⎜ 
⎝ 
⎛ 
⎟ 
⎠ 
⎞ 
⎜ 
⎝ 
⎛ 
ρρ 
where 
 
 
 
 ˆ 
 
tp 
tp 
ρ is the factor of enlargement in the 
estimation of p t compared to the truth comparing the above 
result to formula we can see the actual penalty ratio for 
misses vs false alarms was in the evaluations on tdt 
using ctrk similarly we can compute the enlargement factor 
for tdt using the statistics in table as follows 
 
 
 
 
 ˆ 
 
tp 
tp 
ρ 
which means the actual penalty ratio for misses vs false alarms 
in the evaluation on tdt using ctrk was approximately 
the implications of the above analysis are rather significant 
 ctrk defined in the same formula does not necessarily 
mean the same objective function in evaluation instead 
the optimization criterion depends on the test corpus 
 systems optimized for ctrk would not optimize tdt su 
 and t su because the former favors high-recall 
oriented to an extreme while the latter does not 
 parameters tuned on one corpus e g tdt might not 
work for an evaluation on another corpus say tdt 
unless we account for the previously-unknown subtle 
dependency of ctrk on data 
 results in ctrk in the past years of tdt evaluations may 
not be directly comparable to each other because the 
evaluation collections changed most years and hence the 
penalty ratio in ctrk varied 
although these problems with ctrk were not originally 
anticipated it offered an opportunity to examine the ability of 
systems in trading off precision for extreme recall this was a 
challenging part of the tdt evaluation for af 
comparing the metrics in tdt and trec from a utility or cost 
optimization point of view is important for understanding the 
evaluation results of adaptive filtering methods this is the first 
time this issue is explicitly analyzed to our knowledge 
 methods 
 incremental rocchio for af 
we employed a common version of rocchio-style classifiers 
which computes a prototype vector per topic t as follows 
 
 
 
 
 
td 
d 
td 
d 
tqtp 
tddtdd 
− 
∈ 
 
∈ ∑∑ − 
− 
rr 
rr 
rr 
γβα 
the first term on the rhs is the weighted vector representation 
of topic description whose elements are terms weights the 
second term is the weighted centroid of the set td of 
positive training examples each of which is a vector of 
withindocument term weights the third term is the weighted centroid 
of the set td− of negative training examples which are the 
nearest neighbors of the positive centroid the three terms are 
given pre-specified weights of βα and γ controlling the 
relative influence of these components in the prototype 
the prototype of a topic is updated each time the system makes 
a yes decision on a new document for that topic if relevance 
feedback is available as is the case in trec adaptive filtering 
the new document is added to the pool of 
either td or td− and the prototype is recomputed 
accordingly if relevance feedback is not available as is the case 
in tdt event tracking the system s prediction yes is 
treated as the truth and the new document is added to td for 
updating the prototype both cases are part of our experiments 
in this paper and part of the tdt evaluations for af to 
distinguish the two we call the first case simply rocchio and 
the second case prf rocchio where prf stands for 
pseudorelevance feedback 
the predictions on a new document are made by computing the 
cosine similarity between each topic prototype and the 
document vector and then comparing the resulting scores 
against a threshold 
⎩ 
⎨ 
⎧ 
− 
 
 − 
 
 
 cos 
no 
yes 
dtpsign new θ 
rr 
threshold calibration in incremental rocchio is a challenging 
research topic multiple approaches have been developed the 
simplest is to use a universal threshold for all topics tuned on a 
validation set and fixed during the testing phase more elaborate 
methods include probabilistic threshold calibration which 
converts the non-probabilistic similarity scores to probabilities 
 i e dtp 
r 
 for utility optimization and margin-based 
local regression for risk reduction 
it is beyond the scope of this paper to compare all the different 
ways to adapt rocchio-style methods for af instead our focus 
here is to investigate the robustness of rocchio-style methods in 
terms of how much their performance depends on elaborate 
system tuning and how difficult or how easy it is to get good 
performance through cross-corpus parameter optimization 
hence we decided to use a relatively simple version of rocchio 
as the baseline i e with a universal threshold tuned on a 
validation corpus and fixed for all topics in the testing phase 
this simple version of rocchio has been commonly used in the 
past tdt benchmark evaluations for topic tracking and had 
strong performance in the tdt evaluations for adaptive 
filtering with and without relevance feedback section 
results of more complex variants of rocchio are also discussed 
when relevant 
 logistic regression for af 
logistic regression lr estimates the posterior probability of a 
topic given a document using a sigmoid function 
 xw 
ewxyp 
rrrr ⋅− 
 
where x 
r 
is the document vector whose elements are term 
weights w 
r 
is the vector of regression coefficients and 
} { − ∈y is the output variable corresponding to yes or 
no with respect to a particular topic given a training set of 
labeled documents { } nn yxyxd 
r 
l 
r 
 the 
standard regression problem is defined as to find the maximum 
likelihood estimates of the regression coefficients the model 
parameters 
{ } { } 
{ } exp logminarg 
 logmaxarg maxarg 
ii xwyn 
i 
w 
wdp 
w 
wdp 
w 
mlw 
rr 
r 
r 
r 
r 
r 
r 
⋅− ∑ 
 
this is a convex optimization problem which can be solved 
using a standard conjugate gradient algorithm in o inf time 
for training per topic where i is the average number of 
iterations needed for convergence and n and f are the number 
of training documents and number of features respectively 
once the regression coefficients are optimized on the training 
data the filtering prediction on each incoming document is 
made as 
 
⎩ 
⎨ 
⎧ 
− 
 
 − 
 
 
 
no 
yes 
wxypsign optnew θ 
rr 
note that w 
r 
is constantly updated whenever a new relevance 
judgment is available in the testing phase of af while the 
optimal threshold optθ is constant depending only on the 
predefined utility or cost function for evaluation if t su is the 
metric for example with the penalty ratio of for misses and 
false alarms section the optimal threshold for lr 
is for all topics 
we modified the standard above version of lr to allow more 
flexible optimization criteria as follows 
⎭ 
⎬ 
⎫ 
⎩ 
⎨ 
⎧ 
− ∑ 
⋅− 
 
 log minarg μλ 
rrr rr 
r 
weysw 
n 
i 
xwy 
i 
w 
map 
ii 
where iys is taken to be α β and γ for query positive 
and negative documents respectively which are similar to those 
in rocchio giving different weights to the three kinds of 
training examples topic descriptions queries on-topic 
documents and off-topic documents the second term in the 
objective function is for regularization equivalent to adding a 
gaussian prior to the regression coefficients with mean μ 
r 
and 
covariance variance matrix ι⋅λ where ι is the identity 
matrix tuning λ ≥ is theoretically justified for reducing 
model complexity the effective degree of freedom and 
avoiding over-fitting on training data how to find an 
effective μ 
r 
is an open issue for research depending on the 
user s belief about the parameter space and the optimal range 
the solution of the modified objective function is called the 
maximum a posteriori map estimate which reduces to the 
maximum likelihood solution for standard lr if λ 
 evaluations 
we report our empirical findings in four parts the tdt 
official evaluation results the cross-corpus parameter 
optimization results and the results corresponding to the 
amounts of relevance feedback 
 tdt benchmark results 
the tdt evaluations for adaptive filtering were conducted 
by nist in november multiple research teams 
participated and multiple runs from each team were allowed 
ctrk and tdt su were used as the metrics figure and figure 
 show the results the best run from each team was selected 
with respect to ctrk or tdt su respectively our rocchio 
 with adaptive profiles but fixed universal threshold for all 
topics had the best result in ctrk and our logistic regression 
had the best result in tdt su all the parameters of our runs 
were tuned on the tdt corpus results for other sites are also 
listed anonymously for comparison 
ctrk 
ours 
site 
site 
site 
metric ctrk the lower the better 
 
 
 
 
 
 
 
 
 
 
 
 
ours site site site 
figure tdt results in ctrk of systems using true 
relevance feedback ours is the rocchio method we 
also put the st 
and rd 
quartiles as sticks for each site 
t su 
ours 
site 
site 
site 
metric tdt su the higher the better 
 
 
 
 
 
 
 
 
 
ours site site site 
figure tdt results in tdt su of systems using true 
relevance feedback ours is lr 
with μ 
r 
and λ 
ctrk 
ours 
site 
site 
site 
site 
primary topic traking results in tdt 
 
 
 
 
 
 
 
 
 
 
 
 
ours site site site site 
ctrk 
figure tdt results in ctrk of systems without using 
true relevance feedback ours is prf rocchio 
adaptive filtering without using true relevance feedback was 
also a part of the evaluations in this case systems had only one 
labeled training example per topic during the entire training and 
testing processes although unlabeled test documents could be 
used as soon as predictions on them were made such a setting 
has been conventional for the topic tracking task in tdt until 
 figure shows the summarized official submissions from 
each team our prf rocchio with a fixed threshold for all the 
topics had the best performance 
 
we use quartiles rather than standard deviations since the 
former is more resistant to outliers 
 cross-corpus parameter optimization 
how much the strong performance of our systems depends on 
parameter tuning is an important question 
both rocchio and lr have parameters that must be 
prespecified before the af process the shared parameters include 
the sample weightsα β and γ the sample size of the negative 
training documents i e td− the term-weighting scheme 
and the maximal number of non-zero elements in each 
document vector the method-specific parameters include the 
decision threshold in rocchio and μ 
r 
 λ and mi the maximum 
number of iterations in training in lr given that we only have 
one labeled example per topic in the tdt training sets it is 
impossible to effectively optimize these parameters on the 
training data and we had to choose an external corpus for 
validation among the choices of trec trec and tdt 
we chose tdt c f section because it is most similar to 
tdt in terms of the nature of the topics section we 
optimized the parameters of our systems on tdt and fixed 
those parameters in the runs on tdt for our submissions to 
tdt we also tested our methods on trec and 
trec for further analysis since exhaustive testing of all 
possible parameter settings is computationally intractable we 
followed a step-wise forward chaining procedure instead we 
pre-specified an order of the parameters in a method rocchio 
or lr and then tuned one parameter at the time while fixing 
the settings of the remaining parameters we repeated this 
procedure for several passes as time allowed 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
threshold 
tdt su 
tdt tdt trec trec 
figure performance curves of adaptive rocchio 
figure compares the performance curves in tdt su for 
rocchio on tdt tdt trec and trec when the 
decision threshold varied these curves peak at different 
locations the tdt -optimal is closest to the tdt -optimal 
while the trec -optimal and trec -optimal are quite far 
away from the tdt -optimal if we were using trec or 
trec instead of tdt as the validation corpus for tdt or 
if the tdt corpus were not available we would have difficulty 
in obtaining strong performance for rocchio in tdt the 
difficulty comes from the ad-hoc non-probabilistic scores 
generated by the rocchio method the distribution of the scores 
depends on the corpus making cross-corpus threshold 
optimization a tricky problem 
logistic regression has less difficulty with respect to threshold 
tuning because it produces probabilistic scores of pr xy 
upon which the optimal threshold can be directly computed if 
probability estimation is accurate given the penalty ratio for 
misses vs false alarms as in t su in tdt su and 
 in ctrk section the corresponding optimal 
thresholds t are and respectively 
although the theoretical threshold could be inaccurate it still 
suggests the range of near-optimal settings with these threshold 
settings in our experiments for lr we focused on the 
crosscorpus validation of the bayesian prior parameters that is μ 
r 
and λ table summarizes the results 
 we measured the 
performance of the runs on trec and trec using t su 
and the performance of the runs on tdt and tdt using 
tdt su for comparison we also include the best results of 
rocchio-based methods on these corpora which are our own 
results of rocchio on tdt and tdt and the best results 
reported by nist for trec and trec from this set of 
results we see that lr significantly outperformed rocchio on 
all the corpora even in the runs of standard lr without any 
tuning i e λ this empirical finding is consistent with a 
previous report for lr on trec although our results of 
lr   in t su are stronger than the results 
for standard lr and for lr using rocchio prototype as the 
prior in that report more importantly our cross-benchmark 
evaluation gives strong evidence for the robustness of lr the 
robustness we believe comes from the probabilistic nature of 
the system-generated scores that is compared to the ad-hoc 
scores in rocchio the normalized posterior probabilities make 
the threshold optimization in lr a much easier problem 
moreover logistic regression is known to converge towards the 
bayes classifier asymptotically while rocchio classifiers 
parameters do not 
another interesting observation in these results is that the 
performance of lr did not improve when using a rocchio 
prototype as the mean in the prior instead the performance 
decreased in some cases this observation does not support the 
previous report by but we are not surprised because we are 
not convinced that rocchio prototypes are more accurate than 
lr models for topics in the early stage of the af process and 
we believe that using a rocchio prototype as the mean in the 
gaussian prior would introduce undesirable bias to lr we also 
believe that variance reduction in the testing phase should be 
controlled by the choice of λ but not μ 
r 
 for which we 
conducted the experiments as shown in figure 
table results of lr with different bayesian priors 
corpus tdt tdt trec trec 
lr μ λ 
lr μ λ 
lr μ roc λ 
best rocchio 
 
 
the lr results   on tdt in this table are better 
than our tdt official result because parameter 
optimization has been improved afterwards 
 
the trec -best result by oracle is only available in 
t u which is not directly comparable to the scores in 
t su just indicative 
 μ 
r 
was set to the rocchio prototype 
 
 
 
 
 
 
lambda 
performance 
ctrk on tdt tdt su on tdt 
tdt su on tdt t su on trec 
figure lr with varying lambda 
the performance of lr is summarized with respect to λ tuning 
on the corpora of trec trec and tdt the 
performance on each corpus was measured using the 
corresponding metrics that is t su for the runs on trec 
and trec and tdt su and ctrk for the runs on tdt in 
the case of maximizing the utilities the safe interval for λ is 
between and meaning that the performance of 
regularized lr is stable the same as or improved slightly over 
the performance of standard lr in the case of minimizing ctrk 
the safe range for λ is between and and setting λ between 
 and yielded relatively large improvements over the 
performance of standard lr because training a model for 
extremely high recall is statistically more tricky and hence 
more regularization is needed in either case tuning λ is 
relatively safe and easy to do successfully by cross-corpus 
tuning 
another influential choice in our experiment settings is term 
weighting we examined the choices of binary tf and tf-idf 
 the ltc version schemes we found tf-idf most effective 
for both rocchio and lr and used this setting in all our 
experiments 
 percentages of labeled data 
how much relevance feedback rf would be needed during the 
af process is a meaningful question in real-world applications 
to answer it we evaluated rocchio and lr on tdt with the 
following settings 
 basic rocchio no adaptation at all 
 prf rocchio updating topic profiles without using true 
relevance feedback 
 adaptive rocchio updating topic profiles using relevance 
feedback on system-accepted documents plus 
documents randomly sampled from the pool of 
systemrejected documents 
 lr with 
rr 
 μ λ and threshold 
 all the parameters in rocchio tuned on tdt 
table summarizes the results in ctrk adaptive rocchio with 
relevance feedback on of the test documents reduced the 
tracking cost by over the result of the prf rocchio the 
best system in the tdt evaluation for topic tracking 
without relevance feedback information incremental lr on the 
other hand was weaker but still impressive recall that ctrk is 
an extremely high-recall oriented metric causing frequent 
updating of profiles and hence an efficiency problem in lr for 
this reason we set a higher threshold instead of the 
theoretically optimal threshold in lr to avoid an 
untolerable computation cost the computation time in 
machine-hours was for the run of adaptive rocchio and 
for the run of lr on tdt when optimizing ctrk table 
summarizes the results in tdt su adaptive lr was the winner 
in this case with relevance feedback on of the test 
documents improving the utility by over the results of 
prf rocchio 
table af methods on tdt performance in ctrk 
base roc prf roc adp roc lr 
 of rf 
ctrk 
± baseline - - 
table af methods on tdt performance in tdt su 
base roc prf roc adp roc lr λ 
 of rf 
tdt su 
± - baseline 
evidently both rocchio and lr are highly effective in adaptive 
filtering in terms of using of a small amount of labeled data to 
significantly improve the model accuracy in statistical learning 
which is the main goal of af 
 summary of adaptation process 
after we decided the parameter settings using validation we 
perform the adaptive filtering in the following steps for each 
topic train the lr rocchio model using the provided 
positive training examples and randomly sampled negative 
examples for each document in the test corpus we first 
make a prediction about relevance and then get relevance 
feedback for those predicted positive documents model and 
idf statistics will be incrementally updated if we obtain its true 
relevance feedback 
 concluding remarks 
we presented a cross-benchmark evaluation of incremental 
rocchio and incremental lr in adaptive filtering focusing on 
their robustness in terms of performance consistency with 
respect to cross-corpus parameter optimization our main 
conclusions from this study are the following 
 parameter optimization in af is an open challenge but has 
not been thoroughly studied in the past 
 robustness in cross-corpus parameter tuning is important 
for evaluation and method comparison 
 we found lr more robust than rocchio it had the best 
results in t su ever reported on tdt trec and 
trec without extensive tuning 
 we found rocchio performs strongly when a good 
validation corpus is available and a preferred choice when 
optimizing ctrk is the objective favoring recall over 
precision to an extreme 
for future research we want to study explicit modeling of the 
temporal trends in topic distributions and content drifting 
acknowledgments 
this material is based upon work supported in parts by the 
national science foundation nsf under grant iis- 
by the dod under award -n and by the 
defense advanced research project agency darpa under 
contract no nbchd any opinions findings and 
conclusions or recommendations expressed in this material are 
those of the author s and do not necessarily reflect the views of 
the sponsors 
 references 
 j allan incremental relevance feedback for information 
filtering in sigir- 
 j callan learning while filtering documents in sigir- 
 - 
 j fiscus and g duddington topic detection and tracking 
overview in topic detection and tracking event-based 
information organization - 
 j fiscus and b wheatley overview of the tdt 
evaluation and results in tdt- 
 t hastie r tibshirani and j friedman elements of 
statistical learning springer 
 s robertson and d hull the trec- filtering track final 
report in trec- 
 s robertson and i soboroff the trec- filtering track 
final report in trec- 
 s robertson and i soboroff the trec filtering 
track report in trec- 
 s robertson and s walker microsoft cambridge at 
trec- in trec- 
 r schapire y singer and a singhal boosting and 
rocchio applied to text filtering in sigir- - 
 
 y yang and b kisiel margin-based local regression for 
adaptive filtering in cikm- 
 y zhang and j callan maximum likelihood estimation 
for filtering thresholds in sigir- 
 y zhang using bayesian priors to combine classifiers for 
adaptive filtering in sigir- 
 j zhang and y yang robustness of regularized linear 
classification methods in text categorization in sigir- 
 - 
 t zhang f j oles text categorization based on 
regularized linear classification methods inf retr 
 - 
combining content and link for classification 
using matrix factorization 
shenghuo zhu kai yu yun chi yihong gong 
{zsh kyu ychi ygong} sv nec-labs com 
nec laboratories america inc 
 north wolfe road sw - 
cupertino ca usa 
abstract 
the world wide web contains rich textual contents that are 
interconnected via complex hyperlinks this huge database violates the 
assumption held by most of conventional statistical methods that each 
web page is considered as an independent and identical sample it 
is thus difficult to apply traditional mining or learning methods for 
solving web mining problems e g web page classification by 
exploiting both the content and the link structure the research in this 
direction has recently received considerable attention but are still in 
an early stage though a few methods exploit both the link 
structure or the content information some of them combine the only 
authority information with the content information and the others 
first decompose the link structure into hub and authority features 
then apply them as additional document features being practically 
attractive for its great simplicity this paper aims to design an 
algorithm that exploits both the content and linkage information by 
carrying out a joint factorization on both the linkage adjacency matrix 
and the document-term matrix and derives a new representation 
for web pages in a low-dimensional factor space without explicitly 
separating them as content hub or authority factors further 
analysis can be performed based on the compact representation of web 
pages in the experiments the proposed method is compared with 
state-of-the-art methods and demonstrates an excellent accuracy in 
hypertext classification on the webkb and cora benchmarks 
categories and subject descriptors h information 
systems information search and retrieval 
general terms algorithms experimentation 
 introduction 
with the advance of the world wide web more and more 
hypertext documents become available on the web some examples of 
such data include organizational and personal web pages e g the 
webkb benchmark data set which contains university web pages 
research papers e g data in citeseer online news articles and 
customer-generated media e g blogs comparing to data in 
traditional information management in addition to content these data 
on the web also contain links e g hyperlinks from a student s 
homepage pointing to the homepage of her advisor paper citations 
sources of a news article comments of one blogger on posts from 
another blogger and so on performing information management 
tasks on such structured data raises many new research challenges 
in the following discussion we use the task of web page 
classification as an illustrating example while the techniques we develop 
in later sections are applicable equally well to many other tasks in 
information retrieval and data mining 
for the classification problem of web pages a simple approach 
is to treat web pages as independent documents the advantage 
of this approach is that many off-the-shelf classification tools can 
be directly applied to the problem however this approach 
relies only on the content of web pages and ignores the structure of 
links among them link structures provide invaluable information 
about properties of the documents as well as relationships among 
them for example in the webkb dataset the link structure 
provides additional insights about the relationship among documents 
 e g links often pointing from a student to her advisor or from 
a faculty member to his projects since some links among these 
documents imply the inter-dependence among the documents the 
usual i i d independent and identical distributed assumption of 
documents does not hold any more from this point of view the 
traditional classification methods that ignore the link structure may 
not be suitable 
on the other hand a few studies for example rely solely on 
link structures it is however a very rare case that content 
information can be ignorable for example in the cora dataset the content 
of a research article abstract largely determines the category of the 
article 
to improve the performance of web page classification 
therefore both link structure and content information should be taken 
into consideration to achieve this goal a simple approach is to 
convert one type of information to the other for example in spam 
blog classification kolari et al concatenate outlink features 
with the content features of the blog in document classification 
kurland and lee convert content similarity among documents 
into weights of links however link and content information have 
different properties for example a link is an actual piece of 
evidence that represents an asymmetric relationship whereas the 
content similarity is usually defined conceptually for every pair of 
documents in a symmetric way therefore directly converting one type 
of information to the other usually degrades the quality of 
information on the other hand there exist some studies as we will discuss 
in detail in related work that consider link information and content 
information separately and then combine them we argue that such 
an approach ignores the inherent consistency between link and 
content information and therefore fails to combine the two seamlessly 
some work such as incorporates link information using 
cocitation similarity but this may not fully capture the global link 
structure in figure for example web pages v and v co-cite 
web page v implying that v and v are similar to each other 
in turns v and v should be similar to each other since v and 
v cite similar web pages v and v respectively but using 
cocitation similarity the similarity between v and v is zero without 
considering other information 
v 
v 
v 
v 
v 
v 
v 
v 
figure an example of link structure 
in this paper we propose a simple technique for analyzing 
inter-connected documents such as web pages using factor 
analysis in the proposed technique both content information and 
link structures are seamlessly combined through a single set of 
latent factors our model contains two components the first 
component captures the content information this component has a form 
similar to that of the latent topics in the latent semantic indexing 
 lsi in traditional information retrieval that is documents 
are decomposed into latent topics factors which in turn are 
represented as term vectors the second component captures the 
information contained in the underlying link structure such as links 
from homepages of students to those of faculty members a 
factor can be loosely considered as a type of documents e g those 
homepages belonging to students it is worth noting that we do 
not explicitly define the semantic of a factor a priori instead 
similar to lsi the factors are learned from the data traditional factor 
analysis models the variables associated with entities through the 
factors however in analysis of link structures we need to model 
the relationship of two ends of links i e edges between vertex 
pairs therefore the model should involve factors of both vertices 
of the edge this is a key difference between traditional factor 
analysis and our model in our model we connect two 
components through a set of shared factors that is the latent factors in the 
second component for contents are tied to the factors in the first 
component for links by doing this we search for a unified set 
of latent factors that best explains both content and link structures 
simultaneously and seamlessly 
in the formulation we perform factor analysis based on matrix 
factorization solution to the first component is based on 
factorizing the term-document matrix derived from content features 
solution to the second component is based on factorizing the adjacency 
matrix derived from links because the two factorizations share 
a common base the discovered bases latent factors explain both 
content information and link structures and are then used in further 
information management tasks such as classification 
this paper is organized as follows section reviews related 
work section presents the proposed approach to analyze the web 
page based on the combined information of links and content 
section extends the basic framework and a few variants for fine tune 
section shows the experiment results section discusses the 
details of this approach and section concludes 
 related work 
in the content analysis part our approach is closely related to 
latent semantic indexing lsi lsi maps documents into a 
lower dimensional latent space the latent space implicitly 
captures a large portion of information of documents therefore it is 
called the latent semantic space the similarity between documents 
could be defined by the dot products of the corresponding vectors 
of documents in the latent space analysis tasks such as 
classification could be performed on the latent space the commonly 
used singular value decomposition svd method ensures that the 
data points in the latent space can optimally reconstruct the original 
documents though our approach also uses latent space to 
represent web pages documents we consider the link structure as well 
as the content of web pages 
in the link analysis approach the framework of hubs and 
authorities hits puts web page into two categories hubs and 
authorities using recursive notion a hub is a web page with many 
outgoing links to authorities while an authority is a web page with 
many incoming links from hubs instead of using two categories 
pagerank uses a single category for the recursive notion an 
authority is a web page with many incoming links from authorities 
he et al propose a clustering algorithm for web document 
clustering the algorithm incorporates link structure and the co-citation 
patterns in the algorithm all links are treated as undirected edge of 
the link graph the content information is only used for weighing 
the links by the textual similarity of both ends of the links zhang 
et al uses the undirected graph regularization framework for 
document classification achlioptas et al decompose the web 
into hub and authority attributes then combine them with content 
zhou et al and propose a directed graph regularization 
framework for semi-supervised learning the framework combines 
the hub and authority information of web pages but it is difficult 
to combine the content information into that framework our 
approach consider the content and the directed linkage between topics 
of source and destination web pages in one step which implies the 
topic combines the information of web page as authorities and as 
hubs in a single set of factors 
cohn and hofmann construct the latent space from both 
content and link information using content analysis based on 
probabilistic lsi plsi and link analysis based on phits the 
major difference between the approach of plsi phits and 
our approach is in the part of link analysis in plsi phits the 
link is constructed with the linkage from the topic of the source 
web page to the destination web page in the model the outgoing 
links of the destination web page have no effect on the source web 
page in other words the overall link structure is not utilized in 
phits in our approach the link is constructed with the linkage 
between the factor of the source web page and the factor of the 
destination web page instead of the destination web page itself the 
factor of the destination web page contains information of its 
outgoing links in turn such information is passed to the factor of the 
source web page as the result of matrix factorization the factor 
forms a factor graph a miniature of the original graph preserving 
the major structure of the original graph 
taskar et al propose relational markov networks rmns 
for entity classification by describing a conditional distribution of 
entity classes given entity attributes and relationships the model 
was applied to web page classification where web pages are 
entities and hyperlinks are treated as relationships rmns apply 
conditional random fields to define a set of potential functions on cliques 
of random variables where the link structure provides hints to form 
the cliques however the model does not give an off-the-shelf 
solution because the success highly depends on the arts of designing 
the potential functions on the other hand the inference for rmns 
is intractable and requires belief propagation 
the following are some work on combining documents and 
links but the methods are loosely related to our approach the 
experiments of show that using terms from the linked 
document improves the classification accuracy chakrabarti et al use 
co-citation information in their classification model joachims et 
al combine text kernels and co-citation kernels for 
classification oh et al use the naive bayesian frame to combine link 
information with content 
 our approach 
in this section we will first introduce a novel matrix 
factorization method which is more suitable than conventional matrix 
factorization methods for link analysis then we will introduce our 
approach that jointly factorizes the document-term matrix and link 
matrix and obtains compact and highly indicative factors for 
representing documents or web pages 
 link matrix factorization 
suppose we have a directed graph g v e where the vertex 
set v {vi}n 
i represents the web pages and the edge set e 
represents the hyperlinks between web pages let a {asd} denotes 
the n×n adjacency matrix of g which is also called the link matrix 
in this paper for a pair of vertices vs and vd let asd when 
there is an edge from vs to vd and asd otherwise note that 
a is an asymmetric matrix because hyperlinks are directed 
most machine learning algorithms assume a feature-vector 
representation of instances for web page classification however the 
link graph does not readily give such a vector representation for 
web pages if one directly uses each row or column of a for the job 
she will suffer a very high computational cost because the 
dimensionality equals to the number of web pages on the other hand it 
will produces a poor classification accuracy see our experiments 
in section because a is extremely sparse 
 
the idea of link matrix factorization is to derive a high-quality 
feature representation z of web pages based on analyzing the link 
matrix a where z is an n × l matrix with each row being the 
ldimensional feature vector of a web page the new representation 
of web pages captures the principal factors of the link structure and 
makes further processing more efficient 
one may use a method similar to lsi to apply the well-known 
principal component analysis pca for deriving z from a the 
corresponding optimization problem 
is 
min 
z u 
a − zu 
f γ u 
f 
where γ is a small positive number u is an l ×n matrix and · f 
is the frobenius norm the optimization aims to approximate a by 
zu a product of two low-rank matrices with a regularization on 
u in the end the i-th row vector of z can be thought as the hub 
feature vector of vertex vi and the row vector of u can be thought 
as the authority features a link generation model proposed in is 
similar to the pca approach since a is a nonnegative matrix here 
one can also consider to put nonnegative constraints on u and z 
which produces an algorithm similar to plsa and nmf 
 
due to the sparsity of a links from two similar pages may not 
share any common target pages which makes them to appear 
dissimilar however the two pages may be indirectly linked to many 
common pages via their neighbors 
 
another equivalent form is minz u a − zu 
f s t u u 
i the solution z is identical subject to a scaling factor 
however despite its popularity in matrix analysis pca or other 
similar methods like plsa is restrictive for link matrix 
factorization the major problem is that pca ignores the fact that the rows 
and columns of a are indexed by exactly the same set of objects 
 i e web pages the approximating matrix ˜a zu shows no 
evidence that links are within the same set of objects to see the 
drawback let s consider a link transitivity situation vi → vs → vj 
where page i is linked to page s which itself is linked to page j 
since ˜a zu treats a as links from web pages {vi} to a 
different set of objects let it be denoted by {oi} ˜a zu actually 
splits an linked object os from vs and breaks down the link path 
into two parts vi → os and vs → oj this is obviously a miss 
interpretation to the original link path 
to overcome the problem of pca in this paper we suggest to 
use a different factorization 
min 
z u 
a − zuz 
f γ u 
f 
where u is an l × l full matrix note that u is not symmetric thus 
zuz produces an asymmetric matrix which is the case of a 
again each row vector of z corresponds to a feature vector of a 
web pages the new approximating form ˜a zuz puts a clear 
meaning that the links are between the same set of objects 
represented by features z the factor model actually maps each vertex 
vi into a vector zi {zi k ≤ k ≤ l} in the rl 
space we call 
the rl 
space the factor space then {zi} encodes the information 
of incoming and outgoing connectivity of vertices {vi} the 
factor loadings u explain how these observed connections happened 
based on {zi} once we have the vector zi we can use many 
traditional classification methods such as svms or clustering tools 
 such as k-means to perform the analysis 
illustration based on a synthetic problem 
to further illustrate the advantages of the proposed link matrix 
factorization eq let us consider the graph in figure given 
v 
v 
v 
v 
v 
v 
v 
v 
figure summarize figure with a factor graph 
these observations we can summarize the graph by grouping as 
factor graph depicted in figure in the next we preform the two 
factorization methods eq and eq on this link matrix a 
good low-rank representation should reveal the structure of the 
factor graph 
first we try pca-like decomposition solving eq and 
obtaining 
z u 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 − − 
 − 
 − 
 − − 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 − 
 − 
 − − 
 − 
 − 
 − − 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
we can see that the row vectors of v and v are the same in z 
indicating that v and v have the same hub attributes the row 
vectors of v and v are the same in u indicating that v and 
v have the same authority attributes it is not clear to see the 
similarity between v and v because their inlinks and outlinks 
are different 
then we factorize a by zuz via solving eq and obtain 
the results 
z u 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
− − − − 
− − − 
− − − 
 − − 
 − − 
− − 
− − 
− − − − 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
− − − 
 − − − 
 − 
 − − − 
− − − − 
 
 
 
 
 
 
 
 
 
 
the resultant z is very consistent with the clustering structure 
of vertices the row vectors of v and v are the same those 
of v and v are the same those of v and v are the same 
even interestingly if we add constraints to ensure z and u be 
nonnegative we have 
z u 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
which clearly tells the assignment of vertices to clusters from z 
and the links of factor graph from u when the interpretability is 
not critical in some tasks for example classification we found that 
it achieves better accuracies without the nonnegative constraints 
given our above analysis it is clear that the factorization zuz 
is more expressive than zu in representing the link matrix a 
 content matrix factorization 
now let us consider the content information on the vertices to 
combine the link information and content information we want to 
use the same latent space to approximate the content as the latent 
space for the links using the bag-of-words approach we denote 
the content of web pages by an n×m matrix c each of whose rows 
represents a document each column represents a keyword where 
m is the number of keywords like the latent semantic indexing 
 lsi the l-dimensional latent space for words is denoted by an 
m × l matrix v therefore we use zv to approximate matrix 
c 
min 
v z 
c − zv 
f β v 
f 
where β is a small positive number β v 
f serves as a 
regularization term to improve the robustness 
 joint link-content matrix factorization 
there are many ways to employ both the content and link 
information for web page classification our idea in this paper is not to 
simply combine them but rather to fuse them into a single 
consistent and compact feature representation to achieve this goal we 
solve the following problem 
min 
u v z 
n 
j u v z 
def 
 a − zuz 
f 
α c − zv 
f γ u 
f β v 
f 
o 
 
 
eq is the joined matrix factorization of a and c with 
regularization the new representation z is ensured to capture both the 
structures of the link matrix a and the content matrix c once 
we find the optimal z we can apply the traditional classification 
or clustering methods on vectorial data z the relationship among 
these matrices can be depicted as figure 
a y c 
u z v 
figure relationship among the matrices node y is the 
target of classification 
eq can be solved using gradient methods such as the 
conjugate gradient method and quasi-newton methods then main 
computation of gradient methods is evaluating the object function j 
and its gradients against variables 
∂j 
∂u 
 
 
z zuz z − z az 
 
 γu 
∂j 
∂v 
 α 
 
v z z − c z 
 
 βv 
∂j 
∂z 
 
 
zu z zu zuz zu − a zu − azu 
 
 α 
 
zv v − cv 
 
 
because of the sparsity of a the computational complexity of 
multiplication of a and z is o µal where µa is the number of 
nonzero entries in a similarly the computational complexity of 
c z and cv is o µc l where µc is the number of nonzero 
entries in c the computational complexity of the rest 
multiplications in the gradient computation is o nl 
 therefore the total 
computational complexity in one iteration is o µal µc l nl 
 
the number of links and the number of words in a web page are 
relatively small comparing to the number of web pages and are 
almost constant as the number of web pages documents increases i e 
µa o n and µc o n therefore theoretically the 
computation time is almost linear to the number of web pages documents 
n 
 supervised matrix 
factorization 
consider a web page classification problem we can solve 
eq to obtain z as section then use a traditional classifier 
to perform classification however this approach does not take 
data labels into account in the first step believing that using data 
labels improves the accuracy by obtaining a better z for the 
classification we consider to use the data labels to guide the matrix 
factorization called supervised matrix factorization because 
some data used in the matrix factorization have no label 
information the supervised matrix factorization falls into the category of 
semi-supervised learning 
let c be the set of classes for simplicity we first consider 
binary class problem i e c {− } assume we know the 
labels {yi} for vertices in t ⊂ v we want to find a hypothesis 
h v → r such that we assign vi to when h vi ≥ − 
otherwise we assume a transform from the latent space to r is linear 
i e 
h vi w φ vi b w zi b 
school course dept faculty other project staff student total 
cornell 
texas 
washington 
wisconsin 
table dataset of webkb 
where w and b are parameters to estimate here w is the norm 
of the decision boundary similar to support vector machines 
 svms we can use the hinge loss to measure the loss 
x 
i vi∈t 
 − yih vi 
where x is x if x ≥ if x however the hinge loss 
is not smooth at the hinge point which makes it difficult to apply 
gradient methods on the problem to overcome the difficulty we 
use a smoothed version of hinge loss for each data point 
g yih vi 
where 
g x 
 
 
 
 when x ≥ 
 − x when x ≤ 
 
 
 x − 
when x 
we reduce a multiclass problem into multiple binary ones one 
simple scheme of reduction is the one-against-rest coding scheme 
in the one-against-rest scheme we assign a label vector for each 
class label the element of a label vector is if the data point 
belongs the corresponding class − if the data point does not belong 
the corresponding class if the data point is not labeled let y be 
the label matrix each column of which is a label vector therefore 
y is a matrix of n × c where c is the number of classes c then 
the values of eq form a matrix 
h zw b 
where is a vector of size n whose elements are all one w is a 
c × l parameter matrix and b is a parameter vector of size c the 
total loss is proportional to the sum of eq over all labeled data 
points and the classes 
ly w b z λ 
x 
i vi∈t j∈c 
g yijhij 
where λ is the parameter to scale the term 
to derive a robust solution we also use tikhonov regularization 
for w 
ωw w 
ν 
 
w 
f 
where ν is the parameter to scale the term 
then the supervised matrix factorization problem becomes 
min 
u v z w b 
js u v z w b 
where 
js u v z w b j u v z ly w b z ωw w 
we can also use gradient methods to solve the problem of eq 
the gradients are 
∂js 
∂u 
 
∂j 
∂u 
 
∂js 
∂v 
 
∂j 
∂v 
 
∂js 
∂z 
 
∂j 
∂z 
 λgw 
∂js 
∂w 
 λg z νw 
∂js 
∂b 
 λg 
where g is an n×c matrix whose ik-th element is yikg yikhik 
and 
g x 
 
 
 
 when x ≥ 
− when x ≤ 
 
 
 x − when x 
once we obtain w b and z we can apply h on the vertices with 
unknown class labels or apply traditional classification algorithms 
on z to get the classification results 
 experiments 
 data description 
in this section we perform classification on two datasets to 
demonstrate the our approach the two datasets are the webkb 
data set and the cora data set the webkb data set 
consists of about web pages from computer science departments 
of four schools cornell texas washington and wisconsin the 
web pages are classified into seven categories the numbers of 
pages in each category are shown in table the cora data set 
consists of the abstracts and references of about computer 
science research papers we use part of them to categorize into 
one of subfields of data structure ds hardware and architecture 
 ha machine learning ml and programing language pl we 
remove those articles without reference to other articles in the set 
the number of papers and the number of subfields in each area are 
shown in table 
area of papers of subfields 
data structure ds 
hardware and architecture ha 
machine learning ml 
programing language pl 
table dataset of cora 
 methods 
the task of the experiments is to classify the data based on their 
content information and or link structure we use the following 
methods 
 
 
 
 
 
 
 
 
wisconsinwashingtontexascornell 
accuracy 
dataset 
svm on content 
svm on link 
svm on link-content 
directed graph reg 
plsi phits 
link-content mf 
link-content sup mf 
method cornell texas washington wisconsin 
svm on content ± ± ± ± 
svm on links ± ± ± ± 
svm on link-content ± ± ± ± 
directed graph regularization ± ± ± ± 
plsi phits ± ± ± ± 
link-content mf ± ± ± ± 
link-content sup mf ± ± ± ± 
table classification accuracy mean ± std-err on webkb data set 
 svm on content we apply support vector machines svm 
on the content of documents the features are the 
bag-ofwords and all word are stemmed this method ignores link 
structure in the data linear svm is used the 
regularization parameter of svm is selected using the cross-validation 
method the implementation of svm used in the 
experiments is libsvm 
 svm on links we treat links as the features of each 
document i e the i-th feature is link-to-pagei we apply svm on 
link features this method uses link information but not the 
link structure 
 svm on link-content we combine the features of the above 
two methods we use different weights for these two set 
of features the weights are also selected using 
crossvalidation 
 directed graph regularization this method is described in 
 and this method is solely based on link structure 
 plsi phits this method is described in this method 
combines text content information and link structure for 
analysis the phits algorithm is in spirit similar to eq 
with an additional nonnegative constraint it models the 
outgoing and in-coming structures separately 
 link-content mf this is our approach of matrix 
factorization described in section we use latent factors for z 
after we compute z we train a linear svm using z as the 
feature vectors then apply svm on testing portion of z to 
obtain the final result because of the multiclass output 
 link-content sup mf this method is our approach of the 
supervised matrix factorization in section we use latent 
factors for z after we compute z we train a linear svm 
on the training portion of z then apply svm on testing 
portion of z to obtain the final result because of the multiclass 
output 
we randomly split data into five folds and repeat the experiment 
for five times for each time we use one fold for test four other 
folds for training during the training process we use the 
crossvalidation to select all model parameters we measure the results 
by the classification accuracy i e the percentage of the number 
of correct classified documents in the entire data set the results 
are shown as the average classification accuracies and it standard 
deviation over the five repeats 
 results 
the average classification accuracies for the webkb data set are 
shown in table for this task the accuracies of svm on links 
are worse than that of svm on content but the directed graph 
regularization which is also based on link alone achieves a much 
higher accuracy this implies that the link structure plays an 
important role in the classification of this dataset but individual links 
in a web page give little information the combination of link and 
content using svm achieves similar accuracy as that of svm on 
content alone which confirms individual links in a web page give 
little information since our approach consider the link structure 
as well as the content information our two methods give results 
a highest accuracies among these approaches the difference 
between the results of our two methods is not significant however in 
the experiments below we show the difference between them 
the classification accuracies for the cora data set are shown in 
table in this experiment the accuracies of svm on the 
combination of links and content are higher than either svm on content 
or svm on links this indicates both content and links are 
infor 
 
 
 
 
 
 
 
plmlhads 
accuracy 
dataset 
svm on content 
svm on link 
svm on link-content 
directed graph reg 
plsi phits 
link-content mf 
link-content sup mf 
method ds ha ml pl 
svm on content ± ± ± ± 
svm on links ± ± ± ± 
svm on link-content ± ± ± ± 
directed graph regularization ± ± ± ± 
plsi phits ± ± ± ± 
link-content mf ± ± ± ± 
link-content sup mf ± ± ± ± 
table classification accuracy mean ± std-err on cora data set 
mative for classifying the articles into subfields the method of 
directed graph regularization does not perform as good as svm on 
link-content which confirms the importance of the article content 
in this task though our method of link-content matrix 
factorization perform slightly better than other methods our method of 
linkcontent supervised matrix factorization outperform significantly 
 the number of factors 
as we discussed in section the computational complexity of 
each iteration for solving the optimization problem is quadratic to 
the number of factors we perform experiments to study how the 
number of factors affects the accuracy of predication we use 
different numbers of factors for the cornell data of webkb data set 
and the machine learning ml data of cora data set the result 
shown in figure a and b the figures show that the accuracy 
 
 
 
 
 
 
 
 
 
accuracy 
number of factors 
link-content sup mf 
link-content mf 
 a cornell data 
 
 
 
 
 
 
 
 
 
 
 
accuracy 
number of factors 
link-content sup mf 
link-content mf 
 b ml data 
figure accuracy vs number of factors 
increases as the number of factors increases it is a different 
concept from choosing the optimal number of clusters in clustering 
application it is how much information to represent in the latent 
variables we have considered the regularization over the factors 
which avoids the overfit problem for a large number of factors to 
choose of the number of factors we need to consider the trade-off 
between the accuracy and the computation time which is quadratic 
to the number of factors 
the difference between the method of matrix factorization and 
that of supervised one decreases as the number of factors increases 
this indicates that the usefulness of supervised matrix factorization 
at lower number of factors 
 discussions 
the loss functions la in eq and lc in eq use squared 
loss due to computationally convenience actually squared loss 
does not precisely describe the underlying noise model because 
the weights of adjacency matrix can only take nonnegative 
values in our case zero or one only and the components of 
content matrix c can only take nonnegative integers therefore we 
can apply other types of loss such as hinge loss or smoothed 
hinge loss e g la u z µh a zuz where h a b p 
i j − aijbij 
in our paper we mainly discuss the application of classification 
a entry of matrix z means the relationship of a web page and a 
factor the values of the entries are the weights of linear model 
instead of the probabilities of web pages belonging to latent 
topics therefore we allow the components take any possible real 
values when we come to the clustering application we can use this 
model to find z then apply k-means to partition the web pages 
into clusters actually we can use the idea of nonnegative matrix 
factorization for clustering to directly cluster web pages as 
the example with nonnegative constraints shown in section we 
represent each cluster by a latent topic i e the dimensionality of 
the latent space is set to the number of clusters we want then the 
problem of eq becomes 
min 
u v z 
j u v z s t z ≥ 
solving eq we can obtain more interpretable results which 
could be used for clustering 
 conclusions 
in this paper we study the problem of how to combine the 
information of content and links for web page analysis mainly on 
classification application we propose a simple approach using factors to 
model the text content and link structure of web pages documents 
the directed links are generated from the linear combination of 
linkage of between source and destination factors by sharing 
factors between text content and link structure it is easy to combine 
both the content information and link structure our experiments 
show our approach is effective for classification we also discuss 
an extension for clustering application 
acknowledgment 
we would like to thank dr dengyong zhou for sharing his code 
of his algorithm also thanks to the reviewers for constructive 
comments 
 references 
 cmu world wide knowledge base webkb project 
available at http www cs cmu edu ∼webkb 
 d achlioptas a fiat a r karlin and f mcsherry web 
search via hub synthesis in ieee symposium on 
foundations of computer science pages - 
 s chakrabarti b e dom and p indyk enhanced hypertext 
categorization using hyperlinks in l m haas and 
a tiwary editors proceedings of sigmod- acm 
international conference on management of data pages 
 - seattle us acm press new york us 
 c -c chang and c -j lin libsvm a library for support 
vector machines software available at 
http www csie ntu edu tw ∼cjlin libsvm 
 d cohn and h chang learning to probabilistically identify 
authoritative documents proc icml pp - 
 
 d cohn and t hofmann the missing link - a probabilistic 
model of document content and hypertext connectivity in 
t k leen t g dietterich and v tresp editors advances 
in neural information processing systems pages 
 - mit press 
 c cortes and v vapnik support-vector networks machine 
learning 
 s c deerwester s t dumais t k landauer g w 
furnas and r a harshman indexing by latent semantic 
analysis journal of the american society of information 
science - 
 x he h zha c ding and h simon web document 
clustering using hyperlink structures computational 
statistics and data analysis - 
 t hofmann probabilistic latent semantic indexing in 
proceedings of the twenty-second annual international 
sigir conference 
 t joachims n cristianini and j shawe-taylor composite 
kernels for hypertext categorisation in c brodley and 
a danyluk editors proceedings of icml- th 
international conference on machine learning pages 
 - williams college us morgan kaufmann 
publishers san francisco us 
 j m kleinberg authoritative sources in a hyperlinked 
environment j acm - 
 p kolari t finin and a joshi svms for the blogosphere 
blog identification and splog detection in aaai spring 
symposium on computational approaches to analysing 
weblogs march 
 o kurland and l lee pagerank without hyperlinks 
structural re-ranking using links induced by language 
models in sigir proceedings of the th annual 
international acm sigir conference on research and 
development in information retrieval pages - new 
york ny usa acm press 
 a mccallum k nigam j rennie and k seymore 
automating the contruction of internet portals with machine 
learning information retrieval journal - 
 h -j oh s h myaeng and m -h lee a practical 
hypertext catergorization method using links and 
incrementally available class information in sigir 
proceedings of the rd annual international acm sigir 
conference on research and development in information 
retrieval pages - new york ny usa acm 
press 
 l page s brin r motowani and t winograd pagerank 
citation ranking bring order to the web stanford digital 
library working paper - 
 c spearman general intelligence objectively determined 
and measured the american journal of psychology 
 - apr 
 b taskar p abbeel and d koller discriminative 
probabilistic models for relational data in proceedings of 
 th international uai conference 
 w xu x liu and y gong document clustering based on 
non-negative matrix factorization in sigir 
proceedings of the th annual international acm sigir 
conference on research and development in informaion 
retrieval pages - acm press 
 y yang s slattery and r ghani a study of approaches to 
hypertext categorization journal of intelligent information 
systems - - 
 k yu s yu and v tresp multi-label informed latent 
semantic indexing in sigir proceedings of the th 
annual international acm sigir conference on research 
and development in information retrieval pages - 
new york ny usa acm press 
 t zhang a popescul and b dom linear prediction 
models with graph regularization for web-page 
categorization in kdd proceedings of the th acm 
sigkdd international conference on knowledge discovery 
and data mining pages - new york ny usa 
 acm press 
 d zhou j huang and b sch¨olkopf learning from labeled 
and unlabeled data on a directed graph in proceedings of the 
 nd international conference on machine learning bonn 
germany 
 d zhou b sch¨olkopf and t hofmann semi-supervised 
learning on directed graphs proc neural info processing 
systems 
implicit user modeling for personalized search 
xuehua shen bin tan chengxiang zhai 
department of computer science 
university of illinois at urbana-champaign 
abstract 
information retrieval systems e g web search engines are 
critical for overcoming information overload a major deficiency of 
existing retrieval systems is that they generally lack user 
modeling and are not adaptive to individual users resulting in inherently 
non-optimal retrieval performance for example a tourist and a 
programmer may use the same word java to search for different 
information but the current search systems would return the same 
results in this paper we study how to infer a user s interest from 
the user s search context and use the inferred implicit user model 
for personalized search we present a decision theoretic framework 
and develop techniques for implicit user modeling in information 
retrieval we develop an intelligent client-side web search agent 
 ucair that can perform eager implicit feedback e g query 
expansion based on previous queries and immediate result reranking 
based on clickthrough information experiments on web search 
show that our search agent can improve search accuracy over the 
popular google search engine 
categories and subject descriptors 
h information search and retrieval retrieval models 
relevance feedback search process 
general terms 
algorithms 
 introduction 
although many information retrieval systems e g web search 
engines and digital library systems have been successfully deployed 
the current retrieval systems are far from optimal a major 
deficiency of existing retrieval systems is that they generally lack user 
modeling and are not adaptive to individual users this 
inherent non-optimality is seen clearly in the following two cases 
 different users may use exactly the same query e g java to 
search for different information e g the java island in indonesia or 
the java programming language but existing ir systems return the 
same results for these users without considering the actual user it 
is impossible to know which sense java refers to in a query 
a user s information needs may change over time the same user 
may use java sometimes to mean the java island in indonesia 
and some other times to mean the programming language 
without recognizing the search context it would be again impossible to 
recognize the correct sense 
in order to optimize retrieval accuracy we clearly need to model 
the user appropriately and personalize search according to each 
individual user the major goal of user modeling for information 
retrieval is to accurately model a user s information need which is 
unfortunately a very difficult task indeed it is even hard for a user 
to precisely describe what his her information need is 
what information is available for a system to infer a user s 
information need obviously the user s query provides the most direct 
evidence indeed most existing retrieval systems rely solely on 
the query to model a user s information need however since a 
query is often extremely short the user model constructed based 
on a keyword query is inevitably impoverished an effective way 
to improve user modeling in information retrieval is to ask the user 
to explicitly specify which documents are relevant i e useful for 
satisfying his her information need and then to improve user 
modeling based on such examples of relevant documents this is called 
relevance feedback which has been proved to be quite effective for 
improving retrieval accuracy unfortunately in real world 
applications users are usually reluctant to make the extra effort to 
provide relevant examples for feedback 
it is thus very interesting to study how to infer a user s 
information need based on any implicit feedback information which 
naturally exists through user interactions and thus does not require 
any extra user effort indeed several previous studies have shown 
that implicit user modeling can improve retrieval accuracy in 
a web browser curious browser is developed to record a user s 
explicit relevance ratings of web pages relevance feedback and 
browsing behavior when viewing a page such as dwelling time 
mouse click mouse movement and scrolling implicit feedback 
it is shown that the dwelling time on a page amount of scrolling 
on a page and the combination of time and scrolling have a strong 
correlation with explicit relevance ratings which suggests that 
implicit feedback may be helpful for inferring user information need 
in user clickthrough data is collected as training data to learn 
a retrieval function which is used to produce a customized ranking 
of search results that suits a group of users preferences in 
the clickthrough data collected over a long time period is exploited 
through query expansion to improve retrieval accuracy 
 
while a user may have general long term interests and 
preferences for information often he she is searching for documents to 
satisfy an ad-hoc information need which only lasts for a short 
period of time once the information need is satisfied the user 
would generally no longer be interested in such information for 
example a user may be looking for information about used cars 
in order to buy one but once the user has bought a car he she is 
generally no longer interested in such information in such cases 
implicit feedback information collected over a long period of time 
is unlikely to be very useful but the immediate search context and 
feedback information such as which of the search results for the 
current information need are viewed can be expected to be much 
more useful consider the query java again any of the 
following immediate feedback information about the user could 
potentially help determine the intended meaning of java in the query 
 the previous query submitted by the user is hashtable as 
opposed to e g travel indonesia in the search results the user 
viewed a page where words such as programming software 
and applet occur many times 
to the best of our knowledge how to exploit such immediate 
and short-term search context to improve search has so far not been 
well addressed in the previous work in this paper we study how to 
construct and update a user model based on the immediate search 
context and implicit feedback information and use the model to 
improve the accuracy of ad-hoc retrieval in order to maximally 
benefit the user of a retrieval system through implicit user modeling 
we propose to perform eager implicit feedback that is as soon 
as we observe any new piece of evidence from the user we would 
update the system s belief about the user s information need and 
respond with improved retrieval results based on the updated user 
model we present a decision-theoretic framework for optimizing 
interactive information retrieval based on eager user model 
updating in which the system responds to every action of the user by 
choosing a system action to optimize a utility function in a 
traditional retrieval paradigm the retrieval problem is to match a query 
with documents and rank documents according to their relevance 
values as a result the retrieval process is a simple independent 
cycle of query and result display in the proposed new retrieval 
paradigm the user s search context plays an important role and the 
inferred implicit user model is exploited immediately to benefit the 
user the new retrieval paradigm is thus fundamentally different 
from the traditional paradigm and is inherently more general 
we further propose specific techniques to capture and exploit two 
types of implicit feedback information identifying related 
immediately preceding query and using the query and the 
corresponding search results to select appropriate terms to expand the current 
query and exploiting the viewed document summaries to 
immediately rerank any documents that have not yet been seen by the 
user using these techniques we develop a client-side web search 
agent ucair user-centered adaptive information retrieval on 
top of a popular search engine google experiments on web 
search show that our search agent can improve search accuracy over 
google since the implicit information we exploit already naturally 
exists through user interactions the user does not need to make any 
extra effort thus the developed search agent can improve existing 
web search performance without additional effort from the user 
the remaining sections are organized as follows in section 
we discuss the related work in section we present a 
decisiontheoretic interactive retrieval framework for implicit user modeling 
in section we present the design and implementation of an 
intelligent client-side web search agent ucair that performs eager 
implicit feedback in section we report our experiment results 
using the search agent section concludes our work 
 related work 
implicit user modeling for personalized search has been 
studied in previous work but our work differs from all previous work 
in several aspects we emphasize the exploitation of 
immediate search context such as the related immediately preceding query 
and the viewed documents in the same session while most previous 
work relies on long-term collection of implicit feedback 
information we perform eager feedback and bring the benefit of 
implicit user modeling as soon as any new implicit feedback 
information is available while the previous work mostly exploits 
longterm implicit feedback we propose a retrieval framework 
to integrate implicit user modeling with the interactive retrieval 
process while the previous work either studies implicit user modeling 
separately from retrieval or only studies specific retrieval 
models for exploiting implicit feedback to better match a query with 
documents we develop and evaluate a 
personalized web search agent with online user studies while most existing 
work evaluates algorithms offline without real user interactions 
currently some search engines provide rudimentary 
personalization such as google personalized web search which allows 
users to explicitly describe their interests by selecting from 
predefined topics so that those results that match their interests are 
brought to the top and my yahoo search which gives users 
the option to save web sites they like and block those they 
dislike in contrast ucair personalizes web search through implicit 
user modeling without any additional user efforts furthermore the 
personalization of ucair is provided on the client side there are 
two remarkable advantages on this first the user does not need to 
worry about the privacy infringement which is a big concern for 
personalized search second both the computation of 
personalization and the storage of the user profile are done at the client 
side so that the server load is reduced dramatically 
there have been many works studying user query logs or 
query dynamics ucair makes direct use of a user s query 
history to benefit the same user immediately in the same search 
session ucair first judges whether two neighboring queries 
belong to the same information session and if so it selects terms from 
the previous query to perform query expansion 
our query expansion approach is similar to automatic query 
expansion but instead of using pseudo feedback to expand 
the query we use user s implicit feedback information to expand 
the current query these two techniques may be combined 
 optimization in interactive ir 
in interactive ir a user interacts with the retrieval system through 
an action dialogue in which the system responds to each user 
action with some system action for example the user s action may 
be submitting a query and the system s response may be returning 
a list of document summaries in general the space of user 
actions and system responses and their granularities would depend on 
the interface of a particular retrieval system 
in principle every action of the user can potentially provide new 
evidence to help the system better infer the user s information need 
thus in order to respond optimally the system should use all the 
evidence collected so far about the user when choosing a response 
when viewed in this way most existing search engines are clearly 
non-optimal for example if a user has viewed some documents on 
the first page of search results when the user clicks on the next 
link to fetch more results an existing retrieval system would still 
return the next page of results retrieved based on the original query 
without considering the new evidence that a particular result has 
been viewed by the user 
 
we propose to optimize retrieval performance by adapting 
system responses based on every action that a user has taken and cast 
the optimization problem as a decision task specifically at any 
time the system would attempt to do two tasks user model 
updating monitor any useful evidence from the user regarding 
his her information need and update the user model as soon as such 
evidence is available improving search results rerank 
immediately all the documents that the user has not yet seen as soon 
as the user model is updated we emphasize eager updating and 
reranking which makes our work quite different from any existing 
work below we present a formal decision theoretic framework for 
optimizing retrieval performance through implicit user modeling in 
interactive information retrieval 
 a decision-theoretic framework 
let a be the set of all user actions and r a be the set of all 
possible system responses to a user action a ∈ a at any time let 
at a at be the observed sequence of user actions so far 
 up to time point t and rt− r rt− be the responses that 
the system has made responding to the user actions the system s 
goal is to choose an optimal response rt ∈ r at for the current 
user action at 
let m be the space of all possible user models we further 
define a loss function l a r m ∈ where a ∈ a is a user action 
r ∈ r a is a system response and m ∈ m is a user model 
l a r m encodes our decision preferences and assesses the 
optimality of responding with r when the current user model is m 
and the current user action is a according to bayesian decision 
theory the optimal decision at time t is to choose a response that 
minimizes the bayes risk i e 
r∗ 
t argmin 
r∈r at m 
l at r mt p mt u d at rt− dmt 
where p mt u d at rt− is the posterior probability of the 
user model mt given all the observations about the user u we have 
made up to time t 
to simplify the computation of equation let us assume that the 
posterior probability mass p mt u d at rt− is mostly 
concentrated on the mode m∗ 
t argmaxmt p mt u d at rt− 
we can then approximate the integral with the value of the loss 
function at m∗ 
t that is 
r∗ 
t ≈ argminr∈r at l at r m∗ 
t 
where m∗ 
t argmaxmt p mt u d at rt− 
leaving aside how to define and estimate these probabilistic 
models and the loss function we can see that such a decision-theoretic 
formulation suggests that in order to choose the optimal response 
to at the system should perform two tasks compute the 
current user model and obtain m∗ 
t based on all the useful 
information choose a response rt to minimize the loss function value 
l at rt m∗ 
t when at does not affect our belief about m∗ 
t the 
first step can be omitted and we may reuse m∗ 
t− for m∗ 
t 
note that our framework is quite general since we can 
potentially model any kind of user actions and system responses in most 
cases as we may expect the system s response is some ranking of 
documents i e for most actions a r a consists of all the 
possible rankings of the unseen documents and the decision problem 
boils down to choosing the best ranking of unseen documents based 
on the most current user model when a is the action of submitting 
a keyword query such a response is exactly what a current retrieval 
system would do however we can easily imagine that a more 
intelligent web search engine would respond to a user s clicking of 
the next link to fetch more unseen results with a more 
optimized ranking of documents based on any viewed documents in 
the current page of results in fact according to our eager updating 
strategy we may even allow a system to respond to a user s clicking 
of browser s back button after viewing a document in the same 
way so that the user can maximally benefit from implicit feedback 
these are precisely what our ucair system does 
 user models 
a user model m ∈ m represents what we know about the user 
u so in principle it can contain any information about the user 
that we wish to model we now discuss two important components 
in a user model 
the first component is a component model of the user s 
information need presumably the most important factor affecting the 
optimality of the system s response is how well the response addresses 
the user s information need indeed at any time we may assume 
that the system has some belief about what the user is interested 
in which we model through a term vector x x x v 
where v {w w v } is the set of all terms i e vocabulary 
and xi is the weight of term wi such a term vector is commonly 
used in information retrieval to represent both queries and 
documents for example the vector-space model assumes that both 
the query and the documents are represented as term vectors and 
the score of a document with respect to a query is computed based 
on the similarity between the query vector and the document 
vector in a language modeling approach we may also regard 
the query unigram language model or the relevance model 
 as a term vector representation of the user s information need 
intuitively x would assign high weights to terms that characterize 
the topics which the user is interested in 
the second component we may include in our user model is the 
documents that the user has already viewed obviously even if a 
document is relevant if the user has already seen the document it 
would not be useful to present the same document again we thus 
introduce another variable s ⊂ d d is the whole set of documents 
in the collection to denote the subset of documents in the search 
results that the user has already seen viewed 
in general at time t we may represent a user model as mt 
 s x at rt− where s is the seen documents x is the system s 
understanding of the user s information need and at rt− 
represents the user s interaction history note that an even more 
general user model may also include other factors such as the user s 
reading level and occupation 
if we assume that the uncertainty of a user model mt is solely 
due to the uncertainty of x the computation of our current estimate 
of user model m∗ 
t will mainly involve computing our best estimate 
of x that is the system would choose a response according to 
r∗ 
t argminr∈r at l at r s x∗ 
 at rt− 
where x∗ 
 argmaxx p x u d at rt− this is the 
decision mechanism implemented in the ucair system to be described 
later in this system we avoided specifying the probabilistic model 
p x u d at rt− by computing x∗ 
directly with some existing 
feedback method 
 loss functions 
the exact definition of loss function l depends on the responses 
thus it is inevitably application-specific we now briefly discuss 
some possibilities when the response is to rank all the unseen 
documents and present the top k of them let r d dk be the 
top k documents s be the set of seen documents by the user and 
x∗ 
be the system s best guess of the user s information need we 
 
may simply define the loss associated with r as the negative sum 
of the probability that each of the di is relevant i e l a r m 
− k 
i p relevant di m clearly in order to minimize this 
loss function the optimal response r would contain the k 
documents with the highest probability of relevance which is intuitively 
reasonable 
one deficiency of this top-k loss function is that it is not 
sensitive to the internal order of the selected top k documents so 
switching the ranking order of a non-relevant document and a relevant one 
would not affect the loss which is unreasonable to model 
ranking we can introduce a factor of the user model - the probability 
of each of the k documents being viewed by the user p view di 
and define the following ranking loss function 
l a r m − 
k 
i 
p view di p relevant di m 
since in general if di is ranked above dj i e i j p view di 
p view dj this loss function would favor a decision to rank 
relevant documents above non-relevant ones as otherwise we could 
always switch di with dj to reduce the loss value thus the 
system should simply perform a regular retrieval and rank documents 
according to the probability of relevance 
depending on the user s retrieval preferences there can be many 
other possibilities for example if the user does not want to see 
redundant documents the loss function should include some 
redundancy measure on r based on the already seen documents s 
of course when the response is not to choose a ranked list of 
documents we would need a different loss function we discuss 
one such example that is relevant to the search agent that we 
implement when a user enters a query qt current action our search 
agent relies on some existing search engine to actually carry out 
search in such a case even though the search agent does not have 
control of the retrieval algorithm it can still attempt to optimize the 
search results through refining the query sent to the search engine 
and or reranking the results obtained from the search engine the 
loss functions for reranking are already discussed above we now 
take a look at the loss functions for query refinement 
let f be the retrieval function of the search engine that our agent 
uses so that f q would give us the search results using query q 
given that the current action of the user is entering a query qt i e 
at qt our response would be f q for some q since we have 
no choice of f our decision is to choose a good q formally 
r∗ 
t argminrt l a rt m 
 argminf q l a f q m 
 f argminql qt f q m 
which shows that our goal is to find q∗ 
 argminql qt f q m 
i e an optimal query that would give us the best f q a different 
choice of loss function l qt f q m would lead to a different 
query refinement strategy in ucair we heuristically compute q∗ 
by expanding qt with terms extracted from rt− whenever qt− and 
qt have high similarity note that rt− and qt− are contained in 
m as part of the user s interaction history 
 implicit user modeling 
implicit user modeling is captured in our framework through 
the computation of x∗ 
 argmaxx p x u d at rt− i e the 
system s current belief of what the user s information need is here 
again there may be many possibilities leading to different 
algorithms for implicit user modeling we now discuss a few of them 
first when two consecutive queries are related the previous 
query can be exploited to enrich the current query and provide more 
search context to help disambiguation for this purpose instead of 
performing query expansion as we did in the previous section we 
could also compute an updated x∗ 
based on the previous query and 
retrieval results the computed new user model can then be used to 
rank the documents with a standard information retrieval model 
second we can also infer a user s interest based on the 
summaries of the viewed documents when a user is presented with a 
list of summaries of top ranked documents if the user chooses to 
skip the first n documents and to view the n -th document we 
may infer that the user is not interested in the displayed summaries 
for the first n documents but is attracted by the displayed summary 
of the n -th document we can thus use these summaries as 
negative and positive examples to learn a more accurate user model 
x∗ 
 here many standard relevance feedback techniques can be 
exploited note that we should use the displayed summaries 
as opposed to the actual contents of those documents since it is 
possible that the displayed summary of the viewed document is 
relevant but the document content is actually not similarly a 
displayed summary may mislead a user to skip a relevant document 
inferring user models based on such displayed information rather 
than the actual content of a document is an important difference 
between ucair and some other similar systems 
in ucair both of these strategies for inferring an implicit user 
model are implemented 
 ucair a personalized 
search agent 
 design 
in this section we present a client-side web search agent called 
ucair in which we implement some of the methods discussed 
in the previous section for performing personalized search through 
implicit user modeling ucair is a web browser plug-in 
that 
acts as a proxy for web search engines currently it is only 
implemented for internet explorer and google but it is a matter of 
engineering to make it run on other web browsers and interact with 
other search engines 
the issue of privacy is a primary obstacle for deploying any real 
world applications involving serious user modeling such as 
personalized search for this reason ucair is strictly running as 
a client-side search agent as opposed to a server-side application 
this way the captured user information always resides on the 
computer that the user is using thus the user does not need to release 
any information to the outside client-side personalization also 
allows the system to easily observe a lot of user information that may 
not be easily available to a server furthermore performing 
personalized search on the client-side is more scalable than on the 
serverside since the overhead of computation and storage is distributed 
among clients 
as shown in figure the ucair toolbar has major 
components the implicit user modeling module captures a user s 
search context and history information including the submitted 
queries and any clicked search results and infers search session 
boundaries the query modification module selectively 
improves the query formulation according to the current user model 
 the result re-ranking module immediately re-ranks any unseen 
search results whenever the user model is updated 
in ucair we consider four basic user actions submitting a 
keyword query viewing a document clicking the back 
button clicking the next link on a result page for each 
of these four actions the system responds with respectively 
 
ucair is available at http sifaka cs uiuc edu ir ucair download html 
 
search 
engine 
 e g 
google 
search history log 
 e g past queries 
clicked results 
query 
modification 
result 
re-ranking 
user 
modeling 
result buffer 
ucair 
userquery 
results 
clickthrough  
figure ucair architecture 
generating a ranked list of results by sending a possibly expanded 
query to a search engine updating the information need model 
x reranking the unseen results on the current result page based 
on the current model x and reranking the unseen pages and 
generating the next page of results based on the current model x 
behind these responses there are three basic tasks decide 
whether the previous query is related to the current query and if so 
expand the current query with useful terms from the previous query 
or the results of the previous query update the information 
need model x based on a newly clicked document summary 
rerank a set of unseen documents based on the current model x 
below we describe our algorithms for each of them 
 session boundary detection and query 
expansion 
to effectively exploit previous queries and their corresponding 
clickthrough information ucair needs to judge whether two 
adjacent queries belong to the same search session i e detect 
session boundaries existing work on session boundary detection is 
mostly in the context of web log analysis e g and uses 
statistical information rather than textual features since our 
clientside agent does not have access to server query logs we make 
session boundary decisions based on textual similarity between two 
queries because related queries do not necessarily share the same 
words e g java island and travel indonesia it is insufficient 
to use only query text therefore we use the search results of the 
two queries to help decide whether they are topically related for 
example for the above queries java island and travel 
indonesia the words java bali island indonesia and travel 
may occur frequently in both queries search results yielding a high 
similarity score 
we only use the titles and summaries of the search results to 
calculate the similarity since they are available in the retrieved search 
result page and fetching the full text of every result page would 
significantly slow down the process to compensate for the terseness 
of titles and summaries we retrieve more results than a user would 
normally view for the purpose of detecting session boundaries 
 typically results 
the similarity between the previous query q and the current 
query q is computed as follows let {s s sn } and 
{s s sn} be the result sets for the two queries we use 
the pivoted normalization tf-idf weighting formula to 
compute a term weight vector si for each result si we define the 
average result savg to be the centroid of all the result vectors i e 
 s s sn n the cosine similarity between the two 
average results is calculated as 
s avg · savg s 
 
avg · s 
avg 
if the similarity value exceeds a predefined threshold the two queries 
will be considered to be in the same information session 
if the previous query and the current query are found to belong 
to the same search session ucair would attempt to expand the 
current query with terms from the previous query and its search 
results specifically for each term in the previous query or the 
corresponding search results if its frequency in the results of the 
current query is greater than a preset threshold e g results out 
of the term would be added to the current query to form an 
expanded query in this case ucair would send this expanded 
query rather than the original one to the search engine and return 
the results corresponding to the expanded query currently ucair 
only uses the immediate preceding query for query expansion in 
principle we could exploit all related past queries 
 information need model updating 
suppose at time t we have observed that the user has viewed 
k documents whose summaries are s sk we update our user 
model by computing a new information need vector with a standard 
feedback method in information retrieval i e rocchio 
according to the vector space retrieval model each clicked summary 
si can be represented by a term weight vector si with each term 
weighted by a tf-idf weighting formula rocchio computes 
the centroid vector of all the summaries and interpolates it with the 
original query vector to obtain an updated term vector that is 
x αq − α 
 
k 
k 
i 
si 
where q is the query vector k is the number of summaries the user 
clicks immediately following the current query and α is a parameter 
that controls the influence of the clicked summaries on the inferred 
information need model in our experiments α is set to note 
that we update the information need model whenever the user views 
a document 
 result reranking 
in general we want to rerank all the unseen results as soon as the 
user model is updated currently ucair implements reranking in 
two cases corresponding to the user clicking the back button 
and next link in the internet explorer in both cases the current 
 updated user model would be used to rerank the unseen results so 
that the user would see improved search results immediately 
to rerank any unseen document summaries ucair uses the 
standard vector space retrieval model and scores each summary 
based on the similarity of the result and the current user information 
need vector x since implicit feedback is not completely 
reliable we bring up only a small number e g of highest reranked 
results to be followed by any originally high ranked results 
 
google result user query java map ucair result user query java map 
previous query travel indonesia previous query hashtable 
expanded user query java map indonesia expanded user query java map class 
 java map projections of the world lonely planet - indonesia map map java platform se v 
www btinternet com se js mapproj htm www lonelyplanet com mapshells java sun com j se docs 
 java map projections of the world indonesia tourism central java - map java platform se v interface map 
www btinternet com se js oldmapproj htm www indonesia-tourism com java sun com j se docs api java 
 java map indonesia tourism west java - map an introduction to java map collection classes 
java sun com developer www indonesia-tourism com www oracle com technology 
 java technology concept map indostreets - java map an introduction to java map collection classes 
java sun com developer onlinetraining www indostreets com maps java www theserverside com news 
 science nasa home indonesia regions and islands maps bali java koders - mappings java 
science nasa gov realtime www maps anywhere com maps www koders com java 
 an introduction to java map collection classes indonesia city street map hibernate simplifies inheritance mapping 
www oracle com technology www maps anywhere com maps www ibm com developerworks java 
 lonely planet - java map maps of indonesia tmap map class hierarchy 
www lonelyplanet com mapshells www embassyworld com maps tmap pmel noaa gov 
 onjava com java api map maps of indonesia by peter loud class scope 
www onjava com pub a onjava api map users powernet co uk jalbum net api se datadosen util scope html 
 gta san andreas sam maps of indonesia by peter loud class printsafehashmap 
www gtasanandreas net sam users powernet co uk mkmarina indonesia jalbum net api se datadosen 
 indonesia tourism west java - map indonesiaphoto com java pro - union and vertical mapping of classes 
www indonesia-tourism com www indonesiaphoto com www fawcette com javapro 
table sample results of query expansion 
 evaluation of ucair 
we now present some results on evaluating the two major ucair 
functions selective query expansion and result reranking based on 
user clickthrough data 
 sample results 
the query expansion strategy implemented in ucair is 
intentionally conservative to avoid misinterpretation of implicit user 
models in practice whenever it chooses to expand the query the 
expansion usually makes sense in table we show how ucair can 
successfully distinguish two different search contexts for the query 
java map corresponding to two different previous queries i e 
travel indonesia vs hashtable due to implicit user modeling 
ucair intelligently figures out to add indonesia and class 
respectively to the user s query java map which would 
otherwise be ambiguous as shown in the original results from google 
on march ucair s results are much more accurate than 
google s results and reflect personalization in search 
the eager implicit feedback component is designed to 
immediately respond to a user s activity such as viewing a document in 
figure we show how ucair can successfully disambiguate an 
ambiguous query jaguar by exploiting a viewed document 
summary in this case the initial retrieval results using jaguar shown 
on the left side contain two results about the jaguar cars followed 
by two results about the jaguar software however after the user 
views the web page content of the second result about jaguar 
car and returns to the search result page by clicking back 
button ucair automatically nominates two new search results about 
jaguar cars shown on the right side while the original two results 
about jaguar software are pushed down on the list unseen from the 
picture 
 quantitative evaluation 
to further evaluate ucair quantitatively we conduct a user 
study on the effectiveness of the eager implicit feedback 
component it is a challenge to quantitatively evaluate the potential 
performance improvement of our proposed model and ucair over 
google in an unbiased way here we design a user study 
in which participants would do normal web search and judge a 
randomly and anonymously mixed set of results from google and 
ucair at the end of the search session participants do not know 
whether a result comes from google or ucair 
we recruited graduate students for this user study who have 
different backgrounds computer science biology and 
chem top 
 num number 
 title spammer arrest sue 
 desc description have any spammers 
been arrested or sued for sending unsolicited 
e-mail 
 narr narrative instances of arrests 
prosecutions convictions and punishments 
of spammers and lawsuits against them are 
relevant documents which describe laws to 
limit spam without giving details of lawsuits 
or criminal trials are not relevant 
 top 
figure an example of trec query topic expressed in a 
form which might be given to a human assistant or librarian 
istry we use query topics from trec 
 terabyte track 
and trec web track topic distillation task in the way to 
be described below 
an example topic from trec terabyte track appears in 
figure the title is a short phrase and may be used as a query 
to the retrieval system the description field provides a slightly 
longer statement of the topic requirement usually expressed as a 
single complete sentence or question finally the narrative supplies 
additional information necessary to fully specify the requirement 
expressed in the form of a short paragraph 
initially each participant would browse topics either from 
terabyte track or web track and pick or most interesting topics 
for each picked topic the participant would essentially do the 
normal web search using ucair to find many relevant web pages by 
using the title of the query topic as the initial keyword query 
during this process the participant may view the search results and 
possibly click on some interesting ones to view the web pages just 
as in a normal web search there is no requirement or restriction 
on how many queries the participant must submit or when the 
participant should stop the search for one topic when the participant 
plans to change the search topic he she will simply press a button 
 
text retrieval conference http trec nist gov 
 
figure screen shots for result reranking 
to evaluate the search results before actually switching to the next 
topic 
at the time of evaluation top ranked results from google and 
ucair some are overlapping are randomly mixed together so 
that the participant would not know whether a result comes from 
google or ucair the participant would then judge the relevance 
of these results we measure precision at top n n 
documents of google and ucair we also evaluate precisions at 
different recall levels 
altogether documents judged as relevant from google search 
results and documents judged as relevant from ucair by 
participants scatter plots of precision at top and top documents 
are shown in figure and figure respectively the scatter plot 
of precision at top documents is very similar to precision at top 
 documents each point of the scatter plots represents the 
precisions of google and ucair on one query topic 
table shows the average precision at top n documents among 
 topics from figure figure and table we see that the 
search results from ucair are consistently better than those from 
google by all the measures moreover the performance 
improvement is more dramatic for precision at top documents than that 
at precision at top documents one explanation for this is that 
the more interaction the user has with the system the more 
clickthrough data ucair can be expected to collect thus the retrieval 
system can build more precise implicit user models which lead to 
better retrieval accuracy 
ranking method prec  prec  prec  prec  
google 
ucair 
improvement 
table table of average precision at top n documents for 
query topics 
the plot in figure shows the precision-recall curves for ucair 
and google where it is clearly seen that the performance of ucair 
 
 
 
 
 
 
 
 
 
 
 
 
ucair prec  
googleprec  
scatterplot of precision at top documents 
figure precision at top documents of ucair and google 
is consistently and considerably better than that of google at all 
levels of recall 
 conclusions 
in this paper we studied how to exploit implicit user modeling to 
intelligently personalize information retrieval and improve search 
accuracy unlike most previous work we emphasize the use of 
immediate search context and implicit feedback information as well 
as eager updating of search results to maximally benefit a user we 
presented a decision-theoretic framework for optimizing 
interactive information retrieval based on eager user model updating in 
which the system responds to every action of the user by 
choosing a system action to optimize a utility function we further 
propose specific techniques to capture and exploit two types of implicit 
feedback information identifying related immediately 
preceding query and using the query and the corresponding search results 
to select appropriate terms to expand the current query and 
exploiting the viewed document summaries to immediately rerank 
any documents that have not yet been seen by the user using these 
techniques we develop a client-side web search agent ucair 
on top of a popular search engine google experiments on web 
search show that our search agent can improve search accuracy over 
 
 
 
 
 
 
 
 
 
 
 
 
 
ucair prec  
googleprec  
scatterplot of precision at top documents 
figure precision at top documents of ucair and google 
 
 
 
 
 
 
 
 
 
 
 
 
recall 
precision 
precision−recall curves 
google result 
ucair result 
figure precision at top result of ucair and google 
google since the implicit information we exploit already naturally 
exists through user interactions the user does not need to make any 
extra effort the developed search agent thus can improve 
existing web search performance without any additional effort from the 
user 
 acknowledgement 
we thank the six participants of our evaluation experiments this 
work was supported in part by the national science foundation 
grants iis- and iis- 
 references 
 s m beitzel e c jensen a chowdhury d grossman 
and o frieder hourly analysis of a very large topically 
categorized web query log in proceedings of sigir 
pages - 
 c clarke n craswell and i soboroff overview of the 
trec terabyte track in proceedings of trec 
 
 m claypool p le m waseda and d brown implicit 
interest indicators in proceedings of intelligent user 
interfaces pages - 
 n craswell d hawking r wilkinson and m wu 
overview of the trec web track in proceedings of 
trec 
 w b croft s cronen-townsend and v larvrenko 
relevance feedback and personalization a language 
modeling perspective in proeedings of second delos 
workshop personalisation and recommender systems in 
digital libraries 
 google personalized http labs google com personalized 
 d hawking n craswell p b thistlewaite and d harman 
results and challenges in web search evaluation computer 
networks - - 
 x huang f peng a an and d schuurmans dynamic 
web log session identification with statistical language 
models journal of the american society for information 
science and technology - 
 g jeh and j widom scaling personalized web search in 
proceedings of www pages - 
 t joachims optimizing search engines using clickthrough 
data in proceedings of sigkdd pages - 
 
 d kelly and j teevan implicit feedback for inferring user 
preference a bibliography sigir forum - 
 
 j lafferty and c zhai document language models query 
models and risk minimization for information retrieval in 
proceedings of sigir pages - 
 t lau and e horvitz patterns of search analyzing and 
modeling web query refinement in proceedings of the 
seventh international conference on user modeling um 
pages - 
 v lavrenko and b croft relevance-based language 
models in proceedings of sigir pages - 
 m mitra a singhal and c buckley improving automatic 
query expansion in proceedings of sigir pages 
 - 
 my yahoo http mysearch yahoo com 
 g nunberg as google goes so goes the nation new york 
times may 
 s e robertson the probability ranking principle in ı˚ 
journal of documentation - 
 j j rocchio relevance feedback in information retrieval in 
the smart retrieval system experiments in automatic 
document processing pages - prentice-hall inc 
 
 g salton and c buckley improving retrieval performance 
by retrieval feedback journal of the american society for 
information science - 
 g salton and m j mcgill introduction to modern 
information retrieval mcgraw-hill 
 x shen b tan and c zhai context-sensitive information 
retrieval using implicit feedback in proceedings of sigir 
 pages - 
 x shen and c zhai exploiting query history for document 
ranking in interactive information retrieval poster in 
proceedings of sigir pages - 
 a singhal modern information retrieval a brief overview 
bulletin of the ieee computer society technical committee 
on data engineering - 
 k sugiyama k hatano and m yoshikawa adaptive web 
search based on user profile constructed without any effort 
from users in proceedings of www pages - 
 
 e volokh personalization and privacy communications of 
the acm - 
 r w white j m jose c j van rijsbergen and 
i ruthven a simulated study of implicit feedback models 
in proceedings of ecir pages - 
 j xu and w b croft query expansion using local and 
global document analysis in proceedings of sigir 
pages - 
 c zhai and j lafferty model-based feedback in kl 
divergence retrieval model in proceedings of the cikm 
 pages - 
 
ranking web objects from multiple communities 
le chen 
∗ 
le chen idiap ch 
lei zhang 
leizhang  
microsoft com 
feng jing 
fengjing  
microsoft com 
ke-feng deng 
kefengdeng hotmail com 
wei-ying ma 
wyma microsoft com 
microsoft research asia 
 f sigma center no zhichun road 
haidian district beijing p r china 
abstract 
vertical search is a promising direction as it leverages 
domainspecific knowledge and can provide more precise information 
for users in this paper we study the web object-ranking 
problem one of the key issues in building a vertical search 
engine more specifically we focus on this problem in cases 
when objects lack relationships between different web 
communities and take high-quality photo search as the test bed 
for this investigation we proposed two score fusion methods 
that can automatically integrate as many web communities 
 web forums with rating information as possible the 
proposed fusion methods leverage the hidden links discovered 
by a duplicate photo detection algorithm and aims at 
minimizing score differences of duplicate photos in different 
forums both intermediate results and user studies show the 
proposed fusion methods are practical and efficient solutions 
to web object ranking in cases we have described though 
the experiments were conducted on high-quality photo 
ranking the proposed algorithms are also applicable to other 
ranking problems such as movie ranking and music 
ranking 
categories and subject descriptors 
h information storage and retrieval 
information search and retrieval g discrete 
mathematics graph theory h information storage and 
retrieval online information services - web-based services 
general terms 
algorithms experimentation 
 introduction 
despite numerous refinements and optimizations general 
purpose search engines still fail to find relevant results for 
many queries as a new trend vertical search has shown 
promise because it can leverage domain-specific knowledge 
and is more effective in connecting users with the 
information they want there are many vertical search engines 
including some for paper search e g libra citeseer 
 and google scholar product search e g froogle 
 movie search image search video search 
local search as well as news search we believe the 
vertical search engine trend will continue to grow 
essentially building vertical search engines includes data 
crawling information extraction object identification and 
integration and object-level web information retrieval or 
web object ranking among which ranking is one of the 
most important factors this is because it deals with the 
core problem of how to combine and rank objects coming 
from multiple communities 
although object-level ranking has been well studied in 
building vertical search engines there are still some kinds 
of vertical domains in which objects cannot be effectively 
ranked for example algorithms that evolved from 
pagerank poprank and linkfusion were proposed 
to rank objects coming from multiple communities but can 
only work on well-defined graphs of heterogeneous data 
well-defined means that like objects e g authors in 
paper search can be identified in multiple communities e g 
conferences this allows heterogeneous objects to be well 
linked to form a graph through leveraging all the 
relationships e g cited-by authored-by and published-by among 
the multiple communities 
however this assumption does not always stand for some 
domains high-quality photo search movie search and news 
search are exceptions for example a photograph forum 
 
website usually includes three kinds of objects photos 
authors and reviewers yet different photo forums seem to 
lack any relationships as there are no cited-by relationships 
this makes it difficult to judge whether two authors cited 
are the same author or two photos are indeed identical 
photos consequently although each photo has a rating score 
in a forum it is non-trivial to rank photos coming from 
different photo forums similar problems also exist in movie 
search and news search although two movie titles can be 
identified as the same one by title and director in different 
movie discussion groups it is non-trivial to combine 
rating scores from different discussion groups and rank movies 
effectively we call such non-trivial object relationship in 
which identification is difficult incomplete relationships 
other related work includes rank aggregation for the web 
 and learning algorithm for rank such as rankboost 
 ranksvm and ranknet we will contrast 
differences of these methods with the proposed methods 
after we have described the problem and our methods 
we will specifically focus on web object-ranking 
problem in cases that lack object relationships or have with 
incomplete object relationships and take high-quality photo 
search as the test bed for this investigation in the following 
we will introduce rationale for building high-quality photo 
search 
 high-quality photo search 
in the past ten years the internet has grown to become 
an incredible resource allowing users to easily access a huge 
number of images however compared to the more than 
billion images indexed by commercial search engines actual 
queries submitted to image search engines are relatively 
minor and occupy only - percent of total image and text 
queries submitted to commercial search engines this 
is partially because user requirements for image search are 
far less than those for general text search on the other 
hand current commercial search engines still cannot well 
meet various user requirements because there is no 
effective and practical solution to understand image content 
to better understand user needs in image search we 
conducted a query log analysis based on a commercial search 
engine the result shows that more than of image 
search queries are related to nature and places and daily 
life categories users apparently are interested in enjoying 
high-quality photos or searching for beautiful images of 
locations or other kinds however such user needs are not 
well supported by current image search engines because of 
the difficulty of the quality assessment problem 
ideally the most critical part of a search engine - the 
ranking function - can be simplified as consisting of two 
key factors relevance and quality for the relevance 
factor search in current commercial image search engines 
provide most returned images that are quite relevant to queries 
except for some ambiguity however as to quality factor 
there is still no way to give an optimal rank to an image 
though content-based image quality assessment has been 
investigated over many years it is still far from 
ready to provide a realistic quality measure in the immediate 
future 
seemingly it really looks pessimistic to build an image 
search engine that can fulfill the potentially large 
requirement of enjoying high-quality photos various proliferating 
web communities however notices us that people today 
have created and shared a lot of high-quality photos on the 
web on virtually any topics which provide a rich source for 
building a better image search engine 
in general photos from various photo forums are of higher 
quality than personal photos and are also much more 
appealing to public users than personal photos in addition 
photos uploaded to photo forums generally require rich 
metadata about title camera setting category and description to 
be provide by photographers these metadata are actually 
the most precise descriptions for photos and undoubtedly 
can be indexed to help search engines find relevant results 
more important there are volunteer users in web 
communities actively providing valuable ratings for these photos 
the rating information is generally of great value in solving 
the photo quality ranking problem 
motivated by such observations we have been attempting 
to build a vertical photo search engine by extracting rich 
metadata and integrating information form various photo 
web forums in this paper we specifically focus on how to 
rank photos from multiple web forums 
intuitively the rating scores from different photo forums 
can be empirically normalized based on the number of 
photos and the number of users in each forum however such 
a straightforward approach usually requires large manual 
effort in both tedious parameter tuning and subjective 
results evaluation which makes it impractical when there are 
tens or hundreds of photo forums to combine to address 
this problem we seek to build relationships links between 
different photo forums that is we first adopt an efficient 
algorithm to find duplicate photos which can be considered 
as hidden links connecting multiple forums we then 
formulate the ranking challenge as an optimization problem 
which eventually results in an optimal ranking function 
 main contributions and organization 
the main contributions of this paper are 
 we have proposed and built a vertical image search 
engine by leveraging rich metadata from various photo 
forum web sites to meet user requirements of searching 
for and enjoying high-quality photos which is 
impossible in traditional image search engines 
 we have proposed two kinds of web object-ranking 
algorithms for photos with incomplete relationships 
which can automatically and efficiently integrate as 
many as possible web communities with rating 
information and achieves an equal qualitative result 
compared with the manually tuned fusion scheme 
the rest of this paper is organized as follows in section 
 we present in detail the proposed solutions to the 
ranking problem including how to find hidden links between 
different forums normalize rating scores obtain the 
optimal ranking function and contrast our methods with some 
other related research in section we describe the 
experimental setting and experiments and user studies conducted 
to evaluate our algorithm our conclusion and a discussion 
of future work is in section 
it is worth noting that although we treat vertical photo 
search as the test bed in this paper the proposed ranking 
algorithm can also be applied to rank other content that 
includes video clips poems short stories drawings 
sculptures music and so on 
 
 algorithm 
 overview 
the difficulty of integrating multiple web forums is in 
their different rating systems where there are generally two 
kinds of freedom the first kind of freedom is the rating 
interval or rating scale including the minimal and maximal 
ratings for each web object for example some forums use 
a -point rating scale whereas other forums use -point or 
 -point rating scales it seems easy to fix this freedom but 
detailed analysis of the data and experiments show that it 
is a non-trivial problem 
the second kind of freedom is the varying rating criteria 
found in different web forums that is the same score does 
not mean the same quality in different forums intuitively if 
we can detect same photographers or same photographs we 
can build relationships between any two photo forums and 
therefore can standardize the rating criterion by score 
normalization and transformation fortunately we find that 
quite a number of duplicate photographs exist in various 
web photo forums this fact is reasonable when 
considering that photographers sometimes submit a photo to more 
than one forum to obtain critiques or in hopes of widespread 
publicity in this work we adopt an efficient duplicate photo 
detection algorithm to find these photos 
the proposed methods below are based on the following 
considerations faced with the need to overcome a ranking 
problem a standardized rating criterion rather than a 
reasonable rating criterion is needed therefore we can take 
a large scale forum as the reference forum and align other 
forums by taking into account duplicate web objects 
 duplicate photos in this work ideally the scores of duplicate 
photos should be equal even though they are in different 
forums yet we can deem that scores in different 
forumsexcept for the reference forum - can vary in a parametric 
space this can be determined by minimizing the objective 
function defined by the sum of squares of the score 
differences by formulating the ranking problem as an 
optimization problem that attempts to make the scores of duplicate 
photos in non-reference forums as close as possible to those 
in the reference forum we can effectively solve the ranking 
problem 
for convenience the following notations are employed 
ski and ¯ski denote the total score and mean score of ith web 
object photo in the kth web site respectively the total 
score refers to the sum of the various rating scores e g 
novelty rating and aesthetic rating and the mean score refers 
to the mean of the various rating scores suppose there are 
a total of k web sites we further use 
{skl 
i i ikl k l k k l} 
to denote the set of scores for web objects photos in kth 
web forums that are duplicate with the lth web forums 
where ikl is the total number of duplicate web objects 
between these two web sites in general score fusion can be 
seen as the procedure of finding k transforms 
ψk ski eski k k 
such that eski can be used to rank web objects from different 
web sites the objective function described in the above 
figure web community integration each web 
community forms a subgraph and all communities 
are linked together by some hidden links dashed 
lines 
paragraph can then be formulated as 
min 
{ψk k k} 
kx 
k 
ik x 
i 
¯wk 
i 
 
s k 
i − ψk sk 
i 
 
 
where we use k as the reference forum and thus ψ s i 
s i ¯wk 
i ≥ is the weight coefficient that can be set 
heuristically according to the numbers of voters reviewers or 
commenters in both the reference forum and the non-reference 
forum the more reviewers the more popular the photo is 
and the larger the corresponding weight ¯wk 
i should be in 
this work we do not inspect the problem of how to choose ¯wk 
i 
and simply set them to one but we believe the proper use 
of ¯wk 
i which leverages more information can significantly 
improve the results 
figure illustrates the aforementioned idea the web 
community is the reference community the dashed lines 
are links indicating that the two linked web objects are 
actually the same the proposed algorithm will try to find the 
best ψk k k which has certain parametric forms 
according to certain models so as to minimize the cost 
function defined in eq the summation is taken on all the 
red dashed lines 
we will first discuss the score normalization methods in 
section which serves as the basis for the following work 
before we describe the proposed ranking algorithms we first 
introduce a manually tuned method in section which is 
laborious and even impractical when the number of 
communities become large in section we will briefly explain 
how to precisely find duplicate photos between web forums 
then we will describe the two proposed methods linear 
fusion and non-linear fusion and a performance measure for 
result evaluation in section finally in section we 
will discuss the relationship of the proposed methods with 
some other related work 
 score normalization 
since different web photo forums on the web usually 
have different rating criteria it is necessary to normalize 
them before applying different kinds of fusion methods in 
addition as there are many kinds of ratings such as 
ratings for novelty ratings for aesthetics etc it is reasonable 
to choose a common one - total score or average 
scorethat can always be extracted in any web forum or 
calculated by corresponding ratings this allows the 
normaliza 
tion method on the total score or average score to be viewed 
as an impartial rating method between different web 
forums 
it is straightforward to normalize average scores by 
linearly transforming them to a fixed interval we call this 
kind of score as scaled mean score the difficulty however 
of using this normalization method is that if there are only 
a few users rating an object say a photo in a photo forum 
the average score for the object is likely to be spammed or 
skewed 
total score can avoid such drawbacks that contain more 
information such as a web object s quality and popularity 
the problem is thus how to normalize total scores in 
different web forums the simplest way may be normalization 
by the maximal and minimal scores the drawback of this 
normalization method is it is non robust or in other words 
it is sensitive to outliers 
to make the normalization insensitive to unusual data 
we propose the mode- percentile normalization method 
here the mode score represents the total score that has been 
assigned to more photos than any other total score and the 
high percentile score e g represents the total score for 
which the high percentile of images have a lower total score 
this normalization method utilizes the mode and 
percentile as two reference points to align two rating systems 
which makes the distributions of total scores in different 
forums more consistent the underlying assumption for 
example in different photo forums is that even the qualities of 
top photos in different forums may vary greatly and be less 
dependent on the forum quality the distribution of photos 
of middle-level quality from mode to percentile should 
be almost of the same quality up to the freedom which 
reflects the rating criterion strictness of web forums 
photos of this middle-level in a web forum usually occupy more 
than of total photos in that forum 
we will give more detailed analysis of the scores in section 
 
 manual fusion 
the web movie forum imdb proposed to use a 
bayesian-ranking function to normalize rating scores within 
one community motivated by this ranking function we 
propose this manual fusion method for the kth web site we 
use the following formula 
eski αk · 
„ 
nk · ¯ski 
nk n∗ 
k 
 
n∗ 
k · s∗ 
k 
nk n∗ 
k 
 
 
to rank photos where nk is the number of votes and n∗ 
k 
s∗ 
k and αk are three parameters this ranking function first 
takes a balance between the original mean score ¯ski and a 
reference score s∗ 
k to get a weighted mean score which may 
be more reliable than ¯ski then the weighted mean score is 
scaled by αk to get the final score fski 
for n web communities there are then about n 
parameters in { αk n∗ 
k s∗ 
k k n} to tune though this 
method can achieves pretty good results after careful and 
thorough manual tuning on these parameters when n 
becomes increasingly large say there are tens or hundreds of 
web communities crawled and indexed this method will 
become more and more laborious and will eventually become 
impractical it is therefore desirable to find an effective 
fusion method whose parameters can be automatically 
determined 
 duplicate photo detection 
we use dedup an efficient and effective duplicate 
image detection algorithm to find duplicate photos between 
any two photo forums this algorithm uses hash function 
to map a high dimensional feature to a bits hash code 
 see below for how to construct the hash code its 
computational complexity to find all the duplicate images among 
n images is about o n log n the low-level visual feature 
for each photo is extracted on k × k regular grids based 
on all features extracted from the image database a pca 
model is built the visual features are then transformed to 
a relatively low-dimensional and zero mean pca space or 
 dimensions in our system then the hash code for each 
photo is built as follows each dimension is transformed to 
one if the value in this dimension is greater than and 
otherwise photos in the same bucket are deemed potential 
duplicates and are further filtered by a threshold in terms 
of euclidean similarity in the visual feature space 
figure illustrates the hashing procedure where visual 
features - mean gray values - are extracted on both × 
and × grids the -dimensional features are transformed 
to a -dimensional vector and the hash code is generated 
according to the signs 
figure hashing procedure for duplicate photo 
dectection 
 score fusion 
in this section we will present two solutions on score 
fusion based on different parametric form assumptions of ψk 
in eq 
 linear fusion by duplicate photos 
intuitively the most straightforward way to factor out the 
uncertainties caused by the different criterion is to scale 
rel 
ative to a given center the total scores of each unreferenced 
web photo forum with respect to the reference forum more 
strictly we assume ψk has the following form 
ψk ski αkski tk k k 
ψ s i s i 
which means that the scores of k th forum should be 
scaled by αk relative to the center tk 
 −αk 
as shown in figure 
 
then if we substitute above ψk to eq we get the 
following objective function 
min 
{αk tk k k} 
kx 
k 
ik x 
i 
¯wk 
i 
h 
s k 
i − αksk 
i − tk 
i 
 
by solving the following set of functions 
 
∂f 
∂αk 
 
∂f 
∂tk 
 
 k k 
where f is the objective function defined in eq we get 
the closed form solution as 
„ 
αk 
tk 
 
 a− 
k lk 
where 
ak 
„ p 
i ¯wi sk 
i p 
i ¯wisk 
ip 
i ¯wisk 
i 
p 
i ¯wi 
 
 
lk 
„ p 
i ¯wis k 
i sk 
ip 
i ¯wis k 
i 
 
 
and k k 
this is a linear fusion method it enjoys simplicity and 
excellent performance in the following experiments 
figure linear fusion method 
 nonlinear fusion by duplicate photos 
sometimes we want a method which can adjust scores on 
intervals with two endpoints unchanged as illustrated in 
figure the method can tune scores between c c while 
leaving scores c and c unchanged this kind of fusion 
method is then much finer than the linear ones and 
contains many more parameters to tune and expect to further 
improve the results 
here we propose a nonlinear fusion solution to satisfy 
such constraints first we introduce a transform 
ηc c α x 
 
x−c 
c −c 
α 
 c − c c if x ∈ c c 
x otherwise 
where α this transform satisfies that for x ∈ c c 
ηc c α x ∈ c c with ηc c α c c and ηc c α c 
c then we can utilize this nonlinear transform to adjust 
the scores in certain interval say m t 
ψk ski ηm t α ski 
figure nonlinear fusion method we intent to 
finely adjust the shape of the curves in each segment 
even there is no closed-form solution for the following 
optimization problem 
min 
{αk k∈ k } 
kx 
k 
ik x 
i 
¯wk 
i 
h 
s k 
i − ηm t α ski 
i 
it is not hard to get the numeric one under the same 
assumptions made in section we can use this method to 
adjust scores of the middle-level from the mode point to 
the percentile 
this more complicated non-linear fusion method is 
expected to achieve better results than the linear one 
however difficulties in evaluating the rank results block us from 
tuning these parameters extensively the current 
experiments in section do not reveal any advantages over the 
simple linear model 
 performance measure of the fusion results 
since our objective function is to make the scores of the 
same web objects e g duplicate photos between a 
nonreference forum and the reference forum as close as possible 
it is natural to investigate how close they become to each 
other and how the scores of the same web objects change 
between the two non-reference forums before and after score 
fusion 
taken figure as an example the proposed algorithms 
minimize the score differences of the same web objects in 
two web forums the reference forum the web community 
 and a non-reference forum which corresponds to 
minimizing the objective function on the red dashed hidden 
links after the optimization we must ask what happens to 
the score differences of the same web objects in two 
nonreference forums or in other words whether the scores 
of two objects linked by the green dashed hidden links 
become more consistent 
we therefore define the following performance 
measureδ measure - to quantify the changes for scores of the same 
web objects in different web forums as 
δkl sim slk 
 skl 
 − sim slk 
∗ skl 
∗ 
 
where skl 
 skl 
 skl 
ikl 
 t 
 skl 
∗ eskl 
 eskl 
ikl 
 t 
and 
sim a b 
a · b 
 a b 
 
δkl means after score fusion scores on the same web 
objects between kth and lth web forum become more 
consistent which is what we expect on the contrary if δkl 
those scores become more inconsistent 
although we cannot rely on this measure to evaluate our 
final fusion results as ranking photos by their popularity and 
qualities is such a subjective process that every person can 
have its own results it can help us understand the 
intermediate ranking results and provide insights into the final 
performances of different ranking methods 
 contrasts with other related work 
we have already mentioned the differences of the proposed 
methods with the traditional methods such as pagerank 
 poprank and linkfusion algorithms in 
section here we discuss some other related works 
the current problem can also be viewed as a rank 
aggregation one as we deal with the problem of how to 
combine several rank lists however there are 
fundamental differences between them first of all unlike the web 
pages which can be easily and accurately detected as the 
same pages detecting the same photos in different web 
forums is a non-trivial work and can only be implemented by 
some delicate algorithms while with certain precision and 
recall second the numbers of the duplicate photos from 
different web forums are small relative to the whole photo 
sets see table in another words the top k rank lists 
of different web forums are almost disjointed for a given 
query under this condition both the algorithms proposed 
in and their measurements - kendall tau distance or 
spearman footrule distance - will degenerate to some 
trivial cases 
another category of rank fusion aggregation methods is 
based on machine learning algorithms such as ranksvm 
 rankboost and ranknet all of these 
methods entail some labelled datasets to train a model in 
current settings it is difficult or even impossible to get these 
datasets labelled as to their level of professionalism or 
popularity since the photos are too vague and subjective to rank 
instead the problem here is how to combine several ordered 
sub lists to form a total order list 
 experiments 
in this section we carry out our research on high-quality 
photo search we first briefly introduce the newly proposed 
vertical image search engine - enjoyphoto in section 
then we focus on how to rank photos from different web 
forums in order to do so we first normalize the scores 
 ratings for photos from different multiple web forums in 
section then we try to find duplicate photos in section 
 some intermediate results are discussed using δ measure 
in section finally a set of user studies is carried out 
carefully to justify our proposed method in section 
 enjoyphoto high-quality photo search 
engine 
in order to meet user requirement of enjoying high-quality 
photos we propose and build a high-quality photo search 
engine - enjoyphoto which accounts for the following three 
key issues how to crawl and index photos how to 
determine the qualities of each photo and how to 
display the search results in order to make the search process 
enjoyable for a given text based query this system ranks 
the photos based on certain combination of relevance of the 
photo to this query issue and the quality of the photo 
 issue and finally displays them in an enjoyable manner 
 issue 
as for issue we devise the interface of the system 
deliberately in order to smooth the users process of enjoying 
high-quality photos techniques such as fisheye and slides 
show are utilized in current system figure shows the 
interface we will not talk more about this issue as it is not 
an emphasis of this paper 
figure enjoyphoto an enjoyable high-quality 
photo search engine where records are 
returned for the query fall in about seconds 
as for issue we extracted from a commercial search 
engine a subset of photos coming from various photo forums 
all over the world and explicitly parsed the web pages 
containing these photos the number of photos in the data 
collection is about million after the parsing each photo 
was associated with its title category description camera 
setting exif data 
 when available for digital images 
location when available in some photo forums and many 
kinds of ratings all these metadata are generally precise 
descriptions or annotations for the image content which are 
then indexed by general text-based search technologies 
 in current system the ranking function was 
specifically tuned to emphasize title categorization and rating 
information 
issue is essentially dealt with in the following sections 
which derive the quality of photos by analyzing ratings 
provided by various web photo forums here we chose six photo 
forums to study the ranking problem and denote them as 
web-a web-b web-c web-d web-e and web-f 
 photo score normalization 
detailed analysis of different score normalization 
methods are analyzed in this section in this analysis the zero 
 
digital cameras save jpeg jpg files with exif 
 exchangeable image file data camera settings and scene 
information are recorded by the camera into the image file 
www digicamhelp com what-is-exif 
 
 
 
 
 
 
 
normalized score 
totalnumber 
 a web-a 
 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 b web-b 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 c web-c 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 d web-d 
 
 
 
 
 
 
 
 
 
normalized score 
totalnumber 
 e web-e 
 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 f web-f 
figure distributions of mean scores normalized 
to 
scores that usually occupy about than of the total 
number of photos for some web forums are not currently taken 
into account how to utilize these photos is left for future 
explorations 
in figure we list the distributions of the mean score 
which is transformed to a fixed interval the 
distributions of the average scores of these web forums look quite 
different distributions in figure a b and e looks 
like gaussian distributions while those in figure d and 
 f are dominated by the top score the reason of these 
eccentric distributions for web-d and web-f lies in their 
coarse rating systems in fact web-d and web-f use or 
 point rating scales whereas other web forums use or 
point rating scales therefore it will be problematic if we 
directly use these averaged scores furthermore the average 
score is very likely to be spammed if there are only a few 
users rating a photo 
figure shows the total score normalization method by 
maximal and minimal scores which is one of our base line 
system all the total scores of a given web forum are 
normalized to according to the maximal score and 
minimal score of corresponding web forum we notice that total 
score distribution of web-a in figure a has two larger 
tails than all the others to show the shape of the 
distributions more clearly we only show the distributions on 
in figure b c d e and f 
figure shows the mode- percentile normalization 
method where the modes of the six distributions are 
normalized to and the percentile to we can see that 
this normalization method makes the distributions of total 
scores in different forums more consistent the two proposed 
algorithms are all based on these normalization methods 
 duplicate photo detection 
targeting at computational efficiency the dedup 
algorithm may lose some recall rate but can achieve a high 
precision rate we also focus on finding precise hidden links 
rather than all hidden links figure shows some duplicate 
detection examples the results are shown in table and 
verify that large numbers of duplicate photos exist in any 
two web forums even with the strict condition for dedup 
where we chose first bits as the hash code since there 
are only a few parameters to estimate in the proposed fusion 
methods the numbers of duplicate photos shown table are 
 
 
 
 
 
 
 
 
normalized score 
totalnumber 
 a web-a 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 b web-b 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 c web-c 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 d web-d 
 
 
 
 
 
 
 
normalized score 
totalnumber 
 e web-e 
 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 f web-f 
figure maxmin normalization 
 
 
 
 
 
 
 
 
 
normalized score 
totalnumber 
 a web-a 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 b web-b 
 
 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 c web-c 
 
 
 
 
 
 
 
x 
 
normalized score 
totalnumber 
 d web-d 
 
 
 
 
 
 
 
 
normalized score 
totalnumber 
 e web-e 
 
 
 
 
 
 
 
normalized score 
totalnumber 
 f web-f 
figure mode- percentile normalization 
sufficient to determine these parameters the last table 
column lists the total number of photos in the corresponding 
web forums 
 δ measure 
the parameters of the proposed linear and nonlinear 
algorithms are calculated using the duplicate data shown in 
table where the web-c is chosen as the reference web 
forum since it shares the most duplicate photos with other 
forums 
table and show the δ measure on the linear model and 
nonlinear model as δkl is symmetric and δkk we only 
show the upper triangular part the nan values in both 
tables lie in that no duplicate photos have been detected by 
the dedup algorithm as reported in table 
the linear model guarantees that the δ measures related 
table number of duplicate photos between each 
pair of web forums 
a b c d e f scale 
a k 
b k 
c k 
d k 
e k 
f k 
 
figure some results of duplicate photo detection 
table δ measure on the linear model 
web-b web-c web-d web-e web-f 
web-a nan 
web-b - 
web-c - - 
web-d - - - 
web-e - - - - 
to the reference community should be no less than 
theoretically it is indeed the case see the underlined numbers 
in table but this model can not guarantee that the δ 
measures on the non-reference communities can also be no 
less than as the normalization steps are based on 
duplicate photos between the reference community and a 
nonreference community results shows that all the numbers in 
the δ measure are greater than see all the non-underlined 
numbers in table which indicates that it is probable that 
this model will give optimal results 
on the contrary the nonlinear model does not guarantee 
that δ measures related to the reference community should 
be no less than as not all duplicate photos between the 
two web forums can be used when optimizing this model 
in fact the duplicate photos that lie in different intervals 
will not be used in this model it is these specific duplicate 
photos that make the δ measure negative as a result there 
are both negative and positive items in table but overall 
the number of positive ones are greater than negative ones 
 that indicates the model may be better than the 
normalization only method see next subsection which has an 
all-zero δ measure and worse than the linear model 
 user study 
because it is hard to find an objective criterion to evaluate 
table δ measure on the nonlinear model 
web-b web-c web-d web-e web-f 
web-a - - nan 
web-b - - - - 
web-c - - 
web-d - - - 
web-e - - - - 
which ranking function is better we chose to employ user 
studies for subjective evaluations ten subjects were invited 
to participate in the user study they were recruited from 
nearby universities as search engines of both text search 
and image search are familiar to university students there 
was no prerequisite criterion for choosing students 
we conducted user studies using internet explorer on 
windows xp with -inch lcd monitors set at pixels 
by pixels in -bit color data was recorded with 
server logs and paper-based surveys after each task 
figure user study interface 
we specifically device an interface for user study as shown 
in figure for each pair of fusion methods participants 
were encouraged to try any query they wished for those 
without specific ideas two combo boxes category list and 
query list were listed on the bottom panel where the top 
 image search queries from a commercial search engine 
were provided after a participant submitted a query the 
system randomly selected the left or right frame to display 
each of the two ranking results the participant were then 
required to judge which ranking result was better of the two 
ranking results or whether the two ranking results were of 
equal quality and submit the judgment by choosing the 
corresponding radio button and clicking the submit button 
for example in figure query sunset is submitted to 
the system then photos were returned and ranked 
by the minmax fusion method in the left frame and linear 
fusion method in the right frame a participant then 
compares the two ranking results without knowing the ranking 
methods and submits his her feedback by choosing answers 
in the your option 
table results of user study 
norm only manually linear 
linear 
 nonlinear 
table shows the experimental results where linear 
denotes the linear fusion method nonlinear denotes the 
non linear fusion method norm only means maxmin 
normalization method manually means the manually tuned 
method the three numbers in each item say 
mean that judgments prefer the linear fusion results 
 
judgments prefer the normalization only method and 
judgments consider these two methods as equivalent 
we conduct the anova analysis and obtain the 
following conclusions 
 both the linear and nonlinear methods are significantly 
better than the norm only method with respective 
p-values and this 
result is consistent with the δ-measure evaluation 
result the norm only method assumes that the top 
 photos in different forums are of the same 
quality however this assumption does not stand in 
general for example a top photo in a top tier photo 
forum is generally of higher quality than a top 
photo in a second-tier photo forum this is similar 
to that those top students in a top-tier 
university and those in a second-tier university are generally 
of different quality both linear and nonlinear fusion 
methods acknowledge the existence of such differences 
and aim at quantizing the differences therefore they 
perform better than the norm only method 
 the linear fusion method is significantly better than 
the nonlinear one with p-value × − 
 this 
result is rather surprising as this more complicated 
ranking method is expected to tune the ranking more 
finely than the linear one the main reason for this 
result may be that it is difficult to find the best 
intervals where the nonlinear tuning should be carried out 
and yet simply the middle part of the mode- 
percentile normalization method was chosen the 
timeconsuming and subjective evaluation methods - user 
studies - blocked us extensively tuning these 
parameters 
 the proposed linear and nonlinear methods perform 
almost the same with or slightly better than the 
manually tuned method given that the linear nonlinear 
fusion methods are fully automatic approaches they 
are considered practical and efficient solutions when 
more communities e g dozens of communities need 
to be integrated 
 conclusions and future work 
in this paper we studied the web object-ranking 
problem in the cases of lacking object relationships where 
traditional ranking algorithms are no longer valid and took 
high-quality photo search as the test bed for this 
investigation we have built a vertical high-quality photo search 
engine and proposed score fusion methods which can 
automatically integrate as many data sources web forums as 
possible the proposed fusion methods leverage the hidden 
links discovered by duplicate photo detection algorithm and 
minimize score differences of duplicate photos in different 
forums both the intermediate results and the user 
studies show that the proposed fusion methods are a practical 
and efficient solution to web object ranking in the 
aforesaid relationships though the experiments were conducted 
on high-quality photo ranking the proposed algorithms are 
also applicable to other kinds of web objects including video 
clips poems short stories music drawings sculptures and 
so on 
current system is far from being perfect in order to make 
this system more effective more delicate analysis for the 
vertical domain e g web photo forums are needed the 
following points for example may improve the searching 
results and will be our future work more subtle 
analysis and then utilization of different kinds of ratings e g 
novelty ratings aesthetic ratings differentiating various 
communities who may have different interests and 
preferences or even distinct culture understandings 
incorporating more useful information including photographers and 
reviewers information to model the photos in a 
heterogeneous data space instead of the current homogeneous one 
we will further utilize collaborative filtering to recommend 
relevant high-quality photos to browsers 
one open problem is whether we can find an objective and 
efficient criterion for evaluating the ranking results instead 
of employing subjective and inefficient user studies which 
blocked us from trying more ranking algorithms and tuning 
parameters in one algorithm 
 acknowledgments 
we thank bin wang and zhi wei li for providing dedup 
codes to detect duplicate photos zhen li for helping us 
design the interface of enjoyphoto ming jing li longbin 
chen changhu wang yuanhao chen and li zhuang etc 
for useful discussions special thanks go to dwight daniels 
for helping us revise the language of this paper 
 references 
 google image search http images google com 
 google local search http local google com 
 google news search http news google com 
 google paper search http scholar google com 
 google product search http froogle google com 
 google video search http video google com 
 scientific literature digital library 
http citeseer ist psu edu 
 yahoo image search http images yahoo com 
 r baeza-yates and b ribeiro-neto modern 
information retrieval new york acm press 
harlow england addison-wesley 
 w bin l zhiwei l ming jing and m wei-ying 
large-scale duplicate detection for web image search 
in proceedings of the international conference on 
multimedia and expo page 
 s brin and l page the anatomy of a large-scale 
hypertextual web search engine in computer 
networks volume pages - 
 c burges t shaked e renshaw a lazier 
m deeds n hamilton and g hullender learning 
to rank using gradient descent in proceedings of the 
 nd international conference on machine learning 
pages - 
 c dwork r kumar m naor and d sivakumar 
rank aggregation methods for the web in proceedings 
 th international conference on world wide web 
pages - hong-kong 
 r fagin r kumar and d sivakumar comparing 
top k lists siam journal on discrete mathematics 
 - 
 y freund r iyer r e schapire and y singer an 
efficient boosting algorithm for combining preferences 
 
journal of machine learning research 
 - 
 imdb formula for calculating the top rated titles 
in imdb http www imdb com chart top 
 t joachims optimizing search engines using 
clickthrough data in proceedings of the eighth acm 
sigkdd international conference on knowledge 
discovery and data mining pages - 
 j m kleinberg authoritative sources in a 
hyperlinked environment journal of the acm 
 - 
 r nallapati discriminative models for information 
retrieval in proceedings of the th annual 
international acm sigir conference on research and 
development in information retrieval pages - 
 
 z nie y ma j -r wen and w -y ma object-level 
web information retrieval in technical report of 
microsoft research volume msr-tr- - 
 z nie y zhang j -r wen and w -y ma 
object-level ranking bringing order to web objects 
in proceedings of the th international conference on 
world wide web pages - chiba japan 
 
 l page s brin r motwani and t winograd the 
pagerank citation ranking bringing order to the web 
in technical report stanford digital libraries 
 a savakis s etz and a loui evaluation of image 
appeal in consumer photography in spie human 
vision and electronic imaging pages - 
 d sullivan hitwise search engine ratings search 
engine watch articles http searchenginewatch 
com reports article php august 
 s susstrunk and s winkler color image quality on 
the internet in is t spie electronic imaging 
internet imaging v volume pages - 
 
 h tong m li z h j j he and z c s 
classification of digital photos taken by photographers 
or home users in pacific-rim conference on 
multimedia pcm pages - 
 w xi b zhang z chen y lu s yan w -y ma 
and e a fox link fusion a unified link analysis 
framework for multi-type interrelated data objects in 
proceedings of the th international conference on 
world wide web pages - 
 
a new approach for evaluating query expansion 
query-document term mismatch 
tonya custis 
thomson corporation 
 opperman drive 
st paul mn 
tonya custis thomson com 
khalid al-kofahi 
thomson corporation 
 opperman drive 
st paul mn 
khalid al-kofahi thomson com 
abstract 
the effectiveness of information retrieval ir systems is 
influenced by the degree of term overlap between user queries 
and relevant documents query-document term mismatch 
whether partial or total is a fact that must be dealt with by 
ir systems query expansion qe is one method for 
dealing with term mismatch ir systems implementing query 
expansion are typically evaluated by executing each query 
twice with and without query expansion and then 
comparing the two result sets while this measures an overall 
change in performance it does not directly measure the 
effectiveness of ir systems in overcoming the inherent issue of 
term mismatch between the query and relevant documents 
nor does it provide any insight into how such systems would 
behave in the presence of query-document term mismatch 
in this paper we propose a new approach for evaluating 
query expansion techniques the proposed approach is 
attractive because it provides an estimate of system 
performance under varying degrees of query-document term 
mismatch it makes use of readily available test collections and 
it does not require any additional relevance judgments or 
any form of manual processing 
categories and subject descriptors 
h information storage and retrieval information 
search and retrieval 
general terms 
measurement experimentation 
 introduction 
in our domain 
and unlike web search it is very 
important for attorneys to find all documents e g cases that 
are relevant to an issue missing relevant documents may 
have non-trivial consequences on the outcome of a court 
proceeding attorneys are especially concerned about missing 
relevant documents when researching a legal topic that is 
new to them as they may not be aware of all language 
variations in such topics therefore it is important to develop 
information retrieval systems that are robust with respect to 
language variations or term mismatch between queries and 
relevant documents during our work on developing such 
systems we concluded that current evaluation methods are 
not sufficient for this purpose 
{whooping cough pertussis} {heart attack myocardial 
infarction} {car wash automobile cleaning} {attorney 
legal counsel lawyer} are all examples of things that share 
the same meaning often the terms chosen by users in their 
queries are different than those appearing in the documents 
relevant to their information needs this query-document 
term mismatch arises from two sources the synonymy 
found in natural language both at the term and the phrasal 
level and the degree to which the user is an expert at 
searching and or has expert knowledge in the domain of the 
collection being searched 
ir evaluations are comparative in nature cf trec 
generally ir evaluations show how system a did in 
relation to system b on the same test collection based on various 
precision- and recall-based metrics similarly ir systems 
with qe capabilities are typically evaluated by executing 
each search twice once with and once without query 
expansion and then comparing the two result sets while this 
approach shows which system may have performed better 
overall with respect to a particular test collection it does 
not directly or systematically measure the effectiveness of 
ir systems in overcoming query-document term mismatch 
if the goal of qe is to increase search performance by 
mitigating the effects of query-document term mismatch then 
the degree to which a system does so should be measurable 
in evaluation an effective evaluation method should 
measure the performance of ir systems under varying degrees of 
query-document term mismatch not just in terms of overall 
performance on a collection relative to another system 
 
thomson corporation builds information based solutions 
to the professional markets including legal financial health 
care scientific and tax and accounting 
in order to measure that a particular ir system is able 
to overcome query-document term mismatch by retrieving 
documents that are relevant to a user s query but that do 
not necessarily contain the query terms themselves we 
systematically introduce term mismatch into the test collection 
by removing query terms from known relevant documents 
because we are purposely inducing term mismatch between 
the queries and known relevant documents in our test 
collections the proposed evaluation framework is able to measure 
the effectiveness of qe in a way that testing on the whole 
collection is not if a qe search method finds a document 
that is known to be relevant but that is nonetheless missing 
query terms it shows that qe technique is indeed robust 
with respect to query-document term mismatch 
 related work 
accounting for term mismatch between the terms in user 
queries and the documents relevant to users information 
needs has been a fundamental issue in ir research for 
almost years query expansion qe is one 
technique used in ir to improve search performance by 
increasing the likelihood of term overlap either explicitly or 
implicitly between queries and documents that are relevant 
to users information needs explicit query expansion 
occurs at run-time based on the initial search results as is 
the case with relevance feedback and pseudo relevance 
feedback implicit query expansion can be based on 
statistical properties of the document collection or it may 
rely on external knowledge sources such as a thesaurus or an 
ontology regardless of method qe 
algorithms that are capable of retrieving relevant documents 
despite partial or total term mismatch between queries and 
relevant documents should increase the recall of ir systems 
 by retrieving documents that would have previously been 
missed as well as their precision by retrieving more 
relevant documents 
in practice qe tends to improve the average overall 
retrieval performance doing so by improving performance on 
some queries while making it worse on others qe 
techniques are judged as effective in the case that they help 
more than they hurt overall on a particular collection 
 often the expansion terms added to a query 
in the query expansion phase end up hurting the overall 
retrieval performance because they introduce semantic noise 
causing the meaning of the query to drift as such much 
work has been done with respect to different strategies for 
choosing semantically relevant qe terms to include in order 
to avoid query drift 
the evaluation of ir systems has received much attention 
in the research community both in terms of developing test 
collections for the evaluation of different systems 
 and in terms of the utility of evaluation metrics such as 
recall precision mean average precision precision at rank 
bpref etc in addition there have been 
comparative evaluations of different qe techniques on various 
test collections 
in addition the ir research community has given 
attention to differences between the performance of individual 
queries research efforts have been made to predict which 
queries will be improved by qe and then selectively 
applying it only to those queries to achieve 
optimal overall performance in addition related work on 
predicting query difficulty or which queries are likely to 
perform poorly has been done there is general 
interest in the research community to improve the 
robustness of ir systems by improving retrieval performance on 
difficult queries as is evidenced by the robust track in the 
trec competitions and new evaluation measures such as 
gmap gmap geometric mean average precision gives 
more weight to the lower end of the average precision as 
opposed to map thereby emphasizing the degree to which 
difficult or poorly performing queries contribute to the score 
 
however no attention is given to evaluating the 
robustness of ir systems implementing qe with respect to 
querydocument term mismatch in quantifiable terms by 
purposely inducing mismatch between the terms in queries and 
relevant documents our evaluation framework allows us a 
controlled manner in which to degrade the quality of the 
queries with respect to their relevant documents and then 
to measure the both the degree of induced difficulty of the 
query and the degree to which qe improves the retrieval 
performance of the degraded query 
the work most similar to our own in the literature consists 
of work in which document collections or queries are altered 
in a systematic way to measure differences query 
performance introduces into the document collection 
pseudowords that are ambiguous with respect to word sense in 
order to measure the degree to which word sense 
disambiguation is useful in ir experiments with altering the 
document collection by adding semantically related expansion 
terms to documents at indexing time in cross-language ir 
 explores different query expansion techniques while 
purposely degrading their translation resources in what amounts 
to expanding a query with only a controlled percentage of 
its translation terms although similar in introducing a 
controlled amount of variance into their test collections these 
works differ from the work being presented in this paper 
in that the work being presented here explicitly and 
systematically measures query effectiveness in the presence of 
query-document term mismatch 
 methodology 
in order to accurately measure ir system performance in 
the presence of query-term mismatch we need to be able 
to adjust the degree of term mismatch in a test corpus in 
a principled manner our approach is to introduce 
querydocument term mismatch into a corpus in a controlled 
manner and then measure the performance of ir systems as 
the degree of term mismatch changes we systematically 
remove query terms from known relevant documents 
creating alternate versions of a test collection that differ only in 
how many or which query terms have been removed from 
the documents relevant to a particular query introducing 
query-document term mismatch into the test collection in 
this manner allows us to manipulate the degree of term 
mismatch between relevant documents and queries in a 
controlled manner 
this removal process affects only the relevant documents 
in the search collection the queries themselves remain 
unaltered query terms are removed from documents one by 
one so the differences in ir system performance can be 
measured with respect to missing terms in the most extreme 
case i e when the length of the query is less than or equal 
to the number of query terms removed from the relevant 
documents there will be no term overlap between a query 
and its relevant documents notice that for a given query 
only relevant documents are modified non-relevant 
documents are left unchanged even in the case that they contain 
query terms 
although on the surface we are changing the 
distribution of terms between the relevant and non-relevant 
documents sets by removing query terms from the relevant 
documents doing so does not change the conceptual relevancy 
of these documents systematically removing query terms 
from known relevant documents introduces a controlled amount 
of query-document term mismatch by which we can 
evaluate the degree to which particular qe techniques are able 
to retrieve conceptually relevant documents despite a lack 
of actual term overlap removing a query term from 
relevant documents simply masks the presence of that query 
term in those documents it does not in any way change the 
conceptual relevancy of the documents 
the evaluation framework presented in this paper consists 
of three elements a test collection c a strategy for selecting 
which query terms to remove from the relevant documents in 
that collection s and a metric by which to compare 
performance of the ir systems m the test collection c consists 
of a document collection queries and relevance judgments 
the strategy s determines the order and manner in which 
query terms are removed from the relevant documents in c 
this evaluation framework is not metric-specific any metric 
 map p  recall etc can be used to measure ir system 
performance 
although test collections are difficult to come by it should 
be noted that this evaluation framework can be used on 
any available test collection in fact using this framework 
stretches the value of existing test collections in that one 
collection becomes several when query terms are removed from 
relevant documents thereby increasing the amount of 
information that can be gained from evaluating on a particular 
collection 
in other evaluations of qe effectiveness the controlled 
variable is simply whether or not queries have been 
expanded or not compared in terms of some metric in 
contrast the controlled variable in this framework is the query 
term that has been removed from the documents relevant to 
that query as determined by the removal strategy s query 
terms are removed one by one in a manner and order 
determined by s so that collections differ only with respect 
to the one term that has been removed or masked in the 
documents relevant to that query it is in this way that we 
can explicitly measure the degree to which an ir system 
overcomes query-document term mismatch 
the choice of a query term removal strategy is relatively 
flexible the only restriction in choosing a strategy s is that 
query terms must be removed one at a time two 
decisions must be made when choosing a removal strategy s 
the first is the order in which s removes terms from the 
relevant documents possible orders for removal could be 
based on metrics such as idf or the global probability of a 
term in a document collection based on the purpose of the 
evaluation and the retrieval algorithm being used it might 
make more sense to choose a removal order for s based on 
query term idf or perhaps based on a measure of query 
term probability in the document collection 
once an order for removal has been decided a manner for 
term removal masking must be decided it must be 
determined if s will remove the terms individually i e remove 
just one different term each time or additively i e remove 
one term first then that term in addition to another and so 
on the incremental additive removal of query terms from 
relevant documents allows the evaluation to show the 
degree to which ir system performance degrades as more and 
more query terms are missing thereby increasing the degree 
of query-document term mismatch removing terms 
individually allows for a clear comparison of the contribution of 
qe in the absence of each individual query term 
 experimental set-up 
 ir systems 
we used the proposed evaluation framework to evaluate 
four ir systems on two test collections of the four 
systems used in the evaluation two implement query 
expansion techniques okapi with pseudo-feedback for qe and 
a proprietary concept search engine we ll call it tcs for 
thomson concept search tcs is a language modeling 
based retrieval engine that utilizes a subject-appropriate 
external corpus i e legal or news as a knowledge source 
this external knowledge source is a corpus separate from 
but thematically related to the document collection to be 
searched translation probabilities for qe are calculated 
from these large external corpora 
okapi without feedback and a language model query 
likelihood ql model implemented using indri are 
included as keyword-only baselines okapi without feedback 
is intended as an analogous baseline for okapi with 
feedback and the ql model is intended as an appropriate 
baseline for tcs as they both implement language-modeling 
based retrieval algorithms we choose these as baselines 
because they are dependent only on the words appearing in 
the queries and have no qe capabilities as a result we 
expect that when query terms are removed from relevant 
documents the performance of these systems should degrade 
more dramatically than their counterparts that implement 
qe 
the okapi and ql model results were obtained using the 
lemur toolkit 
okapi was run with the parameters k 
b and k when run with feedback the feedback 
parameters used in okapi were set at documents and 
terms the ql model used jelinek-mercer smoothing with 
λ 
 test collections 
we evaluated the performance of the four ir systems 
outlined above on two different test collections the two test 
collections used were the trec ap collection tipster 
disk and the fsupp collection 
the fsupp collection is a proprietary collection of 
case law documents for which we have queries ranging 
from four to twenty-two words after stop word removal with 
full relevance judgments 
the average length of documents 
in the fsupp collection is words 
 
www lemurproject org 
 
each of the documents was evaluated by domain 
experts with respect to each of the queries 
the trec ap test collection contains 
documents averaging words in length in our evaluation we 
used both the title and the description fields of topics 
 as queries so we have two sets of results for the ap 
collection after stop word removal the title queries range 
from two to eleven words and the description queries range 
from four to twenty-six terms 
 query term removal strategy 
in our experiments we chose to sequentially and 
additively remove query terms from highest-to-lowest inverse 
document frequency idf with respect to the entire 
document collection terms with high idf values tend to 
influence document ranking more than those with lower idf 
values additionally high idf terms tend to be 
domainspecific terms that are less likely to be known to non-expert 
user hence we start by removing these first 
for the fsupp collection queries were evaluated 
incrementally with one two three five and seven terms 
removed from their corresponding relevant documents the 
longer description queries from trec topics - were 
likewise evaluated on the ap collection with one two 
three five and seven query terms removed from their 
relevant documents for the shorter trec title queries we 
removed one two three and five terms from the relevant 
documents 
 metrics 
in this implementation of the evaluation framework we 
chose three metrics by which to compare ir system 
performance mean average precision map precision at 
documents p and recall at documents although 
these are the metrics we chose to demonstrate this 
framework any appropriate ir metrics could be used within the 
framework 
 results 
 fsupp collection 
figures and show the performance in terms of 
map p and recall respectively for the four search 
engines on the fsupp collection as expected the 
performance of the keyword-only ir systems ql and okapi drops 
quickly as query terms are removed from the relevant 
documents in the collection the performance of okapi with 
feedback okapi fb is somewhat surprising in that on the 
original collection i e prior to query term removal its 
performance is worse than that of okapi without feedback on 
all three measures 
tcs outperforms the ql keyword baseline on every 
measure except for map on the original collection i e prior 
to removing any query terms because tcs employs 
implicit query expansion using an external domain specific 
knowledge base it is less sensitive to term removal i e 
mismatch than the okapi fb which relies on terms from 
the top-ranked documents retrieved by an initial 
keywordonly search because overall search engine performance is 
frequently measured in terms of map and because other 
evaluations of qe often only consider performance on the 
entire collection i e they do not consider term mismatch 
the qe implemented in tcs would be considered in 
an 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
meanaverageprecision map 
okapi fb 
okapi 
tcs 
ql 
fsupp mean average precision with query terms removed 
figure the performance of the four retrieval 
systems on the fsupp collection in terms of mean 
average precision map and as a function of the 
number of query terms removed the horizontal axis 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
 
 
 
precisionat documents p 
okapi fb 
okapi 
tcs 
ql 
fsupp p with query terms removed 
figure the performance of the four retrieval 
systems on the fsupp collection in terms of precision 
at and as a function of the number of query terms 
removed the horizontal axis 
other evaluation to hurt performance on the fsupp 
collection however when we look at the comparison of tcs to 
ql when query terms are removed from the relevant 
documents we can see that the qe in tcs is indeed contributing 
positively to the search 
 the ap collection using the 
description queries 
figures and show the performance of the four ir 
systems on the ap collection using the trec topic 
descriptions as queries the most interesting difference 
between the performance on the fsupp collection and the 
ap collection is the reversal of okapi fb and tcs on 
fsupp tcs outperformed the other engines consistently 
 see figures and on the ap collection okapi 
fb is clearly the best performer see figures and 
this is all the more interesting based on the fact that qe in 
okapi fb takes place after the first search iteration which 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
 
recall 
okapi fb 
okapi 
tcs 
indri 
fsupp recall at documents with query terms removed 
figure the recall at of the four retrieval 
systems on the fsupp collection as a function of 
the number of query terms removed the horizontal 
axis 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
meanaverageprecision map 
okapi fb 
okapi 
tcs 
ql 
ap mean average precision with query terms removed description queries 
figure map of the four ir systems on the ap 
collection using trec description queries map 
is measured as a function of the number of query 
terms removed 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
precisionat documents p 
okapi fb 
okapi 
tcs 
ql 
ap p with query terms removed description queries 
figure precision at of the four ir systems 
on the ap collection using trec description 
queries p at is measured as a function of the 
number of query terms removed 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
recall 
okapi fb 
okapi 
tcs 
ql 
ap recall at documents with query terms removed description queries 
figure recall at of the four ir systems 
on the ap collection using trec description 
queries and as a function of the number of query 
terms removed 
we would expect to be handicapped when query terms are 
removed 
looking at p in figure we can see that tcs and 
okapi fb score similarly on p starting at the point where 
one query term is removed from relevant documents at two 
query terms removed tcs starts outperforming okapi fb 
if modeling this in terms of expert versus non-expert users 
we could conclude that tcs might be a better search engine 
for non-experts to use on the ap collection while okapi 
fb would be best for an expert searcher 
it is interesting to note that on each metric for the ap 
description queries tcs performs more poorly than all the 
other systems on the original collection but quickly 
surpasses the baseline systems and approaches okapi fb s 
performance as terms are removed this is again a case where 
the performance of a system on the entire collection is not 
necessarily indicative of how it handles query-document term 
mismatch 
 the ap collection using the title queries 
figures and show the performance of the four ir 
systems on the ap collection using the trec topic titles 
as queries as with the ap description queries okapi 
fb is again the best performer of the four systems in the 
evaluation as before the performance of the okapi and 
ql systems the non-qe baseline systems sharply degrades 
as query terms are removed on the shorter queries tcs 
seems to have a harder time catching up to the performance 
of okapi fb as terms are removed 
perhaps the most interesting result from our evaluation 
is that although the keyword-only baselines performed 
consistently and as expected on both collections with respect 
to query term removal from relevant documents the 
performances of the engines implementing qe techniques differed 
dramatically between collections 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
meanaverageprecision map 
okapi fb 
okapi 
tcs 
ql 
ap mean average precision with query terms removed title queries 
figure map of the four ir systems on the ap 
collection using trec title queries and as a 
function of the number of query terms removed 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
 
 
precisionat documents p 
okapi fb 
okapi 
tcs 
ql 
ap p with query terms removed title queries 
figure precision at of the four ir systems on 
the ap collection using trec title queries and 
as a function of the number of query terms removed 
 
number of query terms removed from relevant documents 
 
 
 
 
 
 
 
 
 
recall 
okapi fb 
okapi 
tcs 
ql 
ap recall at documents with query terms removed title queries 
figure recall at of the four ir systems on 
the ap collection using trec title queries and 
as a function of the number of query terms removed 
 discussion 
the intuition behind this evaluation framework is to 
measure the degree to which various qe techniques overcome 
term mismatch between queries and relevant documents in 
general it is easy to evaluate the overall performance of 
different techniques for qe in comparison to each other or 
against a non-qe variant on any complete test collection 
such an approach does tell us which systems perform better 
on a complete test collection but it does not measure the 
ability of a particular qe technique to retrieve relevant 
documents despite partial or complete term mismatch between 
queries and relevant documents 
a systematic evaluation of ir systems as outlined in this 
paper is useful not only with respect to measuring the 
general success or failure of particular qe techniques in the 
presence of query-document term mismatch but it also 
provides insight into how a particular ir system will perform 
when used by expert versus non-expert users on a 
particular collection the less a user knows about the domain of 
the document collection on which they are searching the 
more prevalent query-document term mismatch is likely to 
be this distinction is especially relevant in the case that 
the test collection is domain-specific i e medical or legal as 
opposed to a more general domain such as news where the 
distinction between experts and non-experts may be more 
marked for example a non-expert in the medical domain 
might search for whooping cough but relevant documents 
might instead contain the medical term pertussis 
since query terms are masked only the in relevant 
documents this evaluation framework is actually biased against 
retrieving relevant documents this is because non-relevant 
documents may also contain query terms which can cause 
a retrieval system to rank such documents higher than it 
would have before terms were masked in relevant documents 
still we think this is a more realistic scenario than removing 
terms from all documents regardless of relevance 
the degree to which a qe technique is well-suited to a 
particular collection can be evaluated in terms of its ability 
to still find the relevant documents even when they are 
missing query terms despite the bias of this approach against 
relevant documents however given that okapi fb and tcs 
outperformed each other on two different collection sets 
further investigation into the degree of compatibility between 
qe expansion approach and target collection is probably 
warranted furthermore the investigation of other term 
removal strategies could provide insight into the behavior of 
different qe techniques and their overall impact on the user 
experience 
as mentioned earlier our choice of the term removal 
strategy was motivated by our desire to see the highest 
impact on system performance as terms are removed and 
because high idf terms in our domain context are more 
likely to be domain specific which allows us to better 
understand the performance of an ir system as experienced 
by expert and non-expert users 
although not attempted in our experiments another 
application of this evaluation framework would be to remove 
query terms individually rather than incrementally to 
analyze which terms or possibly which types of terms are 
being helped most by a qe technique on a particular test 
collection this could lead to insight as to when qe should 
and should not be applied 
this evaluation framework allows us to see how ir 
systems perform in the presence of query-document term 
mismatch in other evaluations the performance of a system is 
measured only on the entire collection in which the degree 
of query-term document mismatch is not known by 
systematically introducing this mismatch we can see that even 
if an ir system is not the best performer on the entire 
collection its performance may nonetheless be more robust to 
query-document term mismatch than other systems such 
robustness makes a system more user-friendly especially to 
non-expert users 
this paper presents a novel framework for ir system 
evaluation the applications of which are numerous the results 
presented in this paper are not by any means meant to be 
exhaustive or entirely representative of the ways in which 
this evaluation could be applied to be sure there is much 
future work that could be done using this framework 
in addition to looking at average performance of ir 
systems the results of individual queries could be examined and 
compared more closely perhaps giving more insight into the 
classification and prediction of difficult queries or perhaps 
showing which qe techniques improve or degrade 
individual query performance under differing degrees of 
querydocument term mismatch indeed this framework would 
also benefit from further testing on a larger collection 
 conclusion 
the proposed evaluation framework allows us to measure 
the degree to which different ir systems overcome or don t 
overcome term mismatch between queries and relevant 
documents evaluations of ir systems employing qe performed 
only on the entire collection do not take into account that 
the purpose of qe is to mitigate the effects of term mismatch 
in retrieval by systematically removing query terms from 
relevant documents we can measure the degree to which 
qe contributes to a search by showing the difference 
between the performances of a qe system and its 
keywordonly baseline when query terms have been removed from 
known relevant documents further we can model the 
behavior of expert versus non-expert users by manipulating 
the amount of query-document term mismatch introduced 
into the collection 
the evaluation framework proposed in this paper is 
attractive for several reasons most importantly it provides 
a controlled manner in which to measure the performance 
of qe with respect to query-document term mismatch in 
addition this framework takes advantage and stretches the 
amount of information we can get from existing test 
collections further this evaluation framework is not 
metricspecific information in terms of any metric map p  
etc can be gained from evaluating an ir system this way 
it should also be noted that this framework is 
generalizable to any ir system in that it evaluates how well ir 
systems evaluate users information needs as represented by 
their queries an ir system that is easy to use should be 
good at retrieving documents that are relevant to users 
information needs even if the queries provided by the users do 
not contain the same keywords as the relevant documents 
 references 
 amati g c carpineto and g romano query 
difficulty robustness and selective application of query 
expansion in proceedings of the th european 
conference on information retrieval ecir 
pp - 
 berger a and j d lafferty information 
retrieval as statistical translation in research and 
development in information retrieval pages - 
 billerbeck b f scholer h e williams and j 
zobel query expansion using associated queries 
in proceedings of cikm pp - 
 billerbeck b and j zobel when query 
expansion fails in proceedings of sigir pp 
 - 
 billerbeck b and j zobel questioning query 
expansion an examination of behaviour and 
parameters in proceedings of the th australasian 
database conference adc pp - 
 billerbeck b and j zobel document 
expansion versus query expansion for ad-hoc 
retrieval in proceedings of the th australasian 
document computing symposium 
 buckley c and e m voorhees evaluating 
evaluation measure stability in proceedings of sigir 
 pp - 
 buckley c and e m voorhees retrieval 
evaluation with incomplete information in 
proceedings of sigir pp - 
 carmel d e yom-tov a darlow d pelleg 
what makes a query difficult in proceedings of 
sigir pp - 
 carpineto c r mori and g romano 
informative term selection for automatic query 
expansion in the th text retrieval conference 
pp 
 carterette b and j allan incremental test 
collections in proceedings of cikm pp 
 - 
 carterette b j allan and r sitaraman 
minimal test collections for retrieval evaluation in 
proceedings of sigir pp - 
 cormack g v c r palmer and c l clarke 
efficient construction of large test collections in 
proceedings of sigir pp - 
 cormack g and t r lynam statistical 
precision of information retrieval evaluation in 
proceedings of sigir pp - 
 cronen-townsend s y zhou and w b croft 
a language modeling framework for selective query 
expansion ciir technical report 
 efthimiadis e n query expansion in martha 
e williams ed annual review of information 
systems and technology arist v pp - 
 evans d a and lefferts r g clarit-trec 
experiments information processing management 
 - 
 fang h and c x zhai semantic term 
matching in axiomatic approaches to information 
retrieval in proceedings of sigir pp - 
 gao j j nie g wu and g cao dependence 
language model for information retrieval in 
proceedings of sigir pp - 
 harman d k relevance feedback revisited in 
proceedings of acm sigir pp - 
 harman d k ed the first text retrieval 
conference trec- 
 harman d k ed the second text retrieval 
conference trec- 
 harman d k ed the third text retrieval 
conference trec- 
 harman d k towards interactive query 
expansion in proceedings of sigir pp - 
 hofmann t probabilistic latent semantic 
indexing in proceedings of sigir pp - 
 jing y and w b croft the association 
thesaurus for information retrieval in proceedings of 
riao pp - 
 lu x a and r b keefer query expansion reduction 
and its impact on retrieval effectiveness in d k 
harman ed the third text retrieval conference 
 trec- gaithersburg md national institute of 
standards and technology - 
 mcnamee p and j mayfield comparing 
cross-language query expansion techniques by 
degrading translation resources in proceedings of 
sigir pp - 
 mitra m a singhal and c buckley 
improving automatic query expansion in 
proceedings of sigir pp - 
 peat h j and p willett the limitations of 
term co-occurrence data for query expansion in 
document retrieval systems journal of the american 
society for information science - 
 ponte j m and w b croft a language 
modeling approach to information retrieval in 
proceedings of sigir pp - 
 qiu y and frei h concept based query 
expansion in proceedings of sigir pp - 
 robertson s on gmap - and other 
transformations in proceedings of cikm pp 
 - 
 robertson s e and k sparck jones relevance 
weighting of search terms journal of the american 
society for information science - 
 robertson s e s walker s jones m m 
hancock-beaulieu and m gatford okapi at 
trec- in d k harman ed the second text 
retrieval conference trec- pp - 
 robertson s e s walker s jones m m 
hancock-beaulieu and m gatford okapi at 
trec- in d k harman ed the third text 
retrieval conference trec- pp - 
 rocchio j j relevance feedback in information 
retrieval in g salton ed the smart retrieval 
system prentice-hall inc englewood cliffs nj pp 
 - 
 salton g automatic information organization 
and retrieval mcgraw-hill 
 salton g the smart retrieval system 
experiments in automatic document processing 
englewood cliffs nj prentice-hall 
 salton g automatic term class construction 
using relevance-a summary of work in automatic 
pseudoclassification information processing 
management - 
 salton g and c buckley on the use of 
spreading activation methods in automatic 
information retrieval in proceedings of sigir 
pp - 
 sanderson m word sense disambiguation and 
information retrieval in proceedings of sigir 
pp - 
 sanderson m and h joho forming test 
collections with no system pooling in proceedings of 
sigir pp - 
 sanderson m and zobel j information 
retrieval system evaluation effort sensitivity and 
reliability in proceedings of sigir pp - 
 smeaton a f and c j van rijsbergen the 
retrieval effects of query expansion on a feedback 
document retrieval system computer journal 
 - 
 song f and w b croft a general language 
model for information retrieval in proceedings of the 
eighth international conference on information and 
knowledge management pages - 
 sparck jones k automatic keyword 
classification for information retrieval london 
butterworths 
 terra e and c l clarke scoring missing 
terms in information retrieval tasks in proceedings of 
cikm pp - 
 turtle howard natural language vs boolean 
query evaluation a comparison of retrieval 
performance in proceedings of sigir pp 
 - 
 voorhees e m a on expanding query vectors 
with lexically related words in harman d k ed 
text retrieval conference trec- 
 voorhees e m b query expansion using 
lexical-semantic relations in proceedings of sigir 
 pp - 
distance measures for mpeg- -based retrieval 
horst eidenberger 
vienna university of technology institute of software technology and interactive systems 
favoritenstrasse - - a- vienna austria 
tel - - - 
eidenberger ims tuwien ac at 
abstract 
in visual information retrieval the careful choice of suitable 
proximity measures is a crucial success factor the evaluation 
presented in this paper aims at showing that the distance measures 
suggested by the mpeg- group for the visual descriptors can be 
beaten by general-purpose measures eight visual mpeg- 
descriptors were selected and distance measures implemented 
three media collections were created and assessed performance 
indicators developed and more than tests performed 
additionally a quantisation model was developed to be able to 
use predicate-based distance measures on continuous data as well 
the evaluation shows that the distance measures recommended in 
the mpeg- -standard are among the best but that other measures 
perform even better 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval - information filtering query formulation 
retrieval models 
general terms 
algorithms measurement experimentation performance theory 
 introduction 
the mpeg- standard defines - among others - a set of 
descriptors for visual media each descriptor consists of a feature 
extraction mechanism a description in binary and xml format 
and guidelines that define how to apply the descriptor on different 
kinds of media e g on temporal media the mpeg- descriptors 
have been carefully designed to meet - partially 
complementaryrequirements of different application domains archival browsing 
retrieval etc in the following we will exclusively deal with 
the visual mpeg- descriptors in the context of media retrieval 
the visual mpeg- descriptors fall in five groups colour 
texture shape motion and others e g face description and sum 
up to basic descriptors for retrieval applications a rule for 
each descriptor is mandatory that defines how to measure the 
similarity of two descriptions common rules are distance 
functions like the euclidean distance and the mahalanobis 
distance unfortunately the mpeg- standard does not include 
distance measures in the normative part because it was not 
designed to be and should not exclusively understood to be 
retrieval-specific however the mpeg- authors give 
recommendations which distance measure to use on a particular 
descriptor these recommendations are based on accurate 
knowledge of the descriptors behaviour and the description 
structures 
in the present study a large number of successful distance 
measures from different areas statistics psychology medicine 
social and economic sciences etc were implemented and applied 
on mpeg- data vectors to verify whether or not the 
recommended mpeg- distance measures are really the best for 
any reasonable class of media objects from the mpeg- tests 
and the recommendations it does not become clear how many and 
which distance measures have been tested on the visual 
descriptors and the mpeg- test datasets the hypothesis is that 
analytically derived distance measures may be good in general but 
only a quantitative analysis is capable to identify the best distance 
measure for a specific feature extraction method 
the paper is organised as follows section gives a minimum of 
background information on the mpeg- descriptors and distance 
measurement in visual information retrieval vir see 
section gives an overview over the implemented distance 
measures section describes the test setup including the test 
data and the implemented evaluation methods finally section 
presents the results per descriptor and over all descriptors 
 background 
 mpeg- visual descriptors 
the visual part of the mpeg- standard defines several 
descriptors not all of them are really descriptors in the sense that 
they extract properties from visual media some of them are just 
structures for descriptor aggregation or localisation the basic 
descriptors are color layout color structure dominant color 
scalable color edge histogram homogeneous texture texture 
browsing region-based shape contour-based shape camera 
motion parametric motion and motion activity 
other descriptors are based on low-level descriptors or semantic 
information group-of-frames group-of-pictures color based on 
scalable color shape d based on d mesh information 
motion trajectory based on object segmentation and face 
recognition based on face extraction 
descriptors for spatiotemporal aggregation and localisation are 
spatial d coordinates grid layout region locator spatial 
time series temporal interpolation temporal and 
spatiotemporal locator combined finally other structures 
exist for colour spaces colour quantisation and multiple d views 
of d objects 
these additional structures allow combining the basic descriptors 
in multiple ways and on different levels but they do not change 
the characteristics of the extracted information consequently 
structures for aggregation and localisation were not considered in 
the work described in this paper 
 similarity measurement on visual data 
generally similarity measurement on visual information aims at 
imitating human visual similarity perception unfortunately 
human perception is much more complex than any of the existing 
similarity models it includes perception recognition and 
subjectivity 
the common approach in visual information retrieval is 
measuring dis-similarity as distance both query object and 
candidate object are represented by their corresponding feature 
vectors the distance between these objects is measured by 
computing the distance between the two vectors consequently 
the process is independent of the employed querying paradigm 
 e g query by example the query object may be natural e g a 
real object or artificial e g properties of a group of objects 
goal of the measurement process is to express a relationship 
between the two objects by their distance iteration for multiple 
candidates allows then to define a partial order over the 
candidates and to address those in a to be defined 
neighbourhood being similar to the query object at this point it 
has to be mentioned that in a multi-descriptor 
environmentespecially in mpeg- - we are only half way towards a statement 
on similarity if multiple descriptors are used e g a descriptor 
scheme a rule has to be defined how to combine all distances to 
a global value for each object still distance measurement is the 
most important first step in similarity measurement 
obviously the main task of good distance measures is to 
reorganise descriptor space in a way that media objects with the 
highest similarity are nearest to the query object if distance is 
defined minimal the query object is always in the origin of 
distance space and similar candidates should form clusters around 
the origin that are as large as possible consequently many well 
known distance measures are based on geometric assumptions of 
descriptor space e g euclidean distance is based on the metric 
axioms unfortunately these measures do not fit ideally with 
human similarity perception e g due to human subjectivity to 
overcome this shortage researchers from different areas have 
developed alternative models that are mostly predicate-based 
 descriptors are assumed to contain just binary elements e g 
tversky s feature contrast model and fit better with human 
perception in the following distance measures of both groups of 
approaches will be considered 
 distance measures 
the distance measures used in this work have been collected from 
various areas subsection because they work on differently 
quantised data subsection sketches a model for unification on 
the basis of quantitative descriptions finally subsection 
introduces the distance measures as well as their origin and the 
idea they implement 
 sources 
distance measurement is used in many research areas such as 
psychology sociology e g comparing test results medicine e g 
comparing parameters of test persons economics e g comparing 
balance sheet ratios etc naturally the character of data available 
in these areas differs significantly essentially there are two 
extreme cases of data vectors and distance measures 
predicatebased all vector elements are binary e g { } and quantitative 
 all vector elements are continuous e g 
predicates express the existence of properties and represent 
highlevel information while quantitative values can be used to measure 
and mostly represent low-level information predicates are often 
employed in psychology sociology and other human-related 
sciences and most predicate-based distance measures were 
therefore developed in these areas descriptions in visual 
information retrieval are nearly ever if they do not integrate 
semantic information quantitative consequently mostly 
quantitative distance measures are used in visual information 
retrieval 
the goal of this work is to compare the mpeg- distance 
measures with the most powerful distance measures developed in 
other areas since mpeg- descriptions are purely quantitative 
but some of the most sophisticated distance measures are defined 
exclusively on predicates a model is mandatory that allows the 
application of predicate-based distance measures on quantitative 
data the model developed for this purpose is presented in the 
next section 
 quantisation model 
the goal of the quantisation model is to redefine the set operators 
that are usually used in predicate-based distance measures on 
continuous data the first in visual information retrieval to follow 
this approach were santini and jain who tried to apply tversky s 
feature contrast model to content-based image retrieval 
 they interpreted continuous data as fuzzy predicates 
and used fuzzy set operators unfortunately their model suffered 
from several shortcomings they described in for 
example the quantitative model worked only for one specific 
version of the original predicate-based measure 
the main idea of the presented quantisation model is that set 
operators are replaced by statistical functions in the authors 
could show that this interpretation of set operators is reasonable 
the model offers a solution for the descriptors considered in the 
evaluation it is not specific to one distance measure but can be 
applied to any predicate-based measure below it will be shown 
that the model does not only work for predicate data but for 
quantitative data as well each measure implementing the model 
can be used as a substitute for the original predicate-based measure 
generally binary properties of two objects e g media objects 
can exist in both objects denoted as a in just one b c or in 
none of them d the operator needed for these relationships are 
union minus and not in the quantisation model they are 
replaced as follows see for further details 
 
∑ 
 
 
 
 
≤ 
 
− 
 
 ∩ 
k 
jkikjkik 
kkji 
else 
xx 
mif 
xx 
ssxxa 
 
 ε 
 
 
∑ 
∑ 
∑ 
 
 
 
 
≤ 
 
− ¬∩¬ 
 
 
 ≤−−− 
 − 
 
 
 ≤−−− 
 − 
k 
jkikjkik 
kkji 
k 
ikjkikjk 
kkij 
k 
jkikjkik 
kkji 
else 
xx 
if 
xx 
mssxxd 
else 
xxmifxx 
ssxxc 
else 
xxmifxx 
ssxxb 
 
 
 
 
 
 
 
 
 
ε 
ε 
ε 
with 
 
 
{ } \ 
 
 
 
 
 
 
 
 
 
 
minmax 
maxmin 
 
∈ 
− 
 
 
 
 
 
≥ 
 
 
 
 
 
− 
 
 
 
 
 
 
≥ 
 
 
 
 
 
− 
 
− 
∈ 
∑ ∑ 
∑ ∑ 
rp 
ki 
x 
where 
else 
pif 
p 
m 
ki 
x 
where 
else 
pif 
p 
m 
xxm 
xxxwithxx 
i k 
ik 
i k 
ik 
ikiki 
µ 
σ 
σ 
σ 
ε 
µ 
µ 
µ 
ε 
a selects properties that are present in both data vectors xi xj 
representing media objects b and c select properties that are 
present in just one of them and d selects properties that are present 
in neither of the two data vectors every property is selected by 
the extent to which it is present a and d mean b and c 
difference and only if the amount to which it is present exceeds a 
certain threshold depending on the mean and standard deviation 
over all elements of descriptor space 
the implementation of these operators is based on one assumption 
it is assumed that vector elements measure on interval scale that 
means each element expresses that the measured property is 
 more or less present not at all m fully present this is 
true for most visual descriptors and all mpeg- descriptors a 
natural origin as it is assumed here is not needed 
introducing p called discriminance-defining parameter for the 
thresholds εε has the positive consequence that a b c d can 
then be controlled through a single parameter p is an additional 
criterion for the behaviour of a distance measure and determines 
the thresholds used in the operators it expresses how accurate 
data items are present quantisation and consequently how 
accurate they should be investigated p can be set by the user or 
automatically interesting are the limits 
 mp →⇒∞→ εε 
in this case all elements properties are assumed to be 
continuous high quantisation in consequence all properties of a 
descriptor are used by the operators then the distance measure is 
not discriminant for properties 
 →⇒→ εεp 
in this case all properties are assumed to be predicates in 
consequence only binary elements predicates are used by the 
operators -bit quantisation the distance measure is then highly 
discriminant for properties 
between these limits a distance measure that uses the 
quantisation model is - depending on p - more or less 
discriminant for properties this means it selects a subset of all 
available description vector elements for distance measurement 
for both predicate data and quantitative data it can be shown that 
the quantisation model is reasonable if description vectors consist 
of binary elements only p should be used as follows for example 
p can easily be set automatically 
 σµεε min ⇒→ pgep 
in this case a b c d measure like the set operators they replace 
for example table shows their behaviour for two 
onedimensional feature vectors xi and xj as can be seen the 
statistical measures work like set operators actually the 
quantisation model works accurate on predicate data for any p≠∞ 
to show that the model is reasonable for quantitative data the 
following fact is used it is easy to show that for predicate data 
some quantitative distance measures degenerate to 
predicatebased measures for example the l 
metric manhattan metric 
degenerates to the hamming distance from without weights 
distancehammingcbxxl 
k 
jkik ≡− ∑ 
if it can be shown that the quantisation model is able to 
reconstruct the quantitative measure from the degenerated 
predicate-based measure the model is obviously able to extend 
predicate-based measures to the quantitative domain this is easy 
to illustrate for purely quantitative feature vectors p should be 
used as follows again p can easily be set automatically 
 ⇒∞→ εεp 
then a and d become continuous functions 
∑ 
∑ 
 
− ⇒≡≤ 
 
 
 ⇒≡≤ 
 
− 
k 
jkik 
kk 
jkik 
k 
jkik 
kk 
jkik 
xx 
mswheresdtruem 
xx 
xx 
swheresatruem 
xx 
m 
 
 
b and c can be made continuous for the following expressions 
 
 
∑ 
∑ 
∑ 
− ⇒ 
 
 
 ≥−− 
 ⇒ 
≥−≡≤−− 
 
 
 ≥−− 
 ⇒ 
≥−≡≤−− 
k 
jkikkk 
k 
ikjkikjk 
kk 
ikjkikjk 
k 
jkikjkik 
kk 
jkikjkik 
xxswherescb 
else 
xxifxx 
swheresc 
xxmxxm 
else 
xxifxx 
swheresb 
xxmxxm 
 
 
 
 
 
 
table quantisation model on predicate vectors 
xi xj a b c d 
 
 
 
 
 
∑ 
∑ 
− − 
− − 
k 
ikjkkk 
k 
jkikkk 
xxswheresbc 
xxswherescb 
this means for sufficiently high p every predicate-based distance 
measure that is either not using b and c or just as b c b-c or c-b 
can be transformed into a continuous quantitative distance 
measure for example the hamming distance again without 
weights 
 
lxxxxswherescb 
k 
jkik 
k 
jkikkk − − ∑∑ 
the quantisation model successfully reconstructs the l 
metric 
and no distance measure-specific modification has to be made to 
the model this demonstrates that the model is reasonable in the 
following it will be used to extend successful predicate-based 
distance measures on the quantitative domain 
the major advantages of the quantisation model are it is 
application domain independent the implementation is 
straightforward the model is easy to use and finally the 
new parameter p allows to control the similarity measurement 
process in a new way discriminance on property level 
 implemented measures 
for the evaluation described in this work next to predicate-based 
 based on the quantisation model and quantitative measures the 
distance measures recommended in the mpeg- standard were 
implemented all together different distance measures 
table summarises those predicate-based measures that 
performed best in the evaluation in sum predicate-based 
measures were investigated for these measures k is the number 
of predicates in the data vectors xi and xj in p the sum is used 
for tversky s f as tversky himself does in and α β are 
weights for element b and c in the author s investigated 
tversky s feature contrast model and found α β to be the 
optimum parameters 
some of the predicate-based measures are very simple e g p 
p but have been heavily exploited in psychological research 
pattern difference p - a very powerful measure - is used in the 
statistics package spss for cluster analysis p is a correlation 
coefficient for predicates developed by pearson 
table shows the best quantitative distance measures that were 
used q and q are metric-based and were implemented as 
representatives for the entire group of minkowski distances the 
wi are weights in q ii σµ are mean and standard deviation 
for the elements of descriptor xi in q m is 
 
m 
 q the 
canberra metric is a normalised form of q similarly q 
clark s divergence coefficient is a normalised version of q q is 
a further-developed correlation coefficient that is invariant against 
sign changes this measure is used even though its particular 
properties are of minor importance for this application domain 
finally q is a measure that takes the differences between 
adjacent vector elements into account this makes it structurally 
different from all other measures 
obviously one important distance measure is missing the 
mahalanobis distance was not considered because different 
descriptors would require different covariance matrices and for 
some descriptors it is simply impossible to define a covariance 
matrix if the identity matrix was used in this case the 
mahalanobis distance would degenerate to a minkowski distance 
additionally the recommended mpeg- distances were 
implemented with the following parameters in the distance 
measure of the color layout descriptor all weights were set to 
 as in all other implemented measures in the distance measure of 
the dominant color descriptor the following parameters were 
used dtww α as recommended in the 
homogeneous texture descriptor s distance all kα were set to 
 and matching was done rotation- and scale-invariant 
important some of the measures presented in this section are 
distance measures while others are similarity measures for the 
tests it is important to notice that all similarity measures were 
inverted to distance measures 
 test setup 
subsection describes the descriptors including parameters 
and the collections including ground truth information that were 
used in the evaluation subsection discusses the evaluation 
method that was implemented and subsection sketches the test 
environment used for the evaluation process 
 test data 
for the evaluation eight mpeg- descriptors were used all 
colour descriptors color layout color structure dominant 
color scalable color all texture descriptors edge histogram 
homogeneous texture texture browsing and one shape 
descriptor region-based shape texture browsing was used even 
though the mpeg- standard suggests that it is not suitable for 
retrieval the other basic shape descriptor contour-based shape 
was not used because it produces structurally different 
descriptions that cannot be transformed to data vectors with 
elements measuring on interval-scales the motion descriptors 
were not used because they integrate the temporal dimension of 
visual media and would only be comparable if the basic colour 
texture and shape descriptors would be aggregated over time this 
was not done finally no high-level descriptors were used 
 localisation face recognition etc see subsection 
because - to the author s opinion - the behaviour of the basic 
descriptors on elementary media objects should be evaluated 
before conclusions on aggregated structures can be drawn 
table predicate-based distance measures 
no measure comment 
p cba βα −− feature contrast model 
tversky 
p a no of co-occurrences 
p cb hamming distance 
p 
k 
a russel 
p 
cb 
a 
 
kulczvnski 
p 
 
k 
bc pattern difference 
p 
 dcdbcaba 
bcad 
 
− pearson 
 
the texture browsing descriptions had to be transformed from 
five bins to an eight bin representation in order that all elements 
of the descriptor measure on an interval scale a manhattan metric 
was used to measure proximity see for details 
descriptor extraction was performed using the mpeg- reference 
implementation in the extraction process each descriptor was 
applied on the entire content of each media object and the 
following extraction parameters were used colour in color 
structure was quantised to bins for dominant color colour 
space was set to ycrcb -bit default quantisation was used and 
the default value for spatial coherency was used homogeneous 
texture was quantised to components scalable color values 
were quantised to sizeof int - bits and bins were used finally 
texture browsing was used with five components 
these descriptors were applied on three media collections with 
image content the brodatz dataset images x pixel a 
subset of the corel dataset images x pixel portrait 
and landscape and a dataset with coats-of-arms images 
images x pixel figure shows examples from the three 
collections 
designing appropriate test sets for a visual evaluation is a highly 
difficult task for example see the trec video report 
of course for identifying the best distance measure for a 
descriptor it should be tested on an infinite number of media 
objects but this is not the aim of this study it is just evaluated if 
- for likely image collections - better proximity measures than 
those suggested by the mpeg- group can be found collections 
of this relatively small size were used in the evaluation because 
the applied evaluation methods are above a certain minimum size 
invariant against collection size and for smaller collections it is 
easier to define a high-quality ground truth still the average ratio 
of ground truth size to collection size is at least especially no 
collection from the mpeg- dataset was used in the evaluation 
because the evaluations should show how well the descriptors 
and the recommended distance measures perform on unknown 
material 
when the descriptor extraction was finished the resulting xml 
descriptions were transformed into a data matrix with lines 
 media objects and columns descriptor elements to be 
usable with distance measures that do not integrate domain 
knowledge the elements of this data matrix were normalised to 
 
for the distance evaluation - next to the normalised data 
matrixhuman similarity judgement is needed in this work the ground 
truth is built of twelve groups of similar images four for each 
dataset group membership was rated by humans based on 
semantic criterions table summarises the twelve groups and the 
underlying descriptions it has to be noticed that some of these 
groups especially and are much harder to find with 
lowlevel descriptors than others 
 evaluation method 
usually retrieval evaluation is performed based on a ground truth 
with recall and precision see for example in 
multidescriptor environments this leads to a problem because the 
resulting recall and precision values are strongly influenced by the 
method used to merge the distance values for one media object 
even though it is nearly impossible to say how big the influence 
of a single distance measure was on the resulting recall and 
precision values this problem has been almost ignored so far 
in subsection it was stated that the major task of a distance 
measure is to bring the relevant media objects as close to the 
origin where the query object lies as possible even in a 
multidescriptor environment it is then simple to identify the similar 
objects in a large distance space consequently it was decided to 
table quantitative distance measures 
no measure comment no measure comment 
q 
∑ − 
k 
jkiki xxw 
city block 
distance l 
 
q 
 ∑ − 
k 
jkiki xxw 
 
euclidean 
distance l 
 
q 
∑ 
− 
k jkik 
jkik 
xx 
xx canberra metric 
lance williams 
 
q 
∑ 
− 
k jkik 
jkik 
xx 
xx 
k 
 
 
divergence 
coefficient 
clark 
q 
 ∑ ∑ 
∑ 
−− 
−− 
k k 
jjkiik 
k 
jjkiik 
xx 
xx 
 
µµ 
µµ correlation 
coefficient 
q 
 
 
 
 
 
 
−  
 
 
 
 
 
−− 
 
 
 
 
 
 
 −− 
∑∑∑ 
∑ ∑∑ 
k 
ik 
k 
jkik 
k 
ik 
k k 
jk 
k 
ikjkik 
xmkmxxmkmx 
xxmkmxx 
 
cohen 
q 
∑ ∑ 
∑ 
k k 
jkik 
k 
jkik 
xx 
xx 
 
angular distance 
gower 
q 
 ∑ 
− 
 −−− 
 
 
 
k 
k 
jkjkikik xxxx 
meehl index 
 
table ground truth information 
coll no images description 
 regular chequered patterns 
 dark white noise 
 moon-like surfaces 
brodatz 
 water-like surfaces 
 humans in nature difficult 
 images with snow mountains skiing 
 animals in nature difficult 
corel 
 large coloured flowers 
 bavarian communal arms 
 all bavarian arms difficult 
 dark objects light unsegmented shield 
arms 
 major charges on blue or red shield 
 
use indicators measuring the distribution in distance space of 
candidates similar to the query object for this evaluation instead 
of recall and precision identifying clusters of similar objects 
 based on the given ground truth is relatively easy because the 
resulting distance space for one descriptor and any distance 
measure is always one-dimensional clusters are found by 
searching from the origin of distance space to the first similar 
object grouping all following similar objects in the cluster 
breaking off the cluster with the first un-similar object and so 
forth 
for the evaluation two indicators were defined the first measures 
the average distance of all cluster means to the origin 
distanceavgclustersno 
sizecluster 
distanceclustersno 
i i 
sizecluster 
j 
ij 
d 
i 
 
 
 
 
∑ 
∑ 
 µ 
where distanceij is the distance value of the j-th element in the i-th 
cluster 
∑ 
∑ ∑ 
 clusters 
i 
i 
clusters 
i 
sizecluster 
j 
ij 
sizecluster 
distance 
distanceavg 
i 
 
 
 
 no clusters is the 
number of found clusters and cluster sizei is the size of the i-th 
cluster the resulting indicator is normalised by the distribution 
characteristics of the distance measure avg distance 
additionally the standard deviation is used in the evaluation 
process this measure turned out to produce valuable results and to 
be relatively robust against parameter p of the quantisation model 
in subsection we noted that p affects the discriminance of a 
predicate-based distance measure the smaller p is set the larger 
are the resulting clusters because the quantisation model is then 
more discriminant against properties and less elements of the data 
matrix are used this causes a side-effect that is measured by the 
second indicator more and more un-similar objects come out with 
exactly the same distance value as similar objects a problem that 
does not exist for large p s and become indiscernible from similar 
objects consequently they are false cluster members this 
phenomenon conceptually similar to the false negatives 
indicator was named cluster pollution and the indicator 
measures the average cluster pollution over all clusters 
clustersno 
doublesno 
cp 
clustersno 
i 
sizecluster 
j 
ij 
i 
 
 
 
∑ ∑ 
 
where no doublesij is the number of indiscernible un-similar 
objects associated with the j-th element of cluster i 
remark even though there is a certain influence it could be 
proven in that no significant correlation exists between 
parameter p of the quantisation model and cluster pollution 
 test environment 
as pointed out above to generate the descriptors the mpeg- 
reference implementation in version was used provided by 
tu munich image processing was done with adobe photoshop 
and normalisation and all evaluations were done with perl the 
querying process was performed in the following steps 
random selection of a ground truth group random selection of 
a query object from this group distance comparison for all 
other objects in the dataset clustering of the resulting distance 
space based on the ground truth and finally evaluation 
for each combination of dataset and distance measure queries 
were issued and evaluations were aggregated over all datasets and 
descriptors the next section shows the - partially 
surprisingresults 
 results 
in the results presented below the first indicator from subsection 
 was used to evaluate distance measures in a first step 
parameter p had to be set in a way that all measures are equally 
discriminant distance measurement is fair if the following 
condition holds true for any predicate-based measure dp and any 
continuous measure dc 
 cp dcppdcp ≈ 
then it is guaranteed that predicate-based measures do not create 
larger clusters with a higher number of similar objects for the 
price of higher cluster pollution in more than test queries 
the optimum value was found to be p 
results are organised as follows subsection summarises the 
figure test datasets left brodatz dataset middle corel dataset right coats-of-arms dataset 
 
best distance measures per descriptor section shows the best 
overall distance measures and section points out other 
interesting results for example distance measures that work 
particularly good on specific ground truth groups 
 best measure per descriptor 
figure shows the evaluation results for the first indicator for 
each descriptor the best measure and the performance of the 
mpeg- recommendation are shown the results are aggregated 
over the tested datasets 
on first sight it becomes clear that the mpeg- 
recommendations are mostly relatively good but never the best 
for color layout the difference between mp and the best 
measure the meehl index q is just and the mpeg- 
measure has a smaller standard deviation the reason why the 
meehl index is better may be that this descriptors generates 
descriptions with elements that have very similar variance 
statistical analysis confirmed that see 
for color structure edge histogram homogeneous texture 
region-based shape and scalable color by far the best measure is 
pattern difference p psychological research on human visual 
perception has revealed that in many situation differences between 
the query object and a candidate weigh much stronger than 
common properties the pattern difference measure implements 
this insight in the most consequent way in the author s opinion 
the reason why pattern difference performs so extremely well on 
many descriptors is due to this fact additional advantages of 
pattern difference are that it usually has a very low variance 
andbecause it is a predicate-based measure - its discriminance and 
cluster structure can be tuned with parameter p 
the best measure for dominant color turned out to be clark s 
divergence coefficient q this is a similar measure to pattern 
difference on the continuous domain the texture browsing 
descriptor is a special problem in the mpeg- standard it is 
recommended to use it exclusively for browsing after testing it 
for retrieval on various distance measures the author supports this 
opinion it is very difficult to find a good distance measure for 
texture browsing the proposed manhattan metric for example 
performs very bad the best measure is predicate-based p it 
works on common properties a d but produces clusters with 
very high cluster pollution for this descriptor the second 
indicator is up to eight times higher than for predicate-based 
measures on other descriptors 
 best overall measures 
figure summarises the results over all descriptors and media 
collections the diagram should give an indication on the general 
potential of the investigated distance measures for visual 
information retrieval 
it can be seen that the best overall measure is a predicate-based 
one the top performance of pattern difference p proves that 
the quantisation model is a reasonable method to extend 
predicate-based distance measures on the continuous domain the 
second best group of measures are the mpeg- 
recommendations which have a slightly higher mean but a lower 
standard deviation than pattern difference the third best measure 
is the meehl index q a measure developed for psychological 
applications but because of its characteristic properties 
tailormade for certain homogeneous descriptors 
minkowski metrics are also among the best measures the average 
mean and variance of the manhattan metric q and the 
euclidean metric q are in the range of q of course these 
measures do not perform particularly well for any of the 
descriptors remarkably for a predicate-based measure tversky s 
feature contrast model p is also in the group of very good 
measures even though it is not among the best that ends with 
q the correlation coefficient the other measures either have a 
significantly higher mean or a very large standard deviation 
 other interesting results 
distance measures that perform in average worse than others may 
in certain situations e g on specific content still perform better 
for color layout for example q is a very good measure on 
colour photos it performs as good as q and has a lower standard 
deviation for artificial images the pattern difference and the 
hamming distance produce comparable results as well 
if colour information is available in media objects pattern 
difference performs well on dominant color just worse q 
and in case of difficult ground truth group the meehl 
index is as strong as p 
 
 
 
 
 
 
 
 
 
q 
mp 
p 
mp 
q 
mp 
p 
mp 
p 
mp 
p 
mp 
p 
mp 
p 
q 
color 
layout 
color 
structure 
dominant 
color 
edge 
histogram 
homog 
texture 
region 
shape 
scalable 
color 
texture 
browsing 
figure results per measure and descriptor the horizontal axis shows the best measure and the performance of the mpeg- 
recommendation for each descriptor the vertical axis shows the values for the first indicator smaller value better cluster structure 
shades have the following meaning black µ-σ good cases black dark grey µ average and black dark grey light grey µ σ bad 
 
 conclusion 
the evaluation presented in this paper aims at testing the 
recommended distance measures and finding better ones for the 
basic visual mpeg- descriptors eight descriptors were selected 
 distance measures were implemented media collections were 
created and assessed performance indicators were defined and 
more than tests were performed to be able to use 
predicate-based distance measures next to quantitative measures a 
quantisation model was defined that allows the application of 
predicate-based measures on continuous data 
in the evaluation the best overall distance measures for visual 
content - as extracted by the visual mpeg- descriptors - turned 
out to be the pattern difference measure and the meehl index for 
homogeneous descriptions since these two measures perform 
significantly better than the mpeg- recommendations they 
should be further tested on large collections of image and video 
content e g from 
the choice of the right distance function for similarity 
measurement depends on the descriptor the queried media 
collection and the semantic level of the user s idea of similarity 
this work offers suitable distance measures for various situations 
in consequence the distance measures identified as the best will 
be implemented in the open mpeg- based visual information 
retrieval framework vizir 
acknowledgements 
the author would like to thank christian breiteneder for his 
valuable comments and suggestions for improvement the work 
presented in this paper is part of the vizir project funded by the 
austrian scientific research fund fwf under grant no p 
references 
 clark p s an extension of the coefficient of divergence for 
use with multiple characters copeia - 
 cohen j a profile similarity coefficient invariant over 
variable reflection psychological bulletin 
 
 del bimbo a visual information retrieval morgan 
kaufmann publishers san francisco ca 
 eidenberger h and breiteneder c a framework for visual 
information retrieval in proceedings visual information 
systems conference hsinchu taiwan march lncs 
 springer verlag - 
 eidenberger h and breiteneder c visual similarity 
measurement with the feature contrast model in 
proceedings spie storage and retrieval for media databases 
conference santa clara ca january spie vol 
 - 
 eidenberger h how good are the visual mpeg- features 
in proceedings spie visual communications and image 
processing conference lugano switzerland july 
spie vol - 
 gower j g multivariate analysis and multidimensional 
geometry the statistician - 
 lance g n and williams w t mixed data classificatory 
programs agglomerative systems australian comp journal 
 - 
 manjunath b s ohm j r vasudevan v v and yamada 
a color and texture descriptors in special issue on 
mpeg ieee transactions on circuits and systems for video 
technology june - 
 meehl p e the problem is epistemology not statistics 
replace significance tests by confidence intervals and 
quantify accuracy of risky numerical predictions in harlow 
l l mulaik s a and steiger j h eds what if there 
were no significance tests erlbaum mahwah nj - 
 pearson k on the coefficients of racial likeness biometrica 
 - 
 santini s and jain r similarity is a geometer multimedia 
tools and application - 
 santini s and jain r similarity measures ieee 
transactions on pattern analysis and machine intelligence 
 september - 
 sint p p similarity structures and similarity measures 
austrian academy of sciences press vienna austria 
 in german 
 smeaton a f and over p the trec- video track 
report nist special publication sp - march 
available from http trec nist gov pubs trec papers 
video over pdf last visited - - 
 smeulders a w m worring m santini s gupta a and 
jain r content-based image retrieval at the end of the early 
years ieee transactions on pattern analysis and machine 
intelligence december - 
 tversky a features of similarity psychological review 
 july - 
 
 
 
 
 
 
 
 
 
 
 
p 
mp 
q 
q 
q 
q 
p 
p 
q 
q 
q 
p 
q 
p 
p 
p 
figure overall results ordered by the first indicator the vertical axis shows the values for the first indicator smaller value better 
cluster structure shades have the following meaning black µ-σ black dark grey µ and black dark grey light grey µ σ 
 
handling locations in search engine queries 
bruno martins mário j silva sérgio freitas and ana paula afonso 
faculdade de ciências da universidade de lisboa 
 - lisboa portugal 
{bmartins mjs sfreitas apa} xldb di fc ul pt 
abstract 
this paper proposes simple techniques for handling place 
references in search engine queries an important aspect of geographical 
information retrieval we address not only the detection but also 
the disambiguation of place references by matching them 
explicitly with concepts at an ontology moreover when a query does not 
reference any locations we propose to use information from 
documents matching the query exploiting geographic scopes previously 
assigned to these documents evaluation experiments using topics 
from clef campaigns and logs from real search engine queries 
show the effectiveness of the proposed approaches 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval 
 introduction 
search engine queries are often associated with geographical 
locations either explicitly i e a location reference is given as part of 
the query or implicitly i e the location reference is not present in 
the query string but the query clearly has a local intent one 
of the concerns of geographical information retrieval gir lies in 
appropriately handling such queries bringing better targeted search 
results and improving user satisfaction 
nowadays gir is getting increasing attention systems that 
access resources on the basis of geographic context are starting to 
appear both in the academic and commercial domains 
accurately and effectively detecting location references in search 
engine queries is a crucial aspect of these systems as they are 
generally based on interpreting geographical terms differently from the 
others detecting locations in queries is also important for 
generalpropose search engines as this information can be used to improve 
ranking algorithms queries with a local intent are best answered 
with localized pages while queries without any geographical 
references are best answered with broad pages 
text mining methods have been successfully used in gir to 
detect and disambiguate geographical references in text or even to 
infer geographic scopes for documents however this body 
of research has been focused on processing web pages and full-text 
documents search engine queries are more difficult to handle in 
the sense that they are very short and with implicit and subjective 
user intents moreover the data is also noisier and more versatile 
in form and we have to deal with misspellings multilingualism 
and acronyms how to automatically understand what the user 
intended given a search query without putting the burden in the user 
himself remains an open text mining problem 
key challenges in handling locations over search engine queries 
include their detection and disambiguation the ranking of possible 
candidates the detection of false positives i e not all contained 
location names refer to geographical locations and the detection of 
implied locations by the context of the query i e when the query 
does not explicitly contain a place reference but it is nonetheless 
geographical simple named entity recognition ner algorithms 
based on dictionary look-ups for geographical names may 
introduce high false positives for queries whose location names do not 
constitute place references for example the query denzel 
washington contains the place name washington but the query is not 
geographical queries can also be geographic without containing 
any explicit reference to locations at the dictionary in these cases 
place name extraction and disambiguation does not give any results 
and we need to access other sources of information 
this paper proposes simple and yet effective techniques for 
handling place references over queries each query is split into a triple 
 what relation where where what specifies the non-geographic 
aspect of the information need where specifies the geographic 
areas of interest and relation specifies a spatial relationship 
connecting what and where when this is not possible i e the query does 
not contain any place references we try using information from 
documents matching the query exploiting geographic scopes 
previously assigned to these documents 
disambiguating place references is one of the most important 
aspects we use a search procedure that combines textual patterns 
with geographical names defined at an ontology and we use 
heuristics to disambiguate the discovered references e g more important 
places are preferred disambiguation results in having the where 
term from the triple above associated with the most likely 
corresponding concepts from the ontology when we cannot detect 
any locations we attempt to use geographical scopes previously 
inferred for the documents at the top search results by doing this 
we assume that the most frequent geographical scope in the results 
should correspond to the geographical context implicit in the query 
experiments with clef topics and sample queries from a 
web search engine show that the proposed methods are accurate 
and may have applications in improving search results 
the rest of this paper is organized as follows we first formalize 
the problem and describe related work to our research next we 
describe our approach for handling place names in queries starting 
with the general approach for disambiguating place references over 
textual strings then presenting the method for splitting a query into 
a what relation where triple and finally discussing the 
technique for exploiting geographic scopes previously assigned to 
documents in the result set section presents evaluation results 
finally we give some conclusions and directions for future research 
 concepts and related work 
search engine performance depends on the ability to capture the 
most likely meaning of a query as intended by the user 
previous studies showed that a significant portion of the queries 
submitted to search engines are geographic a recent enhancement 
to search engine technology is the addition of geographic 
reasoning combining geographic information systems and information 
retrieval in order to build search engines that find information 
associated with given locations the ability to recognize and reason 
about the geographical terminology given in the text documents 
and user queries is a crucial aspect of these geographical 
information retrieval gir systems 
extracting and distinguishing different types of entities in text is 
usually referred to as named entity recognition ner for at least 
a decade this has been an important text mining task and a key 
feature of the message understanding conferences muc ner 
has been successfully automated with near-human performance 
but the specific problem of recognizing geographical references 
presents additional challenges when handling named 
entities with a high level of detail ambiguity problems arise more 
frequently ambiguity in geographical references is bi-directional 
the same name can be used for more than one location referent 
ambiguity and the same location can have more than one name 
 reference ambiguity the former has another twist since the same 
name can be used for locations as well as for other class of 
entities such as persons or company names referent class ambiguity 
besides the recognition of geographical expressions gir also 
requires that the recognized expressions be classified and grounded 
to unique identifiers grounding the recognized expressions 
 e g associating them to coordinates or concepts at an ontology 
assures that they can be used in more advanced gir tasks 
previous works have addressed the tagging and grounding of 
locations in web pages as well as the assignment of geographic 
scopes to these documents this is a complementary 
aspect to the techniques described in this paper since if we have the 
web pages tagged with location information a search engine can 
conveniently return pages with a geographical scope related to the 
scope of the query the task of handling geographical references 
over documents is however considerably different from that of 
handling geographical references over queries in our case queries are 
usually short and often do not constitute proper sentences text 
mining techniques that make use of context information are 
difficult to apply for high accuracy 
previous studies have also addressed the use of text mining and 
automated classification techniques over search engine queries 
 however most of these works did not consider place 
references or geographical categories again these previously proposed 
methods are difficult to apply to the geographic domain 
gravano et al studied the classification of web queries into two 
types namely local and global they defined a query as local if 
its best matches on a web search engine are likely to be local pages 
such as houses for sale a number of classification algorithms 
have been evaluated using search engine queries however their 
experimental results showed that only a rather low precision and 
recall could be achieved the problem addressed in this paper is 
also slightly different since we are trying not only to detect local 
queries but also to disambiguate the local of interest 
wang et al proposed to go further than detecting local queries 
by also disambiguating the implicit local of interest the 
proposed approach works for both queries containing place references 
and queries not containing them by looking for dominant 
geographic references over query logs and text from search results 
in comparison we propose simpler techniques based on matching 
names from a geographic ontology our approach looks for spatial 
relationships at the query string and it also associates the place 
references to ontology concepts in the case of queries not containing 
explicit place references we use geographical scopes previously 
assigned to the documents whereas wang et al proposed to 
extract locations from the text of the top search results 
there are nowadays many geocoding reverse-geocoding and 
mapping services on the web that can be easily integrated with 
other applications geocoding is the process of locating points on 
the surface of the earth from alphanumeric addressing data taking 
a string with an address a geocoder queries a geographical 
information system and returns interpolated coordinate values for the 
given location instead of computing coordinates for a given place 
reference the technique described in this paper aims at assigning 
references to the corresponding ontology concepts however if 
each concept at the ontology contains associated coordinate 
information the approach described here could also be used to build a 
geocoding service most of such existing services are commercial 
in nature and there are no technical publications describing them 
a number of commercial search services have also started to 
support location-based searches google local for instance initially 
required the user to specify a location qualifier separately from the 
search query more recently it added location look-up 
capabilities that extract locations from query strings for example in a 
search for pizza seattle google local returns local results for 
pizza near seattle wa however the intrinsics of their solution 
are not published and their approach also does not handle 
locationimplicit queries moreover google local does not take spatial 
relations into account 
in sum there are already some studies on tagging geographical 
references but web queries pose additional challenges which have 
not been addressed in this paper we explain the proposed 
solutions for the identified problems 
 handlingqueriesin gir systems 
most gir queries can be parsed to what relation where 
triple where the what term is used to specify the general 
nongeographical aspect of the information need the where term is used 
to specify the geographical areas of interest and the relation term 
is used to specify a spatial relationship connecting what and where 
while the what term can assume any form in order to reflect any 
information need the relation and where terms should be part of a 
controlled vocabulary in particular the relation term should refer 
to a well-known geographical relation that the underlying gir 
system can interpret e g near or contained at and the where 
term should be disambiguated into a set of unique identifiers 
corresponding to concepts at the ontology 
different systems can use alternative schemes to take input queries 
from the users three general strategies can be identified and gir 
systems often support more than one of the following schemes 
figure strategies for processing queries in geographical information retrieval systems 
 input to the system is a textual query string this is the 
hardest case since we need to separate the query into the three 
different components and then we need to disambiguate the 
where term into a set of unique identifiers 
 input to the system is provided in two separate strings one 
concerning the what term and the other concerning the where 
the relation term can be either fixed e g always assume the 
near relation specified together with the where string 
or provided separately from the users from a set of 
possible choices although there is no need for separating query 
string into the different components we still need to 
disambiguate the where term into a set of unique identifiers 
 input to the system is provided through a query string 
together with an unambiguous description of the geographical 
area of interest e g a sketch in a map spatial coordinates 
or a selection from a set of possible choices no 
disambiguation is required and therefore the techniques described 
in this paper do not have to be applied 
the first two schemes depend on place name disambiguation 
figure illustrates how we propose to handle geographic queries 
in these first two schemes a common component is the algorithm 
for disambiguating place references into corresponding ontology 
concepts which is described next 
 from place names to ontology concepts 
a required task in handling gir queries consists of associating 
a string containing a geographical reference with the set of 
corresponding concepts at the geographic ontology we propose to do 
this according to the pseudo-code listed at algorithm 
the algorithm considers the cases where a second or even more 
than one location is given to qualify a first e g paris france 
it makes recursive calls to match each location and relies on 
hierarchical part-of relations to detect if two locations share a common 
hierarchy path one of the provided locations should be more 
general and the other more specific in the sense that there must exist 
a part-of relationship among the associated concepts at the 
ontology either direct or transitive the most specific location is a 
sub-region of the most general and the algorithm returns the most 
specific one i e for paris france the algorithm returns the 
ontology concept associated with paris the capital city of france 
we also consider the cases where a geographical type expression 
is used to qualify a given name e g city of lisbon or state 
of new york for instance the name lisbon can correspond 
to many different concepts at a geographical ontology and type 
algorithm matching a place name with ontology concepts 
require o a geographic ontology 
require gn a string with the geographic name to be matched 
 l an empty list 
 index the position in gn for the first occurrence of a comma 
semi-colon or bracket character 
 if index is defined then 
 gn the substring of gn from position to index 
 gn the substring of gn from index to length gn 
 l algorithm o gn 
 l algorithm o gn 
 for each c in l do 
 for each c in l do 
 if c is an ancestor of c at o then 
 l the list l after adding element c 
 else if c is a descendant of c at o then 
 l the list l after adding element c 
 end if 
 end for 
 end for 
 else 
 gn the string gn after removing case and diacritics 
 if gn contains a geographic type qualifier then 
 t the substring of gn containing the type qualifier 
 gn the substring of gn with the type qualifier removed 
 l the list of concepts from o with name gn and type t 
 else 
 l the list of concepts from o with name gn 
 end if 
 end if 
 return the list l 
qualifiers can provide useful information for disambiguation the 
considered type qualifiers should also described at the ontologies 
 e g each geographic concept should be associated to a type that is 
also defined at the ontology such as country district or city 
ideally the geographical reference provided by the user should 
be disambiguated into a single ontology concept however this is 
not always possible since the user may not provide all the required 
information i e a type expression or a second qualifying location 
the output is therefore a list with the possible concepts being 
referred to by the user in a final step we propose to sort this list 
so that if a single concept is required as output we can use the one 
that is ranked higher the sorting procedure reflects the likelihood 
of each concept being indeed the one referred to we propose to 
rank concepts according to the following heuristics 
 the geographical type expression associated with the 
ontology concept for the same name a country is more likely to 
be referenced than a city and in turn a city more likely to be 
referenced than a street 
 number of ancestors at the ontology top places at the 
ontology tend to be more general and are therefore more likely 
to be referenced in search engine queries 
 population count highly populated places are better known 
and therefore more likely to be referenced in queries 
 population counts from direct ancestors at the ontology 
subregions of highly populated places are better known and also 
more likely to be referenced in search engine queries 
 occurrence frequency over web documents e g google 
counts for the geographical names places names that occur 
more frequently over web documents are also more likely to 
be referenced in search engine queries 
 number of descendants at the ontology places that have 
more sub-regions tend to be more general and are therefore 
more likely to be mentioned in search engine queries 
 string size for the geographical names short names are more 
likely to be mentioned in search engine queries 
algorithm plus the ranking procedure can already handle gir 
queries where the where term is given separately from the what and 
relation terms however if the query is given in a single string we 
require the identification of the associated what relation where 
triple before disambiguating the where term into the corresponding 
ontology concepts this is described in the following section 
 handling single query strings 
algorithm provides the mechanism for separating a query string 
into a what relation where triple it uses algorithm to find 
the where term disambiguating it into a set of ontology concepts 
the algorithm starts by tokenizing the query string into 
individual words also taking care of removing case and diacritics we 
have a simple tokenizer that uses the space character as a word 
delimiter but we could also have a tokenization approach similar to 
the proposal of wang et al which relies on web occurrence 
statistics to avoid breaking collocations in the future we plan on 
testing if this different tokenization scheme can improve results 
next the algorithm tests different possible splits of the query 
building the what relation and where terms through 
concatenations of the individual tokens the relation term is matched against 
a list of possible values e g near at around or south 
of corresponding to the operators that are supported by the gir 
system note that is also the responsibility of the underlying gir 
system to interpret the actual meaning of the different spatial 
relations algorithm is used to check whether a where term 
constitutes a geographical reference or not we also check if the last 
word in the what term belongs to a list of exceptions containing for 
instance first names of people in different languages this ensures 
that a query like denzel washington is appropriately handled 
if the algorithm succeeds in finding valid relation and where 
terms then the corresponding triple is returned otherwise we 
return a triple with the what term equaling the query string and the 
relation and where terms set as empty if the entire query string 
constitutes a geographical reference we return a triple with the 
what term set to empty the where term equaling the query string 
and the relation term set the definition i e these queries 
should be answered with information about the given place 
references the algorithm also handles query strings where more 
than one geographical reference is provided using and or an 
equivalent preposition together with a recursive call to algorithm 
 a query like diamond trade in angola and south africa is 
algorithm get what relation where from a query string 
require o a geographical ontology 
require q a non-empty string with the query 
 q the string q after removing case and diacritics 
 tokens n an array of strings with the individual words of q 
 n the size of the tokens array 
 for index to n do 
 if index then 
 what concatenation of tokens index − 
 lastwhat tokens index − 
 else 
 what an empty string 
 lastwhat an empty string 
 end if 
 where concatenation of tokens index n 
 relation an empty string 
 for index index to n − do 
 relation concatenation of tokens index index 
 if relation is a valid geographical relation then 
 where concatenation of s index n 
 relation relation 
 end if 
 end for 
 if relation empty and lastwhat in an exception then 
 testgeo false 
 else 
 testgeo true 
 end if 
 if testgeo and algorithm where empty then 
 if where ends with and surroundings then 
 relation the string near 
 where the substring of where with and 
surroundings removed 
 end if 
 if what ends with and or similar then 
 what relation where algorithm what 
 where concatenation of where with where 
 end if 
 if relation an empty string then 
 if what an empty string then 
 relation the string definition 
 else 
 relation the string contained-at 
 end if 
 end if 
 else 
 what the string q 
 where an empty string 
 relation an empty string 
 end if 
 end for 
 return what relation where 
therefore appropriately handled finally if the geographical 
reference in the query is complemented with an expression similar to 
and its surroundings the spatial relation which is assumed to be 
contained-at if none is provided is changed to near 
 from search results to query locality 
the procedures given so far are appropriate for handling queries 
where a place reference is explicitly mentioned however the fact 
that a query can be associated with a geographical context may 
not be directly observable in the query itself but rather from the 
results returned for instance queries like recommended hotels 
for sigir or seafair lodging can be seen to refer to 
the city of seattle although they do not contain an explicit place 
reference we expect results to be about hotels in seattle 
in the cases where a query does not contain place references we 
start by assuming that the top results from a search engine represent 
the most popular and correct context and usage for the query we 
topic what relation where tgn concepts ml concepts 
vegetable exporters of europe vegetable exporters contained-at europe 
trade unions in europe trade unions contained-at europe 
roman cities in the uk and germany roman cities contained-at uk and germany 
cathedrals in europe cathedrals contained-at europe 
car bombings near madrid car bombings near madrid 
volcanos around quito volcanos near quito 
cities within km of frankfurt cities near frankfurt 
russian troops in south ern caucasus russian troops in south ern contained-at caucasus 
cities near active volcanoes this topic could not be appropriately handled - the relation and where terms are returned empty 
japanese rice imports this topic could not be appropriately handled - the relation and where terms are returned empty 
table example topics from the geoclef evaluation campaigns and the corresponding what relation where triples 
then propose to use the distributional characteristics of 
geographical scopes previously assigned to the documents corresponding to 
these top results in a previous work we presented a text mining 
approach for assigning documents with corresponding 
geographical scopes defined at an ontology that worked as an offline 
preprocessing stage in a gir system this pre-processing step is a 
fundamental stage of gir and it is reasonable to assume that this 
kind of information would be available on any system similarly to 
wang et al we could also attempt to process the results on-line 
in order to detect place references in the documents however 
a gir system already requires the offline stage 
for the top n documents given at the results we check the 
geographic scopes that were assigned to them if a significant portion 
of the results are assigned to the same scope than the query can be 
seen to be related to the corresponding geographic concept this 
assumption could even be relaxed for instance by checking if the 
documents belong to scopes that are hierarchically related 
 evaluation experiments 
we used three different ontologies in evaluation experiments 
namely the getty thesaurus of geographic names tgn and 
two specific resources developed at our group here referred to as 
the pt and ml ontologies tgn and ml include global 
geographical information in multiple languages although tgn is 
considerably larger while the pt ontology focuses on the portuguese 
territory with a high detail place types are also different across 
ontologies as for instance pt includes street names and postal 
addresses whereas ml only goes to the level of cities the reader 
should refer to for a complete description of these resources 
our initial experiments used portuguese and english topics from 
the geoclef and evaluation campaigns topics in 
geoclef correspond to query strings that can be used as input to a gir 
system imageclef also included topics specifying place 
references and participants were encouraged to run their gir 
systems on them our experiments also considered this dataset for 
each topic we measured if algorithm was able to find the 
corresponding what relation where triple the ontologies used 
in this experiment were the tgn and ml as topics were given in 
multiple languages and covered the whole globe 
dataset number of correct triples time per query 
queries ml tgn ml tgn 
geoclef en 
geoclef pt 
geoclef en msec msec 
geoclef pt 
imgclef en 
table summary of results over clef topics 
table illustrates some of the topics and table summarizes 
the obtained results the tables show that the proposed technique 
adequately handles most of these queries a manual inspection of 
the ontology concepts that were returned for each case also revealed 
that the where term was being correctly disambiguated note that 
the tgn ontology indeed added some ambiguity as for instance 
names like madrid can correspond to many different places around 
the globe it should also be noted that some of the considered 
topics are very hard for an automated system to handle some of them 
were ambiguous e g in japanese rice imports the query can 
be said to refer either rice imports in japan or imports of japanese 
rice and others contained no direct geographical references e g 
cities near active volcanoes besides these very hard cases we 
also missed some topics due to their usage of place adjectives and 
specific regions that are not defined at the ontologies e g 
environmental concerns around the scottish trossachs 
in a second experiment we used a sample of around 
real search engine queries the objective was to see if a 
significant number of these queries were geographical in nature also 
checking if the algorithm did not produce many mistakes by 
classifying a query as geographical when that was not the case the 
portuguese ontology was used in this experiment and queries were 
taken from the logs of a portuguese web search engine available 
at www tumba pt table summarizes the obtained results many 
queries were indeed geographical around although 
previous studies reported values above a manual inspection 
showed that the algorithm did not produce many false positives 
and the geographical queries were indeed correctly split into correct 
 what relation where triple the few mistakes we encountered 
were related to place names that were more frequently used in other 
contexts e g in teófilo braga we have the problem that braga 
is a portuguese district and teófilo braga was a well known 
portuguese writer and politician the addition of more names to the 
exception list can provide a workaround for most of these cases 
value 
num queries 
num queries without geographical references 
num queries with geographical references 
table results from an experiment with search engine logs 
we also tested the procedure for detecting queries that are 
implicitly geographical with a small sample of queries from the logs 
for instance for the query estádio do dragão e g home stadium 
for a soccer team from porto the correct geographical context can 
be discovered from the analysis of the results more than of 
the top results are assigned with the scope porto for future 
work we plan on using a larger collection of queries to evaluate 
this aspect besides queries from the search engine logs we also 
plan on using the names of well-known buildings monuments and 
other landmarks as they have a strong geographical connotation 
finally we also made a comparative experiment with popular 
geocoders maporama and microsoft s mappoint the objective 
was to compare algorithm with other approaches in terms of 
being able to correctly disambiguate a string with a place reference 
civil parishes from lisbon maporama mappoint ours 
coded refs out of 
avg time per ref msec 
civil parishes from porto maporama mappoint ours 
coded refs out of 
avg time per ref msec 
table results from a comparison with geocoding services 
the portuguese ontology was used in this experiment taking as 
input the names of civil parishes from the portuguese municipalities 
of lisbon and porto and checking if the systems were able to 
disambiguate the full name e g campo grande lisboa or foz 
do douro porto into the correct geocode we specifically 
measured whether our approach was better at unambiguously returning 
geocodes given the place reference i e return the single correct 
code and providing results rapidly table shows the obtained 
results and the accuracy of our method seems comparable to the 
commercial geocoders note that for maporama and mappoint the 
times given at table include fetching results from the web but we 
have no direct way of accessing the geocoding algorithms in both 
cases fetching static content from the web servers takes around 
 milliseconds although our approach cannot unambiguously 
return the correct geocode in most cases only out of a total of 
 cases it nonetheless returns results that a human user can 
disambiguate e g for madalena lisboa we return both a street and 
a civil parish as opposed to the other systems that often did not 
produce results moreover if we consider the top geocode 
according to the ranking procedure described in section or if we use 
a type qualifier in the name e g civil parish of campo grande 
lisboa our algorithm always returns the correct geocode 
 conclusions 
this paper presented simple approaches for handling place 
references in search engine queries this is a hard text mining problem 
as queries are often ambiguous or underspecify information needs 
however our initial experiments indicate that for many queries the 
referenced places can be determined effectively unlike the 
techniques proposed by wang et al we mainly focused on 
recognizing spatial relations and associating place names to ontology 
concepts the proposed techniques were employed in the prototype 
system that we used for participating in geoclef in queries 
where a geographical reference is not explicitly mentioned we 
propose to use the results for the query exploiting geographic scopes 
previously assigned to these documents in the future we plan on 
doing a careful evaluation of this last approach another idea that 
we would like to test involves the integration of a spelling 
correction mechanism into algorithm so that incorrectly spelled 
place references can be matched to ontology concepts 
the proposed techniques for handling geographic queries can 
have many applications in improving gir systems or even general 
purpose search engines after place references are appropriately 
disambiguated into ontology concepts a gir system can use them 
to retrieve relevant results through the use of appropriate index 
structures e g indexing the spatial coordinates associated with 
ontology concepts and provided that the documents are also assigned 
to scopes corresponding to ontology concepts a different gir 
strategy can involve query expansion by taking the where terms 
from the query and using the ontology to add names from 
neighboring locations in a general purpose search engine and if a local 
query is detected we can forward users to a gir system which 
should be better suited for properly handling the query the regular 
google search interface already does this by presenting a link to 
google local when it detects a geographical query 
 references 
 e amitay n har el r sivan and a soffer web-a-where 
geotagging web content in proceedings of sigir- the 
 th conference on research and development in information 
retrieval 
 m chaves m j silva and b martins a geographic 
knowledge base for semantic web applications in 
proceedings of sbbd- the th brazilian symposium on 
databases 
 n a chinchor overview of muc- met- in 
proceedings of muc- the th message understanding 
conference 
 f gey r larson m sanderson h joho and p clough 
geoclef the clef cross-language geographic 
information retrieval track in working notes for the clef 
 workshop 
 l gravano v hatzivassiloglou and r lichtenstein 
categorizing web queries according to geographical locality 
in proceedings of cikm- the th conference on 
information and knowledge management 
 p harpring proper words in proper places the thesaurus of 
geographic names mda information 
 c jones r purves a ruas m sanderson m sester 
m van kreveld and r weibel spatial information retrieval 
and geographical ontologies an overview of the spirit 
project in proceedings of sigir- the th conference on 
research and development in information retrieval 
 j kohler analyzing search engine queries for the use of 
geographic terms msc thesis 
 a kornai and b sundheim editors proceedings of the 
naacl-hlt workshop on the analysis of geographic 
references 
 y li z zheng and h dai kdd cup- report facing 
a great challenge sigkdd explorations 
 d manov a kiryakov b popov k bontcheva 
d maynard and h cunningham experiments with 
geographic knowledge for information extraction in 
proceedings of the naacl-hlt workshop on the analysis of 
geographic references 
 b martins and m j silva spelling correction for search 
engine queries in proceedings of estal- españa for 
natural language processing 
 b martins and m j silva a graph-ranking algorithm for 
geo-referencing documents in proceedings of icdm- the 
 th ieee international conference on data mining 
 l souza c j davis k borges t delboni and 
a laender the role of gazetteers in geographic knowledge 
discovery on the web in proceedings of la-web- the rd 
latin american web congress 
 e tjong k sang and f d meulder introduction to the 
conll- shared task language-independent named 
entity recognition in proceedings of conll- the th 
conference on natural language learning 
 d vogel s bickel p haider r schimpfky p siemen 
s bridges and t scheffer classifying search engine 
queries using the web as background knowledge sigkdd 
explorations newsletter - 
 l wang c wang x xie j forman y lu w -y ma and 
y li detecting dominant locations from search queries in 
proceedings of sigir- the th conference on research 
and development in information retrieval 
machine learning for information architecture in a large 
governmental website ∗ 
miles efron 
school of information library science 
cb manning hall 
university of north carolina 
chapel hill nc - 
efrom ils unc edu 
jonathan elsas 
school of information library science 
cb manning hall 
university of north carolina 
chapel hill nc - 
jelsas email unc edu 
gary marchionini 
school of information library science 
cb manning hall 
university of north carolina 
chapel hill nc - 
march ils unc edu 
junliang zhang 
school of information library science 
cb manning hall 
university of north carolina 
chapel hill nc - 
junliang email unc edu 
abstract 
this paper describes ongoing research into the application 
of machine learning techniques for improving access to 
governmental information in complex digital libraries under 
the auspices of the govstat project our goal is to identify a 
small number of semantically valid concepts that adequately 
spans the intellectual domain of a collection the goal of this 
discovery is twofold first we desire a practical aid for 
information architects second automatically derived 
documentconcept relationships are a necessary precondition for 
realworld deployment of many dynamic interfaces the current 
study compares concept learning strategies based on three 
document representations keywords titles and full-text in 
statistical and user-based studies human-created keywords 
provide significant improvements in concept learning over 
both title-only and full-text representations 
categories and subject descriptors 
h information storage and retrieval digital 
libraries-systems issues user issues h information 
storage and retrieval information search and 
retrievalclustering 
general terms 
design experimentation 
 introduction 
the govstat project is a joint effort of the university 
of north carolina interaction design lab and the 
university of maryland human-computer interaction lab 
 
citing end-user difficulty in finding governmental information 
 especially statistical data online the project seeks to 
create an integrated model of user access to us government 
statistical information that is rooted in realistic data 
models and innovative user interfaces to enable such models 
and interfaces we propose a data-driven approach based 
on data mining and machine learning techniques in 
particular our work analyzes a particular digital library-the 
website of the bureau of labor statistics 
 bls -in efforts 
to discover a small number of linguistically meaningful 
concepts or bins that collectively summarize the semantic 
domain of the site 
the project goal is to classify the site s web content 
according to these inferred concepts as an initial step towards 
data filtering via active user interfaces cf many 
digital libraries already make use of content classification 
both explicitly and implicitly they divide their resources 
manually by topical relation they organize content into 
hierarchically oriented file systems the goal of the present 
 
http www ils unc edu govstat 
 
http www bls gov 
 
research is to develop another means of browsing the content 
of these collections by analyzing the distribution of terms 
across documents our goal is to supplement the agency s 
pre-existing information structures statistical learning 
technologies are appealing in this context insofar as they stand 
to define a data-driven-as opposed to an 
agency-drivennavigational structure for a site 
our approach combines supervised and unsupervised 
learning techniques a pure document clustering approach 
to such a large diverse collection as bls led to poor results 
in early tests but strictly supervised techniques are 
inappropriate too although bls designers have defined 
high-level subject headings for their collections as we 
discuss in section this scheme is less than optimal thus we 
hope to learn an additional set of concepts by letting the 
data speak for themselves 
the remainder of this paper describes the details of our 
concept discovery efforts and subsequent evaluation in 
section we describe the previously existing human-created 
conceptual structure of the bls website this section also 
describes evidence that this structure leaves room for 
improvement next sections - we turn to a description 
of the concepts derived via content clustering under three 
document representations keyword title only and full-text 
section describes a two-part evaluation of the derived 
conceptual structures finally we conclude in section by 
outlining upcoming work on the project 
 structuring access to the bls 
website 
the bureau of labor statistics is a federal government 
agency charged with compiling and publishing statistics 
pertaining to labor and production in the us and abroad given 
this broad mandate the bls publishes a wide array of 
information intended for diverse audiences the agency s 
website acts as a clearinghouse for this process with over 
 text html documents and many more documents if 
spreadsheets and typeset reports are included providing 
access to the collection provides a steep challenge to 
information architects 
 the relation browser 
the starting point of this work is the notion that access 
to information in the bls website could be improved by 
the addition of a dynamic interface such as the relation 
browser described by marchionini and brunk the 
relation browser allows users to traverse complex data sets by 
iteratively slicing the data along several topics in figure 
 we see a prototype instantiation of the relation browser 
applied to the fedstats website 
 
the relation browser supports information seeking by 
allowing users to form queries in a stepwise fashion slicing and 
re-slicing the data as their interests dictate its motivation 
is in keeping with shneiderman s suggestion that queries 
and their results should be tightly coupled thus in 
fig 
http www fedstats gov 
figure relation browser prototype 
ure users might limit their search set to those documents 
about energy within this subset of the collection they 
might further eliminate documents published more than a 
year ago finally they might request to see only documents 
published in pdf format 
as marchionini and brunk discuss capturing the 
publication date and format of documents is trivial but successful 
implementations of the relation browser also rely on topical 
classification this presents two stumbling blocks for system 
designers 
 information architects must define the appropriate set 
of topics for their collection 
 site maintainers must classify each document into its 
appropriate categories 
these tasks parallel common problems in the metadata 
community defining appropriate elements and marking up 
documents to support metadata-aware information access 
given a collection of over documents these 
hurdles are especially daunting and automatic methods of 
approaching them are highly desirable 
 a pre-existing structure 
prior to our involvement with the project designers at 
bls created a shallow classificatory structure for the most 
important documents in their website as seen in figure 
the bls home page organizes top-level documents into 
 categories these include topics such as employment and 
unemployment productivity and inflation and spending 
 
figure the bls home page 
we hoped initially that these pre-defined categories could 
be used to train a -way document classifier thus 
automating the process of populating the relation browser altogether 
however this approach proved unsatisfactory in personal 
meetings bls officials voiced dissatisfaction with the 
existing topics their form it was argued owed as much to 
the institutional structure of bls as it did to the inherent 
topology of the website s information space in other words 
the topics reflected official divisions rather than semantic 
clusters the bls agents suggested that re-designing this 
classification structure would be desirable 
the agents misgivings were borne out in subsequent 
analysis the bls topics comprise a shallow classificatory 
structure each of the top-level categories is linked to a small 
number of related pages thus there are pages associated 
with inflation altogether the link structure of this 
classificatory system contains documents that is excluding 
navigational links there are documents linked from the 
bls home page where each hyperlink connects a document 
to a topic pages can be linked to multiple topics based on 
this hyperlink structure we defined m a symmetric × 
matrix where mij counts the number of topics in which 
documents i and j are both classified on the bls home page to 
analyze the redundancy inherent in the pre-existing 
structure we derived the principal components of m cf 
figure shows the resultant scree plot 
 
because all documents belong to at least one bls topic 
 
a scree plot shows the magnitude of the kth 
eigenvalue 
versus its rank during principal component analysis scree 
plots visualize the amount of variance captured by each 
component 
m m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
m m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
eigenvalue rankmeigenvalue rankm 
eigenvalue rank 
eigenvlue magnitudemeigenvlue magnitudem 
eigenvluemagnitude 
figure scree plot of bls categories 
the rank of m is guaranteed to be less than or equal to 
 hence eigenvalues what is surprising 
about figure however is the precipitous decline in 
magnitude among the first four eigenvalues the four largest 
eigenvlaues account for of the total variance in the 
data this fact suggests a high degree of redundancy among 
the topics topical redundancy is not in itself problematic 
however the documents in this very shallow classificatory 
structure are almost all gateways to more specific 
information thus the listing of the producer price index under 
three categories could be confusing to the site s users in 
light of this potential for confusion and the agency s own 
request for redesign we undertook the task of topic discovery 
described in the following sections 
 a hybrid approach to topic 
discovery 
to aid in the discovery of a new set of high-level topics for 
the bls website we turned to unsupervised machine 
learning methods in efforts to let the data speak for themselves 
we desired a means of concept discovery that would be based 
not on the structure of the agency but on the content of the 
material to begin this process we crawled the bls 
website downloading all documents of mime type text html 
this led to a corpus of documents based on this 
corpus we hoped to derive k ≈ topical categories such 
that each document di is assigned to one or more classes 
 
document clustering cf provided an obvious but 
only partial solution to the problem of automating this type 
of high-level information architecture discovery the 
problems with standard clustering are threefold 
 mutually exclusive clusters are inappropriate for 
identifying the topical content of documents since 
documents may be about many subjects 
 due to the heterogeneity of the data housed in the 
bls collection tables lists surveys etc many 
documents terms provide noisy topical information 
 for application to the relation browser we require a 
small number k ≈ of topics without significant 
data reduction term-based clustering tends to deliver 
clusters at too fine a level of granularity 
in light of these problems we take a hybrid approach to 
topic discovery first we limit the clustering process to 
a sample of the entire collection described in section 
working on a focused subset of the data helps to overcome 
problems two and three listed above to address the 
problem of mutual exclusivity we combine unsupervised with 
supervised learning methods as described in section 
 focusing on content-rich 
documents 
to derive empirically evidenced topics we initially turned 
to cluster analysis let a be the n×p data matrix with n 
observations in p variables thus aij shows the measurement 
for the ith 
observation on the jth 
variable as described 
in the goal of cluster analysis is to assign each of the 
n observations to one of a small number k groups each of 
which is characterized by high intra-cluster correlation and 
low inter-cluster correlation though the algorithms for 
accomplishing such an arrangement are legion our analysis 
focuses on k-means clustering 
 during which each 
observation oi is assigned to the cluster ck whose centroid is closest 
to it in terms of euclidean distance readers interested in 
the details of the algorithm are referred to for a 
thorough treatment of the subject 
clustering by k-means is well-studied in the statistical 
literature and has shown good results for text analysis cf 
 however k-means clustering requires that the 
researcher specify k the number of clusters to define when 
applying k-means to our document collection 
indicators such as the gap statistic and an analysis of 
the mean-squared distance across values of k suggested that 
k ≈ was optimal this paramterization led to 
semantically intelligible clusters however clusters are far too 
many for application to an interface such as the relation 
 
we have focused on k-means as opposed to other clustering 
algorithms for several reasons chief among these is the 
computational efficiency enjoyed by the k-means approach 
because we need only a flat clustering there is little to be 
gained by the more expensive hierarchical algorithms in 
future work we will turn to model-based clustering as a 
more principled method of selecting the number of clusters 
and of representing clusters 
browser moreover the granularity of these clusters was 
unsuitably fine for instance the -cluster solution derived 
a cluster whose most highly associated words in terms of 
log-odds ratio were drug pharmacy and chemist these 
words are certainly related but they are related at a level 
of specificity far below what we sought 
to remedy the high dimensionality of the data we 
resolved to limit the algorithm to a subset of the collection 
in consultation with employees of the bls we continued 
our analysis on documents that form a series titled from 
the editor s desk 
 these are brief articles written by bls 
employees bls agents suggested that we focus on the 
editor s desk because it is intended to span the intellectual 
domain of the agency the column is published daily and 
each entry describes an important current issue in the bls 
domain the editor s desk column has been written daily 
 five times per week since as such we operated on a 
set of n documents 
limiting attention to these documents not only 
reduced the dimensionality of the problem it also allowed 
the clustering process to learn on a relatively clean data set 
while the entire bls collection contains a great deal of 
nonprose text i e tables lists etc the editor s desk 
documents are all written in clear journalistic prose each 
document is highly topical further aiding the discovery of 
termtopic relations finally the editor s desk column provided 
an ideal learning environment because it is well-supplied 
with topical metadata each of the documents 
contains a list of one or more keywords additionally a subset 
of the documents contained a subject heading this 
metadata informed our learning and evaluation as described 
in section 
 combining supervised and 
unsupervised learning fortopic 
discovery 
to derive suitably general topics for the application of a 
dynamic interface to the bls collection we combined 
document clustering with text classification techniques 
specifically using k-means we clustered each of the 
documents into one of k clusters with the number of clusters 
chosen by analyzing the within-cluster mean squared 
distance at different values of k see section 
constructing mutually exclusive clusters violates our assumption that 
documents may belong to multiple classes however these 
clusters mark only the first step in a two-phase process of 
topic identification at the end of the process 
documentcluster affinity is measured by a real-valued number 
once the editor s desk documents were assigned to 
clusters we constructed a k-way classifier that estimates the 
strength of evidence that a new document di is a member 
of class ck we tested three statistical classification 
techniques probabilistic rocchio prind naive bayes and 
support vector machines svms all were implemented using 
mccallum s bow text classification library prind is a 
probabilistic version of the rocchio classification algorithm 
 interested readers are referred to joachims article for 
 
http www bls gov opub ted 
 
further details of the classification method like prind naive 
bayes attempts to classify documents into the most 
probable class it is described in detail in finally support 
vector machines were thoroughly explicated by vapnik 
and applied specifically to text in they define a 
decision boundary by finding the maximally separating 
hyperplane in a high-dimensional vector space in which document 
classes become linearly separable 
having clustered the documents and trained a suitable 
classifier the remaining documents in the collection 
are labeled by means of automatic classification that is for 
each document di we derive a k-dimensional vector 
quantifying the association between di and each class c ck 
deriving topic scores via naive bayes for the entire 
 document collection required less than two hours of cpu 
time the output of this process is a score for every 
document in the collection on each of the automatically 
discovered topics these scores may then be used to populate a 
relation browser interface or they may be added to a 
traditional information retrieval system to use these weights in 
the relation browser we currently assign to each document 
the two topics on which it scored highest in future work we 
will adopt a more rigorous method of deriving 
documenttopic weight thresholds also evaluation of the utility of 
the learned topics for users will be undertaken 
 evaluation of concept 
discovery 
prior to implementing a relation browser interface and 
undertaking the attendant user studies it is of course 
important to evaluate the quality of the inferred concepts and 
the ability of the automatic classifier to assign documents 
to the appropriate subjects to evaluate the success of the 
two-stage approach described in section we undertook 
two experiments during the first experiment we compared 
three methods of document representation for the 
clustering task the goal here was to compare the quality of 
document clusters derived by analysis of full-text documents 
documents represented only by their titles and documents 
represented by human-created keyword metadata during 
the second experiment we analyzed the ability of the 
statistical classifiers to discern the subject matter of documents 
from portions of the database in addition to the editor s 
desk 
 comparing document representations 
documents from the editor s desk column came 
supplied with human-generated keyword metadata 
additionally the titles of the editor s desk documents tend to be 
germane to the topic of their respective articles with such 
an array of distilled evidence of each document s subject 
matter we undertook a comparison of document 
representations for topic discovery by clustering we hypothesized 
that keyword-based clustering would provide a useful model 
but we hoped to see whether comparable performance could 
be attained by methods that did not require extensive 
human indexing such as the title-only or full-text 
representations to test this hypothesis we defined three modes of 
document representation-full-text title-only and keyword 
only-we generated three sets of topics tfull ttitle and 
tkw respectively 
topics based on full-text documents were derived by 
application of k-means clustering to the editor s desk 
documents where each document was represented by a 
 dimensional vector these dimensions captured the 
tf idf weights of each term ti in document dj for all 
terms that occurred at least three times in the data to 
arrive at the appropriate number of clusters for these data we 
inspected the within-cluster mean-squared distance for each 
value of k as k approached the reduction in 
error with the addition of more clusters declined notably 
suggesting that k ≈ would yield good divisions to 
select a single integer value we calculated which value of k led 
to the least variation in cluster size this metric stemmed 
from a desire to suppress the common result where one large 
cluster emerges from the k-means algorithm accompanied 
by several accordingly small clusters without reason to 
believe that any single topic should have dramatically high 
prior odds of document membership this heuristic led to 
kfull 
clusters based on document titles were constructed 
similarly however in this case each document was represented 
in the vector space spanned by the terms that occur 
at least twice in document titles using the same method 
of minimizing the variance in cluster membership ktitle-the 
number of clusters in the title-based representation-was also 
set to 
the dimensionality of the keyword-based clustering was 
very similar to that of the title-based approach there were 
 keywords in the data all of which were retained the 
median number of keywords per document was where a 
keyword is understood to be either a single word or a 
multiword term such as consumer price index it is worth noting 
that the keywords were not drawn from any controlled 
vocabulary they were assigned to documents by publishers at 
the bls using the keywords the documents were clustered 
into classes 
to evaluate the clusters derived by each method of 
document representation we used the subject headings that were 
included with of the editor s desk documents each 
of these documents was assigned one or more subject 
headings which were withheld from all of the cluster 
applications like the keywords subject headings were assigned 
to documents by bls publishers unlike the keywords 
however subject headings were drawn from a controlled 
vocabulary our analysis began with the assumption that 
documents with the same subject headings should cluster 
together to facilitate this analysis we took a conservative 
approach we considered multi-subject classifications to be 
unique thus if document di was assigned to a single 
subject prices while document dj was assigned to two subjects 
international comparisons prices documents di and dj are 
not considered to come from the same class 
table shows all editor s desk subject headings that were 
assigned to at least documents as noted in the table 
 
table top editor s desk subject headings 
subject count 
prices 
unemployment 
occupational safety health 
international comparisons prices 
manufacturing prices 
employment 
productivity 
consumer expenditures 
earnings wages 
employment unemployment 
compensation costs 
earnings wages metro areas 
benefits compensation costs 
earnings wages occupations 
employment occupations 
benefits 
earnings wage regions 
work stoppages 
earnings wages industries 
total 
table contingecy table for three document 
representations 
representation right wrong accuracy 
full-text 
title 
keyword 
there were such subject headings which altogether 
covered of the documents with subjects assigned 
these document-subject pairings formed the basis of our 
analysis limiting analysis to subjects with n kept 
the resultant χ 
tests suitably robust 
the clustering derived by each document representation 
was tested by its ability to collocate documents with the 
same subjects thus for each of the subject headings 
in table si we calculated the proportion of documents 
assigned to si that each clustering co-classified further 
we assumed that whichever cluster captured the majority of 
documents for a given class constituted the right answer 
for that class for instance there were documents whose 
subject heading was prices taking the bls editors 
classifications as ground truth all of these documents should 
have ended up in the same cluster under the full-text 
representation of these documents were clustered into category 
 while were in category and documents were in 
category taking the majority cluster as the putative right 
home for these documents we consider the accuracy of this 
clustering on this subject to be repeating 
this process for each topic across all three representations 
led to the contingency table shown in table 
the obvious superiority of the keyword-based clustering 
evidenced by table was borne out by a χ 
test on the 
accuracy proportions comparing the proportion right and 
table keyword-based clusters 
benefits costs international jobs 
plans compensation import employment 
benefits costs prices jobs 
employees benefits petroleum youth 
occupations prices productivity safety 
workers prices productivity safety 
earnings index output health 
operators inflation nonfarm occupational 
spending unemployment 
expenditures unemployment 
consumer mass 
spending jobless 
wrong achieved by keyword and title-based clustering led to 
p due to this result in the remainder of this paper 
we focus our attention on the clusters derived by analysis of 
the editor s desk keywords the ten keyword-based clusters 
are shown in table represented by the three terms most 
highly associated with each cluster in terms of the log-odds 
ratio additionally each cluster has been given a label by 
the researchers 
evaluating the results of clustering is notoriously difficult 
in order to lend our analysis suitable rigor and utility we 
made several simplifying assumptions most problematic is 
the fact that we have assumed that each document belongs 
in only a single category this assumption is certainly false 
however by taking an extremely rigid view of what 
constitutes a subject-that is by taking a fully qualified and 
often multipart subject heading as our unit of analysis-we 
mitigate this problem analogically this is akin to 
considering the location of books on a library shelf although a 
given book may cover many subjects a classification system 
should be able to collocate books that are extremely similar 
say books about occupational safety and health the most 
serious liability with this evaluation then is the fact that 
we have compressed multiple subject headings say prices 
international into single subjects this flattening obscures 
the multivalence of documents we turn to a more realistic 
assessment of document-class relations in section 
 accuracy of the document classifiers 
although the keyword-based clusters appear to classify 
the editor s desk documents very well their discovery only 
solved half of the problem required for the successful 
implementation of a dynamic user interface such as the 
relation browser the matter of roughly fourteen thousand 
unclassified documents remained to be addressed to solve 
this problem we trained the statistical classifiers described 
above in section for each document in the collection 
di these classifiers give pi a k-vector of probabilities or 
distances depending on the classification method used where 
pik quantifies the strength of association between the ith 
document and the kth 
class all classifiers were trained on 
the full text of each document regardless of the 
representation used to discover the initial clusters the different 
training sets were thus constructed simply by changing the 
 
table cross validation results for classifiers 
method av percent accuracy se 
prind 
naive bayes 
svm 
class variable for each instance document to reflect its 
assigned cluster under a given model 
to test the ability of each classifier to locate documents 
correctly we first performed a -fold cross validation on 
the editor s desk documents during cross-validation the 
data are split randomly into n subsets in this case n 
the process proceeds by iteratively holding out each of the 
n subsets as a test collection for a model trained on the 
remaining n − subsets cross validation is described in 
 using this methodology we compared the performance 
of the three classification models described above table 
gives the results from cross validation 
although naive bayes is not significantly more accurate 
for these data than the svm classifier we limit the 
remainder of our attention to analysis of its performance our 
selection of naive bayes is due to the fact that it appears to 
work comparably to the svm approach for these data while 
being much simpler both in theory and implementation 
because we have only documents and classes the 
number of training documents per class is relatively small 
in addition to models fitted to the editor s desk data then 
we constructed a fourth model supplementing the training 
sets of each class by querying the google search engine 
and 
applying naive bayes to the augmented test set for each 
class we created a query by submitting the three terms 
with the highest log-odds ratio with that class further 
each query was limited to the domain www bls gov for 
each class we retrieved up to documents from google 
 the actual number varied depending on the size of the 
result set returned by google this led to a training set 
of documents in the augmented model as we call 
it below 
 cross validation suggested that the augmented 
model decreased classification accuracy accuracy 
with standard error as we discuss below however 
augmenting the training set appeared to help generalization 
during our second experiment 
the results of our cross validation experiment are 
encouraging however the success of our classifiers on the editor s 
desk documents that informed the cross validation study 
may not be good predictors of the models performance on 
the remainder to the bls website to test the generality 
of the naive bayes classifier we solicited input from 
human judges who were familiar with the bls website the 
sample was chosen by convenience and consisted of faculty 
and graduate students who work on the govstat project 
however none of the reviewers had prior knowledge of the 
outcome of the classification before their participation for 
the experiment a random sample of documents was 
drawn from the entire bls collection on average each 
re 
http www google com 
 
a more formal treatment of the combination of labeled and 
unlabeled data is available in 
table human-model agreement on sample 
docs 
human judge st 
choice 
model model st 
choice model nd 
choice 
n bayes aug 
n bayes 
human judge nd 
choice 
model model st 
choice model nd 
choice 
n bayes aug 
n bayes 
viewer classified documents placing each document into 
as many of the categories shown in table as he or she saw 
fit 
results from this experiment suggest that room for 
improvement remains with respect to generalizing to the whole 
collection from the class models fitted to the editor s desk 
documents in table we see for each classifier the 
number of documents for which it s first or second most probable 
class was voted best or second best by the human judges 
in the context of this experiment we consider a first- or 
second-place classification by the machine to be accurate 
because the relation browser interface operates on a 
multiway classification where each document is classified into 
multiple categories thus a document with the correct 
class as its second choice would still be easily available to 
a user likewise a correct classification on either the most 
popular or second most popular category among the human 
judges is considered correct in cases where a given document 
was classified into multiple classes there were 
multiclass documents in our sample as seen in figure the 
remaining documents were assigned to or classes 
under this rationale the augmented naive bayes 
classifier correctly grouped documents while the smaller model 
 not augmented by a google search correctly classified 
the resultant χ 
test gave p suggesting that 
increasing the training set improved the ability of the naive 
bayes model to generalize from the editor s desk documents 
to the collection as a whole however the improvement 
afforded by the augmented model comes at some cost in 
particular the augmented model is significantly inferior to the 
model trained solely on editor s desk documents if we 
concern ourselves only with documents selected by the majority 
of human reviewers-i e only first-choice classes limiting 
the right answers to the left column of table gives p 
in favor of the non-augmented model for the purposes of 
applying the relation browser to complex digital library 
content where documents will be classified along multiple 
categories the augmented model is preferable but this is not 
necessarily the case in general 
it must also be said that accuracy under a fairly 
liberal test condition leaves room for improvement in our 
assignment of topics to categories we may begin to 
understand the shortcomings of the described techniques by 
consulting figure which shows the distribution of 
categories across documents given by humans and by the 
augmented naive bayes model the majority of reviewers put 
 
number of human-assigned classesmnumber of human-assigned classesm 
number of human-assigned classes 
frequencymfrequencym 
frequency 
m m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
 m m 
 
m m m m m m m m m m m m m m m m m 
figure number of classes assigned to 
documents by judges 
documents into only three categories jobs benefits and 
occupations on the other hand the naive bayes classifier 
distributed classes more evenly across the topics this behavior 
suggests areas for future improvement most importantly 
we observed a strong correlation among the three most 
frequent classes among the human judges for instance there 
was correlation between benefits and occupations this 
suggests that improving the clustering to produce topics 
that were more nearly orthogonal might improve 
performance 
 conclusions and future work 
many developers and maintainers of digital libraries share 
the basic problem pursued here given increasingly large 
complex bodies of data how may we improve access to 
collections without incurring extraordinary cost and while also 
keeping systems receptive to changes in content over time 
data mining and machine learning methods hold a great deal 
of promise with respect to this problem empirical 
methods of knowledge discovery can aid in the organization and 
retrieval of information as we have argued in this paper 
these methods may also be brought to bear on the design 
and implementation of advanced user interfaces 
this study explored a hybrid technique for aiding 
information architects as they implement dynamic interfaces such 
as the relation browser our approach combines 
unsupervised learning techniques applied to a focused subset of the 
bls website the goal of this initial stage is to discover the 
most basic and far-reaching topics in the collection based 
mjobsjobsmjobsm 
jobs 
benefitsunemploymentpricespricesmpricesm 
prices 
safetyinternationalspendingspendingmspendingm 
spending 
occupationscostscostsmcostsm 
costs 
productivityhuman classificationsmhuman classificationsm 
human classifications 
m m m 
 
 m m 
 
 mjobsjobsmjobsm 
jobs 
benefitsunemploymentpricespricesmpricesm 
prices 
safetyinternationalspendingspendingmspendingm 
spending 
occupationscostscostsmcostsm 
costs 
productivitymachine classificationsmmachine classificationsm 
machine classifications 
m m m 
 
 m m 
 
 
figure distribution of classes across documents 
on a statistical model of these topics the second phase of 
our approach uses supervised learning in particular a naive 
bayes classifier trained on individual words to assign 
topical relations to the remaining documents in the collection 
in the study reported here this approach has 
demonstrated promise in its favor our approach is highly scalable 
it also appears to give fairly good results comparing three 
modes of document representation-full-text title only and 
keyword-we found accuracy as measured by 
collocation of documents with identical subject headings while it 
is not surprising that editor-generated keywords should give 
strong evidence for such learning their superiority over 
fulltext and titles was dramatic suggesting that even a small 
amount of metadata can be very useful for data mining 
however we also found evidence that learning topics from 
a subset of the collection may lead to overfitted models 
after clustering editor s desk documents into 
categories we fitted a -way naive bayes classifier to categorize 
the remaining documents in the collection while we 
saw fairly good results classification accuracy of with 
respect to a small sample of human judges this experiment 
forced us to reconsider the quality of the topics learned by 
clustering the high correlation among human judgments 
in our sample suggests that the topics discovered by 
analysis of the editor s desk were not independent while we do 
not desire mutually exclusive categories in our setting we 
do desire independence among the topics we model 
overall then the techniques described here provide an 
encouraging start to our work on acquiring subject 
metadata for dynamic interfaces automatically it also suggests 
that a more sophisticated modeling approach might yield 
 
better results in the future in upcoming work we will 
experiment with streamlining the two-phase technique described 
here instead of clustering documents to find topics and 
then fitting a model to the learned clusters our goal is to 
expand the unsupervised portion of our analysis beyond a 
narrow subset of the collection such as the editor s desk 
in current work we have defined algorithms to identify 
documents likely to help the topic discovery task supplied with 
a more comprehensive training set we hope to experiment 
with model-based clustering which combines the clustering 
and classification processes into a single modeling procedure 
topic discovery and document classification have long been 
recognized as fundamental problems in information retrieval 
and other forms of text mining what is increasingly clear 
however as digital libraries grow in scope and complexity 
is the applicability of these techniques to problems at the 
front-end of systems such as information architecture and 
interface design finally then in future work we will build 
on the user studies undertaken by marchionini and brunk 
in efforts to evaluate the utility of automatically populated 
dynamic interfaces for the users of digital libraries 
 references 
 a agresti an introduction to categorical data 
analysis wiley new york 
 c ahlberg c williamson and b shneiderman 
dynamic queries for information exploration an 
implementation and evaluation in proceedings of the 
sigchi conference on human factors in computing 
systems pages - 
 r baeza-yates and b ribeiro-neto modern 
information retrieval acm press 
 a blum and t mitchell combining labeled and 
unlabeled data with co-training in proceedings of the 
eleventh annual conference on computational learning 
theory pages - acm press 
 h chen and s dumais hierarchical classification of 
web content in proceedings of the rd annual 
international acm sigir conference on research and 
development in information retrieval pages - 
 
 m efron g marchionini and j zhang implications 
of the recursive representation problem for automatic 
concept identification in on-line governmental 
information in proceedings of the asist special 
interest group on classification research asist 
sig-cr 
 c fraley and a e raftery how many clusters 
which clustering method answers via model-based 
cluster analysis the computer journal 
 - 
 a k jain m n murty and p j flynn data 
clustering a review acm computing surveys 
 - september 
 t joachims a probabilistic analysis of the rocchio 
algorithm with tfidf for text categorization in 
d h fisher editor proceedings of icml- th 
international conference on machine learning pages 
 - nashville us morgan kaufmann 
publishers san francisco us 
 t joachims text categorization with support vector 
machines learning with many relevant features in 
c n´edellec and c rouveirol editors proceedings of 
ecml- th european conference on machine 
learning pages - chemnitz de 
springer verlag heidelberg de 
 i t jolliffe principal component analysis springer 
 nd edition 
 l kaufman and p j rosseeuw finding groups in 
data an introduction to cluster analysis wiley 
 
 g marchionini and b brunk toward a general 
relation browser a gui for information architects 
journal of digital information 
http jodi ecs soton ac uk articles v i marchionini 
 a k mccallum bow a toolkit for statistical 
language modeling text retrieval classification and 
clustering http www cs cmu edu ˜mccallum bow 
 
 t mitchell machine learning mcgraw hill 
 e rasmussen clustering algorithms in w b frakes 
and r baeza-yates editors information retrieval 
data structures and algorithms pages - 
prentice hall 
 r tibshirani g walther and t hastie estimating 
the number of clusters in a dataset via the gap 
statistic 
http citeseer nj nec com tibshirani estimating html 
 v n vapnik the nature of statistical learning 
theory springer 
 
query performance prediction in web search 
environments 
yun zhou and w bruce croft 
department of computer science 
university of massachusetts amherst 
{yzhou croft} cs umass edu 
abstract 
current prediction techniques which are generally designed for 
content-based queries and are typically evaluated on relatively 
homogenous test collections of small sizes face serious 
challenges in web search environments where collections are 
significantly more heterogeneous and different types of retrieval 
tasks exist in this paper we present three techniques to address 
these challenges we focus on performance prediction for two 
types of queries in web search environments content-based and 
named-page finding our evaluation is mainly performed on the 
gov collection in addition to evaluating our models for the two 
types of queries separately we consider a more challenging and 
realistic situation that the two types of queries are mixed together 
without prior information on query types to assist prediction 
under the mixed-query situation a novel query classifier is 
adopted results show that our prediction of web query 
performance is substantially more accurate than the current 
stateof-the-art prediction techniques consequently our paper 
provides a practical approach to performance prediction in 
realworld web settings 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval -query formulation 
general terms 
algorithms experimentation theory 
 introduction 
query performance prediction has many applications in a variety 
of information retrieval ir areas such as improving retrieval 
consistency query refinement and distributed ir the importance 
of this problem has been recognized by ir researchers and a 
number of new methods have been proposed for prediction 
recently 
most work on prediction has focused on the traditional ad-hoc 
retrieval task where query performance is measured according to 
topical relevance these prediction models are evaluated on 
trec document collections which typically consist of no more 
than one million relatively homogenous newswire articles with 
the popularity and influence of the web prediction techniques 
that will work well for web-style queries are highly preferable 
however web search environments pose significant challenges to 
current prediction models that are mainly designed for traditional 
trec settings here we outline some of these challenges 
first web collections which are much larger than conventional 
trec collections include a variety of documents that are 
different in many aspects such as quality and style current 
prediction techniques can be vulnerable to these characteristics of 
web collections for example the reported prediction accuracy of 
the ranking robustness technique and the clarity technique on the 
gov collection a large web collection is significantly worse 
compared to the other trec collections similar prediction 
accuracy on the gov collection using another technique is 
reported in confirming the difficult of predicting query 
performance on a large web collection 
furthermore web search goes beyond the scope of the ad-hoc 
retrieval task based on topical relevance for example the 
named-page np finding task which is a navigational task is 
also popular in web retrieval query performance prediction for 
the np task is still necessary since np retrieval performance is far 
from perfect in fact according to the report on the np task of the 
 terabyte track about of the test queries perform 
poorly no correct answer in the first search results even in the 
best run from the top group to our knowledge little research has 
explicitly addressed the problem of np-query performance 
prediction current prediction models devised for content-based 
queries will be less effective for np queries considering the 
fundamental differences between the two 
third in real-world web search environments user queries are 
usually a mixture of different types and prior knowledge about the 
type of each query is generally unavailable the mixed-query 
situation raises new problems for query performance prediction 
for instance we may need to incorporate a query classifier into 
prediction models despite these problems the ability to handle 
this situation is a crucial step towards turning query performance 
prediction from an interesting research topic into a practical tool 
for web retrieval 
in this paper we present three techniques to address the above 
challenges that current prediction models face in web search 
environments our work focuses on query performance prediction 
for the content-based ad-hoc retrieval task and the name-page 
finding task in the context of web retrieval our first technique 
called weighted information gain wig makes use of both single 
term and term proximity features to estimate the quality of top 
retrieved documents for prediction we find that wig offers 
consistent prediction accuracy across various test collections and 
query types moreover we demonstrate that good prediction 
accuracy can be achieved for the mixed-query situation by using 
wig with the help of a query type classifier query feedback and 
first rank change which are our second and third prediction 
techniques perform well for content-based queries and np 
queries respectively 
our main contributions include considerably improved 
prediction accuracy for web content-based queries over several 
state-of-the-art techniques new techniques for successfully 
predicting np-query performance a practical and fully 
automatic solution to predicting mixed-query performance in 
addition one minor contribution is that we find that the 
robustness score which was originally proposed for 
performance prediction is helpful for query classification 
 related work 
as we mentioned in the introduction a number of prediction 
techniques have been proposed recently that focus on 
contentbased queries in the topical relevance ad-hoc task we know of 
no published work that addresses other types of queries such as 
np queries let alone a mixture of query types next we review 
some representative models 
the major difficulty of performance prediction comes from the 
fact that many factors have an impact on retrieval performance 
each factor affects performance to a different degree and the 
overall effect is hard to predict accurately therefore it is not 
surprising to notice that simple features such as the frequency of 
query terms in the collection and the average idf of query 
terms do not predict well in fact most of the successful 
techniques are based on measuring some characteristics of the 
retrieved document set to estimate topic difficulty for example 
the clarity score measures the coherence of a list of documents 
by the kl-divergence between the query model and the collection 
model the robustness score quantifies another property of a 
ranked list the robustness of the ranking in the presence of 
uncertainty carmel et al found that the distance measured by 
the jensen-shannon divergence between the retrieved document 
set and the collection is significantly correlated to average 
precision vinay et al proposed four measures to capture the 
geometry of the top retrieved documents for prediction the most 
effective measure is the sensitivity to document perturbation an 
idea somewhat similar to the robustness score unfortunately 
their way of measuring the sensitivity does not perform equally 
well for short queries and prediction accuracy drops considerably 
when a state-of-the-art retrieval technique like okapi or a 
language modeling approach is adopted for retrieval instead of 
the tf-idf weighting used in their paper 
the difficulties of applying these models in web search 
environments have already been mentioned in this paper we 
mainly adopt the clarity score and the robustness score as our 
baselines we experimentally show that the baselines even after 
being carefully tuned are inadequate for the web environment 
one of our prediction models wig is related to the markov 
random field mrf model for information retrieval the 
mrf model directly models term dependence and is found be to 
highly effective across a variety of test collections particularly 
web collections and retrieval tasks this model is used to 
estimate the joint probability distribution over documents and 
queries an important part of wig the superiority of wig over 
other prediction techniques based on unigram features which will 
be demonstrated later in our paper coincides with that of mrf 
for retrieval in other word it is interesting to note that term 
dependence when being modeled appropriately can be helpful 
for both improving and predicting retrieval performance 
 prediction models 
 weighted information gain wig 
this section introduces a weighted information gain approach that 
incorporates both single term and proximity features for 
predicting performance for both content-based and named-page 
 np finding queries 
given a set of queries q {qs} s n which includes all 
possible user queries and a set of documents d {dt} t  m 
we assume that each query-document pair qs dt is manually 
judged and will be put in a relevance list if qs is found to be 
relevant to dt the joint probability p qs dt over queries q and 
documents d denotes the probability that pair qs dt will be in 
the relevance list such assumptions are similar to those used in 
 assuming that the user issues query qi ∈q and the retrieval 
results in response to qi is a ranked list l of documents we 
calculate the amount of information contained in p qs dt with 
respect to qi and l by eq which is a variant of entropy called 
the weighted entropy the weights in eq are solely 
determined by qi and l 
 log 
 
 ∑− 
ts 
tststslq dqpdqweightdqh i 
in this paper we choose the weights as follows 
lindocumentsktopthecontainsltwhere 
otherwise 
ltdandisifk 
dqweight 
k 
kt 
ts 
 
 
 
 
 
⎩ 
⎨ 
⎧ ∈ 
 
the cutoff rank k is a parameter in our model that will be 
discussed later accordingly eq can be simplified as follows 
 log 
 
 
 
 ∑∈ 
− 
ltd 
titslq 
kt 
i 
dqp 
k 
dqh 
unfortunately weighted entropy tslq dqh i 
computed by eq 
which represents the amount of information about how likely the 
top ranked documents in l would be relevant to query qi on 
average cannot be compared across different queries making it 
inappropriate for directly predicting query performance to 
mitigate this problem we come up with a background distribution 
p qs c over q and d by imagining that every document in d is 
replaced by the same special document c which represents 
average language usage in this paper c is created by 
concatenating every document in d roughly speaking c is the 
collection the document set {dt} without document boundaries 
similarly weighted entropy cqh slqi 
calculated by eq 
represents the amount of information about how likely an average 
document represented by the whole collection would be relevant 
to query qi 
now we introduce our performance predictor wig which is the 
weighted information gain computed as the difference 
between tslq dqh i 
and cqh slqi 
 specifically given query 
qi collection c and ranked list l of documents wig is 
calculated as follows 
 
 
 
log 
 
 
 
log 
 
 
 
∑∑ ∈ 
 
− 
ltd i 
ti 
ts s 
ts 
ts 
tslqslqi 
kt 
ii 
cqp 
dqp 
kcqp 
dqp 
dqweight 
dqhcqhlcqwig 
wig computed by eq measures the change in information about 
the quality of retrieval in response to query qi from an 
imaginary state that only an average document is retrieved to a 
posterior state that the actual search results are observed we 
hypothesize that wig is positively correlated with retrieval 
effectiveness because high quality retrieval should be much more 
effective than just returning the average document 
the heart of this technique is how to estimate the joint 
distribution p qs dt in the language modeling approach to ir a 
variety of models can be applied readily to estimate this 
distribution although most of these models are based on the 
bagof-words assumption recent work on modeling term dependence 
under the language modeling framework have shown consistent 
and significant improvements in retrieval effectiveness over 
bagof-words models inspired by the success of incorporating term 
proximity features into language models we decide to adopt a 
good dependence model to estimate the probability p qs dt the 
model we chose for this paper is metzler and croft s markov 
random field mrf model which has already demonstrated 
superiority over a number of collections and different retrieval 
tasks 
according to the mrf model log p qi dt can be written as 
 loglog log 
 
 ∑∈ 
 − 
iqf 
tti dpzdqp 
ξ 
ξ ξλ 
where z is a constant that ensures that p qi dt sums up to 
f qi consists of a set of features expanded from the original 
query qi for example assuming that query qi is talented 
student program f qi includes features like program and 
talented student we consider two kinds of features single 
term features t and proximity features p proximity features 
include exact phrase and unordered window uwn features 
as described in note that f qi is the union of t qi and 
p qi for more details on f qi such as how to expand the 
original query qi to f qi we refer the reader to and 
p ξ dt denotes the probability that feature ξ will occur in dt 
more details on p ξ dt will be provided later in this section the 
choice of λξ is somewhat different from that used in since λξ 
plays a dual role in our model the first role which is the same as 
in is to weight between single term and proximity features 
the other role which is specific to our prediction task is to 
normalize the size of f qi we found that the following weight 
strategy for λξ satisfies the above two roles and generalizes well 
on a variety of collections and query types 
 
 
 
 
 
 
⎪ 
⎪ 
⎩ 
⎪ 
⎪ 
⎨ 
⎧ 
∈ 
− 
∈ 
 
i 
i 
t 
i 
i 
t 
qp 
qp 
qt 
qt 
ξ 
λ 
ξ 
λ 
λξ 
where t qi and p qi denote the number of single term and 
proximity features in f qi respectively the reason for choosing 
the square root function in the denominator of λξ is to penalize a 
feature set of large size appropriately making wig more 
comparable across queries of various lengths λt is a fixed 
parameter and set to according to throughout this paper 
similarly log p qi c can be written as 
 loglog log 
 
 ∑∈ 
 − 
iqf 
i cpzcqp 
ξ 
ξ ξλ 
when constant z and z are dropped wig computed in eq can 
be rewritten as follows by plugging in eq and eq 
 
 
 
log 
 
 
 
∑ ∑∈ ∈ 
 
ltd qf 
t 
i 
kt i 
cp 
dp 
k 
lcqwig 
ξ 
ξ 
ξ 
ξ 
λ 
one of the advantages of wig over other techniques is that it can 
handle well both content-based and np queries based on the type 
 or the predicted type of qi the calculation of wig in eq 
differs in two aspects how to estimate p ξ dt and p ξ c and 
 how to choose k 
for content-based queries p ξ c is estimated by the relative 
frequency of feature ξ in collection c as a whole the estimation 
of p ξ dt is the same as in namely we estimate p ξ dt by 
the relative frequency of feature ξ in dt linearly smoothed with 
collection frequency p ξ c k in eq is treated as a free 
parameter note that k is the only free parameter in the 
computation of wig for content-based queries because all 
parameters involved in p ξ dt are assumed to be fixed by taking 
the suggested values in 
regarding np queries we make use of document structure to 
estimate p ξ dt and p ξ c by the so-called mixture of language 
models proposed in and incorporated into the mrf model for 
named-page finding retrieval in the basic idea is that a 
document collection is divided into several fields such as the 
title field the main-body field and the heading field p ξ dt and 
p ξ c are estimated by a linear combination of the language 
models from each field due to space constraints we refer the 
reader to for details we adopt the exact same set of 
parameters as used in for estimation with regard to k in eq 
we set k to because the named-page finding task heavily 
focuses on the first ranked document consequently there are no 
free parameters in the computation of wig for np queries 
 query feedback 
in this section we introduce another technique called query 
feedback qf for prediction suppose that a user issues query q 
to a retrieval system and a ranked list l of documents is returned 
we view the retrieval system as a noisy channel specifically we 
assume that the output of the channel is l and the input is q 
after going through the channel q becomes corrupted and is 
transformed to ranked list l 
by thinking about the retrieval process this way the problem of 
predicting retrieval effectiveness turns to the task of evaluating 
the quality of the channel in other words prediction becomes 
finding a way to measure the degree of corruption that arises 
when q is transformed to l as directly computing the degree of 
the corruption is difficult we tackle this problem by 
approximation our main idea is that we measure to what extent 
information on q can be recovered from l on the assumption that 
only l is observed specifically we design a decoder that can 
accurately translate l back into new query q and the similarity s 
between the original query q and the new query q is adopted as 
a performance predictor this is a sketch of how the qf technique 
predicts query performance before filling in more details we 
briefly discuss why this method would work 
there is a relation between the similarity s defined above and 
retrieval performance on the one hand if the retrieval has 
strayed from the original sense of the query q the new query q 
extracted from ranked list l in response to q would be very 
different from the original query q on the other hand a query 
distilled from a ranked list containing many relevant documents is 
likely to be similar to the original query further examples in 
support of the relation will be provided later 
next we detail how to build the decoder and how to measure the 
similarity s 
in essence the goal of the decoder is to compress ranked list l 
into a few informative terms that should represent the content of 
the top ranked documents in l our approach to this goal is to 
represent ranked list l by a language model distribution over 
terms then terms are ranked by their contribution to the 
language model s kl kullback-leibler divergence from the 
background collection model top ranked terms will be chosen to 
form the new query q this approach is similar to that used in 
section of 
specifically we take three steps to compress ranked list l into 
query q without referring to the original query 
 we adopt the ranked list language model to estimate a 
language model based on ranked list l the model can be written 
as 
 ∑∈ 
 
ld 
ldpdwplwp 
where w is any term d is a document p d l is estimated by a 
linearly decreasing function of the rank of document d 
 each term in p w l is ranked by the following kl-divergence 
contribution 
 
 
 
log 
cwp 
lwp 
lwp 
where p w c is the collection model estimated by the relative 
frequency of term w in collection c as a whole 
 the top n ranked terms by eq form a weighted query 
q { wi ti } i n where wi denotes the i-th ranked term and 
weight ti is the kl-divergence contribution of wi in eq 
term cruise ship vessel sea passenger 
kl 
contribution 
 
table top terms compressed from the ranked list in 
response to query cruise ship damage sea life 
two representative examples one for a poorly performing query 
cruise ship damage sea life trec topic average 
precision and the other for a high performing query 
prostate cancer treatments trec topic average precision 
 are shown in table and respectively these examples 
indicate how the similarity between the original and the new 
query correlates with retrieval performance the parameter n in 
step is set to empirically and choosing a larger value of n is 
unnecessary since the weights after the top are usually too 
small to make any difference 
term prostate cancer treatment men therapy 
kl 
contribution 
 
table top terms compressed from the ranked list in 
response to query prostate cancer treatments 
to measure the similarity between original query q and new 
query q we first use q to do retrieval on the same collection a 
variant of the query likelihood model is adopted for retrieval 
namely documents are ranked by 
 
 
∑∈ 
 
qtw 
t 
i 
ii 
i 
dwpdqp 
where wi is a term in q and ti is the associated weight d is a 
document 
let l denote the new ranked list returned from the above 
retrieval the similarity is measured by the overlap of documents 
in l and l specifically the percentage of documents in the top 
k documents of l that are also present in the top k documents in 
l the cutoff k is treated as a free parameter 
we summarize here how the qf technique predicts performance 
given a query q and the associated ranked list l we first obtain a 
weighted query q compressed from l by the above three steps 
then we use q to perform retrieval and the new ranked list is l 
the overlap of documents in l and l is used for prediction 
 first rank change frc 
in this section we propose a method called the first rank change 
 frc for performance prediction for np queries this method is 
derived from the ranking robustness technique that is mainly 
designed for content-based queries when directly applied to np 
queries the robustness technique will be less effective because it 
takes the top ranked documents as a whole into account while np 
queries usually have only one single relevant document instead 
our technique focuses on the first rank document while the main 
idea of the robustness method remains specifically the 
pseudocode for computing frc is shown in figure 
input ranked list l {di} where i di denotes the i-th 
ranked document query q 
 initialize set the number of trials j counter c 
 for i to j 
 perturb every document in l let the outcome be a set f {di } 
where di denotes the perturbed version of di 
 do retrieval with query q on set f 
 c c if and only if d is ranked first in step 
 end of for 
 return the ratio c j 
figure pseudo-code for computing frc 
frc approximates the probability that the first ranked document 
in the original list l will remain ranked first even after the 
documents are perturbed the higher the probability is the more 
confidence we have in the first ranked document on the other 
hand in the extreme case of a random ranking the probability 
would be as low as we expect that frc has a positive 
association with np query performance we adopt to 
implement the document perturbation step step in fig using 
poisson distributions for more details we refer the reader to 
 evaluation 
we now present the results of predicting query performance by 
our models three state-of-the-art techniques are adopted as our 
baselines we evaluate our techniques across a variety of web 
retrieval settings as mentioned before we consider two types of 
queries that is content-based cb queries and named-page np 
finding queries 
first suppose that the query types are known we investigate the 
correlation between the predicted retrieval performance and the 
actual performance for both types of queries separately results 
show that our methods yield considerable improvements over the 
baselines 
we then consider a more challenging scenario where no prior 
information on query types is available two sub-cases are 
considered in the first one there exists only one type of query but 
the actual type is unknown we assume a mixture of the two 
query types in the second case we demonstrate that our models 
achieve good accuracy under this demanding scenario making 
prediction practical in a real-world web search environment 
 experimental setup 
our evaluation focuses on the gov collection which contains 
about million documents crawled from web sites in the gov 
domain during we create two kinds of data set for cb 
queries and np queries respectively for the cb type we use the 
ad-hoc topics of the terabyte tracks of and and 
name them tb -adhoc tb -adhoc and tb -adhoc 
respectively in addition we also use the ad-hoc topics of the 
 robust track rt to test the adaptability of our 
techniques to a non-web environment for np queries we use the 
named-page finding topics of the terabyte tracks of and 
 and we name them tb -np and tb -np respectively all 
queries used in our experiments are titles of trec topics as we 
center on web retrieval table summarizes the above data sets 
name collection topic number query type 
tb -adhoc gov - cb 
tb -adhoc gov - cb 
tb -adhoc gov - cb 
rt disk 
 minus cr 
 
 - 
cb 
tb -np gov np -np np 
tb -np gov np -np np 
table summary of test collections and topics 
retrieval performance of individual content-based and np queries 
is measured by the average precision and reciprocal rank of the 
first correct answer respectively we make use of the markov 
random field model for both ad-hoc and named-page finding 
retrieval we adopt the same setting of retrieval parameters used 
in the indri search engine is used for all of our 
experiments though not reported here we also tried the query 
likelihood model for ad-hoc retrieval and found that the results 
change little because of the very high correlation between the 
query performances obtained by the two retrieval models 
measured by pearson s coefficient 
 known query types 
suppose that query types are known we treat each type of query 
separately and measure the correlation with average precision or 
the reciprocal rank in the case of np queries we adopt the 
pearson s correlation test which reflects the degree of linear 
relationship between the predicted and the actual retrieval 
performance 
 content-based queries 
methods clarity robust jsd wig qf wig 
 qf 
tb 
 adhoc 
 
tb 
adhoc 
 n a 
table pearson s correlation coefficients for correlation with 
average precision on the terabyte tracks ad-hoc for clarity 
score robustness score the jsd-based method we directly 
cites the score reported in wig query feedback qf and 
a linear combination of wig and qf bold cases mean the 
results are statistically significant at the level 
table shows the correlation with average precision on two data 
sets one is a combination of tb -adhoc and tb -adhoc 
topics in total and the other is tb -adhoc topics the 
reason that we put tb -adhoc and tb -adhoc together is to 
make our results comparable to our baselines are the clarity 
score clarity the robustness score robust and the 
jsdbased method jsd for the clarity and robustness score we 
have tried different parameter settings and report the highest 
correlation coefficients we have found we directly cite the result 
of the jsd-based method reported in the table also shows the 
results for the weighted information gain wig method and the 
query feedback qf method for predicting content-based 
queries as we described in the previous section both wig and 
qf have one free parameter to set that is the cutoff rank k we 
train the parameter on one dataset and test on the other when 
combining wig and qf a simple linear combination is used and 
the combination weight is learned from the training data set 
from these results we can see that our methods are considerably 
more accurate compared to the baselines we also observe that 
further improvements are obtained from the combination of wig 
and qf suggesting that they measure different properties of the 
retrieval process that relate to performance 
we discover that our methods generalize well on tb -adhoc 
while the correlation for the clarity score with retrieval 
performance on this data set is considerably worse further 
investigation shows that the mean average precision of 
tb -adhoc is and is about better than that of the first data set 
while the other three methods typically consider the top or 
less documents given a ranked list the clarity method usually 
needs the top or more documents to adequately measure the 
coherence of a ranked list higher mean average precision makes 
ranked lists retrieved by different queries more similar in terms of 
coherence at the level of top documents we believe that this 
is the main reason for the low accuracy of the clarity score on the 
second data set 
though this paper focuses on a web search environment it is 
desirable that our techniques will work consistently well in other 
situations to this end we examine the effectiveness of our 
techniques on the robust track for our methods we 
evenly divide all of the test queries into five groups and perform 
five-fold cross validation each time we use one group for 
training and the remaining four groups for testing we make use 
of all of the queries for our two baselines that is the clarity score 
and the robustness score the parameters for our baselines are the 
same as those used in the results shown in table 
demonstrate that the prediction accuracy of our methods is on a 
par with that of the two strong baselines 
clarity robust wig qf 
 
table comparison of pearson s correlation coefficients on 
the robust track for clarity score robustness score 
wig and query feedback qf bold cases mean the results 
are statistically significant at the level 
furthermore we examine the prediction sensitivity of our 
methods to the cutoff rank k with respect to wig it is quite 
robust to k on the terabyte tracks - while it prefers a 
small value of k like on the robust track in other words 
a small value of k is a nearly-optimal choice for both kinds of 
tracks considering the fact that all other parameters involved in 
wig are fixed and consequently the same for the two cases this 
means wig can achieve nearly-optimal prediction accuracy in 
two considerably different situations with exactly the same 
parameter setting regarding qf it prefers a larger value of k 
such as on the terabyte tracks and a smaller value of k such 
as on the robust track 
 np queries 
we adopt wig and first rank change frc for predicting 
npquery performance we also try a linear combination of the two as 
in the previous section the combination weight is obtained from 
the other data set we use the correlation with the reciprocal ranks 
measured by the pearson s correlation test to evaluate prediction 
quality the results are presented in table again our baselines 
are the clarity score and the robustness score 
to make a fair comparison we tune the clarity score in different 
ways we found that using the first ranked document to build the 
query model yields the best prediction accuracy we also 
attempted to utilize document structure by using the mixture of 
language models mentioned in section little improvement 
was obtained the correlation coefficients for the clarity score 
reported in table are the best we have found as we can see 
our methods considerably outperform the clarity score technique 
on both of the runs this confirms our intuition that the use of a 
coherence-based measure like the clarity score is inappropriate for 
np queries 
methods clarity robust wig frc wig frc 
tb -np - 
tb -np - 
table pearson s correlation coefficients for correlation with 
reciprocal ranks on the terabyte tracks np for clarity 
score robustness score wig the first rank change frc 
and a linear combination of wig and frc bold cases mean 
the results are statistically significant at the level 
regarding the robustness score we also tune the parameters and 
report the best we have found we observe an interesting and 
surprising negative correlation with reciprocal ranks we explain 
this finding briefly a high robustness score means that a number 
of top ranked documents in the original ranked list are still highly 
ranked after perturbing the documents the existence of such 
documents is a good sign of high performance for content-based 
queries as these queries usually contain a number of relevant 
documents however with regard to np queries one 
fundamental difference is that there is only one relevant document 
for each query the existence of such documents can confuse the 
ranking function and lead to low retrieval performance although 
the negative correlation with retrieval performance exists the 
strength of the correlation is weaker and less consistent compared 
to our methods as shown in table 
based on the above analysis we can see that current prediction 
techniques like clarity score and robustness score that are mainly 
designed for content-based queries face significant challenges and 
are inadequate to deal with np queries our two techniques 
proposed for np queries consistently demonstrate good prediction 
accuracy displaying initial success in solving the problem of 
predicting performance for np queries another point we want to 
stress is that the wig method works well for both types of 
queries a desirable property that most prediction techniques lack 
 unknown query types 
in this section we run two kinds of experiments without access to 
query type labels first we assume that only one type of query 
exists but the type is unknown second we experiment on a 
mixture of content-based and np queries the following two 
subsections will report results for the two conditions respectively 
 only one type exists 
we assume that all queries are of the same type that is they are 
either np queries or content-based queries we choose wig to 
deal with this case because it shows good prediction accuracy for 
both types of queries in the previous section we consider two 
cases cb all title queries from the ad-hoc task of the 
terabyte tracks - np all np queries from the 
named page finding task of the terabyte tracks and 
we take a simple strategy by labeling all of the queries in each 
case as the same type either np or cb regardless of their actual 
type the computation of wig will be based on the labeled query 
type instead of the actual type there are four possibilities with 
respect to the relation between the actual type and the labeled 
type the correlation with retrieval performance under the four 
possibilities is presented in table for example the value 
at the intersection between the second row and the third column 
shows the pearson s correlation coefficient for correlation with 
average precision when the content-based queries are incorrectly 
labeled as the np type 
based on these results we recommend treating all queries as the 
np type when only one query type exists and accurate query 
classification is not feasible considering the risk that a large loss 
of accuracy will occur if np queries are incorrectly labeled as 
content-based queries these results also demonstrate the strong 
adaptability of wig to different query types 
cb labeled np labeled 
cb actual 
np actual 
table comparison of pearson s correlation coefficients for 
correlation with retrieval performance under four possibilities 
on the terabyte tracks np bold cases mean the results are 
statistically significant at the level 
 a mixture of contented-based and np queries 
a mixture of the two types of queries is a more realistic situation 
that a web search engine will meet we evaluate prediction 
accuracy by how accurately poorly-performing queries can be 
identified by the prediction method assuming that actual query 
types are unknown but we can predict query types this is a 
challenging task because both the predicted and actual 
performance for one type of query can be incomparable to that for 
the other type 
next we discuss how to implement our evaluation we create a 
query pool which consists of all of the ad-hoc title queries 
from terabyte track - and all of the np queries 
from terabyte track we divide the queries in the 
pool into classes good better than of the queries of the 
same type in terms of retrieval performance and bad 
 otherwise according to these standards a np query with the 
reciprocal rank above or a content-based query with the 
average precision above will be considered as good 
then each time we randomly select one query q from the pool 
with probability p that q is contented-based the remaining 
queries are used as training data we first decide the type of query 
q according to a query classifier namely the query classifier 
tells us whether query q is np or content-based based on the 
predicted query type and the score computed for query q by a 
prediction technique a binary decision is made about whether 
query q is good or bad by comparing to the score threshold of the 
predicted query type obtained from the training data prediction 
accuracy is measured by the accuracy of the binary decision in 
our implementation we repeatedly take a test query from the 
query pool and prediction accuracy is computed as the 
percentage of correct decisions that is a good bad query is 
predicted to be good bad it is obvious that random guessing will 
lead to accuracy 
let us take the wig method for example to illustrate the process 
two wig thresholds one for np queries and the other for 
content-based queries are trained by maximizing the prediction 
accuracy on the training data when a test query is labeled as the 
np cb type by the query type classifier it will be predicted to 
be good if and only if the wig score for this query is above the 
np cb threshold similar procedures will be taken for other 
prediction techniques 
now we briefly introduce the automatic query type classifier used 
in this paper we find that the robustness score though originally 
proposed for performance prediction is a good indicator of query 
types we find that on average content-based queries have a 
much higher robustness score than np queries for example 
figure shows the distributions of robustness scores for np and 
content-based queries according to this finding the robustness 
score classifier will attach a np cb label to the query if the 
robustness score for the query is below above a threshold 
trained from the training data 
 
 
 
 
 
 
- - - - - 
np content-based 
figure distribution of robustness scores for np and cb 
queries the np queries are the np topics from the 
terabyte track the content-based queries are the ad-hoc 
title from the terabyte tracks - the probability 
distributions are estimated by the kernel density estimation 
method 
strategies robust wig- wig- wig- optimal 
p 
p 
table comparison of prediction accuracy for five strategies 
in the mixed-query situation two ways to sample a query 
from the pool the sampled query is content-based with the 
probability p that is the query is np with probability 
 set the probability p 
we consider five strategies in our experiments in the first 
strategy denoted by robust we use the robustness score for 
query performance prediction with the help of a perfect query 
classifier that always correctly map a query into one of the two 
categories that is np or cb this strategy represents the level of 
prediction accuracy that current prediction techniques can achieve 
in an ideal condition that query types are known in the next 
following three strategies the wig method is adopted for 
performance prediction the difference among the three is that 
three different query classifiers are used for each strategy the 
classifier always classifies a query into the np type the 
robustness score 
probabilitydensity 
classifier is the robust score classifier mentioned above the 
classifier is a perfect one these three strategies are denoted by 
wig- wig- and wig- respectively the reason we are 
interested in wig- is based on the results from section in 
the last strategy denoted by optimal which serves as an upper 
bound on how well we can do so far we fully make use of our 
prediction techniques for each query type assuming a perfect 
query classifier is available specifically we linearly combine 
wig and qf for content-based queries and wig and frc for np 
queries 
the results for the five strategies are shown in table for each 
strategy we try two ways to sample a query from the pool the 
sampled query is cb with probability p the query is np 
with probability set the probability p from table 
we can see that in terms of prediction accuracy wig- the wig 
method with the automatic query classifier is not only better than 
the first two cases but also is close to wig- where a perfect 
classifier is assumed some further improvements over wig- are 
observed when combined with other prediction techniques the 
merit of wig- is that it provides a practical solution to 
automatically identifying poorly performing queries in a web 
search environment with mixed query types which poses 
considerable obstacles to traditional prediction techniques 
 conclusions and future work 
to our knowledge our paper is the first to thoroughly explore 
prediction of query performance in web search environments we 
demonstrated that our models resulted in higher prediction 
accuracy than previously published techniques not specially 
devised for web search scenarios in this paper we focus on two 
types of queries in web search content-based and named-page 
 np finding queries corresponding to the ad-hoc retrieval task 
and the named-page finding task respectively for both types of 
web queries our prediction models were shown to be 
substantially more accurate than the current state-of-the-art 
techniques furthermore we considered a more realistic case that 
no prior information on query types is available we 
demonstrated that the wig method is particularly suitable for this 
situation considering the adaptability of wig to a range of 
collections and query types one of our future plans is to apply 
this method to predict user preference of search results on realistic 
data collected from a commercial search engine 
other than accuracy another major issue that prediction 
techniques have to deal with in a web environment is efficiency 
fortunately since the wig score is computed just over the terms 
and the phrases that appear in the query this calculation can be 
made very efficient with the support of index on the other hand 
the computation of qf and frc is relatively less efficient since 
qf needs to retrieve the whole collection twice and frc needs to 
repeatedly rank the perturbed documents how to improve the 
efficiency of qf and frc is our future work 
in addition the prediction techniques proposed in this paper have 
the potential of improving retrieval performance by combining 
with other ir techniques for example our techniques can be 
incorporated to popular query modification techniques such as 
query expansion and query relaxation guided by performance 
prediction we can make a better decision on when to or how to 
modify queries to enhance retrieval effectiveness we would like 
to carry out research in this direction in the future 
 acknowlegments 
this work was supported in part by the center for intelligent 
information retrieval in part by the defense advanced research 
projects agency darpa under contract number 
hr - -c and in part by an award from google any opinions 
findings and conclusions or recommendations expressed in this 
material are those of the author and do not necessarily reflect 
those of the sponsor in addition we thank donald metzler for his 
valuable comments on this work 
 references 
 y zhou w b croft ranking robustness a novel 
framework to predict query performance in proceedings of 
cikm 
 d carmel e yom-tov a darlow d pelleg what makes a 
query difficult in proceedings of sigir 
 c l a clarke f scholer i soboroff the trec 
terabyte track in the online proceedings of trec 
 b he and i ounis inferring query performance using 
preretrieval predictors in proceedings of the spire 
 s tomlinson robust web and terabyte retrieval with 
hummingbird searchserver at trec in the online 
proceedings of trec 
 s cronen-townsend y zhou w b croft predicting 
query performance in proceedings of sigir 
 v vinay i j cox n mill-frayling k wood on ranking the 
effectiveness of searcher in proceedings of sigir 
 d metzler w b croft a markov random filed model for 
term dependencies in proceedings of sigir 
 d metzler t strohman y zhou w b croft indri at trec 
 terabyte track in the online proceedings of 
trec 
 p ogilvie and j callan combining document 
representations for known-item search in proceedings of 
sigir 
 a berger j lafferty information retrieval as statistical 
translation in proceedings of sigir 
 indri search engine http www lemurproject org indri 
 i j taneja on generalized information measures and their 
applications advances in electronics and electron physics 
academic press usa - 
 s cronen-townsend y zhou and croft w b a 
framework for selective query expansion in proceedings 
of cikm 
 f song w b croft a general language model for 
information retrieval in proceedings of sigir 
 personal email contact with vishwa vinay and our own 
experiments 
 e yom-tov s fine d carmel a darlow learning to 
estimate query difficulty including applications to missing 
content detection and distributed information retrieval in 
proceedings of sigir 
using asymmetric distributions 
to improve text classifier probability estimates 
paul n bennett 
computer science dept 
carnegie mellon university 
pittsburgh pa 
pbennett  cs cmu edu 
abstract 
text classifiers that give probability estimates are more readily 
applicable in a variety of scenarios for example rather than 
choosing one set decision threshold they can be used in a bayesian 
risk model to issue a run-time decision which minimizes a 
userspecified cost function dynamically chosen at prediction time 
however the quality of the probability estimates is crucial we review a 
variety of standard approaches to converting scores and poor 
probability estimates from text classifiers to high quality estimates and 
introduce new models motivated by the intuition that the empirical 
score distribution for the extremely irrelevant hard to 
discriminate and obviously relevant items are often significantly 
different finally we analyze the experimental performance of these 
models over the outputs of two text classifiers the analysis 
demonstrates that one of these models is theoretically attractive 
 introducing few new parameters while increasing flexibility 
computationally efficient and empirically preferable 
categories and subject descriptors 
h information storage and retrieval information search 
and retrieval i artificial intelligence learning i 
 pattern recognition design methodology 
general terms 
algorithms experimentation reliability 
 introduction 
text classifiers that give probability estimates are more flexible 
in practice than those that give only a simple classification or even a 
ranking for example rather than choosing one set decision 
threshold they can be used in a bayesian risk model to issue a 
runtime decision which minimizes the expected cost of a user-specified 
cost function dynamically chosen at prediction time this can be 
used to minimize a linear utility cost function for filtering tasks 
where pre-specified costs of relevant irrelevant are not available 
during training but are specified at prediction time furthermore 
the costs can be changed without retraining the model 
additionally probability estimates are often used as the basis of deciding 
which document s label to request next during active learning 
 effective active learning can be key in many information 
retrieval tasks where obtaining labeled data can be costly - severely 
reducing the amount of labeled data needed to reach the same 
performance as when new labels are requested randomly finally 
they are also amenable to making other types of cost-sensitive 
decisions and for combining decisions however in all of 
these tasks the quality of the probability estimates is crucial 
parametric models generally use assumptions that the data 
conform to the model to trade-off flexibility with the ability to estimate 
the model parameters accurately with little training data since 
many text classification tasks often have very little training data we 
focus on parametric methods however most of the existing 
parametric methods that have been applied to this task have an 
assumption we find undesirable while some of these methods allow the 
distributions of the documents relevant and irrelevant to the topic 
to have different variances they typically enforce the unnecessary 
constraint that the documents are symmetrically distributed around 
their respective modes we introduce several asymmetric 
parametric models that allow us to relax this assumption without 
significantly increasing the number of parameters and demonstrate how 
we can efficiently fit the models additionally these models can be 
interpreted as assuming the scores produced by the text classifier 
have three basic types of empirical behavior - one corresponding 
to each of the extremely irrelevant hard to discriminate and 
obviously relevant items 
we first review related work on improving probability estimates 
and score modeling in information retrieval then we discuss in 
further detail the need for asymmetric models after this we 
describe two specific asymmetric models and using two standard text 
classifiers na¨ıve bayes and svms demonstrate how they can be 
efficiently used to recalibrate poor probability estimates or produce 
high quality probability estimates from raw scores we then review 
experiments using previously proposed methods and the 
asymmetric methods over several text classification corpora to demonstrate 
the strengths and weaknesses of the various methods finally we 
summarize our contributions and discuss future directions 
 related work 
parametric models have been employed to obtain probability 
estimates in several areas of information retrieval lewis gale 
use logistic regression to recalibrate na¨ıve bayes though the quality 
of the probability estimates are not directly evaluated it is simply 
performed as an intermediate step in active learning manmatha 
et al introduced models appropriate to produce probability 
estimates from relevance scores returned from search engines and 
demonstrated how the resulting probability estimates could be 
subsequently employed to combine the outputs of several search 
engines they use a different parametric distribution for the relevant 
and irrelevant classes but do not pursue two-sided asymmetric 
distributions for a single class as described here they also survey the 
long history of modeling the relevance scores of search engines 
our work is similar in flavor to these previous attempts to model 
search engine scores but we target text classifier outputs which we 
have found demonstrate a different type of score distribution 
behavior because of the role of training data 
focus on improving probability estimates has been growing lately 
zadrozny elkan provide a corrective measure for decision 
trees termed curtailment and a non-parametric method for 
recalibrating na¨ıve bayes in more recent work they investigate 
using a semi-parametric method that uses a monotonic 
piecewiseconstant fit to the data and apply the method to na¨ıve bayes and a 
linear svm while they compared their methods to other 
parametric methods based on symmetry they fail to provide significance 
test results our work provides asymmetric parametric methods 
which complement the non-parametric and semi-parametric 
methods they propose when data scarcity is an issue in addition their 
methods reduce the resolution of the scores output by the classifier 
 the number of distinct values output but the methods here do not 
have such a weakness since they are continuous functions 
there is a variety of other work that this paper extends platt 
 uses a logistic regression framework that models noisy class 
labels to produce probabilities from the raw output of an svm 
his work showed that this post-processing method not only can 
produce probability estimates of similar quality to svms directly 
trained to produce probabilities regularized likelihood kernel 
methods but it also tends to produce sparser kernels which generalize 
better finally bennett obtained moderate gains by applying 
platt s method to the recalibration of na¨ıve bayes but found there 
were more problematic areas than when it was applied to svms 
recalibrating poorly calibrated classifiers is not a new problem 
lindley et al first proposed the idea of recalibrating classifiers 
and degroot fienberg gave the now accepted standard 
formalization for the problem of assessing calibration initiated by 
others 
 problem definition approach 
our work differs from earlier approaches primarily in three points 
 we provide asymmetric parametric models suitable for use when 
little training data is available we explicitly analyze the quality 
of probability estimates these and competing methods produce and 
provide significance tests for these results we target text 
classifier outputs where a majority of the previous literature targeted the 
output of search engines 
 problem definition 
the general problem we are concerned with is highlighted in 
figure a text classifier produces a prediction about a document 
and gives a score s d indicating the strength of its decision that 
the document belongs to the positive class relevant to the topic 
we assume throughout there are only two classes the positive and 
the negative or irrelevant class and - respectively 
there are two general types of parametric approaches the first 
of these tries to fit the posterior function directly i e there is one 
p s p s − 
bayes rulep p − 
classifier 
p s d 
predict class c d { −} 
confidence s d that c d 
document d 
and give unnormalized 
figure we are concerned with how to perform the box 
highlighted in grey the internals are for one type of approach 
function estimator that performs a direct mapping of the score s to 
the probability p s d the second type of approach breaks the 
problem down as shown in the grey box of figure an estimator 
for each of the class-conditional densities i e p s and p s − 
is produced then bayes rule and the class priors are used to obtain 
the estimate for p s d 
 motivation for asymmetric distributions 
most of the previous parametric approaches to this problem 
either directly or indirectly when fitting only the posterior 
correspond to fitting gaussians to the class-conditional densities they 
differ only in the criterion used to estimate the parameters we can 
visualize this as depicted in figure since increasing s usually 
indicates increased likelihood of belonging to the positive class then 
the rightmost distribution usually corresponds to p s 
a b 
c 
 
 
 
 
 
 
− − 
p s class { −} 
unnormalized confidence score s 
p s class 
p s class − 
figure typical view of discrimination based on gaussians 
however using standard gaussians fails to capitalize on a basic 
characteristic commonly seen namely if we have a raw output 
score that can be used for discrimination then the empirical 
behavior between the modes label b in figure is often very different 
than that outside of the modes labels a and c in figure 
intuitively the area between the modes corresponds to the hard 
examples which are difficult for this classifier to distinguish while the 
areas outside the modes are the extreme examples that are usually 
easily distinguished this suggests that we may want to uncouple 
the scale of the outside and inside segments of the distribution as 
depicted by the curve denoted as a-gaussian in figure as a 
result an asymmetric distribution may be a more appropriate choice 
for application to the raw output score of a classifier 
ideally i e perfect classification there will exist scores θ− and 
θ such that all examples with score greater than θ are relevant 
and all examples with scores less than θ− are irrelevant 
furthermore no examples fall between θ− and θ the distance 
 θ− − θ corresponds to the margin in some classifiers and 
an attempt is often made to maximize this quantity because text 
classifiers have training data to use to separate the classes the 
final behavior of the score distributions is primarily a factor of the 
amount of training data and the consequent separation in the classes 
achieved this is in contrast to search engine retrieval where the 
distribution of scores is more a factor of language distribution across 
documents the similarity function and the length and type of query 
perfect classification corresponds to using two very asymmetric 
distributions but in this case the probabilities are actually one and 
zero and many methods will work for typical purposes practically 
some examples will fall between θ− and θ and it is often 
important to estimate the probabilities of these examples well since they 
correspond to the hard examples justifications can be given for 
both why you may find more and less examples between θ− and θ 
than outside of them but there are few empirical reasons to believe 
that the distributions should be symmetric 
a natural first candidate for an asymmetric distribution is to 
generalize a common symmetric distribution e g the laplace or the 
gaussian an asymmetric laplace distribution can be achieved by 
placing two exponentials around the mode in the following manner 
p x θ β γ 
 
 
 
βγ 
β γ 
exp −β θ − x x ≤ θ 
 β γ 
βγ 
β γ 
exp −γ x − θ x θ 
 
where θ β and γ are the model parameters θ is the mode of the 
distribution β is the inverse scale of the exponential to the left of 
the mode and γ is the inverse scale of the exponential to the right 
we will use the notation λ x θ β γ to refer to this distribution 
 
 
 
 
 
 
- - - 
p s class { -} 
unnormalized confidence score s 
gaussian 
a-gaussian 
figure gaussians vs asymmetric gaussians a 
shortcoming of symmetric distributions - the vertical lines show the 
modes as estimated nonparametrically 
we can create an asymmetric gaussian in the same manner 
p x θ σl σr 
 
 
 
 √ 
 π σl σr 
exp − x−θ 
 σ 
l 
x ≤ θ 
 σl σr 
 √ 
 π σl σr 
exp − x−θ 
 σ 
r 
x θ 
 
where θ σl and σr are the model parameters to refer to this 
asymmetric gaussian we use the notation γ x θ σl σr while 
these distributions are composed of halves the resulting function 
is a single continuous distribution 
these distributions allow us to fit our data with much greater 
flexibility at the cost of only fitting six parameters we could 
instead try mixture models for each component or other extensions 
but most other extensions require at least as many parameters and 
can often be more computationally expensive in addition the 
motivation above should provide significant cause to believe the 
underlying distributions actually behave in this way furthermore 
this family of distributions can still fit a symmetric distribution 
and finally in the empirical evaluation evidence is presented that 
demonstrates this asymmetric behavior see figure 
to our knowledge neither family of distributions has been 
previously used in machine learning or information retrieval both are 
termed generalizations of an asymmetric laplace in but we 
refer to them as described above to reflect the nature of how we 
derived them for this task 
 estimating the parameters of the 
asymmetric distributions 
this section develops the method for finding maximum 
likelihood estimates mle of the parameters for the above asymmetric 
distributions in order to find the mles we have two choices 
use numerical estimation to estimate all three parameters at once 
 fix the value of θ and estimate the other two β and γ or σl 
and σr given our choice of θ then consider alternate values of θ 
because of the simplicity of analysis in the latter alternative we 
choose this method 
 asymmetric laplace mles 
for d {x x xn } where the xi are i i d and x ∼ 
λ x θ β γ the likelihood is n 
i λ x θ β γ now we fix 
θ and compute the maximum likelihood for that choice of θ then 
we can simply consider all choices of θ and choose the one with 
the maximum likelihood over all choices of θ 
the complete derivation is omitted because of space but is 
available in we define the following values 
nl {x ∈ d x ≤ θ} nr {x ∈ d x θ} 
sl 
x∈d x≤θ 
x sr 
x∈d x θ 
x 
dl nlθ − sl dr sr − nrθ 
note that dl and dr are the sum of the absolute differences 
between the x belonging to the left and right halves of the distribution 
 respectively and θ finally the mles for β and γ for a fixed θ are 
βmle 
n 
dl 
√ 
drdl 
γmle 
n 
dr 
√ 
drdl 
 
these estimates are not wholly unexpected since we would obtain 
nl 
dl 
if we were to estimate β independently of γ the elegance of 
the formulae is that the estimates will tend to be symmetric only 
insofar as the data dictate it i e the closer dl and dr are to being 
equal the closer the resulting inverse scales 
by continuity arguments when n we assign β γ 
where is a small constant that acts to disperse the distribution to 
a uniform similarly when n and dl we assign β inf 
where inf is a very large constant that corresponds to an extremely 
sharp distribution i e almost all mass at θ for that half dr 
is handled similarly 
assuming that θ falls in some range φ ψ dependent upon only 
the observed documents then this alternative is also easily 
computable given nl sl nr sr we can compute the posterior and 
the mles in constant time in addition if the scores are sorted 
then we can perform the whole process quite efficiently starting 
with the minimum θ φ we would like to try we loop through the 
scores once and set nl sl nr sr appropriately then we increase 
θ and just step past the scores that have shifted from the right side 
of the distribution to the left assuming the number of candidate 
θs are o n this process is o n and the overall process is 
dominated by sorting the scores o n log n or expected linear time 
 asymmetric gaussian mles 
for d {x x xn } where the xi are i i d and x ∼ 
γ x θ σl σr the likelihood is n 
i γ x θ β γ the mles 
can be worked out similar to the above 
we assume the same definitions as above the complete 
derivation omitted for space is available in and in addition let 
sl 
x∈d x≤θ 
x 
sr 
x∈d x θ 
x 
dl sl − slθ θ 
nl dr sr − srθ θ 
nr 
the analytical solution for the mles for a fixed θ is 
σl mle 
dl d 
 
l d 
 
r 
n 
 
σr mle 
dr d 
 
r d 
 
l 
n 
 
by continuity arguments when n we assign σr σl 
inf and when n and dl resp dr we 
assign σl resp σr again the same computational 
complexity analysis applies to estimating these parameters 
 experimental analysis 
 methods 
for each of the methods that use a class prior we use a smoothed 
add-one estimate i e p c c 
n 
where n is the number of 
documents for methods that fit the class-conditional densities p s 
and p s − the resulting densities are inverted using bayes rule as 
described above all of the methods below are fit using maximum 
likelihood estimates 
for recalibrating a classifier i e correcting poor probability 
estimates output by the classifier it is usual to use the log-odds of 
the classifier s estimate as s d the log-odds are defined to be 
log p d 
p − d 
 the normal decision threshold minimizing error in 
terms of log-odds is at zero i e p d p − d 
since it scales the outputs to a space −∞ ∞ the log-odds 
make normal and similar distributions applicable lewis 
gale give a more motivating viewpoint that fitting the log-odds 
is a dampening effect for the inaccurate independence assumption 
and a bias correction for inaccurate estimates of the priors in 
general fitting the log-odds can serve to boost or dampen the signal 
from the original classifier as the data dictate 
gaussians 
a gaussian is fit to each of the class-conditional densities using 
the usual maximum likelihood estimates this method is denoted 
in the tables below as gauss 
asymmetric gaussians 
an asymmetric gaussian is fit to each of the class-conditional 
densities using the maximum likelihood estimation procedure 
described above intervals between adjacent scores are divided by 
in testing candidate θs i e points between actual scores 
occurring in the data set are tested this method is denoted as a gauss 
laplace distributions 
even though laplace distributions are not typically applied to 
this task we also tried this method to isolate why benefit is gained 
from the asymmetric form the usual mles were used for 
estimating the location and scale of a classical symmetric laplace 
distribution as described in we denote this method as laplace below 
asymmetric laplace distributions 
an asymmetric laplace is fit to each of the class-conditional 
densities using the maximum likelihood estimation procedure 
described above as with the asymmetric gaussian intervals between 
adjacent scores are divided by in testing candidate θs this 
method is denoted as a laplace below 
logistic regression 
this method is the first of two methods we evaluated that 
directly fit the posterior p s d both methods restrict the set 
of families to a two-parameter sigmoid family they differ 
primarily in their model of class labels as opposed to the above 
methods one can argue that an additional boon of these methods is they 
completely preserve the ranking given by the classifier when this 
is desired these methods may be more appropriate the previous 
methods will mostly preserve the rankings but they can deviate if 
the data dictate it thus they may model the data behavior better at 
the cost of departing from a monotonicity constraint in the output 
of the classifier 
lewis gale use logistic regression to recalibrate na¨ıve 
bayes for subsequent use in active learning the model they use is 
p s d 
exp a b s d 
 exp a b s d 
 
instead of using the probabilities directly output by the classifier 
they use the loglikelihood ratio of the probabilities log p d 
p d − 
 as 
the score s d instead of using this below we will use the 
logodds ratio this does not affect the model as it simply shifts all of 
the scores by a constant determined by the priors we refer to this 
method as logreg below 
logistic regression with noisy class labels 
platt proposes a framework that extends the logistic 
regression model above to incorporate noisy class labels and uses it to 
produce probability estimates from the raw output of an svm 
this model differs from the logreg model only in how the 
parameters are estimated the parameters are still fit using maximum 
likelihood estimation but a model of noisy class labels is used in 
addition to allow for the possibility that the class was mislabeled 
the noise is modeled by assuming there is a finite probability of 
mislabeling a positive example and of mislabeling a negative 
example these two noise estimates are determined by the number 
of positive examples and the number of negative examples using 
bayes rule to infer the probability of incorrect label 
even though the performance of this model would not be 
expected to deviate much from logreg we evaluate it for 
completeness we refer to this method below as lr noise 
 data 
we examined several corpora including the msn web directory 
reuters and trec-ap 
msn web directory 
the msn web directory is a large collection of heterogeneous 
web pages from a may web snapshot that have been 
hierarchically classified we used the same train test split of 
documents as that reported in the msn web hierarchy is a 
seven-level hierarchy we used all of the top-level categories 
the class proportions in the training set vary from to 
in the testing set they range from to the classes 
are general subjects such as health fitness and travel vacation 
human indexers assigned the documents to zero or more categories 
for the experiments below we used only the top words with 
highest mutual information for each class approximately k 
words appear in at least three training documents 
reuters 
the reuters corpus contains reuters news articles 
from for this data set we used the modapte standard train 
test split of documents unused documents the 
classes are economic subjects e g acq for acquisitions earn 
for earnings etc that human taggers applied to the document 
a document may have multiple subjects there are actually 
classes in this domain only of which occur in the training and 
testing set however we only examined the ten most frequent classes 
since small numbers of testing examples make interpreting some 
performance measures difficult due to high variance 
limiting to 
the ten largest classes allows us to compare our results to 
previously published results the class proportions in 
the training set vary from to in the testing set they 
range from to 
for the experiments below we used only the top words with 
highest mutual information for each class approximately k words 
appear in at least three training documents 
trec-ap 
the trec-ap corpus is a collection of ap news stories from 
 to we used the same train test split of 
documents that was used in as described in see also 
 the categories are defined by keywords in a keyword field 
the title and body fields are used in the experiments below there 
are twenty categories in total the class proportions in the training 
set vary from to in the testing set they range from 
 to 
for the experiments described below we use only the top 
words with the highest mutual information for each class 
approximately k words appear in at least training documents 
 classifiers 
we selected two classifiers for evaluation a linear svm 
classifier which is a discriminative classifier that does not normally 
output probability values and a na¨ıve bayes classifier whose 
probability outputs are often poor but can be improved 
 
a separate comparison of only logreg lr noise and 
a laplace over all categories of reuters was also conducted 
after accounting for the variance that evaluation also supported 
the claims made here 
svm 
for linear svms we use the smox toolkit which is based on 
platt s sequential minimal optimization algorithm the features 
were represented as continuous values we used the raw output 
score of the svm as s d since it has been shown to be appropriate 
before the normal decision threshold assuming we are 
seeking to minimize errors for this classifier is at zero 
na¨ıve bayes 
the na¨ıve bayes classifier model is a multinomial model 
we smoothed word and class probabilities using a bayesian 
estimate with the word prior and a laplace m-estimate respectively 
we use the log-odds estimated by the classifier as s d the normal 
decision threshold is at zero 
 performance measures 
we use log-loss and squared error to evaluate the 
quality of the probability estimates for a document d with class c d ∈ 
{ −} i e the data have known labels and not probabilities 
logloss is defined as δ c d log p d δ c d − log p − d 
where δ a b 
 
 if a b and otherwise the squared error is 
δ c d − p d 
 δ c d − − p − d 
 when the 
class of a document is correctly predicted with a probability of one 
log-loss is zero and squared error is zero when the class of a 
document is incorrectly predicted with a probability of one log-loss 
is −∞ and squared error is one thus both measures assess how 
close an estimate comes to correctly predicting the item s class but 
vary in how harshly incorrect predictions are penalized 
we report only the sum of these measures and omit the averages 
for space their averages average log-loss and mean squared 
error mse can be computed from these totals by dividing by the 
number of binary decisions in a corpus 
in addition we also compare the error of the classifiers at their 
default thresholds and with the probabilities this evaluates how 
the probability estimates have improved with respect to the 
decision threshold p d thus error only indicates how the 
methods would perform if a false positive was penalized the same 
as a false negative and not the general quality of the probability 
estimates it is presented simply to provide the reader with a more 
complete understanding of the empirical tendencies of the methods 
we use a a standard paired micro sign test to determine 
statistical significance in the difference of all measures only pairs 
that the methods disagree on are used in the sign test this test 
compares pairs of scores from two systems with the null 
hypothesis that the number of items they disagree on are binomially 
distributed we use a significance level of p 
 experimental methodology 
as the categories under consideration in the experiments are not 
mutually exclusive the classification was done by training n binary 
classifiers where n is the number of classes 
in order to generate the scores that each method uses to fit its 
probability estimates we use five-fold cross-validation on the 
training data we note that even though it is computationally efficient 
to perform leave-one-out cross-validation for the na¨ıve bayes 
classifier this may not be desirable since the distribution of scores can 
be skewed as a result of course as with any application of n-fold 
cross-validation it is also possible to bias the results by holding n 
too low and underestimating the performance of the final classifier 
 results discussion 
the results for recalibrating na¨ıve bayes are given in table a 
table b gives results for producing probabilistic outputs for svms 
log-loss error 
errors 
msn web 
gauss - 
a gauss - 
laplace - 
a laplace - † 
 
logreg - 
lr noise - 
na¨ıve bayes - 
reuters 
gauss - 
a gauss - 
laplace - 
a laplace - ‡ 
 ‡ 
 
logreg - 
lr noise - 
na¨ıve bayes - 
trec-ap 
gauss - 
a gauss - 
laplace - 
a laplace - ‡ 
 
logreg - 
lr noise - 
na¨ıve bayes - 
log-loss error 
errors 
msn web 
gauss - 
a gauss - 
laplace - 
a laplace - 
logreg - 
lr noise - 
linear svm n a n a 
reuters 
gauss - 
a gauss - 
laplace - 
a laplace - 
logreg - 
lr noise - 
linear svm n a n a 
trec-ap 
gauss - 
a gauss - 
laplace - 
a laplace - ‡ 
 ‡ 
logreg - 
lr noise - 
linear svm n a n a 
table a results for na¨ıve bayes left and b svm right the best entry for a corpus is in bold entries that are statistically 
significantly better than all other entries are underlined a † denotes the method is significantly better than all other methods except 
for na¨ıve bayes a ‡ denotes the entry is significantly better than all other methods except for a gauss and na¨ıve bayes for the table 
on the left the reason for this distinction in significance tests is described in the text 
we start with general observations that result from examining 
the performance of these methods over the various corpora the 
first is that a laplace lr noise and logreg quite clearly 
outperform the other methods there is usually little difference 
between the performance of lr noise and logreg both as shown 
here and on a decision by decision basis but this is unsurprising 
since lr noise just adds noisy class labels to the logreg model 
with respect to the three different measures lr noise and 
logreg tend to perform slightly better but never significantly than 
a laplace at some tasks with respect to log-loss and squared error 
however a laplace always produces the least number of errors 
for all of the tasks though at times the degree of improvement is 
not significant 
in order to give the reader a better sense of the behavior of these 
methods figures - show the fits produced by the most 
competitive of these methods versus the actual data behavior as estimated 
nonparametrically by binning for class earn in reuters figure 
shows the class-conditional densities and thus only a laplace is 
shown since logreg fits the posterior directly figure shows the 
estimations of the log-odds i e log p earn s d 
p ¬earn s d 
 viewing the 
log-odds rather than the posterior usually enables errors in 
estimation to be detected by the eye more easily 
we can break things down as the sign test does and just look at 
wins and losses on the items that the methods disagree on looked 
at in this way only two methods na¨ıve bayes and a gauss ever 
have more pairwise wins than a laplace those two sometimes 
have more pairwise wins on log-loss and squared error even though 
the total never wins i e they are dragged down by heavy penalties 
in addition this comparison of pairwise wins means that for 
those cases where logreg and lr noise have better scores than 
a laplace it would not be deemed significant by the sign test at 
any level since they do not have more wins for example of the 
 k binary decisions over the msn web dataset a laplace had 
approximately k pairwise wins versus logreg and lr noise 
no method ever has more pairwise wins than a laplace for the 
error comparison nor does any method every achieve a better total 
the basic observation made about na¨ıve bayes in previous work 
is that it tends to produce estimates very close to zero and one 
 this means if it tends to be right enough of the time it will 
produce results that do not appear significant in a sign test that 
ignores size of difference as the one here the totals of the squared 
error and log-loss bear out the previous observation that when it s 
wrong it s really wrong 
there are several interesting points about the performance of the 
asymmetric distributions as well first a gauss performs poorly 
because similar to na¨ıve bayes there are some examples where 
it is penalized a large amount this behavior results from a 
general tendency to perform like the picture shown in figure note 
the crossover at the tails while the asymmetric gaussian tends 
to place the mode much more accurately than a symmetric 
gaussian its asymmetric flexibility combined with its distance function 
causes it to distribute too much mass to the outside tails while 
failing to fit around the mode accurately enough to compensate figure 
 is actually a result of fitting the two distributions to real data as 
a result at the tails there can be a large discrepancy between the 
likelihood of belonging to each class thus when there are no 
outliers a gauss can perform quite competitively but when there is an 
 
 
 
 
 
 
 
- - - 
p s d class { -} 
s d naive bayes log-odds 
train 
test 
a laplace 
 
 
 
 
 
 
 
 
 
 
- - - 
p s d class { -} 
s d linear svm raw score 
train 
test 
a laplace 
figure the empirical distribution of classifier scores for documents in the training and the test set for class earn in reuters 
also shown is the fit of the asymmetric laplace distribution to the training score distribution the positive class i e earn is the 
distribution on the right in each graph and the negative class i e ¬earn is that on the left in each graph 
- 
- 
- 
 
 
 
 
 
- - - - - 
logodds logp s d -logp - s d 
s d naive bayes log-odds 
train 
test 
a laplace 
logreg 
- 
 
 
 
 
- - 
logodds logp s d -logp - s d 
s d linear svm raw score 
train 
test 
a laplace 
logreg 
figure the fit produced by various methods compared to the empirical log-odds of the training data for class earn in reuters 
outlier a gauss is penalized quite heavily there are enough such 
cases overall that it seems clearly inferior to the top three methods 
however the asymmetric laplace places much more emphasis 
around the mode figure because of the different distance 
function think of the sharp peak of an exponential as a result most 
of the mass stays centered around the mode while the asymmetric 
parameters still allow more flexibility than the standard laplace 
since the standard laplace also corresponds to a piecewise fit in the 
log-odds space this highlights that part of the power of the 
asymmetric methods is their sensitivity in placing the knots at the actual 
modes - rather than the symmetric assumption that the means 
correspond to the modes additionally the asymmetric methods have 
greater flexibility in fitting the slopes of the line segments as well 
even in cases where the test distribution differs from the training 
distribution figure a laplace still yields a solution that gives 
a better fit than logreg figure the next best competitor 
finally we can make a few observations about the usefulness 
of the various performance metrics first log-loss only awards a 
finite amount of credit as the degree to which something is 
correct improves i e there are diminishing returns as it approaches 
zero but it can infinitely penalize for a wrong estimate thus it 
is possible for one outlier to skew the totals but misclassifying this 
example may not matter for any but a handful of actual utility 
functions used in practice secondly squared error has a weakness in 
the other direction that is its penalty and reward are bounded in 
 but if the number of errors is small enough it is possible for 
a method to appear better when it is producing what we generally 
consider unhelpful probability estimates for example consider a 
method that only estimates probabilities as zero or one which na¨ıve 
bayes tends to but doesn t quite reach if you use smoothing this 
method could win according to squared error but with just one 
error it would never perform better on log-loss than any method that 
assigns some non-zero probability to each outcome for these 
reasons we recommend that neither of these are used in isolation as 
they each give slightly different insights to the quality of the 
estimates produced these observations are straightforward from the 
definitions but are underscored by the evaluation 
 future work 
a promising extension to the work presented here is a hybrid 
distribution of a gaussian on the outside slopes and exponentials 
 on the inner slopes from the empirical evidence presented in 
 the expectation is that such a distribution might allow more 
emphasis of the probability mass around the modes as with the 
exponential while still providing more accurate estimates toward 
the tails 
just as logistic regression allows the log-odds of the posterior 
distribution to be fit directly with a line we could directly fit the 
log-odds of the posterior with a three-piece line a spline instead of 
indirectly doing the same thing by fitting the asymmetric laplace 
this approach may provide more power since it retains the 
asymmetry assumption but not the assumption that the class-conditional 
densities are from an asymmetric laplace 
finally extending these methods to the outputs of other 
discriminative classifiers is an open area we are currently evaluating the 
appropriateness of these methods for the output of a voted 
perceptron by analogy to the log-odds the operative score that 
appears promising is log 
weight perceptrons voting 
weight perceptrons voting − 
 
 summary and conclusions 
we have reviewed a wide variety of parametric methods for 
producing probability estimates from the raw scores of a discriminative 
classifier and for recalibrating an uncalibrated probabilistic 
classifier in addition we have introduced two new families that attempt 
to capitalize on the asymmetric behavior that tends to arise from 
learning a discrimination function we have given an efficient way 
to estimate the parameters of these distributions 
while these distributions attempt to strike a balance between the 
generalization power of parametric distributions and the flexibility 
that the added asymmetric parameters give the asymmetric 
gaussian appears to have too great of an emphasis away from the modes 
in striking contrast the asymmetric laplace distribution appears to 
be preferable over several large text domains and a variety of 
performance measures to the primary competing parametric methods 
though comparable performance is sometimes achieved with one 
of two varieties of logistic regression given the ease of 
estimating the parameters of this distribution it is a good first choice for 
producing quality probability estimates 
acknowledgments 
we are grateful to francisco pereira for the sign test code anton 
likhodedov for logistic regression code and john platt for the code 
support for the linear svm classifier toolkit smox also we 
sincerely thank chris meek and john platt for the very useful advice 
provided in the early stages of this work thanks also to jaime 
carbonell and john lafferty for their useful feedback on the final 
versions of this paper 
 references 
 p n bennett assessing the calibration of naive bayes 
posterior estimates technical report cmu-cs- - 
carnegie mellon school of computer science 
 p n bennett using asymmetric distributions to improve 
classifier probabilities a comparison of new and standard 
parametric methods technical report cmu-cs- - 
carnegie mellon school of computer science 
 h bourlard and n morgan a continuous speech 
recognition system embedding mlp into hmm in nips 
 
 g brier verification of forecasts expressed in terms of 
probability monthly weather review - 
 m h degroot and s e fienberg the comparison and 
evaluation of forecasters statistician - 
 m h degroot and s e fienberg comparing probability 
forecasters basic binary concepts and multivariate 
extensions in p goel and a zellner editors bayesian 
inference and decision techniques elsevier science 
publishers b v 
 p domingos and m pazzani beyond independence 
conditions for the optimality of the simple bayesian 
classifier in icml 
 r duda p hart and d stork pattern classification john 
wiley sons inc 
 s t dumais and h chen hierarchical classification of web 
content in sigir 
 s t dumais j platt d heckerman and m sahami 
inductive learning algorithms and representations for text 
categorization in cikm 
 y freund and r schapire large margin classification using 
the perceptron algorithm machine learning - 
 
 i good rational decisions journal of the royal statistical 
society series b 
 t joachims text categorization with support vector 
machines learning with many relevant features in ecml 
 
 s kotz t j kozubowski and k podgorski the laplace 
distribution and generalizations a revisit with 
applications to communications economics engineering 
and finance birkh¨auser 
 d d lewis a sequential algorithm for training text 
classifiers corrigendum and additional data sigir forum 
 - fall 
 d d lewis reuters- distribution 
http www daviddlewis com resources 
testcollections reuters january 
 d d lewis and w a gale a sequential algorithm for 
training text classifiers in sigir 
 d d lewis r e schapire j p callan and r papka 
training algorithms for linear text classifiers in sigir 
 
 d lindley a tversky and r brown on the reconciliation 
of probability assessments journal of the royal statistical 
society 
 r manmatha t rath and f feng modeling score 
distributions for combining the outputs of search engines in 
sigir 
 a mccallum and k nigam a comparison of event models 
for naive bayes text classification in aaai workshop on 
learning for text categorization 
 j c platt probabilistic outputs for support vector machines 
and comparisons to regularized likelihood methods in a j 
smola p bartlett b scholkopf and d schuurmans editors 
advances in large margin classifiers mit press 
 m saar-tsechansky and f provost active learning for class 
probability estimation and ranking in ijcai 
 r l winkler scoring rules and the evaluation of probability 
assessors journal of the american statistical association 
 
 y yang and x liu a re-examination of text categorization 
methods in sigir 
 b zadrozny and c elkan obtaining calibrated probability 
estimates from decision trees and naive bayesian classifiers 
in icml 
 b zadrozny and c elkan reducing multiclass to binary by 
coupling probability estimates in kdd 
