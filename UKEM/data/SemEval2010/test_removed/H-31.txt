a study of poisson query generation model for 
information retrieval 
qiaozhu mei hui fang chengxiang zhai 
department of computer science 
university of illinois at urbana-champaign 
urbana il 
 qmei hfang czhai  uiuc edu 
abstract 
many variants of language models have been proposed for 
information retrieval most existing models are based on 
multinomial distribution and would score documents based 
on query likelihood computed based on a query generation 
probabilistic model in this paper we propose and study a 
new family of query generation models based on poisson 
distribution we show that while in their simplest forms the 
new family of models and the existing multinomial models 
are equivalent they behave differently for many smoothing 
methods we show that the poisson model has several 
advantages over the multinomial model including naturally 
accommodating per-term smoothing and allowing for more 
accurate background modeling we present several variants of 
the new model corresponding to different smoothing 
methods and evaluate them on four representative trec test 
collections the results show that while their basic 
models perform comparably the poisson model can outperform 
multinomial model with per-term smoothing the 
performance can be further improved with two-stage smoothing 
categories and subject descriptors h 
 information search and retrieval retrieval models 
general terms algorithms 
 introduction 
as a new type of probabilistic retrieval models language 
models have been shown to be effective for many retrieval 
tasks among many variants of language 
models proposed the most popular and fundamental one is the 
query-generation language model which leads to the 
query-likelihood scoring method for ranking documents in 
such a model given a query q and a document d we 
compute the likelihood of generating query q with a model 
estimated based on document d i e the conditional 
probability p q d we can then rank documents based on the 
likelihood of generating the query 
virtually all the existing query generation language 
models are based on either multinomial distribution 
or multivariate bernoulli distribution the 
multinomial distribution is especially popular and also shown to be 
quite effective the heavy use of multinomial distribution is 
partly due to the fact that it has been successfully used in 
speech recognition where multinomial distribution is a 
natural choice for modeling the occurrence of a particular word 
in a particular position in text compared with 
multivariate bernoulli multinomial distribution has the advantage 
of being able to model the frequency of terms in the query 
in contrast multivariate bernoulli only models the presence 
and absence of query terms thus cannot capture different 
frequencies of query terms however multivariate bernoulli 
also has one potential advantage over multinomial from the 
viewpoint of retrieval in a multinomial distribution the 
probabilities of all the terms must sum to making it hard 
to accommodate per-term smoothing while in a 
multivariate bernoulli the presence probabilities of different terms 
are completely independent of each other easily 
accommodating per-term smoothing and weighting note that term 
absence is also indirectly captured in a multinomial model 
through the constraint that all the term probabilities must 
sum to 
in this paper we propose and study a new family of query 
generation models based on the poisson distribution in this 
new family of models we model the frequency of each term 
independently with a poisson distribution to score a 
document we would first estimate a multivariate poisson model 
based on the document and then score it based on the 
likelihood of the query given by the estimated poisson model 
in some sense the poisson model combines the advantage of 
multinomial in modeling term frequency and the advantage 
of the multivariate bernoulli in accommodating per-term 
smoothing indeed similar to the multinomial distribution 
the poisson distribution models term frequencies but 
without the constraint that all the term probabilities must sum 
to and similar to multivariate bernoulli it models each 
term independently thus can easily accommodate per-term 
smoothing 
as in the existing work on multinomial language models 
smoothing is critical for this new family of models we 
derive several smoothing methods for poisson model in parallel 
to those used for multinomial distributions and compare the 
corresponding retrieval models with those based on 
multinomial distributions we find that while with some 
smoothing methods the new model and the multinomial model 
lead to exactly the same formula with some other 
smoothing methods they diverge and the poisson model brings in 
more flexibility for smoothing in particular a key difference 
is that the poisson model can naturally accommodate 
perterm smoothing which is hard to achieve with a multinomial 
model without heuristic twist of the semantics of a 
generative model we exploit this potential advantage to develop a 
new term-dependent smoothing algorithm for poisson model 
and show that this new smoothing algorithm can improve 
performance over term-independent smoothing algorithms 
using either poisson or multinomial model this advantage 
is seen for both one-stage and two-stage smoothing another 
potential advantage of the poisson model is that its 
corresponding background model for smoothing can be improved 
through using a mixture model that has a closed form 
formula this new background model is shown to outperform 
the standard background model and reduce the sensitivity 
of retrieval performance to the smoothing parameter 
the rest of the paper is organized as follows in section 
we introduce the new family of query generation models with 
poisson distribution and present various smoothing 
methods which lead to different retrieval functions in section 
we analytically compare the poisson language model with 
the multinomial language model from the perspective of 
retrieval we then design empirical experiments to compare 
the two families of language models in section we discuss 
the related work in and conclude in 
 query generation with poisson 
process 
in the query generation framework a basic assumption is 
that a query is generated with a model estimated based on 
a document in most existing work people 
assume that each query word is sampled independently from 
a multinomial distribution alternatively we assume that a 
query is generated by sampling the frequency of words from 
a series of independent poisson processes 
 the generation process 
let v w wn be a vocabulary set let w be a 
piece of text composed by an author and c w c wn 
be a frequency vector representing w where c wi w is the 
frequency count of term wi in text w in retrieval w could 
be either a query or a document we consider the frequency 
counts of the n unique terms in w as n different types of 
events sampled from n independent homogeneous poisson 
processes respectively 
suppose t is the time period during which the author 
composed the text with a homogeneous poisson process the 
frequency count of each event i e the number of 
occurrences of wi follows a poisson distribution with associated 
parameter λit where λi is a rate parameter characterizing 
the expected number of wi in a unit time the probability 
density function of such a poisson distribution is given by 
p c wi w k λit 
e−λit 
 λit k 
k 
without losing generality we set t to the length of the text 
w people write one word in a unit time i e t w 
with n such independent poisson processes each 
explaining the generation of one term in the vocabulary the 
likelihood of w to be generated from such poisson processes can 
be written as 
p w λ 
n 
i 
p c wi w λ 
n 
i 
e−λi· w 
 λi · w c wi w 
c wi w 
where λ λ λn and w n 
i c wi w we refer 
to these n independent poisson processes with parameter λ 
as a poisson language model 
let d d dm be an observed set of document 
samples generated from the poisson process above the 
maximum likelihood estimate mle of λi is 
ˆλi d∈d c wi d 
d∈d w ∈v c w d 
note that this mle is different from the mle for the 
poisson distribution without considering the document lengths 
which appears in 
given a document d we may estimate a poisson language 
model λd using d as a sample the likelihood that a query 
q is generated from the document language model λd can 
be written as 
p q d 
w∈v 
p c w q λd 
this representation is clearly different from the multinomial 
query generation model as the likelihood includes all the 
terms in the vocabulary v instead of only those appearing 
in q and instead of the appearance of terms the event 
space of this model is the frequencies of each term 
in practice we have the flexibility to choose the 
vocabulary v in one extreme we can use the vocabulary of the 
whole collection however this may bring in noise and 
considerable computational cost in the other extreme we may 
focus on the terms in the query and ignore other terms but 
some useful information may be lost by ignoring the 
nonquery terms as a compromise we may conflate all the 
non-query terms as one single pseudo term in other words 
we may assume that there is exactly one non-query term 
in the vocabulary for each query in our experiments we 
adopt this pseudo non-query term strategy 
a document can be scored with the likelihood in 
equation however if a query term is unseen in the document 
the mle of the poisson distribution would assign zero 
probability to the term causing the probability of the query to 
be zero as in existing language modeling approaches the 
main challenge of constructing a reasonable retrieval model 
is to find a smoothed language model for p · d 
 smoothing in poisson retrieval model 
in general we want to assign non-zero rates for the query 
terms that are not seen in document d many smoothing 
methods have been proposed for multinomial language 
models in general we have to discount the 
probabilities of some words seen in the text to leave some extra 
probability mass to assign to the unseen words in poisson 
language models however we do not have the same constraint 
as in a multinomial model i e w∈v p w d thus 
we do not have to discount the probability of seen words in 
order to give a non-zero rate to an unseen word instead we 
only need to guarantee that k p c w d k d 
in this section we introduce three different strategies to 
smooth a poisson language model and show how they lead 
to different retrieval functions 
 bayesian smoothing using gamma prior 
following the risk minimization framework in we 
assume that a document is generated by the arrival of terms 
in a time period of d according to the document language 
model which essentially consists of a vector of poisson rates 
for each term i e λd λd λd v 
a document is assumed to be generated from a 
potentially different model given a particular document d we 
want to estimate λd the rate of a term is estimated 
independently of other terms we use bayesian estimation with 
the following gamma prior which has two parameters α 
and β 
gamma λ α β 
βα 
γ α 
λα− 
e−βλ 
for each term w the parameters αw and βw are chosen 
to be αw µ λc w and βw µ where µ is a parameter 
and λc w is the rate of w estimated from some background 
language model usually the collection language model 
the posterior distribution of λd is given by 
p λd d c ∝ 
w∈v 
e−λw d µ 
λ 
c w d µλc w− 
w 
which is a product of v gamma distributions with 
parameters c w d µλc w and d µ for each word w given 
that the gamma mean is α 
β 
 we have 
ˆλd w 
λd w 
λd wp λd w d c dλd w 
c w d µλc w 
 d µ 
this is precisely the smoothed estimate of multinomial 
language model with dirichlet prior 
 interpolation jelinek-mercer smoothing 
another straightforward method is to decompose the query 
generation model as a mixture of two component models 
one is the document language model estimated with 
maximum likelihood estimator and the other is a model 
estimated from the collection background p · c which assigns 
non-zero rate to w 
for example we may use an interpolation coefficient 
between and i e δ ∈ with this simple 
interpolation we can score a document with 
score d q 
w∈v 
log − δ p c w q d δp c w q c 
using the maximum likelihood estimator for p · d we 
have λd w c w d 
 d 
 thus equation becomes 
score d q ∝ 
w∈d∩q 
 log 
 − δ 
δ 
e−λd w q 
 λd w q c w q 
c w q · p c w q c 
 
− log 
 − δ e−λd w q 
 δp c w q c 
 − δ δp c w q c 
 
 
w∈d 
log 
 − δ e−λd w q 
 δp c w q c 
 − δ δp c w q c 
we can also use a poisson language model for p · c or use 
some other frequency-based models in the retrieval formula 
above the first summation can be computed efficiently the 
second summation can be actually treated as a document 
prior which penalizes long documents 
as the second summation is difficult to compute efficiently 
we conflate all non-query terms as one pseudo 
non-queryterm denoted as n using the pseudo-term formulation 
and a poisson collection model we can rewrite the retrieval 
formula as 
score d q ∝ 
w∈d∩q 
log 
 − δ 
δ 
e−λd w λd w q c w q 
e−λd c q 
 λd c c w q 
 
 log 
 − δ e−λd n q 
 δe−λc n q 
 − δ δe−λc n q 
 
where λd n 
 d − w∈q c w d 
 d 
and λc n 
 c − w∈q c w c 
 c 
 
 two-stage smoothing 
as discussed in smoothing plays two roles in retrieval 
 to improve the estimation of the document language 
model and to explain the common terms in the query 
in order to distinguish the content and non-discriminative 
words in a query we follow and assume that a query 
is generated by sampling from a two-component mixture 
of poisson language models with one component being the 
document model λd and the other being a query background 
language model p · u p · u models the typical term 
frequencies in the user s queries we may then score each 
document with the query likelihood computed using the 
following two-stage smoothing model 
p c w q λd u − δ p c w q λd δp c w q u 
where δ is a parameter roughly indicating the amount of 
noise in q this looks similar to the interpolation 
smoothing except that p · λd now should be a smoothed language 
model instead of the one estimated with mle 
with no prior knowledge on p · u we could set it to 
p · c any smoothing methods for the document language 
model can be used to estimate p · d such as the gamma 
smoothing as discussed in section 
the empirical study of the smoothing methods is 
presented in section 
 analysis of poisson language 
model 
from the previous section we notice that the poisson 
language model has a strong connection to the multinomial 
language model this is expected since they both belong to the 
exponential family however there are many differences 
when these two families of models are applied with 
different smoothing methods from the perspective of retrieval 
will these two language models perform equivalently if not 
which model provides more benefits to retrieval or provides 
flexibility which could lead to potential benefits in this 
section we analytically discuss the retrieval features of the 
poisson language models by comparing their behavior with 
that of the multinomial language models 
 the equivalence of basic models 
let us begin with the assumption that all the query terms 
appear in every document under this assumption no 
smoothing is needed a document can be scored by the log 
likelihood of the query with the maximum likelihood estimate 
score d q 
w∈v 
log 
e−λd w q 
 λd w q c w q 
c w q 
 
using the mle we have λd w c w d 
w∈v c w d 
 thus 
score d q ∝ 
c w q 
c w q log 
c w d 
w∈v c w d 
this is exactly the log likelihood of the query if the 
document language model is a multinomial with maximum 
likelihood estimate indeed even with gamma smoothing when 
plugging λd w 
c w d µλc w 
 d µ 
and λc w c w c 
 c 
into 
equation it is easy to show that 
score d q ∝ 
w∈q∩d 
c w q log 
c w d 
µ · 
c w c 
 c 
 q log 
µ 
 d µ 
 
which is exactly the dirichlet retrieval formula in note 
that this equivalence holds only when the document length 
variation is modeled with poisson process 
this derivation indicates the equivalence of the basic 
poisson and multinomial language models for retrieval with 
other smoothing strategies however the two models would 
be different nevertheless with this equivalence in basic 
models we could expect that the poisson language model 
performs comparably to the multinomial language model in 
retrieval if only simple smoothing is explored based on this 
equivalence analysis one may ask why we should pursue 
the poisson language model in the following sections we 
show that despite the equivalence in their basic models the 
poisson language model brings in extra flexibility for 
exploring advanced techniques on various retrieval features which 
could not be achieved with multinomial language models 
 term dependent smoothing 
one flexibility of the poisson language model is that it 
provides a natural framework to accommodate term 
dependent per-term smoothing existing work on language model 
smoothing has already shown that different types of queries 
should be smoothed differently according to how 
discriminative the query terms are also predicted that 
different terms should have a different smoothing weights with 
multinomial query generation models people usually use a 
single smoothing coefficient to control the combination of 
the document model and the background model 
this parameter can be made specific for different queries 
but always has to be a constant for all the terms this 
is mandatory since a multinomial language model has the 
constraint that w∈v p w d however from retrieval 
perspective different terms may need to be smoothed 
differently even if they are in the same query for example a 
non-discriminative term e g the is is expected to be 
explained more with the background model while a content 
term e g retrieval bush in the query should be 
explained with the document model therefore a better way 
of smoothing would be to set the interpolation coefficient 
 i e δ in formula and formula specifically for each 
term since the poisson language model does not have the 
sum-to-one constraint across terms it can easily 
accommodate per-term smoothing without needing to heuristically 
twist the semantics of a generative model as in the case of 
multinomial language models below we present a 
possible way to explore term dependent smoothing with poisson 
language models 
essentially we want to use a term-specific smoothing 
coefficient δ in the linear combination denoted as δw this 
coefficient should intuitively be larger if w is a common word 
and smaller if it is a content word the key problem is to find 
a method to assign reasonable values to δw empirical 
tuning is infeasible for so many parameters we may instead 
estimate the parameters ∆ δ δ v by 
maximizing the likelihood of the query given the mixture model of 
p q λq and p q u where λq is the true query model to 
generate the query and p q u is a query background model 
as discussed in section 
with the model p q λq hidden the query likelihood is 
p q ∆ u 
λq w∈v 
 − δw p c w q λq δwp c w q u p λq u dλq 
if we have relevant documents for each query we can 
approximate the query model space with the language models 
of all the relevant documents without relevant documents 
we opt to approximate the query model space with the 
models of all the documents in the collection setting p · u as 
p · c the query likelihood becomes 
p q ∆ u 
d∈c 
πd 
w∈v 
 −δw p c w q ˆλd δwp c w q c 
where πd p ˆλd u p · ˆλd is an estimated poisson 
language model for document d 
if we have prior knowledge on p ˆλd u such as which 
documents are relevant to the query we can set πd accordingly 
because what we want is to find ∆ that can maximize the 
likelihood of the query given relevant documents without 
this prior knowledge we can leave πd as free parameters and 
use the em algorithm to estimate πd and ∆ the updating 
functions are given as 
π 
 k 
d 
πd w∈v − δw p c w q ˆλd δwp c w q c 
d∈c πd w∈v − δw p c w q ˆλd δwp c w q c 
and 
δ 
 k 
w 
d∈c 
πd 
δwp c w q c 
 − δw p c w q ˆλd δwp c w q c 
as discussed in we only need to run the em 
algorithm for several iterations thus the computational cost is 
relatively low we again assume our vocabulary containing 
all query terms plus a pseudo non-query term note that the 
function does not give an explicit way of estimating the 
coefficient for the unseen non-query term in our experiments 
we set it to the average over δw of all query terms 
with this flexibility we expect poisson language models 
could improve the retrieval performance especially for 
verbose queries where the query terms have various 
discriminative values in section we use empirical experiments to 
prove this hypothesis 
 mixture background models 
another flexibility is to explore different background 
 collection models i e p · u or p · c one common 
assumption made in language modeling information retrieval 
is that the background model is a homogeneous model of 
the document models similarly we can also make 
the assumption that the collection model is a poisson 
language model with the rates λc w d∈c c w d 
 c 
 however 
this assumption usually does not hold since the collection 
is far more complex than a single document indeed the 
collection usually consists of a mixture of documents with 
various genres authors and topics etc treating the 
collection model as a mixture of document models instead of 
a single pseudo-document model is more reasonable 
existing work of multinomial language modeling has already 
shown that a better modeling of background improves the 
retrieval performance such as clusters neighbor 
documents and aspects all the approaches 
can be easily adopted using poisson language models 
however a common problem of these approaches is that they 
all require heavy computation to construct the background 
model with poisson language modeling we show that it is 
possible to model the mixture background without paying 
for the heavy computational cost 
poisson mixture has been proposed to model a 
collection of documents which can fit the data much better than 
a single poisson the basic idea is to assume that the 
collection is generated from a mixture of poisson models which 
has the general form of 
p x k pm 
λ 
p λ p x k λ dλ 
p · λ is a single poisson model and p λ is an arbitrary 
probability density function there are three well known poisson 
mixtures -poisson negative binomial and the katz s 
k-mixture note that the -poisson model has actually 
been explored in probabilistic retrieval models which led to 
the well-known bm formula 
all these mixtures have closed forms and can be 
estimated from the collection of documents efficiently this is 
an advantage over the multinomial mixture models such as 
plsi and lda for retrieval for example the 
probability density function of katz s k-mixture is given as 
p c w k αw βw − αw ηk 
αw 
βw 
 
βw 
βw 
 k 
where ηk when k and otherwise 
with the observation of a collection of documents αw and 
βw can be estimated as 
βw 
cf w − df w 
df w 
and αw 
cf w 
nβw 
where cf w and df w are the collection frequency and 
document frequency of w and n is the number of 
documents in the collection to account for the different 
document lengths we assume that βw is a reasonable estimation 
for generating a document of the average length and use 
β βw 
avdl 
 q to generate the query this poisson mixture 
model can be easily used to replace p · c in the retrieval 
functions and 
 other possible flexibilities 
in addition to term dependent smoothing and efficient 
mixture background a poisson language model has also 
some other potential advantages for example in section 
we see that formula introduces a component which does 
document length penalization intuitively when the 
document has more unique words it will be penalized more on 
the other hand if a document is exactly n copies of another 
document it would not get over penalized this feature is 
desirable and not achieved with the dirichlet model 
potentially this component could penalize a document 
according to what types of terms it contains with term specific 
settings of δ we could get even more flexibility for document 
length normalization 
pseudo-feedback is yet another interesting direction where 
the poission model might be able to show its advantage 
with model-based feedback we could again relax the 
combination coefficients of the feedback model and the background 
model and allow different terms to contribute differently to 
the feedback model we could also utilize the relevant 
documents to learn better per-term smoothing coefficients 
 evaluation 
in section we analytically compared the poisson 
language models and multinomial language models from the 
perspective of query generation and retrieval in this 
section we compare these two families of models empirically 
experiment results show that the poisson model with 
perterm smoothing outperforms multinomial model and the 
performance can be further improved with two-stage 
smoothing using poisson mixture as background model also 
improves the retrieval performance 
 datasets 
since retrieval performance could significantly vary from 
one test collection to another and from one query to 
another we select four representative trec test collections 
ap trec trec and wt g web to cover different types 
of queries we follow and construct short-keyword 
 sk keyword title short-verbose sv one sentence 
description and long-verbose lv multiple sentences queries 
the documents are stemmed with the porter s stemmer and 
we do not remove any stop word for each parameter we 
vary its value to cover a reasonably wide range 
 comparison to multinomial 
we compare the performance of the poisson retrieval 
models and multinomial retrieval models using interpolation 
 jelinekmercer jm smoothing and bayesian smoothing with 
conjugate priors table shows that the two jm-smoothed 
models perform similarly on all data sets since the dirichlet 
smoothing for multinomial language model and the gamma 
smoothing for poisson language model lead to the same 
retrieval formula the performance of these two models are 
jointly presented we see that dirichlet gamma smoothing 
methods outperform both jelinek-mercer smoothing 
methods the parameter sensitivity curves for two jelinek-mercer 
 
 
 
 
 
 
 
 
dataset trec 
parameter δ 
averageprecision 
jm−multinomial lv 
jm−multinomial sv 
jm−multinomial sk 
jm−poisson sk 
jm−poisson sv 
jm−poisson lv 
figure poisson and multinomial performs 
similarly with jelinek-mercer smoothing 
smoothing methods are shown in figure clearly these 
two methods perform similarly either in terms of optimality 
data query jm-multinomial jm-poisson dirichlet gamma per-term -stage poisson 
map initpr pr  d map initpr pr  d map initpr pr  d map initpr pr  d 
ap - sk 
sv 
lv 
trec sk 
sv 
lv 
trec sk 
sv 
lv 
web sk 
sv 
lv 
table performance comparison between poisson and multinomial retrieval models basic models perform 
comparably term dependent two-stage smoothing significantly improves poisson 
an asterisk indicates that the difference between the performance of the term dependent two-stage smoothing and that of the 
dirichlet gamma single smoothing is statistically significant according to the wilcoxon signed rank test at the level of 
or sensitivity this similarity of performance is expected as 
we discussed in section 
although the poisson model and multinomial model are 
similar in terms of the basic model and or with simple 
smoothing methods the poisson model has great potential and 
flexibility to be further improved as shown in the 
rightmost column of table term dependent two-stage poisson 
model consistently outperforms the basic smoothing models 
especially for verbose queries this model is given in 
formula with a gamma smoothing for the document model 
p · d and δw which is term dependent the parameter µ of 
the first stage gamma smoothing is empirically tuned the 
combination coefficients i e ∆ are estimated with the em 
algorithm in section the parameter sensitivity curves 
for dirichlet gamma and the per-term two-stage 
smoothing model are plotted in figure the per-term two-stage 
smoothing method is less sensitive to the parameter µ than 
dirichlet gamma and yields better optimal performance 
 
 
 
 
 
 
 
 
dataset ap query type sv 
parameter µ 
averageprecision 
dirichlet gamma smoothing 
term dependent −stage 
figure term dependent two-stage smoothing of 
poisson outperforms dirichlet gamma 
in the following subsections we conduct experiments to 
demonstrate how the flexibility of the poisson model could 
be utilized to achieve better performance which we cannot 
achieve with multinomial language models 
 term dependent smoothing 
to test the effectiveness of the term dependent 
smoothing we conduct the following two experiments in the first 
experiment we relax the constant coefficient in the simple 
jelinek-mercer smoothing formula i e formula and use 
the em algorithm proposed in section to find a δw for 
each unique term since we are using the em algorithm to 
iteratively estimate the parameters we usually do not want 
the probability of p · d to be zero we then use a simple 
laplace method to slightly smooth the document model 
before it goes into the em iterations the documents are then 
still scored with formula but using learnt δw the results 
are labeled with jm l in table 
data q jm jm jm l -stage -stage 
 map pt no yes yes no yes 
ap sk 
sv 
trec sk 
sv 
trec sk 
sv 
web sk 
sv 
table term dependent smoothing improves 
retrieval performance 
an asterisk in column indicates that the difference between 
the jm l method and jm method is statistically significant 
an asterisk in column means that the difference between 
term dependent two-stage method and query dependent two-stage 
method is statistically significant pt stands for per-term 
with term dependent coefficients the performance of the 
jelinek-mercer poisson model is improved in most cases 
however in some cases e g trec sv it performs poorly 
this might be caused by the problem of em estimation with 
unsmoothed document models once non-zero probability 
is assigned to all the terms before entering the em iteration 
the performance on verbose queries can be improved 
significantly this indicates that there is still room to find better 
methods to estimate δw please note that neither the 
perterm jm method nor the jm l method has a parameter 
to tune 
as shown in table the term dependent two-stage 
smoothing can significantly improve retrieval performance to 
understand whether the improvement is contributed by the 
term dependent smoothing or the two-stage smoothing 
framework we design another experiment to compare the 
perterm two-stage smoothing with the two-stage smoothing 
method proposed in their method managed to find 
coefficients specific to the query thus a verbose query would 
use a higher δ however since their model is based on 
multinomial language modeling they could not get per-term 
coefficients we adopt their method to the poisson two-stage 
smoothing and also estimate a per-query coefficient for all 
the terms we compare the performance of such a model 
with the per-term two-stage smoothing model and present 
the results in the right two columns in table again we 
see that the per-term two-stage smoothing outperforms 
the per-query two-stage smoothing especially for verbose 
queries the improvement is not as large as how the 
perterm smoothing method improves over dirichlet gamma 
this is expected since the per-query smoothing has already 
addressed the query discrimination problem to some extent 
this experiment shows that even if the smoothing is already 
per-query making it per-term is still beneficial in brief the 
per-term smoothing improved the retrieval performance of 
both one-stage and two-stage smoothing method 
 mixture background model 
in this section we conduct experiments to examine the 
benefits of using a mixture background model without extra 
computational cost which can not be achieved for 
multinomial models specifically in retrieval formula instead of 
using a single poisson distribution to model the background 
p · c we use katz s k-mixture model which is essentially 
a mixture of poisson distributions p · c can be computed 
efficiently with simple collection statistics as discussed in 
section 
data query jm poisson jm k-mixture 
ap sk 
sv 
trec- sk 
sv 
trec- sk 
sv 
web sk 
sv 
table k-mixture background model improves 
retrieval performance 
the performance of the jm retrieval model with single 
poisson background and with katz s k-mixture background 
model is compared in table clearly using k-mixture to 
model the background model outperforms the single 
poisson background model in most cases especially for verbose 
queries where the improvement is statistically significant 
figure shows that the performance changes over 
different parameters for short verbose queries the model using 
k-mixture background is less sensitive than the one using 
single poisson background given that this type of mixture 
 
 
 
 
 
 
 
data trec query sv 
parameter δ 
averageprecision 
poisson background 
k−mixture background 
figure k-mixture background model deviates the 
sensitivity of verbose queries 
background model does not require any extra computation 
cost it would be interesting to study whether using other 
mixture poisson models such as -poisson and negative 
binomial could help the performance 
 related work 
to the best of our knowledge there has been no study of 
query generation models based on poisson distribution 
language models have been shown to be effective for many 
retrieval tasks the most popular and 
fundamental one is the query-generation language model 
 all existing query generation language models are based 
on either multinomial distribution or 
multivariate bernoulli distribution we introduce a 
new family of language models based on poisson 
distribution poisson distribution has been previously studied in the 
document generation models leading to the 
development of one of the most effective retrieval formula 
bm studies the parallel derivation of three 
different retrieval models which is related to our comparison 
of poisson and multinomial however the poisson model 
in their paper is still under the document generation 
framework and also does not account for the document length 
variation introduces a way to empirically search for an 
exponential model for the documents poisson mixtures 
such as -poisson negative multinomial and katz s 
kmixture has shown to be effective to model and retrieve 
documents once again none of this work explores poisson 
distribution in the query generation framework 
language model smoothing and background 
structures have been studied with 
multinomial language models analytically shows that term 
specific smoothing could be useful we show that 
poisson language model is natural to accommodate the per-term 
smoothing without heuristic twist of the semantics of a 
generative model and is able to efficiently better model the 
mixture background both analytically and empirically 
 conclusions 
we present a new family of query generation language 
models for retrieval based on poisson distribution we 
derive several smoothing methods for this family of models 
including single-stage smoothing and two-stage smoothing 
we compare the new models with the popular multinomial 
retrieval models both analytically and experimentally our 
analysis shows that while our new models and multinomial 
models are equivalent under some assumptions they are 
generally different with some important differences in 
particular we show that poisson has an advantage over 
multinomial in naturally accommodating per-term smoothing we 
exploit this property to develop a new per-term smoothing 
algorithm for poisson language models which is shown to 
outperform term-independent smoothing for both poisson 
and multinomial models furthermore we show that a 
mixture background model for poisson can be used to improve 
the performance and robustness over the standard poisson 
background model our work opens up many interesting 
directions for further exploration in this new family of models 
further exploring the flexibilities over multinomial language 
models such as length normalization and pseudo-feedback 
could be good future work it is also appealing to find 
robust methods to learn the per-term smoothing coefficients 
without additional computation cost 
 acknowledgments 
we thank the anonymous sigir reviewers for their 
useful comments this material is based in part upon work 
supported by the national science foundation under award 
numbers iis- and 
 references 
 d blei a ng and m jordan latent dirichlet 
allocation journal of machine learning research 
 - 
 s f chen and j goodman an empirical study of 
smoothing techniques for language modeling 
technical report tr- - harvard university 
 k church and w gale poisson mixtures nat lang 
eng - 
 w b croft and j lafferty editors language 
modeling and information retrieval kluwer academic 
publishers 
 h fang t tao and c zhai a formal study of 
information retrieval heuristics in proceedings of the 
 th annual international acm sigir conference on 
research and development in information retrieval 
pages - 
 d hiemstra using language models for information 
retrieval phd thesis university of twente enschede 
netherlands 
 d hiemstra term-specific smoothing for the 
language modeling approach to information retrieval 
the importance of a query term in proceedings of the 
 th annual international acm sigir conference on 
research and development in information retrieval 
pages - 
 t hofmann probabilistic latent semantic indexing 
in proceedings of acm sigir pages - 
 s m katz distribution of content words and phrases 
in text and language modelling nat lang eng 
 - 
 o kurland and l lee corpus structure language 
models and ad-hoc information retrieval in 
proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - 
 j lafferty and c zhai document language models 
query models and risk minimization for information 
retrieval in proceedings of sigir pages - 
sept 
 j lafferty and c zhai probabilistic ir models based 
on query and document generation in proceedings of 
the language modeling and ir workshop pages - 
may - june 
 j lafferty and c zhai probabilistic relevance models 
based on document and query generation in w b 
croft and j lafferty editors language modeling and 
information retrieval kluwer academic publishers 
 
 v lavrenko and b croft relevance-based language 
models in proceedings of sigir pages - 
sept 
 x liu and w b croft cluster-based retrieval using 
language models in proceedings of the th annual 
international acm sigir conference on research and 
development in information retrieval pages - 
 
 e l margulis modelling documents with multiple 
poisson distributions inf process manage 
 - 
 a mccallum and k nigam a comparison of event 
models for naive bayes text classification in 
proceedings of aaai- workshop on learning for 
text categorization 
 d metzler v lavrenko and w b croft formal 
multiple-bernoulli models for language modeling in 
proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - 
 d h miller t leek and r schwartz a hidden 
markov model information retrieval system in 
proceedings of the acm sigir conference on 
research and development in information retrieval 
pages - 
 a papoulis probability random variables and 
stochastic processes new york mcgraw-hill 
 nd ed 
 j m ponte and w b croft a language modeling 
approach to information retrieval in proceedings of 
the st annual international acm sigir conference 
on research and development in information retrieval 
pages - 
 s robertson and s walker some simple effective 
approximations to the -poisson model for 
probabilistic weighted retrieval in proceedings of 
sigir pages - 
 s e robertson s walker s jones 
m m hancock-beaulieu and m gatford okapi at 
trec- in d k harman editor the third text 
retrieval conference trec- pages - 
 t roelleke and j wang a parallel derivation of 
probabilistic information retrieval models in 
proceedings of the th annual international acm 
sigir conference on research and development in 
information retrieval pages - 
 t tao x wang q mei and c zhai language 
model information retrieval with document expansion 
in proceedings of hlt naacl pages - 
 
 j teevan and d r karger empirical development of 
an exponential probabilistic model for text retrieval 
using textual analysis to build a better model in 
proceedings of the th annual international acm 
sigir conference on research and development in 
informaion retrieval pages - 
 x wei and w b croft lda-based document models 
for ad-hoc retrieval in proceedings of the th annual 
international acm sigir conference on research and 
development in information retrieval pages - 
 
 c zhai and j lafferty a study of smoothing 
methods for language models applied to ad-hoc 
information retrieval in proceedings of acm 
sigir pages - sept 
 c zhai and j lafferty two-stage language models 
for information retrieval in proceedings of acm 
sigir pages - aug 
