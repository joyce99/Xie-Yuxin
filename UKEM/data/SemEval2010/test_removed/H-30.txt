latent concept expansion using markov random fields 
donald metzler 
metzler cs umass edu 
w bruce croft 
croft cs umass edu 
center for intelligent information retrieval 
department of computer science 
university of massachusetts 
amherst ma 
abstract 
query expansion in the form of pseudo-relevance feedback 
or relevance feedback is a common technique used to 
improve retrieval effectiveness most previous approaches have 
ignored important issues such as the role of features and the 
importance of modeling term dependencies in this paper 
we propose a robust query expansion technique based on 
the markov random field model for information retrieval 
the technique called latent concept expansion provides a 
mechanism for modeling term dependencies during 
expansion furthermore the use of arbitrary features within the 
model provides a powerful framework for going beyond 
simple term occurrence features that are implicitly used by 
most other expansion techniques we evaluate our 
technique against relevance models a state-of-the-art language 
modeling query expansion technique our model 
demonstrates consistent and significant improvements in retrieval 
effectiveness across several trec data sets we also 
describe how our technique can be used to generate 
meaningful multi-term concepts for tasks such as query 
suggestion reformulation 
categories and subject descriptors 
h information storage and retrieval information 
search and retrieval 
general terms 
algorithms experimentation theory 
 introduction 
users of information retrieval systems are required to 
express complex information needs in terms of boolean 
expressions a short list of keywords a sentence a question or 
possibly a longer narrative a great deal of information is 
lost during the process of translating from the information 
need to the actual query for this reason there has been 
a strong interest in query expansion techniques such 
techniques are used to augment the original query to produce a 
representation that better reflects the underlying 
information need 
query expansion techniques have been well studied for 
various models in the past and have shown to significantly 
improve effectiveness in both the relevance feedback and 
pseudo-relevance feedback setting 
recently a markov random field mrf model for 
information retrieval was proposed that goes beyond the 
simplistic bag of words assumption that underlies bm and the 
 unigram language modeling approach to information 
retrieval the mrf model generalizes the unigram 
bigram and other various dependence models most 
past term dependence models have failed to show consistent 
significant improvements over unigram baselines with few 
exceptions the mrf model however has been shown 
to be highly effective across a number of tasks including ad 
hoc retrieval named-page finding and japanese 
language web search 
until now the model has been solely used for ranking 
documents in response to a given query in this work we show 
how the model can be extended and used for query 
expansion using a technique that we call latent concept expansion 
 lce there are three primary contributions of our work 
first lce provides a mechanism for combining term 
dependence with query expansion previous query expansion 
techniques are based on bag of words models therefore by 
performing query expansion using the mrf model we are 
able to study the dynamics between term dependence and 
query expansion 
next as we will show the mrf model allows arbitrary 
features to be used within the model query expansion 
techniques in the past have implicitly only made use of term 
occurrence features by using more robust feature sets it 
is possible to produce better expansion terms that 
discriminate between relevant and non-relevant documents better 
finally our proposed approach seamlessly provides a 
mechanism for generating both single and multi-term concepts 
most previous techniques by default generate terms 
independently there have been several approaches that make 
use of generalized concepts however such approaches were 
somewhat heuristic and done outside of the model 
our approach is both formally motivated and a natural 
extension of the underlying model 
the remainder of this paper is laid out as follows in 
section we describe related query expansion approaches 
section provides an overview of the mrf model and 
details our proposed latent concept expansion technique in 
section we evaluate our proposed model and analyze the 
results finally section concludes the paper and 
summarizes the major results 
 related work 
one of the classic and most widely used approaches to 
query expansion is the rocchio algorithm rocchio s 
approach which was developed within the vector space model 
reweights the original query vector by moving the weights 
towards the set of relevant or pseudo-relevant documents 
and away from the non-relevant documents unfortunately 
it is not possible to formally apply rocchio s approach to 
a statistical retrieval model such as language modeling for 
information retrieval 
a number of formalized query expansion techniques have 
been developed for the language modeling framework 
including zhai and lafferty s model-based feedback and lavrenko 
and croft s relevance models both approaches 
attempt to use pseudo-relevant or relevant documents to 
estimate a better query model 
model-based feedback finds the model that best describes 
the relevant documents while taking a background noise 
model into consideration this separates the content model 
from the background model the content model is then 
interpolated with the original query model to form the 
expanded query 
the other technique relevance models is more closely 
related to our work therefore we go into the details of the 
model much like model-based feedback relevance models 
estimate an improved query model the only difference 
between the two approaches is that relevance models do not 
explicitly model the relevant or pseudo-relevant documents 
instead they model a more generalized notion of relevance 
as we now show 
given a query q a relevance model is a multinomial 
distribution p · q that encodes the likelihood of each term 
given the query as evidence it is computed as 
p w q 
d 
p w d p d q 
≈ 
d∈rq 
p w d p q d p d 
w d∈rq 
p w d p q d p d 
 
where rq is the set of documents that are relevant or 
pseudorelevant to query q in the pseudo-relevant case these are 
the top ranked documents for query q furthermore it is 
assumed that p d is uniform over this set these mild 
assumptions make computing the bayesian posterior more 
practical 
after the model is estimated documents are ranked by 
clipping the relevance model by choosing the k most likely 
terms from p · q this clipped distribution is then 
interpolated with with the original maximum likelihood query 
model this can be thought of as expanding the original 
query by k weighted terms throughout the remainder of 
this work we refer to this instantiation of relevance models 
as rm 
there has been relatively little work done in the area of 
query expansion in the context of dependence models 
however there have been several attempts to expand using 
multi-term concepts xu and croft s local context 
analysis lca method combined passage-level retrieval with 
concept expansion where concepts were single terms and 
phrases expansion concepts were chosen and weighted 
using a metric based on co-occurrence statistics however 
it is not clear based on the analysis done how much the 
phrases helped over the single terms alone 
papka and allan investigate using relevance feedback to 
perform multi-term concept expansion for document 
routing the concepts used in their work are more general 
than those used in lca and include inquery query 
language structures such as uw white house which 
corresponds to the concept the terms white and house occur in 
any order within terms of each other results showed 
that combining single term and large window multi-term 
concepts significantly improved effectiveness however it is 
unclear whether the same approach is also effective for ad 
hoc retrieval due to the differences in the tasks 
 model 
this section details our proposed latent concept expansion 
technique as mentioned previously the technique is an 
extension of the mrf model for information retrieval 
therefore we begin by providing an overview of the mrf 
model and our proposed extensions 
 mrfs for ir 
 basics 
markov random fields which are undirected graphical 
models provide a compact robust way of modeling a joint 
distribution here we are interested in modeling the joint 
distribution over a query q q qn and a document 
d it is assumed the underlying distribution over pairs of 
documents and queries is a relevance distribution that is 
sampling from the distribution gives pairs of documents and 
queries such that the document is relevant to the query 
a mrf is defined by a graph g and a set of non-negative 
potential functions over the cliques in g the nodes in the 
graph represent the random variables and the edges define 
the independence semantics of the distribution a mrf 
satisfies the markov property which states that a node is 
independent of all of its non-neighboring nodes given observed 
values for its neighbors 
given a graph g a set of potentials ψi and a parameter 
vector λ the joint distribution over q and d is given by 
pg λ q d 
 
zλ c∈c g 
ψ c λ 
where z is a normalizing constant we follow common 
convention and parameterize the potentials as ψi c λ 
exp λifi c where fi c is a real-valued feature function 
 constructing g 
given a query q the graph g can be constructed in a 
number of ways however following previous work we 
consider three simple variants these variants are full 
independence where each query term is independent of each 
other given a document sequential dependence which 
assumes a dependence exists between adjacent query terms 
and full dependence which makes no independence 
assumptions 
 parameterization 
mrfs are commonly parameterized based on the 
maximal cliques of g however such a parameterization is too 
coarse for our needs we need a parameterization that allows 
us to associate feature functions with cliques on a more fine 
grained level while keeping the number of features and thus 
the number of parameters reasonable therefore we allow 
cliques to share feature functions and parameters based on 
clique sets that is all of the cliques within a clique set are 
associated with the same feature function and share a 
single parameter this effectively ties together the parameters 
of the features associated with each set which significantly 
reduces the number of parameters while still providing a 
mechanism for fine-tuning on the level of clique sets 
we propose seven clique sets for use with information 
retrieval the first three clique sets consist of cliques that 
contain one or more query terms and the document node 
features over these cliques should encode how well the terms 
in the clique configuration describe the document these 
sets are 
 td - set of cliques containing the document node and 
exactly one query term 
 od - set of cliques containing the document node and 
two or more query terms that appear in sequential 
order within the query 
 ud - set of cliques containing the document node and 
two or more query terms that appear in any order 
within the query 
note that ud is a superset of od by tying the parameters 
among the cliques within each set we can control how much 
influence each type gets this also avoids the problem of 
trying to determine how to estimate weights for each clique 
within the sets instead we now must only estimate a single 
parameter per set 
next we consider cliques that only contain query term 
nodes these cliques which were not considered in are 
defined in an analogous way to those just defined except the 
the cliques are only made up of query term nodes and do 
not contain the document node feature functions over these 
cliques should capture how compatible query terms are to 
one another these clique features may take on the form of 
language models that impose well-formedness of the terms 
therefore we define following query-dependent clique sets 
 tq - set of cliques containing exactly one query term 
 oq - set of cliques containing two or more query terms 
that appear in sequential order within the query 
 uq - set of cliques containing two or more query terms 
that appear in any order within the query 
finally there is the clique that only contains the 
document node features over this node can be used as a type 
of document prior encoding document-centric properties 
this trivial clique set is then 
 d - clique set containing only the singleton node d 
we note that our clique sets form a set cover over the 
cliques of g but are not a partition since some cliques 
appear in multiple clique sets 
after tying the parameters in our clique sets together and 
using the exponential potential function form we end up 
with the following simplified form of the joint distribution 
log pg λ q d 
λtd 
c∈td 
ftd 
 c λod 
c∈od 
fod 
 c λud 
c∈ud 
fud 
 c 
fdq d q - document and query dependent 
 
λtq 
c∈tq 
ftq 
 c λoq 
c∈oq 
foq 
 c λuq 
c∈uq 
fuq 
 c 
fq q - query dependent 
 
λdfd d 
fd d - document dependent 
− log zλ 
document query independent 
where fdq fq and fd are convenience functions defined 
by the document and query dependent query dependent 
and document dependent components of the joint 
distribution respectively these will be used to simplify and clarify 
expressions derived throughout the remainder of the paper 
 features 
any arbitrary feature function over clique configurations 
can be used in the model the correct choice of features 
depends largely on the retrieval task and the evaluation 
metric therefore there is likely not to be a single universally 
applicable set of features 
to provide an idea of the range of features that can be 
used we now briefly describe possible types of features that 
could be used possible query term dependent features 
include tf idf named entities term proximity and text style 
to name a few many types of document dependent features 
can be used as well including document length pagerank 
readability and genre among others 
since it is not our goal here to find optimal features we 
use a simple fixed set of features that have been shown to 
be effective in previous work see table for a list 
of features used these features attempt to capture term 
occurrence and term proximity better feature selection in 
the future will likely lead to improved effectiveness 
 ranking 
given a query q we wish to rank documents in 
descending order according to pg λ d q after dropping document 
independent expressions from log pg λ q d we derive the 
following ranking function 
pg λ d q 
rank 
 fdq d q fd d 
which is a simple weighted linear combination of feature 
functions that can be computed efficiently for reasonable 
graphs 
 parameter estimation 
now that the model has been fully specified the final step 
is to estimate the model parameters although mrfs are 
generative models it is inappropriate to train them using 
feature value 
ftd 
 qi d log − α 
tfqi d 
 d 
 α 
cfqi 
 c 
fod 
 qi qi qi k d log − β 
tf qi qi k d 
 d 
 β 
cf qi qi k 
 c 
fud 
 qi qj d log − β 
tf uw qi qj d 
 d 
 β 
cf uw qi qj 
 c 
ftq 
 qi − log 
cfqi 
 c 
foq 
 qi qi qi k − log 
cf qi qi k 
 c 
fuq 
 qi qj − log 
cf uw qi qj 
 c 
fd 
table feature functions used in markov random field model here tfw d is the number of times term 
w occurs in document d tf qi qi k d denotes the number of times the exact phrase qi qi k occurs in 
document d tf uw qi qj d is the number of times the terms qi qj appear ordered or unordered within a 
window of n terms and d is the length of document d the cf and c values are analogously defined on 
the collection level finally α and β are model hyperparameters that control smoothing for single term and 
phrase features respectively 
conventional likelihood-based approaches because of metric 
divergence that is the maximum likelihood estimate 
is unlikely to be the estimate that maximizes our evaluation 
metric for this reason we discriminatively train our model 
to directly maximize the evaluation metric under 
consideration since our parameter space is small we 
make use of a simple hill climbing strategy although other 
more sophisticated approaches are possible 
 latent concept expansion 
in this section we describe how this extended mrf model 
can be used in a novel way to generate single and 
multiterm concepts that are topically related to some original 
query as we will show the concepts generated using our 
technique can be used for query expansion or other tasks 
such as suggesting alternative query formulations 
we assume that when a user formulates their original 
query they have some set of concepts in mind but are only 
able to express a small number of them in the form of a 
query we treat the concepts that the user has in mind but 
did not explicitly express in the query as latent concepts 
these latent concepts can consist of a single term 
multiple terms or some combination of the two it is therefore 
our goal to recover these latent concepts given some original 
query 
this can be accomplished within our framework by first 
expanding the original graph g to include the type of 
concept we are interested in generating we call this expanded 
graph h in figure the middle graph provides an example 
of how to construct an expanded graph that can generate 
single term concepts similarly the graph on the right 
illustrates an expanded graph that generates two term concepts 
although these two examples make use of the sequential 
dependence assumption i e dependencies between adjacent 
query terms it is important to note that both the original 
query and the expansion concepts can use any independence 
structure 
after h is constructed we compute ph λ e q a 
probability distribution over latent concepts according to 
ph λ e q d∈r ph λ q e d 
d∈r e ph λ q e d 
where r is the universe of all possible documents and e 
is some latent concept that may consist of one or more 
terms since it is not practical to compute this 
summation we must approximate it we notice that ph λ q e d 
is likely to be peaked around those documents d that are 
highly ranked according to query q therefore we 
approximate ph λ e q by only summing over a small subset of 
relevant or pseudo-relevant documents for query q this is 
computed as follows 
ph λ e q ≈ 
d∈rq 
ph λ q e d 
d∈rq e ph λ q e d 
 
∝ 
d∈rq 
exp fqd q d fd d fqd e d fq e 
where rq is a set of relevant or pseudo-relevant documents 
for query q and all clique sets are constructed using h 
as we see the likelihood contribution for each document in 
rq is a combination of the original query s score for the 
document see equation concept e s score for the 
document and e s document-independent score therefore this 
equation can be interpreted as measuring how well q and e 
account for the top ranked documents and the goodness 
of e independent of the documents for maximum 
robustness we use a different set of parameters for fqd q d and 
fqd e d which allows us to weight the term ordered and 
unordered window features differently for the original query 
and the candidate expansion concept 
 query expansion 
to use this framework for query expansion we first choose 
an expansion graph h that encodes the latent concept 
structure we are interested in expanding the query using we 
then select the k latent concepts with the highest likelihood 
given by equation a new graph g is constructed by 
augmenting the original graph g with the k expansion 
concepts e ek finally documents are ranked according 
to pg λ d q e ek using equation 
 comparison to relevance models 
inspecting equations and reveals the close 
connection that exists between lce and relevance models both 
figure graphical model representations of relevance modeling left latent concept expansion using single 
term concepts middle and latent concept expansion using two term concepts right for a three term query 
models essentially compute the likelihood of a term or 
concept in the same manner it is easy to see that just as the 
mrf model can be viewed as a generalization of language 
modeling so too can lce be viewed as a generalization of 
relevance models 
there are important differences between mrfs lce and 
unigram language models relevance models see figure 
for graphical model representations of both models 
unigram language models and relevance models are based on 
the multinomial distribution this distributional 
assumption locks the model into the bag of words representation 
and the implicit use of term occurrence features however 
the distribution underlying the mrf model allows us to 
move beyond both of these assumptions by modeling both 
dependencies between query terms and allowing arbitrary 
features to be explicitly used 
moving beyond the simplistic bag of words assumption in 
this way results in a general robust model and as we show 
in the next section translates into significant improvements 
in retrieval effectiveness 
 experimental results 
in order to better understand the strengths and 
weaknesses of our technique we evaluate it on a wide range of 
data sets table provides a summary of the trec data 
sets considered the wsj ap and robust collections 
are smaller and consist entirely of newswire articles whereas 
wt g and gov are large web collections for each data 
set we split the available topics into a training and test set 
where the training set is used solely for parameter 
estimation and the test set is used for evaluation purposes 
all experiments were carried out using a modified version 
of indri which is part of the lemur toolkit all 
collections were stopped using a standard list of 
common terms and stemmed using a porter stemmer in all 
cases only the title portion of the trec topics are used 
to construct queries we construct g using the sequential 
dependence assumption for all data sets 
 ad-hoc retrieval results 
we now investigate how well our model performs in 
practice in a pseudo-relevance feedback setting we compare 
unigram language modeling with dirichlet smoothing the 
mrf model without expansion relevance models and 
lce to better understand how each model performs across 
the various data sets 
for the unigram language model the smoothing 
parameter was trained for the mrf model we train the model 
parameters i e λ and model hyperparameters i e α β 
for rm and lce we also train the number of 
pseudoname description docs train 
topics 
test 
topics 
wsj wall st 
journal - 
 - - 
ap assoc press 
 - 
 - - 
robust robust 
data 
 - - 
wt g trec web 
collection 
 - - 
gov crawl of 
 gov domain 
 - - 
table overview of trec collections and topics 
relevant feedback documents used and the number of 
expansion terms 
 expansion with single term concepts 
we begin by evaluating how well our model performs when 
expanding using only single terms before we describe and 
analyze the results we explicitly state how expansion term 
likelihoods are computed under this setup i e using the 
sequential dependence assumption expanding with single 
term concepts and using our feature set the expansion 
term likelihoods are computed as follows 
ph λ e q ∝ 
d∈rq 
exp λtd 
w∈q 
log − α 
tfw d 
 d 
 α 
cfw 
 c 
 
λod 
b∈q 
log − β 
tf b d 
 d 
 β 
cf b 
 c 
 
λud 
b∈q 
log − β 
tf uw b d 
 d 
 β 
cf uw b 
 c 
 
log 
 − α 
tfe d 
 d 
 α cfe 
 c 
λtd 
cfe 
 c 
λtq 
 
where b ∈ q denotes the set of bigrams in q this equation 
clearly shows how lce differs from relevance models when 
we set λtd λt d and all other parameters to 
we obtain the exact formula that is used to compute term 
likelihoods in the relevance modeling framework therefore 
lce adds two very important factors to the equation first 
it adds the ordered and unordered window features that are 
applied to the original query second it applies an intuitive 
tf idf-like form to the candidate expansion term w the idf 
factor which is not present in relevance models plays an 
important role in expansion term selection 
 − 
 − 
− 
 − 
− 
 − 
− 
 − 
 
 
 
 
 
 
 
 
 
rm 
lce 
 
ap 
 − 
 − 
− 
 − 
− 
 − 
− 
 − 
 
 
 
 
 
 
 
 
 
rm 
lce 
 
robust 
 − 
 − 
− 
 − 
− 
 − 
− 
 − 
 
 
 
 
 
 
 
 
 
rm 
lce 
 
wt g 
figure histograms that demonstrate and compare the robustness of relevance models rm and latent 
concept expansion lce with respect to the query likelihood model ql for the ap robust and wt g 
data sets 
the results evaluated using mean average precision are 
given in table as we see the mrf model relevance 
models and lce always significantly outperform the unigram 
language model in addition lce shows significant 
improvements over relevance models across all data sets the 
relative improvements over relevance models is for ap 
 for wsj for robust for wt g and 
 for gov 
furthermore lce shows small but not significant 
improvements over relevance modeling for metrics such as 
precision at and however both relevance modeling 
and lce show statistically significant improvements in such 
metrics over the unigram language model 
another interesting result is that the mrf model is 
statistically equivalent to relevance models on the two web data 
sets in fact the mrf model outperforms relevance 
models on the wt g data set this reiterates the importance 
of non-unigram proximity-based features for content-based 
web search observed previously 
although our model has more free parameters than 
relevance models there is surprisingly little overfitting instead 
the model exhibits good generalization properties 
 expansion with multi-term concepts 
we also investigated expanding using both single and two 
word concepts for each query we expanded using a set of 
single term concepts and a set of two term concepts the 
sets were chosen independently unfortunately only 
negligible increases in mean average precision were observed 
this result may be due to the fact that strong 
correlations exist between the single term expansion concepts we 
found that the two word concepts chosen often consisted of 
two highly correlated terms that are also chosen as single 
term concepts for example the two term concept stock 
market was chosen while the single term concepts stock 
and market were also chosen therefore many two word 
concepts are unlikely to increase the discriminative power 
of the expanded query this result suggests that concepts 
should be chosen according to some criteria that also takes 
novelty diversity or term correlations into account 
another potential issue is the feature set used other 
feature sets may ultimately yield different results especially 
if they reduce the correlation among the expansion concepts 
therefore our experiments yield no conclusive results with 
regard to expansion using multi-term concepts instead the 
results introduce interesting open questions and directions 
for future exploration 
lm mrf rm lce 
wsj α 
 α 
 αβγ 
ap α 
 αβ 
 αβγ 
robust α 
 αβ 
 αβγ 
wt g α 
 α 
 αβγ 
gov α 
 α 
 αβγ 
table test set mean average precision for 
language modeling lm markov random field mrf 
relevance models rm and latent concept 
expansion lce the superscripts α β and γ indicate 
statistically significant improvements p over 
lm mrf and rm respectively 
 robustness 
as we have shown relevance models and latent concept 
expansion can significantly improve retrieval effectiveness 
over the baseline query likelihood model in this section 
we analyze the robustness of these two methods here we 
define robustness as the number queries whose effectiveness 
are improved hurt and by how much as the result of 
applying these methods a highly robust expansion technique 
will significantly improve many queries and only minimally 
hurt a few 
figure provides an analysis of the robustness of 
relevance modeling and latent concept expansion for the ap 
robust and wt g data sets the analysis for the 
two data sets not shown is similar the histograms 
provide for various ranges of relative decreases increases in 
mean average precision the number of queries that were 
hurt improved with respect to the query likelihood baseline 
as the results show lce exhibits strong robustness for 
each data set for ap relevance models improve queries 
and hurt whereas lce improves and hurts 
although relevance models improve the effectiveness of more 
queries than lce the relative improvement exhibited by 
lce is significantly larger for the robust data set 
relevance models improve queries and hurt and lce 
improves and hurts finally for the wt g 
collection relevance models improve queries and hurt and 
lce improves and hurts as with ap the amount of 
improvement exhibited by the lce versus relevance models 
is significantly larger for both the robust and wt g 
data sets in addition when lce does hurt performance it 
is less likely to hurt as much as relevance modeling which 
is a desirable property 
 word concepts word concepts word concepts 
telescope hubble telescope hubble space telescope 
hubble space telescope hubble telescope space 
space hubble space space telescope hubble 
mirror telescope mirror space telescope nasa 
nasa telescope hubble hubble telescope astronomy 
launch mirror telescope nasa hubble space 
astronomy telescope nasa space telescope mirror 
shuttle telescope space telescope space nasa 
test hubble mirror hubble telescope mission 
new nasa hubble mirror mirror mirror 
discovery telescope astronomy space telescope launch 
time telescope optical space telescope discovery 
universe hubble optical shuttle space telescope 
optical telescope discovery hubble telescope flaw 
light telescope shuttle two hubble space 
table fifteen most likely one two and three word concepts constructed using the top documents 
retrieved for the query hubble telescope achievements on the robust collection 
overall lce improves effectiveness for - of queries 
depending on the data set when used in combination with 
a highly accurate query performance prediction system it 
may be possible to selectively expand queries and minimize 
the loss associated with sub-baseline performance 
 multi-term concept generation 
although we found that expansion using multi-term 
concepts failed to produce conclusive improvements in 
effectiveness there are other potential tasks that these concepts may 
be useful for such as query suggestion reformulation 
summarization and concept mining for example for a query 
suggestion task the original query could be used to 
generate a set of latent concepts which correspond to alternative 
query formulations 
although evaluating our model on these tasks is beyond 
the scope of this work we wish to show an illustrative 
example of the types of concepts generated using our model in 
table we present the most likely one two and three term 
concepts generated using lce for the query hubble telescope 
achievements using the top ranked documents from the 
robust collection 
it is well known that generating multi-term concepts 
using a unigram-based model produces unsatisfactory results 
since it fails to consider term dependencies this is not 
the case when generating multi-term concepts using our 
model instead a majority of the concepts generated are 
well-formed and meaningful there are several cases where 
the concepts are less coherent such as mirror mirror mirror 
in this case the likelihood of the term mirror appearing in 
a pseudo-relevant document outweighs the language 
modeling features e g foq which causes this non-coherent 
concept to have a high likelihood such examples are in the 
minority however 
not only are the concepts generated well-formed and 
meaningful but they are also topically relevant to the original 
query as we see all of the concepts generated are on topic 
and in some way related to the hubble telescope it is 
interesting to see that the concept hubble telescope flaw is one of 
the most likely three term concepts given that it is 
somewhat contradictory to the original query despite this 
contradiction documents that discuss the telescope flaws are 
also likely to describe the successes as well and therefore 
this is likely to be a meaningful concept 
one important thing to note is that the concepts lce 
generates are of a different nature than those that would 
be generated using a bigram relevance model for example 
a bigram model would be unlikely to generate the concept 
telescope space nasa since none of the bigrams that make 
up the concept have high likelihood however since our 
model is based on a number of different features over various 
types of cliques it is more general and robust than a bigram 
model 
although we only provided the concepts generated for a 
single query we note that the same analysis and conclusions 
generalize across other data sets with coherent topically 
related concepts being consistently generated using lce 
 discussion 
our latent concept expansion technique captures two 
semiorthogonal types of dependence in information retrieval 
there has been a long-term interest in understanding the 
role of term dependence out of this research two broad 
types of dependencies have been identified 
the first type of dependence is syntactic dependence this 
type of dependence covers phrases term proximity and term 
co-occurrence these methods capture the 
fact that queries implicitly or explicitly impose a certain set 
of positional dependencies 
the second type is semantic dependence examples of 
semantic dependence are relevance feedback pseudo-relevance 
feedback synonyms and to some extent stemming these 
techniques have been explored on both the query and 
document side on the query side this is typically done using 
some form of query expansion such as relevance models or 
lce on the document side this is done as document 
expansion or document smoothing 
although there may be some overlap between syntactic 
and semantic dependencies they are mostly orthogonal our 
model uses both types of dependencies the use of phrase 
and proximity features within the model captures 
syntactic dependencies whereas lce captures query-side semantic 
dependence this explains why the initial improvement in 
effectiveness achieved by using the mrf model is not lost 
after query expansion if the same types of dependencies 
were capture by both syntactic and semantic dependencies 
lce would be expected to perform about equally as well 
as relevance models therefore by modeling both types of 
dependencies we see an additive effect rather than an 
absorbing effect 
an interesting area of future work is to determine whether 
or not modeling document-side semantic dependencies can 
add anything to the model previous results that have 
combined query- and document-side semantic dependencies have 
shown mixed results 
 conclusions 
in this paper we proposed a robust query expansion 
technique called latent concept expansion the technique was 
shown to be a natural extension of the markov random field 
model for information retrieval and a generalization of 
relevance models lce is novel in that it performs single or 
multi-term expansion within a framework that allows the 
modeling of term dependencies and the use of arbitrary 
features whereas previous work has been based on the bag of 
words assumption and term occurrence features 
we showed that the technique can be used to produce 
high quality well formed topically relevant multi-term 
expansion concepts the concepts generated can be used in 
an alternative query suggestion module we also showed 
that the model is highly effective in fact it achieves 
significant improvements in mean average precision over relevance 
models across a selection of trec data sets it was also 
shown the mrf model itself without any query expansion 
outperforms relevance models on large web data sets this 
reconfirms previous observations that modeling 
dependencies via the use of proximity features within the mrf has 
more of an impact on larger noisier collections than smaller 
well-behaved ones 
finally we reiterated the importance of choosing 
expansion terms that model relevance rather than the relevant 
documents and showed how lce captures both syntactic 
and query-side semantic dependencies future work will 
look at incorporating document-side dependencies as well 
acknowledgments 
this work was supported in part by the center for intelligent 
information retrieval in part by nsf grant cns- in part 
by arda and nsf grant ccf- and in part by microsoft 
live labs any opinions findings and conclusions or 
recommendations expressed in this material are those of the author s and do not 
necessarily reflect those of the sponsor 
 references 
 n abdul-jaleel j allan w b croft f diaz l larkey 
x li m d smucker and c wade umass at trec 
novelty and hard in online proceedings of the text 
retrieval conf 
 c l a clarke and g v cormack shortest-substring retrieval 
and ranking acm trans inf syst - 
 k collins-thompson and j callan query expansion using 
random walk models in proc th intl conf on information 
and knowledge management pages - 
 w b croft boolean queries and term dependencies in 
probabilistic retrieval models journal of the american society 
for information science - 
 w b croft h turtle and d lewis the use of phrases and 
structured queries in information retrieval in proc th ann 
intl acm sigir conf on research and development in 
information retrieval pages - 
 k eguchi ntcir- query expansion experiments using term 
dependence models in proc of the fifth ntcir workshop 
meeting on evaluation of information access technologies 
pages - 
 j fagan automatic phrase indexing for document retrieval 
an examination of syntactic and non-syntactic methods in 
proc tenth ann intl acm sigir conf on research and 
development in information retrieval pages - 
 j gao j nie g wu and g cao dependence language 
model for information retrieval in proc th ann intl acm 
sigir conf on research and development in information 
retrieval pages - 
 d harper and c j van rijsbergen an evaluation of feedback 
in document retrieval using co-occurrence data journal of 
documentation - 
 t joachims a support vector method for multivariate 
performance measures in proc of the international conf on 
machine learning pages - 
 o kurland and l lee corpus structure language models 
and ad-hoc information retrieval in proc th ann intl 
acm sigir conf on research and development in 
information retrieval pages - 
 v lavrenko and w b croft relevance-based language 
models in proc th ann intl acm sigir conf on 
research and development in information retrieval pages 
 - 
 x liu and w b croft cluster-based retrieval using language 
models in proc th ann intl acm sigir conf on 
research and development in information retrieval pages 
 - 
 d metzler and w b croft a markov random field model for 
term dependencies in proc th ann intl acm sigir 
conf on research and development in information 
retrieval pages - 
 d metzler and w b croft linear feature based models for 
information retrieval information retrieval to appear 
 d metzler t strohman y zhou and w b croft indri at 
terabyte track in online proceedings of the text 
retrieval conf 
 w morgan w greiff and j henderson direct maximization 
of average precision by hill-climbing with a comparison to a 
maximum entropy approach technical report mitre 
 p ogilvie and j p callan experiments using the lemur 
toolkit in proc of the text retrieval conf 
 r papka and j allan why bigger windows are better than 
smaller ones technical report university of massachusetts 
amherst 
 s robertson s walker s jones m m hancock-beaulieu 
and m gatford okapi at trec- in online proceedings of the 
third text retrieval conf pages - 
 j j rocchio relevance feedback in information retrieval 
pages - prentice-hall 
 f song and w b croft a general language model for 
information retrieval in proc eighth international conference 
on information and knowledge management cikm 
pages - 
 t strohman d metzler h turtle and w b croft indri a 
language model-based serach engine for complex queries in 
proc of the international conf on intelligence analysis 
 
 t tao x wang q mei and c zhai language model 
information retrieval with document expansion in proc of 
hlt naacl pages - 
 b taskar c guestrin and d koller max-margin markov 
networks in proc of advances in neural information 
processing systems nips 
 c j van rijsbergen a theoretical basis for the use of 
cooccurrence data in information retrieval journal of 
documentation - 
 x wei and w b croft lda-based document models for 
ad-hoc retrieval in proc th ann intl acm sigir conf 
on research and development in information retrieval 
pages - 
 j xu and w b croft improving the effectiveness of 
information retrieval with local context analysis acm trans 
inf syst - 
 c zhai and j lafferty model-based feedback in the language 
modeling approach to information retrieval in proc th intl 
conf on information and knowledge management pages 
 - 
