interesting nuggets and their impact on definitional 
question answering 
kian-wei kor 
department of computer science 
school of computing 
national university of singapore 
dkor comp nus edu sg 
tat-seng chua 
department of computer science 
school of computing 
national university of singapore 
chuats comp nus edu sg 
abstract 
current approaches to identifying definitional sentences in the 
context of question answering mainly involve the use of linguistic 
or syntactic patterns to identify informative nuggets this is 
insufficient as they do not address the novelty factor that a 
definitional nugget must also possess this paper proposes to address 
the deficiency by building a human interest model from external 
knowledge it is hoped that such a model will allow the 
computation of human interest in the sentence with respect to the topic we 
compare and contrast our model with current definitional question 
answering models to show that interestingness plays an important 
factor in definitional question answering 
categories and subject descriptors 
h information search and retrieval retrieval models h 
 user machine systems human factors 
general terms 
algorithms human factors experimentation 
 definitional question 
answering 
definitional question answering was first introduced to the text 
retrieval conference question answering track main task in 
the definition questions also called other questions in recent years 
are defined as follows given a question topic x the task of a 
definitional qa system is akin to answering the question what is x 
or who is x the definitional qa system is to search through 
a news corpus and return return a set of answers that best describes 
the question topic each answer should be a unique topic-specific 
nugget that makes up one facet in the definition of the question 
topic 
 the two aspects of topic nuggets 
officially topic-specific answer nuggets or simply topic nuggets 
are described as informative nuggets each informative nugget is 
a sentence fragment that describe some factual information about 
the topic depending on the topic type and domain this can include 
topic properties relationships the topic has with some closely 
related entity or events that happened to the topic 
from observation of the answer set for definitional question 
answering from trec to it seems that a significant 
number of topic nuggets cannot simply be described as informative 
nuggets rather these topic nuggets have a trivia-like quality 
associated with them typically these are out of the ordinary pieces 
of information about a topic that can pique a human reader s 
interest for this reason we decided to define answer nuggets that 
can evoke human interest as interesting nuggets in essence 
interesting nuggets answer the questions what is x famous for 
what defines x or what is extraordinary about x 
we now have two very different perspective as to what 
constitutes an answer to definition questions an answer can be some 
important factual information about the topic or some novel and 
interesting aspect about the topic this duality of informativeness 
and interestingness can be clearly observed in the five vital answer 
nuggets for a trec topic of george foreman certain 
answer nuggets are more informative while other nuggets are more 
interesting in nature 
informative nuggets 
- was graduate of job corps 
- became oldest world champion in boxing history 
interesting nuggets 
- has lent his name to line of food preparation products 
- waved american flag after winning olympics championship 
- returned to boxing after yr hiatus 
as an african-american professional heavyweight boxer an 
average human reader would find the last three nuggets about george 
foreman interesting because boxers do not usually lend their names 
to food preparation products nor do boxers retire for years 
before returning to the ring and become the world s oldest boxing 
champion foreman s waving of the american flag at the olympics 
is interesting because the innocent action caused some 
africanamericans to accuse foreman of being an uncle tom as seen 
here interesting nuggets has some surprise factor or unique quality 
that makes them interesting to human readers 
 identifying interesting nuggets 
since the original official description for definitions comprise of 
identifying informative nuggets most research has focused entirely 
on identifying informative nuggets in this paper we focus on 
exploring the properties of interesting nuggets and develop ways of 
identify such interesting nuggets a human interest model 
definitional question answering system is developed with emphasis 
on identifying interesting nuggets in order to evaluate the impact 
of interesting nuggets on the performance of a definitional 
question answering system we further experimented with combining 
the human interest model with a lexical pattern based definitional 
question answering system in order to capture both informative and 
interesting nuggets 
 related work 
there are currently two general methods for definitional 
question answering the more common method uses a lexical 
patternbased approach was first proposed by blair-goldensohn et al 
and xu et al both groups predominantly used patterns such 
as copulas and appositives as well as manually crafted 
lexicosyntactic patterns to identify sentences that contain informative nuggets 
for example xu et al used manually defined structured 
patterns in their definitional question answering system since 
then in an attempt to capture a wider class of informational nuggets 
many such systems of increasing complexity has been created a 
recent system by harabagiu et al created a definitional 
question answering system that combines the use of manually 
defined positive and negative patterns named entity relations and 
specially crafted information extraction templates for target 
domains here a musician template may contain lexical patterns that 
identify information such as the musician s musical style songs 
sung by the musician and the band if any that the musician belongs 
to as one can imagine this is a knowledge intensive approach that 
requires an expert linguist to manually define all possible lexical or 
syntactic patterns required to identify specific types of information 
this process requires a lot of manual labor expertise and is not 
scalable this lead to the development of the soft-pattern approach 
by cui et al instead of manually encoding patterns 
answers to previous definitional question answering evaluations were 
converted into generic patterns and a probabilistic model is trained 
to identify such patterns in sentences given a potential answer 
sentence the probabilistic model outputs a probability that 
indicates how likely the sentence matches one or more patterns that the 
model has seen in training 
such lexicalosyntactic patterns approach have been shown to be 
adept at identifying factual informative nuggets such as a person s 
birthdate or the name of a company s ceo however these 
patterns are either globally applicable to all topics or to a specific set 
of entities such as musicians or organizations this is in direct 
contrast to interesting nuggets that are highly specific to 
individual topics and not to a set of entities for example the interesting 
nuggets for george foreman are specific only george foreman and 
no other boxer or human being topic specificity or topic relevance 
is thus an important criteria that helps identify interesting nuggets 
this leads to the exploration of the second relevance-based 
approach that has been used in definitional question answering 
predominantly this approach has been used as a backup method for 
identifying definitional sentences when the primary method of 
lexicalosyntactic patterns failed to find a sufficient number of 
informative nuggets a similar approach has also been used as a 
baseline system for trec more recently chen et al 
 adapted a bi-gram or bi-term language model for definitional 
question answering 
generally the relevance-based approach requires a definitional 
corpus that contain documents highly relevant to the topic the 
baseline system in trec simply uses the topic words as its 
definitional corpus blair-goldensohn et al uses a machine 
learner to include in the definitonal corpus sentences that are likely 
to be definitional chen et al collect snippets from google to 
build its definitional corpus 
from the definitional corpus a definitional centroid vector is 
built or a set of centroid words are selected this centroid 
vector or set of centroid words is taken to be highly indicative of the 
topic systems can then use this centroid to identify definitional 
answers by using a variety of distance metrics to compare against 
sentences found in the set of retrieved documents for the topic 
blairgoldensohn et al uses cosine similarity to rank sentences by 
centrality chen et al builds a bigram language model using 
the most frequently occurring google snippet terms described 
in their paper as an ordered centroid to estimate the probability that 
a sentence is similar to the ordered centroid 
as described here the relevance-based approach is highly 
specific to individual topics due to its dependence on a topic specific 
definitional corpus however if individual sentences are viewed as 
a document then relevance-based approaches essentially use the 
collected topic specific centroid words as a form of document 
retrieval with automated query expansion to identify strongly 
relevant sentences thus such methods identify relevant sentences and 
not sentences containing definitional nuggets yet the trec 
baseline system outperformed all but one other system the 
bi-term language model is able to report results that are highly 
competitive to state-of-the-art results using this retrieval-based 
approach at trec a simple weighted sum of all terms model 
with terms weighted using solely google snippets outperformed all 
other systems by a significant margin 
we believe that interesting nuggets often come in the form of 
trivia novel or rare facts about the topic that tend to strongly 
cooccur with direct mention of topic keywords this may explain 
why relevance-based method can perform competitively in 
definitional question answering however simply comparing against a 
single centroid vector or set of centroid words may have over 
emphasized topic relevance and has only identified interesting 
definitional nuggets in an indirect manner still relevance based retrieval 
methods can be used as a starting point in identifying interesting 
nuggets we will describe how we expand upon such methods to 
identify interesting nuggets in the next section 
 human interest model 
getting a computer system to identify sentences that a human 
reader would find interesting is a tall order however there are 
many documents on the world wide web that are contain concise 
human written summaries on just about any topic what s more 
these documents are written explicitly for human beings and will 
contain information about the topic that most human readers would 
be interested in assuming we can identify such relevant 
documents on the web we can leverage them to assist in identifying 
definitional answers to such topics we can take the assumption 
that most sentences found within these web documents will 
contain interesting facets about the topic at hand 
this greatly simplifies the problem to that of finding within the 
aquaint corpus sentences similar to those found in web 
documents this approach has been successfully used in several factoid 
and list question answering systems and we feel the use of 
such an approach for definitional or other question answering is 
justified identifying interesting nuggets requires computing 
machinery to understand world knowledge and human insight this 
is still a very challenging task and the use of human written 
documents dramatically simplifies the complexity of the task 
in this paper we report on such an approach by experimenting 
with a simple word-level edit distance based weighted term 
comparison algorithm we use the edit distance algorithm to score the 
similarity of a pair of sentences with one sentence coming from 
web resources and the other sentence selected from the aquaint 
corpus through a series of experiments we will show that even 
such a simple approach can be very effective at definitional 
question answering 
 web resources 
there exists on the internet articles on just about any topic a 
human can think of what s more many such articles are centrally 
located on several prominent websites making them an easily 
accessible source of world knowledge for our work on identifying 
interesting nuggets we focused on finding short one or two page 
articles on the internet that are highly relevant to our desired topic 
such articles are useful as they contain concise information about 
the topic more importantly the articles are written by humans for 
human readers and thus contain the critical human world 
knowledge that a computer system currently is unable to capture 
we leverage this world knowledge by collecting articles for each 
topic from the following external resources to build our interest 
corpus for each topic 
wikipedia is a web-based free-content encyclopedia written 
collaboratively by volunteers this resource has been used by 
many question answering system as a source of knowledge 
about each topic we use a snapshot of wikipedia taken in 
march and include the most relevant article in the 
interest corpus 
newslibrary is a searchable archive of news articles from over 
 different newspaper agencies for each topic we 
download the most relevant articles and include the title and 
first paragraph of each article in the interest corpus 
google snippets are retrieved by issuing the topic as a query to 
the google search engine from the search results we 
extracted the top snippets while google snippets are not 
articles we find that they provide a wide coverage of 
authorative information about most topics 
due to their comprehensive coverage of a wide variety of 
topics the above resources form the bulk of our interest corpus we 
also extracted documents from other resources however as these 
resources are more specific in nature we do not always get any 
single relevant document these resources are listed below 
biography com is the website for the biography television cable 
channel the channel s website contains searchable 
biographies on over notable people if the topic is a person 
and we can find a relevant biography on the person we 
include it it in our interest corpus 
bartleby com contains a searchable copy of several resources 
including the columbia encyclopedia the world factbook 
and several english dictionaries 
s com is a biography dictionary on over notable people 
like biography com we include the most relevant biography 
we can find in the interest corpus 
google definitions google search engine offers a feature called 
definitions that provides the definition for a query if it 
has one we use this feature and extract whatever definitions 
the google search engine has found for each topic into the 
interest corpus 
figure human interest model architecture 
wordnet wordnet is an well-known electronic semantic lexicon 
for the english language besides grouping english words 
into sets of synonyms called synsets it also provide a short 
definition on the meaning of words found in each synset we 
add this short definition if there is one into our interest 
corpus 
we have two major uses for this topic specific interest corpus 
as a source of sentences containing interesting nuggets and as a 
unigram language model of topic terms i 
 multiple interesting centroids 
we have seen that interesting nuggets are highly specific to a 
topic relevance-based approaches such as the bigram language 
model used by chen et al are focused on identifying highly 
relevant sentences and pick up definitional answer nuggets as an 
indirect consequence we believe that the use of only a single 
collection of centroid words has over-emphasized topic relevance and 
choose instead to use multiple centroids 
since sentences in the interest corpus of articles we collected 
from the internet are likely to contain nuggets that are of interest to 
human readers we can essentially use each sentence as 
pseudocentroids each sentence in the interest corpus essentially raises 
a different aspect of the topic for consideration as a sentence of 
interest to human readers by performing a pairwise sentence 
comparison between sentences in the interest corpus and candidate 
sentences retrieved from the aquaint corpus we increase the 
number of sentence comparisons from o n to o nm here n is 
the number of potential candidate sentences and m is the number 
of sentences in the interest corpus in return we obtain a diverse 
ranked list of answers that are individually similar to various 
sentences found in the topic s interest corpus an answer can only be 
highly ranked if it is strongly similar to a sentence in the interest 
corpus and is also strongly relevant to the topic 
 implementation 
figure shows the system architecture for the proposed human 
interest-based definitional qa system 
the aquaint retrieval module shown in figure reuses a 
document retrieval module of a current factoid and list question 
answering system we have implemented given a set of words 
describing the topic the aquaint retrieval module does query 
expansion using google and searches an index of aquaint 
documents to retrieve the most relevant documents for 
consideration 
the web retrieval module on the other hand searches the online 
resources described in section for interesting documents in 
order to populate the interest corpus 
the him ranker or human interest model ranking module is 
the implementation of what is described in this paper the module 
first builds the unigram language model i from the collected web 
documents this language model will be used to weight the 
importance of terms within sentences next a sentence chunker is used 
to segment all retrieved documents into individual sentences 
each of these sentences can be a potential answer sentence that will 
be independently ranked by interestingness we rank sentences by 
interestingness using sentences from both the interest corpus of 
external documents as well as the unigram language model we built 
earlier which we use to weight terms 
a candidate sentence in our top relevant aquaint 
documents is considered interesting if it is highly similar in content to 
a sentence found in our collection of external web-documents to 
achieve this we perform a pairwise similarity comparison between 
a candidate sentence and sentences in our external documents 
using a weighted-term edit distance algorithm term weights are used 
to adjust the relative importance of each unique term found in the 
interest corpus when both sentences share the same term the 
similarity score is incremented by the two times the term s weight 
and every dissimilar term decrements the similarity score by the 
dissimilar term s weight 
we choose the highest achieved similarity score for a candidate 
sentence as the human interest model score for the candidate 
sentence in this manner every candidate sentence is ranked by 
interestingness finally to obtain the answer set we select the top 
highest ranked and non redundant sentences as definitional answers 
for the topic 
 initial experiments 
the human interest-based system described in the previous 
section is designed to identify only interesting nuggets and not 
informative nuggets thus it can be described as a handicapped 
system that only deals with half the problem in definitional question 
answering this is done in order to explore how interestingness 
plays a factor in definitional answers in order to compare and 
contrast the differences between informative and interesting nuggets 
we also implemented the soft-pattern bigram model proposed by 
cui et al in order to ensure comparable results both 
systems are provided identical input data since both system require 
the use of external resources they are both provided the same web 
articles retrieved by our web retrieval module both systems also 
rank the same same set of candidate sentences in the form of 
most relevant documents as retrieved by our aquaint retrieval 
module 
for the experiments we used the trec question set to 
tune any system parameters and use the trec question sets 
to test the both systems both systems are evaluated the results 
using the standard scoring methodology for trec definitions trec 
provides a list of vital and okay nuggets for each question topic 
every question is scored on nugget recall nr and nugget 
precision np and a single final score is computed using f-measure 
 see equation with β to emphasize nugget recall here nr 
is the number of vital nuggets returned divided by total number 
of vital nuggets while np is computed using a minimum allowed 
character length function defined in the evaluation is 
automatically conducted using pourpre v c 
fscore 
β 
 np nr 
 β np nr 
 
system f -score 
best trec system 
soft-pattern sp 
human interest model him 
table performance on trec question set 
figure performance by entity types 
 informativeness vs interestingness 
our first experiment compares the performance of solely 
identifying interesting nuggets against solely identifying informative 
nuggets we compare the results attained by the human interest 
model that only identify interesting nuggets with the results of the 
syntactic pattern finding soft-pattern model as well as the result of 
the top performing definitional system in trec table 
shows the f score the three systems for the trec question 
set 
the human interest model clearly outperform both soft pattern 
and the best trec system with a f score of the 
result is also comparable with the result of a human manual run 
which attained a f score of on the same question set 
this result is confirmation that interesting nuggets does indeed play 
a significant role in picking up definitional answers and may be 
more vital than using information finding lexical patterns 
in order to get a better perspective of how well the human 
interest model performs for different types of topics we manually 
divided the trec topics into four broad categories of 
person organization thing and event as listed in table 
 these categories conform to trec s general division of 
question topics into main entity types the performance of 
human interest model and soft pattern bigram model for each entity 
type can be seen in figure both systems exhibit consistent 
behavior across entity types with the best performance coming from 
person and organization topics and the worst performance 
from thing and event topics this can mainly be attributed 
to our selection of web-based resources for the definitional corpus 
used by both system in general it is harder to locate a single web 
article that describes an event or a general object however given 
the same set of web-based information the human interest model 
consistently outperforms the soft-pattern model for all four entity 
types this suggests that the human interest model is better able 
to leverage the information found in web resources to identify 
definitional answers 
 refinements 
encouraged by the initial experimental results we explored two 
further optimization of the basic algorithm 
 weighting interesting terms 
the word trivia refer to tidbits of unimportant or uncommon 
information as we have noted interesting nuggets often has a 
trivialike quality that makes them of interest to human beings from this 
description of interesting nuggets and trivia we hypothesize that 
interesting nuggets are likely to occur rarely in a text corpora 
there is a possibility that some low-frequency terms may 
actually be important in identifying interesting nuggets a standard 
unigram language model would not capture these low-frequency terms 
as important terms to explore this possibility we experimented 
with three different term weighting schemes that can provide more 
weight to certain low-frequency terms the weighting schemes we 
considered include commonly used tfidf as well as information 
theoretic kullback-leiber divergence and jensen-shannon 
divergence 
tfidf or term frequency × inverse document frequency is 
a standard information retrieval weighting scheme that balances 
the importance of a term in a document and in a corpus for our 
experiments we compute the weight of each term as tf × log n 
nt 
 
where tf is the term frequency nt is the number of sentences in 
the interest corpus having the term and n is the total number of 
sentences in the interest corpus 
kullback-leibler divergence equation is also called kl 
divergence or relative entropy can be viewed as measuring the 
dissimilarity between two probability distributions here we treat the 
aquaint corpus as a unigram language model of general english 
 a and the interest corpus as a unigram language model 
consisting of topic specific terms and general english terms i 
general english words are likely to have similar distributions in both 
language models i and a thus using kl divergence as a term 
weighting scheme will cause strong weights to be given to 
topicspecific terms because their distribution in the interest corpus they 
occur significantly more often or less often than in general english 
in this way high frequency centroid terms as well as low frequency 
rare but topic-specific terms are both identified and highly weighted 
using kl divergence 
dkl i a 
t 
i t log 
i t 
a t 
 
due to the power law distribution of terms in natural language 
there are only a small number of very frequent terms and a large 
number of rare terms in both i and a while the common terms 
in english consist of stop words the common terms in the topic 
specific corpus i consist of both stop words and relevant topic 
words these high frequency topic specific words occur very much 
more frequently in i than in a as a result we found that kl 
divergence has a bias towards highly frequent topic terms as we are 
measuring direct dissimilarity against a model of general english 
where such topic terms are very rare for this reason we explored 
another divergence measure as a possible term weighting scheme 
jensen-shannon divergence or js divergence extends upon kl 
divergence as seen in equation as with kl divergence we also 
use js divergence to measure the dissimilarity between our two 
language models i and a 
djs i a 
 
¢dkl 
 i i a 
 
 dkl 
 a i a 
 
 £ 
figure performance by various term weighting schemes on 
the human interest model 
however js divergence has additional properties 
of being 
symmetric and non-negative as seen in equation the symmetric 
property gives a more balanced measure of dissimilarity and avoids 
the bias that kl divergence has 
djs i a djs a i 
 i a 
 i a 
 
we conducted another experiment substituting the unigram 
languge model weighting scheme we used in the initial experiments 
with the three term weighting schemes described above as lower 
bound reference we included a term weighting scheme consisting 
of a constant for all terms figure show the result of applying 
the five different term weighting schemes on the human interest 
model tfidf performed the worst as we had anticipated the 
reason is that most terms only appear once within each sentence 
resulting in a term frequency of for most terms this causes 
the idf component to be the main factor in scoring sentences 
as we are computing the inverse document frequency for terms 
in the interest corpus collected from web resources idf 
heavily down-weights highly frequency topic terms and relevant terms 
this results in tfidf favoring all low frequency terms over high 
frequency terms in the interest corpus despite this the tfidf 
weighting scheme only scored a slight lower than our lower 
bound reference of constant weights we view this as a positive 
indication that low frequency terms can indeed be useful in finding 
interesting nuggets 
both kl and js divergence performed marginally better than the 
uniform language model probabilistic scheme that we used in our 
initial experiments from inspection of the weighted list of terms 
we observed that while low frequency relevant terms were boosted 
in strength high frequency relevant terms still dominate the top of 
the weighted term list only a handful of low frequency terms were 
weighted as strongly as topic keywords and combined with their 
low frequency may have limited the impact of re-weighting such 
terms however we feel that despite this jensen-shannon 
divergence does provide a small but measurable increase in the 
performance of our human interest model 
 
js divergence also has the property of being bounded allowing 
the results to be treated as a probability if required however the 
bounded property is not required here as we are only treating the 
divergence computed by js divergence as term weights 
 selecting web resources 
in one of our initial experiments we observed that the quality 
of web resources included in the interest corpus may have a direct 
impact on the results we obtain we wanted to determine what 
impact the choice of web resources have on the performance of our 
human interest model for this reason we split our collection of 
web resources into four major groups listed here 
n - news title and first paragraph of the top most relevant 
articles found in newslibrary 
w - wikipedia text from the most relevant article found in 
wikipedia 
s - snippets snippets extracted from the top most relevant 
links after querying google 
m - miscellaneous sources combination of content when 
available from secondary sources including biography com s com 
bartleby com articles google definitions and wordnet definitions 
we conducted a gamut of runs on the trec question set 
using all possible combinations of the above four groups of web 
resources to identify the best possible combination all runs were 
conducted on human interest model using js divergence as term 
weighting scheme the runs were sorted in descending f -score 
and the top best performing runs for each entity class are listed 
in table together with earlier reported f -scores from figure as 
a baseline reference a consistent trend can be observed for each 
entity class 
for person and event topics newslibrary articles are the 
main source of interesting nuggets with google snippets and 
miscellaneous articles offering additional supporting evidence this 
seem intuitive for events as newspapers predominantly focus on 
reporting breaking newsworthy events and are thus excellent sources 
of interesting nuggets we had expected wikipedia rather than 
news articles to be a better source of interesting facts about 
people and were surprised to discover that news articles outperformed 
wikipedia we believe that the reason is because the people 
selected as topics thus far have been celebrities or well known public 
figures human readers are likely to be interested in news events 
that spotlight these personalities 
conversely for organization and thing topics the best 
source of interesting nuggets come from wikipedia s most relevant 
article on the topic with google snippets again providing additional 
information for organizations 
with an oracle that can classify topics by entity class with 
accuracy and by using the best web resources for each entity class 
as shown in table we can attain a f -score of 
 unifying informativeness with 
interestingness 
we have thus far been comparing the human interest model 
against the soft-pattern model in order to understand the 
differences between interesting and informative nuggets however from 
the perspective of a human reader both informative and interesting 
nuggets are useful and definitional informative nuggets present a 
general overview of the topic while interesting nuggets give 
readers added depth and insight by providing novel and unique aspects 
about the topic we believe that a good definitional question 
answering system should provide the reader with a combined mixture 
of both nugget types as a definitional answer set 
rank person org thing event 
baseline 
unigram weighting scheme n w s m 
 
 
n s m w s w m n m 
 
 
n s n w s w s m n s m 
 
 
n m n w s m w s n s 
 
table top runs using different web resources for each 
entity class 
we now have two very different experts at identifying 
definitions the soft pattern bigram model proposed by cui et al is 
an expert in identifying informative nuggets the human 
interest model we have described in this paper on the other hand is an 
expert in finding interesting nuggets we had initially hoped to 
unify the two separate definitional question answering systems by 
applying an ensemble learning method such as voting or 
boosting in order to attain a good mixture of informative and interesting 
nuggets in our answer set however none of the ensemble 
learning methods we attempted could outperform our human interest 
model 
the reason is that both systems are picking up very different 
sentences as definitional answers in essence our two experts are 
disagreeing on which sentences are definitional in the top 
sentences from both systems only of these sentences appeared in 
both answer sets the remaining answers were completely 
different even when we examined the top sentences generated by 
both systems the agreement rate was still an extremely low 
yet despite the low agreement rate between both systems each 
individual system is still able to attain a relatively high f score 
there is a distinct possibility that each system may be selecting 
different sentences with different syntactic structures but actually 
have the same or similar semantic content this could result in both 
systems having the same nuggets marked as correct even though the 
source answer sentences are structurally different unfortunately 
we are unable to automatically verify this as the evaluation software 
we are using does not report correctly identified answer nuggets 
to verify if both systems are selecting the same answer nuggets 
we randomly selected a subset of topics from the trec 
question set and manually identified correct answer nuggets as 
defined by trec accessors from both systems when we compared 
the answer nuggets found by both system for this subset of topics 
we found that the nugget agreement rate between both systems was 
 while the nugget agreement rate is higher than the 
sentence agreement rate both systems are generally still picking up 
different answer nuggets we view this as further indication that 
definitions are indeed made up of a mixture of informative and 
interesting nuggets it is also indication that in general interesting 
and informative nuggets are quite different in nature 
there are thus rational reasons and practical motivation in 
unifying answers from both the pattern based and corpus based 
approaches however the differences between the two systems also 
cause issues when we attempt to combine both answer sets 
currently the best approach we found for combining both answer sets 
is to merge and re-rank both answer sets with boosting agreements 
we first normalize the top ranked sentences from each 
system to obtain the normalized human interest model score 
him s and the normalized soft pattern bigram model score 
sp s for every unique sentence s for each sentence the two 
separate scores for are then unified into a single score using equation 
when only one system believes that the sentence is definitional we 
simply retain that system s normalized score as the unified score 
when both systems agree agree that the sentence is definitional 
the sentence s score is boosted by the degree of agreement between 
between both systems 
score s max shim ssp −min shim ssp 
 
in order to maintain a diverse set of answers as well as to 
ensure that similar sentences are not given similar ranking we 
further re-rank our combined list of answers using maximal marginal 
relevance or mmr using the approach described here we 
achieve a f score of this score is equivalent to the initial 
human interest model score of but fails to outperform the 
optimized human interest model model 
 conclusion 
this paper has presented a novel perspective for answering 
definitional questions through the identification of interesting nuggets 
interesting nuggets are uncommon pieces of information about the 
topic that can evoke a human reader s curiosity the notion of an 
average human reader is an important consideration in our 
approach this is very different from the lexico-syntactic pattern 
approach where the context of a human reader is not even considered 
when finding answers for definitional question answering 
using this perspective we have shown that using a combination 
of a carefully selected external corpus matching against multiple 
centroids and taking into consideration rare but highly topic 
specific terms we can build a definitional question answering 
module that is more focused on identifying nuggets that are of interest 
to human beings experimental results has shown this approach 
can significantly outperform state-of-the-art definitional question 
answering systems 
we further showed that at least two different types of answer 
nuggets are required to form a more thorough set of definitional 
answers what seems to be a good set of definition answers is some 
general information that provides a quick informative overview mixed 
together with some novel or interesting aspects about the topic 
thus we feel that a good definitional question answering system 
would need to pick up both informative and interesting nugget types 
in order to provide a complete definitional coverage on all 
important aspects of the topic while we have attempted to build such a 
system by combining our proposed human interest model with cui 
et al s soft pattern bigram model the inherent differences between 
both types of nuggets seemingly caused by the low agreement rates 
between both models have made this a difficult task indeed this is 
natural as the two models have been designed to identify two very 
different types of definition answers using very different types of 
features as a result we are currently only able to achieve a 
hybrid system that has the same level of performance as our proposed 
human interest model 
we approached the problem of definitional question answering 
from a novel perspective with the notion that interest factor plays 
a role in identifying definitional answers although the methods 
we used are simple they have been shown experimentally to be 
effective our approach may also provide some insight into a few 
anomalies in past definitional question answering s trials for 
instance the top definitional system at the recent trec 
evaluation was able to significantly outperform all other systems using 
relatively simple unigram probabilities extracted from google 
snippets we suspect the main contributor to the system s performance 
entity type topics 
organization depauw university merck co 
norwegian cruise lines ncl united 
parcel service ups little league 
baseball cliffs notes american legion 
sony pictures entertainment spe 
telefonica of spain lions club 
international amway mcdonald s corporation 
harley-davidson u s naval academy 
opec nato international bureau of 
universal postal union upu organization of 
islamic conference oic pbgc 
person bing crosby george foreman akira 
kurosawa sani abacha enrico fermi arnold 
palmer woody guthrie sammy sosa 
michael weiss paul newman jesse 
ventura rose crumb rachel carson paul 
revere vicente fox rocky marciano enrico 
caruso pope pius xii kim jong il 
thing f bollywood viagra howdy doody 
show louvre museum meteorites 
virginia wine counting crows boston big 
dig chunnel longwood gardens camp 
david kudzu u s medal of honor 
tsunami genome food-for-oil agreement 
shiite kinmen island 
event russian submarine kursk sinks miss 
universe crowned port arthur 
massacre france wins world cup in 
soccer plane clips cable wires in italian 
resort kip kinkel school shooting crash 
of egyptair flight preakness 
first bush-gore presidential debate 
 indictment and trial of susan 
mcdougal return of hong kong to chinese 
sovereignty nagano olympic games 
super bowl xxxiv north american 
international auto show mount st 
helens eruption baseball world 
series hindenburg disaster hurricane mitch 
table trec topics grouped by entity type 
is google s pagerank algorithm which mainly consider the 
number of linkages has an indirect effect of ranking web documents by 
the degree of human interest 
in our future work we seek to further improve on the combined 
system by incorporating more evidence in support of correct 
definitional answers or to filter away obviously wrong answers 
 references 
 s blair-goldensohn k r mckeown and a h schlaikjer 
a hybrid approach for qa track definitional questions in 
trec proceedings of the th text retrieval 
conference gaithersburg maryland 
 j g carbonell and j goldstein the use of mmr 
diversity-based reranking for reordering documents and 
producing summaries in research and development in 
information retrieval pages - 
 y chen m zhou and s wang reranking answers for 
definitional qa using language modeling in proceedings of 
the st international conference on computational 
linguistics and th annual meeting of the association for 
computational linguistics pages - sydney 
australia july association for computational 
linguistics 
 h cui m -y kan and t -s chua generic soft pattern 
models for definitional question answering in sigir 
proceedings of the th annual international acm sigir 
conference on research and development in information 
retrieval pages - new york ny usa acm 
press 
 t g dietterich ensemble methods in machine learning 
lecture notes in computer science - 
 s harabagiu d moldovan c clark m bowden a hickl 
and p wang employing two question answering systems at 
trec in trec proceedings of the th text 
retrieval conference gaithersburg maryland 
 m kaisser s scheible and b webber experiments at the 
university of edinburgh for the trec qa track in trec 
 notebook proceedings of the th text retrieval 
conference gaithersburg maryland national 
institute of standards and technology 
 j lin divergence measures based on the shannon entropy 
ieee transactions on information theory - 
jan 
 j lin e abels d demner-fushman d w oard p wu 
and y wu a menagerie of tracks at maryland hard 
enterprise qa and genomics oh my in trec 
proceedings of the th text retrieval conference 
gaithersburg maryland 
 j lin and d demner-fushman automatically evaluating 
answers to definition questions in proceedings of human 
language technology conference and conference on 
empirical methods in natural language processing pages 
 - vancouver british columbia canada october 
 association for computational linguistics 
 r sun j jiang y f tan h cui t -s chua and m -y 
kan using syntactic and semantic relation analysis in 
question answering in trec proceedings of the th 
text retrieval conference gaithersburg maryland 
 e m voorhees overview of the trec question 
answering track in text retrieval conference 
gaithersburg maryland national institute of 
standards and technology 
 e m voorhees overview of the trec question 
answering track in trec proceedings of the th text 
retrieval conference gaithersburg maryland 
national institute of standards and technology 
 j xu a licuanan and r weischedel trec qa at 
bbn answering definitional questions in trec 
proceedings of the th text retrieval conference 
gaithersburg maryland 
 d zhang and w s lee a language modeling approach to 
passage question answering in trec proceedings of 
the th text retrieval conference gaithersburg maryland 
 
