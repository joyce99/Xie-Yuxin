estimation and use of uncertainty 
in pseudo-relevance feedback 
kevyn collins-thompson and jamie callan 
language technologies institute 
school of computer science 
carnegie mellon university 
pittsburgh pa - u s a 
 kct callan  cs cmu edu 
abstract 
existing pseudo-relevance feedback methods typically 
perform averaging over the top-retrieved documents but 
ignore an important statistical dimension the risk or variance 
associated with either the individual document models or 
their combination treating the baseline feedback method 
as a black box and the output feedback model as a random 
variable we estimate a posterior distribution for the 
feedback model by resampling a given query s top-retrieved 
documents using the posterior mean or mode as the enhanced 
feedback model we then perform model combination over 
several enhanced models each based on a slightly modified 
query sampled from the original query we find that 
resampling documents helps increase individual feedback model 
precision by removing noise terms while sampling from the 
query improves robustness worst-case performance by 
emphasizing terms related to multiple query aspects the 
result is a meta-feedback algorithm that is both more robust 
and more precise than the original strong baseline method 
categories and subject descriptors 
h information retrieval retrieval models 
general terms algorithms experimentation 
 introduction 
uncertainty is an inherent feature of information retrieval 
not only do we not know the queries that will be presented 
to our retrieval algorithm ahead of time but the user s 
information need may be vague or incompletely specified by 
these queries even if the query were perfectly specified 
language in the collection documents is inherently complex 
and ambiguous and matching such language effectively is a 
formidable problem by itself with this in mind we wish 
to treat many important quantities calculated by the 
retrieval system whether a relevance score for a document 
or a weight for a query expansion term as random 
variables whose true value is uncertain but where the 
uncertainty about the true value may be quantified by replacing 
the fixed value with a probability distribution over possible 
values in this way retrieval algorithms may attempt to 
quantify the risk or uncertainty associated with their 
output rankings or improve the stability or precision of their 
internal calculations 
current algorithms for pseudo-relevance feedback prf 
tend to follow the same basic method whether we use 
vector space-based algorithms such as rocchio s formula 
or more recent language modeling approaches such as 
relevance models first a set of top-retrieved documents is 
obtained from an initial query and assumed to approximate 
a set of relevant documents next a single feedback model 
vector is computed according to some sort of average 
centroid or expectation over the set of possibly-relevant 
document models for example the document vectors may be 
combined with equal weighting as in rocchio or by query 
likelihood as may be done using the relevance model 
 the 
use of an expectation is reasonable for practical and 
theoretical reasons but by itself ignores potentially valuable 
information about the risk of the feedback model 
our main hypothesis in this paper is that estimating the 
uncertainty in feedback is useful and leads to better 
individual feedback models and more robust combined models 
therefore we propose a method for estimating uncertainty 
associated with an individual feedback model in terms of 
a posterior distribution over language models to do this 
we systematically vary the inputs to the baseline feedback 
method and fit a dirichlet distribution to the output we 
use the posterior mean or mode as the improved feedback 
model estimate this process is shown in figure as we 
show later the mean and mode may vary significantly from 
the single feedback model proposed by the baseline method 
we also perform model combination using several improved 
feedback language models obtained by a small number of 
new queries sampled from the original query a model s 
weight combines two complementary factors the model s 
probability of generating the query and the variance of the 
model with high-variance models getting lower weight 
 
for example an expected parameter vector conditioned on 
the query observation is formed from top-retrieved 
documents which are treated as training strings see p 
figure estimating the uncertainty of the feedback model 
for a single query 
 sampling-based feedback 
in sections - we describe a general method for 
estimating a probability distribution over the set of possible 
language models in sections and we summarize how 
different query samples are used to generate multiple 
feedback models which are then combined 
 modeling feedback uncertainty 
given a query q and a collection c we assume a 
probabilistic retrieval system that assigns a real-valued document 
score f d q to each document d in c such that the score 
is proportional to the estimated probability of relevance we 
make no other assumptions about f d q the nature of 
f d q may be complex for example if the retrieval 
system supports structured query languages then f d q 
may represent the output of an arbitrarily complex 
inference network defined by the structured query operators in 
theory the scoring function can vary from query to query 
although in this study for simplicity we keep the scoring 
function the same for all queries our specific query method 
is given in section 
we treat the feedback algorithm as a black box and 
assume that the inputs to the feedback algorithm are the 
original query and the corresponding top-retrieved documents 
with a score being given to each document we assume that 
the output of the feedback algorithm is a vector of term 
weights to be used to add or reweight the terms in the 
representation of the original query with the vector normalized 
to form a probability distribution we view the the inputs 
to the feedback black box as random variables and analyze 
the feedback model as a random variable that changes in 
response to changes in the inputs like the document scoring 
function f d q the feedback algorithm may implement 
a complex non-linear scoring formula and so as its inputs 
vary the resulting feedback models may have a complex 
distribution over the space of feedback models the sample 
space because of this potential complexity we do not 
attempt to derive a posterior distribution in closed form but 
instead use simulation we call this distribution over 
possible feedback models the feedback model distribution our 
goal in this section is to estimate a useful approximation to 
the feedback model distribution 
for a specific framework for experiments we use the 
language modeling lm approach for information retrieval 
the score of a document d with respect to a query q and 
collection c is given by p q d with respect to language 
models ˆθq and ˆθd estimated for the query and document 
respectively we denote the set of k top-retrieved 
documents from collection c in response to q by dq k c for 
simplicity we assume that queries and documents are 
generated by multinomial distributions whose parameters are 
represented by unigram language models 
to incorporate feedback in the lm approach we assume a 
model-based scheme in which our goal is take the query and 
resulting ranked documents dq k c as input and output 
an expansion language model ˆθe which is then interpolated 
with the original query model ˆθq 
ˆθnew − α · ˆθq α · ˆθe 
this includes the possibility of α where the original 
query mode is completely replaced by the feedback model 
our sample space is the set of all possible language 
models lf that may be output as feedback models our 
approach is to take samples from this space and then fit a 
distribution to the samples using maximum likelihood for 
simplicity we start by assuming the latent feedback 
distribution has the form of a dirichlet distribution although the 
dirichlet is a unimodal distribution and in general quite 
limited in its expressiveness in the sample space it is a 
natural match for the multinomial language model can be 
estimated quickly and can capture the most salient features of 
confident and uncertain feedback models such as the overall 
spread of the distibution 
 resampling document models 
we would like an approximation to the posterior 
distribution of the feedback model lf to accomplish this we 
apply a widely-used simulation technique called bootstrap 
sampling p on the input parameters namely the 
set of top-retrieved documents 
bootstrap sampling allows us to simulate the approximate 
effect of perturbing the parameters within the black box 
feedback algorithm by perturbing the inputs to that 
algorithm in a systematic way while making no assumptions 
about the nature of the feedback algorithm 
specifically we sample k documents with replacement from 
dq k c and calculate an expansion language model θb 
using the black box feedback method we repeat this process 
b times to obtain a set of b feedback language models to 
which we then fit a dirichlet distribution typically b is 
in the range of to samples with performance being 
relatively stable in this range note that instead of treating 
each top document as equally likely we sample according to 
the estimated probabilities of relevance of each document in 
dq k c thus a document is more likely to be chosen the 
higher it is in the ranking 
 justification for a sampling approach 
the rationale for our sampling approach has two parts 
first we want to improve the quality of individual 
feedback models by smoothing out variation when the baseline 
feedback model is unstable in this respect our approach 
resembles bagging an ensemble approach which 
generates multiple versions of a predictor by making bootstrap 
copies of the training set and then averages the numerical 
predictors in our application top-retrieved documents can 
be seen as a kind of noisy training set for relevance 
second sampling is an effective way to estimate basic 
properties of the feedback posterior distribution which can 
then be used for improved model combination for 
example a model may be weighted by its prediction confidence 
estimated as a function of the variability of the posterior 
around the model 
foo - map-dim size units gaussianneighborhood 
 a topic 
foreign 
minorities 
germany 
foo - map-dim size units gaussianneighborhood 
 b topic 
behavioral 
genetics 
foo - map-dim size units gaussianneighborhood 
 c topic 
when can a 
lender foreclose 
on property 
figure visualization of expansion language model 
variance using self-organizing maps showing the distribution of 
language models that results from resampling the inputs to 
the baseline expansion method the language model that 
would have been chosen by the baseline expansion is at 
the center of each map the similarity function is 
jensenshannon divergence 
 visualizing feedback distributions 
before describing how we fit and use the dirichlet 
distribution over feedback models it is instructive to view some 
examples of actual feedback model distributions that result 
from bootstrap sampling the top-retrieved documents from 
different trec topics 
each point in our sample space is a language model which 
typically has several thousand dimensions to help analyze 
the behavior of our method we used a self-organizing map 
 via the som-pak package to  flatten and visualize 
the high-dimensional density function 
 
the density maps for three trec topics are shown in 
figure above the dark areas represent regions of high 
similarity between language models the light areas 
represent regions of low similarity - the  valleys between 
clusters each diagram is centered on the language model that 
would have been chosen by the baseline expansion a single 
peak mode is evident in some examples but more complex 
structure appears in others also while the distribution is 
usually close to the baseline feedback model for some topics 
they are a significant distance apart as measured by 
jensenshannon divergence as in subfigure c in such cases the 
mode or mean of the feedback distribution often performs 
significantly better than the baseline and in a smaller 
proportion of cases significantly worse 
 fitting a posterior feedback distribution 
after obtaining feedback model samples by resampling 
the feedback model inputs we estimate the feedback 
distribution we assume that the multinomial feedback 
models ˆθ ˆθb were generated by a latent dirichlet 
distribution with parameters α αn to estimate the 
 α αn we fit the dirichlet parameters to the b 
language model samples according to maximum likelihood 
using a generalized newton procedure details of which are 
given in minka we assume a simple dirichlet prior over 
the α αn setting each to αi μ · p wi c where μ 
is a parameter and p · c is the collection language model 
estimated from a set of documents from collection c the 
parameter fitting converges very quickly - typically just or 
 
because our points are language models in the 
multinomial simplex we extended som-pak to support 
jensenshannon divergence a widely-used similarity measure 
between probability distributions 
 iterations are enough - so that it is practical to apply at 
query-time when computational overhead must be small in 
practice we can restrict the calculation to the vocabulary of 
the top-retrieved documents instead of the entire collection 
note that for this step we are re-using the existing retrieved 
documents and not performing additional queries 
given the parameters of an n-dimensional dirichlet 
distribution dir α the mean μ and mode x vectors are easy 
to calculate and are given respectively by 
μi αip 
αi 
 and xi αi− p 
αi−n 
 
we can then choose the language model at the mean or the 
mode of the posterior as the final enhanced feedback model 
 we found the mode to give slightly better performance 
for information retrieval the number of samples we will 
have available is likely to be quite small for performance 
reasons - usually less than ten moreover while random 
sampling is useful in certain cases it is perfectly acceptable to 
allow deterministic sampling distributions but these must 
be designed carefully in order to approximate an accurate 
output variance we leave this for future study 
 query variants 
we use the following methods for generating variants of 
the original query each variant corresponds to a different 
assumption about which aspects of the original query may 
be important this is a form of deterministic sampling 
we selected three simple methods that cover complimentary 
assumptions about the query 
no-expansion use only the original query the 
assumption is that the given terms are a complete description 
of the information need 
leave-one-out a single term is left out of the original 
query the assumption is that one of the query terms 
is a noise term 
single-term a single term is chosen from the original query 
this assumes that only one aspect of the query namely 
that represented by the term is most important 
after generating a variant of the original query we combine 
it with the original query using a weight αsub so that we 
do not stray too  far in this study we set αsub for 
example using the indri query language a 
leave-oneout variant of the initial query that omits the term  ireland 
for trec topic is 
 weight combine ireland peace talks 
 combine peace talks 
 combining enhanced feedback models 
from multiple query variants 
when using multiple query variants the resulting 
enhanced feedback models are combined using bayesian model 
combination to do this we treat each word as an item to 
be classified as belonging to a relevant or non-relevant class 
and derive a class probability for each word by combining 
the scores from each query variant each score is given by 
that term s probability in the dirichlet distribution the 
term scores are weighted by the inverse of the variance of 
the term in the enhanced feedback model s dirichlet 
distribution the prior probability of a word s membership in 
the relevant class is given by the probability of the original 
query in the entire enhanced expansion model 
 evaluation 
in this section we present results confirming the usefulness 
of estimating a feedback model distribution from weighted 
resampling of top-ranked documents and of combining the 
feedback models obtained from different small changes in 
the original query 
 general method 
we evaluated performance on a total of queries 
derived from four sets of trec topics - trec- 
 - trec- - trec- and - wt g 
trec- we chose these for their varied content and 
document properties for example wt g documents are 
web pages with a wide variety of subjects and styles while 
trec- documents are more homogeneous news articles 
indexing and retrieval was performed using the indri system 
in the lemur toolkit our queries were derived from 
the words in the title field of the trec topics phrases 
were not used to generate the baseline queries passed to 
indri we wrapped the query terms with indri s combine 
operator for example the initial query for topic is 
 combine ireland peace talks 
we performed krovetz stemming for all experiments 
because we found that the baseline indri expansion method 
performed better using a stopword list with the feedback 
model all experiments used a stoplist of common 
english words however an interesting side-effect of our 
resampling approach is that it tends to remove many stopwords 
from the feedback model making a stoplist less critical this 
is discussed further in section 
 baseline feedback method 
for our baseline expansion method we use an algorithm 
included in indri as the default expansion method this 
method first selects terms using a log-odds calculation 
described by ponte but assigns final term weights using 
lavrenko s relevance model 
we chose the indri method because it gives a consistently 
strong baseline is based on a language modeling approach 
and is simple to experiment with in a trec evaluation 
using the gov corpus the method was one of the 
topperforming runs achieving a gain in map compared 
to using unexpanded queries in this study it achieves an 
average gain in map of over the four collections 
indri s expansion method first calculates a log-odds ratio 
o v for each potential expansion term v given by 
o v 
x 
d 
log 
p v d 
p v c 
 
over all documents d containing v in collection c then 
the expansion term candidates are sorted by descending 
o v and the top m are chosen finally the term weights 
r v used in the expanded query are calculated based on the 
relevance model 
r v 
x 
d 
p q d p v d 
p v 
p d 
 
the quantity p q d is the probability score assigned to the 
document in the initial retrieval we use dirichlet 
smoothing of p v d with μ 
this relevance model is then combined with the original 
query using linear interpolation weighted by a parameter α 
by default we used the top documents for feedback and 
the top expansion terms with the feedback interpolation 
parameter α unless otherwise stated for example 
the baseline expanded query for topic is 
 weight combine ireland peace talks 
 weight ireland peace northern 
 expansion performance 
we measure our feedback algorithm s effectiveness by two 
main criteria precision and robustness robustness and 
the tradeoff between precision and robustness is analyzed 
in section in this section we examine average 
precision and precision in the top documents p we also 
include recall at documents 
for each query we obtained a set of b feedback models 
using the indri baseline each feedback model was obtained 
from a random sample of the top k documents taken with 
replacement for these experiments b and k 
each feedback model contained terms on the query side 
we used leave-one-out loo sampling to create the query 
variants single-term query sampling had consistently worse 
performance across all collections and so our results here 
focus on loo sampling we used the methods described in 
section to estimate an enhanced feedback model from the 
dirichlet posterior distribution for each query variant and 
to combine the feedback models from all the query variants 
we call our method  resampling expansion and denote it as 
rs-fb here we denote the indri baseline feedback method 
as base-fb results from applying both the baseline 
expansion method base-fb and resampling expansion rs-fb 
are shown in table 
we observe several trends in this table first the average 
precision of rs-fb was comparable to base-fb achieving 
an average gain of compared to using no expansion 
across the four collections the indri baseline expansion 
gain was also the rs-fb method achieved 
consistent improvements in p over base-fb for every topic set 
with an average improvement of over base-fb for all 
 topics the lowest p gain over base-fb was 
for trec- and the highest was for wt g 
finally both base-fb and rs-fb also consistently improved 
recall over using no expansion with base-fb achieving 
better recall than rs-fb for all topic sets 
 retrieval robustness 
we use the term robustness to mean the worst-case 
average precision performance of a feedback algorithm ideally 
a robust feedback method would never perform worse than 
using the original query while often performing better using 
the expansion 
to evaluate robustness in this study we use a very 
simple measure called the robustness index ri 
 for a set of 
queries q the ri measure is defined as 
ri q 
n − n− 
 q 
 
where n is the number of queries helped by the feedback 
method and n− is the number of queries hurt here by 
 helped we mean obtaining a higher average precision as a 
result of feedback the value of ri ranges from a minimum 
 
this is sometimes also called the reliability of improvement 
index and was used in sakai et al 
collection noexp base-fb rs-fb 
trec 
 
avgp 
p 
recall 
trec 
avgp 
p 
recall 
trec 
avgp 
p 
recall 
wt g 
avgp 
p - 
recall 
table comparison of baseline base-fb feedback and feedback using re-sampling rs-fb improvement shown for 
basefb and rs-fb is relative to using no expansion 
 a trec upper curve trec 
 lower curve 
 b trec upper curve wt g lower 
curve 
figure the trade-off between robustness and average 
precision for different corpora the x-axis gives the change in 
map over using baseline expansion with α the 
yaxis gives the robustness index ri each curve through 
uncircled points shows the ri map tradeoff using the 
simple small-α strategy see text as α decreases from to 
zero in the direction of the arrow circled points represent 
the tradeoffs obtained by resampling feedback for α 
collection n base-fb rs-fb 
n− ri n− ri 
trec 
trec 
trec 
wt g - 
combined 
table comparison of robustness index ri for baseline 
feedback base-fb vs resampling feedback rs-fb also 
shown are the actual number of queries hurt by feedback 
 n− for each method and collection queries for which 
initial average precision was negligible ≤ were ignored 
giving the remaining query count in column n 
of − when all queries are hurt by the feedback method 
to when all queries are helped the ri measure does 
not take into account the magnitude or distribution of the 
amount of change across the set q however it is easy to 
understand as a general indication of robustness 
one obvious way to improve the worst-case performance 
of feedback is simply to use a smaller fixed α interpolation 
parameter such as α placing less weight on the 
 possibly risky feedback model and more on the original query 
we call this the  small-α strategy since we are also 
reducing the potential gains when the feedback model is  right 
however we would expect some trade-off between average 
precision and robustness we therefore compared the 
precision robustness trade-off between our resampling feedback 
algorithm and the simple small-α method the results are 
summarized in figure in the figure the curve for each 
topic set interpolates between trade-off points beginning 
at x where α and continuing in the direction of 
the arrow as α decreases and the original query is given 
more and more weight as expected robustness 
continuously increases as we move along the curve but mean 
average precision generally drops as the gains from feedback are 
eliminated for comparison the performance of resampling 
feedback at α is shown for each collection as the circled 
point higher and to the right is better this figure shows 
that resampling feedback gives a somewhat better trade-off 
than the small-α approach for of the collections 
figure histogram showing improved robustness of 
resampling feedback rs-fb over baseline feedback base-fb 
for all datasets combined queries are binned by change 
in ap compared to the unexpanded query 
collection ds qv ds no qv 
trec 
 
avgp 
p 
ri - 
trec 
avgp 
p - 
ri - 
trec 
avgp - 
p - 
ri - 
wt g 
avgp - 
p - 
ri - 
table comparison of resampling feedback using 
document sampling ds with qv and without no qv 
combining feedback models from multiple query variants 
table gives the robustness index scores for base-fb 
and rs-fb the rs-fb feedback method obtained higher 
robustness than base-fb on three of the four topic sets with 
only slightly worse performance on trec- 
a more detailed view showing the distribution over 
relative changes in ap is given by the histogram in figure 
compared to base-fb the rs-fb method achieves a 
noticable reduction in the number of queries significantly hurt 
by expansion i e where ap is hurt by or more while 
preserving positive gains in ap 
 effect of query and document 
sampling methods 
given our algorithm s improved robustness seen in 
section an important question is what component of our 
system is responsible is it the use of document re-sampling 
the use of multiple query variants or some other factor the 
results in table suggest that the model combination based 
on query variants may be largely account for the improved 
robustness when query variants are turned off and the 
original query is used by itself with document sampling there 
is little net change in average precision a small decrease in 
p for out of the topic sets but a significant drop in 
robustness for all topic sets in two cases the ri measure 
drops by more than 
we also examined the effect of the document sampling 
method on retrieval effectiveness using two different 
strategies the  uniform weighting strategy ignored the relevance 
scores from the initial retrieval and gave each document in 
the top k the same probability of selection in contrast the 
 relevance-score weighting strategy chose documents with 
probability proportional to their relevance scores in this 
way documents that were more highly ranked were more 
likely to be selected results are shown in table 
the relevance-score weighting strategy performs better 
overall with significantly higher ri and p scores on of 
the topic sets the difference in average precision between 
the methods however is less marked this suggests that 
uniform weighting acts to increase variance in retrieval 
results when initial average precision is high there are many 
relevant documents in the top k and uniform sampling may 
give a more representative relevance model than focusing on 
the highly-ranked items on the other hand when initial 
precision is low there are few relevant documents in the 
bottom ranks and uniform sampling mixes in more of the 
non-relevant documents 
for space reasons we only summarize our findings on 
sample size here the number of samples has some effect on 
precision when less than but performance stabilizes at 
around to samples we used samples for our 
experiments much beyond this level the additional benefits 
of more samples decrease as the initial score distribution is 
more closely fit and the processing time increases 
 the effect of resampling on expansion 
term quality 
ideally a retrieval model should not require a stopword 
list when estimating a model of relevance a robust 
statistical model should down-weight stopwords automatically 
depending on context stopwords can harm feedback if 
selected as feedback terms because they are typically poor 
discriminators and waste valuable term slots in practice 
however because most term selection methods resemble a 
tf · idf type of weighting terms with low idf but very high 
tf can sometimes be selected as expansion term candidates 
this happens for example even with the relevance model 
approach that is part of our baseline feedback to ensure 
as strong a baseline as possible we use a stoplist for all 
experiments reported here if we turn off the stopword list 
however we obtain results such as those shown in table 
where four of the top ten baseline feedback terms for trec 
topic said but their not are stopwords using the 
basefb method the top expansion terms were selected to 
generate this example 
indri s method attempts to address the stopword 
problem by applying an initial step based on ponte to 
select less-common terms that have high log-odds of being 
in the top-ranked documents compared to the whole 
collection nevertheless this does not overcome the stopword 
problem completely especially as the number of feedback 
terms grows 
using resampling feedback however appears to mitigate 
collection qv uniform qv relevance-score 
weighting weighting 
trec 
 
avgp - 
p - 
ri 
trec 
avgp - 
p 
ri 
trec 
avgp 
p 
ri 
wt g 
avgp 
p 
ri 
table comparison of uniform and relevance-weighted document sampling the percentage change compared to uniform 
sampling is shown in parentheses qv indicates that query variants were used in both runs 
baseline fb p wi r resampling fb p wi r 
said court 
court pay 
pay federal 
but education 
employees teachers 
their employees 
not case 
federal their 
workers appeals 
education union 
table feedback term quality when a stoplist is not used 
feedback terms for trec topic merit pay vs seniority 
the effect of stopwords automatically in the example of 
table resampling feedback leaves only one stopword their 
in the top ten we observed similar feedback term behavior 
across many other topics the reason for this effect appears 
to be the interaction of the term selection score with the 
top-m term cutoff while the presence and even 
proportion of particular stopwords is fairly stable across different 
document samples their relative position in the top-m list 
is not as sets of documents with varying numbers of 
better lower-frequency term candidates are examined for each 
sample as a result while some number of stopwords may 
appear in each sampled document set any given stopword 
tends to fall below the cutoff for multiple samples leading 
to its classification as a high-variance low-weight feature 
 related work 
our approach is related to previous work from several 
areas of information retrieval and machine learning our use 
of query variation was inspired by the work of yomtov et 
al carpineto et al and amati et al among 
others these studies use the idea of creating multiple 
subqueries and then examining the nature of the overlap in the 
documents and or expansion terms that result from each 
subquery model combination is performed using heuristics 
in particular the studies of amati et al and carpineto et al 
investigated combining terms from individual distributional 
methods using a term-reranking combination heuristic in 
a set of trec topics they found wide average variation in 
the rank-distance of terms from different expansion 
methods their combination method gave modest positive 
improvements in average precision 
the idea of examining the overlap between lists of 
suggested terms has also been used in early query expansion 
approaches xu and croft s method of local context 
analysis lca includes a factor in the empirically-derived 
weighting formula that causes expansion terms to be 
preferred that have connections to multiple query terms 
on the document side recent work by zhou croft 
explored the idea of adding noise to documents re-scoring 
them and using the stability of the resulting rankings as 
an estimate of query difficulty this is related to our use 
of document sampling to estimate the risk of the feedback 
model built from the different sets of top-retrieved 
documents sakai et al proposed an approach to improving 
the robustness of pseudo-relevance feedback using a method 
they call selective sampling the essence of their method 
is that they allow skipping of some top-ranked documents 
based on a clustering criterion in order to select a more 
varied and novel set of documents later in the ranking for use 
by a traditional pseudo-feedback method their study did 
not find significant improvements in either robustness ri 
or map on their corpora 
greiff morgan and ponte explored the role of variance 
in term weighting in a series of simulations that simplified 
the problem to -feature documents they found that average 
precision degrades as term frequency variance - high 
noiseincreases downweighting terms with high variance resulted 
in improved average precision this seems in accord with 
our own findings for individual feedback models 
estimates of output variance have recently been used for 
improved text classification lee et al used 
queryspecific variance estimates of classifier outputs to perform 
improved model combination instead of using sampling 
they were able to derive closed-form expressions for classifier 
variance by assuming base classifiers using simple types of 
inference networks 
ando and zhang proposed a method that they call 
structural feedback and showed how to apply it to query 
expansion for the trec genomics track they used r query 
variations to obtain r different sets sr of top-ranked 
documents that have been intersected with the top-ranked 
documents obtained from the original query qorig for each si 
the normalized centroid vector ˆwi of the documents is 
calculated principal component analysis pca is then applied 
to the ˆwi to obtain the matrix φ of h left singular vectors 
φh that are used to obtain the new expanded query 
qexp qorig φt 
φqorig 
in the case h we have a single left singular vector φ 
qexp qorig φt 
qorig φ 
so that the dot product φt 
qorig is a type of dynamic weight 
on the expanded query that is based on the similarity of the 
original query to the expanded query the use of variance as 
a feedback model quality measure occurs indirectly through 
the application of pca it would be interesting to study 
the connections between this approach and our own 
modelfitting method 
finally in language modeling approaches to feedback tao 
and zhai describe a method for more robust feedback 
that allows each document to have a different feedback α 
the feedback weights are derived automatically using 
regularized em a roughly equal balance of query and expansion 
model is implied by their em stopping condition they 
propose tailoring the stopping parameter η based on a function 
of some quality measure of feedback documents 
 conclusions 
we have presented a new approach to pseudo-relevance 
feedback based on document and query sampling the use 
of sampling is a very flexible and powerful device and is 
motivated by our general desire to extend current models of 
retrieval by estimating the risk or variance associated with the 
parameters or output of retrieval processes such variance 
estimates for example may be naturally used in a bayesian 
framework for improved model estimation and combination 
applications such as selective expansion may then be 
implemented in a principled way 
while our study uses the language modeling approach as a 
framework for experiments we make few assumptions about 
the actual workings of the feedback algorithm we believe 
it is likely that any reasonably effective baseline feedback 
algorithm would benefit from our approach our results on 
standard trec collections show that our framework 
improves the robustness of a strong baseline feedback method 
across a variety of collections without sacrificing average 
precision it also gives small but consistent gains in 
top precision in future work we envision an investigation 
into how varying the set of sampling methods used and the 
number of samples controls the trade-off between 
robustness accuracy and efficiency 
acknowledgements 
we thank paul bennett for valuable discussions related to 
this work which was supported by nsf grants iis- 
and cns- and u s dept of education grant 
 r g any opinions findings and conclusions or 
recommendations expressed in this material are the authors 
and do not necessarily reflect those of the sponsors 
 references 
 the lemur toolkit for language modeling and retrieval 
http www lemurproject org 
 g amati c carpineto and g romano query difficulty 
robustness and selective application of query expansion in 
proc of the th european conf on information retrieval 
 ecir pages - 
 r k ando and t zhang a high-performance semi-supervised 
learning method for text chunking in proc of the rd 
annual meeting of the acl pages - june 
 l breiman bagging predictors machine learning 
 - 
 c carpineto g romano and v giannini improving retrieval 
feedback with multiple term-ranking function combination 
acm trans info systems - 
 k collins-thompson p ogilvie and j callan initial results 
with structured queries and language models on half a terabyte 
of text in proc of text retrieval conference nist 
special publication 
 r o duda p e hart and d g stork pattern 
classification wiley and sons nd edition 
 w r greiff w t morgan and j m ponte the role of 
variance in term weighting for probabilistic information 
retrieval in proc of the th intl conf on info and 
knowledge mgmt cikm pages - 
 t kohonen j hynninen j kangas and j laaksonen 
sompak the self-organizing map program package technical 
report a helsinki university of technology 
http www cis hut fi research papers som tr ps z 
 v lavrenko a generative theory of relevance phd thesis 
university of massachusetts amherst 
 c -h lee r greiner and s wang using query-specific 
variance estimates to combine bayesian classifiers in proc of 
the rd intl conf on machine learning icml 
pages - 
 d metzler and w b croft combining the language model 
and inference network approaches to retrieval info processing 
and mgmt - 
 t minka estimating a dirichlet distribution technical report 
 http research microsoft com minka papers dirichlet 
 j ponte advances in information retrieval chapter 
language models for relevance feedback pages - 
w b croft ed 
 j m ponte and w b croft a language modeling approach to 
information retrieval in proc of the acm sigir 
conference on research and development in information 
retrieval pages - 
 j rocchio the smart retrieval system chapter relevance 
feedback in information retrieval pages - 
prentice-hall g salton ed 
 t sakai t manabe and m koyama flexible 
pseudo-relevance feedback via selective sampling acm 
transactions on asian language information processing 
 talip - 
 t tao and c zhai regularized estimation of mixture models 
for robust pseudo-relevance feedback in proc of the 
acm sigir conference on research and development in 
information retrieval pages - 
 j xu and w b croft improving the effectiveness of 
information retrieval with local context analysis acm trans 
inf syst - 
 e yomtov s fine d carmel and a darlow learning to 
estimate query difficulty in proc of the acm sigir 
conf on research and development in information 
retrieval pages - 
 y zhou and w b croft ranking robustness a novel 
framework to predict query performance in proc of the th 
acm intl conf on information and knowledge mgmt 
 cikm pages - 
