term feedback for information retrieval 
with language models 
bin tan† 
 atulya velivelli‡ 
 hui fang† 
 chengxiang zhai† 
dept of computer science† 
 dept of electrical and computer engineering‡ 
university of illinois at urbana-champaign 
bintan cs uiuc edu velivell ifp uiuc edu hfang cs uiuc edu czhai cs uiuc edu 
abstract 
in this paper we study term-based feedback for information 
retrieval in the language modeling approach with term feedback 
a user directly judges the relevance of individual terms without 
interaction with feedback documents taking full control of the query 
expansion process we propose a cluster-based method for 
selecting terms to present to the user for judgment as well as effective 
algorithms for constructing refined query language models from 
user term feedback our algorithms are shown to bring significant 
improvement in retrieval accuracy over a non-feedback baseline 
and achieve comparable performance to relevance feedback they 
are helpful even when there are no relevant documents in the top 
categories and subject descriptors 
h information search and retrieval retrieval models 
general terms 
algorithms 
 introduction 
in the language modeling approach to information retrieval 
feedback is often modeled as estimating an improved query model or 
relevance model based on a set of feedback documents 
this is in line with the traditional way of doing relevance feedback 
- presenting a user with documents passages for relevance 
judgment and then extracting terms from the judged documents or 
passages to expand the initial query it is an indirect way of seeking 
user s assistance for query model construction in the sense that the 
refined query model based on terms is learned through feedback 
documents passages which are high-level structures of terms it 
has the disadvantage that irrelevant terms which occur along with 
relevant ones in the judged content may be erroneously used for 
query expansion causing undesired effects for example for the 
trec query hubble telescope achievements when a relevant 
document talks more about the telescope s repair than its 
discoveries irrelevant terms such as spacewalk can be added into the 
modified query 
we can consider a more direct way to involve a user in query 
model improvement without an intermediary step of document 
feedback that can introduce noise the idea is to present a 
 reasonable number of individual terms to the user and ask him her to 
judge the relevance of each term or directly specify their 
probabilities in the query model this strategy has been discussed in 
but to our knowledge it has not been seriously studied in existing 
language modeling literature compared to traditional relevance 
feedback this term-based approach to interactive query model 
refinement has several advantages first the user has better 
control of the final query model through direct manipulation of terms 
he she can dictate which terms are relevant irrelevant and 
possibly to what degree this avoids the risk of bringing unwanted 
terms into the query model although sometimes the user introduces 
low-quality terms second because a term takes less time to judge 
than a document s full text or summary and as few as around 
presented terms can bring significant improvement in retrieval 
performance as we will show later term feedback makes it faster to 
gather user feedback this is especially helpful for interactive 
adhoc search third sometimes there are no relevant documents in 
the top n of the initially retrieved results if the topic is hard this 
is often true when n is constrained to be small which arises from 
the fact that the user is unwilling to judge too many documents in 
this case relevance feedback is useless as no relevant document 
can be leveraged on but term feedback is still often helpful by 
allowing relevant terms to be picked from irrelevant documents 
during our participation in the trec hard track and 
continued study afterward we explored how to exploit term 
feedback from the user to construct improved query models for 
information retrieval in the language modeling approach we identified 
two key subtasks of term-based feedback i e pre-feedback 
presentation term selection and post-feedback query model 
construction with effective algorithms developed for both we imposed a 
secondary cluster structure on terms and found that a cluster view 
sheds additional insight into the user s information need and 
provides a good way of utilizing term feedback through experiments 
we found that term feedback improves significantly over the 
nonfeedback baseline even though the user often makes mistakes in 
relevance judgment among our algorithms the one with best 
retrieval performance is tcfb the combination of tfb the direct 
term feedback algorithm and cfb the cluster-based feedback 
algorithm we also varied the number of feedback terms and 
observed reasonable improvement even at low numbers finally by 
comparing term feedback with document-level feedback we found 
it to be a viable alternative to the latter with competitive retrieval 
performance 
the rest of the paper is organized as follows section discusses 
some related work section outlines our general approach to term 
feedback we present our method for presentation term selection in 
section and algorithms for query model construction in section 
the experiment results are given in section section concludes 
this paper 
 related work 
relevance feedback has long been recognized as an 
effective method for improving retrieval performance normally the 
top n documents retrieved using the original query are presented 
to the user for judgment after which terms are extracted from the 
judged relevant documents weighted by their potential of 
attracting more relevant documents and added into the query model the 
expanded query usually represents the user s information need 
better than the original one which is often just a short keyword query 
a second iteration of retrieval using this modified query usually 
produces significant increase in retrieval accuracy in cases where 
true relevance judgment is unavailable and all top n documents are 
assumed to be relevant it is called blind or pseudo feedback 
and usually still brings performance improvement 
because document is a large text unit when it is used for 
relevance feedback many irrelevant terms can be introduced into the 
feedback process to overcome this passage feedback is proposed 
and shown to improve feedback performance a more direct 
solution is to ask the user for their relevance judgment of feedback 
terms for example in some relevance feedback systems such as 
 there is an interaction step that allows the user to add or 
remove expansion terms after they are automatically extracted from 
relevant documents this is categorized as interactive query 
expansion where the original query is augmented with user-provided 
terms which can come from direct user input free-form text or 
keywords or user selection of system-suggested terms 
 using thesauri or extracted from feedback documents 
 
in many cases term relevance feedback has been found to 
effectively improve retrieval performance for 
example the study in shows that the user prefers to have explicit 
knowledge and direct control of which terms are used for query 
expansion and the penetrable interface that provides this freedom is 
shown to perform better than other interfaces however in some 
other cases there is no significant benefit even if the user 
likes interacting with expansion terms in a simulated study 
carried out in the author compares the retrieval performance of 
interactive query expansion and automatic query expansion with a 
simulated study and suggests that the potential benefits of the 
former can be hard to achieve the user is found to be not good at 
identifying useful terms for query expansion when a simple term 
presentation interface is unable to provide sufficient semantic 
context of the feedback terms 
our work differs from the previous ones in two important 
aspects first when we choose terms to present to the user for 
relevance judgment we not only consider single-term value e g the 
relative frequency of a term in the top documents which can be 
measured by metrics such as robertson selection value and 
simplified kullback-leibler distance as listed in but also 
examine the cluster structure of the terms so as to produce a balanced 
coverage of the different topic aspects second with the language 
modelling framework we allow an elaborate construction of the 
updated query model by setting different probabilities for different 
terms based on whether it is a query term its significance in the 
top documents and its cluster membership although techniques 
for adjusting query term weights exist for vector space models 
and probablistic relevance models most of the aforementioned 
works do not use them choosing to just append feedback terms to 
the original query thus using equal weights for them which can 
lead to poorer retrieval performance the combination of the two 
aspects allows our method to perform much better than the 
baseline 
the usual way for feedback term presentation is just to display 
the terms in a list there have been some works on alternative user 
interfaces arranges terms in a hierarchy and compares 
three different interfaces including terms checkboxes terms 
context sentences checkboxes sentences input text box in 
both studies however there is no significant performance 
difference in our work we adopt the simplest approach of terms 
checkboxes we focus on term presentation and query model 
construction from feedback terms and believe using contexts to improve 
feedback term quality should be orthogonal to our method 
 general approach 
we follow the language modeling approach and base our method 
on the kl-divergence retrieval model proposed in with this 
model the retrieval task involves estimating a query language model 
θq from a given query a document language model θd from each 
document and calculating their kl-divergence d θq θd which 
is then used to score the documents treats relevance feedback 
as a query model re-estimation problem i e computing an updated 
query model θq given the original query text and the extra evidence 
carried by the judged relevant documents we adopt this view and 
cast our task as updating the query model from user term feedback 
there are two key subtasks here first how to choose the best terms 
to present to the user for judgment in order to gather maximal 
evidence about the user s information need second how to compute 
an updated query model based on this term feedback evidence so 
that it captures the user s information need and translates into good 
retrieval performance 
 presentation term selection 
proper selection of terms to be presented to the user for 
judgment is crucial to the success of term feedback if the terms are 
poorly chosen and there are few relevant ones the user will have a 
hard time looking for useful terms to help clarify his her 
information need if the relevant terms are plentiful but all concentrate on 
a single aspect of the query topic then we will only be able to get 
feedback on that aspect and missing others resulting in a breadth 
loss in retrieved results therefore it is important to carefully select 
presentation terms to maximize expected gain from user feedback 
i e those that can potentially reveal most evidence of the user s 
information need this is similar to active feedback which 
suggests that a retrieval system should actively probe the user s 
information need and in the case of relevance feedback the feedback 
documents should be chosen to maximize learning benefits e g 
diversely so as to increase coverage 
in our approach the top n documents from an initial retrieval 
using the original query form the source of feedback terms all 
terms that appear in them are considered candidates to present to 
the user these documents serve as pseudo-feedback since they 
provide a much richer context than the original query usually very 
short while the user is not asked to judge their relevance due to 
the latter reason it is possible to make n quite large e g in our 
experiments we set n to increase its coverage of different 
aspects in the topic 
the simplest way of selecting feedback terms is to choose the 
most frequent m terms from the n documents this method 
however has two drawbacks first a lot of common noisy terms will be 
selected due to their high frequencies in the document collection 
unless a stop-word list is used for filtering second the 
presentation list will tend to be filled by terms from major aspects of the 
topic those from a minor aspect are likely to be missed due to their 
relatively low frequencies 
we solve the above problems by two corresponding measures 
first we introduce a background model θb that is estimated from 
collection statistics and explains the common terms so that they 
are much less likely to appear in the presentation list second the 
terms are selected from multiple clusters in the pseudo-feedback 
documents to ensure sufficient representation of different aspects 
of the topic 
we rely on the mixture multinomial model which is used for 
theme discovery in specifically we assume the n documents 
contain k clusters ci i · · · k each characterized by 
a multinomial word distribution also known as unigram language 
model θi and corresponding to an aspect of the topic the 
documents are regarded as sampled from a mixture of k 
components including the k clusters and the background model 
p w d λbp w θb − λb 
k 
i 
πd ip w θi 
where w is a word λb is the mixture weight for the background 
model θb and πd i is the document-specific mixture weight for the 
i-th cluster model θi we then estimate the cluster models by 
maximizing the probability of the pseudo-feedback documents being 
generated from the multinomial mixture model 
log p d λ 
d∈d w∈v 
c w d log p w d 
where d di i · · · n is the set of the n documents v 
is the vocabulary c w d is w s frequency in d and λ θi i 
 · · · k ∪ πdij i · · · n j · · · k is the set 
of model parameters to estimate the cluster models can be 
efficiently estimated using the expectation-maximization em 
algorithm for its details we refer the reader to table shows the 
cluster models for trec query transportation tunnel disasters 
 k note that only the middle cluster is relevant 
table cluster models for topic transportation tunnel 
disasters 
cluster cluster cluster 
tunnel tunnel tunnel 
transport fire transport 
traffic truck toll 
railwai french amtrak 
harbor smoke train 
rail car airport 
bridg italian turnpik 
kilomet firefight lui 
truck blaze jersei 
construct blanc pass 
· · · · · · · · · 
from each of the k estimated clusters we choose the l 
m k terms with highest probabilities to form a total of m 
presentation terms if a term happens to be in top l in multiple clusters 
we assign it to the cluster where it has highest probability and let the 
other clusters take one more term as compensation we also filter 
out terms in the original query text because they tend to always be 
relevant when the query is short the selected terms are then 
presented to the user for judgment a sample completed feedback 
form is shown in figure 
in this study we only deal with binary judgment a presented 
term is by default unchecked and a user may check it to 
indicate relevance we also do not explicitly exploit negative feedback 
 i e penalizing irrelevant terms because with binary feedback an 
unchecked term is not necessarily irrelevant maybe the user is 
unsure about its relevance we could ask the user for finer 
judgment e g choosing from highly relevant somewhat relevant do 
not know somewhat irrelevant and highly irrelevant but binary 
feedback is more compact taking less space to display and less 
user effort to make judgment 
 estimating query models from 
term feedback 
in this section we present several algorithms for exploiting term 
feedback the algorithms take as input the original query q the 
clusters θi as generated by the theme discovery algorithm the set 
of feedback terms t and their relevance judgment r and outputs 
an updated query language model θq that makes best use of the 
feedback evidence to capture the user s information need 
first we describe our notations 
 θq the original query model derived from query terms only 
p w θq 
c w q 
 q 
where c w q is the count of w in q and q w∈q c w q 
is the query length 
 θq the updated query model which we need to estimate 
from term feedback 
 θi i k the unigram language model of cluster 
ci as estimated using the theme discovery algorithm 
 t ti j i k j l the set of terms 
presented to the user for judgment ti j is the j-th term chosen 
from cluster ci 
 r δw w ∈ t δw is an indicator variable that is if w 
is judged relevant or otherwise 
 tfb direct term feedback 
this is a straight-forward form of term feedback that does not 
involve any secondary structure we give a weight of to terms 
judged relevant by the user a weight of μ to query terms zero 
weight to other terms and then apply normalization 
p w θq 
δw μ c w q 
w ∈t δw μ q 
where w ∈t δw is the total number of terms that are judged 
relevant we call this method tfb direct term feedback 
if we let μ this approach is equivalent to appending the 
relevant terms after the original query which is what standard query 
expansion without term reweighting does if we set μ we are 
putting more emphasis on the query terms than the checked ones 
note that the result model will be more biased toward θq if the 
original query is long or the user feedback is weak which makes 
sense as we can trust more on the original query in either case 
figure filled clarification form for topic 
 transportation tunnel disasters 
please select all terms that are relevant to the topic 
traffic railway 
harbor rail 
bridge kilometer 
construct swiss 
cross link 
kong hong 
river project 
meter shanghai 
fire truck 
french smoke 
car italian 
firefights blaze 
blanc mont 
victim franc 
rescue driver 
chamonix emerge 
toll amtrak 
train airport 
turnpike lui 
jersey pass 
rome z 
center electron 
road boston 
speed bu 
submit 
 cfb cluster feedback 
here we exploit the cluster structure that played an important 
role when we selected the presentation terms the clusters 
represent different aspects of the query topic each of which may or 
may not be relevant if we are able to identify the relevant clusters 
we can combine them to generate a query model that is good at 
discovering documents belonging to these clusters instead of the 
irrelevant ones we could ask the user to directly judge the 
relevance of a cluster after viewing representative terms in that cluster 
but this would sometimes be a difficult task for the user who has to 
guess the semantics of a cluster via its set of terms which may not 
be well connected to one another due to a lack of context 
therefore we propose to learn cluster feedback indirectly inferring the 
relevance of a cluster through the relevance of its feedback terms 
because each cluster has an equal number of terms presented to 
the user the simplest measure of a cluster s relevance is the number 
of terms that are judged relevant in it intuitively the more terms 
are marked relevant in a cluster the closer the cluster is to the query 
topic and the more the cluster should participate in query 
modification if we combine the cluster models using weights determined 
this way and then interpolate with the original query model we 
get the following formula for query updating which we call cfb 
 cluster feedback 
p w θq λp w θq − λ 
k 
i 
l 
j δti j 
k 
k 
l 
j δtk j 
p w θi 
where l 
j δti j is the number of relevant terms in cluster ci and 
k 
k 
l 
j δtk j is the total number of relevant terms 
we note that when there is only one cluster k the above 
formula degenerates to 
p w θq λp w θq − λ p w θ 
which is merely pseudo-feedback of the form proposed in 
 tcfb term-cluster feedback 
tfb and cfb both have their drawbacks tfb assigns non-zero 
probabilities to the presented terms that are marked relevant but 
completely ignores a lot more others which may be left unchecked 
due to the user s ignorance or simply not included in the 
presentation list but we should be able to infer their relevance from the 
checked ones for example in figure since as many as terms 
in the middle cluster the third and fourth columns are checked 
we should have high confidence in the relevance of other terms in 
that cluster cfb remedies tfb s problem by treating the terms 
in a cluster collectively so that unchecked unpresented terms 
receive weights when presented terms in their clusters are judged as 
relevant but it does not distinguish which terms in a cluster are 
presented or judged intuitively the judged relevant terms should 
receive larger weights because they are explicitly indicated as 
relevant by the user therefore we try to combine the two methods 
hoping to get the best out of both 
we do this by interpolating the tfb model with the cfb model 
and call it tcfb 
p w θq αp w θqt f b 
 − α p w θqcf b 
 
 experiments 
in this section we describe our experiment results we first 
describe our experiment setup and present an overview of various 
methods performance then we discuss the effects of varying 
the parameter setting in the algorithms as well as the number of 
presentation terms next we analyze user term feedback behavior 
and its relation to retrieval performance finally we compare term 
feedback to relevance feedback and show that it has its particular 
advantage 
 experiment setup and basic results 
we took the opportunity of trec hard track for the 
evaluation of our algorithms the tracks used the aquaint 
collection a gb corpus of english newswire text the topics 
included ones previously known to be hard i e with low retrieval 
performance it is for these hard topics that user feedback is most 
helpful as it can provide information to disambiguate the queries 
with easy topics the user may be unwilling to spend efforts for 
feedback if the automatic retrieval results are good enough 
participants of the track were able to submit custom-designed clarification 
forms cf to solicit feedback from human assessors provided by 
table retrieval performance for different methods and cf types the last row is the percentage of map improvement over the 
baseline the parameter settings μ λ α are near optimal 
baseline tfb c tfb c tfb c cfb c cfb c cfb c tcfb c tcfb c tcfb c 
map 
pr  
rr 
 
table map variation with the number of presented terms 
 terms tfb c tfb c tfb c cfb c cfb c tcfb c tcfb c 
 
 
 
 
 
 
 
 
nist we designed three sets of clarification forms for term 
feedback differing in the choice of k the number of clusters and l 
the number of presented terms from each cluster they are × 
a big cluster with terms × clusters with terms each 
and × clusters with terms each the total number of 
presented terms m is fixed at so by comparing the performance 
of different types of clarification forms we can know the effects of 
different degree of clustering for each topic an assessor would 
complete the forms ordered by × × and × spending 
up to three minutes on each form the sample clarification form 
shown in figure is of type × it is a simple and compact 
interface in which the user can check relevant terms the form is 
self-explanatory there is no need for extra user training on how to 
use it 
our initinal queries are constructed only using the topic title 
descriptions which are on average words in length as our 
baseline we use the kl divergence retrieval method implemented 
in the lemur toolkit 
with pseudo-feedback documents we 
stem the terms choose dirichlet smoothing with a prior of 
and truncate query language models to terms these settings are 
used throughout the experiments for all other parameters we use 
lemur s default settings the baseline turns out to perform above 
average among the track participants after an initial run using this 
baseline retrieval method we take the top documents for each 
topic and apply the theme discovery algorithm to output the 
clusters or of them based on which we generate clarification 
forms after user feedback is received we run the term feedback 
algorithms tfb cfb or tcfb to estimate updated query 
models which are then used for a second iteration of retrieval 
we evaluate the different retrieval methods performance on their 
rankings of the top documents the evaluation metrics we 
adopt include mean average non-interpolated precision map 
precision at top pr  and total relevant retrieved rr table 
 shows the performance of various methods and configurations of 
k × l the suffixes c c c after tfb cfb tcfb stand 
for the number of clusters k for example tcfb c means the 
tcfb method on the × clarification forms 
from table we can make the following observations 
 
http www lemurproject com 
 all methods perform considerably better than the 
pseudofeedback baseline with tcfb c achieving a highest 
improvement in map indicating significant contribution of 
term feedback for clarification of the user s information need 
in other words term feedback is truly helpful for improving 
retrieval accuracy 
 for tfb the performance is almost equal on the × and 
 × clarification forms in terms of map although the 
latter is slightly better in pr  and rr and a little worse 
on the × ones 
 both cfb c and cfb c perform better than their tfb 
counterparts in all three metrics suggesting that feedback on a 
secondary cluster structure is indeed beneficial cfb c is 
actually worse because it cannot adjust the weight of its 
 single cluster from term feedback and it is merely 
pseudofeedback 
 although tcfb is just a simple mixture of tfb and cfb 
by interpolation it is able to outperform both this supports 
our speculation that tcfb overcomes the drawbacks of tfb 
 paying attention only to checked terms and cfb not 
distinguishing checked and unchecked terms in a cluster 
except for tcfb c v s cfb c the performance advantage 
of tcfb over tfb cfb is significant at p using the 
wilcoxon signed rank test this is not true in the case of tfb 
v s cfb each of which is better than the other in nearly half 
of the topics 
 reduction of presentation terms 
in some situations we may have to reduce the number of 
presentation terms due to limits in display space or user feedback efforts 
it is interesting to know whether our algorithms performance 
deteriorates when the user is presented with fewer terms because the 
presentation terms within each cluster are generated in decreasing 
order of their frequencies the presentation list forms a subset of the 
original one if its size is reduced 
 therefore we can easily 
simulate what happens when the number of presentation terms decreases 
 
there are complexities arising from terms appearing in top l of 
multiple clusters but these are exceptions 
from m to m we will keep all judgments of the top l m k 
terms in each cluster and discard those of others table shows the 
performance of various algorithms as the number of presentation 
terms ranges from to 
we find that the performance of tfb is more susceptible to 
presentation term reduction than that of cfb or tcfb for example 
at terms the map of tfb c is of that at terms while 
the numbers for cfb c and tcfb c are and 
respectively we conjecture the reason to be that while tfb s 
performance heavily depends on how many good terms are chosen 
for query expansion cfb only needs a rough estimate of cluster 
weights to work also the × clarification forms seem to be 
more robust than the × ones at terms the map of tfb c is 
 of that at terms lower than for tfb c similarly 
for cfb it is against this is natual as for a large 
cluster number of it is easier to get into the situation where each 
cluster gets too few presentation terms to make topic diversification 
useful 
overall we are surprised to see that the algorithms are still able 
to perform reasonably well when the number of presentation terms 
is small for example at only terms cfb c the clarification 
form is of size × can still improve over the baseline 
dropping slightly from at terms 
 user feedback analysis 
in this part we study several aspects of user s term feedback 
behavior and whether they are connected to retrieval performance 
figure clarification form completion time distributions 
 − − − − − − 
 
 
 
 
 
 
 
 
completion time seconds 
 topics 
 × 
 × 
 × 
figure shows the distribution of time needed to complete a 
clarification form 
 we see that the user is usually able to finish 
term feedback within a reasonably short amount of time for more 
than half of the topics the clarification form is completed in just 
 minute and only a small fraction of topics less than for 
 × and × take more than minutes this suggests that 
term feedback is suitable for interactive ad-hoc retrieval where a 
user usually does not want to spend too much time on providing 
feedback 
we find that a user often makes mistakes when judging term 
relevance sometimes a relevant term may be left out because its 
connection to the query topic is not obvious to the user other times a 
dubious term may be included but turns out to be irrelevant take 
the topic in figure for example there was a fire disaster in mont 
 
the maximal time is seconds as the nist assessor would be 
forced to submit the form at that moment 
table term selection statistics topic average 
cf type × × × 
 checked terms 
 rel terms 
 rel checked terms 
precision 
recall 
blanc tunnel between france and italy in but the user failed 
to select such keywords as mont blanc french and italian 
due to his her ignorance of the event indeed without proper 
context it would be hard to make perfect judgment 
what is then the extent to which the user is good at term 
feedback does it have serious impact on retrieval performance to 
answer these questions we need a measure of individual terms true 
relevance we adopt the simplified kl divergence metric used in 
 to decide query expansion terms as our term relevance 
measure 
σkld w p w r log 
p w r 
p w ¬r 
where p w r is the probability that a relevant document contains 
term w and p w ¬r is the probability that an irrelevant document 
contains w both of which can be easily computed via maximum 
likelihood estimate given document-level relevance judgment if 
σkld w w is more likely to appear in relevant documents 
than irrelevant ones 
we consider a term relevant if its simplified kl divergence 
value is greater than a certain threshold σ we can then define 
precision and recall of user term judgment accordingly precision 
is the fraction of terms checked by the user that are relevant recall 
is the fraction of presented relevant terms that are checked by the 
user table shows the number of checked terms relevant terms 
and relevant checked terms when σ is set to as well as the 
precision recall of user term judgment 
note that when the clarification forms contain more clusters 
fewer terms are checked for × for × and 
 for × similar pattern holds for relevant terms and relevant 
checked terms there seems to be a trade-off between increasing 
topic diversity by clustering and losing extra relevant terms when 
there are more clusters each of them gets fewer terms to present 
which can hurt a major relevant cluster that contains many relevant 
terms therefore it is not always helpful to have more clusters 
e g tfb c is actually worse than tfb c 
the major finding we can make from table is that the user is 
not particularly good at identifying relevant terms which echoes 
the discovery in in the case of × clarification forms the 
average number of terms checked as relevant by the user is 
per topic and the average number of relevant terms whose σkld 
value exceed is the user is able to recognize only 
of these terms on average indeed the precision and recall of user 
feedback terms as defined previously are far from perfect on 
the other hand if the user had correctly checked all such relevant 
terms the performance of our algorithms would have increased a 
lot as shown in table 
we see that tfb gets big improvement when there is an 
oracle who checks all relevant terms while cfb meets a bottleneck 
around map of since all it does is adjust cluster weights 
and when the learned weights are close to being accurate it 
cannot benefit more from term feedback also note that tcfb fails to 
outperform tfb probably because tfb is sufficiently accurate 
table change of map when using all and only relevant 
terms σkld for feedback 
original term feedback relevant term feedback 
tf 
tf 
tf 
cf 
cf 
tcf 
tcf 
 comparison with relevance feedback 
now we compare term feedback with document-level relevance 
feedback in which the user is presented with the top n documents 
from an initial retrieval and asked to judge their relevance the 
feedback process is simulated using document relevance judgment 
from nist we use the mixture model based feedback method 
proposed in with mixture noise set to and feedback 
coefficient set to 
comparative evaluation of relevance feedback against other 
methods is complicated by the fact that some documents have already 
been viewed during feedback so it makes no sense to include them 
in the retrieval results of the second run however this does not 
hold for term feedback thus to make it fair w r t user s 
information gain if the feedback documents are relevant they should be 
kept in the top of the ranking if they are irrelevant they should be 
left out therefore we use relevance feedback to produce a ranking 
of top retrieved documents but with every feedback document 
excluded and then prepend the relevant feedback documents at the 
front table shows the performance of relevance feedback for 
different values of n and compares it with tcfb c 
table performance of relevance feedback for different 
number of feedback documents n 
n map pr  rr 
 
 
 
tcfb c 
we see that the performance of tcfb c is comparable to that 
of relevance feedback using documents although it is poorer 
than when there are feedback documents in terms of map and 
pr  it does retrieve more documents when going down 
the ranked list 
we try to compare the quality of automatically inserted terms 
in relevance feedback with that of manually selected terms in term 
feedback this is done by truncating the relevance feedback 
modified query model to a size equal to the number of checked terms 
for the same topic we can then compare the terms in the truncated 
model with the checked terms figure shows the distribution of 
the terms σkld scores 
we find that term feedback tends to produce expansion terms 
of higher quality those with σkld compared to relevance 
feedback with feedback documents this does not contradict 
the fact that the latter yields higher retrieval performance actually 
when we use the truncated query model instead of the intact one 
refined from relevance feedback the map is only the truth 
figure comparison of expansion term quality between 
relevance feedback with feedback documents and term 
feedback with × cfs 
− − − − − − − − 
 
 
 
 
 
 
 
 
σkld 
 terms 
relevance feedback 
term feedback 
is although there are many unwanted terms in the expanded query 
model from feedback documents there are also more relevant terms 
than what the user can possibly select from the list of presentation 
terms generated with pseudo-feedback documents and the positive 
effects often outweights the negative ones 
we are interested to know under what circumstances term 
feedback has advantage over relevance feedback one such situation is 
when none of the top n feedback documents is relevant rendering 
relevance feedback useless this is not infrequent as one might 
have thought out of the topics there are such cases when 
n when n and still when n when this 
happens one can only back off to the original retrieval method the 
power of relevance feedback is lost 
surprisingly in out of such cases where relevance 
feedback seems impossible the user is able to check at least 
relevant terms from the × clarification form we consider term 
t to be relevant if σkld t furthermore in out of 
them tcfb c outperforms the pseudo-feedback baseline 
increasing map from to on average these are particularly 
hard topics we think that there are two possible explanations for 
this phenomenon of term feedback being active even when 
relevance feedback does not work first even if none of the top n 
 suppose it is a small number documents are relevant we may 
still find relevant documents in top which is more inclusive but 
usually unreachable when people are doing relevance feedback in 
interactive ad-hoc search from which we can draw feedback terms 
this is true for topic piracy where the top feedback 
documents are all about software piracy yet there are documents 
between - that are about piracy on the seas which is about the 
real information need contributing terms such as pirate ship 
for selection in the clarification form second for some topics 
a document needs to meet some special condition in order to be 
relevant the top n documents may be related to the topic but 
nonetheless irrelevant in this case we may still extract useful 
terms from these documents even if they do not qualify as 
relevant ones for example in topic consumer online shopping 
a document needs to mention what contributes to shopping growth 
to really match the specified information need hence none of the 
top feedback documents are regarded as relevant but 
nevertheless the feedback terms such as retail commerce are good for 
query expansion 
 conclusions 
in this paper we studied the use of term feedback for 
interactive information retrieval in the language modeling approach we 
proposed a cluster-based method for selecting presentation terms 
as well as algorithms to estimate refined query models from user 
term feedback we saw significant improvement in retrieval 
accuracy brought by term feedback in spite of the fact that a user often 
makes mistakes in relevance judgment that hurts its performance 
we found the best-performing algorithm to be tcfb which 
benefits from the combination of directly observed term evidence with 
tfb and indirectly learned cluster relevance with cfb when we 
reduced the number of presentation terms term feedback is still 
able to keep much of its performance gain over the baseline 
finally we compared term feedback to document-level relevance 
feedback and found that tcfb c s performance is on a par with the 
latter with feedback documents we regarded term feedback as a 
viable alternative to traditional relevance feedback especially when 
there are no relevant documents in the top 
we propose to extend our work in several ways first we want 
to study whether the use of various contexts can help the user to 
better identify term relevance while not sacrificing the simplicity 
and compactness of term feedback second currently all terms are 
presented to the user in a single batch we could instead consider 
iterative term feedback by presenting a small number of terms first 
and show more terms after receiving user feedback or stop when 
the refined query is good enough the presented terms should be 
selected dynamically to maximize learning benefits at any moment 
third we have plans to incorporate term feedback into our ucair 
toolbar an internet explorer plugin to make it work for web 
search we are also interested in studying how to combine term 
feedback with relevance feedback or implicit feedback we could 
for example allow the user to dynamically modify terms in a 
language model learned from feedback documents 
 acknowledgment 
this work is supported in part by the national science 
foundation grants iis- and iis- 
 references 
 j allan relevance feedback with too much data in proceedings of 
the th annual international acm sigir conference on research 
and development in information retrieval pages - 
 j allan hard track overview in trec - high accuracy 
retrieval from documents in the fourteenth text retrieval 
conference 
 p anick using terminological feedback for web search refinement 
a log-based study in proceedings of the th annual international 
acm sigir conference on research and development in informaion 
retrieval pages - 
 p g anick and s tipirneni the paraphrase search assistant 
terminological feedback for iterative information seeking in 
proceedings of the nd annual international acm sigir 
conference on research and development in information retrieval 
pages - 
 c buckley g salton j allan and a singhal automatic query 
expansion using smart in proceedings of the third text retrieval 
conference 
 d harman towards interactive query expansion in proceedings of 
the th annual international acm sigir conference on research 
and development in information retrieval pages - 
 n a jaleel a corrada-emmanuel q li x liu c wade and 
j allan umass at trec hard and qa in trec pages 
 - 
 h joho c coverson m sanderson and m beaulieu hierarchical 
presentation of expansion terms in proceedings of the acm 
symposium on applied computing pages - 
 k s jones s walker and s e robertson a probabilistic model of 
information retrieval development and status technical report 
computer laboratory university of cambridge 
 d kelly v d dollu and x fu the loquacious user a 
document-independent source of terms for query expansion in 
proceedings of the th annual international acm sigir 
conference on research and development in information retrieval 
pages - 
 d kelly and x fu elicitation of term relevance feedback an 
investigation of term source and context in proceedings of the th 
annual international acm sigir conference on research and 
development in information retrieval 
 j koenemann and n belkin a case for interaction a study of 
interactive information retrieval behavior and effectiveness in 
proceedings of the sigchi conference on human factors in 
computing systems pages - 
 v lavrenko and w b croft relevance-based language models in 
research and development in information retrieval pages 
 - 
 y nemeth b shapira and m taeib-maimon evaluation of the real 
and perceived value of automatic and interactive query expansion in 
proceedings of the th annual international acm sigir 
conference on research and development in information retrieval 
pages - 
 j ponte a language modeling approach to information retrieval 
phd thesis university of massachusetts at amherst 
 s e robertson s walker s jones m beaulieu and m gatford 
okapi at trec- in proceedings of the third text retrieval 
conference 
 j rocchio relevance feedback in information retrieval in the 
smart retrieval system pages - 
 i ruthven re-examining the potential effectiveness of interactive 
query expansion in proceedings of the th annual international 
acm sigir conference on research and development in informaion 
retrieval pages - 
 g salton and c buckley improving retrieval performance by 
relevance feedback journal of the american society for information 
science - 
 x shen b tan and c zhai implicit user modeling for 
personalized search in proceedings of the th acm international 
conference on information and knowledge management pages 
 - 
 x shen and c zhai active feedback in ad-hoc information 
retrieval in proceedings of the th annual international acm 
sigir conference on research and development in information 
retrieval pages - 
 a spink term relevance feedback and query expansion relation to 
design in proceedings of the th annual international acm sigir 
conference on research and development in information retrieval 
pages - 
 j xu and w b croft query expansion using local and global 
document analysis in proceedings of the th annual international 
acm sigir conference on research and development in information 
retrieval pages - 
 h zaragoza n craswell m taylor s saria and s robertson 
microsoft cambridge at trec- web and hard tracks in 
proceedings of the th text retrieval conference 
 c zhai and j lafferty model-based feedback in the language 
modeling approach to information retrieval in proceedings of the 
tenth international conference on information and knowledge 
management pages - 
 c zhai a velivelli and b yu a cross-collection mixture model 
for comparative text mining in proceedings of the tenth acm 
sigkdd international conference on knowledge discovery and data 
mining pages - 
