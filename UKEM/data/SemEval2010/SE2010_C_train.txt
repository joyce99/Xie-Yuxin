adapting asynchronous messaging middleware 
to ad-hoc networking 
mirco musolesi 
dept of computer science 
university college london 
gower street london 
wc e bt united kingdom 
m musolesi cs ucl ac uk 
cecilia mascolo 
dept of computer science 
university college london 
gower street london 
wc e bt united kingdom 
c mascolo cs ucl ac uk 
stephen hailes 
dept of computer science 
university college london 
gower street london 
wc e bt united kingdom 
s hailes cs ucl ac uk 
abstract 
the characteristics of mobile environments with the 
possibility of frequent disconnections and fluctuating bandwidth 
have forced a rethink of traditional middleware in 
particular the synchronous communication paradigms often 
employed in standard middleware do not appear to be 
particularly suited to ad-hoc environments in which not even 
the intermittent availability of a backbone network can be 
assumed instead asynchronous communication seems to 
be a generally more suitable paradigm for such 
environments message oriented middleware for traditional systems 
has been developed and used to provide an asynchronous 
paradigm of communication for distributed systems and 
recently also for some specific mobile computing systems 
in this paper we present our experience in designing 
implementing and evaluating emma epidemic messaging 
middleware for ad-hoc networks an adaptation of java 
message service jms for mobile ad-hoc environments we 
discuss in detail the design challenges and some possible 
solutions showing a concrete example of the feasibility and 
suitability of the application of the asynchronous paradigm 
in this setting and outlining a research roadmap for the 
coming years 
categories and subject descriptors 
c computer-communication networks distributed 
systems-distributed applications c network 
architecture and design wireless communication 
general terms 
design algorithms 
 introduction 
with the increasing popularity of mobile devices and their 
widespread adoption there is a clear need to allow the 
development of a broad spectrum of applications that operate 
effectively over such an environment unfortunately this is far 
from simple mobile devices are increasingly heterogeneous 
in terms of processing capabilities memory size battery 
capacity and network interfaces each such configuration has 
substantially different characteristics that are both statically 
different - for example there is a major difference in 
capability between a berkeley mote and an g-equipped 
laptop - and that vary dynamically as in situations of 
fluctuating bandwidth and intermittent connectivity mobile ad 
hoc environments have an additional element of complexity 
in that they are entirely decentralised 
in order to craft applications for such complex 
environments an appropriate form of middleware is essential if cost 
effective development is to be achieved in this paper we 
examine one of the foundational aspects of middleware for 
mobile ad-hoc environments that of the communication 
primitives 
traditionally the most frequently used middleware 
primitives for communication assume the simultaneous presence 
of both end points on a network since the stability and 
pervasiveness of the networking infrastructure is not an 
unreasonable assumption for most wired environments in other 
words most communication paradigms are synchronous 
object oriented middleware such as corba and java rmi are 
typical examples of middleware based on synchronous 
communication 
in recent years there has been growing interest in 
platforms based on asynchronous communication paradigms such 
as publish-subscribe systems these have been exploited 
very successfully where there is application level 
asynchronicity from a gartner market report given 
messageoriented-middleware s mom popularity scalability 
flexibility and affinity with mobile and wireless architectures 
by mom will emerge as the dominant form of 
communication middleware for linking mobile and enterprise 
applications probability moreover in mobile ad-hoc 
systems the likelihood of network fragmentation means that 
synchronous communication may in any case be 
impracticable giving situations in which delay tolerant asynchronous 
traffic is the only form of traffic that could be supported 
 middleware companion 
middleware for mobile ad-hoc environments must therefore 
support semi-synchronous or completely asynchronous 
communication primitives if it is to avoid substantial 
limitations to its utility aside from the intellectual challenge in 
supporting this model this work is also interesting because 
there are a number of practical application domains in 
allowing inter-community communication in undeveloped 
areas of the globe thus for example projects that have been 
carried out to help populations that live in remote places of 
the globe such as lapland or in poor areas that lack fixed 
connectivity infrastructure 
there have been attempts to provide mobile middleware 
with these properties including steam lime 
xmiddle bayou see for a more complete review of mobile 
middleware these models differ quite considerably from 
the existing traditional middleware in terms of primitives 
provided furthermore some of them fail in providing a 
solution for the true ad-hoc scenarios 
if the projected success of mom becomes anything like 
a reality there will be many programmers with experience 
of it the ideal solution to the problem of middleware for 
ad-hoc systems is then to allow programmers to utilise the 
same paradigms and models presented by common forms of 
mom and to ensure that these paradigms are supportable 
within the mobile environment this approach has clear 
advantages in allowing applications developed on standard 
middleware platforms to be easily deployed on mobile 
devices indeed some research has already led to the 
adaptation of traditional middleware platforms to mobile settings 
mainly to provide integration between mobile devices and 
existing fixed networks in a nomadic i e mixed 
environment with respect to message oriented middleware the 
current implementations however either assume the 
existence of a backbone network to which the mobile hosts 
connect from time to time while roaming or assume that 
nodes are always somehow reachable through a path 
no adaptation to heterogeneous or completely ad-hoc 
scenarios with frequent disconnection and periodically isolated 
clouds of hosts has been attempted 
in the remainder of this paper we describe an initial 
attempt to adapt message oriented middleware to suit mobile 
and more specifically mobile ad-hoc networks in our case 
we elected to examine jms as one of the most widely known 
mom systems in the latter part of this paper we explore 
the limitations of our results and describe the plans we have 
to take the work further 
 message oriented middleware 
and java message service jms 
message-oriented middleware systems support 
communication between distributed components via message-passing 
the sender sends a message to identified queues which 
usually reside on a server a receiver retrieves the message from 
the queue at a different time and may acknowledge the reply 
using the same asynchronous mechanism message-oriented 
middleware thus supports asynchronous communication in 
a very natural way achieving de-coupling of senders and 
receivers a sender is able to continue processing as soon 
as the middleware has accepted the message eventually 
the receiver will send an acknowledgment message and the 
sender will be able to collect it at a convenient time 
however given the way they are implemented these middleware 
systems usually require resource-rich devices especially in 
terms of memory and disk space where persistent queues 
of messages that have been received but not yet processed 
are stored sun java message service ibm websphere 
mq microsoft msmq are examples of very 
successful message-oriented middleware for traditional distributed 
systems 
the java messaging service jms is a collection of 
interfaces for asynchronous communication between distributed 
components it provides a common way for java programs 
to create send and receive messages jms users are usually 
referred to as clients the jms specification further defines 
providers as the components in charge of implementing the 
messaging system and providing the administrative and 
control functionality i e persistence and reliability required 
by the system clients can send and receive messages 
asynchronously through the jms provider which is in charge of 
the delivery and possibly of the persistence of the messages 
there are two types of communication supported point 
to point and publish-subscribe models in the point to point 
model hosts send messages to queues receivers can be 
registered with some specific queues and can asynchronously 
retrieve the messages and then acknowledge them the 
publish-subscribe model is based on the use of topics that 
can be subscribed to by clients messages are sent to topics 
by other clients and are then received in an asynchronous 
mode by all the subscribed clients clients learn about the 
available topics and queues through java naming and 
directory interface jndi queues and topics are created 
by an administrator on the provider and are registered with 
the jndi interface for look-up 
in the next section we introduce the challenges of mobile 
networks and show how jms can be adapted to cope with 
these requirements 
 jms for mobile computing 
mobile networks vary very widely in their characteristics 
from nomadic networks in which modes relocate whilst 
offline through to ad-hoc networks in which modes move freely 
and in which there is no infrastructure mobile ad-hoc 
networks are most generally applicable in situations where 
survivability and instant deployability are key most notably in 
military applications and disaster relief in between these 
two types of mobile networks there are however a number 
of possible heterogeneous combinations where nomadic and 
ad-hoc paradigms are used to interconnect totally unwired 
areas to more structured networks such as a lan or the 
internet 
whilst the jms specification has been extensively 
implemented and used in traditional distributed systems 
adaptations for mobile environments have been proposed only 
recently the challenges of porting jms to mobile settings 
are considerable however in view of its widespread 
acceptance and use there are considerable advantages in allowing 
the adaptation of existing applications to mobile 
environments and in allowing the interoperation of applications in 
the wired and wireless regions of a network 
in jms was adapted to a nomadic mobile setting 
where mobile hosts can be jms clients and communicate 
through the jms provider that however sits on a 
backbone network providing reliability and persistence the 
client prototype presented in is very lightweight due 
to the delegation of all the heavyweight functionality to the 
middleware for pervasive and ad-hoc computing 
provider on the wired network however this approach is 
somewhat limited in terms of widespread applicability and 
scalability as a consequence of the concentration of 
functionality in the wired portion of the network 
if jms is to be adapted to completely ad-hoc 
environments where no fixed infrastructure is available and where 
nodes change location and status very dynamically more 
issues must be taken into consideration firstly discovery 
needs to use a resilient but distributed model in this 
extremely dynamic environment static solutions are 
unacceptable as discussed in section a jms administrator defines 
queues and topics on the provider clients can then learn 
about them using the java naming and directory interface 
 jndi however due to the way jndi is designed a jndi 
node or more than one needs to be in reach in order to 
obtain a binding of a name to an address i e knowing where 
a specific queue topic is in mobile ad-hoc environments 
the discovery process cannot assume the existence of a fixed 
set of discovery servers that are always reachable as this 
would not match the dynamicity of ad-hoc networks 
secondly a jms provider as suggested by the jms 
specification also needs to be reachable by each node in the 
network in order to communicate this assumes a very 
centralised architecture which again does not match the 
requirements of a mobile ad-hoc setting in which nodes may 
be moving and sparse a more distributed and dynamic 
solution is needed persistence is however essential 
functionality in asynchronous communication environments as hosts 
are by definition connected at different times 
in the following section we will discuss our experience 
in designing and implementing jms for mobile ad-hoc 
networks 
 jmsfor mobile ad-hoc networks 
 adaptation of jms for mobile ad-hoc 
networks 
developing applications for mobile networks is yet more 
challenging in addition to the same considerations as for 
infrastructured wireless environments such as the limited 
device capabilities and power constraints there are issues 
of rate of change of network connectivity and the lack of a 
static routing infrastructure consequently we now describe 
an initial attempt to adapt the jms specification to target 
the particular requirements related to ad-hoc scenarios as 
discussed in section a jms application can use either the 
point to point and the publish-subscribe styles of messaging 
point to point model the point to point model is based 
on the concept of queues that are used to enable 
asynchronous communication between the producer of a message 
and possible different consumers in our solution the 
location of queues is determined by a negotiation process that 
is application dependent for example let us suppose that 
it is possible to know a priori or it is possible to determine 
dynamically that a certain host is the receiver of the most 
part of messages sent to a particular queue in this case the 
optimum location of the queue may well be on this 
particular host in general it is worth noting that according to the 
jms specification and suggested design patterns it is 
common and preferable for a client to have all of its messages 
delivered to a single queue 
queues are advertised periodically to the hosts that are 
within transmission range or that are reachable by means of 
the underlying synchronous communication protocol if 
provided it is important to note that at the middleware level 
it is logically irrelevant whether or not the network layer 
implements some form of ad-hoc routing though considerably 
more efficient if it does the middleware only considers 
information about which nodes are actively reachable at any 
point in time the hosts that receive advertisement 
messages add entries to their jndi registry each entry is 
characterized by a lease a mechanism similar to that present 
in jini a lease represents the time of validity of a 
particular entry if a lease is not refreshed i e its life is 
not extended it can expire and consequently the entry 
is deleted from the registry in other words the host 
assumes that the queue will be unreachable from that point 
in time this may be caused for example if a host storing 
the queue becomes unreachable a host that initiates a 
discovery process will find the topics and the queues present 
in its connected portion of the network in a straightforward 
manner 
in order to deliver a message to a host that is not 
currently in reach 
 we use an asynchronous epidemic routing 
protocol that will be discussed in detail in section if two 
hosts are in the same cloud i e a connected path exists 
between them but no synchronous protocol is available the 
messages are sent using the epidemic protocol in this case 
the delivery latency will be low as a result of the rapidity of 
propagation of the infection in the connected cloud see also 
the simulation results in section given the existence of 
an epidemic protocol the discovery mechanism consists of 
advertising the queues to the hosts that are currently 
unreachable using analogous mechanisms 
publish-subscribe model in the publish-subscribe model 
some of the hosts are similarly designated to hold topics and 
store subscriptions as before topics are advertised through 
the registry in the same way as are queues and a client 
wishing to subscribe to a topic must register with the client 
holding the topic when a client wishes to send a message 
to the topic list it sends it to the topic holder in the same 
way as it would send a message to a queue the topic 
holder then forwards the message to all subscribers using 
the synchronous protocol if possible the epidemic protocol 
otherwise it is worth noting that we use a single message 
with multiple recipients instead of multiple messages with 
multiple recipients when a message is delivered to one of 
the subscribers this recipient is deleted from the list in 
order to delete the other possible replicas we employ 
acknowledgment messages discussed in section returned 
in the same way as a normal message 
we have also adapted the concepts of durable and non 
durable subscriptions for ad-hoc settings in fixed platforms 
durable subscriptions are maintained during the 
disconnections of the clients whether these are intentional or are the 
result of failures in traditional systems while a durable 
subscriber is disconnected from the server it is responsible 
for storing messages when the durable subscriber 
reconnects the server sends it all unexpired messages the 
problem is that in our scenario disconnections are the norm 
 
in theory it is not possible to send a message to a peer that 
has never been reachable in the past since there can be no 
entry present in the registry however to overcome this 
possible limitation we provide a primitive through which 
information can be added to the registry without using the 
normal channels 
 middleware companion 
rather than the exception in other words we cannot 
consider disconnections as failures for these reasons we adopt 
a slightly different semantics with respect to durable 
subscriptions if a subscriber becomes disconnected 
notifications are not stored but are sent using the epidemic 
protocol rather than the synchronous protocol in other words 
durable notifications remain valid during the possible 
disconnections of the subscriber 
on the other hand if a non-durable subscriber becomes 
disconnected its subscription is deleted in other words 
during disconnections notifications are not sent using the 
epidemic protocol but exploit only the synchronous protocol if 
the topic becomes accessible to this host again it must make 
another subscription in order to receive the notifications 
unsubscription messages are delivered in the same way 
as are subscription messages it is important to note that 
durable subscribers have explicitly to unsubscribe from a 
topic in order to stop the notification process however all 
durable subscriptions have a predefined expiration time in 
order to cope with the cases of subscribers that do not meet 
again because of their movements or failures this feature 
is clearly provided to limit the number of the unnecessary 
messages sent around the network 
 message delivery using epidemic routing 
in this section we examine one possible mechanism that 
will allow the delivery of messages in a partially connected 
network the mechanism we discuss is intended for the 
purposes of demonstrating feasibility more efficient 
communication mechanisms for this environment are themselves 
complex and are the subject of another paper 
the asynchronous message delivery described above is 
based on a typical pure epidemic-style routing protocol 
a message that needs to be sent is replicated on each host in 
reach in this way copies of the messages are quickly spread 
through connected networks like an infection if a host 
becomes connected to another cloud of mobile nodes during 
its movement the message spreads through this collection 
of hosts epidemic-style replication of data and messages 
has been exploited in the past in many fields starting with 
the distributed database systems area 
within epidemic routing each host maintains a buffer 
containing the messages that it has created and the replicas 
of the messages generated by the other hosts to improve 
the performance a hash-table indexes the content of the 
buffer when two hosts connect the host with the smaller 
identifier initiates a so-called anti-entropy session sending 
a list containing the unique identifiers of the messages that 
it currently stores the other host evaluates this list and 
sends back a list containing the identifiers it is storing that 
are not present in the other host together with the messages 
that the other does not have the host that has started the 
session receives the list and in the same way sends the 
messages that are not present in the other host should buffer 
overflow occur messages are dropped 
the reliability offered by this protocol is typically best 
effort since there is no guarantee that a message will 
eventually be delivered to its recipient clearly the delivery ratio 
of the protocol increases proportionally to the maximum 
allowed delay time and the buffer size in each host interesting 
simulation results may be found in 
 adaptation of the jms message model 
in this section we will analyse the aspects of our 
adaptation of the specification related to the so-called jms message 
model according to this jms messages are 
characterised by some properties defined using the header field 
which contains values that are used by both clients and 
providers for their delivery the aspects discussed in the 
remainder of this section are valid for both models point to 
point and publish-subscribe 
a jms message can be persistent or non-persistent 
according to the jms specification persistent messages must 
be delivered with a higher degree of reliability than the 
nonpersistent ones however it is worth noting that it is not 
possible to ensure once-and-only-once reliability for 
persistent messages as defined in the specification since as we 
discussed in the previous subsection the underlying epidemic 
protocol can guarantee only best-effort delivery however 
clients maintain a list of the identifiers of the recently 
received messages to avoid the delivery of message duplicates 
in other words we provide the applications with 
at-mostonce reliability for both types of messages 
in order to implement different levels of reliability emma 
treats persistent and non-persistent messages differently 
during the execution of the anti-entropy epidemic protocol since 
the message buffer space is limited persistent messages are 
preferentially replicated using the available free space if 
this is insufficient and non-persistent messages are present 
in the buffer these are replaced only the successful 
deliveries of the persistent messages are notified to the senders 
according to the jms specification it is possible to assign 
a priority to each message the messages with higher 
priorities are delivered in a preferential way as discussed above 
persistent messages are prioritised above the non-persistent 
ones further selection is based on their priorities messages 
with higher priorities are treated in a preferential way in 
fact if there is not enough space to replicate all the 
persistent messages a mechanism based on priorities is used to 
delete and replicate non-persistent messages and if 
necessary persistent messages 
messages are deleted from the buffers using the expiration 
time value that can be set by senders this is a way to free 
space in the buffers one preferentially deletes older 
messages in cases of conflict to eliminate stale replicas in the 
system and to limit the time for which destinations must 
hold message identifiers to dispose of duplicates 
 reliability and acknowledgment 
mechanisms 
as already discussed at-most-once message delivery is the 
best that can be achieved in terms of delivery semantics in 
partially connected ad-hoc settings however it is 
possible to improve the reliability of the system with efficient 
acknowledgment mechanisms emma provides a 
mechanism for failure notification to applications if the 
acknowledgment is not received within a given timeout that can 
be configured by application developers this mechanism 
is the one that distinguishes the delivery of persistent and 
non-persistent messages in our jms implementation the 
deliveries of the former are notified to the senders whereas 
the latter are not 
we use acknowledgment messages not only to inform senders 
about the successful delivery of messages but also to delete 
the replicas of the delivered messages that are still present 
in the network each host maintains a list of the messages 
middleware for pervasive and ad-hoc computing 
successfully delivered that is updated as part of the normal 
process of information exchange between the hosts the lists 
are exchanged during the first steps of the anti-entropic 
epidemic protocol with a certain predefined frequency in the 
case of messages with multiple recipients a list of the actual 
recipients is also stored when a host receives the list it 
checks its message buffer and updates it according to the 
following rules if a message has a single recipient and 
it has been delivered it is deleted from the buffer if a 
message has multiple recipients the identifiers of the 
delivered hosts are deleted from the associated list of recipients 
if the resulting length of the list of recipients is zero the 
message is deleted from the buffer 
these lists have clearly finite dimensions and are 
implemented as circular queues this simple mechanism together 
with the use of expiration timestamps guarantees that the 
old acknowledgment notifications are deleted from the 
system after a limited period of time 
in order to improve the reliability of emma a design 
mechanism for intelligent replication of queues and topics 
based on the context information could be developed 
however this is not yet part of the current architecture of emma 
 implementation and preliminary 
evaluation 
we implemented a prototype of our platform using the 
j me personal profile the size of the executable is about 
 kb including the jms jar file this is a perfectly 
acceptable figure given the available memory of the current 
mobile devices on the market we tested our prototype on 
hp ipaq pdas running linux interconnected with 
wavelan and on a number of laptops with the same network 
interface 
we also evaluated the middleware platform using the 
omnet discrete event simulator in order to explore a 
range of mobile scenarios that incorporated a more realistic 
number of hosts than was achievable experimentally more 
specifically we assessed the performance of the system in 
terms of delivery ratio and average delay varying the 
density of population and the buffer size and using persistent 
and non-persistent messages with different priorities 
the simulation results show that the emma s 
performance in terms of delivery ratio and delay of persistent 
messages with higher priorities is good in general it is 
evident that the delivery ratio is strongly related to the 
correct dimensioning of the buffers to the maximum acceptable 
delay moreover the epidemic algorithms are able to 
guarantee a high delivery ratio if one evaluates performance over 
a time interval sufficient for the dissemination of the replicas 
of messages i e the infection spreading in a large portion 
of the ad-hoc network 
one consequence of the dimensioning problem is that 
scalability may be seriously impacted in peer-to-peer 
middleware for mobile computing due to the resource poverty of 
the devices limited memory to store temporarily messages 
and the number of possible interconnections in ad-hoc 
settings what is worse is that common forms of commercial 
and social organisation six degrees of separation mean that 
even modest ttl values on messages will lead to widespread 
flooding of epidemic messages this problem arises because 
of the lack of intelligence in the epidemic protocol and can 
be addressed by selecting carrier nodes for messages with 
greater care the details of this process are however 
outside the scope of this paper but may be found in and do 
not affect the foundation on which the emma middleware 
is based the ability to deliver messages asynchronously 
 critical view of the state of 
the art 
the design of middleware platforms for mobile 
computing requires researchers to answer new and fundamentally 
different questions simply assuming the presence of wired 
portions of the network on which centralised functionality 
can reside is not generalisable thus it is necessary to 
investigate novel design principles and to devise architectural 
patterns that differ from those traditionally exploited in the 
design of middleware for fixed systems 
as an example consider the recent cross-layering trend in 
ad-hoc networking this is a way of re-thinking software 
systems design explicitly abandoning the classical forms of 
layering since although this separation of concerns afford 
portability it does so at the expense of potential efficiency 
gains we believe that it is possible to view our approach 
as an instance of cross-layering in fact we have added the 
epidemic network protocol at middleware level and at the 
same time we have used the existing synchronous network 
protocol if present both in delivering messages traditional 
layering and in informing the middleware about when 
messages may be delivered by revealing details of the forwarding 
tables layer violation for this reason we prefer to 
consider them jointly as the communication layer of our 
platform together providing more efficient message delivery 
another interesting aspect is the exploitation of context 
and system information to improve the performance of 
mobile middleware platforms again as a result of adopting 
a cross-layering methodology we are able to build systems 
that gather information from the underlying operating 
system and communication components in order to allow for 
adaptation of behaviour we can summarise this conceptual 
design approach by saying that middleware platforms must 
be not only context-aware i e they should be able to 
extract and analyse information from the surrounding context 
but also system-aware i e they should be able to gather 
information from the software and hardware components of 
the mobile system 
a number of middleware systems have been developed to 
support ad-hoc networking with the use of asynchronous 
communication such as lime xmiddle steam 
in particular the steam platform is an interesting 
example of event-based middleware for ad-hoc networks 
providing location-aware message delivery and an effective solution 
for event filtering 
a discussion of jms and its mobile realisation has 
already been conducted in sections and the swiss 
company softwired has developed the first jms middleware for 
mobile computing called ibus mobile the main 
components of this typically infrastructure-based architecture 
are the jms provider the so-called mobile jms gateway 
which is deployed on a fixed host and a lightweight jms 
client library the gateway is used for the communication 
between the application server and mobile hosts the 
gateway is seen by the jms provider as a normal jms client the 
jms provider can be any jms-enabled application server 
such as bea weblogic pronto is an example of 
mid middleware companion 
dleware system based on messaging that is specifically 
designed for mobile environments the platform is composed 
of three classes of components mobile clients implementing 
the jms specification gateways that control traffic 
guaranteeing efficiency and possible user customizations using 
different plug-ins and jms servers different configurations 
of these components are possible with respect to mobile ad 
hoc networks applications the most interesting is 
serverless jms the aim of this configuration is to adapt jms 
to a decentralized model the publish-subscribe model 
exploits the efficiency and the scalability of the underlying ip 
multicast protocol unreliable and reliable message delivery 
services are provided reliability is provided through a 
negative acknowledgment-based protocol pronto represents a 
good solution for infrastructure-based mobile networks but 
it does not adequately target ad-hoc settings since mobile 
nodes rely on fixed servers for the exchange of messages 
other mom implemented for mobile environments exist 
however they are usually straightforward extensions of 
existing middleware the only implementation of mom 
specifically designed for mobile ad-hoc networks was 
developed at the university of newcastle this work is again 
a jms adaptation the focus of that implementation is on 
group communication and the use of application level 
routing algorithms for topic delivery of messages however there 
are a number of differences in the focus of our work the 
importance that we attribute to disconnections makes 
persistence a vital requirement for any middleware that needs 
to be used in mobile ad-hoc networks the authors of 
signal persistence as possible future work not considering 
the fact that routing a message to a non-connected host will 
result in delivery failure this is a remarkable limitation in 
mobile settings where unpredictable disconnections are the 
norm rather than the exception 
 roadmap and conclusions 
asynchronous communication is a useful communication 
paradigm for mobile ad-hoc networks as hosts are allowed to 
come go and pick up messages when convenient also taking 
account of their resource availability e g power 
connectivity levels in this paper we have described the state of the 
art in terms of mom for mobile systems we have also 
shown a proof of concept adaptation of jms to the extreme 
scenario of partially connected mobile ad-hoc networks 
we have described and discussed the characteristics and 
differences of our solution with respect to traditional jms 
implementations and the existing adaptations for mobile 
settings however trade-offs between application-level routing 
and resource usage should also be investigated as mobile 
devices are commonly power resource scarce a key 
limitation of this work is the poorly performing epidemic 
algorithm and an important advance in the practicability of 
this work requires an algorithm that better balances the 
needs of efficiency and message delivery probability we 
are currently working on algorithms and protocols that 
exploiting probabilistic and statistical techniques on the basis 
of small amounts of exchanged information are able to 
improve considerably the efficiency in terms of resources 
 memory bandwidth etc and the reliability of our middleware 
platform 
one futuristic research development which may take these 
ideas of adaptation of messaging middleware for mobile 
environments further is the introduction of more mobility 
oriented communication extensions for instance the support 
of geocast i e the ability to send messages to specific 
geographical areas 
 references 
 m conti g maselli g turi and s giordano 
cross-layering in mobile ad-hoc network design ieee 
computer - february 
 a demers d greene c hauser w irish j larson 
s shenker h sturgis d swinehart and d terry 
epidemic algorithms for replicated database 
maintenance in sixth symposium on principles of 
distributed computing pages - august 
 a doria m uden and d p pandey providing 
connectivity to the saami nomadic community in 
proceedings of the second international conference on 
open collaborative design for sustainable innovation 
december 
 m haahr r cunningham and v cahill supporting 
corba applications in a mobile environment in th 
international conference on mobile computing and 
networking mobicom pages - acm august 
 
 m hapner r burridge r sharma j fialli and 
k stout java message service specification version 
sun microsystems inc april 
http java sun com products jms 
 j hart websphere mq connecting your applications 
without complex programming ibm websphere software 
white papers 
 s hayward and m pezzini marrying middleware and 
mobile computing gartner group research report 
september 
 ibm websphere mq everyplace version november 
 http www- ibm com software integration wmqe 
 itu connecting remote communities documents of the 
world summit on information society 
http www itu int osg spu wsis-themes 
 s maffeis introducing wireless jms softwired ag 
www sofwired-inc com 
 c mascolo l capra and w emmerich middleware for 
mobile computing in e gregori g anastasi and 
s basagni editors advanced lectures on networking 
volume of lecture notes in computer science pages 
 - springer verlag 
 microsoft microsoft message queuing msmq version 
 documentation 
 m musolesi s hailes and c mascolo adaptive routing 
for intermittently connected mobile ad-hoc networks 
technical report ucl-cs research note july 
submitted for publication 
 sun microsystems java naming and directory interface 
 jndi documentation version 
http java sun com products jndi 
 sun microsystems jini specification version 
http java sun com products jini 
 a vahdat and d becker epidemic routing for partially 
connected ad-hoc networks technical report cs- - 
department of computer science duke university 
 a vargas the omnet discrete event simulation 
system in proceedings of the european simulation 
multiconference esm prague june 
 e vollset d ingham and p ezhilchelvan jms on mobile 
ad-hoc networks in personal wireless communications 
 pwc pages - venice september 
 e yoneki and j bacon pronto mobilegateway with 
publish-subscribe paradigm over wireless network 
technical report university of cambridge computer 
laboratory february 
middleware for pervasive and ad-hoc computing 
evaluating adaptive resource management for 
distributed real-time embedded systems 
nishanth shankaran 
∗ 
xenofon koutsoukos douglas c schmidt and aniruddha gokhale 
dept of eecs vanderbilt university nashville 
abstract 
a challenging problem faced by researchers and developers 
of distributed real-time and embedded dre systems is 
devising and implementing effective adaptive resource 
management strategies that can meet end-to-end quality of service 
 qos requirements in varying operational conditions this 
paper presents two contributions to research in adaptive 
resource management for dre systems first we describe the 
structure and functionality of the hybrid adaptive 
resourcemanagement middleware hyarm which provides 
adaptive resource management using hybrid control techniques 
for adapting to workload fluctuations and resource 
availability second we evaluate the adaptive behavior of hyarm 
via experiments on a dre multimedia system that distributes 
video in real-time our results indicate that hyarm yields 
predictable stable and high system performance even in the 
face of fluctuating workload and resource availability 
categories and subject descriptors 
c distributed systems distributed applications 
d organization and design real-time systems and 
embedded systems 
 introduction 
achieving end-to-end real-time quality of service qos 
is particularly important for open distributed real-time and 
embedded dre systems that face resource constraints such 
as limited computing power and network bandwidth 
overutilization of these system resources can yield unpredictable 
and unstable behavior whereas under-utilization can yield 
excessive system cost a promising approach to meeting 
these end-to-end qos requirements effectively therefore is 
to develop and apply adaptive middleware which is 
software whose functional and qos-related properties can be 
modified either statically or dynamically static 
modifications are carried out to reduce footprint leverage 
capabilities that exist in specific platforms enable functional 
subsetting and or minimize hardware software infrastructure 
dependencies objectives of dynamic modifications include 
optimizing system responses to changing environments or 
requirements such as changing component interconnections 
power-levels cpu and network bandwidth availability 
latency jitter and workload 
in open dre systems adaptive middleware must make 
such modifications dependably i e while meeting 
stringent end-to-end qos requirements which requires the 
specification and enforcement of upper and lower bounds on 
system resource utilization to ensure effective use of 
system resources to meet these requirements we have 
developed the hybrid adaptive resource-management 
middleware hyarm which is an open-source 
distributed 
resource management middleware 
hyarm is based on hybrid control theoretic techniques 
which provide a theoretical framework for designing 
control of complex system with both continuous and discrete 
dynamics in our case study which involves a distributed 
real-time video distribution system the task of adaptive 
resource management is to control the utilization of the 
different resources whose utilizations are described by 
continuous variables we achieve this by adapting the resolution 
of the transmitted video which is modeled as a continuous 
variable and by changing the frame-rate and the 
compression which are modeled by discrete actions we have 
implemented hyarm atop the ace orb tao which 
is an implementation of the real-time corba 
specification our results show that hyarm ensures 
effective system resource utilization and end-to-end qos 
requirements of higher priority applications are met even in 
the face of fluctuations in workload 
the remainder of the paper is organized as follows 
section describes the architecture functionality and resource 
utilization model of our dre multimedia system case study 
section explains the structure and functionality of hyarm 
section evaluates the adaptive behavior of hyarm via 
experiments on our multimedia system case study section 
compares our research on hyarm with related work and 
section presents concluding remarks 
 
the code and examples for hyarm are available at www 
dre vanderbilt edu ∼nshankar hyarm 
article 
 case study dre multimedia 
system 
this section describes the architecture and qos 
requirements of our dre multimedia system 
 multimedia system architecture 
wireless link 
wireless link 
wireless 
link 
` 
` 
` 
physical link 
physical link 
physical link 
base station 
end receiver 
end receiver 
end receiver` 
physical link 
end receiver 
uav 
camera 
video 
encoder 
camera 
video 
encoder 
camera 
video 
encoder 
uav 
camera 
video 
encoder 
camera 
video 
encoder 
camera 
video 
encoder 
uav 
camera 
video 
encoder 
camera 
video 
encoder 
camera 
video 
encoder 
figure dre multimedia system architecture 
the architecture for our dre multimedia system is shown 
in figure and consists of the following entities data 
source video capture by uav where video is captured 
 related to subject of interest by camera s on each uav 
followed by encoding of raw video using a specific encoding 
scheme and transmitting the video to the next stage in the 
pipeline data distributor base station where the 
video is processed to remove noise followed by 
retransmission of the processed video to the next stage in the pipeline 
 sinks command and control center where the 
received video is again processed to remove noise then 
decoded and finally rendered to end user via graphical displays 
significant improvements in video encoding decoding and 
 de compression techniques have been made as a result of 
recent advances in video encoding and compression 
techniques common video compression schemes are 
mpeg mpeg- real video and mpeg- each compression 
scheme is characterized by its resource requirement e g the 
computational power to de compress the video signal and 
the network bandwidth required to transmit the compressed 
video signal properties of the compressed video such as 
resolution and frame-rate determine both the quality and the 
resource requirements of the video 
our multimedia system case study has the following 
endto-end real-time qos requirements latency 
interframe delay also know as jitter frame rate and 
picture resolution these qos requirements can be 
classified as being either hard or soft hard qos requirements 
should be met by the underlying system at all times whereas 
soft qos requirements can be missed occasionally 
for our 
case study we treat qos requirements such as latency and 
jitter as harder qos requirements and strive to meet these 
requirements at all times in contrast we treat qos 
requirements such as video frame rate and picture resolution as 
softer qos requirements and modify these video properties 
adaptively to handle dynamic changes in resource 
availabil 
although hard and soft are often portrayed as two discrete 
requirement sets in practice they are usually two ends of 
a continuum ranging from softer to harder rather than 
two disjoint points 
ity effectively 
 dre multimedia system rresources 
there are two primary types of resources in our dre 
multimedia system processors that provide 
computational power available at the uavs base stations and end 
receivers and network links that provide communication 
bandwidth between uavs base stations and end receivers 
the computing power required by the video capture and 
encoding tasks depends on dynamic factors such as speed 
of the uav speed of the subject if the subject is mobile 
and distance between uav and the subject the wireless 
network bandwidth available to transmit video captured by 
uavs to base stations also depends on the wireless 
connectivity between the uavs and the base station which in-turn 
depend on dynamic factors such as the speed of the uavs 
and the relative distance between uavs and base stations 
the bandwidth of the link between the base station and 
the end receiver is limited but more stable than the 
bandwidth of the wireless network resource requirements and 
availability of resources are subjected to dynamic changes 
two classes of applications - qos-enabled and best-effort 
- use the multimedia system infrastructure described above 
to transmit video to their respective receivers qos-enabled 
class of applications have higher priority over best-effort 
class of application in our study emergency response 
applications belong to qos-enabled and surveillance applications 
belong to best-effort class for example since a stream from 
an emergency response application is of higher importance 
than a video stream from a surveillance application it 
receives more resources end-to-end 
since resource availability significantly affects qos we use 
current resource utilization as the primary indicator of 
system performance we refer to the current level of system 
resource utilization as the system condition based on this 
definition we can classify system conditions as being either 
under over or effectively utilized 
under-utilization of system resources occurs when the 
current resource utilization is lower than the desired lower bound 
on resource utilization in this system condition residual 
system resources i e network bandwidth and 
computational power are available in large amounts after meeting 
end-to-end qos requirements of applications these 
residual resources can be used to increase the qos of the 
applications for example residual cpu and network bandwidth 
can be used to deliver better quality video e g with greater 
resolution and higher frame rate to end receivers 
over-utilization of system resources occurs when the 
current resource utilization is higher than the desired upper 
bound on resource utilization this condition can arise 
from loss of resources - network bandwidth and or 
computing power at base station end receiver or at uav - or 
may be due to an increase in resource demands by 
applications over-utilization is generally undesirable since the 
quality of the received video such as resolution and frame 
rate and timeliness properties such as latency and jitter 
are degraded and may result in an unstable and thus 
ineffective system 
effective resource utilization is the desired system 
condition since it ensures that end-to-end qos requirements of 
the uav-based multimedia system are met and utilization of 
both system resources i e network bandwidth and 
computational power are within their desired utilization bounds 
article 
section describes techniques we applied to achieve effective 
utilization even in the face of fluctuating resource 
availability and or demand 
 overview of hyarm 
this section describes the architecture of the hybrid 
adaptive resource-management middleware hyarm hyarm 
ensures efficient and predictable system performance by 
providing adaptive resource management including monitoring 
of system resources and enforcing bounds on application 
resource utilization 
 hyarm structure and functionality 
resource utilization 
legend 
resource allocation 
application parameters 
figure hyarm architecture 
hyarm is composed of three types of entities shown in 
figure and described below 
resource monitors observe the overall resource 
utilization for each type of resource and resource utilization per 
application in our multimedia system there are resource 
monitors for cpu utilization and network bandwidth cpu 
monitors observe the cpu resource utilization of uavs base 
station and end receivers network bandwidth monitors 
observe the network resource utilization of wireless network 
link between uavs and the base station and wired 
network link between the base station and end receivers 
the central controller maintains the system resource 
utilization below a desired bound by processing periodic 
updates it receives from resource monitors and 
modifying the execution of applications accordingly e g by 
using different execution algorithms or operating the 
application with increased decreased qos this adaptation 
process ensures that system resources are utilized efficiently and 
end-to-end application qos requirements are met in our 
multimedia system the hyarm controller determines the 
value of application parameters such as video 
compression schemes such as real video and mpeg- and or 
frame rate and picture resolution from the perspective 
of hybrid control theoretic techniques the different video 
compression schemes and frame rate form the discrete 
variables of application execution and picture resolution forms 
the continuous variables 
application adapters modify application execution 
according to parameters recommended by the controller and 
ensures that the operation of the application is in accordance 
with the recommended parameters in the current 
mplementation of hyarm the application adapter modifies the 
input parameters to the application that affect application 
qos and resource utilization - compression scheme frame 
rate and picture resolution in our future implementations 
we plan to use resource reservation mechanisms such as 
differentiated service and class-based kernel resource 
management to provision reserve network and cpu 
resources in our multimedia system the application adapter 
ensures that the video is encoded at the recommended frame 
rate and resolution using the specified compression scheme 
 applying hyarm to the multimedia 
system case study 
hyarm is built atop tao a widely used open-source 
implementation of real-time corba hyarm can be 
applied to ensure efficient predictable and adaptive resource 
management of any dre system where resource availability 
and requirements are subject to dynamic change 
figure shows the interaction of various parts of the 
dre multimedia system developed with hyarm tao 
and tao s a v streaming service tao s a v streaming 
service is an implementation of the corba a v 
streaming service specification tao s a v streaming service is 
a qos-enabled video distribution service that can transfer 
video in real-time to one or more receivers we use the a v 
streaming service to transmit the video from the uavs to 
the end receivers via the base station three entities of 
receiver 
uav 
tao 
resource 
utilization 
hyarm 
central 
controller 
a v streaming 
service sender 
mpeg 
mpeg 
real 
video 
hyarm 
resource 
monitor 
a v streaming 
service receiver 
compressed 
video compressed 
video 
application 
hyarm 
application 
adapter 
remote object call 
control 
inputs resource 
utilization 
resource 
utilization 
control inputs 
control 
inputs 
legend 
figure developing the dre multimedia system 
with hyarm 
hyarm namely the resource monitors central controller 
and application adapters are built as corba servants so 
they can be distributed throughout a dre system 
resource monitors are remote corba objects that update 
the central controller periodically with the current resource 
utilization application adapters are collocated with 
applications since the two interact closely 
as shown in figure uavs compress the data using 
various compression schemes such as mpeg mpeg and 
real video and uses tao s a v streaming service to 
transmit the video to end receivers hyarm s resource monitors 
continuously observe the system resource utilization and 
notify the central controller with the current utilization 
the interaction between the controller and the resource 
monitors uses the observer pattern when the controller 
receives resource utilization updates from monitors it 
computes the necessary modifications to application s 
parameters and notifies application adapter s via a remote 
operation call application adapter s that are collocated with 
the application modify the input parameters to the 
application - in our case video encoder - to modify the application 
resource utilization and qos 
 
the base station is not included in the figure since it only 
retransmits the video received from uavs to end receivers 
article 
 performance results and 
analysis 
this section first describes the testbed that provides the 
infrastructure for our dre multimedia system which was 
used to evaluate the performance of hyarm we then 
describe our experiments and analyze the results obtained to 
empirically evaluate how hyarm behaves during 
underand over-utilization of system resources 
 overview of the hardware and software 
testbed 
our experiments were performed on the emulab testbed 
at university of utah the hardware configuration consists 
of two nodes acting as uavs one acting as base station 
and one as end receiver video from the two uavs were 
transmitted to a base station via a lan configured with 
the following properties average packet loss ratio of and 
bandwidth mbps the network bandwidth was chosen to 
be mbps since each uav in the dre multimedia system 
is allocated kbps these parameters were chosen to 
emulate an unreliable wireless network with limited bandwidth 
between the uavs and the base station from the base 
station the video was retransmitted to the end receiver via a 
reliable wireline link of mbps bandwidth with no packet 
loss 
the hardware configuration of all the nodes was chosen as 
follows mhz intel pentium iii processor mb 
physical memory intel etherexpress pro mbps ethernet 
ports and gb hard drive a real-time version of linux 
- timesys linux net based on redhat linux 
 was used as the operating system for all nodes the 
following software packages were also used for our experiments 
ffmpeg -pre which is an open-source library http 
 www ffmpeg sourceforge net download php that 
compresses video into mpeg- mpeg- real video and many 
other video formats iftop which is an 
opensource library http www ex-parrot com ∼pdw iftop 
we used for monitoring network activity and bandwidth 
utilization ace tao which is an 
opensource http www dre vanderbilt edu tao 
implementation of the real-time corba specification upon which 
hyarm is built tao provides the corba audio video 
 a v streaming service that we use to transmit the video 
from the uavs to end receivers via the base station 
 experiment configuration 
our experiment consisted of two emulated uavs that 
simultaneously send video to the base station using the 
experimentation setup described in section at the base 
station video was retransmitted to the end receivers without 
any modifications where it was stored to a file each uav 
hosted two applications one qos-enabled application 
 emergency response and one best-effort application 
 surveillance within each uav computational power is shared 
between the applications while the network bandwidth is 
shared among all applications 
to evaluate the qos provided by hyarm we monitored 
cpu utilization at the two uavs and network bandwidth 
utilization between the uav and the base station cpu 
resource utilization was not monitored at the base station and 
the end receiver since they performed no 
computationallyintensive operations the resource utilization of the mpbs 
physical link between the base station and the end receiver 
does not affect qos of applications and is not monitored by 
hyarm since it is nearly times the mb bandwidth 
of the lan between the uavs and the base station the 
experiment also monitors properties of the video that affect 
the qos of the applications such as latency jitter frame 
rate and resolution 
the set point on resource utilization for each resource was 
specified at which is the upper bound typically 
recommended by scheduling techniques such as rate monotonic 
algorithm since studies have shown that human eyes 
can perceive delays more than ms we use this as the 
upper bound on jitter of the received video qos 
requirements for each class of application is specified during system 
initialization and is shown in table 
 empirical results and analysis 
this section presents the results obtained from running 
the experiment described in section on our dre 
multimedia system testbed we used system resource utilization 
as a metric to evaluate the adaptive resource management 
capabilities of hyarm under varying input work loads we 
also used application qos as a metric to evaluate hyarm s 
capabilities to support end-to-end qos requirements of the 
various classes of applications in the dre multimedia 
system we analyze these results to explain the significant 
differences in system performance and application qos 
comparison of system performance is decomposed into 
comparison of resource utilization and application qos for 
system resource utilization we compare network 
bandwidth utilization of the local area network and cpu 
utilization at the two uav nodes for application qos we 
compare mean values of video parameters including 
picture resolution frame rate latency and jitter 
comparison of resource utilization over-utilization 
of system resources in dre systems can yield an unstable 
system in contrast under-utilization of system resources 
increases system cost figure and figure compare the 
system resource utilization with and without hyarm 
figure shows that hyarm maintains system utilization close 
to the desired utilization set point during fluctuation in 
input work load by transmitting video of higher or lower qos 
for qos-enabled or best-effort class of applications during 
over or under utilization of system resources 
figure shows that without hyarm network 
utilization was as high as during increase in workload 
conditions which is greater than the utilization set point of 
by as a result of over-utilization of resources qos of 
the received video such as average latency and jitter was 
affected significantly without hyarm system resources 
were either under-utilized or over-utilized both of which 
are undesirable in contrast with hyarm system resource 
utilization is always close to the desired set point even 
during fluctuations in application workload during 
sudden fluctuation in application workload system conditions 
may be temporarily undesirable but are restored to the 
desired condition within several sampling periods temporary 
over-utilization of resources is permissible in our multimedia 
system since the quality of the video may be degraded for 
a short period of time though application qos will be 
degraded significantly if poor quality video is transmitted for 
a longer period of time 
comparison of application qos figures figure 
and table compare latency jitter resolution and 
framearticle 
class resolution frame rate latency msec jitter msec 
qos enabled x 
best-effort x 
table application qos requirements 
figure resource utilization with hyarm figure resource utilization without hyarm 
rate of the received video respectively table shows that 
hyarm increases the resolution and frame video of 
qosenabled applications but decreases the resolution and frame 
rate of best effort applications during over utilization of 
system resources resolution and frame rate of lower priority 
applications are reduced to adapt to fluctuations in 
application workload and to maintain the utilization of resources 
at the specified set point 
it can be seen from figure and figure that hyarm 
reduces the latency and jitter of the received video 
significantly these figures show that the qos of qos-enabled 
applications is greatly improved by hyarm although 
application parameters such as frame rate and resolutions 
which affect the soft qos requirements of best-effort 
applications may be compromised the hard qos requirements 
such as latency and jitter of all applications are met 
hyarm responds to fluctuation in resource availability 
and or demand by constant monitoring of resource 
utilization as shown in figure when resources utilization 
increases above the desired set point hyarm lowers the 
utilization by reducing the qos of best-effort applications this 
adaptation ensures that enough resources are available for 
qos-enabled applications to meet their qos needs 
figures and show that the values of latency and jitter of 
the received video of the system with hyarm are nearly half 
of the corresponding value of the system without hyarm 
with hyarm values of these parameters are well below 
the specified bounds whereas without hyarm these value 
are significantly above the specified bounds due to 
overutilization of the network bandwidth which leads to network 
congestion and results in packet loss hyarm avoids this 
by reducing video parameters such as resolution frame-rate 
and or modifying the compression scheme used to compress 
the video 
our conclusions from analyzing the results described above 
are that applying adaptive middleware via hybrid control to 
dre system helps to improve application qos 
increase system resource utilization and provide better 
predictability lower latency and inter-frame delay to 
qosenabled applications these improvements are achieved largely 
due to monitoring of system resource utilization efficient 
system workload management and adaptive resource 
provisioning by means of hyarm s network cpu resource 
monitors application adapter and central controller 
respectively 
 related work 
a number of control theoretic approaches have been 
applied to dre systems recently these techniques aid in 
overcoming limitations with traditional scheduling approaches 
that handle dynamic changes in resource availability poorly 
and result in a rigidly scheduled system that adapts poorly 
to change a survey of these techniques is presented in 
one such approach is feedback control scheduling fcs 
 fcs algorithms dynamically adjust resource allocation 
by means of software feedback control loops fcs 
algorithms are modeled and designed using rigorous 
controltheoretic methodologies these algorithms provide robust 
and analytical performance assurances despite uncertainties 
in resource availability and or demand although existing 
fcs algorithms have shown promise these algorithms often 
assume that the system has continuous control variable s 
that can continuously be adjusted while this assumption 
holds for certain classes of systems there are many classes 
of dre systems such as avionics and total-ship computing 
environments that only support a finite a priori set of 
discrete configurations the control variables in such systems 
are therefore intrinsically discrete 
hyarm handles both continuous control variables such 
as picture resolution and discrete control variable such as 
discrete set of frame rates hyarm can therefore be applied 
to system that support continuous and or discrete set of 
control variables the dre multimedia system as described 
in section is an example dre system that offers both 
continuous picture resolution and discrete set frame-rate of 
control variables these variables are modified by hyarm 
to achieve efficient resource utilization and improved 
application qos 
 concluding remarks 
article 
figure comparison of video latency figure comparison of video jitter 
source picture size frame rate 
with hyarm without hyarm 
uav qos enabled application x x 
uav best-effort application x x 
uav qos enabled application x x 
uav best-effort application x x 
table comparison of video quality 
many distributed real-time and embedded dre systems 
demand end-to-end quality of service qos enforcement 
from their underlying platforms to operate correctly these 
systems increasingly run in open environments where 
resource availability is subject to dynamic change to meet 
end-to-end qos in dynamic environments dre systems can 
benefit from an adaptive middleware that monitors system 
resources performs efficient application workload 
management and enables efficient resource provisioning for 
executing applications 
this paper described hyarm an adaptive middleware 
that provides effective resource management to dre 
systems hyarm employs hybrid control techniques to 
provide the adaptive middleware capabilities such as resource 
monitoring and application adaptation that are key to 
providing the dynamic resource management capabilities for 
open dre systems we employed hyarm to a 
representative dre multimedia system that is implemented using 
real-time corba and corba a v streaming service 
we evaluated the performance of hyarm in a system 
composed of three distributed resources and two classes of 
applications with two applications each our empirical 
results indicate that hyarm ensures efficient resource 
utilization by maintaining the resource utilization of system 
resources within the specified utilization bounds qos 
requirements of qos-enabled applications are met at all times 
overall hyarm ensures efficient predictable and adaptive 
resource management for dre systems 
 references 
 t f abdelzaher j stankovic c lu r zhang and y lu 
feddback performance control in software services ieee 
control systems june 
 l abeni l palopoli g lipari and j walpole analysis of a 
reservation-based feedback scheduler in ieee real-time 
systems symposium dec 
 s blake d black m carlson e davies z wang and 
w weiss an architecture for differentiated services network 
information center rfc dec 
 h franke s nagar c seetharaman and v kashyap 
enabling autonomic workload management in linux in 
proceedings of the international conference on autonomic 
computing icac new york new york may ieee 
 e gamma r helm r johnson and j vlissides design 
patterns elements of reusable object-oriented software 
addison-wesley reading ma 
 g ghinea and j p thomas qos impact on user perception 
and understanding of multimedia video clips in 
multimedia proceedings of the sixth acm 
international conference on multimedia pages - bristol 
united kingdom acm press 
 internet engineering task force differentiated services 
working group diffserv charter 
www ietf org html charters diffserv-charter html 
 x koutsoukos r tekumalla b natarajan and c lu hybrid 
supervisory control of real-time systems in th ieee 
real-time and embedded technology and applications 
symposium san francisco california mar 
 j lehoczky l sha and y ding the rate monotonic 
scheduling algorithm exact characterization and average 
case behavior in proceedings of the th ieee real-time 
systems symposium rtss pages - ieee 
computer society press 
 j loyall j gossett c gill r schantz j zinky p pal 
r shapiro c rodrigues m atighetchi and d karr 
comparing and contrasting adaptive middleware support in 
wide-area and embedded distributed object applications in 
proceedings of the st international conference on 
distributed computing systems icdcs- pages - 
ieee apr 
 c lu j a stankovic g tao and s h son feedback 
control real-time scheduling framework modeling and 
algorithms real-time systems journal - july 
 
 object management group real-time corba specification 
omg document formal - - edition aug 
 d c schmidt d l levine and s mungee the design and 
performance of real-time object request brokers computer 
communications - apr 
 thomas sikora trends and perspectives in image and video 
coding in proceedings of the ieee jan 
 x wang h -m huang v subramonian c lu and c gill 
camrit control-based adaptive middleware for real-time 
image transmission in proc of the th ieee real-time and 
embedded tech and applications symp rtas toronto 
canada may 
article 
context awareness for group interaction support 
alois ferscha clemens holzmann stefan oppl 
institut für pervasive computing johannes kepler universität linz 
altenbergerstraße a- linz 
{ferscha holzmann oppl} soft uni-linz ac at 
abstract 
in this paper we present an implemented system for supporting 
group interaction in mobile distributed computing environments 
first an introduction to context computing and a motivation for 
using contextual information to facilitate group interaction is 
given we then present the architecture of our system which 
consists of two parts a subsystem for location sensing that 
acquires information about the location of users as well as spatial 
proximities between them and one for the actual context-aware 
application which provides services for group interaction 
categories and subject descriptors 
c computer-communication networks distributed 
systems - distributed applications 
h models and principles user machine systems - human 
factors 
h information interfaces and presentation group and 
organization interfaces - asynchronous interaction collaborative 
computing theory and models synchronous interaction 
general terms 
design experimentation 
 introduction 
today s computing environments are characterized by an 
increasing number of powerful wirelessly connected mobile 
devices users can move throughout an environment while 
carrying their computers with them and having remote access to 
information and services anytime and anywhere new situations 
appear where the user s context - for example his current 
location or nearby people - is more dynamic computation does 
not occur at a single location and in a single context any longer 
but comprises a multitude of situations and locations this 
development leads to a new class of applications which are 
aware of the context in which they run in and thus bringing virtual 
and real worlds together 
motivated by this and the fact that only a few studies have been 
done for supporting group communication in such computing 
environments we have developed a system which we refer 
to as group interaction support system giss it supports group 
interaction in mobile distributed computing environments in a 
way that group members need not to at the same place any longer 
in order to interact with each other or just to be aware of the 
others situation 
in the following subchapters we will give a short overview on 
context aware computing and motivate its benefits for supporting 
group interaction a software framework for developing 
contextsensitive applications is presented which serves as middleware 
for giss chapter presents the architecture of giss and chapter 
 and discuss the location sensing and group interaction 
concepts of giss in more detail chapter gives a final summary 
of our work 
 what is context computing 
according to merriam-webster s online dictionary 
 context is 
defined as the interrelated conditions in which something exists 
or occurs because this definition is very general many 
approaches have been made to define the notion of context with 
respect to computing environments 
most definitions of context are done by enumerating examples or 
by choosing synonyms for context the term context-aware has 
been introduced first in where context is referred to as 
location identities of nearby people and objects and changes to 
those objects in context is also defined by an enumeration of 
examples namely location identities of the people around the 
user the time of the day season temperature etc defines 
context as the user s location environment identity and time 
here we conform to a widely accepted and more formal 
definition which defines context as any information than can be 
used to characterize the situation of an entity an entity is a 
person place or object that is considered relevant to the 
interaction between a user and an application including the user 
and applications themselves 
 identifies four primary types of context information 
 sometimes referred to as context dimensions that are - with 
respect to characterizing the situation of an entity - more 
important than others these are location identity time and 
activity which can also be used to derive other sources of 
contextual information secondary context types for example if 
we know a person s identity we can easily derive related 
information about this person from several data sources e g day 
of birth or e-mail address 
according to this definition defines a system to be 
contextaware if it uses context to provide relevant information and or 
services to the user where relevancy depends on the user s task 
 also gives a classification of features for context-aware 
applications which comprises presentation of information and 
services to a user automatic execution of a service and tagging of 
context to information for later retrieval 
figure layers of a context-aware system 
context computing is based on two major issues namely 
identifying relevant context identity location time activity and 
using obtained context automatic execution presentation 
tagging in order to do this there are a few layers between see 
figure first the obtained low-level context information has to 
be transformed aggregated and interpreted context 
transformation and represented in an abstract context world 
model context representation either centralized or 
decentralized finally the stored context information is used to 
trigger certain context events context triggering 
 group interaction in context 
after these abstract and formal definitions about what context and 
context computing is we will now focus on the main goal of this 
work namely how the interaction of mobile group members can 
be supported by using context information 
in we have identified organizational systems to be crucial for 
supporting mobile groups see figure first there has to be an 
information and knowledge management system which is 
capable of supporting a team with its information processing- and 
knowledge gathering needs the next part is the awareness 
system which is dedicated to the perceptualisation of the effects 
of team activity it does this by communicating work context 
agenda and workspace information to the users the interaction 
systems provide support for the communication among team 
members either synchronous or asynchronous and for the shared 
access to artefacts such as documents mobility systems deploy 
mechanisms to enable any-place access to team memory as well 
as the capturing and delivery of awareness information from and 
to any places finally yet importantly the organisational 
innovation system integrates aspects of the team itself like roles 
leadership and shared facilities 
with respect to these five aspects of team support we focus on 
interaction and partly cover mobility- and awareness-support 
group interaction includes all means that enable group members 
to communicate freely with all the other members at this point 
the question how context information can be used for supporting 
group interaction comes up we believe that information about 
the current situation of a person provides a surplus value to 
existing group interaction systems context information facilitates 
group interaction by allowing each member to be aware of the 
availability status or the current location of each other group 
member which again makes it possible to form groups 
dynamically to place virtual post-its in the real world or to 
determine which people are around 
figure support for mobile groups 
most of today s context-aware applications use location and time 
only and location is referred to as a crucial type of context 
information we also see the importance of location 
information in mobile and ubiquitous environments wherefore a 
main focus of our work is on the utilization of location 
information and information about users in spatial proximity 
nevertheless we believe that location as the only used type of 
context information is not sufficient to support group interaction 
wherefore we also take advantage of the other three primary 
types namely identity time and activity this provides a 
comprehensive description of a user s current situation and thus 
enabling numerous means for supporting group interaction which 
are described in detail in chapter 
when we look at the types of context information stated above 
we can see that all of them are single user-centred taking into 
account only the context of the user itself we believe that for the 
support of group interaction the status of the group itself has also 
be taken into account therefore we have added a fifth 
contextdimension group-context which comprises more than the sum of 
the individual member s contexts group context includes any 
information about the situation of a whole group for example 
how many members a group currently has or if a certain group 
meets right now 
 context middleware 
the group interaction support system giss uses the 
softwareframework introduced in which serves as a middleware for 
developing context-sensitive applications this so-called context 
framework is based on a distributed communication architecture 
and it supports different kinds of transport protocols and message 
coding mechanisms 
 
a main feature of the framework is the abstraction of context 
information retrieval via various sensors and its delivery to a level 
where no difference appears for the application designer 
between these different kinds of context retrieval mechanisms the 
information retrieval is hidden from the application developer 
this is achieved by so-called entities which describe 
objectse g a human user - that are important for a certain context 
scenario 
entities express their functionality by the use of so-called 
attributes which can be loaded into the entity these attributes 
are complex pieces of software which are implemented as java 
classes typical attributes are encapsulations of sensors but they 
can also be used to implement context services for example to 
notify other entities about location changes of users 
each entity can contain a collection of such attributes where an 
entity itself is an attribute the initial set of attributes an entity 
contains can change dynamically at runtime if an entity loads or 
unloads attributes from the local storage or over the network in 
order to load and deploy new attributes an entity has to reference 
a class loader and a transport and lookup layer which manages 
the lookup mechanism for discovering other entities and the 
transport xml configuration files specify which initial set of 
entities should be loaded and which attributes these entities own 
the communication between entities and attributes is based on 
context events each attribute is able to trigger events which are 
addressed to other attributes and entities respectively 
independently on which physical computer they are running 
among other things and event contains the name of the event and 
a list of parameters delivering information about the event itself 
related with this event-based architecture is the use of eca 
 event-condition-action -rules for defining the behaviour of the 
context system therefore every entity has a rule-interpreter 
which catches triggered events checks conditions associated with 
them and causes certain actions these rules are referenced by the 
entity s xml configuration a rule itself is even able to trigger 
the insertion of new rules or the unloading of existing rules at 
runtime in order to change the behaviour of the context system 
dynamically 
to sum up the context framework provides a flexible distributed 
architecture for hiding low-level sensor data from high-level 
applications and it hides external communication details from the 
application developer furthermore it is able to adapt its 
behaviour dynamically by loading attributes entities or 
ecarules at runtime 
 architecture overview 
as giss uses the context framework described in chapter as 
middleware every user is represented by an entity as well as the 
central server which is responsible for context transformation 
context representation and context triggering cf figure 
a main part of our work is about the automated acquisition of 
position information and its sensor-independent provision at 
application level we do not only sense the current location of 
users but also determine spatial proximities between them 
developing the architecture we focused on keeping the client as 
simple as possible and reducing the communication between 
client and server to a minimum 
each client may have various location and or proximity sensors 
attached which are encapsulated by respective context 
framework-attributes sensor encapsulation these attributes 
are responsible for integrating native sensor-implementations into 
the context framework and sending sensor-dependent position 
information to the server we consider it very important to 
support different types of sensors even at the same time in order 
to improve location accuracy on the one hand while providing a 
pervasive location-sensing environment with seamless transition 
between different location sensing techniques on the other hand 
all location- and proximity-sensors supported are represented by 
server-side context-attributes which correspond to the client-side 
sensor encapsulation-attributes and abstract the sensor-dependent 
position information received from all users via the wireless 
network sensor abstraction this requires a context repository 
where the mapping of diverse physical positions to standardized 
locations is stored 
the standardized location- and proximity-information of each 
user is then passed to the so-called sensor fusion-attributes 
one for symbolic locations and a second one for spatial 
proximities their job is to merge location- and 
proximityinformation of clients respectively which is described in detail in 
chapter every time the symbolic location of a user or the 
spatial proximity between two users changes the sensor 
fusion-attributes notify the giss core-attribute which 
controls the application 
because of the abstraction of sensor-dependent position 
information the system can easily be extended by additional 
sensors just by implementing the typically two attributes for 
encapsulating sensors some sensors may not need a client-side 
part abstracting physical positions and observing the interface to 
giss core 
figure architecture of the group interaction support 
system giss 
the giss core-attribute is the central coordinator of the 
application as it shows to the user it not only serves as an 
interface to the location-sensing subsystem but also collects 
further context information in other dimensions time identity or 
activity 
 
every time a change in the context of one or more users is 
detected giss core evaluates the effect of these changes on 
the user on the groups he belongs to and on the other members of 
these groups whenever necessary events are thrown to the 
affected clients to trigger context-aware activities like changing 
the presentation of awareness information or the execution of 
services 
the client-side part of the application is kept as simple as 
possible furthermore modular design was not only an issue on 
the sensor side but also when designing the user interface 
architecture thus the complete user interface can be easily 
exchanged if all of the defined events are taken into account and 
understood by the new interface-attribute 
the currently implemented user interface is split up in two parts 
which are also represented by two attributes the central attribute 
on client-side is the so-called instant messenger encapsulation 
which on the one hand interacts with the server through events 
and on the other hand serves as a proxy for the external 
application the user interface is built on 
as external application we use an existing open source instant 
messenger - the icq 
-compliant simple instant messenger 
 sim 
 we have chosen and instant messenger as front-end 
because it provides a well-known interface for most users and 
facilitates a seamless integration of group interaction support thus 
increasing acceptance and ease of use as the basic functionality 
of the instant messenger - to serve as a client in an instant 
messenger network - remains fully functional our application is 
able to use the features already provided by the messenger for 
example the contexts activity and identity are derived from the 
messenger network as it is described later 
the instant messenger encapsulation is also responsible for 
supporting group communication through the interface of the 
messenger it provides means of synchronous and asynchronous 
communication as well as a context-aware reminder system and 
tools for managing groups and the own availability status 
the second part of the user interface is a visualisation of the 
user s locations which is implemented in the attribute viewer 
the current implementation provides a two-dimensional map of 
the campus but it can easily be replaced by other visualisations a 
three-dimensional vrml-model for example furthermore this 
visualisation is used to show the artefacts for asynchronous 
communication based on a floor plan-view of the geographical 
area the user currently resides in it gives a quick overview of 
which people are nearby their state and provides means to 
interact with them 
in the following chapters and we describe the location 
sensing-backend and the application front-end for supporting 
group interaction in more detail 
 location sensing 
in the following chapter we will introduce a location model 
which is used for representing locations afterwards we will 
describe the integration of location- and proximity-sensors in 
 
http www icq com 
 
http sim-icq sourceforge net 
more detail finally we will have a closer look on the fusion of 
location- and proximity-information acquired by various sensors 
 location model 
a location model i e a context representation for the 
contextinformation location is needed to represent the locations of users 
in order to be able to facilitate location-related queries like given 
a location return a list of all the objects there or given an 
object return its current location in general there are two 
approaches symbolic models which represent location as 
abstract symbols and a geometric model which represent 
location as coordinates 
we have chosen a symbolic location model which refers to 
locations as abstract symbols like room p or physics 
building because we do not require geometric location data 
instead abstract symbols are more convenient for human 
interaction at application level furthermore we use a symbolic 
location containment hierarchy similar to the one introduced in 
 which consists of top-level regions which contain buildings 
which contain floors and the floors again contain rooms we also 
distinguish four types namely region e g a whole campus 
section e g a building or an outdoor section level e g a certain 
floor in a building and area e g a certain room we introduce a 
fifth type of location which we refer to as semantic these 
socalled semantic locations can appear at any level in the hierarchy 
and they can be nested but they do not necessarily have a 
geographic representation examples for such semantic locations 
are tagged objects within a room e g a desk and a printer on this 
desk or the name of a department which contains certain rooms 
figure symbolic location containment hierarchy 
the hierarchy of symbolic locations as well as the type of each 
position is stored in the context repository 
 sensors 
our architecture supports two different kinds of sensors location 
sensors which acquire location information and proximity 
sensors which detect spatial proximities between users 
as described above each sensor has a server- and in most cases a 
corresponding client-side-implementation too while the 
clientattributes sensor abstraction are responsible for acquiring 
low-level sensor-data and transmitting it to the server the 
corresponding sensor encapsulation-attributes transform them 
into a uniform and sensor-independent format namely symbolic 
locations and ids of users in spatial proximity respectively 
 
afterwards the respective attribute sensor fusion is being 
triggered with this sensor-independent information of a certain 
user detected by a particular sensor such notifications are 
performed every time the sensor acquired new information 
accordingly sensor abstraction-attributes are responsible to 
detect when a certain sensor is no longer available on the client 
side e g if it has been unplugged by the user or when position 
respectively proximity could not be determined any longer e g 
rfid reader cannot detect tags and notify the corresponding 
sensor fusion about this 
 location sensors 
in order to sense physical positions the sensor 
encapsulationattributes asynchronously transmit sensor-dependent position 
information to the server the corresponding location sensor 
abstraction-attributes collect these physical positions delivered 
by the sensors of all users and perform a repository-lookup in 
order to get the associated symbolic location this requires certain 
tables for each sensor which map physical positions to symbolic 
locations one physical position may have multiple symbolic 
locations at different accuracy-levels in the location hierarchy 
assigned to for example if a sensor covers several rooms if such 
a mapping could be found an event is thrown in order to notify 
the attribute location sensor fusion about the symbolic 
locations a certain sensor of a particular user determined 
we have prototypically implemented three kinds of location 
sensors which are based on wlan ieee bluetooth and 
rfid radio frequency identification we have chosen these 
three completely different sensors because of their differences 
concerning accuracy coverage and administrative effort in order 
to evaluate the flexibility of our system see table 
the most accurate one is an rfid sensor which is based on an 
active rfid-reader as soon as the reader is plugged into the 
client it scans for active rfid tags in range and transmits their 
serial numbers to the server where they are mapped to symbolic 
locations we also take into account rssi radio signal strength 
information which provides position accuracy of few 
centimetres and thus enables us to determine which rfid-tag is 
nearest due to this high accuracy rfid is used for locating users 
within rooms the administration is quite simple once a new 
rfid tag is placed its serial number simply has to be assigned to 
a single symbolic location a drawback is the poor availability 
which can be traced back to the fact that rfid readers are still 
very expensive 
the second one is an wlan sensor therefore we 
integrated a purely software-based commercial wlan 
positioning system for tracking clients on the university 
campuswide wlan infrastructure the reached position accuracy is in 
the range of few meters and thus is suitable for location sensing at 
the granularity of rooms a big disadvantage is that a map of the 
whole area has to be calibrated with measuring points at a 
distance of meters each because most mobile computers are 
equipped with wlan technology and the positioning-system is a 
software-only solution nearly everyone is able to use this kind of 
sensor 
finally we have implemented a bluetooth sensor which detects 
bluetooth tags i e bluetooth-modules with known position in 
range and transmits them to the server that maps to symbolic 
locations because of the fact that we do not use signal 
strengthinformation in the current implementation the accuracy is above 
 meters and therefore a single bluetooth mac address is 
associated with several symbolic locations according to the 
physical locations such a bluetooth module covers this leads to 
the disadvantage that the range of each bluetooth-tag has to be 
determined and mapped to symbolic locations within this range 
table comparison of implemented sensors 
sensor accuracy coverage administration 
rfid cm poor easy 
wlan - m very well 
very 
timeconsuming 
bluetooth   m well time-consuming 
 proximity sensors 
any sensor that is able to detect whether two users are in spatial 
proximity is referred to as proximity sensor similar to the 
location sensors the proximity sensor abstraction-attributes 
collect physical proximity information of all users and transform 
them to mappings of user-ids 
we have implemented two types of proximity-sensors which are 
based on bluetooth on the one hand and on fused symbolic 
locations see chapter on the other hand 
the bluetooth-implementation goes along with the 
implementation of the bluetooth-based location sensor the 
already determined bluetooth mac addresses in range of a 
certain client are being compared with those of all other clients 
and each time the attribute bluetooth sensor abstraction 
detects congruence it notifies the proximity sensor fusion about 
this 
the second sensor is based on symbolic locations processed by 
location sensor fusion wherefore it does not need a client-side 
implementation each time the fused symbolic location of a 
certain user changes it checks whether he is at the same symbolic 
location like another user and again notifies the proximity sensor 
fusion about the proximity between these two users the range 
can be restricted to any level of the location containment 
hierarchy for example to room granularity 
a currently unresolved issue is the incomparable granularity of 
different proximity sensors for example the symbolic locations 
at same level in the location hierarchy mostly do not cover the 
same geographic area 
 sensor fusion 
core of the location sensing subsystem is the sensor fusion it 
merges data of various sensors while coping with differences 
concerning accuracy coverage and sample-rate according to the 
two kinds of sensors described in chapter we distinguish 
between fusion of location sensors on the one hand and fusion of 
proximity sensors on the other hand 
the fusion of symbolic locations as well as the fusion of spatial 
proximities operates on standardized information cf figure 
this has the advantage that additional position- and 
proximitysensors can be added easily or the fusion algorithms can be 
replaced by ones that are more sophisticated 
 
fusion is performed for each user separately and takes into 
account the measurements at a single point in time only i e no 
history information is used for determining the current location of 
a certain user the algorithm collects all events thrown by the 
sensor abstraction-attributes performs fusion and triggers the 
giss core-attribute if the symbolic location of a certain user or 
the spatial proximity between users changed 
an important feature is the persistent storage of location- and 
proximity-history in a database in order to allow future retrieval 
this enables applications to visualize the movement of users for 
example 
 location sensor fusion 
goal of the fusion of location information is to improve precision 
and accuracy by merging the set of symbolic locations supplied 
by various location sensors in order to reduce the number of 
these locations to a minimum ideally to a single symbolic 
location per user this is quite difficult because different sensors 
may differ in accuracy and sample rate as well 
the location sensor fusion-attribute is triggered by events 
which are thrown by the location sensor 
abstractionattributes these events contain information about the identity of 
the user concerned his current location and the sensor by which 
the location has been determined 
if the attribute location sensor fusion receives such an event 
it checks if the amount of symbolic locations of the user 
concerned has changed compared with the last event if this is 
the case it notifies the giss core-attribute about all symbolic 
locations this user is currently associated with 
however this information is not very useful on its own if a 
certain user is associated with several locations as described in 
chapter a single location sensor may deliver multiple 
symbolic locations moreover a certain user may have several 
location sensors which supply symbolic locations differing in 
accuracy i e different levels in the location containment 
hierarchy to cope with this challenge we implemented a fusion 
algorithm in order to reduce the number of symbolic locations to a 
minimum ideally to a single location 
in a first step each symbolic location is associated with its 
number of occurrences a symbolic location may occur several 
times if it is referred to by more than one sensor or if a single 
sensor detects multiple tags which again refer to several 
locations furthermore this number is added to the previously 
calculated number of occurrences of each symbolic location 
which is a child-location of the considered one in the location 
containment hierarchy for example if - in figure - room 
occurs two times and desk occurs a single time the value of 
room is added to the value of desk whereby desk finally 
gets the value in a final step only those symbolic locations are 
left which are assigned with the highest number of occurrences 
a further reduction can be achieved by assigning priorities to 
sensors based on accuracy and confidence and cumulating these 
priorities for each symbolic location instead of just counting the 
number of occurrences 
if the remaining fused locations have changed i e if they differ 
from the fused locations the considered user is currently 
associated with they are provided with the current timestamp 
written to the database and the giss-attribute is notified about 
where the user is probably located 
finally the most accurate common location in the location 
hierarchy is calculated i e the least upper bound of these 
symbolic locations in order to get a single symbolic location if it 
changes the giss core-attribute is triggered again 
 proximity sensor fusion 
proximity sensor fusion is much simpler than the fusion of 
symbolic locations the corresponding proximity sensor 
fusionattribute is triggered by events which are thrown by the 
proximity sensor abstraction-attributes these special events 
contain information about the identity of the two users concerned 
if they are currently in spatial proximity or if proximity no longer 
persists and by which proximity-sensor this has been detected 
if the sensor fusion-attribute is notified by a certain proximity 
sensor abstraction-attribute about an existing spatial proximity 
it first checks if these two users are already known to be in 
proximity detected either by another user or by another 
proximity-sensor of the user which caused the event if not this 
change in proximity is written to the context repository with 
current timestamp similarly if the attribute proximity fusion 
is notified about an ended proximity it checks if the users are still 
known to be in proximity and writes this change to the repository 
if not 
finally if spatial proximity between the two users actually 
changed an event is thrown to notify the giss core-attribute 
about this 
 contextsensitive interaction 
 overview 
in most of today s systems supporting interaction in groups the 
provided means lack any awareness of the user s current context 
thus being unable to adapt to his needs 
in our approach we use context information to enhance 
interaction and provide further services which offer new 
possibilities to the user furthermore we believe that interaction 
in groups also has to take into account the current context of the 
group itself and not only the context of individual group 
members for this reason we also retrieve information about the 
group s current context derived from the contexts of the group 
members together with some sort of meta-information see 
chapter 
the sources of context used for our application correspond with 
the four primary context types given in chapter - identity i 
location l time t and activity a as stated before we also 
take into account the context of the group the user is interaction 
with so that we could add a fifth type of context 
informationgroup awareness g - to the classification using this context 
information we can trigger context-aware activities in all of the 
three categories described in chapter - presentation of 
information p automatic execution of services a and tagging 
of context to information for later retrieval t 
table gives an overview of activities we have already 
implemented they are described comprehensively in chapter 
the table also shows which types of context information are used 
for each activity and the category the activity could be classified 
in 
 
table classification of implemented context-aware 
activities 
service l t i a g p a t 
location visualisation x x x 
group building support x x x x 
support for synchronous 
communication 
x x x x 
support for asynchronous 
communication 
x x x x x x x 
availability management x x x 
task management support x x x x 
meeting support x x x x x x 
reasons for implementing these very features are to take 
advantage of all four types of context information in order to 
support group interaction by utilizing a comprehensive knowledge 
about the situation a single user or a whole group is in 
a critical issue for the user acceptance of such a system is the 
usability of its interface we have evaluated several ways of 
presenting context-aware means of interaction to the user until 
we came to the solution we use right now although we think that 
the user interface that has been implemented now offers the best 
trade-off between seamless integration of features and ease of use 
it would be no problem to extend the architecture with other user 
interfaces even on different platforms 
the chosen solution is based on an existing instant messenger 
which offers several possibilities to integrate our system see 
chapter the biggest advantage of this approach is that the 
user is confronted with a graphical user interface he is already 
used to in most cases furthermore our system uses an instant 
messenger account as an identifier so that the user does not have 
to register a further account anywhere else for example the user 
can use his already existing icq 
-account 
 instant messenger integration 
our system is based upon an existing instant messenger the 
socalled simple instant messenger sim 
 the implementation of 
this messenger is carried out as a project at sourceforge 
 
sim supports multiple messenger protocols such as aim 
 icq 
and msn 
 it also supports connections to multiple accounts at 
the same time furthermore full support for sms-notification 
 where provided from the used protocol is given 
sim is based on a plug-in concept all protocols as well as parts 
of the user-interface are implemented as plug-ins its architecture 
is also used to extend the application s abilities to communicate 
with external applications for this purpose a remote control 
plug-in is provided by which sim can be controlled from 
external applications via socket connection this remote control 
interface is extensively used by giss for retrieving the contact 
list setting the user s availability-state or sending messages the 
 
http sourceforge net 
 
http www aim com 
 
http messenger msn com 
functionality of the plug-in was extended in several ways for 
example to accept messages for an account as if they would have 
been sent via the messenger network 
the messenger more exactly the contact list i e a list of profiles 
of all people registered with the instant messenger which is 
visualized by listing their names as it can be seen in figure is 
also used to display locations of other members of the groups a 
user belongs to this provides location awareness without taking 
too much space or requesting the user s full attention a more 
comprehensive description of these features is given in chapter 
 
 sources of context information 
while the location-context of a user is obtained from our location 
sensing subsystem described in chapter we consider further 
types of context than location relevant for the support of group 
interaction too 
local time as a very important context dimension can be easily 
retrieved from the real time clock of the user s system besides 
location and time we also use context information of user s 
activity and identity where we exploit the functionality provided 
by the underlying instant messenger system identity or more 
exactly the mapping of ids to names as well as additional 
information from the user s profile can be distilled out of the 
contents of the user s contact list 
information about the activity or a certain user is only available in 
a very restricted area namely the activity at the computer itself 
other activities like making a phone call or something similar 
cannot be recognized with the current implementation of the 
activity sensor the only context-information used is the instant 
messenger s availability state thus only providing a very coarse 
classification of the user s activity online offline away busy 
etc although this may not seem to be very much information it 
is surely relevant and can be used to improve or even enable 
several services 
having collected the context information from all available users 
it is now possible to distil some information about the context of a 
certain group information about the context of a group includes 
how many members the group currently has if the group meets 
right now which members are participating at a meeting how 
many members have read which of the available posts from other 
team members and so on 
therefore some additional information like a list of members for 
each group is needed these lists can be assembled manually by 
users joining and leaving groups or retrieved automatically the 
context of a group is secondary context and is aggregated from 
the available contexts of the group members every time the 
context of a single group member changes the context of the 
whole group is changing and has to be recalculated 
with knowledge about a user s context and the context of the 
groups he belongs to we can provide several context-aware 
services to the user which enhance his interaction abilities a 
brief description of these services is given in chapter 
 
 group interaction support 
 visualisation of location information 
an important feature is the visualisation of location information 
thus allowing users to be aware of the location of other users and 
members of groups he joined respectively 
as already described in chapter we use two different forms of 
visualisation the maybe more important one is to display 
location information in the contact list of the instant messenger 
right beside the name thus being always visible while not 
drawing the user s attention on it compared with a 
twodimensional view for example which requires a own window for 
displaying a map of the environment 
due to the restricted space in the contact list it has been 
necessary to implement some sort of level-of-detail concept as 
we use a hierarchical location model we are able to determine the 
most accurate common location of two users in the contact list 
the current symbolic location one level below the previously 
calculated common location is then displayed if for example 
user a currently resides in room p at the first floor of a 
building and user b which has to be displayed in the contact list 
of user a is in room p at the third floor the most accurate 
common location of these two users is the building they are in 
for that reason the floor i e one level beyond the common 
location namely the building of user b is displayed in the 
contact list of user a if both people reside on the same floor or 
even in the same room the room would be taken 
figure shows a screenshot of the simple instant messenger 
 
where the current location of those people whose location is 
known by giss is displayed in brackets right beside their name 
on top of the image the heightened integrated giss-toolbar is 
shown which currently contains the following implemented 
functionality from left to right asynchronous communication 
for groups see chapter context-aware reminders see 
chapter two-dimensional visualisation of 
locationinformation forming and managing groups see chapter 
context-aware availability-management see chapter and 
finally a button for terminating giss 
figure giss integration in simple instant messenger 
as displaying just this short form of location may not be enough 
for the user because he may want to see the most accurate 
position available a fully qualified position is shown if a name 
in the contact-list is clicked e g in the form of 
desk room  department   stfloor building  campus 
the second possible form of visualisation is a graphical one we 
have evaluated a three-dimensional view which was based on a 
vrml model of the respective area cf figure due to lacks in 
navigational and usability issues we decided to use a 
twodimensional view of the floor it is referred to as level in the 
location hierarchy cf figure other levels of granularity like 
section e g building and region e g campus are also provided 
in this floor-plan-based view the current locations are shown in 
the manner of icq 
contacts which are placed at the currently 
sensed location of the respective person the availability-status of 
a user for example away if he is not on the computer right now 
or busy if he does not want to be disturbed is visualized by 
colour-coding the icq 
-flower left beside the name furthermore 
the floor-plan-view shows so-called the virtual post-its which are 
virtual counterparts of real-life post-its and serve as our means of 
asynchronous communication more about virtual post-its can be 
found in chapter 
figure d-view of the floor vrml 
figure shows the two-dimensional map of a certain floor where 
several users are currently located visualized by their name and 
the flower left beside the location of the client on which the 
map is displayed is visualized by a green circle down to the 
right two virtual post-its can be seen 
figure d view of the floor 
another feature of the d-view is the visualisation of 
locationhistory of users as we store the complete history of a user s 
locations together with a timestamp we are able to provide 
information about the locations he has been back in time when 
the mouse is moved over the name of a certain user in the 
 dview footprints of a user placed at the locations he has been 
are faded out the stronger the older the location information is 
 
 forming and managing groups 
to support interaction in groups it is first necessary to form 
groups as groups can have different purposes we distinguish two 
types of groups 
so-called static groups are groups which are built up manually 
by people joining and leaving them static groups can be further 
divided into two subtypes in open static groups everybody can 
join and leave anytime useful for example to form a group of 
lecture attendees of some sort of interest group closed static 
groups have an owner who decides which persons are allowed to 
join although everybody could leave again at any time closed 
groups enable users for example to create a group of their friends 
thus being able to communicate with them easily 
in contrast to that we also support the creation of dynamic 
groups they are formed among persons who are at the same 
location at the same time the creation of dynamic groups is only 
performed at locations where it makes sense to form groups for 
example in lecture halls or meeting rooms but not on corridors or 
outdoor it would also be not very meaningful to form a group 
only of the people residing in the left front sector of a hall 
instead the complete hall should be considered for these 
reasons all the defined locations in the hierarchy are tagged 
whether they allow the formation of groups or not dynamic 
groups are also not only formed granularity of rooms but also on 
higher levels in the hierarchy for example with the people 
currently residing in the area of a department 
as the members of dynamic groups constantly change it is 
possible to create an open static group out of them 
 synchronous communication for groups 
the most important form of synchronous communication on 
computers today is instant messaging some people even see 
instant messaging to be the real killer application on the internet 
this has also motivated the decision to build giss upon an 
instant messaging system 
in today s messenger systems peer-to-peer-communication is 
extensively supported however when it comes to 
communication in groups the support is rather poor most of the 
time often only sending a message to multiple recipients is 
supported lacking means to take into account the current state of 
the recipients furthermore groups can only be formed of 
members in one s contact list thus being not able to send 
messages to a group where not all of its members are known 
 which may be the case in settings where the participants of a 
lecture form a group 
our approach does not have the mentioned restrictions we 
introduce group-entries in the user s contact list enable him or 
his to send messages to this group easily without knowing who 
exactly is currently a member of this group furthermore group 
messages are only delivered to persons who are currently not 
busy thus preventing a disturbance by a message which is 
possibly unimportant for the user 
these features cannot be carried out in the messenger network 
itself so whenever a message to a group account is sent we 
intercept it and route it through our system to all the recipients 
which are available at a certain time communication via a group 
account is also stored centrally enabling people to query missed 
messages or simply viewing the message history 
 asynchronous communication for groups 
asynchronous communication in groups is not a new idea the 
goal of this approach is not to reinvent the wheel as email is 
maybe the most widely used form of asynchronous 
communication on computers and is broadly accepted and 
standardized in out work we aim at the combination of 
asynchronous communication with location awareness 
for this reason we introduce the concept of so-called virtual 
postits cp which are messages that are bound to physical 
locations these virtual post-its could be either visible for all 
users that are passing by or they can be restricted to be visible for 
certain groups of people only moreover a virtual post-it can also 
have an expiry date after which it is dropped and not displayed 
anymore virtual post-its can also be commented by others thus 
providing some from of forum-like interaction where each post-it 
forms a thread 
virtual post-its are displayed automatically whenever a user 
 available passes by the first time afterwards post-its can be 
accessed via the d-viewer where all visible post-its are shown 
all readers of a post-it are logged and displayed when viewing it 
providing some sort of awareness about the group members 
activities in the past 
 context-aware availability management 
instant messengers in general provide some kind of availability 
information about a user although this information can be only 
defined in a very coarse granularity we have decided to use these 
means of gathering activity context because the introduction of 
an additional one would strongly decrease the usability of the 
system 
to support the user managing his availability we provide an 
interface that lets the user define rules to adapt his availability to 
the current context these rules follow the form on event e if 
condition c then action a which is directly supported by the 
eca-rules of the context framework described in chapter 
the testing of conditions is periodically triggered by throwing 
events whenever the context of a user changes the condition 
itself is defined by the user who can demand the change of his 
availability status as the action in the rule as a condition the user 
can define his location a certain time also triggering daily every 
week or every month or any logical combination of these criteria 
 context-aware reminders 
reminders are used to give the user the opportunity of 
defining tasks and being reminded of those when certain criteria 
are fulfilled thus a reminder can be seen as a post-it to oneself 
which is only visible in certain cases reminders can be bound to 
a certain place or time but also to spatial proximity of users or 
groups these criteria can be combined with boolean operators 
thus providing a powerful means to remind the user of tasks that 
he wants to carry out when a certain context occurs 
a reminder will only pop up the first time the actual context 
meets the defined criterion on showing up the reminder the user 
has the chance to resubmit it to be reminded again for example 
five minutes later or the next time a certain user is in spatial 
proximity 
 
 context-aware recognition and notification of 
group meetings 
with the available context information we try to recognize 
meetings of a group the determination of the criteria when the 
system recognizes a group having a meeting is part of the 
ongoing work in a first approach we use the location- and 
activity-context of the group members to determine a meeting 
whenever more than of the members of a group are 
available at a location where a meeting is considered to make 
sense e g not on a corridor a meeting minutes post-it is created 
at this location and all absent group members are notified of the 
meeting and the location it takes place 
during the meeting the comment-feature of virtual post-its 
provides a means to take notes for all of the participants when 
members are joining or leaving the meeting this is automatically 
added as a note to the list of comments 
like the recognition of the beginning of a meeting the 
recognition of its end is still part of ongoing work if the end of 
the meeting is recognized all group members get the complete list 
of comments as a meeting protocol at the end of the meeting 
 conclusions 
this paper discussed the potentials of support for group 
interaction by using context information first we introduced the 
notions of context and context computing and motivated their 
value for supporting group interaction 
an architecture is presented to support context-aware group 
interaction in mobile distributed environments it is built upon a 
flexible and extensible framework thus enabling an easy adoption 
to available context sources e g by adding additional sensors as 
well as the required form of representation 
we have prototypically developed a set of services which 
enhance group interaction by taking into account the current 
context of the users as well as the context of groups itself 
important features are dynamic formation of groups visualization 
of location on a two-dimensional map as well as unobtrusively 
integrated in an instant-messenger asynchronous communication 
by virtual post-its which are bound to certain locations and a 
context-aware availability-management which adapts the 
availability-status of a user to his current situation 
to provide location information we have implemented a 
subsystem for automated acquisition of location- and 
proximityinformation provided by various sensors which provides a 
technology-independent presentation of locations and spatial 
proximities between users and merges this information using 
sensor-independent fusion algorithms a history of locations as 
well as of spatial proximities is stored in a database thus enabling 
context history-based services 
 references 
 beer w christian v ferscha a mehrmann l 
modeling context-aware behavior by interpreted eca 
rules in proceedings of the international conference on 
parallel and distributed computing europar 
 klagenfurt austria august - springer verlag 
lncs - 
 brown p j bovey j d chen x context-aware 
applications from the laboratory to the marketplace 
ieee personal communications - 
 chen h kotz d a survey of context-aware mobile 
computing research technical report tr - 
computer science department dartmouth college 
hanover new hampshire november 
 dey a providing architectural support for building 
context-aware applications ph d thesis department of 
computer science georgia institute of technology 
atlanta november 
 svetlana domnitcheva location modeling state of the art 
and challenges in proceedings of the workshop on 
location modeling for ubiquitous computing atlanta 
georgia united states september - 
 ferscha a workspace awareness in mobile virtual teams 
in proceedings of the ieee th 
international workshop on 
enabling technologies infrastructure for collaborative 
enterprises wetice gaithersburg maryland march 
 - ieee computer society press - 
 ferscha a coordination in pervasive computing 
environments in proceedings of the twelfth international 
ieee workshop on enabling technologies infrastructure 
for collaborative enterprises wetice- june - 
 ieee computer society press - 
 leonhard u supporting location awareness in open 
distributed systems ph d thesis department of 
computing imperial college london may 
 ryan n pascoe j morse d enhanced reality 
fieldwork the context-aware archaeological assistant 
gaffney v van leusen m exxon s eds computer 
applications in archaeology 
 schilit b n theimer m disseminating active map 
information to mobile hosts ieee network 
 - 
 schilit b n a system architecture for context-aware 
mobile computing ph d thesis columbia university 
department of computer science may 
 wang b bodily j gupta s k s supporting persistent 
social groups in ubiquitous computing environments 
using context-aware ephemeral group service in 
proceedings of the second ieee international conference 
on pervasive computing and communications 
 percom march - ieee computer society 
press - 
 pascoe j the stick-e note architecture extending the 
interface beyond the user proceedings of the nd 
international conference of intelligent user interfaces 
 iui orlando usa - 
 dey a abowd g cybreminder a context-aware system 
for supporting re-minders proceedings of the nd 
international symposium on handheld and ubiquitous 
computing huc bristol uk - 
 
a cross-layer approach to resource discovery 
and distribution in mobile ad-hoc networks 
chaiporn jaikaeo 
computer engineering 
kasetsart university thailand 
 - ext 
cpj cpe ku ac th 
xiang cao 
computer and information sciences 
university of delaware usa 
 - - 
cao cis udel edu 
chien-chung shen 
computer and information sciences 
university of delaware usa 
 - - 
cshen cis udel edu 
abstract 
this paper describes a cross-layer approach to designing robust 
p p system over mobile ad-hoc networks the design is based on 
simple functional primitives that allow routing at both p p and 
network layers to be integrated to reduce overhead with these 
primitives the paper addresses various load balancing techniques 
preliminary simulation results are also presented 
categories and subject descriptors 
c distributed systems distributed applications 
general terms 
algorithms and design 
 introduction 
mobile ad-hoc networks manets consist of mobile nodes that 
autonomously establish connectivity via multi-hop wireless 
communications without relying on any existing pre-configured 
network infrastructure or centralized control manets are useful 
in situations where impromptu communication facilities are 
required such as battlefield communications and disaster relief 
missions as manet applications demand collaborative 
processing and information sharing among mobile nodes resource 
 service discovery and distribution have become indispensable 
capabilities 
one approach to designing resource discovery and distribution 
schemes over manets is to construct a peer-to-peer p p 
system or an overlay which organizes peers of the system into a 
logical structure on top of the actual network topology however 
deploying such p p systems over manets may result in either a 
large number of flooding operations triggered by the reactive 
routing process or inefficiency in terms of bandwidth utilization in 
proactive routing schemes either way constructing an overlay 
will potentially create a scalability problem for large-scale 
manets 
due to the dynamic nature of manets p p systems should be 
robust by being scalable and adaptive to topology changes these 
systems should also provide efficient and effective ways for peers 
to interact as well as other desirable application specific features 
this paper describes a design paradigm that uses the following 
two functional primitives to design robust resource discovery and 
distribution schemes over manets 
 positive negative feedback query packets are used to 
explore a route to other peers holding resources of interest 
optionally advertisement packets are sent out to advertise 
routes from other peers about available resources when 
traversing a route these control packets measure goodness 
of the route and leave feedback information on each node 
along the way to guide subsequent control packets to 
appropriate directions 
 sporadic random walk as the network topology and or 
the availability of resources change existing routes may 
become stale while better routes become available sporadic 
random walk allows a control packet to explore different 
paths and opportunistically discover new and or better 
routes 
adopting this paradigm the whole manet p p system operates 
as a collection of autonomous entities which consist of different 
types of control packets such as query and advertisement packets 
these packets work collaboratively but indirectly to achieve 
common tasks such as resource discovery routing and load 
balancing with collaboration among these entities a manet p p 
system is able to  learn the network dynamics by itself and adjust 
its behavior accordingly without the overhead of organizing peers 
into an overlay 
the remainder of this paper is organized as follows related work 
is described in the next section section describes the resource 
discovery scheme section describes the resource distribution 
scheme the replica invalidation scheme is described in section 
followed by it performance evaluation in section section 
concludes the paper 
 related work 
for manets p p systems can be classified based on the design 
principle into layered and cross-layer approaches a layered 
approach adopts a p p-like solution where resource discovery is 
facilitated as an application layer protocol and query reply 
messages are delivered by the underlying manet routing protocols 
for instance konark makes use of a underlying multicast 
protocol such that service providers and queriers advertise and 
search services via a predefined multicast group respectively 
proem is a high-level mobile computing platform for p p 
systems over manets it defines a transport protocol that sits on 
top of the existing tcp ip stack hence relying on an existing 
routing protocol to operate with limited control over how control 
and data packets are routed in the network it is difficult to avoid 
the inefficiency of the general-purpose routing protocols which 
are often reactive and flooding-based 
in contrast cross-layer approaches either relies on its own routing 
mechanism or augments existing manet routing algorithms to 
support resource discovery ds which is the pioneering 
work deploying p p system on mobile devices exploits data 
locality and node mobility to dissemination data in a single-hop 
fashion hence long search latency may be resulted as a ds 
node can get data of interest only if the node that holds the data is 
in its radio coverage mohan et al propose an adaptive service 
discovery algorithm that combines both push and pull models 
specifically a service provider querier broadcasts 
advertisement query only when the number of nodes advertising or 
querying which is estimated by received control packets is below a 
threshold during a period of time in this way the number of 
control packets on the network is constrained thus providing good 
scalability despite the mechanism to reduce control packets high 
overhead may still be unavoidable especially when there are 
many clients trying to locate different services due to the fact that 
the algorithm relies on flooding 
for resource replication yin and cao design and evaluate 
cooperative caching techniques for manets caching however 
is performed reactively by intermediate nodes when a querier 
requests data from a server data items or resources are never 
pushed into other nodes proactively thanedar et al propose a 
lightweight content replication scheme using an expanding ring 
technique if a server detects the number of requests exceed a 
threshold within a time period it begins to replicate its data onto 
nodes capable of storing replicas whose hop counts from the 
server are of certain values since data replication is triggered by 
the request frequency alone it is possible that there are replicas 
unnecessarily created in a large scope even though only nodes 
within a small range request this data our proposed resource 
replication mechanism in contrast attempts to replicate a data 
item in appropriate areas instead of a large area around the server 
where the item is requested frequently 
 resource discovery 
we propose a cross-layer hybrid resource discovery scheme that 
relies on the multiple interactions of query reply and 
advertisement packets we assume that each resource is associated with a 
unique id 
 initially when a node wants to discover a resource it 
deploys query packets which carry the corresponding resource 
id and randomly explore the network to search for the requested 
resource upon receiving such a query packet a reply packet is 
generated by the node providing the requested resource 
advertisement packets can also be used to proactively inform other 
nodes about what resources are available at each node in addition 
to discovering the  identity of the node providing the requested 
resource it may be also necessary to discover a  route leading to 
this node for further interaction 
to allow intermediate nodes to make a decision on where to 
forward query packets each node maintains two tables neighbor 
 
the assumption of unique id is made for brevity in exposition 
and resources could be specified via attribute-value assertions 
table and pheromone table the neighbor table maintains a list of 
all current neighbors obtained via a neighbor discovery protocol 
the pheromone table maintains the mapping of a resource id and 
a neighbor id to a pheromone value this table is initially empty 
and is updated by a reply packet generated by a successful query 
figure illustrates an example of a neighbor table and a 
pheromone table maintained by node a having four neighbors when 
node a receives a query packet searching for a resource it makes 
a decision to which neighbor it should forward the query packet 
by computing the desirability of each of the neighbors that have 
not been visited before by the same query packet for a resource 
id r the desirability of choosing a neighbor n δ r n is obtained 
from the pheromone value of the entry whose neighbor and 
resource id fields are n and r respectively if no such entry exists in 
the pheromone table δ r n is set to zero 
once the desirabilities of all valid next hops have been calculated 
they are normalized to obtain the probability of choosing each 
neighbor in addition a small probability is also assigned to those 
neighbors with zero desirability to exercise the sporadic random 
walk primitive based on these probabilities a next hop is 
selected to forward the query packet to when a query packet 
encounters a node with a satisfying resource a reply packet is 
returned to the querying node the returning reply packet also 
updates the pheromone table at each node on its return trip by 
increasing the pheromone value in the entry whose resource id and 
neighbor id fields match the id of the discovered resource and 
the previous hop respectively if such an entry does not exist a 
new entry is added into the table therefore subsequent query 
packets looking for the same resource when encountering this 
pheromone information are then guided toward the same 
destination with a small probability of taking an alternate path 
since the hybrid discovery scheme neither relies on a manet 
routing protocol nor arranges nodes into a logical overlay query 
packets are to traverse the actual network topology in dense 
networks relatively large nodal degrees can have potential impacts 
on this random exploring mechanism to address this issue the 
hybrid scheme also incorporates proactive advertisement in 
addition to the reactive query to perform proactive advertisement 
each node periodically deploys an advertising packet containing a 
list of its available resources ids these packets will traverse 
away from the advertising node in a random walk manner up to a 
limited number of hops and advertise resource information to 
surrounding nodes in the same way as reply packets in the hybrid 
scheme an increase of pheromone serves as a positive feedback 
which indirectly guides query packets looking for similar 
resources intuitively the amount of pheromone increased is 
inversely proportional to the distance the reply packet has traveled 
back and other metrics such as quality of the resource could 
contribute to this amount as well each node also performs an 
implicit negative feedback for resources that have not been given 
a positive feedback for some time by regularly decreasing the 
pheromone in all of its pheromone table entries over time in 
addition pheromone can be reduced by an explicit negative response 
for instance a reply packet returning from a node that is not 
willing to provide a resource due to excessive workload as a result 
load balancing can be achieved via positive and negative 
feedback a node serving too many nodes can either return fewer 
responses to query packets or generate negative responses 
 the rd international conference on mobile technology applications and systems - mobility 
figure example illustrating neighbor and pheromone tables maintained by node a a wireless connectivity around a showing 
that it currently has four neighbors b a s neighbor table and c a possible pheromone table of a 
figure sample scenarios illustrating the three mechanisms supporting load-balancing a resource replication b resource 
relocation and c resource division 
 resource distribution 
in addition to resource discovery a querying node usually 
attempts to access and retrieve the contents of a resource after a 
successful discovery in certain situations it is also beneficial to 
make a resource readily available at multiple nodes when the 
resource can be relocated and or replicated such as data files 
furthermore in manets we should consider not only the amount 
of load handled by a resource provider but also the load on those 
intermediate nodes that are located on the communication paths 
between the provider and other nodes as well hence we describe 
a cross-layer hybrid resource distribution scheme to achieve load 
balancing by incorporating the functionalities of resource 
relocation resource replication and resource division 
 resource replication 
multiple replicas of a resource in the network help prevent a 
single node as well as nodes surrounding it from being overloaded 
by a large number of requests and data transfers an example is 
when a node has obtained a data file from another node the 
requesting node and the intermediate nodes can cache the file and 
start sharing that file with other surrounding nodes right away in 
addition replicable resources can also be proactively replicated at 
other nodes which are located in certain strategic areas for 
instance to help nodes find a resource quickly we could replicate 
the resource so that it becomes reachable by random walk for a 
specific number of hops from any node with some probability as 
depicted in figure a 
to realize this feature the hybrid resource distribution scheme 
employs a different type of control packet called resource 
replication packet which is responsible for finding an appropriate 
place to create a replica of a resource a resource replication 
packet of type r is deployed by a node that is providing the 
resource r itself unlike a query packet which follows higher 
pheromone upstream toward a resource it is looking for a 
resource replication packet tends to be propelled away from similar 
resources by moving itself downstream toward weaker 
pheromone when a resource replication packet finds itself in an area 
with sufficiently low pheromone it makes a decision whether it 
should continue exploring or turn back the decision depends on 
conditions such as current workload and or remaining energy of 
the node being visited as well as popularity of the resource itself 
 resource relocation 
in certain situations a resource may be required to transfer from 
one node to another for example a node may no longer want to 
possess a file due to the shortage of storage space but it cannot 
simply delete the file since other nodes may still need it in the 
future in this case the node can choose to create replicas of the 
file by the aforementioned resource replication mechanism and 
then delete its own copy let us consider a situation where a 
majority of nodes requesting for a resource are located far away from 
a resource provider as shown on the top of figure b if the 
resource r is relocatable it is preferred to be relocated to another 
area that is closer to those nodes similar to the bottom of the 
same figure hence network bandwidth is more efficiently 
utilized 
the rd conference on mobile technology applications and systems - mobility 
the hybrid resource distribution scheme incorporates resource 
relocation algorithms that are adaptive to user requests and aim to 
reduce communication overhead specifically by following the 
same pheromone maintenance concept the hybrid resource 
distribution scheme introduces another type of pheromone which 
corresponds to user requests instead of resources this type of 
pheromone called request pheromone is setup by query packets 
that are in their exploring phases not returning ones to guide a 
resource to a new location 
 resource division 
certain types of resources can be divided into smaller 
subresources e g a large file being broken into smaller files and 
distributed to multiple locations to avoid overloading a single 
node as depicted in figure c the hybrid resource distribution 
scheme incorporates a resource division mechanism that operates 
at a thin layer right above all the other mechanisms described 
earlier the resource division mechanism is responsible for 
decomposing divisible resources into sub-resources and then adds 
an extra keyword to distinguish each sub-resource from one 
another therefore each of these sub-resources will be seen by the 
other mechanisms as one single resource which can be 
independently discovered replicated and relocated the resource division 
mechanism is also responsible for combining data from these 
subresources together e g merging pieces of a file and delivering 
the final result to the application 
 replica invalidation 
although replicas improve accessibility and balance load replica 
invalidation becomes a critical issue when nodes caching 
updatable resources may concurrently update their own replicas 
which renders replicas held by other nodes obsolete most 
existing solutions to the replica invalidation problem either impose 
constrains that only the data source could perform update and 
invalidate other replicas or resort to network-wide flooding which 
results in heavy network traffic and leads to scalability problem 
or both the lack of infrastructure supports and frequent topology 
changes in manets further challenge the issue 
we apply the same cross-layer paradigm to invalidating replicas 
in manets which allows concurrent updates performed by 
multiple replicas to coordinate concurrent updates and disseminate 
replica invalidations a special infrastructure called validation 
mesh or mesh for short is adaptively maintained among nodes 
possessing  valid replicas of a resource once a node has updated 
its replica an invalidation packet will only be disseminated over 
the validation mesh to inform other replica-possessing nodes that 
their replicas become invalid and should be deleted the structure 
 topology of the validation mesh keeps evolving when nodes 
request and cache a resource when nodes update their 
respective replicas and invalidate other replicas and when nodes 
move to accommodate the dynamics our scheme integrates the 
components of swarm intelligence to adaptively maintain the 
validation mesh without relying on any underlying manet routing 
protocol in particular the scheme takes into account concurrent 
updates initiated by multiple nodes to ensure the consistency 
among replicas in addition version number is used to distinguish 
new from old replicas when invalidating any stale replica 
simulation results show that the proposed scheme effectively facilitates 
concurrent replica updates and efficiently perform replica 
invalidation without incurring network-wide flooding 
figure depicts the idea of  validation mesh which maintains 
connectivity among nodes holding valid replicas of a resource to 
avoid network-wide flooding when invalidating replicas 
figure examples showing maintenance of validation mesh 
there are eight nodes in the sample network and we start with 
only node a holding the valid file as shown in figure a later 
on node g issues a query packet for the file and eventually 
obtains the file from a via nodes b and d since intermediate nodes 
are allowed to cache forwarded data nodes b d and g will now 
hold valid replicas of the file as a result a validation mesh is 
established among nodes a b d and g as depicted in figure 
 b in figure c another node h has issued a query packet 
for the same file and obtained it from node b s cache via node e 
at this point six nodes hold valid replicas and are connected 
through the validation mesh now we assume node g updates its 
replica of the file and informs the other nodes by sending an 
invalidation packet over the validation mesh consequently all 
other nodes except g remove their replicas of the file from their 
storage and the validation mesh is torn down however query 
forwarding pheromone as denoted by the dotted arrows in figure 
 d is setup at these nodes via the  reverse paths in which the 
invalidation packets have traversed so that future requests for this 
file will be forwarded to node g in figure e node h makes a 
new request for the file again this time its query packet follows 
the pheromone toward node g where the updated file can be 
obtained eventually a new validation mesh is established over 
nodes g b d e and h 
to maintain a validation mesh among the nodes holding valid 
replicas one of them is designated to be the focal node initially 
the node that originally holds the data is the focal node as nodes 
update replicas the node that last or most recently updates a 
 the rd international conference on mobile technology applications and systems - mobility 
corresponding replica assumes the role of focal node we also 
name nodes such as g and h who originate requests to replicate 
data as clients and nodes b d and e who locally cache passing 
data as data nodes for instance in figures a b and c 
node a is the focal node in figures d e and f node g 
becomes the focal node in addition to accommodate newly 
participating nodes and mobility of nodes the focal node periodically 
floods the validation mesh with a keep-alive packet so that nodes 
who can hear this packet are considered themselves to be part of 
the validation mesh if a node holding a valid updated replica 
doesn t hear a keep-alive packet for a certain time interval it will 
deploy a search packet using the resource discovery mechanism 
described in section to find another node termed attachment 
point currently on the validation mesh so that it can attach itself 
to once an attachment point is found a search reply packet is 
returned to the disconnected node who originated the search 
intermediate nodes who forward the search reply packet will 
become part of the validation mesh as well to illustrate the effect of 
node mobility in figure f node h has moved to a location 
where it is not directly connected to the mesh via the resource 
discovery mechanism node h relies on an intermediate node f to 
connect itself to the mesh here node f although part of the 
validation mesh doesn t hold data replica and hence is termed 
nondata node 
client and data node who keep hearing the keep-alive packets 
from the focal node act as if they are holding a valid replica so 
that they can reply to query packets like node b in figure c 
replying a request from node h while a disconnected node 
attempting to discover an attachment point to reattach itself to the 
mesh the disconnected node can t reply to a query packet for 
instance in figure f node h does not reply to any query packet 
before it reattaches itself to the mesh 
although validation mesh provides a conceptual topology that 
connects all replicas together coordinates concurrent updates 
and disseminates invalidation packets the technical issue is 
how such a mesh topology could be effectively and efficiently 
maintained and evolved when a nodes request and cache a 
resource b when nodes update their respective replicas and 
invalidate other replicas and c when nodes move without relying 
on any manet routing protocols the two primitives work 
together to facilitate efficient search and adaptive maintenance 
 performance evaluation 
we have conducted simulation experiments using the qualnet 
simulator to evaluate the performance of the described resource 
discovery resource distribution and replica invalidation schemes 
however due to space limitation only the performance of the 
replica invalidation is reported in our experiments eighty nodes 
are uniformly distributed over a terrain of size × m 
 
each node has a communication range of approximately m 
over a mbps wireless channel using ieee as the mac 
layer we use the random-waypoint mobility model with a pause 
time of second nodes may move at the minimum and maximum 
speeds of m s and m s respectively table lists other 
parameter settings used in the simulation initially there is one 
resource server node in network two nodes are randomly picked 
up every seconds as clients every β seconds we check the 
number of nodes n which have gotten data then we randomly 
pickup min γ n nodes from them to initiate data update each 
experiment is run for minutes 
table simulation settings 
hop limit 
advertise hop limit 
keepalive interval second 
num search 
advertise interval second 
expiration interval second 
average query generation rate query sec 
max of concurrent update γ 
frequency of update β s 
we evaluate the performance under different mobility speed the 
density the maximum number of concurrent update nodes and 
update frequency using two metrics 
 average overhead per update measures the average number of 
packets transmitted per update in the network 
 average delay per update measures how long our approach 
takes to finish an update on average 
all figures shown present the results with a confidence 
interval 
figure overhead vs speed 
for nodes 
figure overhead vs density 
figure overhead vs max 
 concurrent updates 
figure overhead vs freq 
figure delay vs speed figure delay vs density 
the rd conference on mobile technology applications and systems - mobility 
figure delay vs max 
 concurrent updates 
figure delay vs freq 
figures and show the overhead versus various parameter 
values in figure the overhead increases as the speed increase 
which is due to the fact that as the speed increase nodes move out 
of mesh more frequently and will send out more search packets 
however the overhead is not high and even in speed m sec 
the overhead is below packets in contrast the packets will be 
expected to be more than packets at various speeds when 
flooding is used 
figure shows that the overhead almost remains the same under 
various densities that is attributed to only flooding over the mesh 
instead of the whole network the size of mesh doesn t vary much 
on various densities so that the overhead doesn t vary much 
figure shows that overhead also almost remains the same under 
various maximum number of concurrent updates that s because 
one more node just means one more flood over the mesh during 
update process so that the impact is limited 
figure shows that if updates happen more frequently the 
overhead is higher this is because the more quickly updates happen 
 there will be more keep alive message over the mesh between 
two updates and nodes move out of mesh more frequently and 
send out more search packets 
figures and show the delay versus various parameter 
values from figure we know the delay increases as the speed 
increases which is due to the fact that with increasing speed 
clients will move out of mesh with higher probability when these 
clients want to update data they will spend time to first search the 
mesh the faster the speed the more time clients need to spend to 
search the mesh 
figure shows that delay is negligibly affected by the density 
delay decreases slightly as the number of nodes increases due to 
the fact that the more nodes in the network the more nodes 
receives the advertisement packets which helps the search packet 
find the target so that the delay of update decreases 
figure shows that delay decreases slightly as the maximum 
number of concurrent updates increases the larger the maximum 
number of concurrent updates is the more nodes are picked up to 
do update then with higher probability one of these nodes is still 
in mesh and finishes the update immediately don t need to search 
mesh first which decreases the delay 
figure shows how the delay varies with the update frequency 
when updates happen more frequently the delay will higher 
because the less frequently the more time nodes in mesh have to 
move out of mesh then they need to take time to search the mesh 
when they do update which increases the delay 
the simulation results show that the replica invalidation scheme 
can significantly reduce the overhead with an acceptable delay 
 conclusion 
to facilitate resource discovery and distribution over manets 
one approach is to designing peer-to-peer p p systems over 
manets which constructs an overlay by organizing peers of the 
system into a logical structure on the top of manets physical 
topology however deploying overlay over manets may result 
in either a large number of flooding operations triggered by the 
routing process or inefficiency in terms of bandwidth usage 
specifically overlay routing relies on the network-layer routing 
protocols in the case of a reactive routing protocol routing on the 
overlay may cause a large number of flooded route discovery 
message since the routing path in each routing step must be 
discovered on demand on the other hand if a proactive routing 
protocol is adopted each peer has to periodically broadcast 
control messages which leads to poor efficiency in terms of 
bandwidth usage either way constructing an overlay will potentially 
suffer from the scalability problem the paper describes a design 
paradigm that uses the functional primitives of positive negative 
feedback and sporadic random walk to design robust resource 
discovery and distribution schemes over manets in particular 
the scheme offers the features of cross-layer design of p p 
systems which allows the routing process at both the p p and the 
network layers to be integrated to reduce overhead scalability 
and mobility support which minimizes the use of global flooding 
operations and adaptively combines proactive resource 
advertisement and reactive resource discovery and load balancing 
which facilitates resource replication relocation and division to 
achieve load balancing 
 references 
 a oram peer-to-peer harnessing the power of disruptive 
technologies o reilly march 
 s helal n desai v verma and c lee konark - a 
service discovery and delivery protocol for ad-hoc 
networks in the third ieee conference on wireless 
communication networks wcnc new orleans louisiana 
 g krotuem proem a peer-to-peer computing platform 
for mobile ad-hoc networks in advanced topic workshop 
middleware for mobile computing germany 
 m papadopouli and h schulzrinne a performance 
analysis of ds a peer-to-peer data dissemination and 
prefetching tool for mobile users in advances in wired and 
wireless communications ieee sarnoff symposium digest 
ewing nj best student paper poster award 
 u mohan k almeroth and e belding-royer scalable 
service discovery in mobile ad-hoc networks in ifip 
networking conference athens greece may 
 l yin and g cao supporting cooperative caching in ad 
hoc networks in ieee infocom 
 v thanedar k almeroth and e belding-royer a 
lightweight content replication scheme for mobile ad-hoc 
environments in ifip networking conference athens greece 
may 
 the rd international conference on mobile technology applications and systems - mobility 
a scalable distributed information management system∗ 
praveen yalagandula 
ypraveen cs utexas edu 
mike dahlin 
dahlin cs utexas edu 
department of computer sciences 
the university of texas at austin 
austin tx 
abstract 
we present a scalable distributed information management 
system sdims that aggregates information about large-scale 
networked systems and that can serve as a basic building block for a 
broad range of large-scale distributed applications by providing 
detailed views of nearby information and summary views of global 
information to serve as a basic building block a sdims should have 
four properties scalability to many nodes and attributes flexibility 
to accommodate a broad range of applications administrative 
isolation for security and availability and robustness to node and 
network failures we design implement and evaluate a sdims that 
leverages distributed hash tables dht to create scalable 
aggregation trees provides flexibility through a simple api that lets 
applications control propagation of reads and writes provides 
administrative isolation through simple extensions to current dht 
algorithms and achieves robustness to node and network 
reconfigurations through lazy reaggregation on-demand reaggregation 
and tunable spatial replication through extensive simulations and 
micro-benchmark experiments we observe that our system is an 
order of magnitude more scalable than existing approaches achieves 
isolation properties at the cost of modestly increased read latency 
in comparison to flat dhts and gracefully handles failures 
categories and subject descriptors 
c computer-communication networks distributed 
systems-network operating systems distributed databases 
general terms 
management design experimentation 
 introduction 
the goal of this research is to design and build a scalable 
distributed information management system sdims that aggregates 
information about large-scale networked systems and that can serve 
as a basic building block for a broad range of large-scale distributed 
applications monitoring querying and reacting to changes in 
the state of a distributed system are core components of 
applications such as system management service 
placement data sharing and caching sensor 
monitoring and control multicast tree formation 
 and naming and request routing we therefore 
speculate that a sdims in a networked system would provide a 
distributed operating systems backbone and facilitate the 
development and deployment of new distributed services 
for a large scale information system hierarchical aggregation 
is a fundamental abstraction for scalability rather than expose all 
information to all nodes hierarchical aggregation allows a node to 
access detailed views of nearby information and summary views of 
global information in a sdims based on hierarchical aggregation 
different nodes can therefore receive different answers to the query 
find a nearby node with at least gb of free memory or find 
a nearby copy of file foo a hierarchical system that aggregates 
information through reduction trees allows nodes to access 
information they care about while maintaining system scalability 
to be used as a basic building block a sdims should have 
four properties first the system should be scalable it should 
accommodate large numbers of participating nodes and it should 
allow applications to install and monitor large numbers of data 
attributes enterprise and global scale systems today might have tens 
of thousands to millions of nodes and these numbers will increase 
over time similarly we hope to support many applications and 
each application may track several attributes e g the load and 
free memory of a system s machines or millions of attributes e g 
which files are stored on which machines 
second the system should have flexibility to accommodate a 
broad range of applications and attributes for example 
readdominated attributes like numcpus rarely change in value while 
write-dominated attributes like numprocesses change quite often 
an approach tuned for read-dominated attributes will consume high 
bandwidth when applied to write-dominated attributes conversely 
an approach tuned for write-dominated attributes will suffer from 
unnecessary query latency or imprecision for read-dominated 
attributes therefore a sdims should provide mechanisms to handle 
different types of attributes and leave the policy decision of tuning 
replication to the applications 
third a sdims should provide administrative isolation in a 
large system it is natural to arrange nodes in an organizational or 
an administrative hierarchy a sdims should support 
administrasession distributed information systems 
 
tive isolation in which queries about an administrative domain s 
information can be satisfied within the domain so that the system can 
operate during disconnections from other domains so that an 
external observer cannot monitor or affect intra-domain queries and 
to support domain-scoped queries efficiently 
fourth the system must be robust to node failures and 
disconnections a sdims should adapt to reconfigurations in a timely 
fashion and should also provide mechanisms so that applications 
can tradeoff the cost of adaptation with the consistency level in the 
aggregated results when reconfigurations occur 
we draw inspiration from two previous works astrolabe 
and distributed hash tables dhts 
astrolabe is a robust information management system 
astrolabe provides the abstraction of a single logical aggregation tree 
that mirrors a system s administrative hierarchy it provides a 
general interface for installing new aggregation functions and provides 
eventual consistency on its data astrolabe is robust due to its use 
of an unstructured gossip protocol for disseminating information 
and its strategy of replicating all aggregated attribute values for a 
subtree to all nodes in the subtree this combination allows any 
communication pattern to yield eventual consistency and allows 
any node to answer any query using local information this high 
degree of replication however may limit the system s ability to 
accommodate large numbers of attributes also although the 
approach works well for read-dominated attributes an update at one 
node can eventually affect the state at all nodes which may limit 
the system s flexibility to support write-dominated attributes 
recent research in peer-to-peer structured networks resulted in 
distributed hash tables dhts -a data 
structure that scales with the number of nodes and that distributes 
the read-write load for different queries among the participating 
nodes it is interesting to note that although these systems export 
a global hash table abstraction many of them internally make use 
of what can be viewed as a scalable system of aggregation trees 
to for example route a request for a given key to the right dht 
node indeed rather than export a general dht interface plaxton 
et al s original application makes use of hierarchical 
aggregation to allow nodes to locate nearby copies of objects it seems 
appealing to develop a sdims abstraction that exposes this internal 
functionality in a general way so that scalable trees for aggregation 
can be a basic system building block alongside the dhts 
at a first glance it might appear to be obvious that simply 
fusing dhts with astrolabe s aggregation abstraction will result in a 
sdims however meeting the sdims requirements forces a 
design to address four questions how to scalably map different 
attributes to different aggregation trees in a dht mesh how to 
provide flexibility in the aggregation to accommodate different 
application requirements how to adapt a global flat dht mesh 
to attain administrative isolation property and how to provide 
robustness without unstructured gossip and total replication 
the key contributions of this paper that form the foundation of 
our sdims design are as follows 
 we define a new aggregation abstraction that specifies both 
attribute type and attribute name and that associates an 
aggregation function with a particular attribute type this 
abstraction paves the way for utilizing the dht system s internal 
trees for aggregation and for achieving scalability with both 
nodes and attributes 
 we provide a flexible api that lets applications control the 
propagation of reads and writes and thus trade off update 
cost read latency replication and staleness 
 we augment an existing dht algorithm to ensure path 
convergence and path locality properties in order to achieve 
administrative isolation 
 we provide robustness to node and network reconfigurations 
by a providing temporal replication through lazy 
reaggregation that guarantees eventual consistency and b 
ensuring that our flexible api allows demanding applications gain 
additional robustness by using tunable spatial replication of 
data aggregates or by performing fast on-demand 
reaggregation to augment the underlying lazy reaggregation or by 
doing both 
we have built a prototype of sdims through simulations and 
micro-benchmark experiments on a number of department machines 
and planetlab nodes we observe that the prototype achieves 
scalability with respect to both nodes and attributes through use 
of its flexible api inflicts an order of magnitude lower maximum 
node stress than unstructured gossiping schemes achieves isolation 
properties at a cost of modestly increased read latency compared to 
flat dhts and gracefully handles node failures 
this initial study discusses key aspects of an ongoing system 
building effort but it does not address all issues in building a sdims 
for example we believe that our strategies for providing robustness 
will mesh well with techniques such as supernodes and other 
ongoing efforts to improve dhts for further improving 
robustness also although splitting aggregation among many trees 
improves scalability for simple queries this approach may make 
complex and multi-attribute queries more expensive compared to 
a single tree additional work is needed to understand the 
significance of this limitation for real workloads and if necessary to 
adapt query planning techniques from dht abstractions 
to scalable aggregation tree abstractions 
in section we explain the hierarchical aggregation 
abstraction that sdims provides to applications in sections and we 
describe the design of our system for achieving the flexibility 
scalability and administrative isolation requirements of a sdims in 
section we detail the implementation of our prototype system 
section addresses the issue of adaptation to the topological 
reconfigurations in section we present the evaluation of our 
system through large-scale simulations and microbenchmarks on real 
networks section details the related work and section 
summarizes our contribution 
 aggregation abstraction 
aggregation is a natural abstraction for a large-scale distributed 
information system because aggregation provides scalability by 
allowing a node to view detailed information about the state near it 
and progressively coarser-grained summaries about progressively 
larger subsets of a system s data 
our aggregation abstraction is defined across a tree spanning all 
nodes in the system each physical node in the system is a leaf and 
each subtree represents a logical group of nodes note that logical 
groups can correspond to administrative domains e g department 
or university or groups of nodes within a domain e g 
workstations on a lan in cs department an internal non-leaf node 
which we call virtual node is simulated by one or more physical 
nodes at the leaves of the subtree for which the virtual node is the 
root we describe how to form such trees in a later section 
each physical node has local data stored as a set of attributetype 
attributename value tuples such as configuration numcpus 
 mcast membership session foo yes or file stored foo 
myipaddress the system associates an aggregation function ftype 
with each attribute type and for each level-i subtree ti in the 
system the system defines an aggregate value vi type name for each 
 at 
tributetype attributename pair as follows for a physical leaf 
node t at level v type name is the locally stored value for the 
attribute type and name or null if no matching tuple exists then 
the aggregate value for a level-i subtree ti is the aggregation 
function for the type ftype computed across the aggregate values of 
each of ti s k children 
vi type name ftype v 
i− type name v 
i− type name vk− 
i− type name 
although sdims allows arbitrary aggregation functions it is 
often desirable that these functions satisfy the hierarchical 
computation property f v vn f f v vs f vs vs 
 f vsk vn where vi is the value of an attribute at node 
i for example the average operation defined as avg v vn 
 n ∑n 
i vi does not satisfy the property instead if an attribute 
stores values as tuples sum count the attribute satisfies the 
hierarchical computation property while still allowing the applications 
to compute the average from the aggregate sum and count values 
finally note that for a large-scale system it is difficult or 
impossible to insist that the aggregation value returned by a probe 
corresponds to the function computed over the current values at the 
leaves at the instant of the probe therefore our system provides 
only weak consistency guarantees - specifically eventual 
consistency as defined in 
 flexibility 
a major innovation of our work is enabling flexible aggregate 
computation and propagation the definition of the aggregation 
abstraction allows considerable flexibility in how when and where 
aggregate values are computed and propagated while previous 
systems implement a single static strategy 
we argue that a sdims should provide flexible computation and 
propagation to efficiently support wide variety of applications with 
diverse requirements in order to provide this flexibility we 
develop a simple interface that decomposes the aggregation 
abstraction into three pieces of functionality install update and probe 
this definition of the aggregation abstraction allows our system 
to provide a continuous spectrum of strategies ranging from lazy 
aggregate computation and propagation on reads to aggressive 
immediate computation and propagation on writes in figure we 
illustrate both extreme strategies and an intermediate strategy 
under the lazy update-local computation and propagation strategy 
an update or write only affects local state then a probe or read 
that reads a level-i aggregate value is sent up the tree to the issuing 
node s level-i ancestor and then down the tree to the leaves the 
system then computes the desired aggregate value at each layer up 
the tree until the level-i ancestor that holds the desired value 
finally the level-i ancestor sends the result down the tree to the 
issuing node in the other extreme case of the aggressive update-all 
immediate computation and propagation on writes when an 
update occurs changes are aggregated up the tree and each new 
aggregate value is flooded to all of a node s descendants in this 
case each level-i node not only maintains the aggregate values for 
the level-i subtree but also receives and locally stores copies of all 
of its ancestors level- j j i aggregation values also a leaf 
satisfies a probe for a level-i aggregate using purely local data in an 
intermediate update-up strategy the root of each subtree maintains 
the subtree s current aggregate value and when an update occurs 
the leaf node updates its local state and passes the update to its 
parent and then each successive enclosing subtree updates its 
aggregate value and passes the new value to its parent this strategy 
satisfies a leaf s probe for a level-i aggregate value by sending the 
probe up to the level-i ancestor of the leaf and then sending the 
aggregate value down to the leaf finally notice that other strategies 
exist in general an update-upk-downj strategy aggregates up to 
parameter description optional 
attrtype attribute type 
aggrfunc aggregation function 
up how far upward each update is 
sent default all 
x 
down how far downward each 
aggregate is sent default none 
x 
domain domain restriction default none x 
exptime expiry time 
table arguments for the install operation 
the kth level and propagates the aggregate values of a node at level 
l s t l ≤ k downward for j levels 
a sdims must provide a wide range of flexible computation and 
propagation strategies to applications for it to be a general 
abstraction an application should be able to choose a particular 
mechanism based on its read-to-write ratio that reduces the bandwidth 
consumption while attaining the required responsiveness and 
precision note that the read-to-write ratio of the attributes that 
applications install vary extensively for example a read-dominated 
attribute like numcpus rarely changes in value while a 
writedominated attribute like numprocesses changes quite often an 
aggregation strategy like update-all works well for read-dominated 
attributes but suffers high bandwidth consumption when applied for 
write-dominated attributes conversely an approach like 
updatelocal works well for write-dominated attributes but suffers from 
unnecessary query latency or imprecision for read-dominated 
attributes 
sdims also allows non-uniform computation and propagation 
across the aggregation tree with different up and down parameters 
in different subtrees so that applications can adapt with the 
spatial and temporal heterogeneity of read and write operations with 
respect to spatial heterogeneity access patterns may differ for 
different parts of the tree requiring different propagation strategies 
for different parts of the tree similarly with respect to temporal 
heterogeneity access patterns may change over time requiring 
different strategies over time 
 aggregation api 
we provide the flexibility described above by splitting the 
aggregation api into three functions install installs an aggregation 
function that defines an operation on an attribute type and 
specifies the update strategy that the function will use update inserts 
or modifies a node s local value for an attribute and probe 
obtains an aggregate value for a specified subtree the install 
interface allows applications to specify the k and j parameters of the 
update-upk-downj strategy along with the aggregation function 
the update interface invokes the aggregation of an attribute on the 
tree according to corresponding aggregation function s aggregation 
strategy the probe interface not only allows applications to obtain 
the aggregated value for a specified tree but also allows a probing 
node to continuously fetch the values for a specified time thus 
enabling an application to adapt to spatial and temporal heterogeneity 
the rest of the section describes these three interfaces in detail 
 install 
the install operation installs an aggregation function in the 
system the arguments for this operation are listed in table the 
attrtype argument denotes the type of attributes on which this 
aggregation function is invoked installed functions are soft state that 
must be periodically renewed or they will be garbage collected at 
exptime 
the arguments up and down specify the aggregate computation 
 
update strategy on update on probe for global aggregate value on probe for level- aggregate value 
update-local 
update-up 
update-all 
figure flexible api 
parameter description optional 
attrtype attribute type 
attrname attribute name 
mode continuous or one-shot default 
one-shot 
x 
level level at which aggregate is sought 
 default at all levels 
x 
up how far up to go and re-fetch the 
value default none 
x 
down how far down to go and 
reaggregate default none 
x 
exptime expiry time 
table arguments for the probe operation 
and propagation strategy update-upk-downj the domain 
argument if present indicates that the aggregation function should be 
installed on all nodes in the specified domain otherwise the 
function is installed on all nodes in the system 
 update 
the update operation takes three arguments attrtype attrname 
and value and creates a new attrtype attrname value tuple or 
updates the value of an old tuple with matching attrtype and 
attrname at a leaf node 
the update interface meshes with installed aggregate 
computation and propagation strategy to provide flexibility in particular 
as outlined above and described in detail in section after a leaf 
applies an update locally the update may trigger re-computation 
of aggregate values up the tree and may also trigger propagation 
of changed aggregate values down the tree notice that our 
abstraction associates an aggregation function with only an attrtype 
but lets updates specify an attrname along with the attrtype this 
technique helps achieve scalability with respect to nodes and 
attributes as described in section 
 probe 
the probe operation returns the value of an attribute to an 
application the complete argument set for the probe operation is shown 
in table along with the attrname and the attrtype arguments a 
level argument specifies the level at which the answers are required 
for an attribute in our implementation we choose to return results 
at all levels k l for a level-l probe because i it is inexpensive as 
the nodes traversed for level-l probe also contain level k aggregates 
for k l and as we expect the network cost of transmitting the 
additional information to be small for the small aggregates which we 
focus and ii it is useful as applications can efficiently get several 
aggregates with a single probe e g for domain-scoped queries as 
explained in section 
probes with mode set to continuous and with finite exptime 
enable applications to handle spatial and temporal heterogeneity when 
node a issues a continuous probe at level l for an attribute then 
regardless of the up and down parameters updates for the attribute 
at any node in a s level-l ancestor s subtree are aggregated up to 
level l and the aggregated value is propagated down along the path 
from the ancestor to a note that continuous mode enables sdims 
to support a distributed sensor-actuator mechanism where a 
sensor monitors a level-i aggregate with a continuous mode probe and 
triggers an actuator upon receiving new values for the probe 
the up and down arguments enable applications to perform 
ondemand fast re-aggregation during reconfigurations where a forced 
re-aggregation is done for the corresponding levels even if the 
aggregated value is available as we discuss in section when 
present the up and down arguments are interpreted as described 
in the install operation 
 dynamic adaptation 
at the api level the up and down arguments in install api can be 
regarded as hints since they suggest a computation strategy but do 
not affect the semantics of an aggregation function a sdims 
implementation can dynamically adjust its up down strategies for an 
attribute based on its measured read write frequency but a virtual 
intermediate node needs to know the current up and down 
propagation values to decide if the local aggregate is fresh in order to 
answer a probe this is the key reason why up and down need to be 
statically defined at the install time and can not be specified in the 
update operation in dynamic adaptation we implement a 
leasebased mechanism where a node issues a lease to a parent or a child 
denoting that it will keep propagating the updates to that parent or 
child we are currently evaluating different policies to decide when 
to issue a lease and when to revoke a lease 
 scalability 
our design achieves scalability with respect to both nodes and 
attributes through two key ideas first it carefully defines the 
aggregation abstraction to mesh well with its underlying scalable dht 
system second it refines the basic dht abstraction to form an 
autonomous dht adht to achieve the administrative isolation 
properties that are crucial to scaling for large real-world systems 
in this section we describe these two ideas in detail 
 leveraging dhts 
in contrast to previous systems sdims s 
aggregation abstraction specifies both an attribute type and attribute 
name and associates an aggregation function with a type rather than 
just specifying and associating a function with a name installing a 
single function that can operate on many different named attributes 
matching a type improves scalability for sparse attribute types 
with large sparsely-filled name spaces for example to construct 
a file location service our interface allows us to install a single 
function that computes an aggregate value for any named file a 
subtree s aggregate value for fileloc name would be the id of 
a node in the subtree that stores the named file conversely 
astrolabe copes with sparse attributes by having aggregation functions 
compute sets or lists and suggests that scalability can be improved 
by representing such sets with bloom filters supporting sparse 
names within a type provides at least two advantages first when 
the value associated with a name is updated only the state 
associ 
 
 
 
 
 
 
l 
l 
l 
l 
figure the dht tree corresponding to key dhttree 
and the corresponding aggregation tree 
ated with that name needs to be updated and propagated to other 
nodes second splitting values associated with different names 
into different aggregation values allows our system to leverage 
distributed hash tables dhts to map different names to different 
trees and thereby spread the function s logical root node s load and 
state across multiple physical nodes 
given this abstraction scalably mapping attributes to dhts is 
straightforward dht systems assign a long random id to each 
node and define an algorithm to route a request for key k to a 
node rootk such that the union of paths from all nodes forms a tree 
dhttreek rooted at the node rootk now as illustrated in figure 
by aggregating an attribute along the aggregation tree 
corresponding to dhttreek for k hash attribute type attribute name 
different attributes will be aggregated along different trees 
in comparison to a scheme where all attributes are aggregated 
along a single tree aggregating along multiple trees incurs lower 
maximum node stress whereas in a single aggregation tree 
approach the root and the intermediate nodes pass around more 
messages than leaf nodes in a dht-based multi-tree each node acts as 
an intermediate aggregation point for some attributes and as a leaf 
node for other attributes hence this approach distributes the onus 
of aggregation across all nodes 
 administrative isolation 
aggregation trees should provide administrative isolation by 
ensuring that for each domain the virtual node at the root of the 
smallest aggregation subtree containing all nodes of that domain is 
hosted by a node in that domain administrative isolation is 
important for three reasons i for security - so that updates and probes 
flowing in a domain are not accessible outside the domain ii for 
availability - so that queries for values in a domain are not affected 
by failures of nodes in other domains and iii for efficiency - so 
that domain-scoped queries can be simple and efficient 
to provide administrative isolation to aggregation trees a dht 
should satisfy two properties 
 path locality search paths should always be contained in 
the smallest possible domain 
 path convergence search paths for a key from different 
nodes in a domain should converge at a node in that domain 
existing dhts support path locality or can easily support it 
by using the domain nearness as the distance metric but they 
do not guarantee path convergence as those systems try to optimize 
the search path to the root to reduce response latency for example 
pastry uses prefix routing in which each node s routing table 
contains one row per hexadecimal digit in the nodeid space where 
the ith row contains a list of nodes whose nodeids differ from the 
current node s nodeid in the ith digit with one entry for each 
possible digit value given a routing topology to route a packet to 
an arbitrary destination key a node in pastry forwards a packet to 
the node with a nodeid prefix matching the key in at least one more 
digit than the current node if such a node is not known the 
current node uses an additional data structure the leaf set containing 
 xx 
 xx 
 xx 
 xx 
 xx 
univ 
dep dep 
key xx 
 xx xx xx xx xx 
l 
l 
l 
figure example shows how isolation property is violated 
with original pastry we also show the corresponding 
aggregation tree 
 xx 
 xx 
 xx 
 xx 
 xx 
univ 
dep dep 
key xx 
x 
 xx xx xx xx xx 
l 
l 
l 
figure autonomous dht satisfying the isolation property 
also the corresponding aggregation tree is shown 
l immediate higher and lower neighbors in the nodeid space and 
forwards the packet to a node with an identical prefix but that is 
numerically closer to the destination key in the nodeid space this 
process continues until the destination node appears in the leaf set 
after which the message is routed directly pastry s expected 
number of routing steps is logn where n is the number of nodes but 
as figure illustrates this algorithm does not guarantee path 
convergence if two nodes in a domain have nodeids that match a key 
in the same number of bits both of them can route to a third node 
outside the domain when routing for that key 
simple modifications to pastry s route table construction and 
key-routing protocols yield an autonomous dht adht that 
satisfies the path locality and path convergence properties as figure 
illustrates whenever two nodes in a domain share the same prefix 
with respect to a key and no other node in the domain has a longer 
prefix our algorithm introduces a virtual node at the boundary of 
the domain corresponding to that prefix plus the next digit of the 
key such a virtual node is simulated by the existing node whose id 
is numerically closest to the virtual node s id our adht s routing 
table differs from pastry s in two ways first each node maintains 
a separate leaf set for each domain of which it is a part second 
nodes use two proximity metrics when populating the routing tables 
- hierarchical domain proximity is the primary metric and network 
distance is secondary then to route a packet to a global root for a 
key adht routing algorithm uses the routing table and the leaf set 
entries to route to each successive enclosing domain s root the 
virtual or real node in the domain matching the key in the maximum 
number of digits additional details about the adht algorithm 
are available in an extended technical report 
properties maintaining a different leaf set for each 
administrative hierarchy level increases the number of neighbors that each 
node tracks to b ∗lgb n c l from b ∗lgb n c in unmodified 
pastry where b is the number of bits in a digit n is the number of 
nodes c is the leaf set size and l is the number of domain levels 
routing requires o lgbn l steps compared to o lgbn steps in 
pastry also each routing hop may be longer than in pastry because 
the modified algorithm s routing table prefers same-domain nodes 
over nearby nodes we experimentally quantify the additional 
routing costs in section 
in a large system the adht topology allows domains to 
im 
a a b 
 b b 
 b 
 b b 
 b 
l 
l 
l 
 b b 
 b 
 a a 
 a 
 a a 
 a 
 a a 
 a 
figure example for domain-scoped queries 
prove security for sensitive attribute types by installing them only 
within a specified domain then aggregation occurs entirely within 
the domain and a node external to the domain can neither observe 
nor affect the updates and aggregation computations of the attribute 
type furthermore though we have not implemented this feature 
in the prototype the adht topology would also support 
domainrestricted probes that could ensure that no one outside of a domain 
can observe a probe for data stored within the domain 
the adht topology also enhances availability by allowing the 
common case of probes for data within a domain to depend only on 
a domain s nodes this for example allows a domain that becomes 
disconnected from the rest of the internet to continue to answer 
queries for local data 
aggregation trees that provide administrative isolation also 
enable the definition of simple and efficient domain-scoped 
aggregation functions to support queries like what is the average load 
on machines in domain x for example consider an 
aggregation function to count the number of machines in an example 
system with three machines illustrated in figure each leaf node 
l updates attribute nummachines with a value vl containing a set 
of tuples of form domain count for each domain of which the 
node is a part in the example the node a with name a a 
performs an update with the value a a a an 
aggregation function at an internal virtual node hosted on node n with 
child set c computes the aggregate as a set of tuples for each 
domain d that n is part of form a tuple d ∑c∈c count d count ∈ 
vc this computation is illustrated in the figure now a query 
for nummachines with level set to max will return the 
aggregate values at each intermediate virtual node on the path to the 
root as a set of tuples tree level aggregated value from which 
it is easy to extract the count of machines at each enclosing 
domain for example a would receive b b b 
 a a a a a a note that 
supporting domain-scoped queries would be less convenient and 
less efficient if aggregation trees did not conform to the system s 
administrative structure it would be less efficient because each 
intermediate virtual node will have to maintain a list of all values at 
the leaves in its subtree along with their names and it would be less 
convenient as applications that need an aggregate for a domain will 
have to pick values of nodes in that domain from the list returned 
by a probe and perform computation 
 prototype implementation 
the internal design of our sdims prototype comprises of two 
layers the autonomous dht adht layer manages the overlay 
topology of the system and the aggregation management layer 
 aml maintains attribute tuples performs aggregations stores 
and propagates aggregate values given the adht construction 
described in section each node implements an aggregation 
management layer aml to support the flexible api described in 
section in this section we describe the internal state and 
operation of the aml layer of a node in the system 
local 
mib 
mibs 
ancestor 
reduction mib 
 level mibs 
ancestor 
mib from 
child x 
mib from 
child x 
level 
level 
level 
level 
 xxx 
 xx 
 x 
from parents x 
to parent x 
−− aggregation functions 
from parents 
to parent xx 
 x 
 x 
 x 
to parent xx 
node id xxx 
 x 
 x 
 x 
 x 
virtual node 
figure example illustrating the data structures and the 
organization of them at a node 
we refer to a store of attribute type attribute name value tuples 
as a management information base or mib following the 
terminology from astrolabe and snmp we refer an attribute 
type attribute name tuple as an attribute key 
as figure illustrates each physical node in the system acts as 
several virtual nodes in the aml a node acts as leaf for all attribute 
keys as a level- subtree root for keys whose hash matches the 
node s id in b prefix bits where b is the number of bits corrected 
in each step of the adht s routing scheme as a level-i subtree 
root for attribute keys whose hash matches the node s id in the 
initial i ∗ b bits and as the system s global root for attribute keys 
whose hash matches the node s id in more prefix bits than any 
other node in case of a tie the first non-matching bit is ignored 
and the comparison is continued 
to support hierarchical aggregation each virtual node at the root 
of a level-i subtree maintains several mibs that store child mibs 
containing raw aggregate values gathered from children a 
reduction mib containing locally aggregated values across this raw 
information and an ancestor mib containing aggregate values 
scattered down from ancestors this basic strategy of maintaining 
child reduction and ancestor mibs is based on astrolabe 
but our structured propagation strategy channels information that 
flows up according to its attribute key and our flexible propagation 
strategy only sends child updates up and ancestor aggregate results 
down as far as specified by the attribute key s aggregation 
function note that in the discussion below for ease of explanation we 
assume that the routing protocol is correcting single bit at a time 
 b our system built upon pastry handles multi-bit correction 
 b and is a simple extension to the scheme described here 
for a given virtual node ni at level i each child mib contains the 
subset of a child s reduction mib that contains tuples that match 
ni s node id in i bits and whose up aggregation function attribute is 
at least i these local copies make it easy for a node to recompute 
a level-i aggregate value when one child s input changes nodes 
maintain their child mibs in stable storage and use a simplified 
version of the bayou log exchange protocol sans conflict detection 
and resolution for synchronization after disconnections 
virtual node ni at level i maintains a reduction mib of tuples 
with a tuple for each key present in any child mib containing the 
attribute type attribute name and output of the attribute type s 
aggregate functions applied to the children s tuples 
a virtual node ni at level i also maintains an ancestor mib to 
store the tuples containing attribute key and a list of aggregate 
values at different levels scattered down from ancestors note that the 
 
list for a key might contain multiple aggregate values for a same 
level but aggregated at different nodes see figure so the 
aggregate values are tagged not only with level information but are 
also tagged with id of the node that performed the aggregation 
level- differs slightly from other levels each level- leaf node 
maintains a local mib rather than maintaining child mibs and a 
reduction mib this local mib stores information about the local 
node s state inserted by local applications via update calls we 
envision various sensor programs and applications insert data into 
local mib for example one program might monitor local 
configuration and perform updates with information such as total memory 
free memory etc a distributed file system might perform update 
for each file stored on the local node 
along with these mibs a virtual node maintains two other 
tables an aggregation function table and an outstanding probes 
table an aggregation function table contains the aggregation 
function and installation arguments see table associated with an 
attribute type or an attribute type and name each aggregate 
function is installed on all nodes in a domain s subtree so the aggregate 
function table can be thought of as a special case of the ancestor 
mib with domain functions always installed up to a root within a 
specified domain and down to all nodes within the domain the 
outstanding probes table maintains temporary information 
regarding in-progress probes 
given these data structures it is simple to support the three api 
functions described in section 
install the install operation see table installs on a domain an 
aggregation function that acts on a specified attribute type 
execution of an install operation for function aggrfunc on attribute type 
attrtype proceeds in two phases first the install request is passed 
up the adht tree with the attribute key attrtype null until it 
reaches the root for that key within the specified domain then the 
request is flooded down the tree and installed on all intermediate 
and leaf nodes 
update when a level i virtual node receives an update for an 
attribute from a child below it first recomputes the level-i 
aggregate value for the specified key stores that value in its reduction 
mib and then subject to the function s up and domain parameters 
passes the updated value to the appropriate parent based on the 
attribute key also the level-i i ≥ virtual node sends the updated 
level-i aggregate to all its children if the function s down parameter 
exceeds zero upon receipt of a level-i aggregate from a parent 
a level k virtual node stores the value in its ancestor mib and if 
k ≥ i−down forwards this aggregate to its children 
probe a probe collects and returns the aggregate value for a 
specified attribute key for a specified level of the tree as figure 
illustrates the system satisfies a probe for a level-i aggregate value 
using a four-phase protocol that may be short-circuited when 
updates have previously propagated either results or partial results up 
or down the tree in phase the route probe phase the system 
routes the probe up the attribute key s tree to either the root of the 
level-i subtree or to a node that stores the requested value in its 
ancestor mib in the former case the system proceeds to phase and 
in the latter it skips to phase in phase the probe scatter phase 
each node that receives a probe request sends it to all of its children 
unless the node s reduction mib already has a value that matches 
the probe s attribute key in which case the node initiates phase 
on behalf of its subtree in phase the probe aggregation phase 
when a node receives values for the specified key from each of its 
children it executes the aggregate function on these values and 
either a forwards the result to its parent if its level is less than i 
or b initiates phase if it is at level i finally in phase the 
aggregate routing phase the aggregate value is routed down to the 
node that requested it note that in the extreme case of a function 
installed with up down a level-i probe can touch all nodes 
in a level-i subtree while in the opposite extreme case of a 
function installed with up down all probe is a completely local 
operation at a leaf 
for probes that include phases probe scatter and probe 
aggregation an issue is how to decide when a node should stop 
waiting for its children to respond and send up its current 
aggregate value a node stops waiting for its children when one of three 
conditions occurs all children have responded the adht 
layer signals one or more reconfiguration events that mark all 
children that have not yet responded as unreachable or a watchdog 
timer for the request fires the last case accounts for nodes that 
participate in the adht protocol but that fail at the aml level 
at a virtual node continuous probes are handled similarly as 
one-shot probes except that such probes are stored in the 
outstanding probe table for a time period of exptime specified in the probe 
thus each update for an attribute triggers re-evaluation of 
continuous probes for that attribute 
we implement a lease-based mechanism for dynamic adaptation 
a level-l virtual node for an attribute can issue the lease for 
levell aggregate to a parent or a child only if up is greater than l or it 
has leases from all its children a virtual node at level l can issue 
the lease for level-k aggregate for k l to a child only if down≥ 
k −l or if it has the lease for that aggregate from its parent now a 
probe for level-k aggregate can be answered by level-l virtual node 
if it has a valid lease irrespective of the up and down values we 
are currently designing different policies to decide when to issue a 
lease and when to revoke a lease and are also evaluating them with 
the above mechanism 
our current prototype does not implement access control on 
install update and probe operations but we plan to implement 
astrolabe s certificate-based restrictions also our current 
prototype does not restrict the resource consumption in executing the 
aggregation functions but  techniques from research on resource 
management in server systems and operating systems can be 
applied here 
 robustness 
in large scale systems reconfigurations are common our two 
main principles for robustness are to guarantee i read availability 
- probes complete in finite time and ii eventual consistency - 
updates by a live node will be visible to probes by connected nodes 
in finite time during reconfigurations a probe might return a stale 
value for two reasons first reconfigurations lead to incorrectness 
in the previous aggregate values second the nodes needed for 
aggregation to answer the probe become unreachable our 
system also provides two hooks that applications can use for improved 
end-to-end robustness in the presence of reconfigurations 
ondemand re-aggregation and application controlled replication 
our system handles reconfigurations at two levels - adaptation at 
the adht layer to ensure connectivity and adaptation at the aml 
layer to ensure access to the data in sdims 
 adht adaptation 
our adht layer adaptation algorithm is same as pastry s 
adaptation algorithm - the leaf sets are repaired as soon as a 
reconfiguration is detected and the routing table is repaired lazily note 
that maintaining extra leaf sets does not degrade the fault-tolerance 
property of the original pastry indeed it enhances the resilience 
of adhts to failures by providing additional routing links due 
to redundancy in the leaf sets and the routing table updates can be 
routed towards their root nodes successfully even during failures 
 
reconfig 
reconfig 
notices 
dht 
partial 
dht 
complete 
dht 
ends 
lazy 
time 
data 
 starts 
lazy 
data 
starts 
lazy 
data 
starts 
lazy 
data 
repairrepair 
reaggr reaggr reaggr reaggr 
happens 
figure default lazy data re-aggregation time line 
also note that the administrative isolation property satisfied by our 
adht algorithm ensures that the reconfigurations in a level i 
domain do not affect the probes for level i in a sibling domain 
 aml adaptation 
broadly we use two types of strategies for aml adaptation in 
the face of reconfigurations replication in time as a 
fundamental baseline strategy and replication in space as an 
additional performance optimization that falls back on replication in 
time when the system runs out of replicas we provide two 
mechanisms for replication in time first lazy re-aggregation propagates 
already received updates to new children or new parents in a lazy 
fashion over time second applications can reduce the probability 
of probe response staleness during such repairs through our flexible 
api with appropriate setting of the down parameter 
lazy re-aggregation the dht layer informs the aml layer 
about reconfigurations in the network using the following three 
function calls - newparent failedchild and newchild on 
newparent parent prefix all probes in the outstanding-probes table 
corresponding to prefix are re-evaluated if parent is not null then 
aggregation functions and already existing data are lazily transferred 
in the background any new updates installs and probes for this 
prefix are sent to the parent immediately on failedchild child 
prefix the aml layer marks the child as inactive and any outstanding 
probes that are waiting for data from this child are re-evaluated 
on newchild child prefix the aml layer creates space in its data 
structures for this child 
figure shows the time line for the default lazy re-aggregation 
upon reconfiguration probes initiated between points and and 
that are affected by reconfigurations are reevaluated by aml upon 
detecting the reconfiguration probes that complete or start between 
points and may return stale answers 
on-demand re-aggregation the default lazy aggregation 
scheme lazily propagates the old updates in the system 
additionally using up and down knobs in the probe api applications can 
force on-demand fast re-aggregation of updates to avoid staleness 
in the face of reconfigurations in particular if an application 
detects or suspects an answer as stale then it can re-issue the probe 
increasing the up and down parameters to force the refreshing of 
the cached data note that this strategy will be useful only after the 
dht adaptation is completed point on the time line in figure 
replication in space replication in space is more 
challenging in our system than in a dht file location application because 
replication in space can be achieved easily in the latter by just 
replicating the root node s contents in our system however all internal 
nodes have to be replicated along with the root 
in our system applications control replication in space using up 
and down knobs in the install api with large up and down values 
aggregates at the intermediate virtual nodes are propagated to more 
nodes in the system by reducing the number of nodes that have to 
be accessed to answer a probe applications can reduce the 
probability of incorrect results occurring due to the failure of nodes that 
do not contribute to the aggregate for example in a file location 
application using a non-zero positive down parameter ensures that 
a file s global aggregate is replicated on nodes other than the root 
 
 
 
 
 
 
 
avg numberofmessagesperoperation 
read to write ratio 
update-all 
up all down 
up all down 
update-up 
update-local 
up down 
up down 
figure flexibility of our approach with different up and 
down values in a network of nodes for different 
readwrite ratios 
probes for the file location can then be answered without accessing 
the root hence they are not affected by the failure of the root 
however note that this technique is not appropriate in some cases an 
aggregated value in file location system is valid as long as the node 
hosting the file is active irrespective of the status of other nodes 
in the system whereas an application that counts the number of 
machines in a system may receive incorrect results irrespective of 
the replication if reconfigurations are only transient like a node 
temporarily not responding due to a burst of load the replicated 
aggregate closely or correctly resembles the current state 
 evaluation 
we have implemented a prototype of sdims in java using the 
freepastry framework and performed large-scale simulation 
experiments and micro-benchmark experiments on two real 
networks machines in the department and machines on the 
planetlab testbed in all experiments we use static up and 
down values and turn off dynamic adaptation our evaluation 
supports four main conclusions first flexible api provides different 
propagation strategies that minimize communication resources at 
different read-to-write ratios for example in our simulation we 
observe update-local to be efficient for read-to-write ratios 
below update-up around and update-all above 
second our system is scalable with respect to both nodes and 
attributes in particular we find that the maximum node stress in 
our system is an order lower than observed with an update-all 
gossiping approach third in contrast to unmodified pastry which 
violates path convergence property in upto cases our system 
conforms to the property fourth the system is robust to 
reconfigurations and adapts to failures with in a few seconds 
 simulation experiments 
flexibility and scalability a major innovation of our system 
is its ability to provide flexible computation and propagation of 
aggregates in figure we demonstrate the flexibility exposed by the 
aggregation api explained in section we simulate a system with 
 nodes arranged in a domain hierarchy with branching factor 
 bf of and install several attributes with different up and down 
parameters we plot the average number of messages per operation 
incurred for a wide range of read-to-write ratios of the operations 
for different attributes simulations with other sizes of networks 
with different branching factors reveal similar results this graph 
clearly demonstrates the benefit of supporting a wide range of 
computation and propagation strategies although having a small up 
 
 
 
 
 
 
 
 e 
 e 
 
maximumnodestress 
number of attributes installed 
gossip 
gossip 
gossip 
dht 
dht 
dht 
figure max node stress for a gossiping approach vs adht 
based approach for different number of nodes with increasing 
number of sparse attributes 
value is efficient for attributes with low read-to-write ratios write 
dominated applications the probe latency when reads do occur 
may be high since the probe needs to aggregate the data from all 
the nodes that did not send their aggregate up conversely 
applications that wish to improve probe overheads or latencies can increase 
their up and down propagation at a potential cost of increase in 
write overheads 
compared to an existing update-all single aggregation tree 
approach scalability in sdims comes from leveraging dhts 
to form multiple aggregation trees that split the load across nodes 
and flexible propagation that avoids propagation of all updates 
to all nodes figure demonstrates the sdims s scalability with 
nodes and attributes for this experiment we build a simulator to 
simulate both astrolabe a gossiping update-all approach 
and our system for an increasing number of sparse attributes each 
attribute corresponds to the membership in a multicast session with 
a small number of participants for this experiment the session 
size is set to the branching factor is set to the propagation 
mode for sdims is update-up and the participant nodes perform 
continuous probes for the global aggregate value we plot the 
maximum node stress in terms of messages observed in both schemes 
for different sized networks with increasing number of sessions 
when the participant of each session performs an update operation 
clearly the dht based scheme is more scalable with respect to 
attributes than an update-all gossiping scheme observe that at some 
constant number of attributes as the number of nodes increase in 
the system the maximum node stress increases in the gossiping 
approach while it decreases in our approach as the load of 
aggregation is spread across more nodes simulations with other session 
sizes and yield similar results 
administrative hierarchy and robustness although the 
routing protocol of adht might lead to an increased number of 
hops to reach the root for a key as compared to original pastry the 
algorithm conforms to the path convergence and locality properties 
and thus provides administrative isolation property in figure 
we quantify the increased path length by comparisons with 
unmodified pastry for different sized networks with different branching 
factors of the domain hierarchy tree to quantify the path 
convergence property we perform simulations with a large number of 
probe pairs - each pair probing for a random key starting from two 
randomly chosen nodes in figure we plot the percentage of 
probe pairs for unmodified pastry that do not conform to the path 
convergence property when the branching factor is low the 
domain hierarchy tree is deeper resulting in a large difference between 
 
 
 
 
 
 
 
 
 
pathlength 
number of nodes 
adht bf 
adht bf 
adht bf 
pastry bf 
figure average path length to root in pastry versus adht 
for different branching factors note that all lines 
corresponding to pastry overlap 
 
 
 
 
 
 
 
 
 
 
percentageofviolations 
number of nodes 
bf 
bf 
bf 
figure percentage of probe pairs whose paths to the root 
did not conform to the path convergence property with pastry 
u 
pdate-all 
u 
pdate-u 
p 
u 
pdate-local 
 
 
 
 
 
latency inms 
average latency 
u 
pdate-all 
u 
pdate-u 
p 
u 
pdate-local 
 
 
 
 
latency inms average latency 
 a b 
figure latency of probes for aggregate at global root level 
with three different modes of aggregate propagation on a 
department machines and b planetlab machines 
pastry and adht in the average path length but it is at these small 
domain sizes that the path convergence fails more often with the 
original pastry 
 testbed experiments 
we run our prototype on department machines some 
machines ran multiple node instances so this configuration has a 
total of sdims nodes and also on machines of the 
planetlab testbed we measure the performance of our system with 
two micro-benchmarks in the first micro-benchmark we install 
three aggregation functions of types update-local update-up and 
update-all perform update operation on all nodes for all three 
aggregation functions and measure the latencies incurred by probes 
for the global aggregate from all nodes in the system figure 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
latency inms 
valuesobserved 
time in sec 
values 
latency 
node killed 
figure micro-benchmark on department network showing 
the behavior of the probes from a single node when failures are 
happening at some other nodes all nodes assign a value of 
 to the attribute 
 
 
 
 
 
 
 
 
 
 
 
latency inms 
valuesobserved 
time in sec 
values 
latency 
node killed 
figure probe performance during failures on machines 
of planetlab testbed 
shows the observed latencies for both testbeds notice that the 
latency in update-local is high compared to the update-up policy 
this is because latency in update-local is affected by the presence 
of even a single slow machine or a single machine with a high 
latency network connection 
in the second benchmark we examine robustness we install one 
aggregation function of type update-up that performs sum 
operation on an integer valued attribute each node updates the attribute 
with the value then we monitor the latencies and results 
returned on the probe operation for global aggregate on one chosen 
node while we kill some nodes after every few probes figure 
shows the results on the departmental testbed due to the nature 
of the testbed machines in a department there is little change in 
the latencies even in the face of reconfigurations in figure we 
present the results of the experiment on planetlab testbed the 
root node of the aggregation tree is terminated after about 
seconds there is a x increase in the latencies after the death of the 
initial root node as a more distant node becomes the root node after 
repairs in both experiments the values returned on probes start 
reflecting the correct situation within a short time after the failures 
from both the testbed benchmark experiments and the 
simulation experiments on flexibility and scalability we conclude that 
the flexibility provided by sdims allows applications to tradeoff 
read-write overheads figure read latency and sensitivity to 
slow machines figure a good default aggregation 
strategy is update-up which has moderate overheads on both reads and 
writes figure has moderate read latencies figure and is 
scalable with respect to both nodes and attributes figure and 
 small domain sizes are the cases where dht algorithms fail to 
provide path convergence more often and sdims ensures path 
convergence with only a moderate increase in path lengths figure 
 applications 
sdims is designed as a general distributed monitoring and 
control infrastructure for a broad range of applications above we 
discuss some simple microbenchmarks including a multicast 
membership service and a calculate-sum function van renesse et al 
provide detailed examples of how such a service can be used for a 
peer-to-peer caching directory a data-diffusion service a 
publishsubscribe system barrier synchronization and voting 
additionally we have initial experience using sdims to construct two 
significant applications the control plane for a large-scale distributed 
file system and a network monitor for identifying heavy 
hitters that consume excess resources 
distributed file system control the practi partial 
replication arbitrary consistency topology independence replication 
system provides a set of mechanisms for data replication over which 
arbitrary control policies can be layered we use sdims to provide 
several key functions in order to create a file system over the 
lowlevel practi mechanisms 
first nodes use sdims as a directory to handle read misses 
when a node n receives an object o it updates the readdir o 
attribute with the value n when n discards o from its local store 
it resets readdir o to null at each virtual node the readdir 
aggregation function simply selects a random non-null child value 
 if any and we use the update-up policy for propagating updates 
finally to locate a nearby copy of an object o a node n issues a 
series of probe requests for the readdir o attribute starting with 
level and increasing the level value with each repeated probe 
request until a non-null node id n is returned n then sends a 
demand read request to n and n sends the data if it has it 
conversely if n does not have a copy of o it sends a nack to n 
and n issues a retry probe with the down parameter set to a value 
larger than used in the previous probe in order to force on-demand 
re-aggregation which will yield a fresher value for the retry 
second nodes subscribe to invalidations and updates to interest 
sets of files and nodes use sdims to set up and maintain 
perinterest-set network-topology-sensitive spanning trees for 
propagating this information to subscribe to invalidations for interest 
set i a node n first updates the inval i attribute with its 
identity n and the aggregation function at each virtual node selects 
one non-null child value finally n probes increasing levels of the 
the inval i attribute until it finds the first node n n n then 
uses n as its parent in the spanning tree n also issues a 
continuous probe for this attribute at this level so that it is notified of any 
change to its spanning tree parent spanning trees for streams of 
pushed updates are maintained in a similar manner 
in the future we plan to use sdims for at least two additional 
services within this replication system first we plan to use sdims 
to track the read and write rates to different objects prefetch 
algorithms will use this information to prioritize replication 
second we plan to track the ranges of invalidation sequence 
numbers seen by each node for each interest set in order to augment 
the spanning trees described above with additional hole filling to 
allow nodes to locate specific invalidations they have missed 
overall our initial experience with using sdims for the 
practii replication system suggests that the general aggregation 
interface provided by sdims simplifies the construction of 
distributed applications-given the low-level practi mechanisms 
 
we were able to construct a basic file system that uses sdims for 
several distinct control tasks in under two weeks and the weak 
consistency guarantees provided by sdims meet the requirements 
of this application-each node s controller effectively treats 
information from sdims as hints and if a contacted node does not have 
the needed data the controller retries using sdims on-demand 
reaggregation to obtain a fresher hint 
distributed heavy hitter problem the goal of the heavy hitter 
problem is to identify network sources destinations or protocols 
that account for significant or unusual amounts of traffic as noted 
by estan et al this information is useful for a variety of 
applications such as intrusion detection e g port scanning denial of 
service detection worm detection and tracking fair network 
allocation and network maintenance significant work has been done 
on developing high-performance stream-processing algorithms for 
identifying heavy hitters at one router but this is just a first step 
ideally these applications would like not just one router s views of 
the heavy hitters but an aggregate view 
we use sdims to allow local information about heavy hitters 
to be pooled into a view of global heavy hitters for each 
destination ip address ipx a node updates the attribute destbw ipx 
with the number of bytes sent to ipx in the last time window the 
aggregation function for attribute type destbw is installed with the 
update-up strategy and simply adds the values from child nodes 
nodes perform continuous probe for global aggregate of the 
attribute and raise an alarm when the global aggregate value goes 
above a specified limit note that only nodes sending data to a 
particular ip address perform probes for the corresponding attribute 
also note that techniques from can be extended to hierarchical 
case to tradeoff precision for communication bandwidth 
 related work 
the aggregation abstraction we use in our work is heavily 
influenced by the astrolabe project astrolabe adopts a 
propagateall and unstructured gossiping techniques to attain robustness 
however any gossiping scheme requires aggressive replication of 
the aggregates while such aggressive replication is efficient for 
read-dominated attributes it incurs high message cost for attributes 
with a small read-to-write ratio our approach provides a 
flexible api for applications to set propagation rules according to their 
read-to-write ratios other closely related projects include 
willow cone dasis and somo willow dasis 
and somo build a single tree for aggregation cone builds a tree 
per attribute and requires a total order on the attribute values 
several academic and commercial distributed 
monitoring systems have been designed to monitor the status of 
large networked systems some of them are centralized where all 
the monitoring data is collected and analyzed at a central host 
ganglia uses a hierarchical system where the attributes are 
replicated within clusters using multicast and then cluster 
aggregates are further aggregated along a single tree sophia is 
a distributed monitoring system designed with a declarative logic 
programming model where the location of query execution is both 
explicit in the language and can be calculated during evaluation 
this research is complementary to our work tag collects 
information from a large number of sensors along a single tree 
the observation that dhts internally provide a scalable forest 
of reduction trees is not new plaxton et al s original paper 
describes not a dht but a system for hierarchically aggregating and 
querying object location data in order to route requests to nearby 
copies of objects many systems-building upon both plaxton s 
bit-correcting strategy and upon other strategies 
 -have chosen to hide this power and export a simple and 
general distributed hash table abstraction as a useful building block for 
a broad range of distributed applications some of these systems 
internally make use of the reduction forest not only for routing but 
also for caching but for simplicity these systems do not 
generally export this powerful functionality in their external interface 
our goal is to develop and expose the internal reduction forest of 
dhts as a similarly general and useful abstraction 
although object location is a predominant target application for 
dhts several other applications like multicast and 
dns are also built using dhts all these systems implicitly 
perform aggregation on some attribute and each one of them must 
be designed to handle any reconfigurations in the underlying dht 
with the aggregation abstraction provided by our system designing 
and building of such applications becomes easier 
internal dht trees typically do not satisfy domain locality 
properties required in our system castro et al and gummadi et 
al point out the importance of path convergence from the 
perspective of achieving efficiency and investigate the performance of 
pastry and other dht algorithms respectively skipnet 
provides domain restricted routing where a key search is limited to the 
specified domain this interface can be used to ensure path 
convergence by searching in the lowest domain and moving up to the next 
domain when the search reaches the root in the current domain 
although this strategy guarantees path convergence it loses the 
aggregation tree abstraction property of dhts as the domain constrained 
routing might touch a node more than once as it searches forward 
and then backward to stay within a domain 
 conclusions 
this paper presents a scalable distributed information 
management system sdims that aggregates information in large-scale 
networked systems and that can serve as a basic building block 
for a broad range of applications for large scale systems 
hierarchical aggregation is a fundamental abstraction for scalability 
we build our system by extending ideas from astrolabe and dhts 
to achieve i scalability with respect to both nodes and attributes 
through a new aggregation abstraction that helps leverage dht s 
internal trees for aggregation ii flexibility through a simple api 
that lets applications control propagation of reads and writes iii 
administrative isolation through simple augmentations of current 
dht algorithms and iv robustness to node and network 
reconfigurations through lazy reaggregation on-demand reaggregation 
and tunable spatial replication 
acknowlegements 
we are grateful to j c browne robert van renessee amin 
vahdat jay lepreau and the anonymous reviewers for their helpful 
comments on this work 
 references 
 k albrecht r arnold m gahwiler and r wattenhofer 
join and leave in peer-to-peer systems the dasis 
approach technical report cs eth zurich 
 g back w h hsieh and j lepreau processes in kaffeos 
isolation resource management and sharing in java in 
proc osdi oct 
 g banga p druschel and j mogul resource containers 
a new facility for resource management in server 
systems in osdi feb 
 r bhagwan p mahadevan g varghese and g m voelker 
cone a distributed heap-based approach to resource 
selection technical report cs - ucsd 
 
 k p birman the surprising power of epidemic 
communication in proceedings of fudico 
 b bloom space time tradeoffs in hash coding with 
allowable errors comm of the acm - 
 m castro p druschel y c hu and a rowstron 
exploiting network proximity in peer-to-peer overlay 
networks technical report msr-tr- - msr 
 m castro p druschel a -m kermarrec a nandi 
a rowstron and a singh splitstream high-bandwidth 
multicast in a cooperative environment in sosp 
 m castro p druschel a -m kermarrec and a rowstron 
scribe a large-scale and decentralised application-level 
multicast infrastructure ieee jsac special issue on 
network support for multicast communications 
 j challenger p dantzig and a iyengar a scalable and 
highly available system for serving dynamic data at 
frequently accessed web sites in in proceedings of 
acm ieee supercomputing sc nov 
 r cox a muthitacharoen and r t morris serving dns 
using a peer-to-peer lookup service in iptps 
 m dahlin l gao a nayate a venkataramani 
p yalagandula and j zheng practi replication for 
large-scale systems technical report tr- - the 
university of texas at austin 
 c estan g varghese and m fisk bitmap algorithms for 
counting active flows on high speed links in internet 
measurement conference 
 y fu j chase b chun s schwab and a vahdat 
sharp an architecture for secure resource peering in 
proc sosp oct 
 ganglia distributed monitoring and execution system 
http ganglia sourceforge net 
 s gribble a halevy z ives m rodrig and d suciu 
what can peer-to-peer do for databases and vice versa in 
proceedings of the webdb 
 k gummadi r gummadi s d gribble s ratnasamy 
s shenker and i stoica the impact of dht routing 
geometry on resilience and proximity in sigcomm 
 n j a harvey m b jones s saroiu m theimer and 
a wolman skipnet a scalable overlay network with 
practical locality properties in usits march 
 r huebsch j m hellerstein n lanham b t loo 
s shenker and i stoica querying the internet with pier 
in proceedings of the vldb conference may 
 c intanagonwiwat r govindan and d estrin directed 
diffusion a scalable and robust communication paradigm for 
sensor networks in mobicom 
 s r madden m j franklin j m hellerstein and 
w hong tag a tiny aggregation service for ad-hoc 
sensor networks in osdi 
 d malkhi dynamic lookup networks in fudico 
 m l massie b n chun and d e culler the ganglia 
distributed monitoring system design implementation and 
experience in submission 
 p maymounkov and d mazieres kademlia a peer-to-peer 
information system based on the xor metric in 
proceesings of the iptps march 
 c olston and j widom offering a precision-performance 
tradeoff for aggregation queries over replicated data in 
vldb pages - sept 
 k petersen m spreitzer d terry m theimer and 
a demers flexible update propagation for weakly 
consistent replication in proc sosp oct 
 planetlab http www planet-lab org 
 c g plaxton r rajaraman and a w richa accessing 
nearby copies of replicated objects in a distributed 
environment in acm spaa 
 s ratnasamy p francis m handley r karp and 
s shenker a scalable content addressable network in 
proceedings of acm sigcomm 
 s ratnasamy s shenker and i stoica routing algorithms 
for dhts some open questions in iptps march 
 t roscoe r mortier p jardetzky and s hand infospect 
using a logic language for system health monitoring in 
distributed systems in proceedings of the sigops 
european workshop 
 a rowstron and p druschel pastry scalable distributed 
object location and routing for large-scale peer-to-peer 
systems in middleware 
 s ratnasamy m handley r karp and s shenker 
application-level multicast using content-addressable 
networks in proceedings of the ngc november 
 w stallings snmp snmpv and cmip addison-wesley 
 
 i stoica r morris d karger f kaashoek and 
h balakrishnan chord a scalable peer-to-peer lookup 
service for internet applications in acm sigcomm 
 s zhuang b zhao a joseph r katz and j kubiatowicz 
bayeux an architecture for scalable and fault-tolerant 
wide-area data dissemination in nossdav 
 ibm tivoli monitoring 
www ibm com software tivoli products monitor 
 r vanrenesse k p birman and w vogels astrolabe a 
robust and scalable technology for distributed system 
monitoring management and data mining tocs 
 r vanrenesse and a bozdog willow dht aggregation 
and publish subscribe in one protocol in iptps 
 a venkataramani p weidmann and m dahlin bandwidth 
constrained placement in a wan in podc aug 
 a venkataramani p yalagandula r kokku s sharif and 
m dahlin potential costs and benefits of long-term 
prefetching for content-distribution elsevier computer 
communications - mar 
 m wawrzoniak l peterson and t roscoe sophia an 
information plane for networked systems in hotnets-ii 
 
 r wolski n spring and j hayes the network weather 
service a distributed resource performance forecasting 
service for metacomputing journal of future generation 
computing systems - - oct 
 p yalagandula and m dahlin sdims a scalable 
distributed information management system technical 
report tr- - dept of computer sciences ut austin 
sep 
 z zhang s -m shi and j zhu somo self-organized 
metadata overlay for resource management in p p dht in 
iptps 
 b y zhao j d kubiatowicz and a d joseph tapestry 
an infrastructure for fault-tolerant wide-area location and 
routing technical report ucb csd- - uc 
berkeley apr 
 
globally synchronized dead-reckoning with local lag for 
continuous distributed multiplayer games 
yi zhang 
 ling chen 
 gencai chen 
 college of computer science zhejiang university hangzhou p r china 
 school of computer science and it the university of nottingham nottingham ng bb uk 
{m zhangyi lingchen chengc} cs zju edu cn 
abstract 
dead-reckoning dr is an effective method to maintain 
consistency for continuous distributed multiplayer games 
 cdmg since dr can filter most unnecessary state updates and 
improve the scalability of a system it is widely used in 
commercial cdmg however dr cannot maintain high 
consistency and this constrains its application in highly 
interactive games with the help of global synchronization dr 
can achieve higher consistency but it still cannot eliminate before 
inconsistency in this paper a method named globally 
synchronized dr with local lag gs-dr-ll which combines 
local lag and globally synchronized dr gs-dr is presented 
performance evaluation shows that gs-dr-ll can effectively 
decrease before inconsistency and the effects increase with the 
lag 
categories and subject descriptors 
c computer-communication networks distributed 
systems - distributed applications 
general terms 
algorithms performance experimentation 
 introduction 
nowadays many distributed multiplayer games adopt replicated 
architectures in such games the states of entities are changed not 
only by the operations of players but also by the passing of time 
 these games are referred to as continuous distributed 
multiplayer games cdmg like other distributed applications 
cdmg also suffer from the consistency problem caused by 
network transmission delay although new network techniques 
 e g qos can reduce or at least bound the delay they can not 
completely eliminate it as there exists the physical speed 
limitation of light for instance ms is needed for light to 
propagate from europe to australia there are many studies 
about the effects of network transmission delay in different 
applications in replication based games network 
transmission delay makes the states of local and remote sites to be 
inconsistent which can cause serious problems such as reducing 
the fairness of a game and leading to paradoxical situations etc in 
order to maintain consistency for distributed systems many 
different approaches have been proposed among which local lag 
and dead-reckoning dr are two representative approaches 
mauve et al proposed local lag to maintain high consistency 
for replicated continuous applications it synchronizes the 
physical clocks of all sites in a system after an operation is 
issued at local site it delays the execution of the operation for a 
short time during this short time period the operation is 
transmitted to remote sites and all sites try to execute the 
operation at a same physical time in order to tackle the 
inconsistency caused by exceptional network transmission delay 
a time warp based mechanism is proposed to repair the state 
local lag can achieve significant high consistency but it is based 
on operation transmission which forwards every operation on a 
shared entity to remote sites since operation transmission 
mechanism requests that all operations should be transmitted in a 
reliable way message filtering is difficult to be deployed and the 
scalability of a system is limited 
dr is based on state transmission mechanism in addition to the 
high fidelity model that maintains the accurate states of its own 
entities each site also has a dr model that estimates the states of 
all entities including its own entities after each update of its 
own entities a site compares the accurate state with the estimated 
one if the difference exceeds a pre-defined threshold a state 
update would be transmitted to all sites and all dr models would 
be corrected through state estimation dr can not only maintain 
consistency but also decrease the number of transmitted state 
updates compared with aforementioned local lag dr cannot 
maintain high consistency due to network transmission delay 
when a remote site receives a state update of an entity the state of 
the entity might have changed at the site sending the state update 
in order to make dr maintain high consistency aggarwal et al 
proposed globally synchronized dr gs-dr which 
synchronizes the physical clocks of all sites in a system and adds 
time stamps to transmitted state updates detailed description of 
gs-dr can be found in section 
when a state update is available gs-dr immediately updates the 
state of local site and then transmits the state update to remote 
sites which causes the states of local site and remote sites to be 
inconsistent in the transmission procedure thus with the 
synchronization of physical clocks gs-dr can eliminate after 
inconsistency but it cannot tackle before inconsistency in this 
paper we propose a new method named globally synchronized 
dr with local lag gs-dr-ll which combines local lag and 
gs-dr by delaying the update to local site gs-dr-ll can 
achieve higher consistency than gs-dr the rest of this paper is 
organized as follows section gives the definition of consistency 
and corresponding metrics the cause of the inconsistency of dr 
is analyzed in section section describes how gs-dr-ll 
works performance evaluation is presented in section section 
 concludes the paper 
 consistency definitions and 
metrics 
the consistency of replicated applications has already been well 
defined in discrete domain but few related work 
has been done in continuous domain mauve et al have given a 
definition of consistency for replicated applications in continuous 
domain but the definition is based on operation transmission and 
it is difficult for the definition to describe state transmission based 
methods e g dr here we present an alternative definition of 
consistency in continuous domain which suits state transmission 
based methods well 
given two distinct sites i and j which have replicated a shared 
entity e at a given time t the states of e at sites i and j are si t 
and sj t 
definition the states of e at sites i and j are consistent at 
time t iff 
de i j t si t - sj t 
definition the states of e at sites i and j are consistent 
between time t and t t t iff 
de i j t t dt t s t s 
t 
t 
ji 
in this paper formulas and are used to determine whether 
the states of shared entities are consistent between local and 
remote sites due to network transmission delay it is difficult to 
maintain the states of shared entities absolutely consistent 
corresponding metrics are needed to measure the consistency of 
shared entities between local and remote sites 
de i j t can be used as a metric to measure the degree of 
consistency at a certain time point if de i j t de i j t it 
can be stated that between sites i and j the consistency of the 
states of entity e at time point t is lower than that at time point t 
if de i j t de l k t it can be stated that at time point t the 
consistency of the states of entity e between sites i and j is lower 
than that between sites l and k 
similarly de i j t t can been used as a metric to measure the 
degree of consistency in a certain time period if de i j t t 
de i j t t and t - t t - t it can be stated that between 
sites i and j the consistency of the states of entity e between time 
points t and t is lower than that between time points t and t if 
de i j t t de l k t t it can be stated that between time 
points t and t the consistency of the states of entity e between 
sites i and j is lower than that between sites l and k 
in dr the states of entities are composed of the positions and 
orientations of entities and some prediction related parameters 
 e g the velocities of entities given two distinct sites i and j 
which have replicated a shared entity e at a given time point t the 
positions of e at sites i and j are xit yit zit and xjt yjt zjt de i j 
t and d i j t t could be calculated as 
de i j t zz yy xx jtit 
 
jtit 
 
jtit 
 
 
de i j t t 
 dt zz yy xx 
 t 
 t jtit 
 
jtit 
 
jtit 
 
 
in this paper formulas and are used as metrics to measure 
the consistency of shared entities between local and remote sites 
 inconsistency in dr 
the inconsistency in dr can be divided into two sections by the 
time point when a remote site receives a state update the 
inconsistency before a remote site receives a state update is 
referred to as before inconsistency and the inconsistency after a 
remote site receives a state update is referred to as after 
inconsistency before inconsistency and after inconsistency are 
similar with the terms before export error and after export error 
 
after inconsistency is caused by the lack of synchronization 
between the physical clocks of all sites in a system by employing 
physical clock synchronization gs-dr can accurately calculate 
the states of shared entities after receiving state updates and it 
can eliminate after inconsistency before inconsistency is caused 
by two reasons the first reason is the delay of sending state 
updates as local site does not send a state update unless the 
difference between accurate state and the estimated one is larger 
than a predefined threshold the second reason is network 
transmission delay as a shared entity can be synchronized only 
after remote sites receiving corresponding state update 
figure the paths of a shared entity by using gs-dr 
for example it is assumed that the velocity of a shared entity is 
the only parameter to predict the entity s position and current 
position of the entity can be calculated by its last position and 
current velocity to simplify the description it is also assumed 
that there are only two sites i and j in a game session site i acts as 
 the th workshop on network system support for games - netgames 
local site and site j acts as remote site and t is the time point the 
local site updates the state of the shared entity figure illustrates 
the paths of the shared entity at local site and remote site in x axis 
by using gs-dr at the beginning the positions of the shared 
entity are the same at sites i and j and the velocity of the shared 
entity is before time point t the paths of the shared entity at 
sites i and j in x coordinate are exactly the same at time point t 
the player at site i issues an operation which changes the velocity 
in x axis to v site i first periodically checks whether the 
difference between the accurate position of the shared entity and 
the estimated one in this case is larger than a predefined 
threshold at time point t site i finds that the difference is larger 
than the threshold and it sends a state update to site j the state 
update contains the position and velocity of the shared entity at 
time point t and time point t is also attached as a timestamp at 
time point t the state update reaches site j and the received state 
and the time deviation between time points t and t are used to 
calculate the current position of the shared entity then site j 
updates its replicated entity s position and velocity and the paths 
of the shared entity at sites i and j overlap again 
from figure it can be seen that the after inconsistency is and 
the before consistency is composed of two parts d and d d 
is de i j t t and it is caused by the state filtering mechanism 
of dr d is de i j t t and it is caused by network 
transmission delay 
 globally synchronized dr 
with local lag 
from the analysis in section it can be seen that gs-dr can 
eliminate after inconsistency but it cannot effectively tackle 
before inconsistency in order to decrease before inconsistency 
we propose gs-dr-ll which combines gs-dr with local lag 
and can effectively decrease before inconsistency 
in gs-dr-ll the state of a shared entity at a certain time point t 
is notated as s t pos par par   par n in which pos 
means the position of the entity and par to par n means the 
parameters to calculate the position of the entity in order to 
simplify the description of gs-dr-ll it is assumed that there are 
only one shared entity and one remote site 
at the beginning of a game session the states of the shared entity 
are the same at local and remote sites with the same position p 
and parameters pars pars represents all the parameters local 
site keeps three states the real state of the entity sreal the 
predicted state at remote site sp-remote and the latest state updated 
to remote site slate remote site keep only one state sremote which 
is the real state of the entity at remote site therefore at the 
beginning of a game session sreal sp-remote slate sremote t 
p pars in gs-dr-ll it is assumed that the physical clocks of 
all sites are synchronized with a deviation of less than ms 
 using ntp or gps clock furthermore it is necessary to make 
corrections to a physical clock in a way that does not result in 
decreasing the value of the clock for example by slowing down 
or halting the clock for a period of time additionally it is 
assumed that the game scene is updated at a fixed frequency and 
t stands for the time interval between two consecutive updates 
for example if the scene update frequency is hz t would be 
 ms n stands for the lag value used by local lag and t stands for 
current physical time 
after updating the scene local site waits for a constant amount of 
time t during this time period local site receives the operations 
of the player and stores them in a list l all operations in l are 
sorted by their issue time at the end of time period t local site 
executes all stored operations whose issue time is between t - t 
and t on slate to get the new slate and it also executes all stored 
operations whose issue time is between t - n t and t - n on 
sreal to get the new sreal additionally local site uses sp-remote and 
corresponding prediction methods to estimate the new sp-remote 
after new slate sreal and sp-remote are calculated local site 
compares whether the difference between the new slate and 
spremote exceeds the predefined threshold if yes local site sends 
new slate to remote site and sp-remote is updated with new slate note 
that the timestamp of the sent state update is t after that local 
site uses sreal to update local scene and deletes the operations 
whose issue time is less than t - n from l 
after updating the scene remote site waits for a constant amount 
of time t during this time period remote site stores received 
state update s in a list r all state updates in r are sorted by their 
timestamps at the end of time period t remote site checks 
whether r contains state updates whose timestamps are less than t 
- n note that t is current physical time and it increases during the 
transmission of state updates if yes it uses these state updates 
and corresponding prediction methods to calculate the new sremote 
else they use sremote and corresponding prediction methods to 
estimate the new sremote after that local site uses sremote to update 
local scene and deletes the sate updates whose timestamps are 
less than t - n from r 
from the above description it can been see that the main 
difference between gs-dr and gs-dr-ll is that gs-dr-ll 
uses the operations whose issue time is less than t - n to 
calculate sreal that means that the scene seen by local player is 
the results of the operations issued a period of time i e n ago 
meanwhile if the results of issued operations make the difference 
between slate and sp-remote exceed a predefined threshold 
corresponding state updates are sent to remote sites immediately 
the aforementioned is the basic mechanism of gs-dr-ll in the 
case with multiple shared entities and remote sites local site 
calculates slate sreal and sp-remote for different shared entities 
respectively if there are multiple slate need to be transmitted local 
site packets them in one state update and then send it to all remote 
sites 
figure illustrates the paths of a shared entity at local site and 
remote site while using gs-dr and gs-dr-ll all conditions 
are the same with the conditions used in the aforementioned 
example describing gs-dr compared with t t and n t i e 
the time interval between two consecutive updates is quite small 
and it is ignored in the following description 
at time point t the player at site i issues an operation which 
changes the velocity of the shared entity form to v by using 
gs-dr-ll the results of the operation are updated to local scene 
at time point t n however the operation is immediately used 
to calculate slate thus in spite of gs-dr or gs-dr-ll at time 
point t site i finds that the difference between accurate position 
and the estimated one is larger than the threshold and it sends a 
state update to site j at time point t the state update is received 
by remote site j assuming that the timestamp of the state update 
is less than t - n site j uses it to update local scene immediately 
the th workshop on network system support for games - netgames 
with gs-dr the time period of before inconsistency is t - t 
 t - t whereas it decreases to t - t - n t - t with the 
help of gs-dr-ll note that t - t is caused by network 
transmission delay and t - t is caused by the state filtering 
mechanism of dr if n is larger than t - t gs-dr-ll can 
eliminate the before inconsistency caused by network 
transmission delay but it cannot eliminate the before 
inconsistency caused by the state filtering mechanism of dr 
 unless the threshold is set to in highly interactive games 
which request high consistency and gs-dr-ll might be 
employed the results of operations are quite difficult to be 
estimated and a small threshold must be used thus in practice 
most before inconsistency is caused by network transmission 
delay and gs-dr-ll has the capability to eliminate such before 
inconsistency 
figure the paths of a shared entity by using gs-dr and 
gs-dr-ll 
to gs-dr-ll the selection of lag value n is very important and 
both network transmission delay and the effects of local lag on 
interaction should be considered according to the results of hci 
related researches humans cannot perceive the delay imposed on 
a system when it is smaller than a specific value and the specific 
value depends on both the system and the task for example in a 
graphical user interface a delay of approximately ms cannot 
be noticed for keyboard interaction and the threshold increases to 
 ms for mouse interaction and a delay of up to ms is 
uncritical for a car-racing game thus if network transmission 
delay is less than the specific value of a game system n can be set 
to the specific value else n can be set in terms of the effects of 
local lag on the interaction of a system in the case that a 
large n must be used some hci methods e g echo can be 
used to relieve the negative effects of the large lag in the case 
that n is larger than the network transmission delay gs-dr-ll 
can eliminate most before inconsistency traditional local lag 
requests that the lag value must be larger than typical network 
transmission delay otherwise state repairs would flood the system 
however gs-dr-ll allows n to be smaller than typical network 
transmission delay in this case the before inconsistency caused 
by network transmission delay still exists but it can be decreased 
 performance evaluation 
in order to evaluate gs-dr-ll and compare it with gs-dr in a 
real application we had implemented both two methods in a 
networked game named spaceship spaceship is a very simple 
networked computer game in which players can control their 
spaceships to accelerate decelerate turn and shoot spaceships 
controlled by remote players with laser beams if a spaceship is 
hit by a laser beam its life points decrease one if the life points 
of a spaceship decrease to the spaceship is removed from the 
game and the player controlling the spaceship loses the game 
in our practical implementation gs-dr-ll and gs-dr 
coexisted in the game system and the test bed was composed of 
two computers connected by m switched ethernet with one 
computer acted as local site and the other acted as remote site in 
order to simulate network transmission delay a specific module 
was developed to delay all packets transmitted between the two 
computers in terms of a predefined delay value 
the main purpose of performance evaluation is to study the 
effects of gs-dr-ll on decreasing before inconsistency in a 
particular game system under different thresholds lags and 
network transmission delays two different thresholds were used 
in the evaluation one is pixels deviation in position or 
degrees deviation in orientation and the other is pixels or 
degrees six different combinations of lag and network 
transmission delay were used in the evaluation and they could be 
divided into two categories in one category the lag was fixed at 
 ms and three different network transmission delays ms 
 ms and ms were used in the other category the 
network transmission delay was fixed at ms and three 
different lags ms ms and ms were used therefore 
the total number of settings used in the evaluation was × 
the procedure of performance evaluation was composed of three 
steps in the first step two participants were employed to play the 
game and the operation sequences were recorded based on the 
records a sub operation sequence which lasted about one minute 
and included different operations e g accelerate decelerate and 
turn was selected in the second step the physical clocks of the 
two computers were synchronized first under different settings 
and consistency maintenance approaches the selected sub 
operation sequence was played back on one computer and it 
drove the two spaceships one was local and the other was remote 
to move meanwhile the tracks of the spaceships on the two 
computers were recorded separately and they were called as a 
track couple since there are settings and consistency 
maintenance approaches the total number of recorded track 
couples was in the last step to each track couple the 
inconsistency between them was calculated and the unit of 
inconsistency was pixel since the physical clocks of the two 
computers were synchronized the calculation of inconsistency 
was quite simple the inconsistency at a particular time point was 
the distance between the positions of the two spaceships at that 
time point i e formula 
in order to show the results of inconsistency in a clear way only 
parts of the results which last about seconds are used in the 
following figures and the figures show almost the same parts of 
the results figures and show the results of inconsistency 
when the lag is fixed at ms and the network transmission 
delays are and ms it can been seen that 
inconsistency does exist but in most of the time it is 
additionally inconsistency increases with the network 
transmission delay but decreases with the threshold compared 
with gs-dr gs-dr-ll can decrease more inconsistency and it 
eliminates most inconsistency when the network transmission 
delay is ms and the threshold is pixels or degrees 
 the th workshop on network system support for games - netgames 
according to the prediction and state filtering mechanisms of dr 
inconsistency cannot be completely eliminated if the threshold is 
not with the definitions of before inconsistency and after 
inconsistency it can be indicated that gs-dr and gs-dr-ll 
both can eliminate after inconsistency and gs-dr-ll can 
effectively decrease before inconsistency it can be foreseen that 
with proper lag and threshold e g the lag is larger than the 
network transmission delay and the threshold is gs-dr-ll 
even can eliminate before inconsistency 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
figure inconsistency when the network transmission delay is ms and the lag is ms 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
 
 
 
 
 
 
time seconds 
inconsistency pixels gs-dr-ll gs-dr 
the threshold is pixels or degrees 
figure inconsistency when the network transmission delay is ms and the lag is ms 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
figure inconsistency when the network transmission delay is ms and the lag is ms 
figures and show the results of inconsistency when the 
network transmission delay is fixed at ms and the lag are 
 and ms it can be seen that with gs-dr-ll before 
inconsistency decreases with the lag in traditional local lag the 
lag must be set to a value larger than typical network transmission 
delay otherwise the state repairs would flood the system from 
the above results it can be seen that there does not exist any 
constraint on the selection of the lag with gs-dr-ll a system 
would work fine even if the lag is much smaller than the network 
transmission delay 
the th workshop on network system support for games - netgames 
from all above results it can be indicated that gs-dr and 
gsdr-ll both can eliminate after inconsistency and gs-dr-ll 
can effectively decrease before inconsistency and the effects 
increase with the lag 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
figure inconsistency when the network transmission delay is ms and the lag is ms 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
figure inconsistency when the network transmission delay is ms and the lag is ms 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
 
 
 
 
 
 
time seconds 
inconsistency pixels 
gs-dr-ll gs-dr 
the threshold is pixels or degrees 
figure inconsistency when the network transmission delay is ms and the lag is ms 
 conclusions 
compared with traditional dr gs-dr can eliminate after 
inconsistency through the synchronization of physical clocks but 
it cannot tackle before inconsistency which would significantly 
influence the usability and fairness of a game in this paper we 
proposed a method named gs-dr-ll which combines local lag 
and gs-dr to decrease before inconsistency through delaying 
updating the execution results of local operations to local scene 
performance evaluation indicates that gs-dr-ll can effectively 
decrease before inconsistency and the effects increase with the 
lag 
gs-dr-ll has significant implications to consistency 
maintenance approaches first gs-dr-ll shows that improved 
dr can not only eliminate after inconsistency but also decrease 
 the th workshop on network system support for games - netgames 
before inconsistency with proper lag and threshold it would even 
eliminate before inconsistency as a result the application of dr 
can be greatly broadened and it could be used in the systems 
which request high consistency e g highly interactive games 
second gs-dr-ll shows that by combining local lag and 
gsdr the constraint on selecting lag value is removed and a lag 
which is smaller than typical network transmission delay could 
be used as a result the application of local lag can be greatly 
broadened and it could be used in the systems which have large 
typical network transmission delay e g internet based games 
 references 
 mauve m vogel j hilt v and effelsberg w local-lag 
and timewarp providing consistency for replicated 
continuous applications ieee transactions on multimedia 
vol no - 
 li f w li l w and lau r w supporting continuous 
consistency in multiplayer online games in proc of acm 
multimedia - 
 pantel l and wolf l on the suitability of dead 
reckoning schemes for games in proc of netgames 
 - 
 alhalabi m o horiguchi s and kunifuji s an 
experimental study on the effects of network delay in 
cooperative shared haptic virtual environment computers 
and graphics vol no - 
 pantel l and wolf l c on the impact of delay on 
realtime multiplayer games in proc of nossdav 
 
 meehan m razzaque s whitton m c and brooks f p 
effect of latency on presence in stressful virtual 
environments in proc of ieee vr - 
 bernier y w latency compensation methods in 
client server in-game protocol design and optimization in 
proc of game developers conference 
 aggarwal s banavar h and khandelwal a accuracy in 
dead-reckoning based distributed multi-player games in 
proc of netgames - 
 raynal m and schiper a from causal consistency to 
sequential consistency in shared memory systems in proc 
of conference on foundations of software technology and 
theoretical computer science - 
 ahamad m burns j e hutto p w and neiger g causal 
memory in proc of international workshop on distributed 
algorithms - 
 herlihy m and wing j linearizability a correctness 
condition for concurrent objects acm transactions on 
programming languages and systems vol no 
 - 
 misra j axioms for memory access in asynchronous 
hardware systems acm transactions on programming 
languages and systems vol no - 
 dabrowski j r and munson e v is milliseconds too 
fast in proc of sigchi conference on human factors in 
computing systems - 
 chen h chen l and chen g c effects of local-lag 
mechanism on cooperation performance in a desktop cve 
system journal of computer science and technology vol 
 no - 
 chen l chen h and chen g c echo a method to 
improve the interaction quality of cves in proc of ieee 
vr - 
the th workshop on network system support for games - netgames 
guess gossiping updates for efficient spectrum sensing 
nabeel ahmed 
university of waterloo 
david r cheriton school of 
computer science 
n ahmed uwaterloo ca 
david hadaller 
university of waterloo 
david r cheriton school of 
computer science 
dthadaller uwaterloo ca 
srinivasan keshav 
university of waterloo 
david r cheriton school of 
computer science 
keshav uwaterloo ca 
abstract 
wireless radios of the future will likely be frequency-agile 
that is supporting opportunistic and adaptive use of the rf 
spectrum such radios must coordinate with each other to 
build an accurate and consistent map of spectral 
utilization in their surroundings we focus on the problem of 
sharing rf spectrum data among a collection of wireless 
devices the inherent requirements of such data and the 
time-granularity at which it must be collected makes this 
problem both interesting and technically challenging we 
propose guess a novel incremental gossiping approach to 
coordinated spectral sensing it reduces protocol 
overhead by limiting the amount of information exchanged 
between participating nodes is resilient to network 
alterations due to node movement or node failures and 
allows exponentially-fast information convergence we outline 
an initial solution incorporating these ideas and also show 
how our approach reduces network overhead by up to a 
factor of and results in up to times faster information 
convergence than alternative approaches 
categories and subject descriptors 
c distributed systems distributed applications 
general terms 
algorithms performance experimentation 
 introduction 
there has recently been a huge surge in the growth of 
wireless technology driven primarily by the availability of 
unlicensed spectrum however this has come at the cost 
of increased rf interference which has caused the federal 
communications commission fcc in the united states to 
re-evaluate its strategy on spectrum allocation currently 
the fcc has licensed rf spectrum to a variety of public and 
private institutions termed primary users new spectrum 
allocation regimes implemented by the fcc use dynamic 
spectrum access schemes to either negotiate or 
opportunistically allocate rf spectrum to unlicensed secondary users 
permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page to copy otherwise to 
republish to post on servers or to redistribute to lists requires prior specific 
d 
d 
d 
d 
d 
primary user 
shadowed 
secondary users 
secondary users detect 
primary s signal 
shadowed 
secondary user 
figure without cooperation shadowed users are not 
able to detect the presence of the primary user 
that can use it when the primary user is absent the second 
type of allocation scheme is termed opportunistic spectrum 
sharing the fcc has already legislated this access method 
for the ghz band and is also considering the same for 
tv broadcast bands as a result a new wave of 
intelligent radios termed cognitive radios or software defined 
radios is emerging that can dynamically re-tune their 
radio parameters based on interactions with their surrounding 
environment 
under the new opportunistic allocation strategy 
secondary users are obligated not to interfere with primary 
users senders or receivers this can be done by sensing 
the environment to detect the presence of primary users 
however local sensing is not always adequate especially in 
cases where a secondary user is shadowed from a primary 
user as illustrated in figure here coordination between 
secondary users is the only way for shadowed users to 
detect the primary in general cooperation improves sensing 
accuracy by an order of magnitude when compared to not 
cooperating at all 
to realize this vision of dynamic spectrum access two 
fundamental problems must be solved efficient and 
coordinated spectrum sensing and distributed spectrum 
allocation in this paper we propose strategies for coordinated 
spectrum sensing that are low cost operate on timescales 
comparable to the agility of the rf environment and are 
resilient to network failures and alterations we defer the 
problem of spectrum allocation to future work 
spectrum sensing techniques for cognitive radio networks 
 are broadly classified into three regimes 
centralized coordinated techniques decentralized coordinated 
techniques and decentralized uncoordinated techniques 
we advocate a decentralized coordinated approach similar 
in spirit to ospf link-state routing used in the internet 
this is more effective than uncoordinated approaches 
because making decisions based only on local information is 
fallible as shown in figure moreover compared to 
cen 
tralized approaches decentralized techniques are more 
scalable robust and resistant to network failures and security 
attacks e g jamming 
coordinating sensory data between cognitive radio devices 
is technically challenging because accurately assessing 
spectrum usage requires exchanging potentially large amounts of 
data with many radios at very short time scales data size 
grows rapidly due to the large number i e thousands of 
spectrum bands that must be scanned this data must also 
be exchanged between potentially hundreds of neighboring 
secondary users at short time scales to account for rapid 
changes in the rf environment 
this paper presents guess a novel approach to 
coordinated spectrum sensing for cognitive radio networks our 
approach is motivated by the following key observations 
 low-cost sensors collect approximate data most 
devices have limited sensing resolution because they are 
low-cost and low duty-cycle devices and thus cannot 
perform complex rf signal processing e g matched 
filtering many are typically equipped with simple 
energy detectors that gather only approximate 
information 
 approximate summaries are sufficient for coordination 
approximate statistical summaries of sensed data are 
sufficient for correlating sensed information between 
radios as relative usage information is more 
important than absolute usage data thus exchanging 
exact rf information may not be necessary and more 
importantly too costly for the purposes of spectrum 
sensing 
 rf spectrum changes incrementally on most bands 
rf spectrum utilization changes infrequently 
moreover utilization of a specific rf band affects only that 
band and not the entire spectrum therefore if the 
usage pattern of a particular band changes 
substantially nodes detecting that change can initiate an 
update protocol to update the information for that band 
alone leaving in place information already collected 
for other bands this allows rapid detection of change 
while saving the overhead of exchanging unnecessary 
information 
based on these observations guess makes the following 
contributions 
 a novel approach that applies randomized gossiping 
algorithms to the problem of coordinated spectrum 
sensing these algorithms are well suited to coordinated 
spectrum sensing due to the unique characteristics of 
the problem i e radios are power-limited mobile and 
have limited bandwidth to support spectrum sensing 
capabilities 
 an application of in-network aggregation for 
dissemination of spectrum summaries we argue that 
approximate summaries are adequate for performing accurate 
radio parameter tuning 
 an extension of in-network aggregation and 
randomized gossiping to support incremental maintenance of 
spectrum summaries compared to standard 
gossiping approaches incremental techniques can further 
reduce overhead and protocol execution time by 
requiring fewer radio resources 
the rest of the paper is organized as follows section 
motivates the need for a low cost and efficient approach to 
coordinated spectrum sensing section discusses related 
work in the area while section provides a background on 
in-network aggregation and randomized gossiping sections 
 and discuss extensions and protocol details of these 
techniques for coordinated spectrum sensing section presents 
simulation results showcasing the benefits of guess and 
section presents a discussion and some directions for 
future work 
 motivation 
to estimate the scale of the problem in-stat predicts that 
the number of wifi-enabled devices sold annually alone will 
grow to million by therefore it would be 
reasonable to assume that a typical dense urban environment 
will contain several thousand cognitive radio devices in range 
of each other as a result distributed spectrum sensing and 
allocation would become both important and fundamental 
coordinated sensing among secondary radios is essential 
due to limited device sensing resolution and physical rf 
effects such as shadowing cabric et al illustrate the gains 
from cooperation and show an order of magnitude reduction 
in the probability of interference with the primary user when 
only a small fraction of secondary users cooperate 
however such coordination is non-trivial due to the 
limited bandwidth available for coordination the need to 
communicate this information on short timescales and 
the large amount of sensory data that needs to be exchanged 
limited bandwidth due to restrictions of cost and 
power most devices will likely not have dedicated hardware 
for supporting coordination this implies that both data 
and sensory traffic will need to be time-multiplexed onto a 
single radio interface therefore any time spent 
communicating sensory information takes away from the device s 
ability to perform its intended function thus any such 
coordination must incur minimal network overhead 
short timescales further compounding the problem 
is the need to immediately propagate updated rf sensory 
data in order to allow devices to react to it in a timely 
fashion this is especially true due to mobility as rapid changes 
of the rf environment can occur due to device and obstacle 
movements here fading and multi-path interference 
heavily impact sensing abilities signal level can drop to a deep 
null with just a λ movement in receiver position cm 
at ghz where λ is the wavelength coordination 
which does not support rapid dissemination of information 
will not be able to account for such rf variations 
large sensory data because cognitive radios can 
potentially use any part of the rf spectrum there will be 
numerous channels that they need to scan suppose we wish to 
compute the average signal energy in each of discretized 
frequency bands and each signal can have up to discrete 
energy levels exchanging complete sensory information 
between nodes would require bits per transmission for 
 channels each requiring seven bits of information 
exchanging this information among even a small group of 
devices each second would require time-steps × 
devices × bits per transmission mbps of aggregate 
network bandwidth 
contrast this to the use of a randomized gossip protocol to 
disseminate such information and the use of fm bit vectors 
to perform in-network aggregation by applying gossip and 
fm aggregation aggregate bandwidth requirements drop to 
 c·logn time-steps × devices × bits per transmission 
 mbps since time-steps are needed to propagate 
the data with c for illustrative purpoes 
 this is 
explained further in section 
based on these insights we propose guess a low-overhead 
approach which uses incremental extensions to fm 
aggregation and randomized gossiping for efficient coordination 
within a cognitive radio network as we show in section 
 
convergence time is correlated with the connectivity topology 
of the devices which in turn depends on the environment 
 
x 
a 
a 
x 
b 
b 
x 
figure using fm aggregation to compute average signal level measured by a group of devices 
these incremental extensions can further reduce bandwidth 
requirements by up to a factor of over the standard 
approaches discussed above 
 related work 
research in cognitive radio has increased rapidly 
over the years and it is being projected as one of the leading 
enabling technologies for wireless networks of the future 
as mentioned earlier the fcc has already identified new 
regimes for spectrum sharing between primary users and 
secondary users and a variety of systems have been proposed 
in the literature to support such sharing 
detecting the presence of a primary user is non-trivial 
especially a legacy primary user that is not cognitive 
radio aware secondary users must be able to detect the 
primary even if they cannot properly decode its signals this 
has been shown by sahai et al to be extremely 
difficult even if the modulation scheme is known sophisticated 
and costly hardware beyond a simple energy detector is 
required to improve signal detection accuracy moreover 
a shadowed secondary user may not even be able to detect 
signals from the primary as a result simple local 
sensing approaches have not gained much momentum this has 
motivated the need for cooperation among cognitive radios 
 
more recently some researchers have proposed approaches 
for radio coordination liu et al consider a centralized 
access point or base station architecture in which 
sensing information is forwarded to aps for spectrum allocation 
purposes aps direct mobile clients to collect such 
sensing information on their behalf however due to the need 
of a fixed ap infrastructure such a centralized approach is 
clearly not scalable 
in other work zhao et al propose a distributed 
coordination approach for spectrum sensing and allocation 
cognitive radios organize into clusters and coordination 
occurs within clusters the corvus architecture proposes 
a similar clustering method that can use either a centralized 
or decentralized approach to manage clusters although an 
improvement over purely centralized approaches these 
techniques still require a setup phase to generate the clusters 
which not only adds additional delay but also requires many 
of the secondary users to be static or quasi-static in 
contrast guess does not place such restrictions on secondary 
users and can even function in highly mobile environments 
 background 
this section provides the background for our approach 
we present the fm aggregation scheme that we use to 
generate spectrum summaries and perform in-network 
aggregation we also discuss randomized gossiping techniques for 
disseminating aggregates in a cognitive radio network 
 fm aggregation 
aggregation is the process where nodes in a distributed 
network combine data received from neighboring nodes with 
their local value to generate a combined aggregate this 
aggregate is then communicated to other nodes in the 
network and this process repeats until the aggregate at all 
nodes has converged to the same value i e the global 
aggregate double-counting is a well known problem in this 
process where nodes may contribute more than once to the 
aggregate causing inaccuracy in the final result intuitively 
nodes can tag the aggregate value they transmit with 
information about which nodes have contributed to it however 
this approach is not scalable order and duplicate 
insensitive odi techniques have been proposed in the literature 
 we adopt the odi approach pioneered by flajolet 
and martin fm for the purposes of aggregation next we 
outline the fm approach for full details see 
suppose we want to compute the number of nodes in the 
network i e the count query to do so each node 
performs a coin toss experiment as follows toss an unbiased 
coin stopping after the first head is seen the node then 
sets the ith bit in a bit vector initially filled with zeros 
where i is the number of coin tosses it performed the 
intuition is that as the number of nodes doing coin toss 
experiments increases the probability of a more significant bit 
being set in one of the nodes bit vectors increases 
these bit vectors are then exchanged among nodes when 
a node receives a bit vector it updates its local bit vector 
by bitwise or-ing it with the received vector as shown in 
figure which computes average at the end of the 
aggregation process every node with high probability has 
the same bit vector the actual value of the count aggregate 
is then computed using the following formula aggf m 
 j− 
 where j represents the bit position of the least 
significant zero in the aggregate bit vector 
although such aggregates are very compact in nature 
requiring only o logn state space where n is the number 
of nodes they may not be very accurate as they can only 
approximate values to the closest power of potentially 
causing errors of up to more accurate aggregates can 
be computed by maintaining multiple bit vectors at each 
node as explained in this decreases the error to within 
o 
√ 
m where m is the number of such bit vectors 
queries other than count can also be computed using 
variants of this basic counting algorithm as discussed in and 
shown in figure transmitting fm bit vectors between 
nodes is done using randomized gossiping discussed next 
 gossip protocols 
gossip-based protocols operate in discrete time-steps a 
time-step is the required amount of time for all 
transmissions in that time-step to complete at every time-step each 
node having something to send randomly selects one or more 
neighboring nodes and transmits its data to them the 
randomized propagation of information provides fault-tolerance 
and resilience to network failures and outages we 
emphasize that this characteristic of the protocol also allows it to 
operate without relying on any underlying network 
structure gossip protocols have been shown to provide 
exponentially fast convergence 
 on the order of o log n 
 where n is the number of nodes or radios these 
protocols can therefore easily scale to very dense 
environments 
 
convergence refers to the state in which all nodes have the most 
up-to-date view of the network 
 
two types of gossip protocols are 
 uniform gossip in uniform gossip at each 
timestep each node chooses a random neighbor and sends 
its data to it this process repeats for o log n steps 
 where n is the number of nodes in the network 
uniform gossip provides exponentially fast convergence 
with low network overhead 
 random walk in random walk only a subset of 
the nodes termed designated nodes communicate in a 
particular time-step at startup k nodes are randomly 
elected as designated nodes in each time-step each 
designated node sends its data to a random neighbor 
which becomes designated for the subsequent 
timestep much like passing a token this process repeats 
until the aggregate has converged in the network 
random walk has been shown to provide similar 
convergence bounds as uniform gossip in problems of similar 
context 
 incremental protocols 
 incremental fm aggregates 
one limitation of fm aggregation is that it does not 
support updates due to the probabilistic nature of fm once 
bit vectors have been ored together information cannot 
simply be removed from them as each node s contribution 
has not been recorded we propose the use of delete vectors 
an extension of fm to support updates we maintain a 
separate aggregate delete vector whose value is subtracted from 
the original aggregate vector s value to obtain the resulting 
value as follows 
agginc a− 
 − b− 
 
here a and b represent the bit positions of the least 
significant zero in the original and delete bit vectors respectively 
suppose we wish to compute the average signal level 
detected in a particular frequency to compute this we 
compute the sum of all signal level measurements and divide 
that by the count of the number of measurements a 
sum aggregate is computed similar to count explained 
in section except that each node performs s coin toss 
experiments where s is the locally measured signal level 
figure illustrates the sequence by which the average signal 
energy is computed in a particular band using fm 
aggregation 
now suppose that the measured signal at a node changes 
from s to s the vectors are updated as follows 
 s s we simply perform s − s more coin toss 
experiments and bitwise or the result with the original 
bit vector 
 s s we increase the value of the delete vector by 
performing s − s coin toss experiments and bitwise 
or the result with the current delete vector 
using delete vectors we can now support updates to the 
measured signal level with the original implementation of 
fm the aggregate would need to be discarded and a new one 
recomputed every time an update occurred thus delete 
vectors provide a low overhead alternative for applications 
whose data changes incrementally such as signal level 
measurements in a coordinated spectrum sensing environment 
next we discuss how these aggregates can be communicated 
between devices using incremental routing protocols 
 incremental routing protocol 
we use the following incremental variants of the routing 
protocols presented in section to support incremental 
updates to previously computed aggregates 
update received or 
local update occurs 
recovered 
susceptible 
time-stamp expires 
initial state 
additional 
update 
received 
infectious 
clean up 
figure state diagram each device passes through as 
updates proceed in the system 
 incremental gossip protocol igp when an 
update occurs the updated node initiates the gossiping 
procedure other nodes only begin gossiping once they 
receive the update therefore nodes receiving the 
update become active and continue communicating with 
their neighbors until the update protocol terminates 
after o log n time steps 
 incremental random walk protocol irwp 
when an update or updates occur in the system 
instead of starting random walks at k random nodes in 
the network all k random walks are initiated from the 
updated node s the rest of the protocol proceeds in 
the same fashion as the standard random walk 
protocol the allocation of walks to updates is discussed 
in more detail in where the authors show that the 
number of walks has an almost negligible impact on 
network overhead 
 protocol details 
using incremental routing protocols to disseminate 
incremental fm aggregates is a natural fit for the problem of 
coordinated spectrum sensing here we outline the 
implementation of such techniques for a cognitive radio network 
we continue with the example from section where we 
wish to perform coordination between a group of wireless 
devices to compute the average signal level in a particular 
frequency band 
using either incremental random walk or incremental 
gossip each device proceeds through three phases in order to 
determine the global average signal level for a particular 
frequency band figure shows a state diagram of these 
phases 
susceptible each device starts in the susceptible state 
and becomes infectious only when its locally measured signal 
level changes or if it receives an update message from a 
neighboring device if a local change is observed the device 
updates either the original or delete bit vector as described 
in section and moves into the infectious state if it 
receives an update message it ors the received original 
and delete bit vectors with its local bit vectors and moves 
into the infectious state 
note because signal level measurements may change 
sporadically over time a smoothing function such as an 
exponentially weighted moving average should be applied to 
these measurements 
infectious once a device is infectious it continues to 
send its up-to-date bit vectors using either incremental 
random walk or incremental gossip to neighboring nodes due 
to fm s order and duplicate insensitive odi properties 
simultaneously occurring updates are handled seamlessly by 
the protocol 
update messages contain a time stamp indicating when 
the update was generated and each device maintains a 
lo 
 
 
 
 
 
 
 
number of measured signal changes 
executiontime ms 
incremental gossip uniform gossip 
 a incremental gossip and uniform 
gossip on clique 
 
 
 
 
 
 
 
number of measured signal changes 
executiontime ms 
incremental random walk random walk 
 b incremental random walk and 
random walk on clique 
 
 
 
 
 
 
 
number of measured signal changes 
executiontime ms 
random walk incremental random walk 
 c incremental random walk and 
random walk on power-law random graph 
figure execution times of incremental protocols 
 
 
 
 
 
 
number of measured signal changes 
overheadimprovementratio 
 normalizedtouniformgossip 
incremental gossip uniform gossip 
 a incremental gossip and uniform 
gossip on clique 
 
 
 
 
 
 
number of measured signal changes 
overheadimprovementratio 
 normalizedtorandomwalk incremental random walk random walk 
 b incremental random walk and 
random walk on clique 
 
 
 
 
 
 
 
number of measured signal changes 
overheadimprovementratio 
 normalizedtorandomwalk 
random walk incremental random walk 
 c incremental random walk and 
random walk on power-law random graph 
figure network overhead of incremental protocols 
cal time stamp of when it received the most recent update 
using this information a device moves into the recovered 
state once enough time has passed for the most recent 
update to have converged as discussed in section this 
happens after o log n time steps 
recovered a recovered device ceases to propagate any 
update information at this point it performs clean-up and 
prepares for the next infection by entering the susceptible 
state once all devices have entered the recovered state the 
system will have converged and with high probability all 
devices will have the up-to-date average signal level due 
to the cumulative nature of fm even if all devices have not 
converged the next update will include all previous updates 
nevertheless the probability that gossip fails to converge is 
small and has been shown to be o n 
for coordinated spectrum sensing non-incremental 
routing protocols can be implemented in a similar fashion 
random walk would operate by having devices periodically drop 
the aggregate and re-run the protocol each device would 
perform a coin toss biased on the number of walks to 
determine whether or not it is a designated node this is 
different from the protocol discussed above where only 
updated nodes initiate random walks similar techniques can 
be used to implement standard gossip 
 evaluation 
we now provide a preliminary evaluation of guess in 
simulation a more detailed evaluation of this approach can 
be found in here we focus on how incremental 
extensions to gossip protocols can lead to further improvements 
over standard gossiping techniques for the problem of 
coordinated spectrum sensing 
simulation setup we implemented a custom 
simulator in c we study the improvements of our 
incremental gossip protocols over standard gossiping in two 
dimensions execution time and network overhead we use two 
topologies to represent device connectivity a clique to 
eliminate the effects of the underlying topology on protocol 
performance and a brite-generated power-law random 
graph plrg to illustrate how our results extend to more 
realistic scenarios we simulate a large deployment of 
devices to analyze protocol scalability 
in our simulations we compute the average signal level in 
a particular band by disseminating fm bit vectors in each 
run of the simulation we induce a change in the measured 
signal at one or more devices a run ends when the new 
average signal level has converged in the network 
for each data point we ran simulations and 
confidence intervals error bars are shown 
simulation parameters each transmission involves 
sending bits of information to a neighboring node to 
compute the average aggregate four bit vectors need to 
be transmitted the original sum vector the sum delete 
vector the original count vector and the count delete 
vector non-incremental protocols do not transmit the delete 
vectors each transmission also includes a time stamp of 
when the update was generated 
we assume nodes communicate on a common control 
channel at mbps therefore one time-step of protocol 
execution corresponds to the time required for nodes to 
sequentially send bits at mbps sequential use of the 
control channel is a worst case for our protocols in practice 
multiple control channels could be used in parallel to reduce 
execution time we also assume nodes are loosely time 
synchronized the implications of which are discussed further in 
 finally in order to isolate the effect of protocol 
operation on performance we do not model the complexities of 
the wireless channel in our simulations 
incremental protocols reduce execution time 
figure a compares the performance of incremental gossip 
 igp with uniform gossip on a clique topology we observe 
that both protocols have almost identical execution times 
this is expected as igp operates in a similar fashion to 
 
uniform gossip taking o log n time-steps to converge 
figure b compares the execution times of 
incremental random walk irwp and standard random walk on a 
clique irwp reduces execution time by a factor of for a 
small number of measured signal changes although random 
walk and irwp both use k random walks in our simulations 
k number of nodes irwp initiates walks only from 
updated nodes as explained in section resulting in faster 
information convergence these improvements carry over to 
a plrg topology as well as shown in figure c where 
irwp is times faster than random walk 
incremental protocols reduce network overhead 
figure a shows the ratio of data transmitted using 
uniform gossip relative to incremental gossip on a clique for 
a small number of signal changes incremental gossip incurs 
 times less overhead than uniform gossip this is because 
in the early steps of protocol execution only devices which 
detect signal changes communicate as more signal changes 
are introduced into the system gossip and incremental 
gossip incur approximately the same overhead 
similarly incremental random walk irwp incurs much 
less overhead than standard random walk figure b shows 
a fold reduction in overhead for small numbers of 
signal changes on a clique although each protocol uses the 
same number of random walks irwp uses fewer network 
resources than random walk because it takes less time to 
converge this improvement also holds true on more 
complex plrg topologies as shown in figure c where we 
observe a reduction in network overhead 
from these results it is clear that incremental techniques 
yield significant improvements over standard approaches to 
gossip even on complex topologies because spectrum 
utilization is characterized by incremental changes to usage 
incremental protocols are ideally suited to solve this 
problem in an efficient and cost effective manner 
 discussion and future work 
we have only just scratched the surface in addressing the 
problem of coordinated spectrum sensing using incremental 
gossiping next we outline some open areas of research 
spatial decay devices performing coordinated sensing 
are primarily interested in the spectrum usage of their local 
neighborhood therefore we recommend the use of 
spatially decaying aggregates which limits the impact of an 
update on more distant nodes spatially decaying 
aggregates work by successively reducing by means of a decay 
function the value of the update as it propagates further 
from its origin one challenge with this approach is that 
propagation distance cannot be determined ahead of time 
and more importantly exhibits spatio-temporal variations 
therefore finding the optimal decay function is non-trivial 
and an interesting subject of future work 
significance threshold rf spectrum bands 
continually experience small-scale changes which may not 
necessarily be significant deciding if a change is significant can be 
done using a significance threshold β below which any 
observed change is not propagated by the node choosing an 
appropriate operating value for β is application dependent 
and explored further in 
weighted readings although we argued that most 
devices will likely be equipped with low-cost sensing 
equipment there may be situations where there are some special 
infrastructure nodes that have better sensing abilities than 
others weighting their measurements more heavily could 
be used to maintain a higher degree of accuracy 
determining how to assign such weights is an open area of research 
implementation specifics finally implementing 
gossip for coordinated spectrum sensing is also open if 
implemented at the mac layer it may be feasible to piggy-back 
gossip messages over existing management frames e g 
networking advertisement messages as well we also require 
the use of a control channel to disseminate sensing 
information there are a variety of alternatives for 
implementing such a channel some of which are outlined in the 
trade-offs of different approaches to implementing guess 
is a subject of future work 
 conclusion 
spectrum sensing is a key requirement for dynamic 
spectrum allocation in cognitive radio networks the nature of 
the rf environment necessitates coordination between 
cognitive radio devices we propose guess an approximate 
yet low overhead approach to perform efficient coordination 
between cognitive radios the fundamental contributions of 
guess are an fm aggregation scheme for efficient 
innetwork aggregation a randomized gossiping approach 
which provides exponentially fast convergence and 
robustness to network alterations and incremental variations 
of fm and gossip which we show can reduce the 
communication time by up to a factor of and reduce network 
overhead by up to a factor of our preliminary 
simulation results showcase the benefits of this approach and we 
also outline a set of open problems that make this a new 
and exciting area of research 
 references 
 unlicensed operation in the tv broadcast bands and 
additional spectrum for unlicensed devices below mhz in 
the ghz band may notice of proposed rule-making 
 - federal communications commission 
 in-stat covering the full spectrum of digital communications 
market research from vendor to end-user december 
http www in-stat com catalog scatalogue asp id 
 n ahmed d hadaller and s keshav incremental 
maintenance of global aggregates uw technical report 
cs- - university of waterloo on canada 
 r w brodersen a wolisz d cabric s m mishra and 
d willkomm corvus a cognitive radio approach for 
usage of virtual unlicensed spectrum technical report july 
 
 d cabric s m mishra and r w brodersen implementation 
issues in spectrum sensing for cognitive radios in asilomar 
conference 
 e cohen and h kaplan spatially-decaying aggregation over 
a network model and algorithms in proceedings of sigmod 
 pages - new york ny usa acm press 
 p flajolet and g n martin probabilistic counting 
algorithms for data base applications j comput syst sci 
 - 
 c gkantsidis m mihail and a saberi random walks in 
peer-to-peer networks in proceedings of infocom 
pages - 
 e griffith previewing intel s cognitive radio chip june 
http www internetnews com wireless article php 
 d kempe a dobra and j gehrke gossip-based 
computation of aggregate information in focs page 
 washington dc usa ieee computer society 
 x liu and s shankar sensing-based opportunistic channel 
access in acm mobile networks and applications 
 monet journal march 
 q lv p cao e cohen k li and s shenker search and 
replication in unstructured peer-to-peer networks in 
proceedings of ics 
 a medina a lakhina i matta and j byers brite an 
approach to universal topology generation in proceedings of 
mascots conference aug 
 s m mishra a sahai and r w brodersen cooperative 
sensing among cognitive radios in icc june 
 s nath p b gibbons s seshan and z r anderson 
synopsis diffusion for robust aggregation in sensor networks 
in proceedings of sensys pages - 
 a sahai n hoven s m mishra and r tandra fundamental 
tradeoffs in robust spectrum sensing for opportunistic 
frequency reuse technical report uc berkeley 
 j zhao h zheng and g -h yang distributed coordination 
in dynamic spectrum allocation networks in proceedings of 
dyspan baltimore md nov 
 
heuristics-based scheduling of 
composite web service workloads 
thomas phan wen-syan li 
ibm almaden research center 
 harry rd 
san jose ca 
{phantom wsl} us ibm com 
abstract 
web services can be aggregated to create composite workflows that 
provide streamlined functionality for human users or other systems 
although industry standards and recent research have sought to 
define best practices and to improve end-to-end workflow 
composition one area that has not fully been explored is the scheduling 
of a workflow s web service requests to actual service 
provisioning in a multi-tiered multi-organisation environment this issue 
is relevant to modern business scenarios where business processes 
within a workflow must complete within qos-defined limits 
because these business processes are web service consumers service 
requests must be mapped and scheduled across multiple web 
service providers each with its own negotiated service level 
agreement in this paper we provide heuristics for scheduling service 
requests from multiple business process workflows to web service 
providers such that a business value metric across all workflows is 
maximised we show that a genetic search algorithm is appropriate 
to perform this scheduling and through experimentation we show 
that our algorithm scales well up to a thousand workflows and 
produces better mappings than traditional approaches 
categories and subject descriptors 
c computer-communication networks distributed 
systems-distributed applications d software engineering 
metrics-complexity measures performance measures 
 introduction 
web services can be composed into workflows to provide 
streamlined end-to-end functionality for human users or other systems 
although previous research efforts have looked at ways to 
intelligently automate the composition of web services into workflows 
 e g an important remaining problem is the assignment of 
web service requests to the underlying web service providers in a 
multi-tiered runtime scenario within constraints in this paper we 
address this scheduling problem and examine means to manage a 
large number of business process workflows in a scalable manner 
the problem of scheduling web service requests to providers is 
relevant to modern business domains that depend on multi-tiered 
service provisioning consider the example shown in figure 
that illustrates our problem space workflows comprise multiple 
related business processes that are web service consumers here we 
assume that the workflows represent requested service from 
customers or automated systems and that the workflow has already 
been composed with an existing choreography toolkit these 
workflows are then submitted to a portal not shown that acts as a 
scheduling agent between the web service consumers and the web 
service providers 
in this example a workflow could represent the actions needed to 
instantiate a vacation itinerary where one business process requests 
booking an airline ticket another business process requests a hotel 
room and so forth each of these requests target a particular service 
type e g airline reservations hotel reservations car reservations 
etc and for each service type there are multiple instances of 
service providers that publish a web service interface an important 
challenge is that the workflows must meet some quality-of-service 
 qos metric such as end-to-end completion time of all its business 
processes and that meeting or failing this goal results in the 
assignment of a quantitative business value metric for the workflow 
intuitively it is desired that all workflows meet their respective qos 
goals we further leverage the notion that qos service agreements 
are generally agreed-upon between the web service providers and 
the scheduling agent such that the providers advertise some level 
of guaranteed qos to the scheduler based upon runtime conditions 
such as turnaround time and maximum available concurrency the 
resulting problem is then to schedule and assign the business 
processes requests for service types to one of the service providers 
for that type the scheduling must be done such that the aggregate 
business value across all the workflows is maximised 
in section we state the scenario as a combinatorial problem and 
utilise a genetic search algorithm to find the best assignment 
of web service requests to providers this approach converges 
towards an assignment that maximises the overall business value for 
all the workflows 
in section we show through experimentation that this search 
heuristic finds better assignments than other algorithms greedy 
round-robin and proportional further this approach allows us to 
scale the number of simultaneous workflows up to one thousand 
workflows in our experiments and yet still find effective schedules 
 related work 
in the context of service assignment and scheduling maps 
web service calls to potential servers using linear programming but 
their work is concerned with mapping only single workflows our 
principal focus is on scalably scheduling multiple workflows up 
 
service type 
superhotels com 
business 
process 
business 
process 
workflow 
 
business 
process 
business 
process 
 
hostilehostels com 
incredibleinns com 
business 
process 
business 
process 
business 
process 
 
business 
process 
service 
provider 
skyhighairlines com 
supercrazyflights com 
business 
process 
 
 
 
 
 
 
advertised qos 
service agreement 
carrentalservice com 
figure an example scenario demonstrating the interaction between business processes in workflows and web service providers 
each business process accesses a service type and is then mapped to a service provider for that type 
to one thousand as we show later using different business 
metrics and a search heuristic presents a dynamic 
provisioning approach that uses both predictive and reactive techniques for 
multi-tiered internet application delivery however the 
provisioning techniques do not consider the challenges faced when there are 
alternative query execution plans and replicated data sources 
presents a feedback-based scheduling mechanism for multi-tiered 
systems with back-end databases but unlike our work it assumes 
a tighter coupling between the various components of the system 
our work also builds upon prior scheduling research the classic 
job-shop scheduling problem shown to be np-complete is 
similar to ours in that tasks within a job must be scheduled onto 
machinery c f our scenario is that business processes within a 
workflow must be scheduled onto web service providers the salient 
differences are that the machines can process only one job at a time 
 we assume servers can multi-task but with degraded performance 
and a maximum concurrency level tasks within a job cannot 
simultaneously run on different machines we assume business 
processes can be assigned to any available server and the principal 
metric of performance is the makespan which is the time for the 
last task among all the jobs to complete and as we show later 
optimising on the makespan is insufficient for scheduling the business 
processes necessitating different metrics 
 design 
in this section we describe our model and discuss how we can 
find scheduling assignments using a genetic search algorithm 
 model 
we base our model on the simplified scenario shown in figure 
 specifically we assume that users or automated systems request 
the execution of a workflow the workflows comprise business 
processes each of which makes one web service invocation to a 
service type further business processes have an ordering in the 
workflow the arrangement and execution of the business processes and 
the data flow between them are all managed by a composition or 
choreography tool e g although composition languages 
can use sophisticated flow-control mechanisms such as conditional 
branches for simplicity we assume the processes execute 
sequentially in a given order 
this scenario can be naturally extended to more complex 
relationships that can be expressed in bpel which defines how 
business processes interact messages are exchanged activities are 
ordered and exceptions are handled due to space constraints 
we focus on the problem space presented here and will extend our 
model to more advanced deployment scenarios in the future 
each workflow has a qos requirement to complete within a 
specified number of time units e g on the order of seconds as 
detailed in the experiments section upon completion or failure 
the workflow is assigned a business value we extended this 
approach further and considered different types of workflow 
completion in order to model differentiated qos levels that can be applied 
by businesses for example to provide tiered customer service 
we say that a workflow is successful if it completes within its qos 
requirement acceptable if it completes within a constant factor κ 
 
of its qos bound in our experiments we chose κ or failing 
if it finishes beyond κ times its qos bound for each category 
a business value score is assigned to the workflow with the 
successful category assigned the highest positive score followed by 
acceptable and then failing the business value point 
distribution is non-uniform across workflows further modelling cases 
where some workflows are of higher priority than others 
each service type is implemented by a number of different 
service providers we assume that the providers make service level 
agreements slas to guarantee a level of performance defined by 
the completion time for completing a web service invocation 
although slas can be complex in this paper we assume for 
simplicity that the guarantees can take the form of a linear performance 
degradation under load this guarantee is defined by several 
parameters α is the expected completion time for example on the 
order of seconds if the assigned workload of web service requests 
is less than or equal to β the maximum concurrency and if the 
workload is higher than β the expected completion for a workload 
of size ω is α γ ω − β where γ is a fractional coefficient in our 
experiments we vary α β and γ with different distributions 
ideally all workflows would be able to finish within their qos 
limits and thus maximise the aggregate business value across all 
workflows however because we model service providers with 
degrading performance under load not all workflows will achieve 
their qos limit it may easily be the case that business processes 
are assigned to providers who are overloaded and cannot complete 
within the respective workflow s qos limit the key research 
problem then is to assign the business processes to the web service 
providers with the goal of optimising on the aggregate business 
value of all workflows 
given that the scope of the optimisation is the entire set of 
workflows it may be that the best scheduling assignments may result in 
some workflows having to fail in order for more workflows to 
succeed this intuitive observation suggests that traditional scheduling 
approaches such as round-robin or proportional assignments will 
not fare well which is what we observe and discuss in section 
on the other hand an exhaustive search of all the possible 
assignments will find the best schedule but the computational complexity 
is prohibitively high suppose there are w workflows with an 
average of b business processes per workflow further in the worst 
case each business process requests one service type for which 
there are p providers there are thus w · pb 
combinations to 
explore to find the optimal assignments of business processes to 
providers even for small configurations e g w b p 
the computational time for exhaustive search is significant and in 
our work we look to scale these parameters in the next subsection 
discuss how a genetic search algorithm can be used to converge 
toward the optimum scheduling assignments 
 genetic algorithm 
given an exponential search space of business process 
assignments to web service providers the problem is to find the optimal 
assignment that produces the overall highest aggregate business 
value across all workflows to explore the solution space we use 
a genetic algorithm ga search heuristic that simulates darwinian 
natural selection by having members of a population compete to 
survive in order to pass their genetic chromosomes onto the next 
generation after successive generations there is a tendency for the 
chromosomes to converge toward the best combination 
although other search heuristics exist that can solve 
optimization problems e g simulated annealing or steepest-ascent 
hillclimbing the business process scheduling problem fits well with a 
ga because potential solutions can be represented in a matrix form 
and allows us to use prior research in effective ga chromosome 
recombination to form new members of the population e g 
 
 
 
 
figure an example chromosome representing a scheduling 
assignment of workflow service type → service provider each 
row represents a workflow and each column represents a 
service type for example here there are workflows to and 
 service types to in workflow any request for service 
type goes to provider note that the service provider 
identifier is within a range limited to its service type i e its column 
so the listed for service type is a different server from 
server in other columns 
chromosome representation of a solution in figure we 
show an example chromosome that encodes one scheduling 
assignment the representation is a -dimensional matrix that maps 
{workflow service type} to a service provider for a business 
process in workflow i and utilising service type j the i j th 
entry in 
the table is the identifier for the service provider to which the 
business process is assigned note that the service provider identifier is 
within a range limited to its service type 
ga execution a ga proceeds as follows initially a random 
set of chromosomes is created for the population the 
chromosomes are evaluated hashed to some metric and the best ones 
are chosen to be parents in our problem the evaluation produces 
the net business value across all workflows after executing all 
business processes once they are assigned to their respective service 
providers according to the mapping in the chromosome the 
parents recombine to produce children simulating sexual crossover 
and occasionally a mutation may arise which produces new 
characteristics that were not available in either parent the principal idea 
is that we would like the children to be different from the parents 
 in order to explore more of the solution space yet not too 
different in order to contain the portions of the chromosome that result 
in good scheduling assignments note that finding the global 
optimum is not guaranteed because the recombination and mutation 
are stochastic 
ga recombination and mutation as mentioned the 
chromosomes are -dimensional matrices that represent scheduling 
assignments to simulate sexual recombination of two chromosomes to 
produce a new child chromosome we applied a one-point crossover 
scheme twice once along each dimension the crossover is best 
explained by analogy to cartesian space as follows a random 
point is chosen in the matrix to be coordinate matrix 
elements from quadrants ii and iv from the first parent and elements 
from quadrants i and iii from the second parent are used to create 
the new child this approach follows ga best practices by keeping 
contiguous chromosome segments together as they are transmitted 
from parent to child 
the uni-chromosome mutation scheme randomly changes one 
of the service provider assignments to another provider within the 
available range other recombination and mutation schemes are an 
area of research in the ga community and we look to explore new 
operators in future work 
ga evaluation function an important ga component is the 
evaluation function given a particular chromosome representing 
one scheduling mapping the function deterministically calculates 
the net business value across all workloads the business 
processes in each workload are assigned to service providers and each 
provider s completion time is calculated based on the service 
agreement guarantee using the parameters mentioned in section 
namely the unloaded completion time α the maximum 
concur 
rency β and a coefficient γ that controls the linear performance 
degradation under heavy load note that the evaluation function 
can be easily replaced if desired for example other evaluation 
functions can model different service provider guarantees or 
parallel workflows 
 experiments and results 
in this section we show the benefit of using our ga-based 
scheduler because we wanted to scale the scenarios up to a large number 
of workflows up to in our experiments we implemented a 
simulation program that allowed us to vary parameters and to 
measure the results with different metrics the simulator was written 
in standard c and was run on a linux fedora core desktop 
computer running at ghz with gb of ram 
we compared our algorithm against alternative candidates 
 a well-known round-robin algorithm that assigns each 
business process in circular fashion to the service providers for a 
particular service type this approach provides the simplest 
scheme for load-balancing 
 a random-proportional algorithm that proportionally assigns 
business processes to the service providers that is for a 
given service type the service providers are ranked by their 
guaranteed completion time and business processes are 
assigned proportionally to the providers based on their 
completion time we also tried a proportionality scheme based 
on both the completion times and maximum concurrency but 
attained the same results so only the former scheme s results 
are shown here 
 a strawman greedy algorithm that always assigns business 
processes to the service provider that has the fastest 
guaranteed completion time this algorithm represents a naive 
approach based on greedy local observations of each workflow 
without taking into consideration all workflows 
in the experiments that follow all results were averaged across 
 trials and to help normalise the effects of randomisation used 
during the ga each trial started by reading in pre-initialised data 
from disk in table we list our experimental parameters 
in figure we show the results of running our ga against the 
three candidate alternatives the x-axis shows the number for 
workflows scaled up to and the y-axis shows the aggregate 
business value for all workflows as can be seen the ga consistently 
produces the highest business value even as the number of 
workflows grows at workflows the ga produces a 
improvement over the next-best alternative note that although we 
are optimising against the business value metric we defined earlier 
genetic algorithms are able to converge towards the optimal value 
of any metric as long as the evaluation function can consistently 
measure a chromosome s value with that metric 
as expected the greedy algorithm performs very poorly because 
it does the worst job at balancing load all business processes for 
a given service type are assigned to only one server the one 
advertised to have the fastest completion time and as more 
business processes arrive the provider s performance degrades linearly 
the round-robin scheme is initially outperformed by the 
randomproportional scheme up to around workflows as shown in the 
magnified graph of figure but as the number of workflows 
increases the round-robin scheme consistently wins over 
randomproportional the reason is that although the random-proportional 
scheme assigns business processes to providers proportionally 
according to the advertised completion times which is a measure of 
the power of the service provider even the best providers will 
eventually reach a real-world maximum concurrency for the large 
- 
- 
 
 
 
 
 
 
 
 
 
aggregatebusinessvalueacrossallworkflows 
total number of workflows 
business value scores of scheduling algorithms 
genetic algorithm 
round robin 
random proportional 
greedy 
figure net business value scores of different scheduling algorithms 
- 
 
 
 
 
 
 
 
 
 
 aggregatebusinessvalueacrossallworkflows 
total number of workflows 
business value scores of scheduling algorithms 
genetic algorithm 
round robin 
random proportional 
greedy 
figure magnification of the left-most region in figure 
number of workflows that we are considering for a very large 
number of workflows the round-robin scheme is able to better 
balance the load across all service providers 
to better understand the behaviour resulting from the scheduling 
assignments we show the workflow completion results in figures 
 and for and workflows respectively these 
figures show the percentage of workflows that are successful can 
complete with their qos limit acceptable can complete within 
κ times their qos limit and failed cannot complete within κ 
times their qos limit the ga consistently produces the highest 
percentage of successful workflows resulting in higher business 
values for the aggregate set of workflows further the round-robin 
scheme produces better results than the random-proportional for a 
large number of workflows but does not perform as well as the ga 
in figure we graph the makespan resulting from the same 
experiments above makespan is a traditional metric from the job 
scheduling community measuring elapsed time for the last job to 
complete while useful it does not capture the high-level business 
value metric that we are optimising against indeed the makespan 
is oblivious to the fact that we provide multiple levels of 
completion successful acceptable and failed and assign business value 
scores accordingly for completeness we note that the ga 
provides the fastest makespan but it is matched by the round robin 
algorithm the ga produces better business values as shown in 
figure because it is able to search the solution space to find 
better mappings that produce more successful workflows as shown in 
figures to 
we also looked at the effect of the scheduling algorithms on 
balancing the load figure shows the percentage of services 
providers that were accessed while the workflows ran as expected 
the greedy algorithm always hits one service provider on the other 
hand the round-robin algorithm is the fastest to spread the business 
 
experimental parameter comment 
workflows to 
business processes per workflow uniform random - 
service types 
service providers per service type uniform random - 
workflow qos goal uniform random - seconds 
service provider completion time α uniform random - seconds 
service provider maximum concurrency β uniform random - 
service provider degradation coefficient γ uniform random - 
business value for successful workflows uniform random - points 
business value for acceptable workflows uniform random - points 
business value for failed workflows uniform random - - points 
ga number of parents 
ga number of children 
ga number of generations 
table experimental parameters 
failed 
acceptable completed but not within qos 
successful completed within qos 
 
 
 
 
 
 
roundrobinrandproportionalgreedygeneticalg 
percentageofallworkflows 
workflow behaviour workflows 
figure workflow behaviour for workflows 
failed 
acceptable completed but not within qos 
successful completed within qos 
 
 
 
 
 
 
roundrobinrandproportionalgreedygeneticalg 
percentageofallworkflows 
workflow behaviour workflows 
figure workflow behaviour for workflows 
failed 
acceptable completed but not within qos 
successful completed within qos 
 
 
 
 
 
 
roundrobinrandproportionalgreedygeneticalg 
percentageofallworkflows 
workflow behaviour workflows 
figure workflow behaviour for workflows 
 
 
 
 
 
 
 
 
makespan seconds 
number of workflows 
maximum completion time for all workflows 
genetic algorithm 
round robin 
random proportional 
greedy 
figure maximum completion time for all workflows this value 
is the makespan metric used in traditional scheduling research 
although useful the makespan does not take into consideration the 
business value scoring in our problem domain 
processes figure is the percentage of accessed service providers 
 that is the percentage of service providers represented in figure 
 that had more assigned business processes than their advertised 
maximum concurrency for example in the greedy algorithm only 
one service provider is utilised and this one provider quickly 
becomes saturated on the other hand the random-proportional 
algorithm uses many service providers but because business processes 
are proportionally assigned with more assignments going to the 
better providers there is a tendency for a smaller percentage of 
providers to become saturated 
for completeness we show the performance of the genetic 
algorithm itself in figure the algorithm scales linearly with an 
increasing number of workflows we note that the round-robin 
random-proportional and greedy algorithms all finished within 
second even for the largest workflow configuration however we 
feel that the benefit of finding much higher business value scores 
justifies the running time of the ga further we would expect that 
the running time will improve with both software tuning as well as 
with a computer faster than our off-the-shelf pc 
 conclusion 
business processes within workflows can be orchestrated to 
access web services in this paper we looked at multi-tiered service 
provisioning where web service requests to service types can be 
mapped to different service providers the resulting problem is 
that in order to support a very large number of workflows the 
assignment of business process to web service provider must be 
intelligent we used a business value metric to measure the 
be 
 
 
 
 
 
 
 
percentageofallserviceproviders 
number of workflows 
service providers utilised 
genetic algorithm 
round robin 
random proportional 
greedy 
figure the percentage of service providers utilized during 
workload executions the greedy algorithm always hits the one service 
provider while the round robin algorithm spreads requests evenly 
across the providers 
 
 
 
 
 
 
 
percentageofallserviceproviders 
number of workflows 
service providers saturated 
genetic algorithm 
round robin 
random proportional 
greedy 
figure the percentage of service providers that are saturated 
among those providers who were utilized that is percentage of the 
service providers represented in figure a saturated service provider 
is one whose workload is greater that its advertised maximum 
concurrency 
 
 
 
 
 
 
 
runningtimeinseconds 
total number of workflows 
running time of genetic algorithm 
ga running time 
figure running time of the genetic algorithm 
haviour of workflows meeting or failing qos values and we 
optimised our scheduling to maximise the aggregate business value 
across all workflows since the solution space of scheduler 
mappings is exponential we used a genetic search algorithm to search 
the space and converge toward the best schedule with a default 
configuration for all parameters and using our business value 
scoring the ga produced up to business value improvement over 
the next best algorithm finally because a genetic algorithm will 
converge towards the optimal value using any metric even other 
than the business value metric we used we believe our approach 
has strong potential for continuing work 
in future work we look to acquire real-world traces of web 
service instances in order to get better estimates of service agreement 
guarantees although we expect that such guarantees between the 
providers and their consumers are not generally available to the 
public we will also look at other qos metrics such as cpu and 
i o usage for example we can analyse transfer costs with 
varying bandwidth latency data size and data distribution further 
we hope to improve our genetic algorithm and compare it to more 
scheduler alternatives finally since our work is complementary 
to existing work in web services choreography because we rely on 
pre-configured workflows we look to integrate our approach with 
available web service workflow systems expressed in bpel 
 references 
 a ankolekar et al daml-s semantic markup for web 
services in proc of the int l semantic web working 
symposium 
 l davis job shop scheduling with genetic algorithms 
in proc of the int l conference on genetic algorithms 
 h -l fang p ross and d corne a promising genetic 
algorithm approach to job-shop scheduling rescheduling 
and open-shop scheduling problems in proc on the th 
int l conference on genetic algorithms 
 m gary and d johnson computers and intractability a 
guide to the theory of np-completeness freeman 
 j holland adaptation in natural and artificial systems 
an introductory analysis with applications to biology 
control and artificial intelligence mit press 
 d goldberg genetic algorithms in search optimization 
and machine learning kluwer academic publishers 
 business processes in a web services world 
www- ibm com developerworks 
webservices library ws-bpelwp 
 g soundararajan k manassiev j chen a goel and c 
amza back-end databases in shared dynamic content 
server clusters in proc of the ieee int l conference on 
autonomic computing 
 b srivastava and j koehler web service composition 
current solutions and open problems icap 
 b urgaonkar p shenoy a chandra and p goyal 
dynamic provisioning of multi-tier internet applications 
in proc of the ieee int l conference on autonomic 
computing 
 l zeng b benatallah m dumas j kalagnanam and q 
sheng quality driven web services composition in 
proc of the www conference 
 
concept and architecture of a pervasive document editing 
and managing system 
stefania leone thomas b hodel harald gall 
university of zurich switzerland university of zurich switzerland university of zurich switzerland 
department of informatics department of informatics department of informatics 
leone ifi unizh ch hodel ifi unizh ch gall ifi unizh ch 
abstract 
collaborative document processing has been addressed by many 
approaches so far most of which focus on document versioning 
and collaborative editing we address this issue from a different 
angle and describe the concept and architecture of a pervasive 
document editing and managing system it exploits database 
techniques and real-time updating for sophisticated collaboration 
scenarios on multiple devices each user is always served with 
upto-date documents and can organize his work based on document 
meta data for this we present our conceptual architecture for 
such a system and discuss it with an example 
categories and subject descriptors 
c distributed systems computer-communication 
networks computer system organization distributed systems 
distributed applications 
general terms 
management measurement documentation economics human 
factors 
 introduction 
text documents are a valuable resource for virtually any enterprise 
and organization documents like papers reports and general 
business documentations contain a large part of today s business 
knowledge documents are mostly stored in a hierarchical folder 
structure on file servers and it is difficult to organize them in regard 
to classification versioning etc although it is of utmost importance 
that users can find retrieve and edit up-to-date versions of 
documents whenever they want and in a user-friendly way 
 problem description 
with most of the commonly used word-processing applications 
documents can be manipulated by only one user at a time tools for 
pervasive collaborative document editing and management are 
rarely deployed in today s world despite the fact that people strive 
for location- and time- independence the importance of pervasive 
collaborative work i e collaborative document editing and 
management is totally neglected documents could therefore be 
seen as a vulnerable source in today s world which demands for an 
appropriate solution the need to store retrieve and edit these 
documents collaboratively anytime everywhere and with almost 
every suitable device and with guaranteed mechanisms for security 
consistency availability and access control is obvious 
in addition word processing systems ignore the fact that the history 
of a text document contains crucial information for its management 
such meta data includes creation date creator authors version 
location-based information such as time and place when where a 
user reads edits a document and so on such meta data can be 
gathered during the documents creation process and can be used 
versatilely especially in the field of pervasive document 
management meta data is of crucial importance since it offers 
totally new ways of organizing and classifying documents on the 
one hand the user s actual situation influences the user s objectives 
meta data could be used to give the user the best possible view on 
the documents dependent of his actual information on the other 
hand as soon as the user starts to work i e reads or edits a 
document new meta data can be gathered in order to make the 
system more adaptable and in a sense to the users situation and to 
offer future users a better view on the documents 
as far as we know no system exists that satisfies the 
aforementioned requirements a very good overview about 
realtime communication and collaboration system is described in 
we therefore strive for a pervasive document editing and 
management system which enables pervasive and collaborative 
document editing and management users should be able to read and 
edit documents whenever wherever with whomever and with 
whatever device 
in this paper we present collaborative database-based real-time 
word processing which provides pervasive document editing and 
management functionality it enables the user to work on 
documents collaboratively and offers sophisticated document 
management facility the user is always served with up-to-date 
documents and can organize and manage documents on the base of 
meta data additionally document data is treated as  first class 
citizen of the database as demanded in 
 underlying concepts 
the concept of our pervasive document editing and management 
system requires an appropriate architectural foundation our 
concept and implementation are based on the tendax 
collaborative database-based document editing and management 
system which enables pervasive document editing and managing 
tendax is a text native database extension it enables the 
storage of text in databases in a native form so that editing text is 
finally represented as real-time transactions under the term  text 
editing we understand the following writing and deleting text 
 characters copying pasting text defining text layout 
structure inserting notes setting access rights defining business 
processes inserting tables pictures and so on i e all the actions 
regularly carried out by word processing users with  real-time 
transaction we mean that editing text e g writing a 
character word invokes one or several database transactions so that 
everything which is typed appears within the editor as soon as these 
objects are stored persistently instead of creating files and storing 
them in a file system the content and all of the meta data belonging 
to the documents is stored in a special way in the database which 
enables very fast real-time transactions for all editing tasks 
the database schema and the above-mentioned transactions are 
created in such a way that everything can be done within a 
multiuser environment as is usual done by database technology as a 
consequence many of the achievements with respect to data 
organization and querying recovery integrity and security 
enforcement multi-user operation distribution management 
uniform tool access etc are now by means of this approach also 
available for word processing 
 approach 
our pervasive editing and management system is based on the 
above-mentioned database-based tendax approach where 
document data is stored natively in the database and supports 
pervasive collaborative text editing and document management 
we define the pervasive document editing and management system 
as a system where documents can easily be accessed and 
manipulated everywhere within the network anytime 
 independently of the number of users working on the same 
document and with any device desktop notebook pda mobile 
phone etc 
db 
rtsc 
rtsc 
rtsc 
rtsc 
as 
as 
db 
db 
as 
as 
db 
a 
b 
c 
d 
e 
f 
g 
figure tendax application architecture 
in contrast to documents stored locally on the hard drive or on a file 
server our system automatically serves the user with the up-to-date 
version of a document and changes done on the document are stored 
persistently in the database and immediately propagated to all 
clients who are working on the same document additionally meta 
data gathered during the whole document creation process enables 
sophisticated document management with the text sql api as 
abstract interface this approach can be used by any tool and for any 
device 
the system is built on the following components see figure an 
editor in java implements the presentation layer a-g in figure 
the aim of this layer is the integration in a well-known 
wordprocessing application such as openoffice 
the business logic layer represents the interface between the 
database and the word-processing application it consists of the 
following three components the application server marked as as 
 - in figure enables text editing within the database 
environment and takes care of awareness security document 
management etc all within a collaborative real-time and multi-user 
environment the real-time server component marked as rtsc 
 in figure is responsible for the propagation of information i e 
updates between all of the connected editors 
the storage engine data layer primarily stores the content of 
documents as well as all related meta data within the database 
databases can be distributed in a peer-to-peer network db - in 
figure 
in the following we will briefly present the database schema the 
editor and the real-time server component as well as the concept of 
dynamic folders which enables sophisticated document 
management on the basis of meta data 
 application architecture 
a database-based real-time collaborative editor allows the same 
document to be opened and edited simultaneously on the same 
computer or over a network of several computers and mobile 
devices all concurrency issues as well as message propagation are 
solved within this approach while multiple instances of the same 
document are being opened each insert or delete action is a 
database transaction and as such is immediately stored persistently 
in the database and propagated to all clients working on the same 
document 
 database schema 
as it was mentioned earlier that text is stored in a native way each 
character of a text document is stored as a single object in the 
database when storing text in such a native form the 
performance of the employed database system is of crucial 
importance the concept and performance issues of such a text 
database are described in collaborative layouting in 
dynamic collaborative business processes within documents in 
the text editing creation time meta data model in and the relation 
to xml databases in 
figure depicts the core database schema by connecting a client to 
the database a session instance is created one important attribute 
of the session is the documentsession this attribute refers to 
documentsession instances which administrates all opened 
documents for each opened document a documentsession 
instance is created the documentsession is important for the 
realtime server component which in case of a 
 
is beforeis after 
char 
 id 
has 
textelement 
 id 
starts 
with 
is used 
by 
internalfile 
 id 
is in includes 
created 
at 
has 
inserted 
by 
inserted 
is active 
ir 
ir 
charactervalue 
 unicode 
has 
list 
 id 
starts 
starts 
with 
ends ends with 
filesize 
has 
user 
 id 
last read by 
last written by 
created 
at 
created by 
style 
dtd 
 id 
is used 
by 
uses 
uses 
is used 
by 
authors 
arehas 
description 
password 
picture 
usercolors 
userlistsecurity 
has 
has 
has 
has 
has 
has 
filenode 
 id 
references isreferencedby 
is dynamic dynstructure 
nodedetails 
has 
has is nodetype 
is parent 
of 
has 
parent 
has 
role 
 id 
created 
at 
created 
created 
by 
name 
has 
description 
is user 
name 
has 
has 
main role 
filenodeaccessmatrix 
 id 
has 
is 
accessmatrix 
read option 
grand option 
write option 
contains 
has 
access 
times 
opened   times with   by 
contains ispartof 
ir 
ir 
is andincludes 
lineage 
 id 
references 
is after 
is before 
copypaste 
 id 
references 
is in 
is copy 
of 
is a copy 
from 
hascopypaste 
 id 
is activelength has 
str stream 
has 
inserted by inserted 
regularchar 
startchar endchar 
file 
externalfile 
is from 
url 
type 
 extension 
is of 
title 
has 
documentsession 
 id 
is opened 
by 
has 
opened 
has 
opened 
session 
 id 
isconnectedwith 
launched by 
versionnumber 
uses 
has 
read option 
grand option 
write option 
ends with 
is used 
by 
is in has 
is unique 
dtd stream 
has 
has 
name 
column 
 id 
has set on 
on off 
isvisible for 
false 
languageprofile 
 id 
has 
contains 
name 
profile 
marking 
 id 
has 
parent 
internal 
is copy 
from 
hasrank 
is onposition 
starts 
with 
ends with 
is logical style 
is itemized 
is italic 
is enumerated 
is underline 
is 
is part of 
alignment 
size has 
font has 
hascolor 
is bold 
has 
uses 
elementname 
stylesheetname 
isused 
by 
process 
 id 
is running by os 
is web session 
mainroles 
roles has 
has 
timestamp 
 date time 
created 
at 
timestamp 
 date time 
timestamp 
 date time 
timestamp 
 date time 
timestamp 
 date time created 
at 
type 
has 
port 
ip 
has 
has 
messagepropagator 
 id 
picture 
 stream 
name 
picture 
 id 
has 
contains 
layoutblock workflowblocklogicalblock 
contains 
blockdatatype 
has 
property 
blockdata is of 
workflowinstance 
 id 
isin 
taskinstance 
 id 
has 
parent 
timestamp 
 date time 
timestamp 
 date time 
timestamp 
 date time 
timestamp 
 date time 
last modified at 
completed at 
started at 
created 
at 
is on 
has 
name 
created by 
has 
attached 
comment 
typeis of 
timestamp 
 date time 
timestamp 
 date time 
timestamp 
 date time 
created 
at 
started at 
 last modified at 
is 
category 
editors 
has 
status 
has 
timestamp 
 date time 
 status last modified 
timestamp 
 date time 
is due at 
duetype 
has 
timezone 
has 
notes 
has 
securitylevel 
hasset 
timestamp 
 date time 
 is completed at 
isfollowedby 
task 
 code 
description 
has 
indent 
references 
hasbeenopenedat by 
timestamp 
redohistory 
is before 
is after 
references 
hascharcounter 
is inhas 
has 
offset 
actionid 
 code 
timestamp 
 date time 
invoked 
at 
invoked 
by 
version 
 id 
isbuild 
from 
has 
created 
byarchived 
has 
comment 
timestamp 
 date time 
 createdat 
undohistory 
 id 
starts 
ends 
has 
name 
created 
by 
name 
has 
is before 
is after 
 references 
charcounter 
has 
is in 
created 
at 
timestamp 
is active 
created 
by 
is used 
by 
offset 
has 
created 
at 
timestamp 
index 
 id 
lastmodifiedby 
lexicon 
 id 
isof 
frequency 
is 
occurring 
is stop word 
term 
is 
is in 
ends with 
starts 
with 
 original starts with 
wordnumber 
sentencenumber 
paragraphnumber 
citatons 
has 
is in 
is 
is in 
istemporary 
is in 
has 
structure 
has 
elementpath 
createdat 
timestamp 
 describes 
spiderbuild 
 id 
is updated 
is deleted 
timestamp 
 date time 
 lastupdatedat 
has validated structure 
 neededtoindex 
time 
 ms 
indexupdate 
nextupdatein 
hasindexed 
isrunningbyos 
lastupdate 
enabled 
timestamp 
time 
 s 
documents 
stopcharacter 
description 
character 
value 
 ascii 
is sentence stop 
is paragraph stop 
name 
has 
is 
is 
optionssettings 
show information show warningsshow exceptions 
do lineage recording 
do internal lineage recording 
ask for unknown source 
show intra document 
lineage information 
are set 
for 
x 
x 
x 
virtualborder 
 id 
isonhas 
{ } 
{ } 
ir 
ir 
usermode 
 code 
usermode 
 code 
figure tendax database schema object role modeling diagram 
change on a document done by a client is responsible for sending 
update information to all the clients working on the same 
document the documentid in the class documentsession points 
to a filenode instance and corresponds to the id of the opened 
document instances of the class filenode either represent a 
folder node or a document node the folder node corresponds to a 
folder of a file system and the document node to that of a file 
instances of the class char represent the characters of a 
document the value of a character is stored in the attribute 
charactervalue the sequence is defined by the attributes after 
and before of the class char particular instances of char mark 
the beginning and the end of a document the methods 
insertchars and removechars are used to add and delete 
characters 
 editor 
as seen above each document is natively stored in the database 
our editor does not have a replica of one part of the native text 
database in the sense of database replicas instead it has a so-called 
image as its replica even if several authors edit the same text at the 
same time they work on one unique document at all times the 
system guarantees this unique view 
editing a document involves a number of steps first getting the 
required information out of the image secondly invoking the 
corresponding methods within the database thirdly changing the 
image and fourthly informing all other clients about the changes 
 real-time server component 
the real-time server component is responsible for the real-time 
propagation of any changes on a document done within an editor to 
all the editors who are working or have opened the same document 
when an editor connects to the application server which in turn 
connects to the database the database also establishes a connection 
to the real-time server component if there isn t already a 
connection the database system informs the real-time server 
component about each new editor session session which the 
realtime server component administrates in his sessionmanager then 
the editor as well connects to the real-time server component the 
real-time server component adds the editor socket to the client s 
data structure in the sessionmanager and is then ready to 
communicate 
each time a change on a document from an editor is persistently 
stored in the database the database sends a message to the real-time 
server component which in turns sends the changes to all the 
 
editors working on the same document therefore a special 
communication protocol is used the update protocol 
update protocol 
the real-time server component uses the update protocol to 
communicate with the database and the editors messages are sent 
from the database to the real-time server component which sends 
the messages to the affected editors the update protocol consists of 
different message types messages consist of two packages 
package one contains information for the real-time server 
component whereas package two is passed to the editors and 
contains the update information as depicted in figure 
 rtsc parameter   parameter editor data 
protocol between database system and 
real-time server component 
protocol between real -time server 
component and editors 
figure update protocol 
in the following two message types are presented 
 u sessionid sessionid editor data 
u update message sessionid id of the client session 
with this message type the real-time server component sends the 
editor data package to all editors specified in the sessionid list 
 ud fileid editor data 
ud update document message fileid id of the file 
with this message type the real-time server component sends the 
editor data to all editors who have opened the document with the 
indicated file-id 
class model 
figure depicts the class model as well as the environment of the 
real-time server component the environment consists mainly of the 
editor and the database but any other client application that could 
make use of the real-time server component can connect 
connectionlistener this class is responsible for the connection to 
the clients i e to the database and the editors depending on the 
connection type database or editor the connection is passed to an 
editorworker instance or databasemessageworker instance 
respectively 
editorworker this class manages the connections of type  editor 
the connection a socket and its input and output stream is stored 
in the sessionmanager 
sessionmanager this class is similar to an  in-memory database 
all editor session information e g the editor sockets which editor 
has opened which document etc are stored within this data 
structure 
databasemessageworker this class is responsible for the 
connections of type  database at run-time only one connection 
exists for each database update messages from the database are 
sent to the databasemessageworker and with the help of 
additional information from the sessionmanager sent to the 
corresponding clients 
serviceclass this class offers a set of methods for reading writing 
and logging messages 
tdb mp editor tdb mp database 
tdb mp mgmt 
editorworker 
databasemessageworker 
sessionmanager 
messagehandler 
connectionlistener 
serviceclass 
messagequeue 
tdb mp listener tdb mp service 
junit tests 
 
 
 
 
 
 
 
 
 
 
editors datenbanksystem 
 
 
 
 
 
 
 
tcp ip 
figure real-time server component class diagram 
 dynamic folders 
as mentioned above every editing action invoked by a user is 
immediately transferred to the database at the same time more 
information about the current transaction is gathered 
as all information is stored in the database one character can hold a 
multitude of information which can later be used for the retrieval of 
documents meta data is collected at character level from document 
structure layout workflow template semantics security 
workflow and notes on the level of a document section and on the 
level of the whole document 
all of the above-mentioned meta data is crucial information for 
creating content and knowledge out of word processing documents 
this meta data can be used to create an alternative storage system 
for documents in any case it is not an easy task to change users 
familiarity to the well known hierarchical file system this is also 
the main reason why we do not completely disregard the classical 
file system but rather enhance it folders which correspond to the 
classical hierarchical file system will be called static folders 
folders where the documents are organized according to meta data 
will be called dynamic folders as all information is stored in the 
database the file system too is based on the database 
the dynamic folders build up sub-trees which are guided by the 
meta data selected by the user thus the first step in using a 
dynamic folder is the definition of how it should be built for each 
level of a dynamic folder exactly one meta data item is used to the 
following example illustrates the steps which have to be taken in 
order to define a dynamic folder and the meta data which should be 
used 
as a first step the meta data which will be used for the dynamic 
folder must be chosen see table the sequence of the meta data 
influences the structure of the folder furthermore for each meta 
data used restrictions and granularity must be defined by the user 
if no restrictions are defined all accessible documents are listed 
the granularity therefore influences the number of sub-folders 
which will be created for the partitioning of the documents 
 
as the user enters the tree structure of the dynamic folder he can 
navigate through the branches to arrive at the document s he is 
looking for the directory names indicate which meta data 
determines the content of the sub-folder in question at each level 
the documents which have so far been found to match the meta 
data can be inspected 
table defining dynamic folders example 
level meta data restrictions granularity 
 creator only show documents 
which have been created 
by the users leone or 
hodel or gall 
one folder per 
creator 
 current 
location 
only show documents 
which where read at my 
current location 
one folder per 
task status 
 authors only show documents 
where at least was 
written by user  leone 
each one 
folder 
ad-hoc changes of granularity and restrictions are possible in order 
to maximize search comfort for the user it is possible to predefine 
dynamic folders for frequent use e g a location-based folder as 
well as to create and modify dynamic folders on an ad-hoc basis 
furthermore the content of such dynamic folders can change from 
one second to another depending on the changes made by other 
users at that moment 
 validation 
the proposed architecture is validated on the example of a character 
insertion insert operations are the mostly used operations in a 
 collaborative editing system the character insertion is based on 
the tendax insert algorithm which is formally described in the 
following the algorithm is simplified for this purpose 
 insert characters algorithm 
the symbol c stands for the object character p stands for the 
previous character n stands for the next character of a character 
object c and the symbol l stands for a list of character objects 
c character 
p previous character 
n next character 
l list of characters 
the symbol c stands for the first character in the list l ci stands 
for a character in the list l at the position i whereas i is a value 
between and the length of the list l and cn stands for the last 
character in the list l 
c first character in list l 
ci character at position i in list l 
cn last character in list l 
the symbol β stands for the special character that marks the 
beginning of a document and ε stands for the special character 
that marks the end of a document 
β beginning of document 
ε end of document 
the function startta starts a transaction 
startta start transaction 
the function committa commits a transaction that was started 
committa commit transaction 
the function checkwriteaccess checks if the write access for a 
document session s is granted 
checkwriteaccess s check if write access for document session 
s is granted 
the function lock acquires an exclusive lock for a character c and 
returns for a success and for no success 
lock c acquire the lock for character c 
success return no success return 
the function releaselocks releases all locks that a transaction has 
acquired so far 
releaselocks release all locks 
the function getprevious returns the previous character and 
getnext returns the next character of a character c 
getprevious c return previous character of character c 
getnext c return next character of character c 
the function linkbefore links a preceding character p with a 
succeeding character x and the function linkafter links a 
succeeding character n with a preceding character y 
linkbefore p x link character p to character x 
linkafter n y link character n to character y 
the function updatestring links a character p with the first 
character c of a character list l and a character n with the last 
character cn of a character list l 
updatestring l p n linkbefore p cl ∧ linkafter n cn 
the function insertchar inserts a character c in the table char 
with the fields after set to a character p and before set to a 
character n 
insertchar c p n linkafter c p ∧ linkbefore c n ∧ 
linkbefore p c ∧ linkafter n c 
the function checkpreceding determines the previous character s 
charactervalue of a character c and if the previous character s 
status is active 
checkpreceding c return status and charactervalue of the 
previous character 
the function checksucceeding determines the next character s 
charactervalue of a character c and if the next character s status is 
active 
 
checksucceeding c return status and charactervalue of the 
next character 
the function checkcharvalue determines the charactervalue of a 
character c 
checkcharvalue c return charactervalue of character c 
the function sendupdate sends an update message 
 updatemessage from the database to the real-time server 
component 
sendupdate updatemessage 
the function read is used in the real-time server component to 
read the updatemessage 
read updateinformationmessage 
the function allocateditors checks on the base of the 
updatemessage and the sessionmanager which editors have to 
be informed 
allocateeditors updateinformationmessage sessionmanager 
returns the affected editors 
the function sendmessage editordata sends the editor part of 
the updatemessage to the editors 
sendmessage editordata 
in tendax the insert algorithm is implemented in the class 
method insertchars of the class char which is depicted in figure 
 the relevant parameters for the definitions beneath are 
introduced in the following list 
- nextcharacteroid oid of the character situated next to the 
string to be inserted 
- previouscharacteroid oid of the character situated 
previously to the string to be inserted 
- characteroids list list of character which have to be 
inserted 
thus the insertion of characters can be defined stepwise as 
follows 
start a transaction 
startta 
select the character that is situated before the character that 
follows the string to be inserted 
getprevious nextcharacteroid prevchar prevcharoid ⇐ 
π after ϑoid nextcharacteroid char 
acquire the lock for the character that is situated in the document 
before the character that follows the string which shall be inserted 
lock prevcharid 
at this time the list characteroids contains the characters c to cn 
that shall be inserted 
characteroids { c   cn } 
each character of the string is inserted at the appropriate position 
by linking the preceding and the succeeding character to it 
for each character ci of characteroids 
insertchar ci p n 
whereas ci ∈ { c   cn } 
check if the preceding and succeeding characters are active or if it 
is the beginning or the end of the document 
checkpreceding prevcharoid isok isactive 
charactervalue ⇐ π isactive charactervalue ϑ oid 
nextcharacteroid char 
checksucceeding nextcharacteroid isok isactive 
charactervalue ⇐ π isactive charactervalue ϑ oid 
nextcharacteroid char 
update characters before and after the string to be inserted 
updatestring characteroids prevcharoid nextcharacteroid 
release all locks and commit transaction 
releaselocks 
committa 
send update information to the real-time server component 
sendupdate updatenmessage 
read update message and inform affected editors of the change 
read updatemessage 
allocate editors updatemessage sessionmanager 
sendmessage editordata 
 insert characters example 
figure gives a snapshot the system i e of its architecture four 
databases are distributed over a peer-to-peer network each 
database is connected to an application server as and each 
application server is connected to a real-time server component 
 rtsc editors are connected to one or more real-time server 
components and to the corresponding databases 
considering that editor a connected to database and and 
editor b connected to database and are working on the same 
document stored in database editor b now inserts a character 
into this document the insert operation is passed to application 
server which in turns passes it to the database where an 
insert operation is invoked the characters are inserted according 
to the algorithm discussed in the previous section after the 
insertion database sends an update message according to the 
update protocol discussed before to real-time server component 
 via as rtcs combines the received update information 
with the information in his sessionmanager and sends the editor 
data to the affected editors in this case to editor a and b where 
the changes are immediately shown 
occurring collaboration conflicts are solved and described in 
 summary 
with the approach presented in this paper and the implemented 
prototype we offer real-time collaborative editing and management 
of documents stored in a special way in a database with this 
approach we provide security consistency and availability of 
documents and consequently offer pervasive document editing and 
management pervasive document editing and management is 
enabled due to the proposed architecture with the embedded 
real 
time server component which propagates changes to a document 
immediately and consequently offers up-to-date documents 
document editing and managing is consequently enabled anywhere 
anytime and with any device 
the above-descried system is implemented in a running prototype 
the system will be tested soon in line with a student workshop next 
autumn 
references 
 abiteboul s agrawal r et al the lowell database 
research self assessment massachusetts usa 
 hodel t b businger d and dittrich k r supporting 
collaborative layouting in word processing ieee 
international conference on cooperative information 
systems coopis larnaca cyprus ieee 
 hodel t b and dittrich k r concept and prototype of a 
collaborative business process environment for document 
processing data knowledge engineering special 
issue collaborative business process technologies 
 
 hodel t b dubacher m and dittrich k r using 
database management systems for collaborative text 
editing acm european conference of 
computersupported cooperative work ecscw cew 
helsinki finland 
 hodel t b gall h and dittrich k r dynamic 
collaborative business processes within documents acm 
special interest group on design of communication 
 sigdoc memphis usa 
 hodel t b r hacmac and dittrich k r using text 
editing creation time meta data for document 
management conference on advanced information 
systems engineering caise porto portugal springer 
lecture notes 
 hodel t b specker f and dittrich k r embedded 
soap server on the operating system level for ad-hoc 
automatic real-time bidirectional communication 
information resources management association irma 
san diego usa 
 o kelly p revolution in real-time communication and 
collaboration for real this time application strategies 
in-depth research report burton group 
 
authority assignment in distributed multi-player 
proxy-based games 
sudhir aggarwal justin christofoli 
department of computer science 
florida state university tallahassee fl 
{sudhir christof} cs fsu edu 
sarit mukherjee sampath rangarajan 
center for networking research 
bell laboratories holmdel nj 
{sarit sampath} bell-labs com 
abstract 
we present a proxy-based gaming architecture and 
authority assignment within this architecture that can lead to 
better game playing experience in massively multi-player 
online games the proposed game architecture consists of 
distributed game clients that connect to game proxies referred 
to as communication proxies which forward game related 
messages from the clients to one or more game servers 
unlike proxy-based architectures that have been proposed in 
the literature where the proxies replicate all of the game 
state the communication proxies in the proposed 
architecture support clients that are in proximity to it in the 
physical network and maintain information about selected 
portions of the game space that are relevant only to the clients 
that they support using this architecture we propose an 
authority assignment mechanism that divides the 
authority for deciding the outcome of different actions events that 
occur within the game between client and servers on a per 
action event basis we show that such division of 
authority leads to a smoother game playing experience by 
implementing this mechanism in a massively multi-player online 
game called rpgquest in addition we argue that cheat 
detection techniques can be easily implemented at the 
communication proxies if they are made aware of the game-play 
mechanics 
categories and subject descriptors 
c computer-communication networks distributed 
systems-distributed applications 
general terms 
games performance 
 introduction 
in massively multi-player on-line games mmog game 
clients who are positioned across the internet connect to 
a game server to interact with other clients in order to be 
part of the game in current architectures these 
interactions are direct in that the game clients and the servers 
exchange game messages with each other in addition current 
mmogs delegate all authority to the game server to make 
decisions about the results pertaining to the actions that 
game clients take and also to decide upon the result of other 
game related events such centralized authority has been 
implemented with the claim that this improves the security 
and consistency required in a gaming environment 
a number of works have shown the effect of network latency 
on distributed multi-player games it has been 
shown that network latency has real impact on practical 
game playing experience some types of games can 
function quite well even in the presence of large delays for 
example shows that in a modern rpg called everquest 
 the breakpoint of the game when adding artificial 
latency was ms this is accounted to the fact that the 
combat system used in everquest is queueing based and 
has very low interaction for example a player queues up 
 or spells they wish to cast each of these spells take - 
seconds to actually perform giving the server plenty of time 
to validate these actions but there are other games such as 
fps games that break even in the presence of moderate 
network latencies latency compensation techniques have 
been proposed to alleviate the effect of latency but 
it is obvious that if mmogs are to increase in 
interactivity and speed more architectures will have to be developed 
that address responsiveness accuracy and consistency of the 
gamestate 
in this paper we propose two important features that would 
make game playing within mmogs more responsive for 
movement and scalable first we propose that centralized 
server-based architectures be made hierarchical through the 
introduction of communication proxies so that game updates 
made by clients that are time sensitive such as movement 
can be more efficiently distributed to other players within 
their game-space second we propose that assignment of 
authority in terms of who makes the decision on client 
actions such as object pickups and hits and collisions between 
players be distributed between the clients and the servers in 
order to distribute the computing load away from the central 
server in order to move towards more complex real-time 
networked games we believe that definitions of authority 
must be refined 
most currently implemented mmogs have game servers 
that have almost absolute authority we argue that there is 
no single consistent view of the virtual game space that can 
be maintained on any one component within a network that 
has significant latency such as the one that many mmog 
players would experience we believe that in most cases the 
client with the most accurate view of an entity is the best 
suited to make decisions for that entity when the causality 
of that action will not immediately affect any other 
players in this paper we define what it means to have authority 
within the context of events and objects in a virtual game 
space we then show the benefits of delegating authority 
for different actions and game events between the clients 
and server 
in our model the game space consists of game clients 
 representing the players and objects that they control we 
divide the client actions and game events we will 
collectively refer to these as events such as collisions hits etc 
into three different categories a events for which the game 
client has absolute authority b events for which the game 
server has absolute authority and c events for which the 
authority changes dynamically from client to the server and 
vice-versa depending on who has the authority that 
entity will make decisions on the events that happen within a 
game space we propose that authority for all decisions that 
pertain to a single player or object in the game that neither 
affects the other players or objects nor are affected by the 
actions of other players be delegated to that player s game 
client these type of decisions would include collision 
detection with static objects within the virtual game space and 
hit detection with linear path bullets whose trajectory is 
fixed and does not change with time fired by other players 
authority for decisions that could be affected by two or more 
players should be delegated to the impartial central server 
in some cases to ensure that no conflicts occur and in other 
cases can be delegated to the clients responsible for those 
players for example collision detection of two players that 
collide with each other and hit detection of non-linear 
bullets that changes trajectory with time should be delegated 
to the server decision on events such as item pickup for 
example picking up items in a game to accumulate points 
should be delegated to a server if there are multiple 
players within close proximity of an item and any one of the 
players could succeed in picking the item for item pick-up 
contention where the client realizes that no other player 
except its own player is within a certain range of the item 
the client could be delegated the responsibility to claim the 
item the client s decision can always be accurately verified 
by the server 
in summary we argue that while current authority models 
that only delegate responsibility to the server to make 
authoritative decisions on events is more secure than allowing 
the clients to make the decisions these types of models add 
undesirable delays to events that could very well be decided 
by the clients without any inconsistency being introduced 
into the game as networked games become more complex 
our architecture will become more applicable this 
architecture is applicable for massively multiplayer games where 
the speed and accuracy of game-play are a major concern 
while consistency between player game-states is still desired 
we propose that a mixed authority assignment mechanism 
such as the one outlined above be implemented in high 
interaction mmogs 
our paper has the following contributions first we propose 
an architecture that uses communication proxies to enable 
clients to connect to the game server a communication 
proxy in the proposed architecture maintains information 
only about portions of the game space that are relevant to 
clients connected to it and is able to process the movement 
information of objects and players within these portions 
in addition it is capable of multicasting this information 
only to a relevant subset of other communication proxies 
these functionalities of a communication proxy leads to a 
decrease in latency of event update and subsequently better 
game playing experience second we propose a mixed 
authority assignment mechanism as described above that 
improves game playing experience third we implement the 
proposed mixed authority assignment mechanism within a 
mmog called rpgquest to validate its viability within 
mmogs 
in section we describe the proxy-based game 
architecture in more detail and illustrate its advantages in 
section we provide a generic description of the mixed 
authority assignment mechanism and discuss how it improves 
game playing experience in section we show the 
feasibility of implementing the proposed mixed authority 
assignment mechanism within existing mmogs by describing a 
proof-of-concept implementation within an existing mmog 
called rpgquest section discusses related work in 
section we present our conclusions and discuss future work 
 proxy-based game architecture 
massively multi-player online games mmogs usually 
consist of a large game space in which the players and 
different game objects reside and move around and interact with 
each-other state information about the whole game space 
could be kept in a single central server which we would 
refer to as a central-server architecture but to alleviate 
the heavy demand on the processing for handling the large 
player population and the objects in the game in real-time a 
mmog is normally implemented using a distributed server 
architecture where the game space is further sub-divided 
into regions so that each region has relatively smaller 
number of players and objects that can be handled by a single 
server in other words the different game regions are hosted 
by different servers in a distributed fashion when a player 
moves out of one game region to another adjacent one the 
player must communicate with a different server than it was 
currently communicating with hosting the new region the 
servers communicate with one another to hand off a player 
or an object from one region to another in this model the 
player on the client machine has to establish multiple 
gaming sessions with different servers so that it can roam in the 
entire game space 
we propose a communication proxy based architecture where 
a player connects to a geographically nearby proxy instead 
of connecting to a central server in the case of a 
centralserver architecture or to one of the servers in case of 
dis the th workshop on network system support for games - netgames 
tributed server architecture in the proposed architecture 
players who are close by geographically join a particular 
proxy the proxy then connects to one or more game servers 
as needed by the set of players that connect to it and 
maintains persistent transport sessions with these server this 
alleviates the problem of each player having to connect 
directly to multiple game servers which can add extra 
connection setup delay introduction of communication proxies 
also mitigates the overhead of a large number of transport 
sessions that must be managed and reduces required network 
bandwidth and processing at the game servers both with 
central server and distributed server architectures with 
central server architectures communication proxies reduce 
the overhead at the server by not requiring the server to 
terminate persistent transport sessions from every one of the 
clients with distributed-server architectures additionally 
communication proxies eliminate the need for the clients to 
maintain persistent transport sessions to every one of the 
servers figure shows the proposed architecture 
figure architecture of the gaming environment 
note that the communication proxies need not be cognizant 
of the game they host a number of players and inform the 
servers which players are hosted by the proxy in question 
also note that the players hosted by a proxy may not be in 
the same game space that is a proxy hosts players that 
are geographically close to it but the players themselves 
can reside in different parts of the game space the proxy 
communicates with the servers responsible for maintaining 
the game spaces subscribed by the different players the 
proxies communicate with one another in a peer-to-peer to 
fashion the responsiveness of the game can be improved 
for updates that do not need to wait on processing at a 
central authority in this way information about players can 
be disseminated faster before even the game server gets to 
know about it this definitely improves the responsiveness 
of the game however it ignores consistency that is critical 
in mmorpgs the notion that an architecture such as this 
one can still maintain temporal consistency will be discussed 
in detail in section 
figure shows and example of the working principle of the 
proposed architecture assume that the game space is 
divided into regions and there are three servers responsible 
for managing the regions server s owns regions and 
s manages and and s is responsible for and 
 
figure an example 
there are four communication proxies placed in 
geographically distant locations players a b c join proxy p proxy p 
hosts players d e f players g h are with proxy p whereas 
players i j k l are with proxy p underneath each player 
the figure shows which game region the player is located 
currently for example players a b c are in regions 
respectively therefore proxy p must communicate with 
servers s and s the reader can verify the rest of the links 
between the proxies and the servers 
players can move within the region and between regions 
player movement within a region will be tracked by the 
proxy hosting the player and this movement information 
 for example the player s new coordinates will be 
multicast to a subset of other relevant communication proxies 
directly at the same time this information will be sent 
to the server responsible for that region with the indication 
that this movement has already been communicated to all 
the other relevant communication proxies so that the server 
does not have to relay this information to all the proxies 
for example if player a moves within region this 
information will be communicated by proxy p to server s and 
multicast to proxies p and p note that proxies that do 
not keep state information about this region at this point 
in time because they do not have any clients within that 
region such as p do not have to receive this movement 
information 
if a player is at the boundary of a region and moves into 
a new region there are two possibilities the first 
possibility is that the proxy hosting the player can identify the 
region into which the player is moving based on the 
trajectory information because it is also maintaining state 
information about the new region at that point in time in 
this case the proxy can update movement information 
directly at the other relevant communication proxies and also 
send information to the appropriate server informing of the 
movement this may require handoff between servers as we 
will describe consider the scenario where player a is at 
the boundary of region and proxy p can identify that the 
player is moving into region because proxy p is currently 
keeping state information about region it can inform all 
the th workshop on network system support for games - netgames 
the other relevant communication proxies in this example 
no other proxy maintains information about region at this 
point and so no update needs to be sent to any of the other 
proxies about this movement and then inform the server 
independently in this particular case server s is responsible 
for region as well and so no handoff between servers would 
be needed now consider another scenario where player j 
moves from region to region and that proxy p is able 
to identify this movement again because proxy p 
maintains state information about region it can inform any 
other relevant communication proxies again none in this 
example about this movement but now regions and 
are managed by different servers servers s and s 
respectively and thus a hand-off between these servers is needed 
we propose that in this particular scenario the handoff be 
managed by the proxy p itself when the proxy sends 
movement update to server s informing the server that 
the player is moving out of its region it would also send 
a message to server s informing the server of the presence 
and location of the player in one of its region 
in the intra-region and inter-region scenarios described above 
the proxy is able to manage movement related information 
update only the relevant communication proxies about the 
movement update the servers with the movement and 
enable handoff of a player between the servers if needed in 
this way the proxy performs movement updates without 
involving the servers in any way in this time-critical function 
thereby speeding up the game and improving game 
playing experience for the players we consider this the fast 
path for movement update we envision the proxies to be 
just communication proxies in that they do not know about 
the workings of specific games they merely process 
movement information of players and objects and communicate 
this information to the other proxies and the servers if the 
proxies are made more intelligent in that they understand 
more of the game logic it is possible for them to quickly 
check on claims made by the clients and mitigate cheating 
the servers could perform the same functionality but with 
more delay even without being aware of game logic the 
proxies can provide additional functionalities such as 
timestamping messages to make the game playing experience 
more accurate and fair 
the second possibility that should be considered is when 
players move between regions it is possible that a player 
moves from one region to another but the proxy that is 
hosting the player is not able to determine the region into 
which the player is moving a the proxy does not 
maintain state information about all the regions into which the 
player could potentially move or b the proxy is not able 
to determine which region the player may move into even if 
maintains state information about all these regions in this 
case we propose that the proxy be not responsible for 
making the movement decision but instead communicate the 
movement indication to the server responsible for the region 
within which the player is currently located the server will 
then make the movement decision and then a inform all 
the proxies including the proxy hosting the player and b 
initiate handoff with another server if the player moves into 
a region managed by another server we consider this the 
slow path for movement update in that the servers need 
to be involved in determining the new position of the player 
in the example assume that player a moves from region 
to region proxy p does not maintain state information 
about region and thus would pass the movement 
information to server s the server will identify that the player 
has moved into region and would inform proxy p as well 
as proxy p which is the only other proxy that maintains 
information about region at this point in time server s 
will also initiate a handoff of player a with server s proxy 
p will now start maintaining state information about 
region because one of its hosted players player a has moved 
into this region it will do so by requesting and receiving 
the current state information about region from server s 
which is responsible for this region 
thus a proxy architecture allows us to make use of faster 
movement updates through the fast path through a proxy if 
and when possible as opposed to conventional server-based 
architectures that always have to use the slow path through 
the server for movement updates by selectively maintaining 
relevant regional game state information at the proxies we 
are able to achieve this capability in our architecture without 
the need for maintaining the complete game state at every 
proxy 
 assignment of authority 
as a mmog is played the players and the game objects that 
are part of the game continually change their state for 
example consider a player who owns a tank in a battlefield 
game based on action of the player the tank changes its 
position in the game space the amount of ammunition the 
tank contains changes as it fires at other tanks the tank 
collects bonus firing power based on successful hits etc 
similarly objects in the battlefield such as flags buildings etc 
change their state when a flag is picked up by a player i e 
tank or a building is destroyed by firing at it that is 
some decision has to be made on the state of each player 
and object as the game progresses note that the state of 
a player and or object can contain several parameters e g 
position amount of ammunition fuel storage points 
collected etc and if any of the parameters changes the state 
of the player object changes 
in a client-server based game the server controls all the 
players and the objects when a player at a client machine 
makes a move the move is transmitted to the server over 
the network the server then analyzes the move and if 
the move is a valid one changes the state of the player at 
the server and informs the client of the change the client 
subsequently updates the state of the player and renders 
the player at the new location in this case the authority to 
change the state of the player resides with the server entirely 
and the client simply follows what the server instructs it to 
do 
most of the current first person shooter fps games and 
role playing games rpg fall under this category in 
current fps games much like in rpg games the client is not 
trusted all moves and actions that it makes are validated 
if a client detects that it has hit another player with a bullet 
it proceeds assuming that it is a hit meanwhile an update 
is sent to the server and the server will send back a message 
either affirming or denying that the player was hit if the 
remote player was not hit then the client will know that it 
 the th workshop on network system support for games - netgames 
did not actually make the shot if it did make the hit an 
update will also be sent from the server to the other clients 
informing them that the other player was hit a difference 
that occurs in some rpgs is that they use very dumb client 
programs some rpgs do not maintain state information 
at the client and therefore cannot predict anything such as 
hits at the client state information is not maintained 
because the client is not trusted with it in rpgs a cheating 
player with a hacked game client can use state information 
stored at the client to gain an advantage and find things 
such as hidden treasure or monsters lurking around the 
corner this is a reason why most mmorpgs do not send a 
lot of state information to the client and causes the game 
to be less responsive and have lower interaction game-play 
than fps games 
in a peer-to-peer game each peer controls the player and 
object that it owns when a player makes a move the 
peer machine analyzes the move and if it is a valid one 
changes the state of the player and places the player in new 
position afterwards the owner peer informs all other peers 
about the new state of the player and the rest of the peers 
update the state of the player in this scenario the authority 
to change the state of the player is given to the owning peer 
and all other peers simply follow the owner 
for example battle zone flag bzflag is a 
multiplayer client-server game where the client has all authority 
for making decisions it was built primarily with lan play 
in mind and cheating as an afterthought clients in bzflag 
are completely authoritative and when they detect that they 
were hit by a bullet they send an update to the server which 
simply forwards the message along to all other players the 
server does no sort of validation 
each of the above two traditional approaches has its own set 
of advantages and disadvantages the first approach which 
we will refer to as server authoritative henceforth uses a 
centralized method to assign authority while a centralized 
approach can keep the state of the game i e state of all the 
players and objects consistent across any number of client 
machines it suffers from delayed response in game-play as 
any move that a player at the client machine makes must go 
through one round-trip delay to the server before it can take 
effect on the client s screen in addition to the round-trip 
delay there is also queuing delay in processing the state change 
request at the server this can result in additional 
processing delay and can also bring in severe scalability problems 
if there are large number of clients playing the game one 
definite advantage of the server authoritative approach is 
that it can easily detect if a client is cheating and can take 
appropriate action to prevent cheating 
the peer-to-peer approach henceforth referred to as client 
authoritative can make games very responsive however 
it can make the game state inconsistent for a few players 
and tie break or roll back has to be performed to bring the 
game back to a consistent state neither tie break nor roll 
back is a desirable feature of online gaming for example 
assume that for a game the goal of each player is to collect 
as many flags as possible from the game space e g bzflag 
when two players in proximity try to collect the same flag 
at the same time depending on the algorithm used at the 
client-side both clients may determine that it is the winner 
although in reality only one player can pick the flag up both 
players will see on their screen that it is the winner this 
makes the state of the game inconsistent ways to recover 
from this inconsistency are to give the flag to only one player 
 using some tie break rule or roll the game back so that the 
players can try again neither of these two approaches is 
a pleasing experience for online gaming another problem 
with client authoritative approach is that of cheating by 
clients as there is no cross checking of the validation of the 
state changes authorized by the owner client 
we propose to use a hybrid approach to assign the authority 
dynamically between the client and the server that is we 
assign the authority to the client to make the game 
responsive and use the server s authority only when the client s 
individual authoritative decisions can make the game state 
inconsistent by moving the authority of time critical 
updates to the client we avoid the added delay caused by 
requiring the server to validate these updates for example 
in the flag pickup game the clients will be given the 
authority to pickup flags only when other players are not within 
a range that they could imminently pickup a flag only 
when two or more players are close by so that more than 
one player may claim to have picked up a flag the authority 
for movement and flag pickup would go to the central server 
so that the game state does not become inconsistent we 
believe that in a large game-space where a player is often 
in a very wide open and sparsely populated area such as 
those often seen in the game second life this hybrid 
architecture would be very beneficial because of the long 
periods that the client would have authority to send movement 
updates for itself this has two advantages over the 
centralauthority approach it distributes the processing load down 
to the clients for the majority of events and it allows for a 
more responsive game that does not need to wait on a server 
for validation 
we believe that our notion of authority can be used to 
develop a globally consistent state model of the evolution of 
a game fundamentally the consistent state of the system 
is the one that is defined by the server however if local 
authority is delegated to the client in this case the client s 
state is superimposed on the server s state to determine the 
correct global state for example if the client is 
authoritative with respect to movement of a player then the 
trajectory of the player is the true trajectory and must 
replace the server s view of the player s trajectory note that 
this could be problematic and lead to temporal 
inconsistency only if for example two or more entities are moving 
in the same region and can interact with each other in 
this situation the client authority must revert to the server 
and the sever would then make decisions thus the client 
is only authoritative in situations where there is no 
potential to imminently interact with other players we believe 
that in complex mmogs when allowing more rapid 
movement it will still be the case that local authority is possible 
for significant spans of game time note that it might also 
be possible to minimize the occurrences of the dead man 
shooting problem described in this could be done by 
allowing the client to be authoritative for more actions such 
as its player s own death and disallowing other players from 
making preemptive decisions based on a remote player 
the th workshop on network system support for games - netgames 
one reason why the client-server based architecture has gained 
popularity is due to belief that the fastest route to the other 
clients is through the server while this may be true we aim 
to create a new architecture where decisions do not always 
have to be made at the game server and the fastest route to 
a client is actually through a communication proxy located 
close to the client that is the shortest distance in our 
architecture is not through the game server but through the 
communication proxy after a client makes an action such 
as movement it will simultaneously distribute it directly to 
the clients and the game server by way of the 
communications proxy we note that our architecture however is not 
practical for a game where game players setup their own 
servers in an ad-hoc fashion and do not have access to 
proxies at the various isps this proxy and distributed authority 
architecture can be used to its full potential only when the 
proxies can be placed at strategic places within the main 
isps and evenly distributed geographically 
our game architecture does not assume that the client is 
not to be trusted we are designing our architecture on the 
fact that there will be sufficient cheat deterring and 
detection mechanisms present so that it will be both undesirable 
and very difficult to cheat in our proposed approach 
we can make the games cheat resilient by using the 
proxybased architecture when client authoritative decisions take 
place in order to achieve this the proxies have to be game 
cognizant so that decisions made by a client can be cross 
checked by a proxy that the client connects to for 
example assume that in a game a plane controlled by a client 
moves in the game space it is not possible for the plane to 
go through a building unharmed in a client authoritative 
mode it is possible for the client to cheat by maneuvering 
the plane through a building and claiming the plane to be 
unharmed however when such move is published by the 
client the proxy being aware of the game space that the 
plane is in can quickly check that the client has misused 
the authority and then can block such move this allows us 
to distribute authority to make decisions about the clients 
in the following section we use a multiplayer game called 
rpgquest to implement different authoritative schemes and 
discuss our experience with the implementation our 
implementation shows the viability of our proposed solution 
 implementation experience 
we have experimented with the authority assignment 
mechanism described in the last section by implementing the 
mechanisms in a game called rpgquest a screen shot from 
this game is shown in figure the purpose of the 
implementation is to test its feasibility in a real game rpgquest 
is a basic first person game where the player can move 
around a three dimensional environment objects are placed 
within the game world and players gain points for each 
object that is collected the game clients connect to a game 
server which allows many players to coexist in the same 
game world the basic functionality of this game is 
representative of current online first person shooter and role playing 
games the game uses the directx graphics api and 
directplay networking api in this section we will discuss the 
three different versions of the game that we experimented 
with 
figure the rpgquest game 
the first version of the game which is the original 
implementation of rpgquest was created with a completely 
authoritative server and a non-authoritative client authority 
given to the server includes decisions of when a player 
collides with static objects and other players and when a player 
picks up an object this version of the game performs well 
up to ms round-trip latency between the client and the 
server there is little lag between the time player hits a 
wall and the time the server corrects the player s position 
however as more latency is induced between the client and 
server the game becomes increasingly difficult to play with 
the increased latency the messages coming from the server 
correcting the player when it runs into a wall are not 
received fast enough this causes the player to pass through 
the wall for the period that it is waiting for the server to 
resolve the collision 
when studying the source code of the original version of 
the rpgquest game there is a substantial delay that is 
unavoidable each time an action must be validated by the 
server whenever a movement update is sent to the server 
the client must then wait whatever the round trip delay is 
plus some processing time at the server in order to receive 
its validated or corrected position this is obviously 
unacceptable in any game where movement or any other rapidly 
changing state information must be validated and 
disseminated to the other clients rapidly 
in order to get around this problem we developed a second 
version of the game which gives all authority to the client 
the client was delegated the authority to validate its own 
movement and the authority to pick up objects without 
validation from the server in this version of the game when 
a player moves around the game space the client validates 
that the player s new position does not intersect with any 
walls or static objects a position update is then sent to the 
server which then immediately forwards the update to the 
other clients within the region the update does not have 
to go through any extra processing or validation 
this game model of complete authority given to the client 
is beneficial with respect to movement when latencies of 
 the th workshop on network system support for games - netgames 
 ms and up are induced into the link between the client 
and server the game is still playable since time critical 
aspects of the game like movement do not have to wait on a 
reply from the server when a player hits a wall the 
collision is processed locally and does not have to wait on the 
server to resolve the collision 
although game playing experience with respect to 
responsiveness is improved when the authority for movement is 
given to the client there are still aspects of games that do 
not benefit from this approach the most important of these 
is consistency although actions such as movement are time 
critical other actions are not as time critical but instead 
require consistency among the player states an example of 
a game aspect that requires consistency is picking up objects 
that should only be possessed by a single player 
in our client authoritative version of rpgquest clients send 
their own updates to all other players whenever they pick up 
an object from our tests we have realized this is a problem 
because when there is a realistic amount of latency between 
the client and server it is possible for two players to pick 
up the same object at the same time when two players 
attempt to pick up an object at physical times which are 
close to each other the update sent by the player who picked 
up the object first will not reach the second player in time 
for it to see that the object has already been claimed the 
two players will now both think that they own the object 
this is why a server is still needed to be authoritative in this 
situation and maintain consistency throughout the players 
these two versions of the rpgquest game has showed us 
why it is necessary to mix the two absolute models of 
authority it is better to place authority on the client for quickly 
changing actions such as movement it is not desirable to 
have to wait for server validation on a movement that could 
change before the reply is even received it is also sometimes 
necessary to place consistency over efficiency in aspects of 
the game that cannot tolerate any inconsistencies such as 
object ownership we believe that as the interactivity of 
games increases our architecture of mixed authority that 
does not rely on server validation will be necessary 
to test the benefits and show the feasibility of our 
architecture of mixed authority we developed a third version of 
the rpgquest game that distributed authority for 
different actions between the client and server in this version 
in the interest of consistency the server remained 
authoritative for deciding who picked up an object the client 
was given full authority to send positional updates to other 
clients and verify its own position without the need to 
verify its updates with the server when the player tries to 
move their avatar the client verifies that the move will not 
cause it to move through a wall a positional update is then 
sent to the server which then simply forwards it to the other 
clients within the region this eliminates any extra 
processing delay that would occur at the server and is also a more 
accurate means of verification since the client has a more 
accurate view of its own state than the server 
this version of the rpgquest game where authority is 
distributed between the client and server is an improvement 
from the server authoritative version the client has no 
delay in waiting for an update for its own position and other 
clients do not have to wait on the server to verify the update 
the inconsistencies where two clients can pick up the same 
object in the client authoritative architecture are not present 
in this version of the client however the benefits of mixed 
authority will not truly be seen until an implementation of 
our communication proxy is integrated into the game with 
the addition of the communication proxy after the client 
verifies its own positional updates it will be able to send the 
update to all clients within its region through a low latency 
link instead of having to first go through the game server 
which could possibly be in a very remote location 
the coding of the different versions of the game was very 
simple the complexity of the client increased very slightly 
in the client authoritative and hybrid models the 
original dumb clients of rpgquest know the position of other 
players it is not just sent a screen snapshot from the server 
the server updates each client with the position of all nearby 
clients the dumb clients use client side prediction to fill 
in the gaps between the updates they receive the only 
extra processing the client has to do in the hybrid architecture 
is to compare its current position to the positions of all 
objects walls boxes etc in its area this obviously means 
that each client will have to already have downloaded the 
locations of all static objects within its current region 
 related work 
it has been noted that in addition to latency bandwidth 
requirements also dictate the type of gaming architecture to 
be used in different types of architectures are 
studied with respect to bandwidth efficiencies and latency it is 
pointed out that central server architectures are not 
scalable because of bandwidth requirements at the server but 
the overhead for consistency checks are limited as they are 
performed at the server a peer-to-peer architecture on the 
other hand is scalable but there is a significant overhead 
for consistency checks as this is required at every player 
the paper proposes a hybrid architecture which is 
peer-topeer in terms of message exchange and thereby is scalable 
where a central server is used for off-line consistency checks 
 thereby mitigating consistency check overhead the paper 
provides an implementation example of bzflag which is a 
peer-to-peer game which is modified to transfer all 
authority to a central server in essence this paper advocates an 
authority architecture which is server based even for 
peerto-peer games but does not consider division of authority 
between a client and a server to minimize latency which 
could affect game playing experience even with the type of 
latency found in server based games where all authority is 
with the server 
there is also previous work that has suggested that proxy 
based architectures be used to alleviate the latency 
problem and in addition use proxies to provide congestion 
control and cheat-proof mechanisms in distributed multi-player 
games in a proxy server-network architecture is 
presented that is aimed at improving scalability of 
multiplayer games and lowering latency in server-client data 
transmission the main goal of this work is to improve scalability 
of first-person shooter fps and rpg games the further 
objective is to improve the responsiveness mmogs by 
providing low latency communications between the client and 
the th workshop on network system support for games - netgames 
server the architecture uses interconnected proxy servers 
that each have a full view of the global game state proxy 
servers are located at various different isps it is mentioned 
in this work that dividing the game space among multiple 
games servers such as the federated model presented in 
is inefficient for a relatively fast game flow and that the 
proposed architecture alleviates this problem because users 
do not have to connect to a different server whenever they 
cross the server boundary this architecture still requires all 
proxies to be aware of the overall game state over the whole 
game space unlike our work where we require the proxies 
to maintain only partial state information about the game 
space 
fidelity based agent architectures have been proposed in 
 these works propose a distributed client-server 
architecture for distributed interactive simulations where 
different servers are responsible for different portions of the game 
space when an object moves from one portion to another 
there is a handoff from one server to another although 
these works propose an architecture where different portions 
of the simulation space are managed by different servers 
they do not address the issue of decreasing the bandwidth 
required through the use of communication proxies 
our work differs from the above discussed previous works by 
proposing a a distributed proxy-based architecture to 
decrease bandwidth requirements at the clients and the servers 
without requiring the proxies to keep state information about 
the whole game space b a dynamic authority assignment 
technique to reduce latency by performing consistency checks 
locally at the client whenever possible by splitting the 
authority between the clients and servers on a per object basis 
and c proposing that cheat detection can be built into the 
proxies if they are provided more information about the 
specific game instead of using them purely as communication 
proxies although this idea has not been implemented yet 
and is part of our future work 
 conclusions and future work 
in this paper we first proposed a proxy-based 
architecture for mmogs that enables mmogs to scale to a large 
number of users by mitigating the need for a large 
number of transport sessions to be maintained and decreasing 
both bandwidth overhead and latency of event update 
second we proposed a mixed authority assignment mechanism 
that divides authority for making decisions on actions and 
events within the game between the clients and server and 
argued how such an authority assignment leads to better 
game playing experience without sacrificing the consistency 
of the game third to validate the viability of the mixed 
authority assignment mechanism we implemented it within 
a mmog called rpgquest and described our 
implementation experience 
in future work we propose to implement the 
communications proxy architecture described in this paper and 
integrate the mixed authority mechanism within this 
architecture we propose to evaluate the benefits of the proxy-based 
architecture in terms of scalability accuracy and 
responsiveness we also plan to implement a version of the rpgquest 
game with dynamic assignment of authority to allow players 
the authority to pickup objects when no other players are 
near as discussed earlier this will allow for a more efficient 
and responsive game in certain situations and alleviate some 
of the processing load from the server 
also since so much trust is put into the clients of our 
architecture it will be necessary to integrate into the 
architecture many of the cheat detection schemes that have been 
proposed in the literature software such as punkbuster 
and a reputation system like those proposed by and 
would be integral to the operation of an architecture such as 
ours which has a lot of trust placed on the client we further 
propose to make the proxies in our architecture more game 
cognizant so that cheat detection mechanisms can be built 
into the proxies themselves 
 references 
 y w bernier latency compensation methods in 
client server in-game protocol design and 
optimization in proc of game developers 
conference 
 lothar pantel and lars c wolf on the impact of 
delay on real-time multiplayer games in nossdav 
 proceedings of the th international workshop on 
network and operating systems support for digital 
audio and video pages - new york ny usa 
 acm press 
 g armitage sensitivity of quake players to network 
latency in proc of imw workshop poster 
session november http www geocities com 
gj armitage q quake-results html 
 tobias fritsch hartmut ritter and jochen schiller 
the effect of latency and network limitations on 
mmorpgs a field study of everquest in netgames 
 proceedings of th acm sigcomm workshop on 
network and system support for games pages - 
new york ny usa acm press 
 tom beigbeder rory coughlan corey lusher john 
plunkett emmanuel agu and mark claypool the 
effects of loss and latency on user performance in 
unreal tournament in netgames 
proceedings of rd acm sigcomm workshop on 
network and system support for games pages 
 - new york ny usa acm press 
 y lin k guo and s paul sync-ms synchronized 
messaging service for real-time multi-player 
distributed games in proc of th ieee 
international conference on network protocols 
 icnp nov 
 katherine guo sarit mukherjee sampath 
rangarajan and sanjoy paul a fair message 
exchange framework for distributed multi-player 
games in netgames proceedings of the nd 
workshop on network and system support for games 
pages - new york ny usa acm press 
 t barron multiplayer game programming chapter 
 - pages - prima tech s game 
development series prima publishing 
 the th workshop on network system support for games - netgames 
 carsten griwodz and p˚al halvorsen the fun of using 
tcp for an mmorpg in nossdav proceedings of 
the international workshop on network and operating 
systems support for digital audio and video new 
york ny usa acm press 
 sudhir aggarwal hemant banavar amit khandelwal 
sarit mukherjee and sampath rangarajan accuracy 
in dead-reckoning based distributed multi-player 
games in netgames proceedings of rd acm 
sigcomm workshop on network and system support 
for games pages - new york ny usa 
acm press 
 sudhir aggarwal hemant banavar sarit mukherjee 
and sampath rangarajan fairness in dead-reckoning 
based distributed multi-player games in netgames 
 proceedings of th acm sigcomm workshop on 
network and system support for games pages - 
new york ny usa acm press 
 riker t et al bzflag http www bzflag org 
 - 
 linden lab second life http secondlife com 
 
 martin mauve how to keep a dead man from 
shooting in idms proceedings of the th 
international workshop on interactive distributed 
multimedia systems and telecommunication services 
pages - london uk springer-verlag 
 max skibinsky massively multiplayer game 
development chapter the quest for holy 
scalepart p p continuum pages - charles river 
media 
 joseph d pellegrino and constantinos dovrolis 
bandwidth requirement and state consistency in three 
multiplayer game architectures in netgames 
proceedings of the nd workshop on network and 
system support for games pages - new york 
ny usa acm press 
 m mauve j widmer and s fischer a generic proxy 
systems for networked computer games in proc of 
the workshop on network games netgames 
april 
 s gorlatch j muller s fischer and m mauve a 
proxy server network architecture for real-time 
computer games in euor-par parallel 
processing th international euro-par 
conference august-september 
 h hazeyama t limura and y kadobayashi zoned 
federation of game servers a peer-to-peer approach 
to scalable multiplayer on-line games in proc of 
acm workshop on network games netgames 
august-september 
 b kelly and s aggarwal a framework for a fidelity 
based agent architecture for distributed interactive 
simulation in proc th workshop on standards for 
distributed interactive simulation pages - 
march 
 s aggarwal and b kelly hierarchical structuring for 
distributed interactive simulation in proc th 
workshop on standards for distributed interactive 
simulation pages - sept 
 even balance inc punkbuster 
http www evenbalance com - 
 y wang and j vassileva trust and reputation 
model in peer-to-peer networks in third 
international conference on peer-to-peer computing 
 
the th workshop on network system support for games - netgames 
composition of a dids by integrating heterogeneous idss 
on grids 
paulo f silva and carlos b westphall and carla m westphall 
network and management laboratory 
department of computer science and statistics 
federal university of santa catarina florianópolis brazil 
marcos d assunção 
grid computing and distributed systems laboratory and nicta victoria laboratory 
department of computer science and software engineering 
the university of melbourne victoria australia 
{paulo westphal assuncao carla} lrg ufsc br 
abstract 
this paper considers the composition of a dids distributed 
intrusion detection system by integrating heterogeneous idss 
 intrusion detection systems a grid middleware is used for this 
integration in addition an architecture for this integration is 
proposed and validated through simulation 
categories and subject descriptors 
c distributed systes client server distributed 
applications 
 introduction 
solutions for integrating heterogeneous idss intrusion detection 
systems have been proposed by several groups 
some reasons for integrating idss are described by the idwg 
 intrusion detection working group from the ietf internet 
engineering task force as follows 
 many idss available in the market have strong and weak 
points which generally make necessary the deployment of 
more than one ids to provided an adequate solution 
 attacks and intrusions generally originate from multiple 
networks spanning several administrative domains these 
domains usually utilize different idss the integration of 
idss is then needed to correlate information from multiple 
networks to allow the identification of distributed attacks and 
or intrusions 
 the interoperability integration of different ids components 
would benefit the research on intrusion detection and speed 
up the deployment of idss as commercial products 
didss distributed intrusion detection systems therefore started 
to emerge in early s to allow the correlation of intrusion 
information from multiple hosts networks or domains to detect 
distributed attacks research on didss has then received much 
interest mainly because centralised idss are not able to provide 
the information needed to prevent such attacks 
however the realization of a dids requires a high degree of 
coordination computational grids are appealing as they enable 
the development of distributed application and coordination in a 
distributed environment grid computing aims to enable 
coordinate resource sharing in dynamic groups of individuals 
and or organizations moreover grid middleware provides means 
for secure access management and allocation of remote resources 
resource information services and protocols and mechanisms for 
transfer of data 
according to foster et al grids can be viewed as a set of 
aggregate services defined by the resources that they share ogsa 
 open grid service architecture provides the foundation for this 
service orientation in computational grids the services in ogsa 
are specified through well-defined open extensible and 
platformindependent interfaces which enable the development of 
interoperable applications 
this article proposes a model for integration of idss by using 
computational grids the proposed model enables heterogeneous 
idss to work in a cooperative way this integration is termed 
didsog distributed intrusion detection system on grid each 
of the integrated idss is viewed by others as a resource accessed 
through the services that it exposes a grid middleware provides 
several features for the realization of a didsog including 
decentralized coordination of resources use of standard protocols 
and interfaces and the delivery of optimized qos quality of 
service 
the service oriented architecture followed by grids ogsa 
allows the definition of interfaces that are adaptable to different 
platforms different implementations can be encapsulated by a 
service interface this virtualisation allows the consistent access to 
resources in heterogeneous environments the virtualisation of 
the environment through service interfaces allows the use of 
services without the knowledge of how they are actually 
implemented this characteristic is important for the integration 
of idss as the same service interfaces can be exposed by different 
idss 
grid middleware can thus be used to implement a great variety of 
services some functions provided by grid middleware are i 
data management services including access services replication 
and localisation ii workflow services that implement coordinate 
execution of multiple applications on multiple resources iii 
auditing services that perform the detection of frauds or 
intrusions iv monitoring services which implement the 
discovery of sensors in a distributed environment and generate 
alerts under determined conditions v services for identification 
of problems in a distributed environment which implement the 
correlation of information from disparate and distributed logs 
these services are important for the implementation of a didsog 
a dids needs services for the location of and access to 
distributed data from different idss auditing and monitoring 
services take care of the proper needs of the didss such as 
secure storage data analysis to detect intrusions discovery of 
distributed sensors and sending of alerts the correlation of 
distributed logs is also relevant because the detection of 
distributed attacks depends on the correlation of the alert 
information generated by the different idss that compose the 
didsog 
the next sections of this article are organized as follows section 
 presents related work the proposed model is presented in 
section section describes the development and a case study 
results and discussion are presented in section conclusions 
and future work are discussed in section 
 related work 
didma is a flexible scalable reliable and 
platformindependent dids didma architecture allows distributed 
analysis of events and can be easily extended by developing new 
agents however the integration with existing idss and the 
development of security components are presented as future work 
 the extensibility of dids didma and the integration with 
other idss are goals pursued by didsog the flexibility 
scalability platform independence reliability and security 
components discussed in are achieved in didsog by using a 
grid platform 
more efficient techniques for analysis of great amounts of data in 
wide scale networks based on clustering and applicable to didss 
are presented in the integration of heterogeneous idss to 
increase the variety of intrusion detection techniques in the 
environment is mentioned as future work didsog thus aims 
at integrating heterogeneous idss 
ref presents a hierarchical architecture for a dids 
information is collected aggregated correlated and analysed as it 
is sent up in the hierarchy the architecture comprises of several 
components for monitoring correlation intrusion detection by 
statistics detection by signatures and answers components in the 
same level of the hierarchy cooperate with one another the 
integration proposed by didsog also follows a hierarchical 
architecture each ids integrated to the didsog offers 
functionalities at a given level of the hierarchy and requests 
functionalities from idss from another level the hierarchy 
presented in integrates homogeneous idss whereas the 
hierarchical architecture of didsog integrates heterogeneous 
idss 
there are proposals on integrating computational grids and idss 
 ref and propose the use of globus 
toolkit for intrusion detection especially for dos denial of 
service and ddos distributed denial of service attacks 
globus is used due to the need to process great amounts of data to 
detect these kinds of attack a two-phase processing architecture 
is presented the first phase aims at the detection of momentary 
attacks while the second phase is concerned with chronic or 
perennial attacks 
traditional idss or didss are generally coordinated by a central 
point a characteristic that leaves them prone to attacks leu et al 
 point out that idss developed upon grids platforms are less 
vulnerable to attacks because of the distribution provided for such 
platforms leu et al have used tools to generate several 
types of attacks - including tcp icmp and udp flooding - and 
have demonstrated through experimental results the advantages of 
applying computational grids to idss 
this work proposes the development of a dids upon a grid 
platform however the resulting dids integrates heterogeneous 
idss whereas the didss upon grids presented by leu et al 
 do not consider the integration of heterogeneous idss the 
processing in phases is also contemplated by didsog 
which is enabled by the specification of several levels of 
processing allowed by the integration of heterogeneous idss 
the dids gida grid intrusion detection architecture targets 
at the detection of intrusions in a grid environment gridsim 
grid simulator was used for the validation of dids gida 
homogeneous resources were used to simplify the development 
 however the possibility of applying heterogeneous 
detection systems is left for future work 
another dids for grids is presented by choon and samsudim 
 scenarios demonstrating how a dids can execute on a grid 
environment are presented 
didsog does not aim at detecting intrusions in a grid 
environment in contrast didsog uses the grid to compose a 
dids by integrating specific idss the resulting dids could 
however be used to identify attacks in a grid environment local 
and distributed attacks can be detected through the integration of 
traditional idss while attacks particular to grids can be detected 
through the integration of grid idss 
 the proposed model 
didsog presents a hierarchy of intrusion detection services this 
hierarchy is organized through a two-dimensional vector defined 
by scope complexity the idss composing didsog can be 
organized in different levels of scope or complexity depending on 
its functionalities the topology of the target environment and 
expected results 
figure presents a didsog composed by different intrusion 
detection services i e data gathering data aggregation data 
correlation analysis intrusion response and management 
provided by different idss the information flow and the 
relationship between the levels of scope and complexity are 
presented in this figure 
information about the environment host network or application 
is collected by sensors located both in user s and user s 
computers in domain the information is sent to both simple 
analysers that act on the information from a single host level 
 and to aggregation and correlation services that act on 
information from multiple hosts from the same domain level 
simple analysers in the first scope level send the information to 
more complex analysers in the next levels of complexity level 
n when an analyser detects an intrusion it communicates with 
countermeasure and monitoring services registered to its scope 
an analyser can invoke a countermeasure service that replies to a 
detected attack or informs a monitoring service about the 
ongoing attack so the administrator can act accordingly 
aggregation and correlation resources in the second scope receive 
information from sensors from different users computers user 
 s and user s in the domain these resources process the 
received information and send it to the analysis resources 
registered to the first level of complexity in the second scope 
 level the information is also sent to the aggregation and 
correlation resources registered in the first level of complexity in 
the next scope level 
user 
domain 
analysers 
level 
local 
sensors 
analysers 
level n 
aggreg 
correlation 
level 
user 
domain 
local 
sensors 
analysers 
level 
analysers 
level n 
aggreg 
correlation 
level 
domain 
monitor 
level 
monitor 
level 
analysers 
level 
analysers 
level n 
monitor 
level 
response 
level 
response 
level 
response 
level 
fig how didsog works 
the analysis resources in the second scope act like the analysis 
resources in the first scope directing the information to a more 
complex analysis resource and putting the countermeasure and 
monitoring resources in action in case of detected attacks 
aggregation and correlation resources in the third scope receive 
information from domains and these resources then carry out 
the aggregation and correlation of the information from different 
domains and send it to the analysis resources in the first level of 
complexity in the third scope level the information could 
also be sent to the aggregate service in the next scope in case of 
any resources registered to such level 
the analysis resources in the third scope act similar to the analysis 
resources in the first and second scopes except that the analysis 
resources in the third scope act on information from multiple 
domains 
the functionalities of the registered resources in each of the 
scopes and complexity level can vary from one environment to 
another the model allows the development of n levels of scope 
and complexity 
figure presents the architecture of a resource participating in the 
didsog initially the resource registers itself to gis grid 
information service so other participating resources can query 
the services provided after registering itself the resource 
requests information about other intrusion detection resources 
registered to the gis 
a given resource of didsog interacts with other resources by 
receiving data from the source resources processing it and 
sending the results to the destination resources therefore 
forming a grid of intrusion detection resources 
grid resource 
basenative 
ids 
grid origin resources 
grid destination resources 
grid information service 
descri 
ptor 
connec 
tor 
fig architecture of a resource participating of the didsog 
a resource is made up of four components base connector 
descriptor and native ids native ids corresponds to the ids 
being integrated to the didsog this component process the data 
received from the origin resources and generates new data to be 
sent to the destination resources a native ids component can 
be any tool processes information related to intrusion detection 
including analysis data gathering data aggregation data 
correlation intrusion response or management 
the descriptor is responsible for the information that identifies a 
resource and its respective destination resources in the didsog 
figure presents the class diagram of the stored information by 
the descriptor the resourcedescriptor class has feature level 
datatype and target resources type members feature class 
represents the functionalities that a resource has type name and 
version attributes refer to the functions offered by the native ids 
component its name and version respectively level class 
identifies the level of target and complexity in which the resource 
acts datatype class represents the data format that the resource 
accepts to receive datatype class is specialized by classes text 
xml and binary class xml contains the dtdfile attribute to 
specify the dtd file that validates the received xml 
-ident 
-version 
-description 
resourcedescriptor 
-featuretype 
-name 
-version 
feature 
 
 
-type 
-version 
datatype 
-escope 
-complex 
level 
 
 
text binary 
-dtdfile 
xml 
 
 
targetresources 
 
 
-featuretype 
resource 
 
 
fig class diagram of the descriptor component 
targetresources class represents the features of the destination 
resources of a determined resource this class aggregates 
resource the resource class identifies the characteristics of a 
destination resource this identification is made through the 
featuretype attribute and the level and datatype classes 
a given resource analyses the information from descriptors from 
other resources and compares this information with the 
information specified in targetresources to know to which 
resources to send the results of its processing 
the base component is responsible for the communication of a 
resource with other resources of the didsog and with the grid 
information service it is this component that registers the 
resource and the queries other resources in the gis 
the connector component is the link between base and native 
ids the information that base receives from origin resources is 
passed to connector component the connector component 
performs the necessary changes in the data so that it is understood 
by native ids and sends this data to native ids for processing 
the connector component also has the responsibility of collecting 
the information processed by native ids and making the 
necessary changes so the information can pass through the 
didsog again after these changes connector sends the 
information to the base which in turn sends it to the destination 
resources in accordance with the specifications of the descriptor 
component 
 implementation 
we have used gridsim toolkit for development and 
evaluation of the proposed model we have used and extended 
gridsim features to model and simulate the resources and 
components of didsog 
figure presents the class diagram of the simulated didsog 
the simulation didsog class starts the simulation components 
the simulation user class represents a user of didsog this 
class function is to initiate the processing of a resource sensor 
from where the gathered information will be sent to other 
resources didsog gis keeps a registry of the didsog 
resources the didsog baseresource class implements the base 
component see figure didsog baseresource interacts with 
didsog descriptor class which represents the descriptor 
component the didsog descriptor class is created from an 
xml file that specifies a resource descriptor see figure 
didsog baseresource 
didsog descriptor 
 
didsog gis 
simulation user 
simulation didsog 
 
 
 
 
gridinformationservice 
gridsim gridresource 
fig class diagram of the simulatated didsog 
a connector component must be developed for each native ids 
integrated to didsog the connector component is implemented 
by creating a class derived from didsog baseresource the new 
class will implement new functionalities in accordance with the 
needs of the corresponding native ids 
in the simulation environment data collection resources analysis 
aggregation correlation and generation of answers were 
integrated classes were developed to simulate the processing of 
each native ids components associated to the resources for each 
simulated native ids a class derived from 
didsog baseresource was developed this class corresponds to 
the connector component of the native ids and aims at the 
integrating the ids to didsog 
a xml file describing each of the integrated resources is chosen 
by using the connector component the resulting relationship 
between the resources integrated to the didsog in accordance 
with the specification of its respective descriptors is presented in 
figure 
the sensor and sensor resources generate simulated data in 
the tcpdump format the generated data is directed to 
analyser and aggreg corr resources in the case of 
sensor and to aggreg corr in the case of sensor 
according to the specification of their descriptors 
user 
analyser 
 
level 
sensor 
aggreg 
corr 
level 
user 
sensor 
analyser 
level 
analyser 
level 
tcpdump 
tcpdump 
tcpdumpag 
tcpdumpag 
idmef 
idmef 
idmef 
tcpdump 
 
countermeasure 
level 
 
countermeasure 
level 
fig flow of the execution of the simulation 
the native ids of analyser generates alerts for any attempt of 
connection to port the data received from analyser had 
presented such features generating an idmef intrusion 
detection message exchange format alert the generated 
alert was sent to countermeasure resource where a warning 
was dispatched to the administrator informing him of the alert 
received 
the aggreg corr resource received the information generated 
by sensors and its processing activities consist in correlating 
the source ip addresses with the received data the resultant 
information of the processing of aggreg corr was directed to 
the analyser resource 
the native ids component of the analyser generates alerts 
when a source tries to connect to the same port number of 
multiple destinations this situation is identified by the 
analyser in the data received from aggreg corr and an alert 
in idmef format is then sent to the countermeasures resource 
in addition to generating alerts in idmef format analyser also 
directs the received data to the analyser in the level of 
complexity the native ids component of analyser 
generates alerts when the transmission of icmp messages from a 
given source to multiple destinations is detected this situation is 
detected in the data received from analyser and an idmef 
alert is then sent to the countermeasure resource 
the countermeasure resource receives the alerts generated by 
analysers and in accordance with the implementation of its 
native ids component warnings on alerts received are 
dispatched to the administrator 
the simulation carried out demonstrates how didsog works 
simulated data was generated to be the input for a grid of 
intrusion detection systems composed by several distinct 
resources the resources carry out tasks such as data collection 
aggregation and analysis and generation of alerts and warnings in 
an integrated manner 
 experiment results 
the hierarchic organization of scope and complexity provides a 
high degree of flexibility to the model the didsog can be 
modelled in accordance with the needs of each environment the 
descriptors define data flow desired for the resulting dids 
each native ids is integrated to the didsog through a 
connector component the connector component is also flexible 
in the didsog adaptations conversions of data types and 
auxiliary processes that native idss need are provided by the 
connector filters and generation of specific logs for each native 
ids or environment can also be incorporated to the connector 
if the integration of a new ids to an environment already 
configured is desired it is enough to develop the connector for 
the desired ids and to specify the resource descriptor after the 
specification of the connector and the descriptor the new ids is 
integrated to the didsog 
through the definition of scopes resources can act on data of 
different source groups for example scope can be related to a 
given set of hosts scope to another set of hosts while scope 
can be related to hosts from scopes and scopes can be defined 
according to the needs of each environment 
the complexity levels allow the distribution of the processing 
between several resources inside the same scope in an analysis 
task for example the search for simple attacks can be made by 
resources of complexity whereas the search for more complex 
attacks that demands more time can be performed by resources 
of complexity with this the analysis of the data is made by two 
resources 
the distinction between complexity levels can also be organized 
in order to integrate different techniques of intrusion detection 
the complexity level could be defined for analyses based on 
signatures which are simpler techniques the complexity level 
for techniques based on behaviour that require greater 
computational power and the complexity level for intrusion 
detection in applications where the techniques are more specific 
and depend on more data 
the division of scopes and the complexity levels make the 
processing of the data to be carried out in phases no resource has 
full knowledge about the complete data processing flow each 
resource only knows the results of its processing and the 
destination to which it sends the results resources of higher 
complexity must be linked to resources of lower complexity 
therefore the hierarchic structure of the didsog is maintained 
facilitating its extension and integration with other domains of 
intrusion detection 
by carrying out a hierarchic relationship between the several 
chosen analysers for an environment the sensor resource is not 
overloaded with the task to send the data to all the analysers an 
initial analyser will exist complexity level to which the sensor 
will send its data and this analyser will then direct the data to the 
next step of the processing flow another feature of the 
hierarchical organization is the easy extension and integration 
with other domains if it is necessary to add a new host sensor to 
the didsog it is enough to plug it to the first hierarchy of 
resources if it is necessary to add a new analyser it will be in the 
scope of several domains it is enough to relate it to another 
resource of same scope 
the didsog allows different levels to be managed by different 
entities for example the first scope can be managed by the local 
user of a host the second scope comprising several hosts of a 
domain can be managed by the administrator of the domain a 
third entity can be responsible for managing the security of 
several domains in a joint way this entity can act in the scope 
independently from others 
with the proposed model for integration of idss in grids the 
different idss of an environment or multiple idss integrated act 
in a cooperative manner improving the intrusion detection 
services mainly in two aspects first the information from 
multiple sources are analysed in an integrated way to search for 
distributed attacks this integration can be made under several 
scopes second there is a great diversity of data aggregation 
techniques data correlation and analysis and intrusion response 
that can be applied to the same environment these techniques can 
be organized under several levels of complexity 
 conclusion 
the integration of heterogeneous idss is important however the 
incompatibility and diversity of ids solutions make such 
integration extremely difficult this work thus proposed a model 
for composition of dids by integrating existing idss on a 
computational grid platform didsog idss in didsog are 
encapsulated as grid services for intrusion detection a 
computational grid platform is used for the integration by 
providing the basic requirements for communication localization 
resource sharing and security mechanisms 
the components of the architecture of the didsog were 
developed and evaluated using the gridsim grid simulator 
services for communication and localization were used to carry 
out the integration between components of different resources 
based on the components of the architecture several resources 
were modelled forming a grid of intrusion detection the 
simulation demonstrated the usefulness of the proposed model 
data from the sensor resources was read and this data was used to 
feed other resources of didsog 
the integration of distinct idss could be observed through the 
simulated environment resources providing different intrusion 
detection services were integrated e g analysis correlation 
aggregation and alert the communication and localization 
services provided by gridsim were used to integrate components 
of different resources various resources were modelled following 
the architecture components forming a grid of intrusion detection 
the components of didsog architecture have served as base for 
the integration of the resources presented in the simulation 
during the simulation the different idss cooperated with one 
another in a distributed manner however in a coordinated way 
with an integrated view of the events having thus the capability 
to detect distributed attacks this capability demonstrates that the 
idss integrated have resulted in a dids 
related work presents cooperation between components of a 
specific dids some work focus on either the development of 
didss on computational grids or the application of idss to 
computational grids however none deals with the integration of 
heterogeneous idss in contrast the proposed model developed 
and simulated in this work can shed some light into the question 
of integration of heterogeneous idss 
didsog presents new research opportunities that we would like 
to pursue including deployment of the model in a more realistic 
environment such as a grid incorporation of new security 
services parallel analysis of data by native idss in multiple 
hosts 
in addition to the integration of idss enabled by a grid 
middleware the cooperation of heterogeneous idss can be 
viewed as an economic problem idss from different 
organizations or administrative domains need incentives for 
joining a grid of intrusion detection services and for collaborating 
with other idss the development of distributed strategy proof 
mechanisms for integration of idss is a challenge that we would 
like to tackle 
 references 
 sulistio a poduvaly g buyya r and tham ck 
constructing a grid simulation with differentiated network 
service using gridsim proc of the th international 
conference on internet computing icomp june - 
 las vegas usa 
 choon o t samsudim a grid-based intrusion detection 
system the th 
ieee asia-pacific conference 
communications september 
 foster i kesselman c tuecke s the physiology of the 
grid an open grid service architecture for distributed 
system integration draft june available at 
http www globus org research papers ogsa pdf access feb 
 
 foster ian kesselman carl tuecke steven the anatomy 
of the grid enabling scalable virtual organizations 
international journal of supercomputer applications 
 kannadiga p zulkernine m didma a distributed 
intrusion detection system using mobile agents 
proceedings of the ieee sixth international conference on 
software engineering artificial intelligence networking 
and parallel distributed computing may 
 leu fang-yie et al integrating grid with intrusion 
detection proceedings of th 
ieee aina march 
 leu fang-yie et al a performance-based grid intrusion 
detection system proceedings of the th 
ieee 
compsac july 
 mccanne s leres c jacobson v tcpdump libpcap 
http www tcpdump org 
 snapp s r et al dids distributed intrusion detection 
system - motivation architecture and an early prototype 
proceeding of the fifteenth ieee national computer 
security conference baltimore md october 
 sterne d et al a general cooperative intrusion detection 
architecture for manets proceedings of the third ieee 
iwia march 
 tolba m f et al gida toward enabling grid intrusion 
detection systems th ieee international symposium on 
cluster computing and the grid may 
 wood m intrusion detection message exchange 
requirements draft-ietf-idwg-requirements- october 
 available at 
http www ietf org internet-drafts draftietf-idwg-requirements- txt access march 
 zhang yu-fang xiong z wang x distributed intrusion 
detection based on clustering proceedings of ieee 
international conference machine learning and cybernetics 
august 
 curry d debar h intrusion detection message exchange 
format data model and extensible markup language xml 
document type definition draft-ietf-idwg-idmef-xml- 
march available at 
http www ietf org internetdrafts draft-ietf-idwg-idmef-xml- txt 
remote access to large spatial databases ∗ 
egemen tanin 
frantiˇsek brabec 
hanan samet 
computer science department 
center for automation research 
institute for advanced computer studies 
university of maryland college park md 
{egemen brabec hjs} umiacs umd edu 
www cs umd edu { egemen  brabec  hjs} 
abstract 
enterprises in the public and private sectors have been 
making their large spatial data archives available over the 
internet however interactive work with such large volumes 
of online spatial data is a challenging task we propose 
two efficient approaches to remote access to large spatial 
data first we introduce a client-server architecture where 
the work is distributed between the server and the 
individual clients for spatial query evaluation data visualization 
and data management we enable the minimization of the 
requirements for system resources on the client side while 
maximizing system responsiveness as well as the number of 
connections one server can handle concurrently second for 
prolonged periods of access to large online data we 
introduce appoint an approach for peer-to-peer oﬄoading 
the internet this is a centralized peer-to-peer approach 
that helps internet users transfer large volumes of online 
data efficiently in appoint active clients of the 
clientserver architecture act on the server s behalf and 
communicate with each other to decrease network latency improve 
service bandwidth and resolve server congestions 
categories and subject descriptors 
c computer-communication networks 
distributed systems-client server distributed applications 
distributed databases h database management 
database applications-spatial databases and gis 
general terms 
performance management 
 introduction 
in recent years enterprises in the public and private 
sectors have provided access to large volumes of spatial data 
over the internet interactive work with such large volumes 
of online spatial data is a challenging task we have been 
developing an interactive browser for accessing spatial online 
databases the sand spatial and non-spatial data 
internet browser users of this browser can interactively and 
visually manipulate spatial data remotely unfortunately 
interactive remote access to spatial data slows to a crawl 
without proper data access mechanisms we developed two 
separate methods for improving the system performance 
together form a dynamic network infrastructure that is highly 
scalable and provides a satisfactory user experience for 
interactions with large volumes of online spatial data 
the core functionality responsible for the actual database 
operations is performed by the server-based sand system 
sand is a spatial database system developed at the 
university of maryland the client-side sand internet 
browser provides a graphical user interface to the facilities 
of sand over the internet users specify queries by 
choosing the desired selection conditions from a variety of menus 
and dialog boxes 
sand internet browser is java-based which makes it 
deployable across many platforms in addition since java has 
often been installed on target computers beforehand our 
clients can be deployed on these systems with little or no 
need for any additional software installation or 
customization the system can start being utilized immediately 
without any prior setup which can be extremely beneficial in 
time-sensitive usage scenarios such as emergencies 
there are two ways to deploy sand first any standard 
web browser can be used to retrieve and run the client piece 
 sand internet browser as a java application or an applet 
this way users across various platforms can continuously 
access large spatial data on a remote location with little or 
 
no need for any preceding software installation the second 
option is to use a stand-alone sand internet browser along 
with a locally-installed internet-enabled database 
management system server piece in this case the sand internet 
browser can still be utilized to view data from remote 
locations however frequently accessed data can be downloaded 
to the local database on demand and subsequently accessed 
locally power users can also upload large volumes of spatial 
data back to the remote server using this enhanced client 
we focused our efforts in two directions we first aimed at 
developing a client-server architecture with efficient caching 
methods to balance local resources on one side and the 
significant latency of the network connection on the other the 
low bandwidth of this connection is the primary concern in 
both cases the outcome of this research primarily addresses 
the issues of our first type of usage i e as a remote browser 
application or an applet for our browser and other similar 
applications the second direction aims at helping users 
that wish to manipulate large volumes of online data for 
prolonged periods we have developed a centralized 
peerto-peer approach to provide the users with the ability to 
transfer large volumes of data i e whole data sets to the 
local database more efficiently by better utilizing the 
distributed network resources among active clients of a 
clientserver architecture we call this architecture 
appointapproach for peer-to-peer oﬄoading the internet the 
results of this research addresses primarily the issues of the 
second type of usage for our sand internet browser i e 
as a stand-alone application 
the rest of this paper is organized as follows section 
describes our client-server approach in more detail section 
focuses on appoint our peer-to-peer approach section 
discusses our work in relation to existing work section 
outlines a sample sand internet browser scenario for both 
of our remote access approaches section contains 
concluding remarks as well as future research directions 
 the client-server approach 
traditionally geographic information systems gis 
such as arcinfo from esri and many spatial databases 
are designed to be stand-alone products the spatial 
database is kept on the same computer or local area network 
from where it is visualized and queried this architecture 
allows for instantaneous transfer of large amounts of data 
between the spatial database and the visualization module 
so that it is perfectly reasonable to use large-bandwidth 
protocols for communication between them there are however 
many applications where a more distributed approach is 
desirable in these cases the database is maintained in one 
location while users need to work with it from possibly distant 
sites over the network e g the internet these connections 
can be far slower and less reliable than local area networks 
and thus it is desirable to limit the data flow between the 
database server and the visualization unit client in order 
to get a timely response from the system 
our client-server approach figure allows the actual 
database engine to be run in a central location maintained 
by spatial database experts while end users acquire a 
javabased client component that provides them with a gateway 
into the sand spatial database engine 
our client is more than a simple image viewer instead it 
operates on vector data allowing the client to execute many 
operations such as zooming or locational queries locally in 
figure sand internet browser - client-server 
architecture 
essence a simple spatial database engine is run on the client 
this database keeps a copy of a subset of the whole database 
whose full version is maintained on the server this is a 
concept similar to  caching in our case the client acts as 
a lightweight server in that given data it evaluates queries 
and provides the visualization module with objects to be 
displayed it initiates communication with the server only 
in cases where it does not have enough data stored locally 
since the locally run database is only updated when 
additional or newer data is needed our architecture allows the 
system to minimize the network traffic between the client 
and the server when executing the most common user-side 
operations such as zooming and panning in fact as long 
as the user explores one region at a time i e he or she is 
not panning all over the database no additional data needs 
to be retrieved after the initial population of the client-side 
database this makes the system much more responsive 
than the web mapping services due to the complexity of 
evaluating arbitrary queries i e more complex queries than 
window queries that are needed for database visualization 
we do not perform user-specified queries on the client all 
user queries are still evaluated on the server side and the 
results are downloaded onto the client for display however 
assuming that the queries are selective enough i e there are 
far fewer elements returned from the query than the number 
of elements in the database the response delay is usually 
within reasonable limits 
 client-server communication 
as mentioned above the sand internet browser is a 
client piece of the remotely accessible spatial database server 
built around the sand kernel in order to communicate 
with the server whose application programming interface 
 api is a tcl-based scripting language a servlet specifically 
designed to interface the sand internet browser with the 
sand kernel is required on the server side this servlet 
listens on a given port of the server for incoming requests from 
the client it translates these requests into the sand-tcl 
language next it transmits these sand-tcl commands or 
scripts to the sand kernel after results are provided by 
the kernel the servlet fetches and processes them and then 
sends those results back to the originating client 
once the java servlet is launched it waits for a client to 
initiate a connection it handles both requests for the actual 
client java code needed when the client is run as an applet 
and the sand traffic when the client piece is launched 
it connects back to the sand servlet the communication 
is driven by the client piece the server only responds to 
the client s queries the client initiates a transaction by 
 
sending a query the java servlet parses the query and 
creates a corresponding sand-tcl expression or script in 
the sand kernel s native format it is then sent to the 
kernel for evaluation or execution the kernel s response 
naturally depends on the query and can be a boolean value 
a number or a string representing a value e g a default 
color or a whole tuple e g in response to a nearest tuple 
query if a script was sent to the kernel e g requesting 
all the tuples matching some criteria then an arbitrary 
amount of data can be returned by the sand server in this 
case the data is first compressed before it is sent over the 
network to the client the data stream gets decompressed 
at the client before the results are parsed 
notice that if another spatial database was to be used 
instead of the sand kernel then only a simple 
modification to the servlet would need to be made in order for the 
sand internet browser to function properly in 
particular the queries sent by the client would need to be recoded 
into another query language which is native to this different 
spatial database the format of the protocol used for 
communication between the servlet and the client is unaffected 
 the peer-to-peer approach 
many users may want to work on a complete spatial data 
set for a prolonged period of time in this case making an 
initial investment of downloading the whole data set may be 
needed to guarantee a satisfactory session unfortunately 
spatial data tends to be large a few download requests 
to a large data set from a set of idle clients waiting to be 
served can slow the server to a crawl this is due to the fact 
that the common client-server approach to transferring data 
between the two ends of a connection assumes a designated 
role for each one of the ends i e some clients and a server 
we built appoint as a centralized peer-to-peer system 
to demonstrate our approach for improving the common 
client-server systems a server still exists there is a 
central source for the data and a decision mechanism for the 
service the environment still functions as a client-server 
environment under many circumstances yet unlike many 
common client-server environments appoint maintains 
more information about the clients this includes 
inventories of what each client downloads their availabilities etc 
when the client-server service starts to perform poorly or 
a request for a data item comes from a client with a poor 
connection to the server appoint can start appointing 
appropriate active clients of the system to serve on behalf 
of the server i e clients who have already volunteered their 
services and can take on the role of peers hence moving 
from a client-server scheme to a peer-to-peer scheme the 
directory service for the active clients is still performed by 
the server but the server no longer serves all of the requests 
in this scheme clients are used mainly for the purpose of 
sharing their networking resources rather than introducing 
new content and hence they help oﬄoad the server and scale 
up the service the existence of a server is simpler in terms 
of management of dynamic peers in comparison to pure 
peerto-peer approaches where a flood of messages to discover 
who is still active in the system should be used by each peer 
that needs to make a decision the server is also the main 
source of data and under regular circumstances it may not 
forward the service 
data is assumed to be formed of files a single file forms 
the atomic means of communication appoint optimizes 
requests with respect to these atomic requests frequently 
accessed data sets are replicated as a byproduct of having 
been requested by a large number of users this opens up 
the potential for bypassing the server in future downloads for 
the data by other users as there are now many new points of 
access to it bypassing the server is useful when the server s 
bandwidth is limited existence of a server assures that 
unpopular data is also available at all times the service 
depends on the availability of the server the server is now 
more resilient to congestion as the service is more scalable 
backups and other maintenance activities are already 
being performed on the server and hence no extra 
administrative effort is needed for the dynamic peers if a peer goes 
down no extra precautions are taken in fact appoint 
does not require any additional resources from an already 
existing client-server environment but instead expands its 
capability the peers simply get on to or get off from a table 
on the server 
uploading data is achieved in a similar manner as 
downloading data for uploads the active clients can again be 
utilized users can upload their data to a set of peers other 
than the server if the server is busy or resides in a distant 
location eventually the data is propagated to the server 
all of the operations are performed in a transparent 
fashion to the clients upon initial connection to the server 
they can be queried as to whether or not they want to share 
their idle networking time and disk space the rest of the 
operations follow transparently after the initial contact 
appoint works on the application layer but not on lower 
layers this achieves platform independence and easy 
deployment of the system appoint is not a replacement but 
an addition to the current client-server architectures we 
developed a library of function calls that when placed in a 
client-server architecture starts the service we are 
developing advanced peer selection schemes that incorporate the 
location of active clients bandwidth among active clients 
data-size to be transferred load on active clients and 
availability of active clients to form a complete means of selecting 
the best clients that can become efficient alternatives to the 
server 
with appoint we are defining a very simple api that 
could be used within an existing client-server system easily 
instead of denial of service or a slow connection this api 
can be utilized to forward the service appropriately the 
api for the server side is 
start serverportno 
makefileavailable file location boolean 
callback receivedfile file location 
callback errorreceivingfile file location error 
stop 
similarly the api for the client side is 
start clientportno serverportno serveraddress 
makefileavailable file location boolean 
receivefile file location 
sendfile file location 
stop 
the server after starting the appoint service can make 
all of the data files available to the clients by using the 
makefileavailable method this will enable appoint 
to treat the server as one of the peers 
the two callback methods of the server are invoked when 
a file is received from a client or when an error is 
encountered while receiving a file from a client appoint 
guar 
figure the localization operation in appoint 
antees that at least one of the callbacks will be called so 
that the user who may not be online anymore can always 
be notified i e via email clients localizing large data 
files can make these files available to the public by using the 
makefileavailable method on the client side 
for example in our sand internet browser we have the 
localization of spatial data as a function that can be chosen 
from our menus this functionality enables users to 
download data sets completely to their local disks before starting 
their queries or analysis in our implementation we have 
calls to the appoint service both on the client and the 
server sides as mentioned above hence when a localization 
request comes to the sand internet browser the browser 
leaves the decisions to optimally find and localize a data set 
to the appoint service our server also makes its data 
files available over appoint the mechanism for the 
localization operation is shown with more details from the 
appoint protocols in figure the upload operation is 
performed in a similar fashion 
 related work 
there has been a substantial amount of research on 
remote access to spatial data one specific approach has 
been adopted by numerous web-based mapping services 
 mapquest mapsonus etc the goal in this 
approach is to enable remote users typically only equipped 
with standard web browsers to access the company s 
spatial database server and retrieve information in the form of 
pictorial maps from them the solution presented by most 
of these vendors is based on performing all the calculations 
on the server side and transferring only bitmaps that 
represent results of user queries and commands although the 
advantage of this solution is the minimization of both 
hardware and software resources on the client site the resulting 
product has severe limitations in terms of available 
functionality and response time each user action results in a new 
bitmap being transferred to the client 
work described in examines a client-server 
architecture for viewing large images that operates over a 
lowbandwidth network connection it presents a technique 
based on wavelet transformations that allows the 
minimization of the amount of data needed to be transferred over 
the network between the server and the client in this case 
while the server holds the full representation of the large 
image only a limited amount of data needs to be transferred 
to the client to enable it to display a currently requested 
view into the image on the client side the image is 
reconstructed into a pyramid representation to speed up zooming 
and panning operations both the client and the server keep 
a common mask that indicates what parts of the image are 
available on the client and what needs to be requested this 
also allows dropping unnecessary parts of the image from the 
main memory on the server 
other related work has been reported in where a 
client-server architecture is described that is designed to 
provide end users with access to a server it is assumed that 
this data server manages vast databases that are impractical 
to be stored on individual clients this work blends raster 
data management stored in pyramids with vector data 
stored in quadtrees 
for our peer-to-peer transfer approach appoint 
napster is the forefather where a directory service is centralized 
on a server and users exchange music files that they have 
stored on their local disks our application domain where 
the data is already freely available to the public forms a 
prime candidate for such a peer-to-peer approach gnutella 
is a pure decentralized peer-to-peer file exchange system 
unfortunately it suffers from scalability issues i e floods of 
messages between peers in order to map connectivity in the 
system are required other systems followed these popular 
systems each addressing a different flavor of sharing over 
the internet many peer-to-peer storage systems have also 
recently emerged past eternity service cfs 
and oceanstore are some peer-to-peer storage systems 
some of these systems have focused on anonymity while 
others have focused on persistence of storage also other 
approaches like seti home made other resources such 
as idle cpus work together over the internet to solve large 
scale computational problems our goal is different than 
these approaches with appoint we want to improve 
existing client-server systems in terms of performance by using 
idle networking resources among active clients hence other 
issues like anonymity decentralization and persistence of 
storage were less important in our decisions confirming 
the authenticity of the indirectly delivered data sets is not 
yet addressed with appoint we want to expand our 
research in the future to address this issue 
from our perspective although appoint employs some 
of the techniques used in peer-to-peer systems it is also 
closely related to current web caching architectures 
squirrel forms the middle ground it creates a pure 
peer-topeer collaborative web cache among the web browser caches 
of the machines in a local-area network except for this 
recent peer-to-peer approach web caching is mostly a 
wellstudied topic in the realm of server proxy level caching 
 collaborative web caching systems the most 
relevant of these for our research focus on creating 
either a hierarchical hash-based central directory-based or 
multicast-based caching schemes we do not compete with 
these approaches in fact appoint can work in 
tandem with collaborative web caching if they are deployed 
together we try to address the situation where a request 
arrives at a server meaning all the caches report a miss 
hence the point where the server is reached can be used to 
take a central decision but then the actual service request 
can be forwarded to a set of active clients i e the 
down 
load and upload operations cache misses are especially 
common in the type of large data-based services on which 
we are working most of the web caching schemes that are 
in use today employ a replacement policy that gives a 
priority to replacing the largest sized items over smaller-sized 
ones hence these policies would lead to the immediate 
replacement of our relatively large data files even though they 
may be used frequently in addition in our case the user 
community that accesses a certain data file may also be very 
dispersed from a network point of view and thus cannot take 
advantage of any of the caching schemes finally none of 
the web caching methods address the symmetric issue of 
large data uploads 
 a sample application 
fedstats is an online source that enables ordinary 
citizens access to official statistics of numerous federal agencies 
without knowing in advance which agency produced them 
we are using a fedstats data set as a testbed for our work 
our goal is to provide more power to the users of fedstats 
by utilizing the sand internet browser as an example 
we looked at two data files corresponding to 
environmental protection agency epa -regulated facilities that have 
chlorine and arsenic respectively for each file we had the 
following information available epa-id name street city 
state zip code latitude longitude followed by flags to 
indicate if that facility is in the following epa programs 
hazardous waste wastewater discharge air emissions 
abandoned toxic waste dump and active toxic release 
we put this data into a sand relation where the spatial 
attribute  location corresponds to the latitude and 
longitude some queries that can be handled with our system on 
this data include 
 find all epa-regulated facilities that have arsenic and 
participate in the air emissions program and 
 a lie in georgia to illinois alphabetically 
 b lie within arkansas or miles within its border 
 c lie within miles of the border of arkansas i e 
both sides of the border 
 for each epa-regulated facility that has arsenic find 
all epa-regulated facilities that have chlorine and 
 a that are closer to it than to any other 
eparegulated facility that has arsenic 
 b that participate in the air emissions program 
and are closer to it than to any other 
eparegulated facility which has arsenic in order to 
avoid reporting a particular facility more than 
once we use our  group by epa-id mechanism 
figure illustrates the output of an example query that 
finds all arsenic sites within a given distance of the border of 
arkansas the sites are obtained in an incremental manner 
with respect to a given point this ordering is shown by 
using different color shades 
with this example data it is possible to work with the 
sand internet browser online as an applet connecting to 
a remote server or after localizing the data and then 
opening it locally in the first case for each action taken the 
client-server architecture will decide what to ask for from 
the server in the latter case the browser will use the 
peerto-peer appoint architecture for first localizing the data 
 concluding remarks 
an overview of our efforts in providing remote access to 
large spatial data has been given we have outlined our 
approaches and introduced their individual elements our 
client-server approach improves the system performance by 
using efficient caching methods when a remote server is 
accessed from thin-clients appoint forms an alternative 
approach that improves performance under an existing 
clientserver system by using idle client resources when individual 
users want work on a data set for longer periods of time 
using their client computers 
for the future we envision development of new efficient 
algorithms that will support large online data transfers within 
our peer-to-peer approach using multiple peers 
simultaneously we assume that a peer client can become 
unavailable at any anytime and hence provisions need to be in place 
to handle such a situation to address this we will augment 
our methods to include efficient dynamic updates upon 
completion of this step of our work we also plan to run 
comprehensive performance studies on our methods 
another issue is how to access data from different sources 
in different formats in order to access multiple data sources 
in real time it is desirable to look for a mechanism that 
would support data exchange by design the xml 
protocol has emerged to become virtually a standard for 
describing and communicating arbitrary data gml is 
an xml variant that is becoming increasingly popular for 
exchange of geographical data we are currently working 
on making sand xml-compatible so that the user can 
instantly retrieve spatial data provided by various agencies in 
the gml format via their web services and then explore 
query or process this data further within the sand 
framework this will turn the sand system into a universal tool 
for accessing any spatial data set as it will be deployable on 
most platforms work efficiently given large amounts of data 
be able to tap any gml-enabled data source and provide 
an easy to use graphical user interface this will also 
convert the sand system from a research-oriented prototype 
into a product that could be used by end users for 
accessing viewing and analyzing their data efficiently and with 
minimum effort 
 references 
 fedstats the gateway to statistics from over u s 
federal agencies http www fedstats gov 
 arcinfo scalable system of software for geographic 
data creation management integration analysis and 
dissemination http www esri com software 
arcgis arcinfo index html 
 extensible markup language xml 
http www w org xml 
 geography markup language gml 
http opengis net gml - gml html 
 mapquest consumer-focused interactive mapping site 
on the web http www mapquest com 
 mapsonus suite of online geographic services 
http www mapsonus com 
 r anderson the eternity service in proceedings of 
the pragocrypt pages - prague czech 
republic september 
 l breslau p cao l fan g phillips and 
s shenker web caching and zipf-like distributions 
 
figure sample output from the sand internet browser - large dark dots indicate the result of a query 
that looks for all arsenic sites within a given distance from arkansas different color shades are used to 
indicate ranking order by the distance from a given point 
evidence and implications in proceedings of the ieee 
infocom pages - new york ny march 
 
 e chang c yap and t yen realtime visualization 
of large images over a thinwire in r yagel and 
h hagen editors proceedings ieee visualization 
 late breaking hot topics pages - phoenix 
az october 
 f dabek m f kaashoek d karger r morris and 
i stoica wide-area cooperative storage with cfs in 
proceedings of the acm sosp pages - 
banff al october 
 a dingle and t partl web cache coherence 
computer networks and isdn systems 
 - - may 
 c esperan¸ca and h samet experience with 
sand tcl a scripting tool for spatial databases 
journal of visual languages and computing 
 - april 
 s iyer a rowstron and p druschel squirrel a 
decentralized peer-to-peer web cache rice 
university microsoft research submitted for 
publication 
 d karger a sherman a berkheimer b bogstad 
r dhanidina k iwamoto b kim l matkins and 
y yerushalmi web caching with consistent hashing 
computer networks - - may 
 j kubiatowicz d bindel y chen s czerwinski 
p eaton d geels r gummadi s rhea 
h weatherspoon w weimer c wells and b zhao 
oceanstore an architecture for global-scale persistent 
store in proceedings of the acm asplos pages 
 - cambridge ma november 
 m potmesil maps alive viewing geospatial 
information on the www computer networks and 
isdn systems - - september 
also hyper proceedings of the th international world 
wide web conference santa clara ca april 
 m rabinovich j chase and s gadde not all hits 
are created equal cooperative proxy caching over a 
wide-area network computer networks and isdn 
systems - - november 
 a rowstron and p druschel storage management 
and caching in past a large-scale persistent 
peer-to-peer storage utility in proceedings of the acm 
sosp pages - banff al october 
 h samet applications of spatial data structures 
computer graphics image processing and gis 
addison-wesley reading ma 
 h samet the design and analysis of spatial data 
structures addison-wesley reading ma 
 seti home http setiathome ssl berkeley edu 
 
 l j williams pyramidal parametrics computer 
graphics - july also proceedings of 
the siggraph conference detroit july 
 
an architectural framework and a middleware for 
cooperating smart components 
∗ 
ant´onio casimiro 
u lisboa 
casim di fc ul pt 
j¨org kaiser 
u ulm 
 
kaiser informatik uniulm de 
paulo ver´ıssimo 
u lisboa 
pjv di fc ul pt 
abstract 
in a future networked physical world a myriad of smart 
sensors and actuators assess and control aspects of their 
environments and autonomously act in response to it 
examples range in telematics traffic management team robotics 
or home automation to name a few to a large extent such 
systems operate proactively and independently of direct 
human control driven by the perception of the environment and 
the ability to organize respective computations dynamically 
the challenging characteristics of these applications include 
sentience and autonomy of components issues of 
responsiveness and safety criticality geographical dispersion mobility 
and evolution a crucial design decision is the choice of 
the appropriate abstractions and interaction mechanisms 
looking to the basic building blocks of such systems we 
may find components which comprise mechanical 
components hardware and software and a network interface thus 
these components have different characteristics compared to 
pure software components they are able to spontaneously 
disseminate information in response to events observed in 
the physical environment or to events received from other 
component via the network interface larger autonomous 
components may be composed recursively from these 
building blocks 
the paper describes an architectural framework and a 
middleware supporting a component-based system and an 
integrated view on events-based communication comprising 
the real world events and the events generated in the 
system it starts by an outline of the component-based system 
construction the generic event architecture gear is 
introduced which describes the event-based interaction between 
the components via a generic event layer the generic event 
layer hides the different communication channels including 
∗this work was partially supported by the ec through 
project ist- - cortex and by the fct 
through the large-scale informatic systems 
laboratory lasige and project posi chs 
 defeats 
the interactions through the environment an appropriate 
middleware is presented which reflects these needs and 
allows to specify events which have quality attributes to 
express temporal constraints this is complemented by the 
notion of event channels which are abstractions of the 
underlying network and allow to enforce quality attributes they 
are established prior to interaction to reserve the needed 
computational and network resources for highly predictable 
event dissemination 
categories and subject descriptors 
c computer-communication networks distributed 
systems-distributed applications c special-purpose 
and application-based systems real-time and 
embedded systems 
general terms 
design 
 introduction 
in recent years we have seen the continuous improvement 
of technologies that are relevant for the construction of 
distributed embedded systems including trustworthy visual 
auditory and location sensing communication and 
processing we believe that in a future networked physical 
world a new class of applications will emerge composed of 
a myriad of smart sensors and actuators to assess and 
control aspects of their environments and autonomously act in 
response to it the anticipated challenging characteristics 
of these applications include autonomy responsiveness and 
safety criticality large scale geographical dispersion 
mobility and evolution 
in order to deal with these challenges it is of 
fundamental importance to use adequate high-level models 
abstractions and interaction paradigms unfortunately when 
facing the specific characteristics of the target systems the 
shortcomings of current architectures and middleware 
interaction paradigms become apparent looking to the basic 
building blocks of such systems we may find components 
which comprise mechanical parts hardware software and 
a network interface however classical event object 
models are usually software oriented and as such when 
trans 
ported to a real-time embedded systems setting their 
harmony is cluttered by the conflict between on the one side 
send receive of software events message-based and on 
the other side input output of hardware or real-world 
events register-based in terms of interaction paradigms 
and although the use of event-based models appears to be 
a convenient solution these often lack the 
appropriate support for non-functional requirements like reliability 
timeliness or security 
this paper describes an architectural framework and a 
middleware supporting a component-based system and an 
integrated view on event-based communication comprising 
the real world events and the events generated in the system 
when choosing the appropriate interaction paradigm it 
is of fundamental importance to address the challenging 
issues of the envisaged sentient applications unlike classical 
approaches that confine the possible interactions to the 
application boundaries i e to its components we consider 
that the environment surrounding the application also plays 
a relevant role in this respect therefore the paper starts by 
clarifying several issues concerning our view of the system 
about the interactions that may take place and about the 
information flows this view is complemented by 
providing an outline of the component-based system construction 
and in particular by showing that it is possible to 
compose larger applications from basic components following 
an hierarchical composition approach 
this provides the necessary background to introduce the 
generic-events architecture gear which describes 
the event-based interaction between the components via a 
generic event layer while allowing the seamless integration 
of physical and computer information flows in fact the 
generic event layer hides the different communication 
channels including the interactions through the environment 
additionally the event layer abstraction is also adequate 
for the proper handling of the non-functional requirements 
namely reliability and timeliness which are particularly 
stringent in real-time settings the paper devotes particular 
attention to this issue by discussing the temporal aspects of 
interactions and the needs for predictability 
an appropriate middleware is presented which reflects 
these needs and allows to specify events which have quality 
attributes to express temporal constraints this is 
complemented by the notion of event channels ec which are 
abstractions of the underlying network while being abstracted 
by the event layer in fact event channels play a 
fundamental role in securing the functional and non-functional 
 e g reliability and timeliness properties of the envisaged 
applications that is in allowing the enforcement of quality 
attributes they are established prior to interaction to 
reserve the needed computational and network resources for 
highly predictable event dissemination 
the paper is organized as follows in section we 
introduce the fundamental notions and abstractions that we 
adopt in this work to describe the interactions taking place 
in the system then in section we describe the 
componentbased approach that allows composition of objects gear 
is then described in section and section focuses on 
temporal aspects of the interactions section describes the 
cosmic middleware which may be used to specify the 
interaction between sentient objects a simple example to 
highlight the ideas presented in the paper appears in 
section and section concludes the paper 
 related work 
our work considers a wired physical world in which a 
very large number of autonomous components cooperate 
it is inspired by many research efforts in very different 
areas event-based systems in general have been introduced to 
meet the requirements of applications in which entities 
spontaneously generate information and disseminate it 
 intended for large systems and requiring quite complex 
infrastructures these event systems do not consider 
stringent quality aspects like timeliness and dependability issues 
secondly they are not created to support inter-operability 
between tiny smart devices with substantial resource 
constraints 
in a real-time event system for corba has been 
introduced the events are routed via a central event server 
which provides scheduling functions to support the real-time 
requirements such a central component is not available 
in an infrastructure envisaged in our system architecture 
and the developed middleware tao the ace orb is quite 
complex and unsuitable to be directly integrated in smart 
devices 
there are efforts to implement corba for control 
networks tailored to connect sensor and actuator components 
 they are targeted for the can-bus a popular 
network developed for the automotive industry however in 
these approaches the support for timeliness or 
dependability issues does not exist or is only very limited 
a new scheme to integrate smart devices in a corba 
environment is proposed in and has lead to the proposal of 
a standard by the object management group omg 
smart transducers are organized in clusters that are 
connected to a corba system by a gateway 
the clusters form isolated subnetworks a special master 
node enforces the temporal properties in the cluster subnet 
a corba gateway allows to access sensor data and write 
actuator data by means of an interface file system ifs 
the basic structure is similar to the wan-of-cans 
structure which has been introduced in the cortex project 
islands of tight control may be realized by a control network 
and cooperate via wired or wireless networks covering a large 
number of these subnetworks however in contrast to the 
event channel model introduced in this paper all 
communication inside a cluster relies on a single technical solution of 
a synchronous communication channel secondly although 
the temporal behaviour of a single cluster is rigorously 
defined no model to specify temporal properties for 
clusterto-corba or cluster-to-cluster interactions is provided 
 information flow and 
interaction model 
in this paper we consider a component-based system model 
that incorporates previous work developed in the context of 
the ist cortex project as mentioned above a 
fundamental idea underlying the approach is that applications can 
be composed of a large number of smart components that 
are able to sense their surrounding environment and 
interact with it these components are referred to as sentient 
objects a metaphor elaborated in cortex and inspired 
on the generic concept of sentient computing introduced in 
 sentient objects accept input events from a variety of 
different sources including sensors but not constrained to 
that process them and produce output events whereby 
 
they actuate on the environment and or interact with other 
objects therefore the following kinds of interactions can 
take place in the system 
environment-to-object interactions correspond to a 
flow of information from the environment to 
application objects reporting about the state of the former 
and or notifying about events taking place therein 
object-to-object interactions correspond to a flow of 
information among sentient objects serving two 
purposes the first is related with complementing the 
assessment of each individual object about the state 
of the surrounding space the second is related to 
collaboration in which the object tries to influence other 
objects into contributing to a common goal or into 
reacting to an unexpected situation 
object-to-environment interactions correspond to a 
flow of information from an object to the environment 
with the purpose of forcing a change in the state of the 
latter 
before continuing we need to clarify a few issues with 
respect to these possible forms of interaction we consider 
that the environment can be a producer or consumer of 
information while interacting with sentient objects the 
environment is the real physical world surrounding an 
object not necessarily close to the object or limited to certain 
boundaries quite clearly the information produced by the 
environment corresponds to the physical representation of 
real-time entities of which typical examples include 
temperature distance or the state of a door on the other hand 
actuation on the environment implies the manipulation of 
these real-time entities like increasing the temperature 
 applying more heat changing the distance applying some 
movement or changing the state of the door closing or 
opening it the required transformations between system 
representations of these real-time entities and their physical 
representations is accomplished generically by sensors and 
actuators we further consider that there may exist dumb 
sensors and actuators which interact with the objects by 
disseminating or capturing raw transducer information and 
smart sensors and actuators with enhanced processing 
capabilities capable of speaking some more elaborate event 
dialect see sections and interaction with the 
environment is therefore done through sensors and actuators 
which may or may not be part of sentient objects as 
discussed in section 
state or state changes in the environment are considered 
as events captured by sensors in the environment or within 
sentient objects and further disseminated to other 
potentially interested sentient objects in the system in 
consequence it is quite natural to base the communication and 
interaction among sentient objects and with the environment 
on an event-based communication model moreover typical 
properties of event-based models such as anonymous and 
non-blocking communication are highly desirable in systems 
where sentient objects can be mobile and where interactions 
are naturally very dynamic 
a distinguishing aspect of our work from many of the 
existing approaches is that we consider that sentient objects 
may indirectly communicate with each other through the 
environment when they act on it thus the environment 
constitutes an interaction and communication channel and 
is in the control and awareness loop of the objects in other 
words when a sentient object actuates on the environment it 
will be able to observe the state changes in the environment 
by means of events captured by the sensors clearly other 
objects might as well capture the same events thus 
establishing the above-mentioned indirect communication path 
in systems that involve interactions with the environment 
it is very important to consider the possibility of 
communication through the environment it has been shown that 
the hidden channels developing through the latter e g 
feedback loops may hinder software-based algorithms ignoring 
them therefore any solution to the problem requires 
the definition of convenient abstractions and appropriate 
architectural constructs 
on the other hand in order to deal with the information 
flow through the whole computer system and environment in 
a seamless way handling software and hardware events 
uniformly it is also necessary to find adequate abstractions 
as discussed in section the generic-events architecture 
introduces the concept of generic event and an event layer 
abstraction which aim at dealing among others with these 
issues 
 sentient object composition 
in this section we analyze the most relevant issues related 
with the sentient object paradigm and the construction of 
systems composed of sentient objects 
 component-based system construction 
sentient objects can take several different forms they 
can simply be software-based components but they can also 
comprise mechanical and or hardware parts amongst which 
the very sensorial apparatus that substantiates sentience 
mixed with software components to accomplish their task 
we refine this notion by considering a sentient object as an 
encapsulating entity a component with internal logic and 
active processing elements able to receive transform and 
produce new events this interface hides the internal 
hardware software structure of the object which may be 
complex and shields the system from the low-level functional 
and temporal details of controlling a specific sensor or 
actuator 
furthermore given the inherent complexity of the 
envisaged applications the number of simultaneous input events 
and the internal size of sentient objects may become too 
large and difficult to handle therefore it should be 
possible to consider the hierarchical composition of sentient 
objects so that the application logic can be separated across as 
few or as many of these objects as necessary on the other 
hand composition of sentient objects should normally be 
constrained by the actual hardware component s structure 
preventing the possibility of arbitrarily composing sentient 
objects this is illustrated in figure where a sentient 
object is internally composed of a few other sentient 
objects each of them consuming and producing events some 
of which only internally propagated 
observing the figure and recalling our previous discussion 
about the possible interactions we identify all of them here 
an object-to-environment interaction occurs between the 
object controlling a wlan transmitter and some wlan 
receiver in the environment an environment-to-object 
interaction takes place when the object responsible for the gps 
 
g p s 
r e c e p t i o n 
w i r e l e s s 
t r a n s m i s s i o n 
d o p p l e r 
r a d a r 
p h y s i c a l f e e d b a c k 
o b j e c t s b o d y 
i n t e r n a l n e t w o r k 
figure component-aware sentient object 
composition 
signal reception uses the information transmitted by the 
satellites finally explicit object-to-object interactions occur 
internally to the container object through an internal 
communication network additionally it is interesting to 
observe that implicit communication can also occur whether 
the physical feedback develops through the environment 
internal to the container object as depicted or through the 
environment external to this object however there is a 
subtle difference between both cases while in the former the 
feedback can only be perceived by objects internal to the 
container bounding the extent to which consistency must 
be ensured such bounds do not exist in the latter in fact 
the notion of sentient object as an encapsulating entity may 
serve other purposes e g the confinement of feedback and 
of the propagation of events beyond the mere hierarchical 
composition of objects 
to give a more concrete example of such component-aware 
object composition we consider a scenario of cooperating 
robots each robot is made of several components 
corresponding for instance to axis and manipulator controllers 
together with the control software each of these controllers 
may be a sentient object on the other hand a robot itself 
is a sentient object composed of the objects materialized 
by the controllers and the environment internal to its own 
structure or body 
this means that it should be possible to define 
cooperation activities using the events produced by robot sentient 
objects without the need to know the internal structure of 
robots or the events produced by body objects or by smart 
sensors within the body from an engineering point of view 
however this also means that robot sentient object may 
have to generate new events that reflect its internal state 
which requires the definition of a gateway to make the bridge 
between the internal and external environments 
 encapsulation and scoping 
now an important question is about how to represent and 
disseminate events in a large scale networked world as we 
have seen above any event generated by a sentient object 
could in principle be visible anywhere in the system and 
thus received by any other sentient object however there 
are substantial obstacles to such universal interactions 
originating from the components heterogeneity in such a 
largescale setting 
firstly the components may have severe performance 
constraints particularly because we want to integrate smart 
sensors and actuators in such an architecture secondly the 
bandwidth of the participating networks may vary largely 
such networks may be low power low bandwidth fieldbuses 
or more powerful wireless networks as well as high speed 
backbones thirdly the networks may have widely different 
reliability and timeliness characteristics consider a 
platoon of cooperating vehicles inside a vehicle there may be 
a field-bus like can ttp a or lin with a 
comparatively low bandwidth on the other hand the 
vehicles are communicating with others in the platoon via a 
direct wireless link finally there may be multiple platoons 
of vehicles which are coordinated by an additional wireless 
network layer 
at the abstraction level of sentient objects such 
heterogeneity is reflected by the notion of body-vs-environment 
at the network level we assume the wan-of-cans 
structure to model the different networks the notion of 
body and environment is derived from the recursively 
defined component-based object model a body is similar to 
a cell membrane and represents a quality of service 
container for the sentient objects inside on the network level 
it may be associated with the components coupled by a 
certain can a can defines the dissemination quality which 
can be expected by the cooperating objects 
in the above example a vehicle may be a sentient object 
whose body is composed of the respective lower level objects 
 sensors and actuators which are connected by the internal 
network see figure correspondingly the platoon can be 
seen itself as an object composed of a collection of 
cooperating vehicles its body being the environment encapsulated by 
the platoon zone at the network level the wireless network 
represents the respective can however several platoons 
united by their cans may interact with each other and 
objects further away through some wider-range possible fixed 
networking substrate hence the concept of wan-of-cans 
the notions of body-environment and wan-of-cans are 
very useful when defining interaction properties across such 
boundaries their introduction obeyed to our belief that 
a single mechanism to provide quality measures for 
interactions is not appropriate instead a high level construct 
for interaction across boundaries is needed which allows to 
specify the quality of dissemination and exploits the 
knowledge about body and environment to assess the feasibility of 
quality constraints as we will see in the following section 
the notion of an event channel represents this construct in 
our architecture it disseminates events and allows the 
network independent specification of quality attributes these 
attributes must be mapped to the respective properties of 
the underlying network structure 
 a generic events architecture 
in order to successfully apply event-based object-oriented 
models addressing the challenges enumerated in the 
introduction of this paper it is necessary to use adequate 
architectural constructs which allow the enforcement of 
fundamental properties such as timeliness or reliability 
we propose the generic-events architecture gear 
depicted in figure which we briefly describe in what 
follows for a more detailed description please refer to 
the l-shaped structure is crucial to ensure some of the 
properties described 
environment the physical surroundings remote and close 
solid and etherial of sentient objects 
 
c o m m sc o m m sc o m m s 
t r a n s l a t i o n 
l a y e r 
t r a n s l a t i o n 
l a y e r 
b o d y 
e n v i r o n m e n t 
b o d y 
e n v i r o n m e n t 
b o d y 
e n v i r o n m e n t 
 i n c l u d i n g o p e r a t i o n a l n e t w o r k 
 o f o b j e c t o r o b j e c t c o m p o u n d 
t r a n s l a t i o n 
l a y e r 
t r a n s l a t i o n 
s e n t i e n t 
o b j e c t 
s e n t i e n t 
o b j e c t 
s e n t i e n t 
o b j e c t 
r e g u l a r n e t w o r k 
c o n s u m ep r o d u c e 
e v e n t 
l a y e r 
e v e n t 
l a y e r 
e v e n t 
l a y e r 
s e n t i e n t 
o b j e c t 
figure generic-events architecture 
body the physical embodiment of a sentient object e g 
the hardware where a mechatronic controller resides 
the physical structure of a car note that due to the 
compositional approach taken in our model part of 
what is environment to a smaller object seen 
individually becomes body for a larger containing object 
in fact the body is the internal environment of the 
object this architecture layering allows composition 
to take place seamlessly in what concerns information 
flow 
inside a body there may also be implicit knowledge 
which can be exploited to make interaction more 
efficient like the knowledge about the number of 
cooperating entities the existence of a specific 
communication network or the simple fact that all components are 
co-located and thus the respective events do not need 
to specify location in their context attributes such 
intrinsic information is not available outside a body and 
therefore more explicit information has to be carried 
by an event 
translation layer the layer responsible for physical event 
transformation from to their native form to event 
channel dialect between environment body and an event 
channel essentially one doing observation and 
actuation operations on the lower side and doing 
transactions of event descriptions on the other on the lower 
side this layer may also interact with dumb sensors or 
actuators therefore talking the language of the 
specific device these interactions are done through 
operational networks hence the antenna symbol in the 
figure 
event layer the layer responsible for event propagation 
in the whole system through several event channels 
 ec in concrete terms this layer is a kind of 
middleware that provides important event-processing services 
which are crucial for any realistic event-based system 
for example some of the services that imply the 
processing of events may include publishing subscribing 
discrimination zoning filtering fusion tracing and 
queuing 
communication layer the layer responsible for 
wrapping events as a matter of fact event descriptions 
in ec dialect into carrier event-messages to be 
transported to remote places for example a 
sensing event generated by a smart sensor is wrapped in 
an event-message and disseminated to be caught by 
whoever is concerned the same holds for an 
actuation event produced by a sentient object to be 
delivered to a remote smart actuator likewise this may 
apply to an event-message from one sentient object 
to another dumb sensors and actuators do not send 
event-messages since they are unable to understand 
the ec dialect they do not have an event layer 
neither a communication layer- they communicate if 
needed through operational networks 
regular network this is represented in the horizontal 
axis of the block diagram by the communication layer 
which encompasses the usual lan tcp ip and 
realtime protocols desirably augmented with reliable and or 
ordered broadcast and other protocols 
the gear introduces some innovative ideas in distributed 
systems architecture while serving an object model based 
on production and consumption of generic events it treats 
events produced by several sources environment body 
objects in a homogeneous way this is possible due to the use 
of a common basic dialect for talking about events and due 
to the existence of the translation layer which performs the 
necessary translation between the physical representation of 
a real-time entity and the ec compliant format crucial to 
the architecture is the event layer which uses event channels 
to propagate events through regular network infrastructures 
the event layer is realized by the cosmic middleware as 
described in section 
 information flow in gear 
the flow of information external environment and 
computational part is seamlessly supported by the l-shaped 
architecture it occurs in a number of different ways which 
demonstrates the expressiveness of the model with regard to 
the necessary forms of information encountered in real-time 
cooperative and embedded systems 
smart sensors produce events which report on the 
environment body sensors produce events which report on 
the body they are disseminated by the local event layer 
module on an event channel ec propagated through the 
regular network to any relevant remote event layer 
modules where entities showed an interest on them normally 
sentient objects attached to the respective local event layer 
modules 
sentient objects consume events they are interested in 
process them and produce other events some of these 
events are destined to other sentient objects they are 
published on an ec using the same ec dialect that serves e g 
sensor originated events however these events are 
semantically of a kind such that they are to be subscribed by 
the relevant sentient objects for example the sentient 
objects composing a robot controller system or at a higher 
level the sentient objects composing the actual robots in 
 
a cooperative application smart actuators on the other 
hand merely consume events produced by sentient objects 
whereby they accept and execute actuation commands 
alternatively to talking to other sentient objects sentient 
objects can produce events of a lower level for example 
actuation commands on the body or environment they 
publish these exactly the same way on an event channel 
through the local event layer representative now if these 
commands are of concern to local actuator units e g body 
including internal operational networks they are passed on 
to the local translation layer if they are of concern to a 
remote smart actuator they are disseminated through the 
distributed event layer to reach the former in any case 
if they are also of interest to other entities such as other 
sentient objects that wish to be informed of the actuation 
command then they are also disseminated through the ec 
to these sentient objects 
a key advantage of this architecture is that event-messages 
and physical events can be globally ordered if necessary 
since they all pass through the event layer the model also 
offers opportunities to solve a long lasting problem in 
realtime computer control and embedded systems the 
inconsistency between message passing and the feedback loop 
information flow subsystems 
 temporal aspects of the 
interactions 
any interaction needs some form of predictability if safety 
critical scenarios are considered as it is done in cortex 
temporal aspects become crucial and have to be made 
explicit the problem is how to define temporal constraints 
and how to enforce them by appropriate resource usage in a 
dynamic ad-hoc environment in an system where 
interactions are spontaneous it may be also necessary to determine 
temporal properties dynamically to do this the respective 
temporal information must be stated explicitly and available 
during run-time secondly it is not always ensured that 
temporal properties can be fulfilled in these cases 
adaptations and timing failure notification must be provided 
 in most real-time systems the notion of a deadline 
is the prevailing scheme to express and enforce timeliness 
however a deadline only weakly reflect the temporal 
characteristics of the information which is handled secondly a 
deadline often includes implicit knowledge about the system 
and the relations between activities in a rather well defined 
closed environment it is possible to make such implicit 
assumptions and map these to execution times and deadlines 
e g the engineer knows how long a vehicle position can be 
used before the vehicle movement outdates this information 
thus he maps this dependency between speed and position 
on a deadline which then assures that the position error 
can be assumed to be bounded in a open environment this 
implicit mapping is not possible any more because as an 
obvious reason the relation between speed and position and 
thus the error bound cannot easily be reverse engineered 
from a deadline therefore our event model includes 
explicit quality attributes which allow to specify the temporal 
attributes for every individual event this is of course an 
overhead compared to the use of implicit knowledge but in 
a dynamic environment such information is needed 
to illustrate the problem consider the example of the 
position of a vehicle a position is a typical example for 
time value entity thus the position is useful if we 
can determine an error bound which is related to time e g if 
we want a position error below meters to establish a safety 
property between cooperating cars moving with m sec 
the position has a validity time of seconds in a time 
value entity entity we can trade time against the precision 
of the value this is known as value over time and time over 
value once having established the time-value relation 
and captured in event attributes subscribers of this event 
can locally decide about the usefulness of an information in 
the gear architecture temporal validity is used to reason 
about safety properties in a event-based system we 
will briefly review the respective notions and see how they 
are exploited in our cosmic event middleware 
consider the timeline of generating an event representing 
some real-time entity from its occurrence to the 
notification of a certain sentient object figure the real-time 
entity is captured at the sensor interface of the system and 
has to be transformed in a form which can be treated by a 
computer during the time interval t the sensor reads the 
real-time entity and a time stamp is associated with the 
respective value the derived time value entity represents 
an observation it may be necessary to perform substantial 
local computations to derive application relevant 
information from the raw sensor data however it should be noted 
that the time stamp of the observation is associated with 
the capture time and thus independent from further signal 
processing and event generation this close relationship 
between capture time and the associated value is supported by 
smart sensors described above 
the processed sensor information is assembled in an event 
data structure after ts to be published to an event channel 
as is described later the event includes the time stamp of 
generation and the temporal validity as attributes 
the temporal validity is an application defined measure 
for the expiration of a time value as we explained in 
the example of a position above it may vary dependent on 
application parameters temporal validity is a more general 
concept than that of a deadline it is independent of a 
certain technical implementation of a system while deadlines 
may be used to schedule the respective steps in an event 
generation and dissemination a temporal validity is an 
intrinsic property of a time value entity carried in an event 
a temporal validity allows to reason about the usefulness 
of information and is beneficial even in systems in which 
timely dissemination of events cannot be enforced because 
it enables timing failure detection at the event consumer it 
is obvious that deadlines or periods can be derived from the 
temporal validity of an event to set a deadline knowledge 
of an implementation worst case execution times or 
message dissemination latencies is necessary thus in the 
timeline of figure every interval may have a deadline event 
dissemination through soft real-time channels in cosmic 
exploits the temporal validity to define dissemination 
deadlines quality attributes can be defined for instance in 
terms of validity interval omission degree pairs these 
allow to characterize the usefulness of the event for a certain 
application in a certain context because of that quality 
attributes of an event clearly depend on higher level issues 
such as the nature of the sentient object or of the smart 
sensor that produced the event for instance an event 
containing an indication of some vehicle speed must have 
different quality attributes depending on the kind of vehicle 
 
real-world 
event 
observation 
 time stamp value 
event generated 
ready to be transmitted 
event 
received 
notification 
 to 
t 
event 
producer communication network 
event 
consumer 
event channel 
push event 
 ts tm tt tn 
 t o t i m e t o o b t a i n a n o b s e r v a t i o n 
 t s t i m e t o p r o c e s s s e n s o r r e a d i n g 
 t m t i m e t o a s s e m b l e a n e v e n t m e s s a g e 
 t t t i m e t o t r a n s f e r t h e e v e n t o n t h e r e g u l a r n e t w o r k 
 t n t i m e f o r n o t i f i c a t i o n o n t h e c o n s u m e r s i t e 
figure event processing and dissemination 
from which it originated or depending on its current speed 
the same happens with the position event of the car 
example above whose validity depends on the current speed 
and on a predefined required precision however since 
quality attributes are strictly related with the semantics of the 
application or at least with some high level knowledge of 
the purpose of the system from which the validity of the 
information can be derived the definition of these quality 
attributes may be done by exploiting the information 
provided at the programming interface therefore it is 
important to understand how the system programmer can 
specify non-functional requirements at the api and how these 
requirements translate into quality attributes assigned to 
events while temporal validity is identified as an intrinsic 
event property which is exploited to decide on the 
usefulness of data at a certain point in time it is still necessary 
to provide a communication facility which can disseminate 
the event before the validity is expired 
in a wan-of-cans network structure we have to cope 
with very different network characteristics and quality of 
service properties therefore when crossing the network 
boundaries the quality of service guarantees available in a 
certain network will be lost and it will be very hard costly 
and perhaps impossible to achieve these properties in the 
next larger area of the wan-of cans structure cortex 
has a couple of abstractions to cope with this situation 
 network zones body environment which have been discussed 
above from the temporal point of view we need a high 
level abstraction like the temporal validity for the 
individual event now to express our quality requirements of the 
dissemination over the network the bound coverage pair 
introduced in relation with the tcb seems to be an 
appropriate approach it considers the inherent uncertainty 
of networks and allows to trade the quality of dissemination 
against the resources which are needed in relation with 
the event channel model discussed later the bound 
coverage pair allows to specify the quality properties of an event 
channel independently of specific technical issues given 
the typical environments in which sentient applications will 
operate where it is difficult or even impossible to provide 
timeliness or reliability guarantees we proposed an 
alternative way to handle non-functional application requirements 
in relation with the tcb approach the proposed 
approach exploits intrinsic characteristics of applications such 
as fail-safety or time-elasticity in order to secure qos 
specifications of the form bound coverage instead of 
constructing systems that rely on guaranteed bounds the idea 
is to use possibly changing bounds that are secured with a 
constant probability all over the execution this obviously 
requires an application to be able to adapt to changing 
conditions and or changing bounds or if this is not possible 
to be able to perform some safety procedures when the 
operational conditions degrade to an unbearable level the 
bounds we mentioned above refer essentially to timeliness 
bounds associated to the execution of local or distributed 
activities or combinations thereof from these bounds it is 
then possible to derive the quality attributes in particular 
validity intervals that characterize the events published in 
the event channel 
 the role of smart sensors and actuators 
smart devices encapsulate hardware software and 
mechanical components and provide information and a set of 
well specified functions and which are closely related to 
the interaction with the environment the built-in 
computational components and the network interface enable the 
implementation of a well-defined high level interface that 
does not just provide raw transducer data but a processed 
application-related set of events moreover they exhibit an 
autonomous spontaneous behaviour they differ from 
general purpose nodes because they are dedicated to a certain 
functionality which complies to their sensing and 
actuating capabilities while general purpose node may execute any 
program 
concerning the sentient object model smart sensors and 
actuators may be basic sentient objects themselves 
consuming events from the real-world environment and producing 
the respective generic events for the system s event layer or 
 
vice versa consuming a generic event and converting it to a 
real-world event by an actuation smart components 
therefore constitute the periphery i e the real-world interface of 
a more complex sentient object the model of sentient 
objects also constitutes the framework to built more complex 
virtual sensors by relating multiple primary i e sensors 
which directly sense a physical entity sensors 
smart components translate events of the environment 
to an appropriate form available at the event layer or vice 
versa transform a system event into an actuation for smart 
components we can assume that 
 smart components have dedicated resources to 
perform a specific function 
 these resources are not used for other purposes during 
normal real-time operation 
 no local temporal conflicts occur that will change the 
observable temporal behaviour 
 the functions of a component can usually only be 
changed during a configuration procedure which is not 
performed when the component is involved in critical 
operations 
 an observation of the environment as a time value 
pair can be obtained with a bounded jitter in time 
many predictability and scheduling problems arise from 
the fact that very low level timing behaviours have to be 
handled on a single processor here temporal 
encapsulation of activities is difficult because of the possible side 
effects when sharing a single processor resource consider the 
control of a simple ir-range detector which is used for 
obstacle avoidance dependent on its range and the speed of 
a vehicle it has to be polled to prevent the vehicle from 
crashing into an obstacle on a single central processor 
this critical activity has to be coordinated with many 
similar possibly less critical functions it means that a very 
fine grained schedule has to be derived based purely on the 
artifacts of the low level device control in a smart 
sensor component all this low level timing behaviour can be 
optimized and encapsulated thus we can assume temporal 
encapsulation similar to information hiding in the functional 
domain of course there is still the problem to guarantee 
that an event will be disseminated and recognized in due 
time by the respective system components but this relates 
to application related events rather than the low artifacts of 
a device timing the main responsibility to provide 
timeliness guarantees is shifted to the event layer where these 
events are disseminated smart sensors thus lead to network 
centric system model the network constitute the shared 
resource which has to be scheduled in a predictable way the 
cosmic middleware introduced in the next section is an 
approach to provide predictable event dissemination for a 
network of smart sensors and actuators 
 an event model andmiddleware 
for cooperating smart devices 
an event model and a middleware suitable for smart 
components must support timely and reliable communication 
and also must be resource efficient cosmic 
 cooperating smart devices is aimed at supporting the 
interaction between those components according to the concepts 
introduced so far based on the model of a wan-of-cans 
we assume that the components are connected to some form 
of can as a fieldbus or a special wireless sensor network 
which provides specific network properties e g a fieldbus 
developed for control applications usually includes 
mechanisms for predictable communication while other networks 
only support a best effort dissemination a gateway 
connects these cans to the next level in the network hierarchy 
the event system should allow the dynamic interaction over 
a hierarchy of such networks and comply with the overall 
cortex generic event model events are typed 
information carriers and are disseminated in a publisher subscriber 
style which is particularly suitable because it 
supports generative anonymous communication and does 
not create any artificial control dependencies between 
producers of information and the consumers this decoupling 
in space no references or names of senders or receivers are 
needed for communication and the flow decoupling no 
control transfer occurs with a data transfer are well known 
 and crucial properties to maintain autonomy of 
components and dynamic interactions 
it is obvious that not all networks can provide the same 
qos guarantees and secondly applications may have widely 
differing requirements for event dissemination 
additionally when striving for predictability resources have to be 
reserved and data structures must be set up before 
communication takes place thus these things can not predictably 
be made on the fly while disseminating an event therefore 
we introduced the notion of an event channel to cope with 
differing properties and requirements and have an object to 
which we can assign resources and reservations the 
concept of an event channel is not new however it has 
not yet been used to reflect the properties of the underlying 
heterogeneous communication networks and mechanisms as 
described by the gear architecture rather existing event 
middleware allows to specify the priorities or deadlines of 
events handled in an event server event channels allow 
to specify the communication properties on the level of the 
event system in a fine grained way an event channel is 
defined by 
event channel subject quality attributelist 
handlers 
the subject determines the types of events event which 
may be issued to the channel the quality attributes model 
the properties of the underlying communication network 
and dissemination scheme these attributes include latency 
specifications dissemination constraints and reliability 
parameters the notion of zones which represent a guaranteed 
quality of service in a subnetwork support this approach 
our goal is to handle the temporal specifications as bound 
coverage pairs orthogonal to the more technical 
questions of how to achieve a certain synchrony property of the 
dissemination infrastructure currently we support 
quality attributes of event channels in a can-bus environment 
represented by explicit synchrony classes 
the cosmic middleware maps the channel properties to 
lower level protocols of the regular network based on our 
previous work on predictable protocols for the can-bus 
cosmic defines an abstract network which provides hard 
soft and non real-time message classes 
correspondingly we distinguish three event channel classes 
according to their synchrony properties hard real-time 
channels soft real-time channels and non-real-time channels 
hard real-time channels hrtc guarantee event 
propagation within the defined time constraints in the presence 
 
of a specified number of omission faults hrtecs are 
supported by a reservation scheme which is similar to the scheme 
used in time-triggered protocols like ttp ttp a 
and ttcan however a substantial advantage over a 
tdma scheme is that due to can-bus properties 
bandwidth which was reserved but is not needed by a hrtec 
can be used by less critical traffic 
soft real-time channels srtc exploit the temporal 
validity interval of events to derive deadlines for scheduling 
the validity interval defines the point in time after which 
an event becomes temporally inconsistent therefore in a 
real-time system an event is useless after this point and may 
me discarded the transmission deadline dl is defined as 
the latest point in time when a message has to be 
transmitted and is specified in a time interval which is derived from 
the expiration time 
tevent ready dl texpiration − ∆notification 
texpiration defines the point in time when the temporal 
validity expires ∆notification is the expected end-to-end 
latency which includes the transfer time over the network and 
the time the event may be delayed by the local event 
handling in the nodes as said before event deadlines are used 
to schedule the dissemination by srtecs however 
deadlines may be missed in transient overload situations or due 
to arbitrary arrival times of events on the publisher side 
the application s exception handler is called whenever the 
event deadline expires before event transmission at this 
point in time the event is also not expected to arrive at the 
subscriber side before the validity expires therefore the 
event is removed from the sending queue on the subscriber 
side the expiration time is used to schedule the delivery of 
the event if the event cannot be delivered until its 
expiration time it is removed from the respective queues allocated 
by the cosmic middleware this prevents the 
communication system to be loaded by outdated messages 
non-real-time channels do not assume any temporal 
specification and disseminate events in a best effort manner an 
instance of an event channel is created locally whenever a 
publisher makes an announcement for publication or a 
subscriber subscribes for an event notification when a 
publisher announces publication the respective data structures 
of an event channel are created by the middleware when 
a subscriber subscribes to an event channel it may specify 
context attributes of an event which are used to filter events 
locally e g a subscriber may only be interested in events 
generated at a certain location additionally the subscriber 
specifies quality properties of the event channel a more 
detailed description of the event channels can be found in 
currently cosmic handles all event channels which 
disseminate events beyond the can network boundary as non 
real-time event channels this is mainly because we use the 
tcp ip protocol to disseminate events over wireless links 
or to the standard ethernet however there are a 
number of possible improvements which can easily be integrated 
in the event channel model the timely computing base 
 tcb can be exploited for timing failure detection and 
thus would provide awareness for event dissemination in 
environments where timely delivery of events cannot be 
enforced additionally there are wireless protocols which can 
provide timely and reliable message delivery which 
may be exploited for the respective event channel classes 
events are the information carriers which are exchanged 
between sentient objects through event channels to cope 
with the requirements of an ad-hoc environment an event 
includes the description of the context in which it has been 
generated and quality attributes defining requirements for 
dissemination this is particularly important in an open 
dynamic environment where an event may travel over 
multiple networks an event instance is specified as 
event subject context attributelist 
quality attributelist contents 
a subject defines the type of the event and is related 
to the event contents it supports anonymous 
communication and is used to route an event the subject has to 
match to the subject of the event channel through which 
the event is disseminated attributes are complementary 
to the event contents they describe individual functional 
and non-functional properties of the event the context 
attributes describe the environment in which the event has 
been generated e g a location an operational mode or a 
time of occurrence the quality attributes specify 
timeliness and dependability aspects in terms of validity 
interval omission degree pairs the validity interval defines the 
point in time after which an event becomes temporally 
inconsistent as described above the temporal validity 
can be mapped to a deadline however usually a 
deadline is an engineering artefact which is used for scheduling 
while the temporal validity is a general property of a time 
value entity in a environment where a deadline cannot 
be enforced a consumer of an event eventually must decide 
whether the event still is temporally consistent i e 
represents a valid time value entity 
 the architecture of the cosmic 
middleware 
on the architectural level cosmic distinguish three 
layers roughly depicted in figure two of them the event 
layer and the abstract network layer are implemented by the 
cosmic middleware the event layer provides the api for 
the application and realizes the abstraction of event and 
event channels 
the abstract network implements real-time message classes 
and adapts the quality requirements to the underlying real 
network an event channel handler resides in every node it 
supports the programming interface and provides the 
necessary data structures for event-based communication 
whenever an object subscribes to a channel or a publisher 
announces a channel the event channel handler is involved it 
initiates the binding of the channel s subject which is 
represented by a network independent unique identifier to an 
address of the underlying abstract network to enable 
communication the event channel handler then tightly 
cooperates with the respective handlers of the abstract network 
layer to disseminate events or receive event notifications it 
should be noted that the qos properties of the event layer 
in general depend on what the abstract network layer can 
provide thus it may not always be possible to e g support 
hard real-time event channels because the abstract network 
layer cannot provide the respective guarantees in we 
describe the protocols and services of the abstract network 
layer particularly for the can-bus 
as can be seen in figure the hard real-time hrt 
message class is supported by a dedicated handler which is 
able to provide the time triggered message dissemination 
 
 
event 
notifications 
hrt-msg 
list 
srt-msg 
queue 
nrt-msg 
queue 
hrt-msg 
calendar 
hrtc 
handler 
s nrtc 
handler 
abstract network 
layer 
can layer 
rx buffer tx buffer 
rx tx error 
interrupts 
event channel 
specs 
event layer 
send 
messages 
exception 
notification 
exceptions 
notifications 
ech 
event channel 
handler 
p u b l i s h a n n o u n c e s u b s c r i b e 
b i n d i n g 
p r o t o c o l 
c o n f i g 
p r o t o c o l 
global 
time 
service 
event 
notifications 
hrt-msg 
list 
srt-msg 
queue 
nrt-msg 
queue 
hrt-msg 
calendar 
hrtc 
handler 
s nrtc 
handler 
abstract network 
layer 
can layer 
rx buffer tx buffer 
rx tx error 
interrupts 
event channel 
specs 
event layer 
send 
messages 
exception 
notification 
exceptions 
notifications 
ech 
event channel 
handler 
p u b l i s h a n n o u n c e s u b s c r i b e 
b i n d i n g 
p r o t o c o l 
c o n f i g 
p r o t o c o l 
global 
time 
service 
figure architecture layers of cosmic 
the hrt handler maintains the hrt message list which 
contains an entry for each local hrt message to be sent 
the entry holds the parameters for the message the 
activation status and the binding information messages are 
scheduled on the bus according to the hrt message 
calendar which comprises the precise start time for each time slot 
allocated for a message soft real-time message queues order 
outgoing messages according to their transmission deadlines 
derived from the temporal validity interval if the 
transmission deadline is exceeded the event message is purged out of 
the queue the respective application is notified via the 
exception notification interface and can take actions like trying 
to publish the event again or publish it to a channel of 
another class incoming event messages are ordered according 
to their temporal validity if an event message arrive the 
respective applications are notified at the moment an 
outdated message is deleted from the queue and if the queue 
runs out of space the oldest message is discarded 
however there are other policies possible depending on event 
attributes and available memory space non real-time 
messages are fifo ordered in a fixed size circular buffer 
 status of cosmic 
the goal for developing cosmic was to provide a 
platform to seamlessly integrate smart tiny components in a 
large system therefore cosmic should run also on the 
small resource constraint devices which are built around 
 bit or even -bit micro-controllers the distributed 
cosmic middleware has been implemented and tested on 
various platforms under rt-linux we support the real-time 
channels over the can bus as described above the 
rtlinux version runs on pentium processors and is currently 
evaluated before we intent to port it to a smart sensor or 
actuator for the interoperability in a wan-of-cans 
environment we only provide non real-time channels at the 
moment this version includes a gateway between the 
canbus and a tcp ip network it allows us to use a 
standard wireless network the non real-time version of 
cosmic is available on linux rt-linux and on the 
microcontroller families c infineon and hc motorola 
both micro-controllers have an on-board can controller 
and thus do not require additional hardware components for 
the network the memory footprint of cosmic is about 
kbyte on a c and slightly more on the hc where it 
fits into the on-board flash memory without problems 
because only a few channels are required on such a smart sensor 
or actuator component the requirement of ram which is 
a scarce resource on many single chip systems to hold the 
dynamic data structures of a channel is low the cosmic 
middleware makes it very easy to include new smart sensors 
in an existing system particularly the application running 
on a smart sensor to condition and process the raw physical 
data must not be aware of any low level network specific 
details it seamlessly interacts with other components of the 
system exclusively via event channels 
the demo example briefly described in the next chapter 
is using a distributed infrastructure of tiny smart sensors 
and actuators directly cooperating via event channels over 
heterogeneous networks 
 an illustrative example 
a simple example for many important properties of the 
proposed system showing the coordination through the 
environment and events disseminated over the network is the 
demo of two cooperating robots depicted in figure 
each robot is equipped with smart distance sensors speed 
sensors acceleration sensors and one of the robots the guide 
 kurt in front figure has a tracking camera 
allowing to follow a white line the robots form a wan-of-cans 
system in which their local cans are interconnected via a 
wireless network cosmic provides the event layer 
for seamless interaction the blind robot n n is 
searching the guide randomly whenever the blind robot detects 
 by its front distance sensors an obstacle it checks whether 
this may be the guide for this purpose it dynamically 
subscribes to the event channel disseminating distance events 
from rear distance sensors of the guide s and compares 
these with the distance events from its local front sensors 
if the distance is approximately the same it infers that it 
is really behind a guide now n n also subscribes to the 
event channels of the tracking camera and the speed sensors 
 
figure cooperating robots 
to follow the guide the demo application highlights the 
following properties of the system 
 dynamic interaction of robots which is not known in 
advance in principle any two a priori unknown robots 
can cooperate all what publishers and subscribers 
have to know to dynamically interact in this 
environment is the subject of the respective event class a 
problem will be to receive only the events of the robot 
which is closest a robot identity does not help much 
to solve this problem rather the position of the event 
generation entity which is captured in the respective 
attributes can be evaluated to filter the relevant event 
out of the event stream a suitable wireless protocol 
which uses proximity to filter events has been proposed 
by meier and cahill in the cortex project 
 interaction through the environment the 
cooperation between the robots is controlled by sensing the 
distance between the robots if the guide detects that 
the distance grows it slows down respectively if the 
blind robot comes too close it reduces its speed the 
local distance sensors produce events which are 
disseminated through a low latency highly predictable event 
channel the respective reaction time can be 
calculated as function of the speed and the distance of the 
robots and define a dynamic dissemination deadline 
for events thus the interaction through the 
environment will secure the safety properties of the 
application i e the follower may not crash into the guide and 
the guide may not loose the follower additionally the 
robots have remote subscriptions to the respective 
distance events which are used to check it with the local 
sensor readings to validate that they really follow the 
guide which they detect with their local sensors 
because there may be longer latencies and omissions this 
check occasionally will not be possible the 
unavailability of the remote events will decrease the quality 
of interaction and probably and slow down the robots 
but will not affect safety properties 
 cooperative sensing the blind robot subscribes to the 
events of the line tracking camera thus it can see 
through the eye of the guide because it knows the 
distance to the guide and the speed as well it can foresee 
the necessary movements the proposed system 
provides the architectural framework for such a 
cooperation the respective sentient object controlling the 
actuation of the robot receives as input the position 
and orientation of the white line to be tracked in the 
case of the guide robot this information is directly 
delivered as a body event with a low latency and a high 
reliability over the internal network for the follower 
robot the information comes also via an event channel 
but with different quality attributes these quality 
attributes are reflected in the event channel description 
the sentient object controlling the actuation of the 
follower is aware of the increased latency and higher 
probability of omission 
 conclusion and future work 
the paper addresses problems of building large distributed 
systems interacting with the physical environment and 
being composed from a huge number of smart components 
we cannot assume that the network architecture in such a 
system is homogeneous rather multiple edge- networks 
are fused to a hierarchical heterogeneous wide area 
network they connect the tiny sensors and actuators 
perceiving the environment and providing sentience to the 
application additionally mobility and dynamic deployment of 
components require the dynamic interaction without fixed 
a priori known addressing and routing schemes the work 
presented in the paper is a contribution towards the 
seamless interaction in such an environment which should not be 
restricted by technical obstacles rather it should be 
possible to control the flow of information by explicitly specifying 
functional and temporal dissemination constraints 
the paper presented the general model of a sentient 
object to describe composition encapsulation and interaction 
in such an environment and developed the generic event 
architecture gear which integrates the interaction through 
the environment and the network while appropriate 
abstractions and interaction models can hide the functional 
heterogeneity of the networks it is impossible to hide the 
quality differences therefore one of the main concerns is 
to define temporal properties in such an open 
infrastructure the notion of an event channel has been introduced 
which allows to specify quality aspects explicitly they can 
be verified at subscription and define a boundary for event 
dissemination the cosmic middleware is a first attempt 
to put these concepts into operation cosmic allows the 
interoperability of tiny components over multiple network 
boundaries and supports the definition of different real-time 
event channel classes 
there are many open questions that emerged from our 
work one direction of future research will be the inclusion 
of real-world communication channels established between 
sensors and actuators in the temporal analysis and the 
ordering of such events in a cause-effect chain additionally 
the provision of timing failure detection for the adaptation 
of interactions will be in the focus of our research to reduce 
network traffic and only disseminate those events to the 
subscribers which they are really interested in and which have 
a chance to arrive timely the encapsulation and scoping 
schemes have to be transformed into respective multi-level 
filtering rules the event attributes which describe aspects 
of the context and temporal constraints for the 
dissemination will be exploited for this purpose finally it is intended 
to integrate the results in the cosmic middleware to 
enable experimental assessment 
 
 references 
 j bacon k moody j bates r hayton c ma 
a mcneil o seidel and m spiteri generic support 
for distributed applications ieee computer 
 - 
 l b becker m gergeleit s schemmer and e nett 
using a flexible real-time scheduling strategy in a 
distributed embedded application in proc of the th 
ieee international conference on emerging 
technologies and factory automation etfa lisbon 
portugal sept 
 n carriero and d gelernter linda in context 
communications of the acm - apr 
 a casimiro ed preliminary definition of cortex 
system architecture cortex project 
ist- - deliverable d apr 
 cortex project annex description of work 
technical report cortex project ist- - 
oct http cortex di fc ul pt 
 r cunningham and v cahill time bounded medium 
access control for ad-hoc networks in proceedings of 
the second acm international workshop on principles 
of mobile computing pomc pages - toulouse 
france oct acm press 
 p t eugster p felber r guerraoui and a -m 
kermarrec the many faces of publish subscribe 
technical report dsc id epfl lausanne 
switzerland 
 t f¨uhrer b m¨uller w dieterle f hartwich 
r hugel and m walther time triggered 
communication on can 
http www can-cia org can ttcan fuehrer pdf 
 r b gmbh can specification version technical 
report sept 
 t harrison d levine and d schmidt the design 
and performance of a real-time corba event service in 
proceedings of the conference on object oriented 
programming systems languages and applications 
 oopsla pages - atlanta georgia usa 
 acm press 
 j hightower and g borriello location systems for 
ubiquitous computing ieee computer - 
aug 
 a hopper the clifford paterson lecture 
sentient computing philosophical transactions of the 
royal society london - aug 
 j kaiser c mitidieri c brudna and c pereira 
cosmic a middleware for event-based interaction 
on can in proc ieee conference on emerging 
technologies and factory automation lisbon 
portugal sept 
 j kaiser and m mock implementing the real-time 
publisher subscriber model on the controller area 
network can in proceedings of the nd international 
symposium on object-oriented real-time distributed 
computing isorc saint-malo france may 
 k kim g jeon s hong t kim and s kim 
integrating subscription-based and connection-oriented 
communications into the embedded corba for 
the can bus in proceedings of the ieee real-time 
technology and application symposium may 
 h kopetz and g gr¨unsteidl ttp - a 
time-triggered protocol for fault-tolerant real-time 
systems technical report rr- - institut f¨ur 
technische informatik technische universit¨at wien 
treilstr a- vienna austria 
 h kopetz m holzmann and w elmenreich a 
universal smart transducer interface ttp a 
international journal of computer system science 
engineering mar 
 h kopetz and p ver´ıssimo real-time and 
dependability concepts in s j mullender editor 
distributed systems nd edition acm-press 
chapter pages - addison-wesley 
 s lankes a jabs and t bemmerl integration of a 
can-based connection-oriented communication model 
into real-time corba in workshop on parallel and 
distributed real-time systems nice france apr 
 
 local interconnect network lin specification 
package revision technical report nov 
 m livani j kaiser and w jia scheduling hard and 
soft real-time communication in the controller area 
network control engineering - 
 r meier and v cahill steam event-based 
middleware for wireless ad-hoc networks in proceedings 
of the international workshop on distributed 
event-based systems icdcs debs pages 
 - vienna austria 
 e nett and s schemmer reliable real-time 
communication in cooperative mobile applications 
ieee transactions on computers - feb 
 
 b oki m pfluegl a seigel and d skeen the 
information bus - an architecture for extensible 
distributed systems operating systems review 
 - 
 o m g omg corbaservices common object 
services specification - notification service 
specification version 
 o m g omg smart transducer interface initial 
submission june 
 p ver´ıssimo v cahill a casimiro k cheverst 
a friday and j kaiser cortex towards supporting 
autonomous and cooperating sentient entities in 
proceedings of european wireless florence italy 
feb 
 p ver´ıssimo and a casimiro the timely computing 
base model and architecture transactions on 
computers - special issue on asynchronous real-time 
systems - aug 
 p ver´ıssimo and a casimiro event-driven support of 
real-time sentient objects in proceedings of the th 
ieee international workshop on object-oriented 
real-time dependable systems guadalajara mexico 
jan 
 p ver´ıssimo and l rodrigues distributed systems for 
system architects kluwer academic publishers 
 
selfish caching in distributed systems 
a game-theoretic analysis 
byung-gon chun 
∗ 
bgchun cs berkeley edu 
kamalika chaudhuri 
† 
kamalika cs berkeley edu 
hoeteck wee 
‡ 
hoeteck cs berkeley edu 
marco barreno 
§ 
barreno cs berkeley edu 
christos h papadimitriou 
† 
christos cs berkeley edu 
john kubiatowicz 
∗ 
kubitron cs berkeley edu 
computer science division 
university of california berkeley 
abstract 
we analyze replication of resources by server nodes that act 
selfishly using a game-theoretic approach we refer to this as the 
selfish caching problem in our model nodes incur either cost for 
replicating resources or cost for access to a remote replica we show the 
existence of pure strategy nash equilibria and investigate the price 
of anarchy which is the relative cost of the lack of coordination 
the price of anarchy can be high due to undersupply problems but 
with certain network topologies it has better bounds with a 
payment scheme the game can always implement the social optimum 
in the best case by giving servers incentive to replicate 
categories and subject descriptors 
c computer-communication networks distributed 
systems 
general terms 
algorithms economics theory performance 
 introduction 
wide-area peer-to-peer file systems peer-to-peer 
caches and web caches have become popular over 
the last few years caching 
of files in selected servers is widely 
used to enhance the performance availability and reliability of 
these systems however most such systems assume that servers 
cooperate with one another by following protocols optimized for 
overall system performance regardless of the costs incurred by 
each server 
in reality servers may behave selfishly - seeking to maximize 
their own benefit for example parties in different 
administrative domains utilize their local resources servers to better 
support clients in their own domains they have obvious incentives to 
cache objects 
that maximize the benefit in their domains possibly 
at the expense of globally optimum behavior it has been an open 
question whether these caching scenarios and protocols maintain 
their desirable global properties low total social cost for example 
in the face of selfish behavior 
in this paper we take a game-theoretic approach to analyzing 
the problem of caching in networks of selfish servers through 
theoretical analysis and simulations we model selfish caching as a 
non-cooperative game in the basic model the servers have two 
possible actions for each object if a replica of a requested object 
is located at a nearby node the server may be better off accessing 
the remote replica on the other hand if all replicas are located too 
far away the server is better off caching the object itself decisions 
about caching the replicas locally are arrived at locally taking into 
account only local costs we also define a more elaborate payment 
model in which each server bids for having an object replicated at 
another site each site now has the option of replicating an object 
and collecting the related bids once all servers have chosen a 
strategy each game specifies a configuration that is the set of servers 
that replicate the object and the corresponding costs for all servers 
game theory predicts that such a situation will end up in a nash 
equilibrium that is a set of possibly randomized strategies with 
the property that no player can benefit by changing its strategy 
while the other players keep their strategies unchanged 
foundational considerations notwithstanding it is not easy to accept 
randomized strategies as the behavior of rational agents in a 
distributed system see for an extensive discussion - but this 
is what classical game theory can guarantee in certain very 
fortunate situations however see the existence of pure that is 
deterministic nash equilibria can be predicted 
with or without randomization however the lack of 
coordination inherent in selfish decision-making may incur costs well 
beyond what would be globally optimum this loss of efficiency is 
 
we will use caching and replication interchangeably 
 
we use the term object as an abstract entity that represents files 
and other data objects 
 
quantified by the price of anarchy the price of anarchy is 
the ratio of the social total cost of the worst possible nash 
equilibrium to the cost of the social optimum the price of anarchy 
bounds the worst possible behavior of a selfish system when left 
completely on its own however in reality there are ways whereby 
the system can be guided through seeding or incentives to a 
preselected nash equilibrium this optimistic version of the price of 
anarchy is captured by the smallest ratio between a nash 
equilibrium and the social optimum 
in this paper we address the following questions 
 do pure strategy nash equilibria exist in the caching game 
 if pure strategy nash equilibria do exist how efficient are 
they in terms of the price of anarchy or its optimistic 
counterpart under different placement costs network topologies 
and demand distributions 
 what is the effect of adopting payments will the nash 
equilibria be improved 
we show that pure strategy nash equilibria always exist in the 
caching game the price of anarchy of the basic game model can 
be o n where n is the number of servers the intuitive reason is 
undersupply under certain topologies the price of anarchy does 
have tighter bounds for complete graphs and stars it is o for 
d-dimensional grids it is o n 
d 
d even the optimistic price of 
anarchy can be o n in the payment model however the game 
can always implement a nash equilibrium that is same as the social 
optimum so the optimistic price of anarchy is one 
our simulation results show several interesting phases as the 
placement cost increases from zero the price of anarchy increases 
when the placement cost first exceeds the maximum distance 
between servers the price of anarchy is at its highest due to 
undersupply problems as the placement cost further increases the price 
of anarchy decreases and the effect of replica misplacement 
dominates the price of anarchy 
the rest of the paper is organized as follows in section we 
discuss related work section discusses details of the basic game and 
analyzes the bounds of the price of anarchy in section we discuss 
the payment game and analyze its price of anarchy in section we 
describe our simulation methodology and study the properties of 
nash equilibria observed we discuss extensions of the game and 
directions for future work in section 
 related work 
there has been considerable research on wide-area peer-to-peer 
file systems such as oceanstore cfs past 
farsite and pangaea web caches such as netcache and 
summarycache and peer-to-peer caches such as squirrel 
most of these systems use caching for performance availability 
and reliability the caching protocols assume obedience to the 
protocol and ignore participants incentives our work starts from the 
assumption that servers are selfish and quantifies the cost of the 
lack of coordination when servers behave selfishly 
the placement of replicas in the caching problem is the most 
important issue there is much work on the placement of web 
replicas instrumentation servers and replicated resources all 
protocols assume obedience and ignore participants incentives in 
gribble et al discuss the data placement problem in peer-to-peer 
systems ko and rubenstein propose a self-stabilizing distributed 
graph coloring algorithm for the replicated resource placement 
chen katz and kubiatowicz propose a dynamic replica 
placement algorithm exploiting underlying distributed hash tables 
douceur and wattenhofer describe a hill-climbing algorithm to 
exchange replicas for reliability in farsite radar is a 
system that replicates and migrates objects for an internet hosting 
service tang and chanson propose a coordinated en-route web 
caching that caches objects along the routing path 
centralized algorithms for the placement of objects web proxies mirrors 
and instrumentation servers in the internet have been studied 
extensively 
the facility location problem has been widely studied as a 
centralized optimization problem in theoretical computer science and 
operations research since the problem is np-hard 
approximation algorithms based on primal-dual techniques greedy 
algorithms and local search have been explored our 
caching game is different from all of these in that the optimization 
process is performed among distributed selfish servers 
there is little research in non-cooperative facility location games 
as far as we know vetta considers a class of problems where 
the social utility is submodular submodularity means decreasing 
marginal utility in the case of competitive facility location among 
corporations he proves that any nash equilibrium gives an expected 
social utility within a factor of of optimal plus an additive term 
that depends on the facility opening cost their results are not 
directly applicable to our problem however because we consider 
each server to be tied to a particular location while in their model 
an agent is able to open facilities in multiple locations note that in 
that paper the increase of the price of anarchy comes from 
oversupply problems due to the fact that competing corporations can open 
facilities at the same location on the other hand the significant 
problems in our game are undersupply and misplacement 
in a recent paper goemans et al analyze content distribution on 
ad-hoc wireless networks using a game-theoretic approach as 
in our work they provide monetary incentives to mobile users for 
caching data items and provide tight bounds on the price of 
anarchy and speed of convergence to approximate nash equilibria 
however their results are incomparable to ours because their 
payoff functions neglect network latencies between users they 
consider multiple data items markets and each node has a limited 
budget to cache items 
cost sharing in the facility location problem has been studied 
using cooperative game theory goemans and skutella 
show strong connections between fair cost allocations and linear 
programming relaxations for facility location problems p´al 
and tardos develop a method for cost-sharing that is approximately 
budget-balanced and group strategyproof and show that the method 
recovers of the total cost for the facility location game 
devanur mihail and vazirani give a strategyproof cost allocation 
for the facility location problem but cannot achieve group 
strategyproofness 
 basic game 
the caching problem we study is to find a configuration that 
meets certain objectives e g minimum total cost figure shows 
examples of caching among four servers in network a a stores 
an object suppose b wants to access the object if it is cheaper 
to access the remote replica than to cache it b accesses the remote 
replica as shown in network b in network c c wants to access 
the object if c is far from a c caches the object instead of 
accessing the object from a it is possible that in an optimal configuration 
it would be better to place replicas in a and b understanding the 
placement of replicas by selfish servers is the focus of our study 
the caching problem is abstracted as follows there is a set n of 
n servers and a set m of m objects the distance between servers 
can be represented as a distance matrix d i e dij is the distance 
 
server 
server 
server 
server 
a 
b 
c 
d 
 a 
server 
server 
server 
server 
a 
b 
c 
d 
 b 
server 
server 
server 
server 
a 
b 
c 
d 
 c 
figure caching there are four servers labeled a b c and d the rectangles are object replicas in a a stores an object if b incurs less cost 
accessing a s replica than it would caching the object itself it accesses the object from a as in b if the distance cost is too high the server caches 
the object itself as c does in c this figure is an example of our caching game model 
from server i to server j d models an underlying network 
topology for our analysis we assume that the distances are symmetric 
and the triangle inequality holds on the distances for all servers 
i j k dij djk ≥ dik each server has demand from clients 
that is represented by a demand matrix w i e wij is the demand 
of server i for object j when a server caches objects the server 
incurs some placement cost that is represented by a matrix α i e 
αij is a placement cost of server i for object j 
in this study we assume that servers have no capacity limit as 
we discuss in the next section this fact means that the caching 
behavior with respect to each object can be examined separately 
consequently we can talk about configurations of the system with 
respect to a given object 
definition a configuration x for some object o is the set 
of servers replicating this object 
the goal of the basic game is to find configurations that are achieved 
when servers optimize their cost functions locally 
 game model 
we take a game-theoretic approach to analyzing the 
uncapacitated caching problem among networked selfish servers we model 
the selfish caching problem as a non-cooperative game with n 
players servers nodes whose strategies are sets of objects to cache in 
the game each server chooses a pure strategy that minimizes its 
cost our focus is to investigate the resulting configuration which 
is the nash equilibrium of the game it should be emphasized that 
we consider only pure strategy nash equilibria in this paper 
the cost model is an important part of the game let ai be the 
set of feasible strategies for server i and let si ∈ ai be the strategy 
chosen by server i given a strategy profile s s s sn 
the cost incurred by server i is defined as 
ci s 
j∈si 
αij 
j ∈si 
wij di i j 
where αij is the placement cost of object j wij is the demand that 
server i has for object j i j is the closest server to i that caches 
object j and dik is the distance between i and k when no server 
caches the object we define distance cost di i j to be dm -large 
enough that at least one server will choose to cache the object 
the placement cost can be further divided into first-time 
installation cost and maintenance cost 
αij k i k i 
updatesizej 
objectsizej 
 
t 
pj 
k 
wkj 
where k i is the installation cost k i is the relative weight 
between the maintenance cost and the installation cost pj is the 
ratio of the number of writes over the number of reads and writes 
updatesizej is the size of an update objectsizej is the size of 
the object and t is the update period we see tradeoffs between 
different parameters in this equation for example placing replicas 
becomes more expensive as updatesizej increases pj increases 
or t decreases however note that by varying αij itself we can 
capture the full range of behaviors in the game for our analysis 
we use only αij 
since there is no capacity limit on servers we can look at each 
single object as a separate game and combine the pure strategy 
equilibria of these games to obtain a pure strategy equilibrium of 
the multi-object game fabrikant papadimitriou and talwar 
discuss this existence argument if two games are known to have pure 
equilibria and their cost functions are cross-monotonic then their 
union is also guaranteed to have pure nash equilibria by a 
continuity argument a nash equilibrium for the multi-object game is 
the cross product of nash equilibria for single-object games 
therefore we can focus on the single object game in the rest of this paper 
for single object selfish caching each server i has two strategies 
- to cache or not to cache the object under consideration is j 
we define si to be when server i caches j and otherwise the 
cost incurred by server i is 
ci s αij si wij di i j − si 
we refer to this game as the basic game the extent to which ci s 
represents actual cost incurred by server i is beyond the scope of 
this paper we will assume that an appropriate cost function of the 
form of equation can be defined 
 nash equilibrium solutions 
in principle we can start with a random configuration and let 
this configuration evolve as each server alters its strategy and 
attempts to minimize its cost game theory is interested in stable 
solutions called nash equilibria a pure strategy nash equilibrium 
is reached when no server can benefit by unilaterally changing its 
strategy a nash equilibrium 
 s∗ 
i s∗ 
−i for the basic game 
specifies a configuration x such that ∀i ∈ n i ∈ x ⇔ s∗ 
i 
thus we can consider a set e of all pure strategy nash equilibrium 
configurations 
x ∈ e ⇔ ∀i ∈ n 
∀si ∈ ai ci s∗ 
i s∗ 
−i ≤ ci si s∗ 
−i 
 
by this definition no server has incentive to deviate in the 
configurations since it cannot reduce its cost 
for the basic game we can easily see that 
x ∈ e ⇔ ∀i ∈ n ∃j ∈ x s t dji ≤ α 
and ∀j ∈ x ¬∃k ∈ x s t dkj α 
 
the first condition guarantees that there is a server that places the 
replica within distance α of each server i if the replica is not placed 
 
the notation for strategy profile s∗ 
i s∗ 
−i separates node i s 
strategy s∗ 
i from the strategies of other nodes s∗ 
−i 
 
a b −α 
 
 
 
 
 
 
 
 
 
 
n 
nodes 
 
n 
nodes 
 a 
a b −α 
 
 
 
 
 
 
 
 
 
 
n 
nodes 
 
n 
nodes 
 b 
a b −α 
 
n 
nodes 
 
n 
nodes 
n 
n 
n 
n 
n n 
n 
n 
n 
n 
 c 
figure potential inefficiency of nash equilibria illustrated by two clusters of n 
 
servers the intra-cluster distances are all zero and the distance 
between clusters is α − where α is the placement cost the dark nodes replicate the object network a shows a nash equilibrium in the basic 
game where one server in a cluster caches the object network b shows the social optimum where two replicas one for each cluster are placed the 
price of anarchy is o n and even the optimistic price of anarchy is o n this high price of anarchy comes from the undersupply of replicas due to 
the selfish nature of servers network c shows a nash equilibrium in the payment game where two replicas one for each cluster are placed each 
light node in each cluster pays n to the dark node and the dark node replicates the object here the optimistic price of anarchy is one 
at i then it is placed at another server within distance α of i so i has 
no incentive to cache if the replica is placed at i then the second 
condition ensures there is no incentive to drop the replica because 
no two servers separated by distance less than α both place replicas 
 social optimum 
the social cost of a given strategy profile is defined as the total 
cost incurred by all servers namely 
c s 
n− 
i 
ci s 
where ci s is the cost incurred by server i given by equation 
the social optimum cost referred to as c so for the remainder 
of the paper is the minimum social cost the social optimum cost 
will serve as an important base case against which to measure the 
cost of selfish caching we define c so as 
c so min 
s 
c s 
where s varies over all possible strategy profiles note that in the 
basic game this means varying configuration x over all possible 
configurations in some sense c so represents the best possible 
caching behavior - if only nodes could be convinced to cooperate 
with one another 
the social optimum configuration is a solution of a mini-sum 
facility location problem which is np-hard to find such 
configurations we formulate an integer programming problem 
minimize 
èi 
èj 
¢αij xij 
èk wij dikyijk 
£ 
subject to 
∀i j 
èk yijk i wij 
∀i j k xij − ykji ≥ 
∀i j xij ∈ { } 
∀i j k yijk ∈ { } 
 
here xij is if server i replicates object j and otherwise yijk 
is if server i accesses object j from server k and otherwise 
i w returns if w is nonzero and otherwise the first constraint 
specifies that if server i has demand for object j then it must access 
j from exactly one server the second constraint ensures that server 
i replicates object j if any other server accesses j from i 
 analysis 
to analyze the basic game we first give a proof of the existence 
of pure strategy nash equilibria we discuss the price of anarchy in 
general and then on specific underlying topologies in this analysis 
we use simply α in place of αij since we deal with a single object 
and we assume placement cost is the same for all servers in 
addition when we compute the price of anarchy we assume that all 
nodes have the same demand i e ∀i ∈ n wij 
theorem pure strategy nash equilibria exist in the basic 
game 
proof we show a constructive proof first initialize the set 
v to n then remove all nodes with zero demand from v each 
node x defines βx where βx α 
wxj 
 furthermore let z y 
{z dzy ≤ βz z ∈ v } z y represents all nodes z for which y 
lies within βz from z 
pick a node y ∈ v such that βy ≤ βx for all x ∈ v place a 
replica at y and then remove y and all z ∈ z y from v no such z 
can have incentive to replicate the object because it can access y s 
replica at lower or equal cost iterate this process of placing 
replicas until v is empty because at each iteration y is the remaining 
node with minimum β no replica will be placed within distance 
βy of any such y by this process the resulting configuration is a 
pure-strategy nash equilibrium of the basic game 
the price of anarchy poa to quantify the cost of lack of 
coordination we use the price of anarchy and the optimistic 
price of anarchy the price of anarchy is the ratio of the social 
costs of the worst-case nash equilibrium and the social optimum 
and the optimistic price of anarchy is the ratio of the social costs of 
the best-case nash equilibrium and the social optimum 
we show general bounds on the price of anarchy throughout 
our discussion we use c sw to represent the cost of worst case 
nash equilibrium c so to represent the cost of social optimum 
and poa to represent the price of anarchy which is c sw 
c so 
 
the worst case nash equilibrium maximizes the total cost 
under the constraint that the configuration meets the nash condition 
formally we can define c sw as follows 
c sw max 
x∈e 
 α x 
i 
min 
j∈x 
dij 
where minj∈x dij is the distance to the closest replica including i 
itself from node i and x varies through nash equilibrium 
configurations 
bounds on the price of anarchy we show bounds of the price 
of anarchy varying α let dmin min i j ∈n×n i j dij and 
dmax max i j ∈n×n dij we see that if α ≤ dmin poa 
 
topology poa 
complete graph 
star ≤ 
line o 
√ 
n 
d-dimensional grid o n 
d 
d 
table poa in the basic game for specific topologies 
trivially since every server caches the object for both nash 
equilibrium and social optimum when α dmax there is a transition 
in nash equilibria since the placement cost is greater than any 
distance cost only one server caches the object and other servers 
access it remotely however the social optimum may still place 
multiple replicas since α ≤ c so ≤ α minj∈n 
èi dij when α 
dmax we obtain 
α maxj∈n 
èi dij 
α minj∈n 
èi dij 
≤ poa ≤ 
α maxj∈n 
èi dij 
α 
 
note that depending on the underlying topology even the lower 
bound of poa can be o n finally there is a transition when 
α maxj∈n 
èi dij in this case poa 
α maxj∈n 
èi dij 
α minj∈n 
èi dij 
and 
it is upper bounded by 
figure shows an example of the inefficiency of a nash 
equilibrium in the network there are two clusters of servers whose 
size is n 
 
 the distance between two clusters is α − where α is 
the placement cost figure a shows a nash equilibrium where 
one server in a cluster caches the object in this case c sw 
α α − n 
 
 since all servers in the other cluster accesses the 
remote replica however the social optimum places two replicas one 
for each cluster as shown in figure b therefore c so α 
poa 
α α− n 
 
 α 
 which is o n this bad price of anarchy 
comes from an undersupply of replicas due to the selfish nature of 
the servers note that all nash equilibria have the same cost thus 
even the optimistic price of anarchy is o n 
in appendix a we analyze the price of anarchy with specific 
underlying topologies and show that poa can have tighter bounds 
than o n for the complete graph star line and d-dimensional 
grid in these topologies we set the distance between directly 
connected nodes to one we describe the case where α since 
poa trivially when α ≤ a summary of the results is 
shown in table 
 payment game 
in this section we present an extension to the basic game with 
payments and analyze the price of anarchy and the optimistic price 
of anarchy of the game 
 game model 
the new game which we refer to as the payment game allows 
each player to offer a payment to another player to give the latter 
incentive to replicate the object the cost of replication is shared 
among the nodes paying the server that replicates the object 
the strategy for each player i is specified by a triplet vi bi ti ∈ 
{n ê ê } vi specifies the player to whom i makes a bid 
bi ≥ is the value of the bid and ti ≥ denotes a threshold 
for payments beyond which i will replicate the object in addition 
we use ri to denote the total amount of bids received by a node i 
 ri 
èj vj i bj 
a node i replicates the object if and only if ri ≥ ti that is the 
amount of bids it receives is greater than or equal to its threshold 
let ii denote the corresponding indicator variable that is ii equals 
 if i replicates the object and otherwise we make the rule that 
if a node i makes a bid to another node j and j replicates the object 
then i must pay j the amount bi if j does not replicate the object 
i does not pay j 
given a strategy profile the outcome of the game is the set of 
tuples { ii vi bi ri } ii tells us whether player i replicates the 
object or not bi is the payment player i makes to player vi and 
ri is the total amount of bids received by player i to compute 
the payoffs given the outcome we must now take into account the 
payments a node makes in addition to the placement costs and 
access costs of the basic game 
by our rules a server node i pays bi to node vi if vi replicates 
the object and receives a payment of ri if it replicates the object 
itself its net payment is biivi − riii the total cost incurred by 
each node is the sum of its placement cost access cost and net 
payment it is defined as 
ci s αij ii wij di i j − ii biivi − riii 
the cost of social optimum for the payment game is same as that 
for the basic game since the net payments made cancel out 
 analysis 
in analyzing the payment model we first show that a nash 
equilibrium in the basic game is also a nash equilibrium in the payment 
game we then present an important positive result - in the 
payment game the socially optimal configuration can always be 
implemented by a nash equilibrium we know from the counterexample 
in figure that this is not guaranteed in the the basic game in this 
analysis we use α to represent αij 
theorem any configuration that is a pure strategy nash 
equilibrium in the basic game is also a pure strategy nash 
equilibrium in the payment game therefore the price of anarchy of the 
payment game is at least that of the basic game 
proof consider any nash equilibrium configuration in the 
basic game for each node i replicating the object set its threshold ti 
to everyone else has threshold α also for all i bi 
a node that replicates the object does not have incentive to change 
its strategy changing the threshold does not decrease its cost and 
it would have to pay at least α to access a remote replica or 
incentivize a nearby node to cache therefore it is better off keeping its 
threshold and bid at and replicating the object 
a node that is not replicating the object can access the object 
remotely at a cost less than or equal to α lowering its threshold does 
not decrease its cost since all bi are zero the payment necessary 
for another server to place a replica is at least α 
no player has incentive to deviate so the current configuration 
is a nash equilibrium 
in fact appendix b shows that the poa of the payment game 
can be more than that of the basic game in a given topology 
now let us look at what happens to the example shown in 
figure in the best case suppose node b s neighbors each decide 
to pay node b an amount n b does not have an incentive to 
deviate since accessing the remote replica does not decrease its 
cost the same argument holds for a because of symmetry in the 
graph since no one has an incentive to deviate the configuration is 
a nash equilibrium its total cost is α the same as in the socially 
optimal configuration shown in figure b next we prove that 
indeed the payment game always has a strategy profile that 
implements the socially optimal configuration as a nash equilibrium we 
first present the following observation which is used in the proof 
about thresholds in the payment game 
observation if node i replicates the object j is the 
nearest node to i among the other nodes that replicate the object and 
dij α in a nash equilibrium then i should have a threshold at 
 
least α − dij otherwise it cannot collect enough payment to 
compensate for the cost of replicating the object and is better off 
accessing the replica at j 
theorem in the payment game there is always a pure 
strategy nash equilibrium that implements the social optimum 
configuration the optimistic price of anarchy in the payment game is 
therefore always one 
proof consider the socially optimal configuration φopt let 
no be the set of nodes that replicate the object and nc n − no 
be the rest of the nodes also for each i in no let qi denote the 
set of nodes that access the object from i not including i itself in 
the socially optimal configuration dij ≤ α for all j in qi 
we want to find a set of payments and thresholds that makes this 
configuration implementable the idea is to look at each node i in 
no and distribute the minimum payment needed to make i replicate 
the object among the nodes that access the object from i for each 
i in no and for each j in qi we define 
δj min{α min 
k∈no−{i} 
djk} − dji 
note that δj is the difference between j s cost for accessing the 
replica at i and j s next best option among replicating the object 
and accessing some replica other than i it is clear that δj ≥ 
claim for each i ∈ no let be the nearest node to i in 
no then 
èj∈qi 
δj ≥ α − di 
proof of claim assume the contrary that is 
èj∈qi 
δj 
α − di consider the new configuration φnew wherein i does not 
replicate and each node in qi chooses its next best strategy either 
replicating or accessing the replica at some node in no − {i} in 
addition we still place replicas at each node in no − {i} it is easy 
to see that cost of φopt minus cost of φnew is at least 
 α 
j∈qi 
dij − di 
j∈qi 
min{α min 
k∈no−{i} 
dik} 
 α − di − 
j∈qi 
δj 
which contradicts the optimality of φopt 
we set bids as follows for each i in no bi and for each j 
in qi j bids to i i e vj i the amount 
bj max{ δj − i qi } j ∈ qi 
where i 
èj∈qi 
δj − α di ≥ and qi is the cardinality of 
qi for the thresholds we have 
ti 
α if i ∈ nc èj∈qi 
bj if i ∈ no 
 
this fully specifies the strategy profile of the nodes and it is easy 
to see that the outcome is indeed the socially optimal configuration 
next we verify that the strategies stipulated constitute a nash 
equilibrium having set ti to α for i in nc means that any node 
in n is at least as well off lowering its threshold and replicating 
as bidding α to some node in nc to make it replicate so we may 
disregard the latter as a profitable strategy by observation to 
ensure that each i in no does not deviate we require that if is the 
nearest node to i in no then 
èj∈qi 
bj is at least α − di 
otherwise i will raise ti above 
èj∈qi 
bj so that it does not replicate 
and instead accesses the replica at we can easily check that 
j∈qi 
bj ≥ 
j∈qi 
δj − 
 qi i 
 qi 
 α − di 
i 
 qi 
≥ α − di 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c ne c so 
averagenumberofreplicas 
alpha 
poa 
ratio 
opoa 
replica so 
replica ne 
figure we present p oa ratio and op oa results for the basic 
game varying α on a -node line topology and we show number 
of replicas placed by the nash equilibria and by the optimal solution 
we see large peaks in p oa and op oa at α where a phase 
transition causes an abrupt transition in the lines 
therefore each node i ∈ no does not have incentive to change 
ti since i loses its payments received or there is no change and i 
does not have incentive to bi since it replicates the object each 
node j in nc has no incentive to change tj since changing tj does 
not reduce its cost it also does not have incentive to reduce bj 
since the node where j accesses does not replicate and j has to 
replicate the object or to access the next closest replica which costs 
at least the same from the definition of bj no player has incentive 
to deviate so this strategy profile is a nash equilibrium 
 simulation 
we run simulations to compare nash equilibria for the 
singleobject caching game with the social optimum computed by solving 
the integer linear program described in equation using mosek 
we examine price of anarchy poa optimistic price of anarchy 
 opoa and the average ratio of the costs of nash equilibria and 
social optima ratio and when relevant we also show the average 
numbers of replicas placed by the nash equilibrium replica ne 
and the social optimum replica so the poa and opoa are 
taken from the worst and best nash equilibria respectively that we 
observe over the runs each data point in our figures is based on 
 runs randomly varying the initial strategy profile and player 
order the details of the simulations including protocols and a 
discussion of convergence are presented in appendix c 
in our evaluation we study the effects of variation in four 
categories placement cost underlying topology demand distribution 
and payments as we vary the placement cost α we directly 
influence the tradeoff between caching and not caching in order to get 
a clear picture of the dependency of poa on α in a simple case we 
first analyze the basic game with a -node line topology whose 
edge distance is one 
we also explore transit-stub topologies generated using the 
gtitm library and power-law topologies router-level 
barabasialbert model generated using the brite topology generator 
for these topologies we generate an underlying physical graph of 
 physical nodes both topologies have similar minimum 
average and maximum physical node distances the average distance 
is we create an overlay of server nodes and use the same 
overlay for all experiments with the given topology 
in the game each server has a demand whose distribution is 
bernoulli p where p is the probability of having demand for the 
object the default unless otherwise specified is p 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c ne c so 
averagenumberofreplicas 
alpha 
poa 
ratio 
opoa 
replica so 
replica ne 
 a 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c ne c so 
averagenumberofreplicas 
alpha 
poa 
ratio 
opoa 
replica so 
replica ne 
 b 
figure transit-stub topology a basic game b payment game we show the p oa ratio op oa and the number of replicas placed while 
varying α between and with servers on a -physical-node transit-stub topology 
 
 
 
 
 
 
 
 
 
 
 
 
 
c ne c so 
averagenumberofreplicas 
alpha 
poa 
ratio 
opoa 
replica so 
replica ne 
 a 
 
 
 
 
 
 
 
 
 
 
 
 
 
c ne c so 
averagenumberofreplicas 
alpha 
poa 
ratio 
opoa 
replica so 
replica ne 
 b 
figure power-law topology a basic game b payment game we show the p oa ratio op oa and the number of replicas placed while 
varying α between and with servers on a -physical-node power-law topology 
 varying placement cost 
figure shows poa opoa and ratio as well as number 
of replicas placed for the line topology as α varies we observe 
two phases as α increases the poa rises quickly to a peak at 
 after there is a gradual decline opoa and ratio show 
behavior similar to poa 
these behaviors can be explained by examining the number of 
replicas placed by nash equilibria and by optimal solutions we see 
that when α is above one nash equilibrium solutions place fewer 
replicas than optimal on average for example when α is 
the social optimum places four replicas but the nash equilibrium 
places only one the peak in poa at α occurs at the point 
for a -node line where the worst-case cost of accessing a remote 
replica is slightly less than the cost of placing a new replica so 
selfish servers will never place a second replica the optimal solution 
however places multiple replicas to decrease the high global cost 
of access as α continues to increase the undersupply problem 
lessens as the optimal solution places fewer replicas 
 different underlying topologies 
in figure a we examine an overlay graph on the more realistic 
transit-stub topology the trends for the poa opoa and ratio 
are similar to the results for the line topology with a peak in poa 
at α due to maximal undersupply 
in figure a we examine an overlay graph on the power-law 
topology we observe several interesting differences between the 
power-law and transit-stub results first the poa peaks at a lower 
level in the power-law graph around at α while the 
peak poa in the transit-stub topology is almost at α 
after the peak poa and ratio decrease more slowly as α 
increases opoa is close to one for the whole range of α values 
this can be explained by the observation in figure a that there 
is no significant undersupply problem here like there was in the 
transit-stub graph indeed the high poa is due mostly to 
misplacement problems when α is from to since there is little 
decrease in poa when the number of replicas in social optimum 
changes from two to one the opoa is equal to one in the figure 
when the same number of replicas are placed 
 varying demand distribution 
now we examine the effects of varying the demand distribution 
the set of servers with demand is random for p so we 
calculate the expected poa by averaging over trials each data point 
is based on runs we run simulations for demand levels of 
p ∈ { } as α is varied on the servers on top of 
the transit-stub graph we observe that as demand falls so does 
expected poa as p decreases the number of replicas placed in 
the social optimum decreases but the number in nash equilibria 
changes little furthermore when α exceeds the overlay diameter 
the number in nash equilibria stays constant when p varies 
therefore lower p leads to a lesser undersupply problem agreeing with 
intuition we do not present the graph due to space limitations and 
redundancy the poa for p is identical to poa in figure a 
and the lines for p and p are similar but lower and flatter 
 
 effects of payment 
finally we discuss the effects of payments on the efficiency of 
nash equilibria the results are presented in figure b and 
figure b as shown in the analysis the simulations achieve opoa 
close to one it is not exactly one because of randomness in the 
simulations the ratio for the payment game is much lower than 
the ratio for the basic game since the protocol for the payment 
game tends to explore good regions in the space of nash 
equilibria we observe in figure that for α ≥ the average number 
of replicas of nash equilibria gets closer with payments to that of 
the social optimum than it does without we observe in figure 
that more replicas are placed with payments than without when α 
is between and the only range of significant undersupply in 
the power-law case the results confirm that payments give servers 
incentive to replicate the object and this leads to better equilibria 
 discussion and future work 
we suggest several interesting extensions and directions one 
extension is to consider multiple objects in the capacitated caching 
game in which servers have capacity limits when placing objects 
since caching one object affects the ability to cache another there 
is no separability of a multi-object game into multiple single object 
games as studied in one way to formulate this problem is to 
find the best response of a server by solving a knapsack problem 
and to compute nash equilibria 
in our analyses we assume that all nodes have the same demand 
however nodes could have different demand depending on objects 
we intend to examine the effects of heterogeneous demands or 
heterogeneous placement costs analytically we also want to look 
at the following aggregation effect suppose there are n − 
clustered nodes with distance of α− from a node hosting a replica 
all nodes have demands of one in that case the price of anarchy 
is o n however if we aggregate n − nodes into one node with 
demand n − the price of anarchy becomes o since α should 
be greater than n − α − to replicate only one object such 
aggregation can reduce the inefficiency of nash equilibria 
we intend to compute the bounds of the price of anarchy under 
different underlying topologies such as random graphs or 
growthrestricted metrics we want to investigate whether there are certain 
distance constraints that guarantee o price of anarchy in 
addition we want to run large-scale simulations to observe the change 
in the price of anarchy as the network size increases 
another extension is to consider server congestion suppose the 
distance is the network distance plus γ × number of accesses 
where γ is an extra delay when an additional server accesses the 
replica then when α γ it can be shown that poa is bounded 
by α 
γ 
 as γ increases the price of anarchy bound decreases since 
the load of accesses is balanced across servers 
while exploring the caching problem we made several 
observations that seem counterintuitive first the poa in the payment 
game can be worse than the poa in the basic game another 
observation we made was that the number of replicas in a nash 
equilibrium can be more than the number of replicas in the social 
optimum even without payments for example a graph with diameter 
slightly more than α may have a nash equilibrium configuration 
with two replicas at the two ends however the social optimum 
may place one replica at the center we leave the investigation of 
more examples as an open issue 
 conclusions 
in this work we introduce a novel non-cooperative game model 
to characterize the caching problem among selfish servers without 
any central coordination we show that pure strategy nash 
equilibria exist in the game and that the price of anarchy can be o n in 
general where n is the number of servers due to undersupply 
problems with specific topologies we show that the price of anarchy 
can have tighter bounds more importantly with payments servers 
are incentivized to replicate and the optimistic price of anarchy is 
always one non-cooperative caching is a more realistic model than 
cooperative caching in the competitive internet hence this work is 
an important step toward viable federated caching systems 
 acknowledgments 
we thank kunal talwar for enlightening discussions regarding 
this work 
 references 
 http www mosek com 
 a adya et al farsite federated available and reliable 
storage for an incompletely trusted environment in proc 
of usenix osdi 
 e anshelevich a dasgupta e tardos and t wexler 
near-optimal network design with selfish agents in proc 
of acm stoc 
 y chen r h katz and j d kubiatowicz scan a 
dynamic scalable and efficient content distribution 
network in proc of intl conf on pervasive computing 
 
 f dabek et al wide-area cooperative storage with cfs in 
proc of acm sosp oct 
 p b danzig netcache architecture and deploment in 
computer networks and isdn systems 
 n devanur m mihail and v vazirani strategyproof 
cost-sharing mechanisms for set cover and facility 
location games in proc of acm ec 
 j r douceur and r p wattenhofer large-scale simulation 
of replica placement algorithms for a serverless distributed 
file system in proc of mascots 
 a fabrikant c h papadimitriou and k talwar the 
complexity of pure nash equilibria in proc of acm stoc 
 
 l fan p cao j almeida and a z broder summary 
cache a scalable wide-area web cache sharing protocol 
ieee acm trans on networking - 
 m r garey and d s johnson computers and 
intractability a guide to the theory of np-completeness 
w h freeman and co 
 m x goemans l li v s mirrokni and m thottan 
market sharing games applied to content distribution in 
ad-hoc networks in proc of acm mobihoc 
 m x goemans and m skutella cooperative facility 
location games in proc of acm-siam soda 
 s gribble et al what can databases do for peer-to-peer in 
webdb workshop on databases and the web june 
 k p gummadi et al measurement modeling and analysis 
of a peer-to-peer file-sharing workload in proc of acm 
sosp october 
 s iyer a rowstron and p druschel squirrel a 
decentralized peer-to-peer web cache in proc of acm 
podc 
 k jain and v v vazirani primal-dual approximation 
algorithms for metric facility location and k-median 
problems in proc of ieee focs 
 
 s jamin et al on the placement of internet instrumentation 
in proc of ieee infocom pages - 
 s jamin et al constrained mirror placement on the internet 
in proc of ieee infocom pages - 
 b -j ko and d rubenstein a distributed self-stabilizing 
protocol for placement of replicated resources in emerging 
networks in proc of ieee icnp 
 e koutsoupias and c papadimitriou worst-case equilibria 
in stacs 
 j kubiatowicz et al oceanstore an architecture for 
global-scale persistent storage in proc of acm asplos 
acm november 
 b li m j golin g f italiano and x deng on the 
optimal placement of web proxies in the internet in proc 
of ieee infocom 
 m mahdian y ye and j zhang improved approximation 
algorithms for metric facility location problems in proc 
of intl workshop on approximation algorithms for 
combinatorial optimization problems 
 a medina a lakhina i matta and j byers brite 
universal topology generation from a user s perspective 
technical report - 
 r r mettu and c g plaxton the online median problem 
in proc of ieee focs 
 p b mirchandani and r l francis discrete location 
theory wiley-interscience series in discrete mathematics 
and optimization 
 m j osborne and a rubinstein a course in game theory 
mit press 
 m pal and e tardos group strategyproof mechanisms via 
primal-dual algorithms in proc of ieee focs 
 l qiu v n padmanabhan and g m voelker on the 
placement of web server replicas in proc of ieee 
infocom 
 m rabinovich i rabinovich r rajaraman and 
a aggarwal a dynamic object replication and migration 
protocol for an internet hosting service in proc of ieee 
icdcs 
 a rowstron and p druschel storage management and 
caching in past a large-scale persistent peer-to-peer 
storage utility in proc of acm sosp october 
 y saito c karamanolis m karlsson and m mahalingam 
taming aggressive replication in the pangaea wide-area 
file system in proc of usenix osdi 
 x tang and s t chanson coordinated en-route web 
caching in ieee trans computers 
 a vetta nash equilibria in competitive societies with 
applications to facility location traffic routing and 
auctions in proc of ieee focs 
 e w zegura k l calvert and s bhattacharjee how to 
model an internetwork in proc of ieee infocom 
appendix 
a analyzing specific topologies 
we now analyze the price of anarchy poa for the basic game 
with specific underlying topologies and show that poa can have 
better bounds we look at complete graph star line and 
ddimensional grid in all these topologies we set the distance 
between two directly connected nodes to one we describe the case 
where α since poa trivially when α ≤ 
a bc d 
α 
 
α 
 
 α 
 
α 
 
α 
figure example where the payment game has a nash equilibrium 
which is worse than any nash equilibrium in the basic game the 
unlabeled distances between the nodes in the cluster are all the 
thresholds of white nodes are all α and the thresholds of dark nodes are all 
α the two dark nodes replicate the object in this payment game 
nash equilibrium 
for a complete graph poa and for a star poa ≤ 
for a complete graph when α both nash equilibria and 
social optima place one replica at one server so poa for 
star when α the worst case nash equilibrium places 
replicas at all leaf nodes however the social optimum places 
one replica at the center node therefore poa n− α 
α n− 
≤ 
 n− 
 n− 
≤ when α the worst case nash equilibrium 
places one replica at a leaf node and the other nodes access the 
remote replica and the social optimum places one replica at the 
center poa α n− 
α n− 
 n 
α n− 
≤ 
for a line the price of anarchy is o 
√ 
n when α n 
the worst case nash equilibrium places replicas every α so that 
there is no overlap between areas covered by two adjacent servers 
that cache the object the social optimum places replicas at least 
every 
√ 
 α the placement of replicas for the social optimum is 
as follows suppose there are two replicas separated by distance 
d by placing an additional replica in the middle we want to have 
the reduction of distance to be at least α the distance reduction 
is d { d − − d − − d − 
d − d } ≥ d 
 d should be at most 
√ 
 α therefore the 
distance between replicas in the social optimum is at most 
√ 
 α 
c sw α n− 
 α 
 α α 
 
 n− 
 α 
 θ αn c so ≥ α n− √ 
 α 
 
 
√ 
 α 
√ 
 α 
 
n− √ 
 α 
 c so ω 
√ 
αn therefore poa 
o 
√ 
α when α n − the worst case nash equilibrium places 
one replica at a leaf node and c sw α n− n 
 
 however 
the social optimum still places replicas every 
√ 
 α if we view 
poa as a continuous function of α and compute a derivative of 
poa the derivative becomes when α is θ n 
 which means 
the function decreases as α increases from n therefore poa is 
maximum when α is n and poa θ n 
 
ω 
√ 
nn 
 o 
√ 
n when 
α n− n 
 
 the social optimum also places only one replica and 
poa is trivially bounded by this result holds for the ring and 
it can be generalized to the d-dimensional grid as the dimension 
in the grid increases the distance reduction of additional replica 
placement becomes ω dd 
 where d is the distance between two 
adjacent replicas therefore poa θ n 
ω n 
 
d n 
 o n 
d 
d 
b payment can do worse 
consider the network in figure where α α any nash 
equilibrium in the basic game model would have exactly two 
replicas - one in the left cluster and one in the right it is easy to verify 
that the worst placement in terms of social cost of two replicas 
occurs when they are placed at nodes a and b this placement can 
be achieved as a nash equilibrium in the payment game but not in 
the basic game since a and b are a distance α apart 
 
algorithm initialization for the basic game 
l a random subset of servers 
for each node i in n do 
if i ∈ l then 
si replicate the object 
else 
si 
algorithm move selection of i for the basic game 
cost α 
cost minj∈x−{i} dij x is the current configuration 
costmin min{cost cost } 
if costnow costmin then 
if costmin cost then 
si 
else 
si 
c nash dynamics protocols 
the simulator initializes the game according to the given 
parameters and a random initial strategy profile and then iterates through 
rounds initially the order of player actions is chosen randomly in 
each round each server performs the nash dynamics protocol that 
adjusts its strategies greedily in the chosen order when a round 
passes without any server changing its strategy the simulation ends 
and a nash equilibrium is reached 
in the basic game we pick a random initial subset of servers to 
replicate the object as shown in algorithm after the 
initialization each player runs the move selection procedure described in 
algorithm in algorithms and costnow represents the 
current cost for node i this procedure chooses greedily between 
replication and non-replication it is not hard to see that this nash 
dynamics protocol converges in two rounds 
in the payment game we pick a random initial subset of servers 
to replicate the object by setting their thresholds to in addition 
we initialize a second random subset of servers to replicate the 
object with payments from other servers the details are shown in 
algorithm after the initialization each player runs the move 
selection procedure described in algorithm this procedure chooses 
greedily between replication and accessing a remote replica with 
the possibilities of receiving and making payments respectively 
in the protocol each node increases its threshold value by incr if it 
does not replicate the object by this ramp up procedure the cost of 
replicating an object is shared fairly among the nodes that access a 
replica from a server that does cache if incr is small cost is shared 
more fairly and the game tends to reach equilibria that encourages 
more servers to store replicas though the convergence takes longer 
if incr is large the protocol converges quickly but it may miss 
efficient equilibria in the simulations we set incr to most of our 
a 
b c 
a 
b 
c 
α 
 α − 
 α 
figure an example where the nash dynamics protocol does not 
converge in the payment game 
algorithm initialization for the payment game 
l a random subset of servers 
for each node i in n do 
bi 
if i ∈ l then 
ti replicate the object 
else 
ti α 
l {} 
for each node i in n do 
if coin toss head then 
mi {j d j i mink∈l ∪l d j k } 
if mi ∅ then 
for each node j ∈ mi do 
bj max{ 
α 
èk∈mi 
d i k 
 mi 
− d i j } 
l l ∪ {i} 
algorithm move selection of i for the payment game 
cost α − ri 
cost minj∈n−{i}{tj − rj dij } 
costmin min{cost cost } 
if costnow costmin then 
if costmin cost then 
ti ri 
else 
ti ri incr 
vi argminj{tj − rj dij} 
bi tvi − rvi 
simulation runs converged but there were a very few cases where 
the simulation did not converge due to the cycles of dynamics the 
protocol does not guarantee convergence within a certain number 
of rounds like the protocol for the basic game 
we provide an example graph and an initial condition such that 
the nash dynamics protocol does not converge in the payment game 
if started from this initial condition the graph is represented by 
a shortest path metric on the network shown in figure in the 
starting configuration only a replicates the object and a pays it 
an amount α to do so the thresholds for a b and c are α 
each and the thresholds for a b and c are α it is not hard to 
verify that the nash dynamics protocol will never converge if we 
start with this condition 
the nash dynamics protocol for the payment game needs 
further investigation the dynamics protocol for the payment game 
should avoid cycles of actions to achieve stabilization of the 
protocol finding a self-stabilizing dynamics protocol is an interesting 
problem in addition a fixed value of incr cannot adapt to changing 
environments a small value of incr can lead to efficient equilibria 
but it can take long time to converge an important area for future 
research is looking at adaptively changing incr 
 
fairness in dead-reckoning based distributed 
multi-player games 
sudhir aggarwal hemant banavar 
department of computer science 
florida state university tallahassee fl 
email {sudhir banavar} cs fsu edu 
sarit mukherjee sampath rangarajan 
center for networking research 
bell laboratories holmdel nj 
email {sarit sampath} bell-labs com 
abstract 
in a distributed multi-player game that uses dead-reckoning vectors 
to exchange movement information among players there is 
inaccuracy in rendering the objects at the receiver due to network delay 
between the sender and the receiver the object is placed at the 
receiver at the position indicated by the dead-reckoning vector but by 
that time the real position could have changed considerably at the 
sender this inaccuracy would be tolerable if it is consistent among 
all players that is at the same physical time all players see 
inaccurate with respect to the real position of the object but the same 
position and trajectory for an object but due to varying network 
delays between the sender and different receivers the inaccuracy 
is different at different players as well this leads to unfairness 
in game playing in this paper we first introduce an error 
measure for estimating this inaccuracy then we develop an algorithm 
for scheduling the sending of dead-reckoning vectors at a sender 
that strives to make this error equal at different receivers over time 
this algorithm makes the game very fair at the expense of 
increasing the overall mean error of all players to mitigate this effect we 
propose a budget based algorithm that provides improved fairness 
without increasing the mean error thereby maintaining the accuracy 
of game playing we have implemented both the scheduling 
algorithm and the budget based algorithm as part of bzflag a popular 
distributed multi-player game we show through experiments that 
these algorithms provide fairness among players in spite of widely 
varying network delays an additional property of the proposed 
algorithms is that they require less number of drs to be exchanged 
 compared to the current implementation of bzflag to achieve the 
same level of accuracy in game playing 
categories and subject descriptors 
c computer-communication networks distributed 
systems-distributed applications 
general terms 
algorithms design experimentation performance 
 introduction 
in a distributed multi-player game players are normally 
distributed across the internet and have varying delays to each other 
or to a central game server usually in such games the players are 
part of the game and in addition they may control entities that make 
up the game during the course of the game the players and the 
entities move within the game space a player sends information 
about her movement as well as the movement of the entities she 
controls to the other players using a dead-reckoning dr vector 
a dr vector contains information about the current position of the 
player entity in terms of x y and z coordinates at the time the dr 
vector was sent as well as the trajectory of the entity in terms of 
the velocity component in each of the dimensions each of the 
participating players receives such dr vectors from one another and 
renders the other players entities on the local consoles until a new 
dr vector is received for that player entity in a peer-to-peer game 
players send dr vectors directly to each other in a client-server 
game these dr vectors may be forwarded through a game server 
the idea of dr is used because it is almost impossible for 
players entities to exchange their current positions at every time unit 
dr vectors are quantization of the real trajectory which we refer 
to as real path at a player normally a new dr vector is computed 
and sent whenever the real path deviates from the path extrapolated 
using the previous dr vector say in terms of distance in the x y 
z plane by some amount specified by a threshold we refer to the 
trajectory that can be computed using the sequence of dr vectors 
as the exported path therefore at the sending player there is a 
deviation between the real path and the exported path the error due 
to this deviation can be removed if each movement of player entity 
is communicated to the other players at every time unit that is a 
dr vector is generated at every time unit thereby making the real 
and exported paths the same given that it is not feasible to 
satisfy this due to bandwidth limitations this error is not of practical 
interest therefore the receiving players can at best follow the 
exported path because of the network delay between the sending 
and receiving players when a dr vector is received and rendered 
at a player the original trajectory of the player entity may have 
already changed thus in physical time there is a deviation at the 
receiving player between the exported path and the rendered 
trajectory which we refer to as placed path we refer to this error 
as the export error note that the export error in turn results in a 
deviation between the real and the placed paths 
the export error manifests itself due to the deviation between the 
exported path at the sender and the placed path at the receiver i 
 
before the dr vector is received at the receiver referred to as the 
before export error and ii after the dr vector is received at the 
receiver referred to as the after export error in an earlier paper 
we showed that by synchronizing the clocks at all the players and 
by using a technique based on time-stamping messages that carry 
the dr vectors we can guarantee that the after export error is made 
zero that is the placed and the exported paths match after the dr 
vector is received we also showed that the before export error can 
never be eliminated since there is always a non-zero network delay 
but can be significantly reduced using our technique 
henceforth we assume that the players use such a technique which results 
in unavoidable but small overall export error 
in this paper we consider the problem of different and varying 
network delays between each sender-receiver pair of a dr vector 
and consequently the different and varying export errors at the 
receivers due to the difference in the export errors among the 
receivers the same entity is rendered at different physical time at 
different receivers this brings in unfairness in game playing for 
instance a player with a large delay would always see an entity 
late in physical time compared to the other players and 
therefore her action on the entity would be delayed in physical time 
even if she reacted instantaneously after the entity was rendered 
our goal in this paper is to improve the fairness of these games in 
spite of the varying network delays by equalizing the export error 
at the players we explore whether the time-average of the export 
errors which is the cumulative export error over a period of time 
averaged over the time period at all the players can be made the 
same by scheduling the sending of the dr vectors appropriately at 
the sender we propose two algorithms to achieve this 
both the algorithms are based on delaying or dropping the 
sending of dr vectors to some players on a continuous basis to 
try and make the export error the same at all the players at an 
abstract level the algorithm delays sending dr vectors to players 
whose accumulated error so far in the game is smaller than others 
this would mean that the export error due to this dr vector at these 
players will be larger than that of the other players thereby making 
them the same the goal is to make this error at least approximately 
equal at every dr vector with the deviation in the error becoming 
smaller as time progresses 
the first algorithm which we refer to as the scheduling 
algorithm is based on estimating the delay between players and 
refining the sending of dr vectors by scheduling them to be sent 
to different players at different times at every dr generation point 
through an implementation of this algorithm using the open source 
game bzflag we show that this algorithm makes the game very fair 
 we measure fairness in terms of the standard deviation of the 
error the drawback of this algorithm is that it tends to push the 
error of all the players towards that of the player with the worst 
error which is the error at the farthest player in terms of delay 
from the sender of the dr to alleviate this effect we propose 
a budget based algorithm which budgets how the drs are sent to 
different players at a high level the algorithm is based on the 
idea of sending more drs to players who are farther away from 
the sender compared to those who are closer experimental results 
from bzflag illustrates that the budget based algorithm follows a 
more balanced approach it improves the fairness of the game but 
at the same time does so without pushing up the mean error of the 
players thereby maintaining the accuracy of the game in addition 
the budget based algorithm is shown to achieve the same level of 
accuracy of game playing as the current implementation of bzflag 
using much less number of dr vectors 
 previous work 
earlier work on network games to deal with network latency has 
mostly focussed on compensation techniques for packet delay and 
loss these methods are aimed at making large delays and 
message loss tolerable for players but does not consider the 
problems that may be introduced by varying delays from the server to 
different players or from the players to one another for example 
the concept of local lag has been used in where each player 
delays every local operation for a certain amount of time so that 
remote players can receive information about the local operation and 
execute the same operation at the about same time thus reducing 
state inconsistencies the online multi-player game mimaze 
 for example takes a static bucket synchronization approach to 
compensate for variable network delays in mimaze each player 
delays all events by ms regardless of whether they are 
generated locally or remotely players with a network delay larger 
than ms simply cannot participate in the game in general 
techniques based on bucket synchronization depend on imposing a 
worst case delay on all the players 
there have been a few papers which have studied the problem of 
fairness in a distributed game by more sophisticated message 
delivery mechanisms but these works assume the existence of 
a global view of the game where a game server maintains a view 
 or state of the game players can introduce objects into the game 
or delete objects that are already part of the game for example in 
a first-person shooter game by shooting down the object these 
additions and deletions are communicated to the game server 
using action messages based on these action messages the state 
of the game is changed at the game server and these changes are 
communicated to the players using update messages fairness is 
achieved by ordering the delivery of action and update messages at 
the game server and players respectively based on the notion of a 
fair-order which takes into account the delays between the game 
server and the different players objects that are part of the game 
may move but how this information is communicated to the players 
seems to be beyond the scope of these works in this sense these 
works are very limited in scope and may be applicable only to 
firstperson shooter games and that too to only games where players are 
not part of the game 
dr vectors can be exchanged directly among the players 
 peerto-peer model or using a central server as a relay client-server 
model it has been shown in that multi-player games that 
use dr vectors together with bucket synchronization are not 
cheatproof unless additional mechanisms are put in place both the 
scheduling algorithm and the budget-based algorithm described in 
our paper use dr vectors and hence are not cheat-proof for 
example a receiver could skew the delay estimate at the sender to 
make the sender believe that the delay between the sender and the 
receiver is high thereby gaining undue advantage we emphasize 
that the focus of this paper is on fairness without addressing the 
issue of cheating 
in the next section we describe the game model that we use 
and illustrate how senders and receivers exchange dr vectors and 
how entities are rendered at the receivers based on the time-stamp 
augmented dr vector exchange as described in in section 
we describe the dr vector scheduling algorithm that aims to make 
the export error equal across the players with varying delays from 
the sender of a dr vector followed by experimental results 
obtained from instrumentation of the scheduling algorithm on the 
open source game bzflag section describes the budget based 
algorithm that achieves improved fairness but without reducing the 
level accuracy of game playing conclusions are presented in 
section 
 
 game model 
the game architecture is based on players distributed across the 
internet and exchanging dr vectors to each other the dr 
vectors could either be sent directly from one player to another 
 peerto-peer model or could be sent through a game server which 
receives the dr vector from a player and forwards it to other players 
 client-server model as mentioned before we assume 
synchronized clocks among the participating players 
each dr vector sent from one player to another specifies the 
trajectory of exactly one player entity we assume a linear dr vector 
in that the information contained in the dr vector is only enough at 
the receiving player to compute the trajectory and render the entity 
in a straight line path such a dr vector contains information about 
the starting position and velocity of the player entity where the 
velocity is constant 
 thus the dr vectors sent by a player specifies 
the current time at the player when the dr vector is computed not 
the time at which this dr vector is sent to the other players as we 
will explain later the current position of the player entity in terms 
of the x y z coordinates and the velocity vector in the direction 
of x y and z coordinates specifically the ith 
dr vector sent by 
player j about the kth 
entity is denoted by drj 
ik and is represented 
by the following tuple tj 
ik xj 
ik yj 
ik zj 
ik vxj 
ik vyj 
ik vzj 
ik 
without loss of generality in the rest of the discussion we 
consider a sequence of dr vectors sent by only one player and for 
only one entity for simplicity we consider a two dimensional 
game space rather than a three dimensional one hence we use 
dri to denote the ith 
such dr vector represented as the tuple 
 ti xi yi vxi vyi the receiving player computes the starting 
position for the entity based on xi yi and the time difference 
between when the dr vector is received and the time ti at which it 
was computed note that the computation of time difference is 
feasible since all the clocks are synchronized the receiving player 
then uses the velocity components to project and render the 
trajectory of the entity this trajectory is followed until a new dr vector 
is received which changes the position and or velocity of the entity 
timet 
real 
exported 
placed 
dt 
a 
b 
c 
d 
dr 
 t x y vx vy 
computed at time t and 
sent to the receiver 
dr 
 t x y vx vy 
computed at time t and 
sent to the receiver 
t 
dt 
placed 
e 
figure trajectories and deviations 
based on this model figure illustrates the sending and 
receiv 
other type of dr vectors include quadratic dr vectors which 
specify the acceleration of the entity and cubic spline dr vectors 
that consider the starting position and velocity and the ending 
position and velocity of the entity 
ing of dr vectors and the different errors that are encountered the 
figure shows the reception of dr vectors at a player henceforth 
called the receiver the horizontal axis shows the time which is 
synchronized among all the players the vertical axis tries to 
conceptually capture the two-dimensional position of an entity 
assume that at time t a dr vector dr is computed by the sender 
and immediately sent to the receiver assume that dr is received 
at the receiver after a delay of dt time units the receiver 
computes the initial position of the entity as x vx × dt y 
vy × dt shown as point e the thick line ebd represents the 
projected and rendered trajectory at the receiver based on the 
velocity components vx and vy placed path at time t a dr vector 
dr is computed for the same entity and immediately sent to the 
receiver 
 assume that dr is received at the receiver after a delay 
of dt time units when this dr vector is received assume that the 
entity is at point d a new position for the entity is computed as 
 x vx × dt y vy × dt and the entity is moved to this 
position point c the velocity components vx and vy are used 
to project and render this entity further 
let us now consider the error due to network delay although 
dr was computed at time t and sent to the receiver it did not 
reach the receiver until time t dt this means although the 
exported path based on dr at the sender at time t is the 
trajectory ac until time t dt at the receiver this entity was being 
rendered at trajectory bd based on dr only at time t dt 
did the entity get moved to point c from which point onwards the 
exported and the placed paths are the same the deviation between 
the exported and placed paths creates an error component which we 
refer to as the export error a way to represent the export error is 
to compute the integral of the distance between the two trajectories 
over the time when they are out of sync we represent the integral 
of the distances between the placed and exported paths due to some 
dr dri over a time interval t t as err dri t t in the 
figure the export error due to dr is computed as the integral of 
the distance between the trajectories ac and bd over the time 
interval t t dt note that there could be other ways of 
representing this error as well but in this paper we use the integral of 
the distance between the two trajectories as a measure of the export 
error note that there would have been an export error created due 
to the reception of dr at which time the placed path would have 
been based on a previous dr vector this is not shown in the figure 
but it serves to remind the reader that the export error is cumulative 
when a sequence of dr vectors are received starting from time 
t onwards there is a deviation between the real and the exported 
paths as we discussed earlier this export error is unavoidable 
the above figure and example illustrates one receiver only but 
in reality dr vectors dr and dr are sent by the sender to all 
the participating players each of these players receives dr and 
dr after varying delays thereby creating different export error 
values at different players the goal of the dr vector scheduling 
algorithm to be described in the next section is to make this 
 cumulative export error equal at every player independently for each of 
the entities that make up the game 
 scheduling algorithm 
forsending dr vectors 
in section we showed how delay from the sender of a new dr 
 
normally dr vectors are not computed on a periodic basis but 
on an on-demand basis where the decision to compute a new dr 
vector is based on some threshold being exceeded on the deviation 
between the real path and the path exported by the previous dr 
vector 
 
vector to the receiver of the dr vector could lead to export error 
because of the deviation of the placed path from the exported path 
at the receiver until this new dr vector is received we also 
mentioned that the goal of the dr vector scheduling algorithm is to 
make the export error equal at all receivers over a period of time 
since the game is played in a distributed environment it makes 
sense for the sender of an entity to keep track of all the errors at the 
receivers and try to make them equal however the sender cannot 
know the actual error at a receiver till it gets some information 
regarding the error back from the receiver our algorithm estimates 
the error to compute a schedule to send dr vectors to the receivers 
and corrects the error when it gets feedbacks from the receivers in 
this section we provide motivations for the algorithm and describe 
the steps it goes through throughout this section we will use the 
following example to illustrate the algorithm 
timet 
exported path 
placed path 
at receiver 
dt 
a 
b 
c 
d 
e 
f 
t 
g 
g 
dt 
dr sent 
to receiver 
dr sent 
to receiver 
t 
 t 
 
da 
da 
g 
h 
i 
j 
k 
l 
n 
m 
dr estimated 
to be received 
by receiver 
dr estimated 
to be received 
by receiver 
dr actually 
received by 
receiver 
dr actually 
received by 
receiver 
dr sent to 
both receivers 
dr computed 
by sender 
placed path 
at receiver 
figure dr vector flow between a sender and two receivers 
and the evolution of estimated and actual placed paths at the 
receivers dr t t x y vx vy sent at time t to 
both receivers dr t t 
 x y vx vy sent at time 
t 
 t δ to receiver and dr t t 
 x y vx vy 
sent at time t 
 t δ to receiver 
consider the example in figure the figure shows a single 
sender sending dr vectors for an entity to two different receivers 
 and dr computed at t is sent and received by the receivers 
sometime between t and t at which time they move the location 
of the entity to match the exported path thus the path of the 
entity is shown only from the point where the placed path matches 
the exported path for dr now consider dr at time t dr 
is computed by the sender but assume that it is not immediately 
sent to the receivers and is only sent after time δ to receiver 
 at time t 
 t δ and after time δ to receiver at time 
t 
 t δ note that the sender includes the sending 
timestamp with the dr vector as shown in the figure assume that 
the sender estimates it will be clear shortly why the sender has to 
estimate the delay that after a delay of dt receiver will receive 
it will use the coordinate and velocity parameters to compute the 
entity s current location and move it there point c and from this 
time onwards the exported and the placed paths will become the 
same however in reality receiver receives dr after a delay 
of da which is less than sender s estimates of dt and moves 
the corresponding entity to point h similarly the sender estimates 
that after a delay of dt receiver will receive dr will compute 
the current location of the entity and move it to that point point 
e while in reality it receives dr after a delay of da dt and 
moves the entity to point n the other points shown on the placed 
and exported paths will be used later in the discussion to describe 
different error components 
 computation of relative export error 
referring back to the discussion from section from the sender s 
perspective the export error at receiver due to dr is given 
by err dr t t δ dt the integral of the distance 
between the trajectories ac and db over the time interval t t 
δ dt of figure this is due to the fact that the sender uses 
the estimated delay dt to compute this error similarly the 
export error from the sender s perspective at received due to dr 
is given by err dr t t δ dt the integral of the 
distance between the trajectories ae and df over the time interval 
 t t δ dt note that the above errors from the sender s 
perspective are only estimates in reality the export error will be 
either smaller or larger than the estimated value based on whether 
the delay estimate was larger or smaller than the actual delay that 
dr experienced this difference between the estimated and the 
actual export error is the relative export error which could either 
be positive or negative which occurs for every dr vector that is 
sent and is accumulated at the sender 
the concept of relative export error is illustrated in figure 
since the actual delay to receiver is da the export error 
induced by dr at receiver is err dr t t δ da 
this means there is an error in the estimated export error and the 
sender can compute this error only after it gets a feedback from the 
receiver about the actual delay for the delivery of dr i e the 
value of da we propose that once receiver receives dr it 
sends the value of da back to the sender the receiver can 
compute this information as it knows the time at which dr was sent 
 t 
 t δ which is appended to the dr vector as shown in 
figure and the local receiving time which is synchronized with 
the sender s clock therefore the sender computes the relative 
export error for receiver represented using r as 
r err dr t t δ dt 
− err dr t t δ da 
 err dr t δ dt t δ da 
similarly the relative export error for receiver is computed as 
r err dr t t δ dt 
− err dr t t δ da 
 err dr t δ dt t δ da 
note that r as da dt and r as da dt 
relative export errors are computed by the sender as and when it 
receives the feedback from the receivers this example shows the 
 
relative export error values after dr is sent and the corresponding 
feedbacks are received 
 equalization of error among receivers 
we now explain what we mean by making the errors equal 
at all the receivers and how this can be achieved as stated 
before the sender keeps estimates of the delays to the receivers dt 
and dt in the example of figure this says that at time t 
when dr is computed the sender already knows how long it may 
take messages carrying this dr vector to reach the receivers the 
sender uses this information to compute the export errors which are 
err dr t t δ dt and err dr t t δ dt 
for receivers and respectively note that the areas of these error 
components are a function of δ and δ as well as the network 
delays dt and dt if we are to make the exports errors due to dr 
the same at both receivers the sender needs to choose δ and δ 
such that 
err dr t t δ dt err dr t t δ dt 
but when t was computed there could already have been 
accumulated relative export errors due to previous dr vectors dr and 
the ones before let us represent the accumulated relative error up 
to dri for receiver j as ri 
j to accommodate these accumulated 
relative errors the sender should now choose δ and δ such that 
r 
 err dr t t δ dt 
r 
 err dr t t δ dt 
the δi determines the scheduling instant of the dr vector at the 
sender for receiver i this method of computation of δ s ensures 
that the accumulated export error i e total actual error for each 
receiver equalizes at the transmission of each dr vector 
in order to establish this assume that the feedback for dr vector 
di from a receiver comes to the sender before schedule for di is 
computed let si 
m and ai 
m denote the estimated error for receiver 
m used for computing schedule for di and accumulated error for 
receiver m computed after receiving feedback for di respectively 
then ri 
m ai 
m −si 
m in order to compute the schedule instances 
 i e δ s for di for any pair of receivers m and n we do ri− 
m 
si 
m ri− 
n si 
n the following theorem establishes the fact 
that the accumulated export error is equalized at every scheduling 
instant 
theorem when the schedule instances for sending di 
are computed for any pair of receivers m and n the following 
condition is satisfied 
i− 
k 
ak 
m si 
m 
i− 
k 
ak 
n si 
n 
proof by induction assume that the premise holds for some i 
we show that it holds for i the base case for i holds since 
initially r 
m r 
n and the s 
m s 
n is used to compute the 
scheduling instances 
in order to compute the schedule for di the we first compute 
the relative errors as 
ri 
m ai 
m − si 
m and ri 
n ai 
n − si 
n 
then to compute δ s we execute 
ri 
m si 
m ri 
n si 
n 
ai 
m − si 
m si 
m ai 
n − si 
n si 
n 
adding the condition of the premise on both sides we get 
i 
k 
ak 
m si 
m 
i 
k 
ak 
n si 
n 
 computation of the export error 
let us now consider how the export errors can be computed 
from the previous section to find δ and δ we need to find 
err dr t t δ dt and err dr t t δ dt 
note that the values of r 
 and r 
 are already known at the sender 
consider the computation of err dr t t δ dt this is 
the integral of the distance between the trajectories ac due to dr 
and bd due to dr from dr and dr point a is x y 
 x y and point b is x y x t − t × vx y 
 t − t × vy the trajectory ac can be represented as a 
function of time as x t y t x vx × t y vy × t 
and the trajectory of bd can be represented as x t y t 
 x vx × t y vy × t 
the distance between the two trajectories as a function of time 
then becomes 
dist t x t − x t y t − y t 
 x − x vx − vx t 
 y − y vy − vy t 
 vx − vx vy − vy t 
 x − x vx − vx 
 y − y vy − vy t 
 x − x y − y 
let 
a vx − vx 
 vy − vy 
b x − x vx − vx 
 y − y vy − vy 
c x − x 
 y − y 
then dist t can be written as 
dist t a × t b × t c 
then err dr t t for some time interval t t becomes 
t 
t 
dist t dt 
t 
t 
a × t b × t c dt 
a closed form solution for the indefinite integral 
a × t b × t c dt 
 at b 
√ 
at bt c 
 a 
 
 
 
ln 
 
 b 
 at 
√ 
a 
 at bt c c 
 
√ 
a 
− 
 
 
ln 
 
 b 
 at 
√ 
a 
 at bt c b 
a− 
 
err dr t t δ dt and err dr t t δ dt 
can then be calculated by applying the appropriate limits to the 
above solution in the next section we consider the computation 
of the δ s for n receivers 
 
 computation of scheduling instants 
we again look at the computation of δ s by referring to figure 
the sender chooses δ and δ such that r 
 err dr t t 
δ dt r 
 err dr t t δ dt if r 
 and r 
 both 
are zero then δ and δ should be chosen such that err dr t t 
δ dt err dr t t δ dt this equality will hold 
if δ dt δ dt thus if there is no accumulated relative 
export error all that the sender needs to do is to choose the δ s in 
such a way that they counteract the difference in the delay to the 
two receivers so that they receive the dr vector at the same time 
as discussed earlier because the sender is not able to a priori learn 
the delay there will always be an accumulated relative export error 
from a previous dr vector that does have to be taken into account 
to delve deeper into this consider the computation of the 
export error as illustrated in the previous section to compute the 
δ s we require that r 
 err dr t t δ dt r 
 
err dr t t δ dt that is 
r 
 
t δ dt 
t 
dist t dt r 
 
t δ dt 
t 
dist t dt 
that is 
r 
 
t dt 
t 
dist t dt 
t dt δ 
t dt 
dist t dt 
r 
 
t dt 
t 
dist t dt 
t dt δ 
t dt 
dist t dt 
the components r 
 r 
 are already known to or estimated by 
the sender further the error components 
t dt 
t 
dist t dt and 
t dt 
t 
dist t dt can be a priori computed by the sender using 
estimated values of dt and dt let us use e to denote r 
 
t dt 
t 
dist t dt and e to denote r 
 
t dt 
t 
dist t dt 
then we require that 
e 
t dt δ 
t dt 
dist t dt e 
t dt δ 
t dt 
dist t dt 
assume that e e then for the above equation to hold we 
require that 
t dt δ 
t dt 
dist t dt 
t dt δ 
t dt 
dist t dt 
to make the game as fast as possible within this framework the δ 
values should be made as small as possible so that dr vectors are 
sent to the receivers as soon as possible subject to the fairness 
requirement given this we would choose δ to be zero and compute 
δ from the equation 
e e 
t dt δ 
t dt 
dist t dt 
in general if there are n receivers n when a sender 
generates a dr vector and decides to schedule them to be sent it first 
computes the ei values for all of them from the accumulated 
relative export errors and estimates of delays then it finds the smallest 
of these values let ek be the smallest value the sender makes δk 
to be zero and computes the rest of the δ s from the equality 
ei 
t dti δi 
t dti 
dist t dt ek 
∀i ≤ i ≤ n i k 
the δ s thus obtained gives the scheduling instants of the dr 
vector for the receivers 
 steps of the scheduling algorithm 
for the purpose of the discussion below as before let us denote 
the accumulated relative export error at a sender for receiver k up 
until dri to be ri 
k let us denote the scheduled delay at the sender 
before dri is sent to receiver k as δi 
k given the above discussion 
the algorithm steps are as follows 
 the sender computes dri at say time ti and then 
computes δi 
k and ri− 
k ∀k ≤ k ≤ n based on the estimation 
of delays dtk ∀k ≤ k ≤ n as per equation it 
schedules dri to be sent to receiver k at time ti δi 
k 
 the dr vectors are sent to the receivers at the scheduled 
times which are received after a delay of dak ∀k ≤ k ≤ 
n where dak ≤ or dtk the receivers send the value of 
dak back to the sender the receiver can compute this value 
based on the time stamps on the dr vector as described 
earlier 
 the sender computes ri 
k as described earlier and illustrated 
in figure the sender also recomputes using exponential 
averaging method similar to round-trip time estimation by 
tcp the estimate of delay dtk from the new value of 
dak for receiver k 
 go back to step to compute dri when it is required 
and follow the steps of the algorithm to schedule and send 
this dr vector to the receivers 
 handling cases in practice 
so far we implicity assumed that dri is sent out to all receivers 
before a decision is made to compute the next dr vector dri 
and the receivers send the value of dak corresponding to dri and 
this information reaches the sender before it computes dri so 
that it can compute ri 
k and then use it in the computation of δi 
k 
two issues need consideration with respect to the above algorithm 
when it is used in practice 
 it may so happen that a new dr vector is computed even 
before the previous dr vector is sent out to all receivers 
how will this situation be handled 
 what happens if the feedback does not arrive before dri 
is computed and scheduled to be sent 
let us consider the first scenario we assume that dri has been 
scheduled to be sent and the scheduling instants are such that δi 
 
δi 
 · · · δi 
n assume that dri is to be computed because 
the real path has deviated exceeding a threshold from the path 
exported by dri at time ti where ti δi 
k ti ti δi 
k 
this means dri has been sent only to receivers up to k in the 
scheduled order in our algorithm in this case the scheduled delay 
ordering queue is flushed which means dri is not sent to receivers 
still queued to receive it but a new scheduling order is computed 
for all the receivers to send dri 
for those receivers who have been sent dri assume for now 
that daj ≤ j ≤ k has been received from all receivers the 
scenario where daj has not been received will be considered as a part 
of the second scenario later for these receivers ei 
j ≤ j ≤ k 
can be computed for those receivers j k ≤ j ≤ n to 
whom dri was not sent ei 
j does not apply consider a receiver 
j k ≤ j ≤ n to whom dri was not sent refer to 
figure for such a receiver j when dri is to be scheduled and 
 
timeti 
exported path 
dtj 
a 
b 
c 
d 
ti- 
gi 
j 
dri computed by sender 
and dri for receiver k to 
n is removed from queue 
dri scheduled 
for receiver k 
ti 
g 
h 
e 
f 
dri scheduled 
for receiver j 
dri computed 
by sender 
placed path 
at receiver k 
gi 
j 
figure schedule computation when dri is not sent to 
receiver j k ≤ j ≤ n 
δi 
j needs to be computed the total export error is the accumulated 
relative export error at time ti when schedule for dri was 
computed plus the integral of the distance between the two trajectories 
ac and bd of figure over the time interval ti ti δi 
j 
dtj note that this integral is given by err dri ti ti 
err dri ti ti δi 
j dtj therefore instead of ei 
j 
of equation we use the value ri− 
j err dri ti ti 
err dri ti ti δi 
j dtj where ri− 
j is relative 
export error used when the schedule for dri was computed 
now consider the second scenario here the feedback dak 
corresponding to dri has not arrived before dri is computed and 
scheduled in this case ri 
k cannot be computed thus in the 
computation of δk for dri this will be assumed to be zero we 
do assume that a reliable mechanism is used to send dak back to 
the sender when this information arrives at a later time ri 
k will 
be computed and accumulated to future relative export errors for 
example ri 
k if dak is received before dri is computed and 
used in the computation of δk when a future dr vector is to be 
scheduled for example dri 
 experimental results 
in order to evaluate the effectiveness and quantify benefits 
obtained through the use of the scheduling algorithm we implemented 
the proposed algorithm in bzflag battle zone flag game 
it is a first-person shooter game where the players in teams drive 
tanks and move within a battle field the aim of the players is to 
navigate and capture flags belonging to the other team and bring 
them back to their own area the players shoot each other s tanks 
using shooting bullets the movement of the tanks as well as that 
of the shots are exchanged among the players using dr vectors 
we have modified the implementation of bzflag to 
incorporate synchronized clocks among the players and the server and 
exchange time-stamps with the dr vector we set up a testbed with 
four players running the instrumented version of bzflag with one 
as a sender and the rest as receivers the scheduling approach and 
the base case where each dr vector was sent to all the receivers 
concurrently at every trigger point were implemented in the same 
run by tagging the dr vectors according to the type of approach 
used to send the dr vector nistnet was used to introduce 
delays across the sender and the three receivers mean delays of 
 ms ms and ms were introduced between the sender and 
first second and the third receiver respectively we introduce a 
variance of msec to the mean delay of each receiver to model 
variability in delay the sender logged the errors of each receiver 
every milliseconds for both the scheduling approach and the 
base case the sender also calculated the standard deviation and 
the mean of the accumulated export error of all the receivers every 
 milliseconds figure plots the mean and standard deviation 
of the accumulated export error of all the receivers in the 
scheduling case against the base case note that the x-axis of these graphs 
 and the other graphs that follow represents the system time when 
the snapshot of the game was taken 
observe that the standard deviation of the error with scheduling 
is much lower as compared to the base case this implies that the 
accumulated errors of the receivers in the scheduling case are closer 
to one another this shows that the scheduling approach achieves 
fairness among the receivers even if they are at different distances 
 i e latencies from the sender 
observe that the mean of the accumulated error increased 
multifold with scheduling in comparison to the base case further 
exploration for the reason for the rise in the mean led to the conclusion 
that every time the dr vectors are scheduled in a way to equalize 
the total error it pushes each receivers total error higher also as 
the accumulated error has an estimated component the schedule is 
not accurate to equalize the errors for the receivers leading to the 
dr vector reaching earlier or later than the actual schedule in 
either case the error is not equalized and if the dr vector reaches 
late it actually increases the error for a receiver beyond the highest 
accumulated error this means that at the next trigger this receiver 
will be the one with highest error and every other receiver s error 
will be pushed to this error value this flip-flop effect leads to 
the increase in the accumulated error for all the receivers 
the scheduling for fairness leads to the decrease in standard 
deviation i e increases the fairness among different players but it 
comes at the cost of higher mean error which may not be a 
desirable feature this led us to explore different ways of equalizing the 
accumulated errors the approach discussed in the following 
section is a heuristic approach based on the following idea using the 
same amount of dr vectors over time as in the base case instead 
of sending the dr vectors to all the receivers at the same frequency 
as in the base case if we can increase the frequency of sending 
the dr vectors to the receiver with higher accumulated error and 
decrease the frequency of sending dr vectors to the receiver with 
lower accumulated error we can equalize the export error of all 
receivers over time at the same time we wish to decrease the 
error of the receiver with the highest accumulated error in the base 
case of course this receiver would be sent more dr vectors than 
in the base case we refer to such an algorithm as a budget based 
algorithm 
 budget based algorithm 
in a game the sender of an entity sends dr vectors to all the 
receivers every time a threshold is crossed by the entity lower 
the threshold more dr vectors are generated during a given time 
period since the dr vectors are sent to all the receivers and the 
network delay between the sender-receiver pairs cannot be avoided 
the before export error 
with the most distant player will always 
 
note that after export error is eliminated by using synchronized 
clock among the players 
 
 
 
 
 
 
 
 
meanaccumulatederror 
time in seconds 
base case 
scheduling algorithm 
 
 
 
 
 
 
 
 
 
 
 
 
standarddeviationofaccumulatederror 
time in seconds 
base case 
scheduling algorithm 
figure mean and standard deviation of error with scheduling and without i e base case 
be higher than the rest in order to mitigate the imbalance in the 
error we propose to send dr vectors selectively to different 
players based on the accumulated errors of these players the budget 
based algorithm is based on this idea and there are two variations 
of it one is a probabilistic budget based scheme and the other a 
deterministic budget base scheme 
 probabilistic budget based scheme 
the probabilistic budget based scheme has three main steps a 
lower the dead reckoning threshold but at the same time keep the 
total number of drs sent the same as the base case b at every 
trigger probabilistically pick a player to send the dr vector to 
and c send the dr vector to the chosen player these steps are 
described below 
the lowering of dr threshold is implemented as follows 
lowering the threshold is equivalent to increasing the number of trigger 
points where dr vectors are generated suppose the threshold is 
such that the number of triggers caused by it in the base case is t 
and at each trigger n dr vectors sent by the sender which results 
in a total of nt dr vectors our goal is to keep the total number of 
dr vectors sent by the sender fixed at nt but lower the number of 
dr vectors sent at each trigger i e do not send the dr vector to 
all the receivers let n and t be the number of dr vectors sent 
at each trigger and number of triggers respectively in the modified 
case we want to ensure n t nt since we want to increase the 
number of trigger points i e t t this would mean that n n 
that is not all receivers will be sent the dr vector at every trigger 
in the probabilistic budget based scheme at each trigger a 
probability is calculated for each receiver to be sent a dr vector and 
only one receiver is sent the dr n this probability is based 
on the relative weights of the receivers accumulated errors that 
is a receiver with a higher accumulated error will have a higher 
probability of being sent the dr vector consider that the 
accumulated error for three players are a a and a respectively 
then the probability of player receiving the dr vector would 
be a 
a a a 
 similarly for the other players once the player is 
picked the dr vector is sent to that player 
to compare the probabilistic budget based algorithm with the 
base case we needed to lower the threshold for the base case for 
fair comparison as the dead reckoning threshold in the base 
case was already very fine it was decided that instead of 
lowering the threshold the probabilistic budget based approach would 
be compared against a modified base case that would use the 
normal threshold as the budget based algorithm but the base case was 
modified such that every third trigger would be actually used to 
send out a dr vector to all the three receivers used in our 
experiments this was called as the base case as it resulted in 
number of dr vectors being sent as compared to the base case 
the budget per trigger for the probability based approach was 
calculated as one dr vector at each trigger as compared to three dr 
vectors at every third trigger in the base case thus the two cases 
lead to the same number of dr vectors being sent out over time 
in order to evaluate the effectiveness of the probabilistic budget 
based algorithm we instrumented the bzflag game to use this 
approach we used the same testbed consisting of one sender and 
three receivers with delays of ms ms and ms from the 
sender and with low delay variance ms and moderate delay 
variance ms the results are shown in figures and as 
mentioned earlier the x-axis of these graphs represents the system 
time when the snapshot of the game was taken observe from the 
figures that the standard deviation of the accumulated error among 
the receivers with the probabilistic budget based algorithm is less 
than the base case and the mean is a little higher than the 
base case this implies that the game is fairer as compared to the 
 base case at the cost of increasing the mean error by a small 
amount as compared to the base case 
the increase in mean error in the probabilistic case compared to 
the base case can be attributed to the fact that the even though 
the probabilistic approach on average sends the same number of 
dr vectors as the base case it sometimes sends dr vectors to 
a receiver less frequently and sometimes more frequently than the 
 base case due to its probabilistic nature when a receiver does 
not receive a dr vector for a long time the receiver s trajectory 
is more and more off of the sender s trajectory and hence the rate 
of buildup of the error at the receiver is higher at times when 
a receiver receives dr vectors more frequently it builds up error 
at a lower rate but there is no way of reversing the error that was 
built up when it did not receive a dr vector for a long time this 
leads the receivers to build up more error in the probabilistic case 
as compared to the base case where the receivers receive a dr 
vector almost periodically 
 
 
 
 
 
 
 
 
meanaccumulatederror 
time in seconds 
 base case 
deterministic algorithm 
probabilistic algorithm 
 
 
 
 
 
 
 
 
 
 
 
 
standarddeviationofaccumulatederror 
time in seconds 
 base case 
deterministic algorithm 
probabilistic algorithm 
figure mean and standard deviation of error for different algorithms including budget based algorithms for low delay variance 
 
 
 
 
 
 
 
meanaccumulatederror 
time in seconds 
 base case 
deterministic algorithm 
probabilistic algorithm 
 
 
 
 
 
 
 
 
standarddeviationofaccumulatederror 
time in seconds 
 base case 
deterministic algorithm 
probabilistic algorithm 
figure mean and standard deviation of error for different algorithms including budget based algorithms for moderate delay 
variance 
 deterministic budget based scheme 
to bound the increase in mean error we decided to modify the 
budget based algorithm to be deterministic the first two steps 
of the algorithm are the same as in the probabilistic algorithm the 
trigger points are increased to lower the threshold and accumulated 
errors are used to compute the probability that a receiver will 
receiver a dr vector once these steps are completed a deterministic 
schedule for the receiver is computed as follows 
 if there is any receiver s tagged to receive a dr vector at 
the current trigger the sender sends out the dr vector to the 
respective receiver s if at least one receiver was sent a dr 
vector the sender calculates the probabilities of each receiver 
receiving a dr vector as explained before and follows steps 
 to else it does not do anything 
 for each receiver the probability value is multiplied with the 
budget available at each trigger which is set to as explained 
below to give the frequency of sending the dr vector to each 
receiver 
 if any of the receiver s frequency after multiplying with the 
budget goes over the receiver s frequency is set as and 
the surplus amount is equally distributed to all the receivers 
by adding the amount to their existing frequencies this 
process is repeated until all the receivers have a frequency of 
less than or equal to this is due to the fact that at a trigger 
we cannot send more than one dr vector to the respective 
receiver that will be wastage of dr vectors by sending 
redundant information 
 frequency gives us the schedule at which the sender should 
send dr vectors to the respective receiver credit obtained 
previously explained in step if any is subtracted from the 
schedule observe that the resulting value of the schedule 
might not be an integer hence the value is rounded off by 
taking the ceiling of the schedule for example if the 
frequency is this implies that we would like to have a dr 
vector sent every triggers however we are constrained 
to send it at the th trigger giving us a credit of when we 
do send the dr vector next time we would be able to send it 
 
on the rd trigger because of the credit 
 the difference between the schedule and the ceiling of the 
schedule is the credit that the receiver has obtained which 
is remembered for the future and used at the next time as 
explained in step 
 for each of those receivers who were sent a dr vector at 
the current trigger the receivers are tagged to receive the 
next dr vector at the trigger that happens exactly schedule 
 the ceiling of the schedule number of times away from the 
current trigger observe that no other receiver s schedule is 
modified at this point as they all are running a schedule 
calculated at some previous point of time those schedules will 
be automatically modified at the trigger when they are 
scheduled to receive the next dr vector at the first trigger the 
sender sends the dr vector to all the receivers and uses a 
relative probability of n for each receiver and follows the 
steps to to calculate the next schedule for each receiver 
in the same way as mentioned for other triggers this 
algorithm ensures that every receiver has a guaranteed schedule 
of receiving dr vectors and hence there is no irregularity in 
sending the dr vector to any receiver as was observed in the 
budget based probabilistic algorithm 
we used the testbed described earlier three receivers with 
varying delays to evaluate the deterministic algorithm using the budget 
of dr vector per trigger so as to use the same number of dr 
vectors as in the base case results from our experiments are 
shown in figures and it can be observed that the standard 
deviation of error in the deterministic budget based algorithm is less 
than the base case and also has the same mean error as the 
base case this indicates that the deterministic algorithm is more 
fair than the base case and at the same time does not increase 
the mean error thereby leading to a better game quality compared 
to the probabilistic algorithm 
in general when comparing the deterministic approach to the 
probabilistic approach we found that the mean accumulated 
error was always less in the deterministic approach with respect to 
standard deviation of the accumulated error we found that in the 
fixed or low variance cases the deterministic approach was 
generally lower but in higher variance cases it was harder to draw 
conclusions as the probabilistic approach was sometimes better than 
the deterministic approach 
 conclusions and future work 
in distributed multi-player games played across the internet 
object and player trajectory within the game space are exchanged in 
terms of dr vectors due to the variable delay between players 
these dr vectors reach different players at different times there is 
unfair advantage gained by receivers who are closer to the sender 
of the dr as they are able to render the sender s position more 
accurately in real time in this paper we first developed a model 
for estimating the error in rendering player trajectories at the 
receivers we then presented an algorithm based on scheduling the 
dr vectors to be sent to different players at different times thereby 
equalizing the error at different players this algorithm is aimed 
at making the game fair to all players but tends to increase the 
mean error of the players to counter this effect we presented 
budget based algorithms where the dr vectors are still 
scheduled to be sent at different players at different times but the 
algorithm balances the need for fairness with the requirement that the 
error of the worst case players who are furthest from the sender 
are not increased compared to the base case where all dr vectors 
are sent to all players every time a dr vector is generated we 
presented two variations of the budget based algorithms and through 
experimentation showed that the algorithms reduce the standard 
deviation of the error thereby making the game more fair and at the 
same time has comparable mean error to the base case 
 references 
 s aggarwal h banavar a khandelwal s mukherjee and 
s rangarajan accuracy in dead-reckoning based 
distributed multi-player games proceedings of acm 
sigcomm workshop on network and system support 
for games netgames aug 
 l gautier and c diot design and evaluation of mimaze 
a multiplayer game on the internet in proc of ieee 
multimedia icmcs 
 m mauve consistency in replicated continuous 
interactive media in proc of the acm conference on 
computer supported cooperative work cscw 
pp - 
 s k singhal and d r cheriton exploiting position 
history for efficient remote rendering in networked 
virtual reality presence teleoperators and virtual 
environments vol no pp - 
 c diot and l gautier a distributed architecture for 
multiplayer interactive applications on the internet in 
ieee network magazine vol pp - 
 l pantel and l c wolf on the impact of delay on 
real-time multiplayer games in proc of acm 
nossdav may 
 y lin k guo and s paul sync-ms synchronized 
messaging service for real-time multi-player distributed 
games in proc of th ieee international conference on 
network protocols icnp nov 
 k guo s mukherjee s rangarajan and s paul a fair 
message exchange framework for distributed multi-player 
games in proc of netgames may 
 n e baughman and b n levine cheat-proof playout for 
centralized and distributed online games in proc of ieee 
infocom april 
 m allman and v paxson on estimating end-to-end 
network path properties in proc of acm sigcomm 
sept 
 bzflag forum bzflag game url 
http www bzflag org 
 nation institute of standards and technology nist net 
url http snad ncsl nist gov nistnet 
 
tsar a two tier sensor storage architecture using 
interval skip graphs 
∗ 
peter desnoyers deepak ganesan and prashant shenoy 
department of computer science 
university of massachusetts 
amherst ma 
pjd cs umass edu dganesan cs umass edu shenoy cs umass edu 
abstract 
archival storage of sensor data is necessary for applications that query 
mine and analyze such data for interesting features and trends we argue 
that existing storage systems are designed primarily for flat hierarchies of 
homogeneous sensor nodes and do not fully exploit the multi-tier nature 
of emerging sensor networks where an application can comprise tens of 
tethered proxies each managing tens to hundreds of untethered sensors 
we present tsar a fundamentally different storage architecture that 
envisions separation of data from metadata by employing local archiving at 
the sensors and distributed indexing at the proxies at the proxy tier tsar 
employs a novel multi-resolution ordered distributed index structure the 
interval skip graph for efficiently supporting spatio-temporal and value 
queries at the sensor tier tsar supports energy-aware adaptive 
summarization that can trade off the cost of transmitting metadata to the proxies 
against the overhead of false hits resulting from querying a coarse-grain 
index we implement tsar in a two-tier sensor testbed comprising 
stargatebased proxies and mote-based sensors our experiments demonstrate the 
benefits and feasibility of using our energy-efficient storage architecture in 
multi-tier sensor networks 
categories and subject descriptors c computer - 
communication networks distributed systems 
general terms algorithms performance experimentation 
 introduction 
 motivation 
many different kinds of networked data-centric sensor 
applications have emerged in recent years sensors in these applications 
sense the environment and generate data that must be processed 
filtered interpreted and archived in order to provide a useful 
infrastructure to its users to achieve its goals a typical sensor 
application needs access to both live and past sensor data whereas 
access to live data is necessary in monitoring and surveillance 
applications access to past data is necessary for applications such as 
mining of sensor logs to detect unusual patterns analysis of 
historical trends and post-mortem analysis of particular events archival 
storage of past sensor data requires a storage system the key 
attributes of which are where the data is stored whether it is indexed 
and how the application can access this data in an energy-efficient 
manner with low latency 
there have been a spectrum of approaches for constructing 
sensor storage systems in the simplest sensors stream data or events 
to a server for long-term archival storage where the server 
often indexes the data to permit efficient access at a later time since 
sensors may be several hops from the nearest base station network 
costs are incurred however once data is indexed and archived 
subsequent data accesses can be handled locally at the server without 
incurring network overhead in this approach the storage is 
centralized reads are efficient and cheap while writes are expensive 
further all data is propagated to the server regardless of whether 
it is ever used by the application 
an alternate approach is to have each sensor store data or events 
locally e g in flash memory so that all writes are local and incur 
no communication overheads a read request such as whether an 
event was detected by a particular sensor requires a message to 
be sent to the sensor for processing more complex read requests 
are handled by flooding for instance determining if an intruder 
was detected over a particular time interval requires the request to 
be flooded to all sensors in the system thus in this approach 
the storage is distributed writes are local and inexpensive while 
reads incur significant network overheads requests that require 
flooding due to the lack of an index are expensive and may waste 
precious sensor resources even if no matching data is stored at 
those sensors research efforts such as directed diffusion 
have attempted to reduce these read costs however by intelligent 
message routing 
between these two extremes lie a number of other sensor storage 
systems with different trade-offs summarized in table the 
geographic hash table ght approach advocates the use of 
an in-network index to augment the fully distributed nature of 
sensor storage in this approach each data item has a key associated 
with it and a distributed or geographic hash table is used to map 
keys to nodes that store the corresponding data items thus writes 
cause data items to be sent to the hashed nodes and also trigger 
updates to the in-network hash table a read request requires a lookup 
in the in-network hash table to locate the node that stores the data 
 
item observe that the presence of an index eliminates the need for 
flooding in this approach 
most of these approaches assume a flat homogeneous 
architecture in which every sensor node is energy-constrained in this 
paper we propose a novel storage architecture called tsar 
that 
reflects and exploits the multi-tier nature of emerging sensor 
networks where the application is comprised of tens of tethered 
sensor proxies or more each controlling tens or hundreds of 
untethered sensors tsar is a component of our presto predictive 
storage architecture which combines archival storage with caching 
and prediction we believe that a fundamentally different storage 
architecture is necessary to address the multi-tier nature of future 
sensor networks specifically the storage architecture needs to 
exploit the resource-rich nature of proxies while respecting resource 
constraints at the remote sensors no existing sensor storage 
architecture explicitly addresses this dichotomy in the resource 
capabilities of different tiers 
any sensor storage system should also carefully exploit current 
technology trends which indicate that the capacities of flash 
memories continue to rise as per moore s law while their costs continue 
to plummet thus it will soon be feasible to equip each sensor with 
 gb of flash storage for a few tens of dollars an even more 
compelling argument is the energy cost of flash storage which can be 
as much as two orders of magnitude lower than that for 
communication newer nand flash memories offer very low write and 
erase energy costs - our comparison of a gb samsung nand 
flash storage and the chipcon cc wireless radio 
 in section indicates a ratio in per-byte energy cost 
between the two devices even before accounting for network protocol 
overheads these trends together with the energy-constrained 
nature of untethered sensors indicate that local storage offers a viable 
energy-efficient alternative to communication in sensor networks 
tsar exploits these trends by storing data or events locally on 
the energy-efficient flash storage at each sensor sensors send 
concise identifying information which we term metadata to a nearby 
proxy depending on the representation used this metadata may be 
an order of magnitude or more smaller than the data itself 
imposing much lower communication costs the resource-rich proxies 
interact with one another to construct a distributed index of the 
metadata reported from all sensors and thus an index of the 
associated data stored at the sensors this index provides a unified 
logical view of the distributed data and enables an application to 
query and read past data efficiently - the index is used to 
pinpoint all data that match a read request followed by messages to 
retrieve that data from the corresponding sensors in-network 
index lookups are eliminated reducing network overheads for read 
requests this separation of data which is stored at the sensors 
and the metadata which is stored at the proxies enables tsar to 
reduce energy overheads at the sensors by leveraging resources at 
tethered proxies 
 contributions 
this paper presents tsar a novel two-tier storage architecture 
for sensor networks to the best of our knowledge this is the first 
sensor storage system that is explicitly tailored for emerging 
multitier sensor networks our design and implementation of tsar has 
resulted in four contributions 
at the core of the tsar architecture is a novel distributed index 
structure based on interval skip graphs that we introduce in this 
paper this index structure can store coarse summaries of sensor 
data and organize them in an ordered manner to be easily 
search 
tsar tiered storage architecture for sensor networks 
able this data structure has o log n expected search and update 
complexity further the index provides a logically unified view of 
all data in the system 
second at the sensor level each sensor maintains a local archive 
that stores data on flash memory our storage architecture is fully 
stateless at each sensor from the perspective of the metadata index 
all index structures are maintained at the resource-rich proxies and 
only direct requests or simple queries on explicitly identified 
storage locations are sent to the sensors storage at the remote sensor 
is in effect treated as appendage of the proxy resulting in low 
implementation complexity which makes it ideal for small 
resourceconstrained sensor platforms further the local store is optimized 
for time-series access to archived data as is typical in many 
applications each sensor periodically sends a summary of its data to a 
proxy tsar employs a novel adaptive summarization technique 
that adapts the granularity of the data reported in each summary to 
the ratio of false hits for application queries more fine grain 
summaries are sent whenever more false positives are observed thereby 
balancing the energy cost of metadata updates and false positives 
third we have implemented a prototype of tsar on a multi-tier 
testbed comprising stargate-based proxies and mote-based sensors 
our implementation supports spatio-temporal value and 
rangebased queries on sensor data 
fourth we conduct a detailed experimental evaluation of tsar 
using a combination of emstar emtos and our prototype 
while our emstar emtos experiments focus on the scalability of 
tsar in larger settings our prototype evaluation involves latency 
and energy measurements in a real setting our results demonstrate 
the logarithmic scaling property of the sparse skip graph and the 
low latency of end-to-end queries in a duty-cycled multi-hop 
network 
the remainder of this paper is structured as follows section 
presents key design issues that guide our work section and 
present the proxy-level index and the local archive and 
summarization at a sensor respectively section discusses our prototype 
implementation and section presents our experimental results we 
present related work in section and our conclusions in section 
 design considerations 
in this section we first describe the various components of a 
multi-tier sensor network assumed in our work we then present a 
description of the expected usage models for this system followed 
by several principles addressing these factors which guide the 
design of our storage system 
 system model 
we envision a multi-tier sensor network comprising multiple tiers 
- a bottom tier of untethered remote sensor nodes a middle tier of 
tethered sensor proxies and an upper tier of applications and user 
terminals see figure 
the lowest tier is assumed to form a dense deployment of 
lowpower sensors a canonical sensor node at this tier is equipped 
with low-power sensors a micro-controller and a radio as well as 
a significant amount of flash memory e g gb the common 
constraint for this tier is energy and the need for a long lifetime 
in spite of a finite energy constraint the use of radio processor 
ram and the flash memory all consume energy which needs to 
be limited in general we assume radio communication to be 
substantially more expensive than accesses to flash memory 
the middle tier consists of power-rich sensor proxies that have 
significant computation memory and storage resources and can use 
 
table characteristics of sensor storage systems 
system data index reads writes order preserving 
centralized store centralized centralized index handled at store send to store yes 
local sensor store fully distributed no index flooding diffusion local no 
ght dcs fully distributed in-network index hash to node send to hashed node no 
tsar presto fully distributed distributed index at proxies proxy lookup sensor query local plus index update yes 
user 
unified logical store 
queries 
 time space value 
query 
response 
cache 
query forwarding 
proxy 
remote 
sensors 
local data archive 
on flash memory 
interval 
skip graph 
query 
forwarding 
summaries 
start index 
end index 
linear 
traversal 
query 
response 
cache-miss 
triggered 
query forwarding 
summaries 
figure architecture of a multi-tier sensor network 
these resources continuously in urban environments the proxy tier 
would comprise a tethered base-station class nodes e g crossbow 
stargate each with with multiple radios-an radio that 
connects it to a wireless mesh network and a low-power radio e g 
 that connects it to the sensor nodes in remote sensing 
applications this tier could comprise a similar stargate node 
with a solar power cell each proxy is assumed to manage several 
tens to hundreds of lower-tier sensors in its vicinity a typical 
sensor network deployment will contain multiple geographically 
distributed proxies for instance in a building monitoring application 
one sensor proxy might be placed per floor or hallway to monitor 
temperature heat and light sensors in their vicinity 
at the highest tier of our infrastructure are applications that query 
the sensor network through a query interface in this work we 
focus on applications that require access to past sensor data to 
support such queries the system needs to archive data on a 
persistent store our goal is to design a storage system that exploits the 
relative abundance of resources at proxies to mask the scarcity of 
resources at the sensors 
 usage models 
the design of a storage system such as tsar is affected by the 
queries that are likely to be posed to it a large fraction of queries 
on sensor data can be expected to be spatio-temporal in nature 
sensors provide information about the physical world two key 
attributes of this information are when a particular event or activity 
occurred and where it occurred some instances of such queries 
include the time and location of target or intruder detections e g 
security and monitoring applications notifications of specific types 
of events such as pressure and humidity values exceeding a 
threshold e g industrial applications or simple data collection queries 
which request data from a particular time or location e g weather 
or environment monitoring 
expected queries of such data include those requesting ranges 
of one or more attributes for instance a query for all image data 
from cameras within a specified geographic area for a certain 
period of time in addition it is often desirable to support efficient 
access to data in a way that maintains spatial and temporal 
ordering there are several ways of supporting range queries such as 
locality-preserving hashes such as are used in dims 
however the most straightforward mechanism and one which naturally 
provides efficient ordered access is via the use of order-preserving 
data structures order-preserving structures such as the well-known 
b-tree maintain relationships between indexed values and thus 
allow natural access to ranges as well as predecessor and successor 
operations on their key values 
applications may also pose value-based queries that involve 
determining if a value v was observed at any sensor the query 
returns a list of sensors and the times at which they observed this 
value variants of value queries involve restricting the query to a 
geographical region or specifying a range v v rather than a 
single value v value queries can be handled by indexing on the 
values reported in the summaries specifically if a sensor reports 
a numerical value then the index is constructed on these values a 
search involves finding matching values that are either contained in 
the search range v v or match the search value v exactly 
hybrid value and spatio-temporal queries are also possible such 
queries specify a time interval a value range and a spatial region 
and request all records that match these attributes - find all 
instances where the temperature exceeded o 
f at location r 
during the month of august these queries require an index on both 
time and value 
in tsar our focus is on range queries on value or time with 
planned extensions to include spatial scoping 
 design principles 
our design of a sensor storage system for multi-tier networks is 
based on the following set of principles which address the issues 
arising from the system and usage models above 
 principle store locally access globally current 
technology allows local storage to be significantly more 
energyefficient than network communication while technology 
trends show no signs of erasing this gap in the near future 
for maximum network life a sensor storage system should 
leverage the flash memory on sensors to archive data locally 
substituting cheap memory operations for expensive radio 
transmission but without efficient mechanisms for retrieval 
the energy gains of local storage may be outweighed by 
communication costs incurred by the application in searching for 
data we believe that if the data storage system provides 
the abstraction of a single logical store to applications as 
 
does tsar then it will have additional flexibility to 
optimize communication and storage costs 
 principle distinguish data from metadata data must 
be identified so that it may be retrieved by the application 
without exhaustive search to do this we associate 
metadata with each data record - data fields of known syntax 
which serve as identifiers and may be queried by the storage 
system examples of this metadata are data attributes such as 
location and time or selected or summarized data values we 
leverage the presence of resource-rich proxies to index 
metadata for resource-constrained sensors the proxies share this 
metadata index to provide a unified logical view of all data in 
the system thereby enabling efficient low-latency lookups 
such a tier-specific separation of data storage from metadata 
indexing enables the system to exploit the idiosyncrasies of 
multi-tier networks while improving performance and 
functionality 
 principle provide data-centric query support in a sensor 
application the specific location i e offset of a record in a 
stream is unlikely to be of significance except if it conveys 
information concerning the location and or time at which the 
information was generated we thus expect that applications 
will be best served by a query interface which allows them 
to locate data by value or attribute e g location and time 
rather than a read interface for unstructured data this in turn 
implies the need to maintain metadata in the form of an index 
that provides low cost lookups 
 system design 
tsar embodies these design principles by employing local 
storage at sensors and a distributed index at the proxies the key 
features of the system design are as follows 
in tsar writes occur at sensor nodes and are assumed to 
consist of both opaque data as well as application-specific metadata 
this metadata is a tuple of known types which may be used by the 
application to locate and identify data records and which may be 
searched on and compared by tsar in the course of locating data 
for the application in a camera-based sensing application for 
instance this metadata might include coordinates describing the field 
of view average luminance and motion values in addition to basic 
information such as time and sensor location depending on the 
application this metadata may be two or three orders of magnitude 
smaller than the data itself for instance if the metadata consists of 
features extracted from image or acoustic data 
in addition to storing data locally each sensor periodically sends 
a summary of reported metadata to a nearby proxy the summary 
contains information such as the sensor id the interval t t 
over which the summary was generated a handle identifying the 
corresponding data record e g its location in flash memory 
and a coarse-grain representation of the metadata associated with 
the record the precise data representation used in the summary 
is application-specific for instance a temperature sensor might 
choose to report the maximum and minimum temperature values 
observed in an interval as a coarse-grain representation of the 
actual time series 
the proxy uses the summary to construct an index the index 
is global in that it stores information from all sensors in the 
system and it is distributed across the various proxies in the system 
thus applications see a unified view of distributed data and can 
query the index at any proxy to get access to data stored at any 
sensor specifically each query triggers lookups in this distributed 
index and the list of matches is then used to retrieve the 
corresponding data from the sensors there are several distributed index and 
lookup methods which might be used in this system however the 
index structure described in section is highly suited for the task 
since the index is constructed using a coarse-grain summary 
instead of the actual data index lookups will yield approximate 
matches the tsar summarization mechanism guarantees that 
index lookups will never yield false negatives - i e it will never miss 
summaries which include the value being searched for however 
index lookups may yield false positives where a summary matches 
the query but when queried the remote sensor finds no matching 
value wasting network resources the more coarse-grained the 
summary the lower the update overhead and the greater the 
fraction of false positives while finer summaries incur update overhead 
while reducing query overhead due to false positives remote 
sensors may easily distinguish false positives from queries which result 
in search hits and calculate the ratio between the two based on this 
ratio tsar employs a novel adaptive technique that dynamically 
varies the granularity of sensor summaries to balance the metadata 
overhead and the overhead of false positives 
 data structures 
at the proxy tier tsar employs a novel index structure called 
the interval skip graph which is an ordered distributed data 
structure for finding all intervals that contain a particular point or range 
of values interval skip graphs combine interval trees an 
interval-based binary search tree with skip graphs a ordered 
distributed data structure for peer-to-peer systems the 
resulting data structure has two properties that make it ideal for 
sensor networks first it has o log n search complexity for 
accessing the first interval that matches a particular value or range and 
constant complexity for accessing each successive interval 
second indexing of intervals rather than individual values makes the 
data structure ideal for indexing summaries over time or value 
such summary-based indexing is a more natural fit for 
energyconstrained sensor nodes since transmitting summaries incurs less 
energy overhead than transmitting all sensor data 
definitions we assume that there are np proxies and ns 
sensors in a two-tier sensor network each proxy is responsible for 
multiple sensor nodes and no assumption is made about the 
number of sensors per proxy each sensor transmits interval summaries 
of data or events regularly to one or more proxies that it is 
associated with where interval i is represented as lowi highi these 
intervals can correspond to time or value ranges that are used for 
indexing sensor data no assumption is made about the size of an 
interval or about the amount of overlap between intervals 
range queries on the intervals are posed by users to the network 
of proxies and sensors each query q needs to determine all index 
values that overlap the interval lowq highq the goal of the 
interval skip graph is to index all intervals such that the set that overlaps 
a query interval can be located efficiently in the rest of this section 
we describe the interval skip graph in greater detail 
 skip graph overview 
in order to inform the description of the interval skip graph we 
first provide a brief overview of the skip graph data structure for 
a more extensive description the reader is referred to figure 
shows a skip graph which indexes keys the keys may be seen 
along the bottom and above each key are the pointers associated 
with that key each data element consisting of a key and its 
associated pointers may reside on a different node in the network 
 
 
level 
level 
level 
key 
single skip graph element 
 each may be on different node 
find 
node-to-node messages 
figure skip graph of elements 
 
 
 low high 
max 
contains 
match 
no 
match 
halt 
figure interval skip graph 
 
 
 
 
 
 
 
 
node 
node 
node 
level 
level 
level 
figure distributed interval skip graph 
and pointers therefore identify both a remote node as well as a data 
element on that node in this figure we may see the following 
properties of a skip graph 
 ordered index the keys are members of an ordered data 
type for instance integers lookups make use of ordered 
comparisons between the search key and existing index 
entries in addition the pointers at the lowest level point 
directly to the successor of each item in the index 
 in-place indexing data elements remain on the nodes 
where they were inserted and messages are sent between 
nodes to establish links between those elements and others 
in the index 
 log n height there are log n pointers associated with each 
element where n is the number of data elements indexed 
each pointer belongs to a level l in log n − and 
together with some other pointers at that level forms a chain 
of n l 
elements 
 probabilistic balance rather than relying on re-balancing 
operations which may be triggered at insert or delete skip 
graphs implement a simple random balancing mechanism 
which maintains close to perfect balance on average with 
an extremely low probability of significant imbalance 
 redundancy and resiliency each data element forms an 
independent search tree root so searches may begin at any 
node in the network eliminating hot spots at a single search 
root in addition the index is resilient against node failure 
data on the failed node will not be accessible but remaining 
data elements will be accessible through search trees rooted 
on other nodes 
in figure we see the process of searching for a particular value 
in a skip graph the pointers reachable from a single data element 
form a binary tree a pointer traversal at the highest level skips over 
n elements n at the next level and so on search consists 
of descending the tree from the highest level to level at each 
level comparing the target key with the next element at that level 
and deciding whether or not to traverse in the perfectly balanced 
case shown here there are log n levels of pointers and search will 
traverse or pointers at each level we assume that each data 
element resides on a different node and measure search cost by the 
number messages sent i e the number of pointers traversed this 
will clearly be o log n 
tree update proceeds from the bottom as in a b-tree with the 
root s being promoted in level as the tree grows in this way for 
instance the two chains at level always contain n entries each 
and there is never a need to split chains as the structure grows the 
update process then consists of choosing which of the l 
chains to 
insert an element into at each level l and inserting it in the proper 
place in each chain 
maintaining a perfectly balanced skip graph as shown in 
figure would be quite complex instead the probabilistic balancing 
method introduced in skip lists is used which trades off a 
small amount of overhead in the expected case in return for simple 
update and deletion the basis for this method is the observation 
that any element which belongs to a particular chain at level l can 
only belong to one of two chains at level l to insert an element 
we ascend levels starting at randomly choosing one of the two 
possible chains at each level an stopping when we reach an empty 
chain 
one means of implementation e g as described in is to 
assign each element an arbitrarily long random bit string each 
chain at level l is then constructed from those elements whose bit 
strings match in the first l bits thus creating l 
possible chains 
at each level and ensuring that each chain splits into exactly two 
chains at the next level although the resulting structure is not 
perfectly balanced following the analysis in we can show that 
the probability of it being significantly out of balance is extremely 
small in addition since the structure is determined by the random 
number stream input data patterns cannot cause the tree to become 
imbalanced 
 interval skip graph 
a skip graph is designed to store single-valued entries in this 
section we introduce a novel data structure that extends skip graphs 
to store intervals lowi highi and allows efficient searches for all 
intervals covering a value v i e {i lowi ≤ v ≤ highi} our data 
structure can be extended to range searches in a straightforward 
manner 
the interval skip graph is constructed by applying the method of 
augmented search trees as described by cormen leiserson and 
rivest and applied to binary search trees to create an interval 
tree the method is based on the observation that a search structure 
based on comparison of ordered keys such as a binary tree may 
also be used to search on a secondary key which is non-decreasing 
in the first key 
given a set of intervals sorted by lower bound - lowi ≤ 
lowi - we define the secondary key as the cumulative maximum 
maxi maxk i highk the set of intervals intersecting a 
value v may then be found by searching for the first interval and 
thus the interval with least lowi such that maxi ≥ v we then 
 
traverse intervals in increasing order lower bound until we find the 
first interval with lowi v selecting those intervals which 
intersect v 
using this approach we augment the skip graph data structure as 
shown in figure so that each entry stores a range lower bound 
and upper bound and a secondary key cumulative maximum of 
upper bound to efficiently calculate the secondary key maxi for 
an entry i we take the greatest of highi and the maximum values 
reported by each of i s left-hand neighbors 
to search for those intervals containing the value v we first 
search for v on the secondary index maxi and locate the first entry 
with maxi ≥ v by the definition of maxi for this data element 
maxi highi if lowi v then this interval does not contain 
v and no other intervals will either so we are done otherwise we 
traverse the index in increasing order of mini returning matching 
intervals until we reach an entry with mini v and we are done 
searches for all intervals which overlap a query range or which 
completely contain a query range are straightforward extensions 
of this mechanism 
lookup complexity lookup for the first interval that matches 
a given value is performed in a manner very similar to an interval 
tree the complexity of search is o log n the number of 
intervals that match a range query can vary depending on the amount of 
overlap in the intervals being indexed as well as the range specified 
in the query 
insert complexity in an interval tree or interval skip list the 
maximum value for an entry need only be calculated over the 
subtree rooted at that entry as this value will be examined only when 
searching within the subtree rooted at that entry for a simple 
interval skip graph however this maximum value for an entry must be 
computed over all entries preceding it in the index as searches may 
begin anywhere in the data structure rather than at a distinguished 
root element it may be easily seen that in the worse case the 
insertion of a single interval one that covers all existing intervals in 
the index will trigger the update of all entries in the index for a 
worst-case insertion cost of o n 
 sparse interval skip graph 
the final extensions we propose take advantage of the 
difference between the number of items indexed in a skip graph and the 
number of systems on which these items are distributed the cost 
in network messages of an operation may be reduced by 
arranging the data structure so that most structure traversals occur locally 
on a single node and thus incur zero network cost in addition 
since both congestion and failure occur on a per-node basis we 
may eliminate links without adverse consequences if those links 
only contribute to load distribution and or resiliency within a 
single node these two modifications allow us to achieve reductions 
in asymptotic complexity of both update and search 
as may be in section insert and delete cost on an 
interval skip graph has a worst case complexity of o n compared to 
o log n for an interval tree the main reason for the difference 
is that skip graphs have a full search structure rooted at each 
element in order to distribute load and provide resilience to system 
failures in a distributed setting however in order to provide load 
distribution and failure resilience it is only necessary to provide a 
full search structure for each system if as in tsar the number 
of nodes proxies is much smaller than the number of data 
elements data summaries indexed then this will result in significant 
savings 
implementation to construct a sparse interval skip graph we 
ensure that there is a single distinguished element on each system 
the root element for that system all searches will start at one of 
these root elements when adding a new element rather than 
splitting lists at increasing levels l until the element is in a list with no 
others we stop when we find that the element would be in a list 
containing no root elements thus ensuring that the element is reachable 
from all root elements an example of applying this optimization 
may be seen in figure in practice rather than designating 
existing data elements as roots as shown it may be preferable to insert 
null values at startup 
when using the technique of membership vectors as in this 
may be done by broadcasting the membership vectors of each root 
element to all other systems and stopping insertion of an element 
at level l when it does not share an l-bit prefix with any of the np 
root elements the expected number of roots sharing a log np-bit 
prefix is giving an expected expected height for each element of 
log np o an alternate implementation which distributes 
information concerning root elements at pointer establishment time 
is omitted due to space constraints this method eliminates the need 
for additional messages 
performance in a non-interval sparse skip graph since the 
expected height of an inserted element is now log np o 
expected insertion complexity is o log np rather than o log n 
where np is the number of root elements and thus the number of 
separate systems in the network in the degenerate case of a 
single system we have a skip list with splitting probability the 
expected height of an individual element is note that since 
searches are started at root elements of expected height log n 
search complexity is not improved 
for an interval sparse skip graph update performance is 
improved considerably compared to the o n worst case for the 
nonsparse case in an augmented search structure such as this an 
element only stores information for nodes which may be reached from 
that element-e g the subtree rooted at that element in the case of 
a tree thus when updating the maximum value in an interval tree 
the update is only propagated towards the root in a sparse interval 
skip graph updates to a node only propagate towards the np root 
elements for a worst-case cost of np log n 
shortcut search when beginning a search for a value v rather 
than beginning at the root on that proxy we can find the element 
that is closest to v e g using a secondary local index and then 
begin the search at that element the expected distance between 
this element and the search terminus is log np and the search 
will now take on average log np o steps to illustrate this 
optimization in figure depending on the choice of search root a 
search for beginning at node may take network hops 
traversing to node then back to node and finally to node 
where the destination is located for a cost of messages the 
shortcut search however locates the intermediate data element on 
node and then proceeds directly to node for a cost of message 
performance this technique may be applied to the primary key 
search which is the first of two insertion steps in an interval skip 
graph by combining the short-cut optimization with sparse 
interval skip graphs the expected cost of insertion is now o log np 
independent of the size of the index or the degree of overlap of the 
inserted intervals 
 alternative data structures 
thus far we have only compared the sparse interval skip graph 
with similar structures from which it is derived a comparison with 
several other data structures which meet at least some of the 
requirements for the tsar index is shown in table 
 
table comparison of distributed index structures 
range query support interval representation re-balancing resilience small networks large networks 
dht ght no no no yes good good 
local index flood query yes yes no yes good bad 
p-tree rp distributed b-trees yes possible yes no good good 
dims yes no yes yes yes yes 
interval skipgraph yes yes no yes good good 
 
roots node 
node 
figure sparse interval skip graph 
the hash-based systems dht and ght lack the 
ability to perform range queries and are thus not well-suited to indexing 
spatio-temporal data indexing locally using an appropriate 
singlenode structure and then flooding queries to all proxies is a 
competitive alternative for small networks for large networks the linear 
dependence on the number of proxies becomes an issue two 
distributed b-trees were examined - p-trees and rp each 
of these supports range queries and in theory could be modified 
to support indexing of intervals however they both require 
complex re-balancing and do not provide the resilience characteristics 
of the other structures dims provides the ability to perform 
spatio-temporal range queries and has the necessary resilience to 
failures however it cannot be used index intervals which are used 
by tsar s data summarization algorithm 
 data storage and summarization 
having described the proxy-level index structure we turn to the 
mechanisms at the sensor tier tsar implements two key 
mechanisms at the sensor tier the first is a local archival store at each 
sensor node that is optimized for resource-constrained devices the 
second is an adaptive summarization technique that enables each 
sensor to adapt to changing data and query characteristics the rest 
of this section describes these mechanisms in detail 
 local storage at sensors 
interval skip graphs provide an efficient mechanism to lookup 
sensor nodes containing data relevant to a query these queries are 
then routed to the sensors which locate the relevant data records 
in the local archive and respond back to the proxy to enable such 
lookups each sensor node in tsar maintains an archival store of 
sensor data while the implementation of such an archival store 
is straightforward on resource-rich devices that can run a database 
sensors are often power and resource-constrained consequently 
the sensor archiving subsystem in tsar is explicitly designed to 
exploit characteristics of sensor data in a resource-constrained 
setting 
timestamp 
calibration 
parameters 
opaque datadata event attributes size 
figure single storage record 
sensor data has very distinct characteristics that inform our 
design of the tsar archival store sensors produce time-series data 
streams and therefore temporal ordering of data is a natural and 
simple way of storing archived sensor data in addition to 
simplicity a temporally ordered store is often suitable for many sensor data 
processing tasks since they involve time-series data processing 
examples include signal processing operations such as fft wavelet 
transforms clustering similarity matching and target detection 
consequently the local archival store is a collection of records 
designed as an append-only circular buffer where new records are 
appended to the tail of the buffer the format of each data record is 
shown in figure each record has a metadata field which includes 
a timestamp sensor settings calibration parameters etc raw 
sensor data is stored in the data field of the record the data field 
is opaque and application-specific-the storage system does not 
know or care about interpreting this field a camera-based sensor 
for instance may store binary images in this data field in order 
to support a variety of applications tsar supports variable-length 
data fields as a result record sizes can vary from one record to 
another 
our archival store supports three operations on records create 
read and delete due to the append-only nature of the store 
creation of records is simple and efficient the create operation simply 
creates a new record and appends it to the tail of the store since 
records are always written at the tail the store need not maintain 
a free space list all fields of the record need to be specified at 
creation time thus the size of the record is known a priori and the 
store simply allocates the the corresponding number of bytes at the 
tail to store the record since writes are immutable the size of a 
record does not change once it is created 
proxy 
proxy 
proxy 
record 
 record 
summary 
local archive in 
flash memory 
data summary 
start end offset 
time interval 
sensor 
summary 
sent to proxy 
insert summaries 
into interval skip graph 
figure sensor summarization 
 
the read operation enables stored records to be retrieved in 
order to answer queries in a traditional database system efficient 
lookups are enabled by maintaining a structure such as a b-tree that 
indexes certain keys of the records however this can be quite 
complex for a small sensor node with limited resources consequently 
tsar sensors do not maintain any index for the data stored in their 
archive instead they rely on the proxies to maintain this metadata 
index-sensors periodically send the proxy information 
summarizing the data contained in a contiguous sequence of records as well 
as a handle indicating the location of these records in flash memory 
the mechanism works as follows in addition to the summary 
of sensor data each node sends metadata to the proxy containing 
the time interval corresponding to the summary as well as the start 
and end offsets of the flash memory location where the raw data 
corresponding is stored as shown in figure thus random 
access is enabled at granularity of a summary-the start offset of each 
chunk of records represented by a summary is known to the proxy 
within this collection records are accessed sequentially when a 
query matches a summary in the index the sensor uses these offsets 
to access the relevant records on its local flash by sequentially 
reading data from the start address until the end address any 
queryspecific operation can then be performed on this data thus no 
index needs to be maintained at the sensor in line with our goal 
of simplifying sensor state management the state of the archive 
is captured in the metadata associated with the summaries and is 
stored and maintained at the proxy 
while we anticipate local storage capacity to be large eventually 
there might be a need to overwrite older data especially in high 
data rate applications this may be done via techniques such as 
multi-resolution storage of data or just simply by overwriting 
older data when older data is overwritten a delete operation is 
performed where an index entry is deleted from the interval skip 
graph at the proxy and the corresponding storage space in flash 
memory at the sensor is freed 
 adaptive summarization 
the data summaries serve as glue between the storage at the 
remote sensor and the index at the proxy each update from a sensor 
to the proxy includes three pieces of information the summary a 
time period corresponding to the summary and the start and end 
offsets for the flash archive in general the proxy can index the 
time interval representing a summary or the value range reported 
in the summary or both the former index enables quick lookups 
on all records seen during a certain interval while the latter index 
enables quick lookups on all records matching a certain value 
as described in section there is a trade-off between the 
energy used in sending summaries and thus the frequency and 
resolution of those summaries and the cost of false hits during queries 
the coarser and less frequent the summary information the less 
energy required while false query hits in turn waste energy on 
requests for non-existent data 
tsar employs an adaptive summarization technique that 
balances the cost of sending updates against the cost of false positives 
the key intuition is that each sensor can independently identify the 
fraction of false hits and true hits for queries that access its local 
archive if most queries result in true hits then the sensor 
determines that the summary can be coarsened further to reduce update 
costs without adversely impacting the hit ratio if many queries 
result in false hits then the sensor makes the granularity of each 
summary finer to reduce the number and overhead of false hits 
the resolution of the summary depends on two 
parametersthe interval over which summaries of the data are constructed and 
transmitted to the proxy as well as the size of the 
applicationspecific summary our focus in this paper is on the interval over 
which the summary is constructed changing the size of the data 
summary can be performed in an application-specific manner e g 
using wavelet compression techniques as in and is beyond the 
scope of this paper currently tsar employs a simple 
summarization scheme that computes the ratio of false and true hits and 
decreases increases the interval between summaries whenever this 
ratio increases decreases beyond a threshold 
 tsar implementation 
we have implemented a prototype of tsar on a multi-tier 
sensor network testbed our prototype employs crossbow stargate 
nodes to implement the proxy tier each stargate node employs a 
 mhz intel xscale processor with mb ram and runs the 
linux kernel and emstar release the proxy nodes 
are equipped with two wireless radios a cisco aironet -based 
 b radio and a hostmote bridge to the mica sensor nodes 
using the emstar transceiver the b wireless network is 
used for inter-proxy communication within the proxy tier while 
the wireless bridge enables sensor-proxy communication the 
sensor tier consists of crossbow mica s and mica dots each 
consisting of a mhz cc radio a bmac protocol stack a mb 
on-board flash memory and an atmega l processor the 
sensor nodes run tinyos in addition to the on-board flash the 
sensor nodes can be equipped with external mmc sd flash cards 
using a custom connector the proxy nodes can be equipped with 
external storage such as high-capacity compact flash up to gb 
 gb micro-drives or up to gb inch mobile disk drives 
since sensor nodes may be several hops away from the nearest 
proxy the sensor tier employs multi-hop routing to communicate 
with the proxy tier in addition to reduce the power consumption 
of the radio while still making the sensor node available for queries 
low power listening is enabled in which the radio receiver is 
periodically powered up for a short interval to sense the channel for 
transmissions and the packet preamble is extended to account for 
the latency until the next interval when the receiving radio wakes 
up our prototype employs the multihoplepsm routing protocol 
with the bmac layer configured in the low-power mode with a 
 duty cycle one of the default bmac parameters 
our tsar implementation on the mote involves a data 
gathering task that periodically obtains sensor readings and logs these 
reading to flash memory the flash memory is assumed to be a 
circular append-only store and the format of the logged data is 
depicted in figure the mote sends a report to the proxy every n 
readings summarizing the observed data the report contains i 
the address of the mote ii a handle that contains an offset and the 
length of the region in flash memory containing data referred to by 
the summary iii an interval t t over which this report is 
generated iv a tuple low high representing the minimum and the 
maximum values observed at the sensor in the interval and v a 
sequence number the sensor updates are used to construct a sparse 
interval skip graph that is distributed across proxies via network 
messages between proxies over the b wireless network 
our current implementation supports queries that request records 
matching a time interval t t or a value range v v spatial 
constraints are specified using sensor ids given a list of matching 
intervals from the skip graph tsar supports two types of 
messages to query the sensor lookup and fetch a lookup message 
triggers a search within the corresponding region in flash memory 
and returns the number of matching records in that memory region 
 but does not retrieve data in contrast a fetch message not only 
 
 
 
 
 
 
 
 
 
 
 
numberofmessages 
index size entries 
insert skipgraph 
insert sparse skipgraph 
initial lookup 
 a james reserve data 
 
 
 
 
 
 
 
 
 
 
numberofmessages 
index size entries 
insert skipgraph 
insert sparse skipgraph 
initial lookup 
 b synthetic data 
figure skip graph insert performance 
triggers a search but also returns all matching data records to the 
proxy lookup messages are useful for polling a sensor for 
instance to determine if a query matches too many records 
 experimental evaluation 
in this section we evaluate the efficacy of tsar using our 
prototype and simulations the testbed for our experiments consists 
of four stargate proxies and twelve mica and mica dot sensors 
three sensors each are assigned to each proxy given the limited 
size of our testbed we employ simulations to evaluate the 
behavior of tsar in larger settings our simulation employs the emtos 
emulator which enables us to run the same code in simulation 
and the hardware platform 
rather than using live data from a real sensor to ensure 
repeatable experiments we seed each sensor node with a dataset 
 i e a trace that dictates the values reported by that node to the 
proxy one section of the flash memory on each sensor node is 
programmed with data points from the trace these observations 
are then replayed during an experiment logged to the local archive 
 located in flash memory as well and reported to the proxy the 
first dataset used to evaluate tsar is a temperature dataset from 
james reserve that includes data from eleven temperature 
sensor nodes over a period of days the second dataset is 
synthetically generated the trace for each sensor is generated using a 
uniformly distributed random walk though the value space 
our experimental evaluation has four parts first we run 
emtos simulations to evaluate the lookup update and delete overhead 
for sparse interval skip graphs using the real and synthetic datasets 
second we provide summary results from micro-benchmarks of 
the storage component of tsar which include empirical 
characterization of the energy costs and latency of reads and writes for the 
flash memory chip as well as the whole mote platform and 
comparisons to published numbers for other storage and 
communication technologies these micro-benchmarks form the basis for our 
full-scale evaluation of tsar on a testbed of four stargate proxies 
and twelve motes we measure the end-to-end query latency in our 
multi-hop testbed as well as the query processing overhead at the 
mote tier finally we demonstrate the adaptive summarization 
capability at each sensor node the remainder of this section presents 
our experimental results 
 sparse interval skip graph performance 
this section evaluates the performance of sparse interval skip 
graphs by quantifying insert lookup and delete overheads 
we assume a proxy tier with proxies and construct sparse 
interval skip graphs of various sizes using our datasets for each skip 
 
 
 
 
 
 
 
 
 
numberofmessages 
index size entries 
initial lookup 
traversal 
 a james reserve data 
 
 
 
 
 
 
 
 
 
numberofmessages 
index size entries 
initial lookup 
traversal 
 b synthetic data 
figure skip graph lookup performance 
 
 
 
 
 
 
 
 
 
numberofmessages 
number of proxies 
skipgraph insert 
sparse skipgraph insert 
initial lookup 
 a impact of number of 
proxies 
 
 
 
 
 
 
 
 
numberofmessages 
index size entries 
insert redundant 
insert non-redundant 
lookup redundant 
lookup non-redundant 
 b impact of redundant 
summaries 
figure skip graph overheads 
graph we evaluate the cost of inserting a new value into the index 
each entry was deleted after its insertion enabling us to quantify 
the delete overhead as well figure a and b quantify the insert 
overhead for our two datasets each insert entails an initial traversal 
that incurs log n messages followed by neighbor pointer update at 
increasing levels incurring a cost of log n messages our results 
demonstrate this behavior and show as well that performance of 
delete-which also involves an initial traversal followed by pointer 
updates at each level-incurs a similar cost 
next we evaluate the lookup performance of the index 
structure again we construct skip graphs of various sizes using our 
datasets and evaluate the cost of a lookup on the index structure 
figures a and b depict our results there are two components 
for each lookup-the lookup of the first interval that matches the 
query and in the case of overlapping intervals the subsequent 
linear traversal to identify all matching intervals the initial lookup 
can be seen to takes log n messages as expected the costs of 
the subsequent linear traversal however are highly data dependent 
for instance temperature values for the james reserve data exhibit 
significant spatial correlations resulting in significant overlap 
between different intervals and variable high traversal cost see 
figure a the synthetic data however has less overlap and incurs 
lower traversal overhead as shown in figure b 
since the previous experiments assumed proxies we evaluate 
the impact of the number of proxies on skip graph performance we 
vary the number of proxies from to and distribute a skip graph 
with entries among these proxies we construct regular 
interval skip graphs as well as sparse interval skip graphs using these 
entries and measure the overhead of inserts and lookups thus the 
experiment also seeks to demonstrate the benefits of sparse skip 
graphs over regular skip graphs figure a depicts our results 
in regular skip graphs the complexity of insert is o log n in the 
 
expected case and o n in the worst case where n is the number 
of elements this complexity is unaffected by changing the 
number of proxies as indicated by the flat line in the figure sparse 
skip graphs require fewer pointer updates however their overhead 
is dependent on the number of proxies and is o log np in the 
expected case independent of n this can be seen to result in 
significant reduction in overhead when the number of proxies is small 
which decreases as the number of proxies increases 
failure handling is an important issue in a multi-tier sensor 
architecture since it relies on many components-proxies sensor nodes 
and routing nodes can fail and wireless links can fade handling 
of many of these failure modes is outside the scope of this 
paper however we consider the case of resilience of skip graphs 
to proxy failures in this case skip graph search and subsequent 
repair operations can follow any one of the other links from a 
root element since a sparse skip graph has search trees rooted 
at each node searching can then resume once the lookup request 
has routed around the failure together these two properties 
ensure that even if a proxy fails the remaining entries in the skip 
graph will be reachable with high probability-only the entries on 
the failed proxy and the corresponding data at the sensors becomes 
inaccessible 
to ensure that all data on sensors remains accessible even in the 
event of failure of a proxy holding index entries for that data we 
incorporate redundant index entries tsar employs a simple 
redundancy scheme where additional coarse-grain summaries are used 
to protect regular summaries each sensor sends summary data 
periodically to its local proxy but less frequently sends a 
lowerresolution summary to a backup proxy-the backup summary 
represents all of the data represented by the finer-grained summaries 
but in a lossier fashion thus resulting in higher read overhead due 
to false hits if the backup summary is used the cost of 
implementing this in our system is low - figure b shows the overhead of 
such a redundancy scheme where a single coarse summary is send 
to a backup for every two summaries sent to the primary proxy 
since a redundant summary is sent for every two summaries the 
insert cost is times the cost in the normal case however these 
redundant entries result in only a negligible increase in lookup 
overhead due the logarithmic dependence of lookup cost on the index 
size while providing full resilience to any single proxy failure 
 storage microbenchmarks 
since sensors are resource-constrained the energy consumption 
and the latency at this tier are important measures for evaluating the 
performance of a storage architecture before performing an 
endto-end evaluation of our system we provide more detailed 
information on the energy consumption of the storage component used 
to implement the tsar local archive based on empirical 
measurements in addition we compare these figures to those for other 
local storage technologies as well as to the energy consumption of 
wireless communication using information from the literature for 
empirical measurements we measure energy usage for the storage 
component itself i e current drawn by the flash chip as well as 
for the entire mica mote 
the power measurements in table were performed for the 
at db flash memory on a mica mote which is an older 
nor flash device the most promising technology for low-energy 
storage on sensing devices is nand flash such as the samsung 
k k g u m device published power numbers for this 
device are provided in the table published energy requirements for 
wireless transmission using the chipcon cc radio used 
in micaz and telos motes are provided for comparison assuming 
energy energy byte 
mote flash 
read byte page µj 
 µj total 
 µj 
write byte page µj 
 µj total 
 µj 
nand flash 
read byte page µj nj 
write byte page µj nj 
erase k byte sector µj nj 
cc radio 
transmit bits 
 - dbm 
 µj µj 
receive bits µj µj 
mote avr processor 
in-memory search 
 bytes 
 µj nj 
table storage and communication energy costs measured 
values 
 
 
 
 
 
 
 
latency ms 
number of hops 
 a multi-hop query 
performance 
 
 
 
 
 
 
 
latency ms 
index size entries 
sensor communication 
proxy communication 
sensor lookup processing 
 b query performance 
figure query processing latency 
zero network and protocol overhead comparing the total energy 
cost for writing flash erase write to the total cost for 
communication transmit receive we find that the nand flash is almost 
 times more efficient than radio communication even assuming 
perfect network protocols 
 prototype evaluation 
this section reports results from an end-to-end evaluation of the 
tsar prototype involving both tiers in our setup there are four 
proxies connected via links and three sensors per proxy the 
multi-hop topology was preconfigured such that sensor nodes were 
connected in a line to each proxy forming a minimal tree of depth 
 
 
 
 
 
 
retrievallatency ms 
archived data retrieved bytes 
 a data query and fetch 
time 
 
 
 
 
 
 
 
latency ms 
number of -byte records searched 
 b sensor query 
processing delay 
figure query latency components 
 
 due to resource constraints we were unable to perform 
experiments with dozens of sensor nodes however this topology ensured 
that the network diameter was as large as for a typical network of 
significantly larger size 
our evaluation metric is the end-to-end latency of query 
processing a query posed on tsar first incurs the latency of a sparse 
skip graph lookup followed by routing to the appropriate sensor 
node s the sensor node reads the required page s from its local 
archive processes the query on the page that is read and transmits 
the response to the proxy which then forwards it to the user we 
first measure query latency for different sensors in our multi-hop 
topology depending on which of the sensors is queried the total 
latency increases almost linearly from about ms to second as 
the number of hops increases from to see figure a 
figure b provides a breakdown of the various components 
of the end-to-end latency the dominant component of the total 
latency is the communication over one or more hops the 
typical time to communicate over one hop is approximately ms 
this large latency is primarily due to the use of a duty-cycled mac 
layer the latency will be larger if the duty cycle is reduced e g 
the setting as opposed to the setting used in this 
experiment and will conversely decrease if the duty cycle is increased 
the figure also shows the latency for varying index sizes as 
expected the latency of inter-proxy communication and skip graph 
lookups increases logarithmically with index size not surprisingly 
the overhead seen at the sensor is independent of the index size 
the latency also depends on the number of packets transmitted 
in response to a query-the larger the amount of data retrieved by a 
query the greater the latency this result is shown in figure a 
the step function is due to packetization in tinyos tinyos sends 
one packet so long as the payload is smaller than bytes and splits 
the response into multiple packets for larger payloads as the data 
retrieved by a query is increased the latency increases in steps 
where each step denotes the overhead of an additional packet 
finally figure b shows the impact of searching and 
processing flash memory regions of increasing sizes on a sensor each 
summary represents a collection of records in flash memory and 
all of these records need to be retrieved and processed if that 
summary matches a query the coarser the summary the larger the 
memory region that needs to be accessed for the search sizes 
examined amortization of overhead when searching multiple flash 
pages and archival records as well as within the flash chip and its 
associated driver results in the appearance of sub-linear increase 
in latency with search size in addition the operation can be seen 
to have very low latency in part due to the simplicity of our query 
processing requiring only a compare operation with each stored 
element more complex operations however will of course incur 
greater latency 
 adaptive summarization 
when data is summarized by the sensor before being reported 
to the proxy information is lost with the interval summarization 
method we are using this information loss will never cause the 
proxy to believe that a sensor node does not hold a value which it in 
fact does as all archived values will be contained within the interval 
reported however it does cause the proxy to believe that the sensor 
may hold values which it does not and forward query messages to 
the sensor for these values these false positives constitute the cost 
of the summarization mechanism and need to be balanced against 
the savings achieved by reducing the number of reports the goal 
of adaptive summarization is to dynamically vary the summary size 
so that these two costs are balanced 
 
 
 
 
 
 
 
fractionoftruehits 
summary size number of records 
 a impact of summary 
size 
 
 
 
 
 
 
 
 
 
summarizationsize num records 
normalized time units 
query rate 
query rate 
query rate 
 b adaptation to query 
rate 
figure impact of summarization granularity 
figure a demonstrates the impact of summary granularity 
on false hits as the number of records included in a summary 
is increased the fraction of queries forwarded to the sensor which 
match data held on that sensor true positives decreases next 
in figure b we run the a emtos simulation with our 
adaptive summarization algorithm enabled the adaptive algorithm 
increases the summary granularity defined as the number of records 
per summary when cost updates 
cost falsehits 
 and reduces it if 
cost updates 
cost falsehits 
 − where is a small constant to 
demonstrate the adaptive nature of our technique we plot a time series 
of the summarization granularity we begin with a query rate of 
query per samples decrease it to every samples and then 
increase it again to query every samples as shown in 
figure b the adaptive technique adjusts accordingly by sending 
more fine-grain summaries at higher query rates in response to the 
higher false hit rate and fewer coarse-grain summaries at lower 
query rates 
 related work 
in this section we review prior work on storage and indexing 
techniques for sensor networks while our work addresses both 
problems jointly much prior work has considered them in isolation 
the problem of archival storage of sensor data has received 
limited attention in sensor network literature elf is a 
logstructured file system for local storage on flash memory that 
provides load leveling and matchbox is a simple file system that is 
packaged with the tinyos distribution both these systems 
focus on local storage whereas our focus is both on storage at the 
remote sensors as well as providing a unified view of distributed 
data across all such local archives multi-resolution storage is 
intended for in-network storage and search in systems where there 
is significant data in comparison to storage resources in contrast 
tsar addresses the problem of archival storage in two-tier systems 
where sufficient resources can be placed at the edge sensors the 
rise platform being developed as part of the node project 
at ucr addresses the issues of hardware platform support for large 
amounts of storage in remote sensor nodes but not the indexing 
and querying of this data 
in order to efficiently access a distributed sensor store an index 
needs to be constructed of the data early work on sensor networks 
such as directed diffusion assumes a system where all useful 
sensor data was stored locally at each sensor and spatially scoped 
queries are routed using geographic co-ordinates to locations where 
the data is stored sources publish the events that they detect and 
sinks with interest in specific events can subscribe to these events 
the directed diffusion substrate routes queries to specific locations 
 
if the query has geographic information embedded in it e g find 
temperature in the south-west quadrant and if not the query is 
flooded throughout the network 
these schemes had the drawback that for queries that are not 
geographically scoped search cost o n for a network of n nodes 
may be prohibitive in large networks with frequent queries 
local storage with in-network indexing approaches address this 
issue by constructing indexes using frameworks such as geographic 
hash tables and quad trees recent research has seen 
a growing body of work on data indexing schemes for sensor 
networks one such scheme is dcs which provides 
a hash function for mapping from event name to location dcs 
constructs a distributed structure that groups events together 
spatially by their named type distributed index of features in 
sensornets difs and multi-dimensional range queries in sensor 
networks dim extend the data-centric storage approach to 
provide spatially distributed hierarchies of indexes to data 
while these approaches advocate in-network indexing for sensor 
networks we believe that indexing is a task that is far too 
complicated to be performed at the remote sensor nodes since it involves 
maintaining significant state and large tables tsar provides a 
better match between resource requirements of storage and indexing 
and the availability of resources at different tiers thus complex 
operations such as indexing and managing metadata are performed 
at the proxies while storage at the sensor remains simple 
in addition to storage and indexing techniques specific to sensor 
networks many distributed peer-to-peer and spatio-temporal index 
structures are relevant to our work dhts can be used for 
indexing events based on their type quad-tree variants such as 
rtrees can be used for optimizing spatial searches and k-d 
trees can be used for multi-attribute search while this paper 
focuses on building an ordered index structure for range queries we 
will explore the use of other index structures for alternate queries 
over sensor data 
 conclusions 
in this paper we argued that existing sensor storage systems 
are designed primarily for flat hierarchies of homogeneous sensor 
nodes and do not fully exploit the multi-tier nature of emerging 
sensor networks we presented the design of tsar a fundamentally 
different storage architecture that envisions separation of data from 
metadata by employing local storage at the sensors and distributed 
indexing at the proxies at the proxy tier tsar employs a novel 
multi-resolution ordered distributed index structure the sparse 
interval skip graph for efficiently supporting spatio-temporal and 
range queries at the sensor tier tsar supports energy-aware 
adaptive summarization that can trade-off the energy cost of 
transmitting metadata to the proxies against the overhead of false hits 
resulting from querying a coarser resolution index structure we 
implemented tsar in a two-tier sensor testbed comprising 
stargatebased proxies and mote-based sensors our experimental 
evaluation of tsar demonstrated the benefits and feasibility of 
employing our energy-efficient low-latency distributed storage architecture 
in multi-tier sensor networks 
 references 
 james aspnes and gauri shah skip graphs in fourteenth annual acm-siam 
symposium on discrete algorithms pages - baltimore md usa 
 - january 
 jon louis bentley multidimensional binary search trees used for associative 
searching commun acm - 
 philippe bonnet j e gehrke and praveen seshadri towards sensor database 
systems in proceedings of the second international conference on mobile 
data management january 
 chipcon cc ghz ieee zigbee-ready rf transceiver 
 thomas h cormen charles e leiserson ronald l rivest and clifford stein 
introduction to algorithms the mit press and mcgraw-hill second edition 
edition 
 adina crainiceanu prakash linga johannes gehrke and jayavel 
shanmugasundaram querying peer-to-peer networks using p-trees 
technical report tr - cornell university 
 hui dai michael neufeld and richard han elf an efficient log-structured 
flash file system for micro sensor nodes in sensys proceedings of the nd 
international conference on embedded networked sensor systems pages 
 - new york ny usa acm press 
 peter desnoyers deepak ganesan huan li and prashant shenoy presto a 
predictive storage architecture for sensor networks in tenth workshop on hot 
topics in operating systems hotos x june 
 deepak ganesan ben greenstein denis perelyubskiy deborah estrin and 
john heidemann an evaluation of multi-resolution storage in sensor networks 
in proceedings of the first acm conference on embedded networked sensor 
systems sensys 
 l girod t stathopoulos n ramanathan j elson d estrin e osterweil 
and t schoellhammer a system for simulation emulation and deployment of 
heterogeneous sensor networks in proceedings of the second acm conference 
on embedded networked sensor systems baltimore md 
 b greenstein d estrin r govindan s ratnasamy and s shenker difs a 
distributed index for features in sensor networks elsevier journal of ad-hoc 
networks 
 antonin guttman r-trees a dynamic index structure for spatial searching in 
sigmod proceedings of the acm sigmod international 
conference on management of data pages - new york ny usa 
acm press 
 nicholas harvey michael b jones stefan saroiu marvin theimer and alec 
wolman skipnet a scalable overlay network with practical locality properties 
in in proceedings of the th usenix symposium on internet technologies and 
systems usits seattle wa march 
 jason hill robert szewczyk alec woo seth hollar david culler and 
kristofer pister system architecture directions for networked sensors in 
proceedings of the ninth international conference on architectural support for 
programming languages and operating systems asplos-ix pages - 
cambridge ma usa november acm 
 atmel inc -megabit -volt or -volt dataflash at db b 
 samsung semiconductor inc k w g u m k k g u m m x bit 
 g x bit nand flash memory 
 chalermek intanagonwiwat ramesh govindan and deborah estrin directed 
diffusion a scalable and robust communication paradigm for sensor networks 
in proceedings of the sixth annual international conference on mobile 
computing and networking pages - boston ma august acm 
press 
 xin li young-jin kim ramesh govindan and wei hong multi-dimensional 
range queries in sensor networks in proceedings of the first acm conference 
on embedded networked sensor systems sensys to appear 
 witold litwin marie-anne neimat and donovan a schneider rp a family 
of order preserving scalable distributed data structures in vldb 
proceedings of the th international conference on very large data bases 
pages - san francisco ca usa 
 samuel madden michael franklin joseph hellerstein and wei hong tag a 
tiny aggregation service for ad-hoc sensor networks in osdi boston ma 
 
 a mitra a banerjee w najjar d zeinalipour-yazti d gunopulos and 
v kalogeraki high performance low power sensor platforms featuring 
gigabyte scale storage in senmetrics third international workshop on 
measurement modeling and performance analysis of wireless sensor 
networks july 
 j polastre j hill and d culler versatile low power media access for wireless 
sensor networks in proceedings of the second acm conference on embedded 
networked sensor systems sensys november 
 william pugh skip lists a probabilistic alternative to balanced trees commun 
acm - 
 s ratnasamy d estrin r govindan b karp l yin s shenker and f yu 
data-centric storage in sensornets in acm first workshop on hot topics in 
networks 
 s ratnasamy p francis m handley r karp and s shenker a scalable 
content addressable network in proceedings of the acm sigcomm 
conference 
 s ratnasamy b karp l yin f yu d estrin r govindan and s shenker 
ght - a geographic hash-table for data-centric storage in first acm 
international workshop on wireless sensor networks and their applications 
 
 n xu e osterweil m hamilton and d estrin 
http www lecs cs ucla edu ˜nxu ess james reserve data 
 
a holistic approach to high-performance computing 
xgrid experience 
david przybyla 
ringling school of art and design 
 north tamiami trail 
sarasota florida 
 - - 
dprzybyl ringling edu 
karissa miller 
ringling school of art and design 
 north tamiami trail 
sarasota florida 
 - - 
kmiller ringling edu 
mahmoud pegah 
ringling school of art and design 
 north tamiami trail 
sarasota florida 
 - - 
mpegah ringling edu 
abstract 
the ringling school of art and design is a fully accredited 
fouryear college of visual arts and design with a student to computer 
ratio of better than -to- the ringling school has achieved 
national recognition for its large-scale integration of technology 
into collegiate visual art and design education we have found 
that mac os x is the best operating system to train future artists 
and designers moreover we can now buy macs to run high-end 
graphics nonlinear video editing animation multimedia web 
production and digital video applications rather than expensive 
unix workstations as visual artists cross from paint on canvas 
to creating in the digital realm the demand for a 
highperformance computing environment grows in our public 
computer laboratories students use the computers most often 
during the workday at night and on weekends the computers see 
only light use in order to harness the lost processing time for 
tasks such as video rendering we are testing xgrid a suite of mac 
os x applications recently developed by apple for parallel and 
distributed high-performance computing 
as with any new technology deployment it managers need to 
consider a number of factors as they assess plan and implement 
xgrid therefore we would like to share valuable information we 
learned from our implementation of an xgrid environment with 
our colleagues in our report we will address issues such as 
assessing the needs for grid computing potential applications 
management tools security authentication integration into 
existing infrastructure application support user training and user 
support furthermore we will discuss the issues that arose and the 
lessons learned during and after the implementation process 
categories and subject descriptors 
c computer-communication networks distributed 
systemsdistributed applications 
general terms 
management documentation performance design economics 
reliability experimentation 
 introduction 
grid computing does not have a single universally accepted 
definition the technology behind grid computing model is not 
new its roots lie in early distributed computing models that date 
back to early s where scientists harnessed the computing 
power of idle workstations to let compute intensive applications 
to run on multiple workstations which dramatically shortening 
processing times although numerous distributed computing 
models were available for discipline-specific scientific 
applications only recently have the tools became available to use 
general-purpose applications on a grid consequently the grid 
computing model is gaining popularity and has become a show 
piece of utility computing since in the it industry various 
computing models are used interchangeably with grid computing 
we first sort out the similarities and difference between these 
computing models so that grid computing can be placed in 
perspective 
 clustering 
a cluster is a group of machines in a fixed configuration united to 
operate and be managed as a single entity to increase robustness 
and performance the cluster appears as a single high-speed 
system or a single highly available system in this model 
resources can not enter and leave the group as necessary there 
are at least two types of clusters parallel clusters and 
highavailability clusters clustered machines are generally in spatial 
proximity such as in the same server room and dedicated solely 
to their task 
in a high-availability cluster each machine provides the same 
service if one machine fails another seamlessly takes over its 
workload for example each computer could be a web server for 
a web site should one web server die another provides the 
service so that the web site rarely if ever goes down 
a parallel cluster is a type of supercomputer problems are split 
into many parts and individual cluster members are given part of 
the problem to solve an example of a parallel cluster is 
composed of apple power mac g computers at virginia tech 
university 
 distributed computing 
distributed computing spatially expands network services so that 
the components providing the services are separated the major 
objective of this computing model is to consolidate processing 
power over a network a simple example is spreading services 
such as file and print serving web serving and data storage across 
multiple machines rather than a single machine handling all the 
tasks distributed computing can also be more fine-grained where 
even a single application is broken into parts and each part located 
on different machines a word processor on one server a spell 
checker on a second server etc 
 utility computing 
literally utility computing resembles common utilities such as 
telephone or electric service a service provider makes computing 
resources and infrastructure management available to a customer 
as needed and charges for usage rather than a flat rate the 
important thing to note is that resources are only used as needed 
and not dedicated to a single customer 
 grid computing 
grid computing contains aspects of clusters distributed 
computing and utility computing in the most basic sense grid 
turns a group of heterogeneous systems into a centrally managed 
but flexible computing environment that can work on tasks too 
time intensive for the individual systems the grid members are 
not necessarily in proximity but must merely be accessible over a 
network the grid can access computers on a lan wan or 
anywhere in the world via the internet in addition the computers 
comprising the grid need not be dedicated to the grid rather they 
can function as normal workstations and then advertise their 
availability to the grid when not in use 
the last characteristic is the most fundamental to the grid 
described in this paper a well-known example of such an ad 
hoc grid is the seti home project of the university of 
california at berkeley which allows any person in the world with 
a computer and an internet connection to donate unused processor 
time for analyzing radio telescope data 
 comparing the grid and cluster 
a computer grid expands the capabilities of the cluster by loosing 
its spatial bounds so that any computer accessible through the 
network gains the potential to augment the grid a fundamental 
grid feature is that it scales well the processing power of any 
machine added to the grid is immediately availably for solving 
problems in addition the machines on the grid can be 
generalpurpose workstations which keep down the cost of expanding the 
grid 
 assessing the need for grid 
computing 
effective use of a grid requires a computation that can be divided 
into independent i e parallel tasks the results of each task 
cannot depend on the results of any other task and so the 
members of the grid can solve the tasks in parallel once the tasks 
have been completed the results can be assembled into the 
solution examples of parallelizable computations are the 
mandelbrot set of fractals the monte carlo calculations used in 
disciplines such as solid state physics and the individual frames 
of a rendered animation this paper is concerned with the last 
example 
 applications appropriate for grid 
computing 
the applications used in grid computing must either be 
specifically designed for grid use or scriptable in such a way that 
they can receive data from the grid process the data and then 
return results in other words the best candidates for grid 
computing are applications that run the same or very similar 
computations on a large number of pieces of data without any 
dependencies on the previous calculated results applications 
heavily dependent on data handling rather than processing power 
are generally more suitable to run on a traditional environment 
than on a grid platform of course the applications must also run 
on the computing platform that hosts the grid our interest is in 
using the alias maya application with apple s xgrid on 
mac os x 
commercial applications usually have strict license requirements 
this is an important concern if we install a commercial 
application such as maya on all members of our grid by its 
nature the size of the grid may change as the number of idle 
computers changes how many licenses will be required our 
resolution of this issue will be discussed in a later section 
 integration into the existing 
infrastructure 
the grid requires a controller that recognizes when grid members 
are available and parses out job to available members the 
controller must be able to see members on the network this does 
not require that members be on the same subnet as the controller 
but if they are not any intervening firewalls and routers must be 
configured to allow grid traffic 
 xgrid 
xgrid is apple s grid implementation it was inspired by zilla a 
desktop clustering application developed by next and acquired 
by apple in this report we describe the xgrid technology 
preview a free download that requires mac os x or later 
and a minimum mb ram 
xgrid leverages apple s traditional ease of use and configuration 
if the grid members are on the same subnet by default xgrid 
automatically discovers available resources through rendezvous 
 tasks are submitted to the grid through a gui interface or by 
the command line a system preference pane controls when each 
computer is available to the grid 
it may be best to view xgrid as a facilitator the xgrid 
architecture handles software and data distribution job execution 
and result aggregation however xgrid does not perform the 
actual calculations 
 xgrid components 
xgrid has three major components the client controller and the 
agent each component is included in the default installation and 
any computer can easily be configured to assume any role in 
 
fact for testing purposes a computer can simultaneously assume 
all roles in local mode the more typical production use is 
called cluster mode 
the client submits jobs to the controller through the xgrid gui or 
command line the client defines how the job will be broken into 
tasks for the grid if any files or executables must be sent as part 
of a job they must reside on the client or at a location accessible 
to the client when a job is complete the client can retrieve the 
results from the controller a client can only connect to a single 
controller at a time 
the controller runs the gridserver process it queues tasks 
received from clients distributes those tasks to the agents and 
handles failover if an agent cannot complete a task in xgrid 
technology preview a controller can handle a maximum of 
 agent connections only one controller can exist per 
logical grid 
the agents run the gridagent process when the gridagent 
process starts it registers with a controller an agent can only be 
connected to one controller at a time agents receive tasks from 
their controller perform the specified computations and then 
send the results back to the controller an agent can be configured 
to always accept tasks or to just accept them when the computer 
is not otherwise busy 
 security and authentication 
by default xgrid requires two passwords first a client needs a 
password to access a controller second the controller needs a 
password to access an agent either password requirement can be 
disabled xgrid uses two-way-random mutual authentication 
protocol with md hashes at this time data encryption is only 
used for passwords 
as mentioned earlier an agent registers with a controller when the 
gridagent process starts there is no native method for the 
controller to reject agents and so it must accept any agent that 
registers this means that any agent could submit a job that 
consumes excessive processor and disk space on the agents of 
course since mac os x is a bsd-based operating system the 
controller could employ unix methods of restricting network 
connections from agents 
the xgrid daemons run as the user nobody which means the 
daemons can read write or execute any file according to world 
permissions thus xgrid jobs can execute many commands and 
write to tmp and volumes in general this is not a major security 
risk but is does require a level of trust between all members of the 
grid 
 using xgrid 
 installation 
basic xgrid installation and configuration is described both in 
apple documentation and online at the university of utah web 
site the installation is straightforward and offers no options 
for customization this means that every computer on which 
xgrid is installed has the potential to be a client controller or 
agent 
 agent and controller configuration 
the agents and controllers can be configured through the xgrid 
preference pane in the system preferences or xml files in 
 library preferences here the gridserver and gridagent 
processes are started passwords set and the controller discovery 
method used by agents is selected by default agents use 
rendezvous to find a controller although the agents can also be 
configured to look for a specific host 
the xgrid preference pane also sets whether the agents will 
always accept jobs or only accept jobs when idle in xgrid terms 
idle either means that the xgrid screen saver has activated or the 
mouse and keyboard have not been used for more than 
minutes even if the agent is configured to always accept tasks if 
the computer is being used these tasks will run in the background 
at a low priority 
however if an agent only accepts jobs when idle any unfinished 
task being performed when the computer ceases being idle are 
immediately stopped and any intermediary results lost then the 
controller assigns the task to another available member of the 
grid 
advertising the controller via rendezvous can be disabled by 
editing library preferences com apple xgrid controller plist this 
however will not prevent an agent from connecting to the 
controller by hostname 
 sending jobs from an xgrid client 
the client sends jobs to the controller either through the xgrid 
gui or the command line the xgrid gui submits jobs via small 
applications called plug-ins sample plug-ins are provided by 
apple but they are only useful as simple testing or as examples of 
how to create a custom plug-in if we are to employ xgrid for 
useful work we will require a custom plug-in 
james reynolds details the creation of custom plug-ins on the 
university of utah mac os web site xgrid stores plug-ins in 
 library xgrid plug-ins or   library xgrid plug-ins depending 
on whether the plug-in was installed with xgrid or created by a 
user 
the core plug-in parameter is the command which includes the 
executable the agents will run another important parameter is the 
working directory this directory contains necessary files that 
are not installed on the agents or available to them over a network 
the working directory will always be copied to each agent so it is 
best to keep this directory small if the files are installed on the 
agents or available over a network the working directory 
parameter is not needed 
the command line allows the options available with the gui 
plug-in but it can be slightly more cumbersome however the 
command line probably will be the method of choice for serious 
work the command arguments must be included in a script 
unless they are very basic this can be a shell perl or python 
script as long as the agent can interpret it 
 running the xgrid job 
when the xgrid job is started the command tells the controller 
how to break the job into tasks for the agents then the command 
is tarred and gzipped and sent to each agent if there is a working 
directory this is also tarred and gzipped and sent to the agents 
 
the agents extract these files into tmp and run the task recall 
that since the gridagent process runs as the user nobody 
everything associated with the command must be available to 
nobody 
executables called by the command should be installed on the 
agents unless they are very simple if the executable depends on 
libraries or other files it may not function properly if transferred 
even if the dependent files are referenced in the working directory 
when the task is complete the results are available to the client 
in principle the results are sent to the client but whether this 
actually happens depends on the command if the results are not 
sent to the client they will be in tmp on each agent when 
available a better solution is to direct the results to a network 
volume accessible to the client 
 limitations and idiosyncrasies 
since xgrid is only in its second preview release there are some 
rough edges and limitations apple acknowledges some 
limitations for example the controller cannot determine 
whether an agent is trustworthy and the controller always copies 
the command and working directory to the agent without checking 
to see if these exist on the agent 
other limitations are likely just a by-product of an unfinished 
work neither the client nor controller can specify which agents 
will receive the tasks which is particularly important if the agents 
contain a variety of processor types and speeds and the user wants 
to optimize the calculations at this time the best solution to this 
problem may be to divide the computers into multiple logical 
grids there is also no standard way to monitor the progress of a 
running job on each agent the xgrid gui and command line 
indicate which agents are working on tasks but gives no 
indication of progress 
finally at this time only mac os x clients can submit jobs to the 
grid the framework exists to allow third parties to write plug-ins 
for other unix flavors but apple has not created them 
 xgrid implementation 
our goal is an xgrid render farm for alias maya the ringling 
school has about apple power mac g s and g s in 
computer labs the computers range from mhz 
singleprocessor g s and mhz and ghz dual-processor g s to 
 ghz dual-processor g s all of these computers are lightly 
used in the evening and on weekends and represent an enormous 
processing resource for our student rendering projects 
 software installation 
during our xgrid testing we loaded software on each computer 
multiple times including the operating systems we saved time by 
facilitating our installations with the remote administration 
daemon radmind software developed at the university of 
michigan 
everything we installed for testing was first created as a radmind 
base load or overload thus mac os x mac os x developer 
tools xgrid pov-ray and alias maya were stored on a 
radmind server and then installed on our test computers when 
needed 
 initial testing 
we used six ghz dual-processor apple power mac g s for 
our xgrid tests each computer ran mac os x and 
contained gb ram as shown in figure one computer 
served as both client and controller while the other five acted as 
agents 
before attempting maya rendering with xgrid we performed 
basic calculations to cement our understanding of xgrid apple s 
xgrid documentation is sparse so finding helpful web sites 
facilitated our learning 
we first ran the mandelbrot set plug-in provided by apple which 
allowed us to test the basic functionality of our grid then we 
performed benchmark rendering with the open source 
application pov-ray as described by daniel côté and 
james reynolds our results showed that one dual-processor 
g rendering the benchmark pov-ray image took minutes 
breaking the image into three equal parts and using xgrid to send 
the parts to three agents required minutes however two 
agents finished their rendering in minutes while the third 
agent used minutes the entire render was only as fast as the 
slowest agent 
these results gave us two important pieces of information first 
the much longer rendering time for one of the tasks indicated that 
we should be careful how we split jobs into tasks for the agents 
all portions of the rendering will not take equal amounts of time 
even if the pixel size is the same second since pov-ray cannot 
take advantage of both processors in a g neither can an xgrid 
task running pov-ray alias maya does not have this limitation 
 rendering with alias maya 
we first installed alias maya for mac os x on the 
client controller and each agent maya requires licenses for use 
as a workstation application however if it is just used for 
rendering from the command line or a script no license is needed 
we thus created a minimal installation of maya as a radmind 
overload the application was installed in a hidden directory 
inside applications this was done so that normal users of the 
workstations would not find and attempt to run maya which 
would fail because these installations are not licensed for such 
use 
in addition maya requires the existence of a directory ending in 
the path maya the directory must be readable and writable by 
the maya user for a user running maya on a mac os x 
workstation the path would usually be   documents maya 
unless otherwise specified this directory will be the default 
location for maya data and output files if the directory does not 
figure xgrid test grid 
client 
controller 
agent 
agent 
agent 
agent 
agent 
network 
volume 
jobs 
data 
data 
 
exist maya will try to create it even if the user specifies that the 
data and output files exist in other locations 
however xgrid runs as the user nobody which does not have a 
home directory maya is unable to create the needed directory 
and looks instead for alias maya this directory also does not 
exist and the user nobody has insufficient rights to create it our 
solution was to manually create alias maya and give the user 
nobody read and write permissions 
we also created a network volume for storage of both the 
rendering data and the resulting rendered frames this avoided 
sending the maya files and associated textures to each agent as 
part of a working directory such a solution worked well for us 
because our computers are geographically close on a lan if 
greater distance had separated the agents from the 
client controller specifying a working directory may have been a 
better solution 
finally we created a custom gui plug-in for xgrid the plug-in 
command calls a perl script with three arguments two arguments 
specify the beginning and end frames of the render and the third 
argument the number of frames in each job which we call the 
cluster size the script then calculates the total number of jobs 
and parses them out to the agents for example if we begin at 
frame and end at frame with frames for each job the 
plug-in will create jobs and send them out to the agents 
once the jobs are sent to the agents the script executes the 
 usr sbin render command on each agent with the parameters 
appropriate for the particular job the results are sent to the 
network volume 
with the setup described we were able to render with alias maya 
 on our test grid rendering speed was not important at this time 
our first goal was to implement the grid and in that we succeeded 
 pseudo code for perl script in custom xgrid 
plug-in 
in this section we summarize in simplified pseudo code format the 
perl script used in our xgrig plug-in 
agent jobs{ 
 read beginning frame end frame and cluster size of 
render 
 check whether the render can be divided into an integer 
number of jobs based on the cluster size 
 if there are not an integer number of jobs reduce the cluster 
size of the last job and set its last frame to the end frame of 
the render 
 determine the start frame and end frame for each job 
 execute the render command 
} 
 lessons learned 
rendering with maya from the xgrid gui was not trivial the 
lack of xgrid documentation and the requirements of maya 
combined into a confusing picture where it was difficult to decide 
the true cause of the problems we encountered trial and error 
was required to determine the best way to set up our grid 
the first hurdle was creating the directory alias maya with read 
and write permissions for the user nobody the second hurdle was 
learning that we got the best performance by storing the rendering 
data on a network volume 
the last major hurdle was retrieving our results from the agents 
unlike the pov-ray rendering tests our initial maya results were 
never returned to the client instead maya stored the results in 
 tmp on each agent specifying in the plug-in where to send the 
results would not change this behavior we decided this was 
likely a maya issue rather than an xgrid issue and the solution 
was to send the results to the network volume via the perl script 
 future plans 
maya on xgrid is not yet ready to be used by the students of 
ringling school in order to do this we must address at least the 
following concerns 
 continue our rendering tests through the command line 
rather than the gui plug-in this will be essential for the 
following step 
 develop an appropriate interface for users to send jobs to the 
xgrid controller this will probably be an extension to the 
web interface of our existing render farm where the student 
specifies parameters that are placed in a script that issues the 
render command 
 perform timed maya rendering tests with xgrid part of this 
should compare the rendering times for power mac g s and 
g s 
 conclusion 
grid computing continues to advance recently the it industry 
has witnessed the emergence of numerous types of contemporary 
grid applications in addition to the traditional grid framework for 
compute intensive applications for instance peer-to-peer 
applications such as kazaa are based on storage grids that do not 
share processing power but instead an elegant protocol to swap 
files between systems although in our campuses we discourage 
students from utilizing peer-to-peer applications from music 
sharing the same protocol can be utilized on applications such as 
decision support and data mining the national virtual 
collaboratory grid project will link earthquake researchers 
across the u s with computing resources allowing them to share 
extremely large data sets research equipment and work together 
as virtual teams over the internet 
there is an assortment of new grid players in the it world 
expanding the grid computing model and advancing the grid 
technology to the next level sap is piloting a project to 
grid-enable sap erp applications dell has partnered with 
platform computing to consolidate computing resources and 
provide grid-enabled systems for compute intensive applications 
oracle has integrated support for grid computing in their g 
release united devices offers hosting service for 
gridon-demand and sun microsystems continues their research and 
development of sun s n grid engine which combines grid 
and clustering platforms 
simply the grid computing is up and coming the potential 
benefits of grid computing are colossal in higher education 
learning while the implementation costs are low today it would 
be difficult to identify an application with as high a return on 
investment as grid computing in information technology divisions 
in higher education institutions it is a mistake to overlook this 
technology with such a high payback 
 
 acknowledgments 
the authors would like to thank scott hanselman of the it team 
at the ringling school of art and design for providing valuable 
input in the planning of our xgrid testing we would also like to 
thank the posters of the xgrid mailing list for providing 
insight into many areas of xgrid 
 references 
 apple academic research 
http www apple com education science profiles vatech 
 seti home search for extraterrestrial intelligence at 
home http setiathome ssl berkeley edu 
 alias http www alias com 
 apple computer xgrid http www apple com acg xgrid 
 xgrid guide http www apple com acg xgrid 
 apple mac os x features 
http www apple com macosx features rendezvous 
 xgrid manual page 
 james reynolds xgrid presentation university of utah 
http www macos utah edu xgrid 
 research systems unix group radmind university of 
michigan http rsug itd umich edu software radmind 
 using the radmind command line tools to maintain 
multiple mac os x machines 
 
http rsug itd umich edu software radmind files radmindtutorial- pdf 
 pov-ray http www povray org 
 daniel côté xgrid example parallel graphics rendering in 
povray http unu novajo ca simple 
 neesgrid http www neesgrid org 
 sap http www sap com 
 platform computing http platform com 
 grid http www oracle com technologies grid 
 united devices inc http ud com 
 n grid engine http www sun com 
software gridware index html 
 xgrig users mailing list 
 
http www lists apple com mailman listinfo xgridusers 
 
congestion games with load-dependent failures 
identical resources 
michal penn 
technion - iit 
haifa israel 
mpenn ie technion ac il 
maria polukarov 
technion - iit 
haifa israel 
pmasha tx technion ac il 
moshe tennenholtz 
technion - iit 
haifa israel 
moshet ie technion ac il 
abstract 
we define a new class of games congestion games with 
loaddependent failures cglfs which generalizes the well-known 
class of congestion games by incorporating the issue of 
resource failures into congestion games in a cglf agents 
share a common set of resources where each resource has a 
cost and a probability of failure each agent chooses a 
subset of the resources for the execution of his task in order to 
maximize his own utility the utility of an agent is the 
difference between his benefit from successful task completion 
and the sum of the costs over the resources he uses cglfs 
possess two novel features it is the first model to 
incorporate failures into congestion settings which results in a 
strict generalization of congestion games in addition it is 
the first model to consider load-dependent failures in such 
framework where the failure probability of each resource 
depends on the number of agents selecting this resource 
although as we show cglfs do not admit a potential 
function and in general do not have a pure strategy nash 
equilibrium our main theorem proves the existence of a pure 
strategy nash equilibrium in every cglf with identical 
resources and nondecreasing cost functions 
categories and subject descriptors 
c computer-communication networks distributed 
systems i artificial intelligence distributed 
artificial intelligence -multiagent systems 
general terms 
theory economics 
 introduction 
we study the effects of resource failures in congestion 
settings this study is motivated by a variety of situations 
in multi-agent systems with unreliable components such as 
machines computers etc we define a model for congestion 
games with load-dependent failures cglfs which provides 
simple and natural description of such situations in this 
model we are given a finite set of identical resources service 
providers where each element possesses a failure 
probability describing the probability of unsuccessful completion of 
its assigned tasks as a nondecreasing function of its 
congestion there is a fixed number of agents each having 
a task which can be carried out by any of the resources 
for reliability reasons each agent may decide to assign his 
task simultaneously to a number of resources thus the 
congestion on the resources is not known in advance but 
is strategy-dependent each resource is associated with a 
cost which is a nonnegative function of the congestion 
experienced by this resource the objective of each agent is to 
maximize his own utility which is the difference between his 
benefit from successful task completion and the sum of the 
costs over the set of resources he uses the benefits of the 
agents from successful completion of their tasks are allowed 
to vary across the agents 
the resource cost function describes the cost suffered by 
an agent for selecting that resource as a function of the 
number of agents who have selected it thus it is natural 
to assume that these functions are nonnegative in addition 
in many real-life applications of our model the resource cost 
functions have a special structure in particular they can 
monotonically increase or decrease with the number of the 
users depending on the context the former case is 
motivated by situations where high congestion on a resource 
causes longer delay in its assigned tasks execution and as 
a result the cost of utilizing this resource might be higher 
a typical example of such situation is as follows assume 
we need to deliver an important package since there is no 
guarantee that a courier will reach the destination in time 
we might send several couriers to deliver the same package 
the time required by each courier to deliver the package 
increases with the congestion on his way in addition the 
payment to a courier is proportional to the time he spends 
in delivering the package thus the payment to the courier 
increases when the congestion increases the latter case 
 decreasing cost functions describes situations where a group 
of agents using a particular resource have an opportunity to 
share its cost among the group s members or the cost of 
 
using a resource decreases with the number of users 
according to some marketing policy 
our results 
we show that cglfs and in particular cglfs with 
nondecreasing cost functions do not admit a 
potential function therefore the cglf model can not be 
reduced to congestion games nevertheless if the 
failure probabilities are constant do not depend on the 
congestion then a potential function is guaranteed to 
exist 
we show that cglfs and in particular cglfs with 
decreasing cost functions do not possess pure 
strategy nash equilibria however as we show in our main 
result there exists a pure strategy nash 
equilibrium in any cglf with nondecreasing cost 
functions 
related work 
our model extends the well-known class of congestion games 
 in a congestion game every agent has to choose from a 
finite set of resources where the utility or cost of an agent 
from using a particular resource depends on the number of 
agents using it and his total utility cost is the sum of 
the utilities costs obtained from the resources he uses an 
important property of these games is the existence of pure 
strategy nash equilibria monderer and shapley 
introduced the notions of potential function and potential game 
and proved that the existence of a potential function implies 
the existence of a pure strategy nash equilibrium they 
observed that rosenthal proved his theorem on 
congestion games by constructing a potential function hence 
every congestion game is a potential game moreover they 
showed that every finite potential game is isomorphic to a 
congestion game hence the classes of finite potential games 
and congestion games coincide 
congestion games have been extensively studied and 
generalized in particular leyton-brown and tennenholtz 
extended the class of congestion games to the class of 
localeffect games in a local-effect game each agent s payoff is 
effected not only by the number of agents who have chosen 
the same resources as he has chosen but also by the number 
of agents who have chosen neighboring resources in a given 
graph structure monderer dealt with another type of 
generalization of congestion games in which the resource 
cost functions are player-specific ps-congestion games he 
defined ps-congestion games of type q q-congestion games 
where q is a positive number and showed that every game 
in strategic form is a q-congestion game for some q 
playerspecific resource cost functions were discussed for the first 
time by milchtaich he showed that simple and 
strategysymmetric ps-congestion games are not potential games 
but always possess a pure strategy nash equilibrium 
pscongestion games were generalized to weighted congestion 
games or id-congestion games in which the 
resource cost functions are not only player-specific but also 
depend on the identity of the users of the resource 
ackermann et al showed that weighted congestion games 
admit pure strategy nash equilibria if the strategy space of 
each player consists of the bases of a matroid on the set of 
resources 
much of the work on congestion games has been inspired 
by the fact that every such game has a pure strategy nash 
equilibrium in particular fabrikant et al studied 
the computational complexity of finding pure strategy nash 
equilibria in congestion games intensive study has also 
been devoted to quantify the inefficiency of equilibria in 
congestion games koutsoupias and papadimitriou 
proposed the worst-case ratio of the social welfare achieved 
by a nash equilibrium and by a socially optimal strategy 
profile dubbed the price of anarchy as a measure of the 
performance degradation caused by lack of coordination 
christodoulou and koutsoupias considered the price of 
anarchy of pure equilibria in congestion games with linear 
cost functions roughgarden and tardos used this 
approach to study the cost of selfish routing in networks with 
a continuum of users 
however the above settings do not take into 
consideration the possibility that resources may fail to execute their 
assigned tasks in the computer science context of 
congestion games where the alternatives of concern are machines 
computers communication lines etc which are obviously 
prone to failures this issue should not be ignored 
penn polukarov and tennenholtz were the first to 
incorporate the issue of failures into congestion settings 
they introduced a class of congestion games with failures 
 cgfs and proved that these games while not being 
isomorphic to congestion games always possess nash equilibria 
in pure strategies the cgf-model significantly differs from 
ours in a cgf the authors considered the delay associated 
with successful task completion where the delay for an agent 
is the minimum of the delays of his successful attempts and 
the aim of each agent is to minimize his expected delay in 
contrast with the cgf-model in our model we consider the 
total cost of the utilized resources where each agent wishes 
to maximize the difference between his benefit from a 
successful task completion and the sum of his costs over the 
resources he uses 
the above differences imply that cgfs and cglfs 
possess different properties in particular if in our model the 
resource failure probabilities were constant and known in 
advance then a potential function would exist this however 
does not hold for cgfs in cgfs the failure probabilities 
are constant but there is no potential function 
furthermore the procedures proposed by the authors in for 
the construction of a pure strategy nash equilibrium are 
not valid in our model even in the simple agent-symmetric 
case where all agents have the same benefit from successful 
completion of their tasks 
our work provides the first model of congestion settings 
with resource failures which considers the sum of 
congestiondependent costs over utilized resources and therefore does 
not extend the cgf-model but rather generalizes the classic 
model of congestion games moreover it is the first model 
to consider load-dependent failures in the above context 
 
organization 
the rest of the paper is organized as follows in section 
we define our model in section we present our results 
in we show that cglfs in general do not have pure 
strategy nash equilibria in we focus on cglfs with 
nondecreasing cost functions nondecreasing cglfs we 
show that these games do not admit a potential function 
however in our main result we show the existence of pure 
strategy nash equilibria in nondecreasing cglfs section 
 is devoted to a short discussion many of the proofs are 
omitted from this conference version of the paper and will 
appear in the full version 
 the model 
the scenarios considered in this work consist of a finite set 
of agents where each agent has a task that can be carried 
out by any element of a set of identical resources service 
providers the agents simultaneously choose a subset of 
the resources in order to perform their tasks and their aim 
is to maximize their own expected payoff as described in 
the sequel 
let n be a set of n agents n ∈ n and let m be a set 
of m resources m ∈ n agent i ∈ n chooses a 
strategy σi ∈ σi which is a potentially empty subset of the 
resources that is σi is the power set of the set of 
resources σi p m given a subset s ⊆ n of the agents 
the set of strategy combinations of the members of s is 
denoted by σs ×i∈sσi and the set of strategy 
combinations of the complement subset of agents is denoted by 
σ−s σ−s σn s ×i∈n sσi the set of pure strategy 
profiles of all the agents is denoted by σ σ σn 
each resource is associated with a cost c · and a 
failure probability f · each of which depends on the 
number of agents who use this resource we assume that the 
failure probabilities of the resources are independent let 
σ σ σn ∈ σ be a pure strategy profile the 
 m-dimensional congestion vector that corresponds to σ is 
hσ 
 hσ 
e e∈m where hσ 
e 
˛ 
˛{i ∈ n e ∈ σi} 
˛ 
˛ the 
failure probability of a resource e is a monotone nondecreasing 
function f { n} → of the congestion 
experienced by e the cost of utilizing resource e is a function 
c { n} → r of the congestion experienced by e 
the outcome for agent i ∈ n is denoted by xi ∈ {s f} 
where s and f respectively indicate whether the task 
execution succeeded or failed we say that the execution of 
agent s i task succeeds if the task of agent i is successfully 
completed by at least one of the resources chosen by him 
the benefit of agent i from his outcome xi is denoted by 
vi xi where vi s vi a given nonnegative value and 
vi f 
the utility of agent i from strategy profile σ and his 
outcome xi ui σ xi is the difference between his benefit from 
the outcome vi xi and the sum of the costs of the 
resources he has used 
ui σ xi vi xi − 
x 
e∈σi 
c hσ 
e 
the expected utility of agent i from strategy profile σ ui σ 
is therefore 
ui σ − 
y 
e∈σi 
f hσ 
e 
 
vi − 
x 
e∈σi 
c hσ 
e 
where − 
q 
e∈σi 
f hσ 
e denotes the probability of successful 
completion of agent i s task we use the convention thatq 
e∈∅ f hσ 
e hence if agent i chooses an empty set 
σi ∅ does not assign his task to any resource then his 
expected utility ui ∅ σ−i equals zero 
 pure strategy nash equilibria 
in cglfs 
in this section we present our results on cglfs we 
investigate the property of the non- existence of pure strategy 
nash equilibria in these games we show that this class of 
games does not in general possess pure strategy equilibria 
nevertheless if the resource cost functions are 
nondecreasing then such equilibria are guaranteed to exist despite the 
non-existence of a potential function 
 decreasing cost functions 
we start by showing that the class of cglfs and in 
particular the subclass of cglfs with decreasing cost 
functions does not in general possess nash equilibria in pure 
strategies 
consider a cglf with two agents n { } and two 
resources m {e e } the cost function of each resource 
is given by c x 
xx where x ∈ { } and the failure 
probabilities are f and f the benefits 
of the agents from successful task completion are v 
and v below we present the payoff matrix of the game 
∅ {e } {e } {e e } 
∅ u u u u 
u u u u 
{e } u u u u 
u u u u 
{e } u u u u 
u u u u 
{e e } u − u − u − u 
u u u u 
table example for non-existence of pure strategy nash 
equilibria in cglfs 
it can be easily seen that for every pure strategy profile σ 
in this game there exist an agent i and a strategy σi ∈ σi 
such that ui σ−i σi ui σ that is every pure strategy 
profile in this game is not in equilibrium 
however if the cost functions in a given cglf do not 
decrease in the number of users then as we show in the 
main result of this paper a pure strategy nash equilibrium 
is guaranteed to exist 
 
 nondecreasing cost functions 
this section focuses on the subclass of cglfs with 
nondecreasing cost functions henceforth nondecreasing cglfs 
we show that nondecreasing cglfs do not in general 
admit a potential function therefore these games are not 
congestion games nevertheless we prove that all such games 
possess pure strategy nash equilibria 
 the non- existence of a potential function 
recall that monderer and shapley introduced the 
notions of potential function and potential game where 
potential game is defined to be a game that possesses a potential 
function a potential function is a real-valued function over 
the set of pure strategy profiles with the property that the 
gain or loss of an agent shifting to another strategy while 
the other agents strategies are kept unchanged equals to 
the corresponding increment of the potential function the 
authors showed that the classes of finite potential games 
and congestion games coincide 
here we show that the class of cglfs and in particular 
the subclass of nondecreasing cglfs does not admit a 
potential function and therefore is not included in the class of 
congestion games however for the special case of constant 
failure probabilities a potential function is guaranteed to 
exist to prove these statements we use the following 
characterization of potential games 
a path in σ is a sequence τ σ 
→ σ 
→ · · · such 
that for every k ≥ there exists a unique agent say agent 
i such that σk 
 σk− 
−i σi for some σi σk− 
i in σi a 
finite path τ σ 
→ σ 
→ · · · → σk 
 is closed if σ 
 σk 
 
it is a simple closed path if in addition σl 
 σk 
for every 
 ≤ l k ≤ k − the length of a simple closed path is 
defined to be the number of distinct points in it that is the 
length of τ σ 
→ σ 
→ · · · → σk 
 is k 
theorem let g be a game in strategic form with 
a vector u u un of utility functions for a finite 
path τ σ 
→ σ 
→ · · · → σk 
 let u τ 
pk 
k uik σk 
 − 
uik σk− 
 where ik is the unique deviator at step k then 
g is a potential game if and only if u τ for every 
simple closed path τ of length 
load-dependent failures 
based on theorem we present the following 
counterexample that demonstrates the non-existence of a potential 
function in cglfs 
we consider the following agent-symmetric game g in 
which two agents n { } wish to assign a task to two 
resources m {e e } the benefit from a successful task 
completion of each agent equals v and the failure 
probability function strictly increases with the congestion consider 
the simple closed path of length which is formed by 
α ∅ {e } β {e } {e } 
γ {e } {e e } δ ∅ {e e } 
{e } {e e } 
∅ u u 
u − f v − c u 
` 
 − f 
´ 
v − c 
{e } u − f v − c u − f v − c 
u − f v − c u − f f v − c − c 
table example for non-existence of potentials in cglfs 
therefore 
u α − u β u β − u γ u γ − u δ 
 u δ − u α v − f f − f 
thus by theorem nondecreasing cglfs do not 
admit potentials as a result they are not congestion games 
however as presented in the next section the special case 
in which the failure probabilities are constant always 
possesses a potential function 
constant failure probabilities 
we show below that cglfs with constant failure 
probabilities always possess a potential function this follows from 
the fact that the expected benefit revenue of each agent in 
this case does not depend on the choices of the other agents 
in addition for each agent the sum of the costs over his 
chosen subset of resources equals the payoff of an agent 
choosing the same strategy in the corresponding congestion game 
assume we are given a game g with constant failure 
probabilities let τ α → β → γ → δ → α be an arbitrary 
simple closed path of length let i and j denote the active 
agents deviators in τ and z ∈ σ−{i j} be a fixed 
strategy profile of the other agents let α xi xj z β 
 yi xj z γ yi yj z δ xi yj z where xi yi ∈ σi 
and xj yj ∈ σj then 
u τ ui xi xj z − ui yi xj z 
 uj yi xj z − uj yi yj z 
 ui yi yj z − ui xi yj z 
 uj xi yj z − uj xi xj z 
 
 
 − f xi 
 
vi − 
x 
e∈xi 
c h 
 xi xj z 
e − 
− 
 
 − f xj 
 
vj 
x 
e∈xj 
c h 
 xi xj z 
e 
 
 
 − f xi 
 
vi − − 
 
 − f xj 
 
vj 
 
− 
 x 
e∈xi 
c h 
 xi xj z 
e − − 
x 
e∈xj 
c h 
 xi xj z 
e 
 
 
notice that 
 
 − f xi 
 
vi − − 
 
 − f xj 
 
vj 
 
 as 
a sum of a telescope series the remaining sum equals by 
applying theorem to congestion games which are known 
to possess a potential function thus by theorem g is a 
potential game 
 
we note that the above result holds also for the more 
general settings with non-identical resources having 
different failure probabilities and cost functions and general cost 
functions not necessarily monotone and or nonnegative 
 the existence of a pure strategy nash 
equilibrium 
in the previous section we have shown that cglfs and 
in particular nondecreasing cglfs do not admit a 
potential function but this fact in general does not contradict 
the existence of an equilibrium in pure strategies in this 
section we present and prove the main result of this 
paper theorem which shows the existence of pure strategy 
nash equilibria in nondecreasing cglfs 
theorem every nondecreasing cglf possesses a nash 
equilibrium in pure strategies 
the proof of theorem is based on lemmas and 
 which are presented in the sequel we start with some 
definitions and observations that are needed for their proofs 
in particular we present the notions of a- d- and s-stability 
and show that a strategy profile is in equilibrium if and only 
if it is a- d- and s- stable furthermore we prove the 
existence of such a profile in any given nondecreasing cglf 
definition for any strategy profile σ ∈ σ and for any 
agent i ∈ n the operation of adding precisely one resource 
to his strategy σi is called an a-move of i from σ 
similarly the operation of dropping a single resource is called a 
d-move and the operation of switching one resource with 
another is called an s-move 
clearly if agent i deviates from strategy σi to strategy σi 
by applying a single a- d- or s-move then max { σi σi 
 σi σi } and vice versa if max { σi σi σi σi } 
 then σi is obtained from σi by applying exactly one such 
move for simplicity of exposition for any pair of sets a 
and b let µ a b max { a b b a } 
the following lemma implies that any strategy profile in 
which no agent wishes unilaterally to apply a single a- 
dor s-move is a nash equilibrium more precisely we show 
that if there exists an agent who benefits from a unilateral 
deviation from a given strategy profile then there exists a 
single a- d- or s-move which is profitable for him as well 
lemma given a nondecreasing cglf let σ ∈ σ be a 
strategy profile which is not in equilibrium and let i ∈ n 
such that ∃xi ∈ σi for which ui σ−i xi ui σ then 
there exists yi ∈ σi such that ui σ−i yi ui σ and µ yi σi 
 
therefore to prove the existence of a pure strategy nash 
equilibrium it suffices to look for a strategy profile for which 
no agent wishes to unilaterally apply an a- d- or s-move 
based on the above observation we define a- d- and 
sstability as follows 
definition a strategy profile σ is said to be a-stable 
 resp d-stable s-stable if there are no agents with a 
profitable a- resp d- s- move from σ similarly we 
define a strategy profile σ to be ds-stable if there are no 
agents with a profitable d- or s-move from σ 
the set of all ds-stable strategy profiles is denoted by 
σ 
 obviously the profile ∅ ∅ is ds-stable so σ 
is not empty our goal is to find a ds-stable profile for 
which no profitable a-move exists implying this profile is 
in equilibrium to describe how we achieve this we define 
the notions of light heavy resources and nearly- even 
strategy profiles which play a central role in the proof of 
our main result 
definition given a strategy profile σ resource e is 
called σ-light if hσ 
e ∈ arg mine∈m hσ 
e and σ-heavy otherwise 
a strategy profile σ with no heavy resources will be termed 
even a strategy profile σ satisfying hσ 
e − hσ 
e ≤ for all 
e e ∈ m will be termed nearly-even 
obviously every even strategy profile is nearly-even in 
addition in a nearly-even strategy profile all heavy resources 
 if exist have the same congestion we also observe that the 
profile ∅ ∅ is even and ds-stable so the subset of 
even ds-stable strategy profiles is not empty 
based on the above observations we define two types of 
an a-move that are used in the sequel suppose σ ∈ σ 
is a nearly-even ds-stable strategy profile for each agent 
i ∈ n let ei ∈ arg mine∈m σi hσ 
e that is ei is a 
lightest resource not chosen previously by i then if there 
exists any profitable a-move for agent i then the a-move 
with ei is profitable for i as well this is since if agent i 
wishes to unilaterally add a resource say a ∈ m σi then 
ui σ−i σi ∪ {a} ui σ hence 
 − 
y 
e∈σi 
f hσ 
e f hσ 
a 
 
vi − 
x 
e∈σi 
c hσ 
e − c hσ 
a 
 − 
y 
e∈σi 
f hσ 
e 
 
vi − 
x 
e∈σi 
c hσ 
e 
⇒ vi 
y 
e∈σi 
f hσ 
e 
c hσ 
a 
 − f hσ 
a 
≥ 
c hσ 
ei 
 
 − f hσ 
ei 
 
⇒ ui σ−i σi ∪ {ei} ui σ 
if no agent wishes to change his strategy in this 
manner i e ui σ ≥ ui σ−i σi ∪{ei} for all i ∈ n then by the 
above ui σ ≥ ui σ−i σi ∪{a} for all i ∈ n and a ∈ m σi 
hence σ is a-stable and by lemma σ is a nash 
equilibrium strategy profile otherwise let n σ denote the subset 
of all agents for which there exists ei such that a unilateral 
addition of ei is profitable let a ∈ arg minei i∈n σ hσ 
ei 
 let 
also i ∈ n σ be the agent for which ei a if a is σ-light 
then let σ σ−i σi ∪ {a} in this case we say that σ is 
obtained from σ by a one-step addition of resource a and a 
is called an added resource if a is σ-heavy then there exists 
a σ-light resource b and an agent j such that a ∈ σj and 
b ∈ σj then let σ 
` 
σ−{i j} σi ∪ {a} σj {a} ∪ {b} 
´ 
 
in this case we say that σ is obtained from σ by a two-step 
addition of resource b and b is called an added resource 
we notice that in both cases the congestion of each 
resource in σ is the same as in σ except for the added 
resource for which its congestion in σ increased by thus 
since the added resource is σ-light and σ is nearly-even σ 
is nearly-even then the following lemma implies the 
sstability of σ 
 
lemma in a nondecreasing cglf every nearly-even 
strategy profile is s-stable 
coupled with lemma the following lemma shows that 
if σ is a nearly-even and ds-stable strategy profile and σ is 
obtained from σ by a one- or two-step addition of resource 
a then the only potential cause for a non-ds-stability of σ 
is the existence of an agent k ∈ n with σk σk who wishes 
to drop the added resource a 
lemma let σ be a nearly-even ds-stable strategy 
profile of a given nondecreasing cglf and let σ be obtained 
from σ by a one- or two-step addition of resource a then 
there are no profitable d-moves for any agent i ∈ n with 
σi σi for an agent i ∈ n with σi σi the only possible 
profitable d-move if exists is to drop the added resource a 
we are now ready to prove our main result - theorem 
 let us briefly describe the idea behind the proof by 
lemma it suffices to prove the existence of a strategy 
profile which is a- d- and s-stable we start with the set 
of even and ds-stable strategy profiles which is obviously 
not empty in this set we consider the subset of strategy 
profiles with maximum congestion and maximum sum of the 
agents utilities assuming on the contrary that every 
dsstable profile admits a profitable a-move we show the 
existence of a strategy profile x in the above subset such that a 
 one-step addition of some resource a to x results in a 
dsstable strategy then by a finite series of one- or two-step 
addition operations we obtain an even ds-stable strategy 
profile with strictly higher congestion on the resources 
contradicting the choice of x the full proof is presented below 
proof of theorem let σ 
⊆ σ 
be the subset of 
all even ds-stable strategy profiles observe that since 
 ∅ ∅ is an even ds-stable strategy profile then σ 
is not empty and minσ∈σ 
˛ 
˛{e ∈ m e is σ−heavy} 
˛ 
˛ 
then σ 
could also be defined as 
σ 
 arg min 
σ∈σ 
˛ 
˛{e ∈ m e is σ−heavy} 
˛ 
˛ 
with hσ 
being the common congestion 
now let σ 
⊆ σ 
be the subset of σ 
consisting of all 
those profiles with maximum congestion on the resources 
that is 
σ 
 arg max 
σ∈σ 
hσ 
 
let un σ 
p 
i∈n ui σ denotes the group utility of the 
agents and let σ 
⊆ σ 
be the subset of all profiles in σ 
with maximum group utility that is 
σ 
 arg max 
σ∈σ 
x 
i∈n 
ui σ arg max 
σ∈σ 
un σ 
consider first the simple case in which maxσ∈σ hσ 
 
obviously in this case σ 
 σ 
 σ 
 {x ∅ ∅ } 
we show below that by performing a finite series of 
 onestep addition operations on x we obtain an even 
dsstable strategy profile y with higher congestion that is with 
hy 
 hx 
 in contradiction to x ∈ σ 
 let z ∈ σ 
be 
a nearly-even not necessarily even ds-stable profile such 
that mine∈m hz 
e and note that the profile x satisfies 
the above conditions let n z be the subset of agents for 
which a profitable a-move exists and let i ∈ n z 
obviously there exists a z-light resource a such that ui z−i zi ∪ 
{a} ui z otherwise arg mine∈m hz 
e ⊆ zi in 
contradiction to mine∈m hz 
e consider the strategy profile 
z z−i zi ∪ {a} which is obtained from z by a one-step 
addition of resource a by agent i since z is nearly-even and 
a is z-light we can easily see that z is nearly-even then 
lemma implies that z is s-stable since i is the only agent 
using resource a in z by lemma no profitable d-moves 
are available thus z is a ds-stable strategy profile 
therefore since the number of resources is finite there is a finite 
series of one-step addition operations on x ∅ ∅ that 
leads to strategy profile y ∈ σ 
with hy 
 hx 
 in 
contradiction to x ∈ σ 
 
we turn now to consider the other case where maxσ∈σ hσ 
≥ in this case we select from σ 
a strategy profile x 
as described below and use it to contradict our contrary 
assumption specifically we show that there exists x ∈ σ 
such that for all j ∈ n 
vjf hx 
 xj − 
≥ 
c hx 
 
 − f hx 
 
let x be a strategy profile which is obtained from x by 
a one-step addition of some resource a ∈ m by some 
agent i ∈ n x note that x is nearly-even then 
is derived from and essentially equivalent to the inequality 
uj x ≥ uj x−j xj {a} for all a ∈ xj that is after 
performing an a-move with a by i there is no profitable 
d-move with a then by lemmas and x is ds-stable 
following the same lines as above we construct a procedure 
that initializes at x and achieves a strategy profile y ∈ σ 
with hy 
 hx 
 in contradiction to x ∈ σ 
 
now let us confirm the existence of x ∈ σ 
that 
satisfies let x ∈ σ 
and let m x be the subset of all 
resources for which there exists a profitable one-step 
addition first we show that holds for all j ∈ n such that 
xj ∩m x ∅ that is for all those agents with one of their 
resources being desired by another agent 
let a ∈ m x and let x be the strategy profile that is 
obtained from x by the one-step addition of a by agent i 
assume on the contrary that there is an agent j with a ∈ xj 
such that 
vjf hx 
 xj − 
 
c hx 
 
 − f hx 
 
let x x−j xj {a} below we demonstrate that x 
is a ds-stable strategy profile and since x and x 
correspond to the same congestion vector we conclude that x 
lies in σ 
 in addition we show that un x un x 
contradicting the fact that x ∈ σ 
 
to show that x ∈ σ 
we note that x is an even strategy 
profile and thus no s-moves may be performed for x in 
addition since hx 
 hx 
and x ∈ σ 
 there are no profitable 
d-moves for any agent k i j it remains to show that 
there are no profitable d-moves for agents i and j as well 
 
since ui x ui x we get 
vif hx 
 xi 
 
c hx 
 
 − f hx 
⇒ vif hx 
 xi − 
 vif hx 
 xi 
 
c hx 
 
 − f hx 
 
c hx 
 
 − f hx 
 
c hx 
 
 − f hx 
 
which implies ui x ui x−i xi {b} for all b ∈ xi 
thus there are no profitable d-moves for agent i by the 
ds-stability of x for agent j and for all b ∈ xj we have 
uj x ≥ uj x−j xj {b} ⇒ vjf hx 
 xj − 
≥ 
c hx 
 
 − f hx 
 
then 
vjf hx 
 xj − 
 vjf hx 
 xj 
 vjf hx 
 xj − 
≥ 
c hx 
 
 − f hx 
 
c hx 
 
 − f hx 
⇒ uj x uj x−j xj {b} for all b ∈ xi therefore x 
is ds-stable and lies in σ 
 
to show that un x the group utility of x satisfies 
un x un x we note that hx 
 hx 
 and thus uk x 
uk x for all k ∈ n {i j} therefore we have to show 
that ui x uj x ui x uj x or ui x − ui x 
uj x − uj x observe that 
ui x ui x ⇒ vif hx 
 xi 
 
c hx 
 
 − f hx 
and 
uj x uj x ⇒ vjf hx 
 xj − 
 
c hx 
 
 − f hx 
 
which yields 
vif hx 
 xi 
 vjf hx 
 xj − 
 
thus ui x − ui x 
 
 
 − f hx 
 xi 
 
vi − xi c hx 
 
− 
h 
 − f hx 
 xi 
 
vi − xi c hx 
 
i 
 vif hx 
 xi 
 − f hx 
 − c hx 
 
 vjf hx 
 xj − 
 − f hx 
 − c hx 
 
 
 
 − f hx 
 xj 
 
vj − xj c hx 
 
− 
h 
 − f hx 
 xj − 
 
vj − xi − c hx 
 
i 
 uj x − uj x 
therefore x lies in σ 
and satisfies un x un x in 
contradiction to x ∈ σ 
 
hence if x ∈ σ 
then holds for all j ∈ n such that 
xj ∩m x ∅ now let us see that there exists x ∈ σ 
such 
that holds for all the agents for that choose an agent 
i ∈ arg mink∈n vif hx 
 xk 
 if there exists a ∈ xi ∩ m x 
then i satisfies implying by the choice of agent i that 
the above obviously yields the correctness of for any 
agent k ∈ n otherwise if no resource in xi lies in m x 
then let a ∈ xi and a ∈ m x since a ∈ xi a ∈ xi 
and hx 
a hx 
a then there exists agent j such that a ∈ xj 
and a ∈ xj one can easily check that the strategy 
profile x 
` 
x−{i j} xi {a} ∪ {a } xj {a } ∪ {a} 
´ 
lies 
in σ 
 thus x satisfies for agent i and therefore for 
any agent k ∈ n 
now let x ∈ σ 
satisfy we show below that by 
performing a finite series of one- and two-step addition 
operations on x we can achieve a strategy profile y that lies 
in σ 
 such that hy 
 hx 
 in contradiction to x ∈ σ 
 let 
z ∈ σ 
be a nearly-even not necessarily even ds-stable 
strategy profile such that 
vi 
y 
e∈zi {b} 
f hz 
e ≥ 
c hz 
b 
 − f hz 
b 
 
for all i ∈ n and for all z-light resource b ∈ zi we note that 
for profile x ∈ σ 
⊆ σ 
 with all resources being x-light 
conditions and are equivalent let z be obtained 
from z by a one- or two-step addition of a z-light resource 
a obviously z is nearly-even in addition hz 
e ≥ hz 
e for 
all e ∈ m and mine∈m hz 
e ≥ mine∈m hz 
e to complete the 
proof we need to show that z is ds-stable and in addition 
that if mine∈m hz 
e mine∈m hz 
e then z has property 
the ds-stability of z follows directly from lemmas and 
and from with respect to z it remains to prove property 
 for z with mine∈m hz 
e mine∈m hz 
e using with 
respect to z for any agent k with zk zk and for any 
zlight resource b ∈ zk we get 
vk 
y 
e∈zk 
{b} 
f hz 
e ≥ vk 
y 
e∈zk {b} 
f hz 
e 
≥ 
c hz 
b 
 − f hz 
b 
 
c hz 
b 
 − f hz 
b 
 
as required now let us consider the rest of the agents 
assume z is obtained by the one-step addition of a by agent 
i in this case i is the only agent with zi zi the required 
property for agent i follows directly from ui z ui z in 
the case of a two-step addition let z 
` 
z−{i j} zi ∪ {b} 
 zj {b} ∪ {a} where b is a z-heavy resource for agent 
i from ui z−i zi ∪ {b} ui z we get 
 − 
y 
e∈zi 
f hz 
e f hz 
b 
 
vi − 
x 
e∈zi 
c hz 
e − c hz 
b 
 − 
y 
e∈zi 
f hz 
e 
 
vi − 
x 
e∈zi 
c hz 
e 
⇒ vi 
y 
e∈zi 
f hz 
e 
c hz 
b 
 − f hz 
b 
 
and note that since hz 
b ≥ hz 
e for all e ∈ m and in 
particular for all z -light resources then 
c hz 
b 
 − f hz 
b 
≥ 
c hz 
e 
 − f hz 
e 
 
for any z -light resource e 
 
now since hz 
e ≥ hz 
e for all e ∈ m and b is z-heavy then 
vi 
y 
e∈zi {e } 
f hz 
e ≥ vi 
y 
e∈zi {e } 
f hz 
e 
 vi 
y 
e∈ zi∪{b} {e } 
f hz 
e ≥ vi 
y 
e∈zi 
f hz 
e 
for any z -light resource e the above coupled with 
and yields the required for agent j we just use 
with respect to z and the equality hz 
b hz 
a for any z -light 
resource e 
vj 
y 
e∈zj {e } 
f hz 
e ≥ vi 
y 
e∈zi {e } 
f hz 
e 
≥ 
c hz 
e 
 − f hz 
e 
 
c hz 
e 
 − f hz 
e 
 
thus since the number of resources is finite there is a finite 
series of one- and two-step addition operations on x that 
leads to strategy profile y ∈ σ 
with hy 
 hx 
 in 
contradiction to x ∈ σ 
 this completes the proof 
 discussion 
in this paper we introduce and investigate congestion 
settings with unreliable resources in which the probability of a 
resource s failure depends on the congestion experienced by 
this resource we defined a class of congestion games with 
load-dependent failures cglfs which generalizes the 
wellknown class of congestion games we study the existence of 
pure strategy nash equilibria and potential functions in the 
presented class of games we show that these games do not 
in general possess pure strategy equilibria nevertheless 
if the resource cost functions are nondecreasing then such 
equilibria are guaranteed to exist despite the non-existence 
of a potential function 
the cglf-model can be modified to the case where the 
agents pay only for non-faulty resources they selected both 
the model discussed in this paper and the modified one are 
reasonable in the full version we will show that the 
modified model leads to similar results in particular we can 
show the existence of a pure strategy equilibrium for 
nondecreasing cglfs also in the modified model 
in future research we plan to consider various extensions 
of cglfs in particular we plan to consider cglfs where 
the resources may have different costs and failure 
probabilities as well as cglfs in which the resource failure 
probabilities are mutually dependent in addition it is of 
interest to develop an efficient algorithm for the computation of 
pure strategy nash equilibrium as well as discuss the social 
 in efficiency of the equilibria 
 references 
 h ackermann h r¨oglin and b v¨ocking pure nash 
equilibria in player-specific and weighted congestion 
games in wine- 
 g christodoulou and e koutsoupias the price of 
anarchy of finite congestion games in proceedings of 
the th annual acm symposium on theory and 
computing stoc- 
 a fabrikant c papadimitriou and k talwar the 
complexity of pure nash equilibria in stoc- pages 
 - 
 e koutsoupias and c papadimitriou worst-case 
equilibria in proceedings of the th annual 
symposium on theoretical aspects of computer 
science pages - 
 k leyton-brown and m tennenholtz local-effect 
games in ijcai- 
 i milchtaich congestion games with player-specific 
payoff functions games and economic behavior 
 - 
 d monderer solution-based congestion games 
advances in mathematical economics - 
 
 d monderer multipotential games in ijcai- 
 
 d monderer and l shapley potential games games 
and economic behavior - 
 m penn m polukarov and m tennenholtz 
congestion games with failures in proceedings of the 
 th acm conference on electronic commerce 
 ec- pages - 
 r rosenthal a class of games possessing 
pure-strategy nash equilibria international journal of 
game theory - 
 t roughgarden and e tardos how bad is selfish 
routing journal of the acm - 
 
assured service quality by improved fault management 
service-oriented event correlation 
andreas hanemann 
munich network management 
team 
leibniz supercomputing 
center 
barer str d- 
munich germany 
hanemann lrz de 
martin sailer 
munich network management 
team 
university of munich lmu 
oettingenstr d- 
munich germany 
sailer nm ifi lmu de 
david schmitz 
munich network management 
team 
leibniz supercomputing 
center 
barer str d- 
munich germany 
schmitz lrz de 
abstract 
the paradigm shift from device-oriented to service-oriented 
management has also implications to the area of event 
correlation today s event correlation mainly addresses the 
correlation of events as reported from management tools 
however a correlation of user trouble reports concerning services 
should also be performed this is necessary to improve the 
resolution time and to reduce the effort for keeping the 
service agreements we refer to such a type of correlation as 
service-oriented event correlation the necessity to use this 
kind of event correlation is motivated in the paper 
to introduce service-oriented event correlation for an it 
service provider an appropriate modeling of the correlation 
workflow and of the information is necessary therefore we 
examine the process management frameworks it 
infrastructure library itil and enhanced telecom operations map 
 etom for their contribution to the workflow modeling in 
this area the different kinds of dependencies that we find 
in our general scenario are then used to develop a 
workflow for the service-oriented event correlation the mnm 
service model which is a generic model for it service 
management proposed by the munich network management 
 mnm team is used to derive an appropriate information 
modeling an example scenario the web hosting service 
of the leibniz supercomputing center lrz is used to 
demonstrate the application of service-oriented event 
correlation 
categories and subject descriptors 
c computer systems organization 
computercommunication networks-distributed applications 
general terms 
management performance reliability 
 introduction 
in huge networks a single fault can cause a burst of failure 
events to handle the flood of events and to find the root 
cause of a fault event correlation approaches like rule-based 
reasoning case-based reasoning or the codebook approach 
have been developed the main idea of correlation is to 
condense and structure events to retrieve meaningful 
information until now these approaches address primarily the 
correlation of events as reported from management tools or 
devices therefore we call them device-oriented 
in this paper we define a service as a set of functions 
which are offered by a provider to a customer at a customer 
provider interface the definition of a service is therefore 
more general than the definition of a web service but a 
web service is included in this service definition as 
a consequence the results are applicable for web services 
as well as for other kinds of services a service level 
agreement sla is defined as a contract between customer and 
provider about guaranteed service performance 
as in today s it environments the offering of such services 
with an agreed service quality becomes more and more 
important this change also affects the event correlation it 
has become a necessity for providers to offer such 
guarantees for a differentiation from other providers to avoid sla 
violations it is especially important for service providers to 
identify the root cause of a fault in a very short time or even 
act proactively the latter refers to the case of recognizing 
the influence of a device breakdown on the offered services 
as in this scenario the knowledge about services and their 
slas is used we call it service-oriented it can be addressed 
from two directions 
top-down perspective several customers report a 
problem in a certain time interval are these trouble 
reports correlated how to identify a resource as being 
the problem s root cause 
 
bottom-up perspective a device e g router server 
breaks down which services and especially which 
customers are affected by this fault 
the rest of the paper is organized as follows in section 
 we describe how event correlation is performed today and 
present a selection of the state-of-the-art event correlation 
techniques section describes the motivation for 
serviceoriented event correlation and its benefits after having 
motivated the need for such type of correlation we use two 
well-known it service management models to gain 
requirements for an appropriate workflow modeling and present 
our proposal for it see section in section we present 
our information modeling which is derived from the mnm 
service model an application of the approach for a web 
hosting scenario is performed in section the last section 
concludes the paper and presents future work 
 today s event correlation 
techniques 
in the task of event correlation is defined as a 
conceptual interpretation procedure in the sense that a new 
meaning is assigned to a set of events that happen in a certain 
time interval we can distinguish between three aspects 
for event correlation 
functional aspect the correlation focuses on functions 
which are provided by each network element it is also 
regarded which other functions are used to provide a 
specific function 
topology aspect the correlation takes into account how 
the network elements are connected to each other and 
how they interact 
time aspect when explicitly regarding time constraints 
a start and end time has to be defined for each event 
the correlation can use time relationships between the 
events to perform the correlation this aspect is only 
mentioned in some papers but it has to be treated 
in an event correlation system 
in the event correlation it is also important to distinguish 
between the knowledge acquisition representation and the 
correlation algorithm examples of approaches to 
knowledge acquisition representation are gruschke s dependency 
graphs and ensel s dependency detection by neural 
networks it is also possible to find the dependencies by 
analyzing interactions in addition there is an approach 
to manage service dependencies with xml and to define a 
resource description framework 
to get an overview about device-oriented event correlation 
a selection of several event correlation techniques being used 
for this kind of correlation is presented 
model-based reasoning model-based reasoning mbr 
 represents a system by modeling each of its 
components a model can either represent a physical 
entity or a logical entity e g lan wan domain 
service business process the models for physical 
entities are called functional model while the models 
for all logical entities are called logical model a 
description of each model contains three categories of 
information attributes relations to other models and 
behavior the event correlation is a result of the 
collaboration among models 
as all components of a network are represented with 
their behavior in the model it is possible to perform 
simulations to predict how the whole network will 
behave 
a comparison in showed that a large mbr system 
is not in all cases easy to maintain it can be difficult to 
appropriately model the behavior for all components 
and their interactions correctly and completely 
an example system for mbr is netexpert from 
osi which is a hybrid mbr rbr system in osi 
was acquired by agilent technologies 
rule-based reasoning rule-based reasoning rbr 
 uses a set of rules for event correlation the rules 
have the form conclusion if condition the condition 
uses received events and information about the system 
while the conclusion contains actions which can either 
lead to system changes or use system parameters to 
choose the next rule 
an advantage of the approach is that the rules are 
more or less human-readable and therefore their effect 
is intuitive the correlation has proved to be very fast 
in practice by using the rete algorithm 
in the literature it is claimed that rbr 
systems are classified as relatively inflexible frequent 
changes in the modeled it environment would lead to 
many rule updates these changes would have to be 
performed by experts as no automation has currently 
been established in some systems information about 
the network topology which is needed for the event 
correlation is not used explicitly but is encoded into the 
rules this intransparent usage would make rule 
updates for topology changes quite difficult the system 
brittleness would also be a problem for rbr systems 
it means that the system fails if an unknown case 
occurs because the case cannot be mapped onto similar 
cases the output of rbr systems would also be 
difficult to predict because of unforeseen rule interactions 
in a large rule set according to an rbr system 
is only appropriate if the domain for which it is used 
is small nonchanging and well understood 
the gte impact system is an example of a 
rulebased system it also uses mbr gte has merged 
with bell atlantic in and is now called verizon 
 
codebook approach the codebook approach has 
similarities to rbr but takes a further step and 
encodes the rules into a correlation matrix 
the approach starts using a dependency graph with 
two kinds of nodes for the modeling the first kind 
of nodes are the faults denoted as problems in the 
cited papers which have to be detected while the 
second kind of nodes are observable events symptoms in 
the papers which are caused by the faults or other 
events the dependencies between the nodes are 
denoted as directed edges it is possible to choose weights 
for the edges e g a weight for the probability that 
 
fault event a causes event b another possible 
weighting could indicate time dependencies there are 
several possibilities to reduce the initial graph if e g 
a cyclic dependency of events exists and there are no 
probabilities for the cycles edges all events can be 
treated as one event 
after a final input graph is chosen the graph is 
transformed into a correlation matrix where the columns 
contain the faults and the rows contain the events 
if there is a dependency in the graph the weight of 
the corresponding edge is put into the according 
matrix cell in case no weights are used the matrix cells 
get the values for dependency and otherwise 
afterwards a simplification can be done where events 
which do not help to discriminate faults are deleted 
there is a trade-off between the minimization of the 
matrix and the robustness of the results if the matrix 
is minimized as much as possible some faults can only 
be distinguished by a single event if this event cannot 
be reliably detected the event correlation system 
cannot discriminate between the two faults a measure 
how many event observation errors can be 
compensated by the system is the hamming distance the 
number of rows events that can be deleted from the 
matrix can differ very much depending on the 
relationships 
the codebook approach has the advantage that it uses 
long-term experience with graphs and coding this 
experience is used to minimize the dependency graph 
and to select an optimal group of events with respect 
to processing time and robustness against noise 
a disadvantage of the approach could be that similar 
to rbr frequent changes in the environment make it 
necessary to frequently edit the input graph 
smarts incharge is an example of such a 
correlation system 
case-based reasoning in contrast to other approaches 
case-based reasoning cbr systems do not 
use any knowledge about the system structure the 
knowledge base saves cases with their values for system 
parameters and successful recovery actions for these 
cases the recovery actions are not performed by the 
cbr system in the first place but in most cases by a 
human operator 
if a new case appears the cbr system compares the 
current system parameters with the system 
parameters in prior cases and tries to find a similar one to 
identify such a match it has to be defined for which 
parameters the cases can differ or have to be the same 
if a match is found a learned action can be performed 
automatically or the operator can be informed with a 
recovery proposal 
an advantage of this approach is that the ability to 
learn is an integral part of it which is important for 
rapid changing environments 
there are also difficulties when applying the approach 
 the fields which are used to find a similar case 
and their importance have to be defined appropriately 
if there is a match with a similar case an adaptation 
of the previous solution to the current one has to be 
found 
an example system for cbr is spectrorx from 
cabletron systems the part of cabletron that developed 
spectrorx became an independent software company 
in and is now called aprisma management 
technologies 
in this section four event correlation approaches were 
presented which have evolved into commercial event correlation 
systems the correlation approaches have different focuses 
mbr mainly deals with the knowledge acquisition and 
representation while rbr and the codebook approach 
propose a correlation algorithm the focus of cbr is its ability 
to learn from prior cases 
 motivation of service-oriented 
event correlation 
fig shows a general service scenario upon which we 
will discuss the importance of a service-oriented correlation 
several services like ssh a web hosting service or a video 
conference service are offered by a provider to its customers 
at the customer provider interface a customer can allow 
several users to use a subscribed service the quality and 
cost issues of the subscribed services between a customer 
and a provider are agreed in slas on the provider side 
the services use subservices for their provisioning in case 
of the services mentioned above such subservices are dns 
proxy service and ip service both services and subservices 
depend on resources upon which they are provisioned as 
displayed in the figure a service can depend on more than 
one resource and a resource can be used by one or more 
services 
ssh 
dns 
proxy 
ip 
service dependency resource dependency 
user a 
user b 
user c 
customer sla 
web a 
video conf 
ssh sun 
provider 
video conf 
web 
services 
subservices 
resources 
figure scenario 
to get a common understanding we distinguish between 
different types of events 
resource event we use the term resource event for 
network events and system events a network event refers 
to events like node up down or link up down whereas 
system events refer to events like server down or 
authentication failure 
service event a service event indicates that a service 
does not work properly a trouble ticket which is 
generated from a customer report is a kind of such an 
 
event other service events can be generated by the 
provider of a service if the provider himself detects a 
service malfunction 
in such a scenario the provider may receive service events 
from customers which indicate that ssh web hosting 
service and video conference service are not available when 
referring to the service hierarchy the provider can conclude 
in such a case that all services depend on dns therefore 
it seems more likely that a common resource which is 
necessary for this service does not work properly or is not 
available than to assume three independent service failures in 
contrast to a resource-oriented perspective where all of the 
service events would have to be processed separately the 
service events can be linked together their information can 
be aggregated and processed only once if e g the problem 
is solved one common message to the customers that their 
services are available again is generated and distributed by 
using the list of linked service events this is certainly a 
simplified example however it shows the general principle of 
identifying the common subservices and common resources 
of different services 
it is important to note that the service-oriented 
perspective is needed to integrate service aspects especially qos 
aspects an example of such an aspect is that a fault does not 
lead to a total failure of a service but its qos parameters 
respectively agreed service levels at the customer-provider 
interface might not be met a degradation in service 
quality which is caused by high traffic load on the backbone 
is another example in the resource-oriented perspective it 
would be possible to define events which indicate that there 
is a link usage higher than a threshold but no mechanism 
has currently been established to find out which services are 
affected and whether a qos violation occurs 
to summarize the reasons for the necessity of a 
serviceoriented event correlation are the following 
keeping of slas top-down perspective the time 
interval between the first symptom recognized either 
by provider network management tools or customers 
that a service does not perform properly and the 
verified fault repair needs to be minimized this is 
especially needed with respect to slas as such agreements 
often contain guarantees like a mean time to repair 
effort reduction top-down perspective if several 
user trouble reports are symptoms of the same fault 
fault processing should be performed only once and 
not several times if the fault has been repaired the 
affected customers should be informed about this 
automatically 
impact analysis bottom-up perspective in case of 
a fault in a resource its influence on the associated 
services and affected customers can be determined this 
analysis can be performed for short term when there 
is currently a resource failure or long term e g 
network optimization considerations 
 workflow modeling 
in the following we examine the established it process 
management frameworks it infrastructure library itil 
and enhanced telecom operations map etom the aim is 
find out where event correlation can be found in the process 
structure and how detailed the frameworks currently are 
after that we present our solution for a workflow modeling 
for the service-oriented event correlation 
 it infrastructure library itil 
the british office of government commerce ogc and 
the it service management forum itsmf provide a 
collection of best practices for it processes in the area of 
it service management which is called itil the service 
management is described by modules which are grouped 
into service support set provider internal processes and 
service delivery set processes at the customer-provider 
interface each module describes processes functions roles 
and responsibilities as well as necessary databases and 
interfaces in general itil describes contents processes and 
aims at a high abstraction level and contains no information 
about management architectures and tools 
the fault management is divided into incident 
management process and problem management process 
incident management the incident management 
contains the service desk as interface to customers e g 
receives reports about service problems in case of 
severe errors structured queries are transferred to the 
problem management 
problem management the problem management s 
tasks are to solve problems take care of keeping 
priorities minimize the reoccurrence of problems and to 
provide management information after receiving 
requests from the incident management the problem 
has to be identified and information about necessary 
countermeasures is transferred to the change 
management 
the itil processes describe only what has to be done but 
contain no information how this can be actually performed 
as a consequence event correlation is not part of the 
modeling the itil incidents could be regarded as input for the 
service-oriented event correlation while the output could be 
used as a query to the itil problem management 
 enhanced telecom operations map 
 etom 
the telemanagement forum tmf is an 
international non-profit organization from service providers and 
suppliers in the area of telecommunications services similar 
to itil a process-oriented framework has been developed at 
first but the framework was designed for a narrower focus 
i e the market of information and communications service 
providers a horizontal grouping into processes for 
customer care service development operations network 
systems management and partner supplier is performed the 
vertical grouping fulfillment assurance billing reflects the 
service life cycle 
in the area of fault management three processes have been 
defined along the horizontal process grouping 
problem handling the purpose of this process is to 
receive trouble reports from customers and to solve them 
by using the service problem management the aim 
is also to keep the customer informed about the 
current status of the trouble report processing as well as 
about the general network status e g planned 
maintenance it is also a task of this process to inform the 
 
qos sla management about the impact of current 
errors on the slas 
service problem management in this process reports 
about customer-affecting service failures are received 
and transformed their root causes are identified and 
a problem solution or a temporary workaround is 
established the task of the diagnose problem 
subprocess is to find the root cause of the problem by 
performing appropriate tests nothing is said how this 
can be done e g no event correlation is mentioned 
resource trouble management a subprocess of the 
resource trouble management is responsible for 
resource failure event analysis alarm correlation 
filtering and failure event detection reporting 
another subprocess is used to execute different tests to 
find a resource failure there is also another 
subprocess which keeps track about the status of the trouble 
report processing this subprocess is similar to the 
functionality of a trouble ticket system 
the process description in etom is not very detailed it 
is useful to have a check list which aspects for these processes 
have to be taken into account but there is no detailed 
modeling of the relationships and no methodology for applying 
the framework event correlation is only mentioned in the 
resource management but it is not used in the service level 
 workflow modeling for the 
service-oriented event correlation 
fig shows a general service scenario which we will use 
as basis for the workflow modeling for the service-oriented 
event correlation we assume that the dependencies are 
already known e g by using the approaches mentioned 
in section the provider offers different services which 
depend on other services called subservices service 
dependency another kind of dependency exists between 
services subservices and resources these dependencies are 
called resource dependencies these two kinds of 
dependencies are in most cases not used for the event correlation 
performed today this resource-oriented event correlation 
deals only with relationships on the resource level e g 
network topology 
service dependency 
resources 
subservices 
provider 
services 
resource dependency 
figure different kinds of dependencies for the 
service-oriented event correlation 
the dependencies depicted in figure reflect a situation 
with no redundancy in the service provisioning the 
relationships can be seen as and relationships in case of 
redundancy if e g a provider has independent web servers 
another modeling see figure should be used or 
relationship in such a case different relationships are possible 
the service could be seen as working properly if one of the 
servers is working or a certain percentage of them is working 
services 
 and relationship b or relationship 
resources 
figure modeling of no redundancy a and of 
redundancy b 
as both itil and etom contain no description how event 
correlation and especially service-oriented event correlation 
should actually be performed we propose the following 
design for such a workflow see fig the additional 
components which are not part of a device-oriented event 
correlation are depicted with a gray background the workflow is 
divided into the phases fault detection fault diagnosis and 
fault recovery 
in the fault detection phase resource events and service 
events can be generated from different sources the 
resource events are issued during the use of a resource e g 
via snmp traps the service events are originated from 
customer trouble reports which are reported via the customer 
service management see below access point in addition 
to these two passive ways to get the events a provider 
can also perform active tests these tests can either deal 
with the resources resource active probing or can assume 
the role of a virtual customer and test a service or one of its 
subservices by performing interactions at the service access 
points service active probing 
the fault diagnosis phase is composed of three event 
correlation steps the first one is performed by the resource 
event correlator which can be regarded as the event 
correlator in today s commercial systems therefore it deals only 
with resource events the service event correlator does a 
correlation of the service events while the aggregate event 
correlator finally performs a correlation of both resource and 
service events if the correlation result in one of the 
correlation steps shall be improved it is possible to go back to 
the fault detection phase and start the active probing to get 
additional events these events can be helpful to confirm a 
correlation result or to reduce the list of possible root causes 
after the event correlation an ordered list of possible root 
causes is checked by the resource management when the 
root cause is found the failure repair starts this last step 
is performed in the fault recovery phase 
the next subsections present different elements of the 
event correlation process 
 customer service management and 
intelligent assistant 
the customer service management csm access point 
was proposed by as a single interface between customer 
 
fault 
detection 
fault 
diagnosis 
resource 
active 
probing 
resource event 
resource 
event 
correlator 
resource 
management 
candidate 
list 
fault 
recovery 
resource 
usage 
service 
active 
probing 
intelligent 
assistant 
service 
event 
correlator 
aggregate 
event 
correlator 
service eventcsm ap 
figure event correlation workflow 
and provider its functionality is to provide information 
to the customer about his subscribed services e g reports 
about the fulfillment of agreed slas it can also be used to 
subscribe services or to allow the customer to manage his 
services in a restricted way reports about problems with a 
service can be sent to the customer via csm the csm is 
also contained in the mnm service model see section 
to reduce the effort for the provider s first level support 
an intelligent assistant can be added to the csm the 
intelligent assistant structures the customer s information about 
a service problem the information which is needed for a 
preclassification of the problem is gathered from a list of 
questions to the customer the list is not static as the 
current question depends on the answers to prior questions or 
from the result of specific tests a decision tree is used 
to structure the questions and tests the tests allow the 
customer to gain a controlled access to the provider s 
management at the lrz a customer of the e-mail service can 
e g use the intelligent assistant to perform ping requests 
to the mail server but also more complex requests could be 
possible e g requests of a combination of snmp variables 
 active probing 
active probing is useful for the provider to check his 
offered services the aim is to identify and react to problems 
before a customer notices them the probing can be done 
from a customer point of view or by testing the resources 
which are part of the services it can also be useful to 
perform tests of subservices own subservices or subservices 
offered by suppliers 
different schedules are possible to perform the active 
probing the provider could select to test important services 
and resources in regular time intervals other tests could 
be initiated by a user who traverses the decision tree of the 
intelligent assistant including active tests another 
possibility for the use of active probing is a request from the event 
correlator if the current correlation result needs to be 
improved the results of active probing are reported via service 
or resource events to the event correlator or if the test was 
demanded by the intelligent assistant the result is reported 
to it too while the events that are received from 
management tools and customers denote negative events something 
does not work the events from active probing should also 
contain positive events for a better discrimination 
 event correlator 
the event correlation should not be performed by a single 
event correlator but by using different steps the reason 
for this are the different characteristics of the dependencies 
 see fig 
on the resource level there are only relationships between 
resources network topology systems configuration an 
example for this could be a switch linking separate lans if 
the switch is down events are reported that other network 
components which are located behind the switch are also not 
reachable when correlating these events it can be figured 
out that the switch is the likely error cause at this stage 
the integration of service events does not seem to be helpful 
the result of this step is a list of resources which could be 
the problem s root cause the resource event correlator is 
used to perform this step 
in the service-oriented scenario there are also service and 
resource dependencies as next step in the event 
correlation process the service events should be correlated with 
each other using the service dependencies because the 
service dependencies have no direct relationship to the resource 
level the result of this step which is performed by the 
service event correlator is a list of services subservices which 
could contain a failure in a resource if e g there are 
service events from customers that two services do not work 
and both services depend on a common subservice it seems 
more likely that the resource failure can be found inside the 
subservice the output of this correlation is a list of 
services subservices which could be affected by a failure in an 
associated resource 
in the last step the aggregate event correlator matches 
the lists from resource event correlator and service event 
correlator to find the problem s possible root cause this is 
done by using the resource dependencies 
the event correlation techniques presented in section 
could be used to perform the correlation inside the three 
event correlators if the dependencies can be found precisely 
an rbr or codebook approach seems to be appropriate a 
case database cbr could be used if there are cases which 
could not be covered by rbr or the codebook approach 
these cases could then be used to improve the modeling in 
a way that rbr or the codebook approach can deal with 
them in future correlations 
 information modeling 
in this section we use a generic model for it service 
management to derive the information necessary for the event 
correlation process 
 mnm service model 
the mnm service model is a generic model for it 
service management a distinction is made between customer 
side and provider side the customer side contains the 
basic roles customer and user while the provider side contains 
the role provider the provider makes the service available 
to the customer side the service as a whole is divided into 
usage which is accessed by the role user and management 
which is used by the role customer 
the model consists of two main views the service view 
 see fig shows a common perspective of the service for 
customer and provider everything that is only important 
 
for the service realization is not contained in this view for 
these details another perspective the realization view is 
defined see fig 
customer domain 
supplies supplies 
provider domain 
 role 
provider 
accesses uses concludes accesses 
implements observesrealizes 
provides directs 
substantiates 
usesuses 
manages 
implementsrealizes 
manages 
service 
concludes 
qos 
parameters 
usage 
functionality 
service 
access point 
management 
functionality 
service implementation service management implementation 
service 
agreement 
customersideprovidersidesideindependent 
 role 
user 
 role 
customer 
csm 
access point 
service 
client 
csm 
client 
figure service view 
the service view contains the service for which the 
functionality is defined for usage as well as for management there 
are two access points service access point and csm access 
point where user and customer can access the usage and 
management functionality respectively associated to each 
service is a list of qos parameters which have to be met by 
the service at the service access point the qos surveillance 
is performed by the management 
provider domain 
implements observesrealizes 
provides directs 
implementsrealizes 
accesses uses concludes accesses 
usesuses 
manages 
side independent 
side independent 
manages 
manages 
concludes 
acts as 
service implementation service management implementation 
manages 
uses 
acts as 
service 
logic 
sub-service 
client 
service 
client 
csm 
client 
uses 
resources 
usesuses 
service 
management logic 
sub-service 
management client 
basic 
management functionality 
 role 
customer 
 role 
provider 
 role 
user 
figure realization view 
in the realization view the service implementation and the 
service management implementation are described in detail 
for both there are provider-internal resources and 
subservices for the service implementation a service logic uses 
internal resources devices knowledge staff and external 
subservices to provide the service analogous the service 
management implementation includes a service management 
logic using basic management functionalities and external 
management subservices 
the mnm service model can be used for a similar 
modeling of the used subservices i e the model can be applied 
recursively 
as the service-oriented event correlation has to use 
dependencies of a service from subservices and resources the 
model is used in the following to derive the needed 
information for service events 
 information modeling for service events 
today s event correlation deals mainly with events which 
are originated from resources beside a resource identifier 
these events contain information about the resource status 
e g snmp variables to perform a service-oriented event 
correlation it is necessary to define events which are related 
to services these events can be generated from the 
provider s own service surveillance or from customer reports 
at the csm interface they contain information about the 
problems with the agreed qos in our information 
modeling we define an event superclass which contains common 
attributes e g time stamp resource event and service 
event inherit from this superclass 
derived from the mnm service model we define the 
information necessary for a service event 
service as a service event shall represent the problems of 
a single service a unique identification of the affected 
service is contained here 
event description this field has to contain a description 
of the problem depending on the interactions at the 
service access point service view a classification of 
the problem into different categories should be defined 
it should also be possible to add an informal 
description of the problem 
qos parameters for each service qos parameters 
 service view are defined between the provider and the 
customer this field represents a list of these qos 
parameters and agreed service levels the list can help 
the provider to set the priority of a problem with 
respect to the service levels agreed 
resource list this list contains the resources realization 
view which are needed to provide the service this 
list is used by the provider to check if one of these 
resources causes the problem 
subservice service event identification in the service 
hierarchy realization view the service for which this 
service event has been issued may depend on 
subservices if there is a suspicion that one of these 
subservices causes the problem child service events are 
issued from this service event for the subservices in 
such a case this field contains links to the 
corresponding events 
other event identifications in the event correlation 
process the service event can be correlated with other 
service events or with resource events this field then 
contains links to other events which have been 
correlated to this service event this is useful to e g send a 
common message to all affected customers when their 
subscribed services are available again 
issuer s identification this field can either contain an 
identification of the customer who reported the 
problem an identification of a service provider s employee 
 
 in case the failure has been detected by the provider s 
own service active probing or a link to a parent 
service event the identification is needed if there are 
ambiguities in the service event or the issuer should 
be informed e g that the service is available again 
the possible issuers refer to the basic roles customer 
provider in the service model 
assignee to keep track of the processing the name and 
address of the provider s employee who is solving or 
solved the problem is also noted this is a 
specialization of the provider role in the service model 
dates this field contains key dates in the processing of the 
service event such as initial date problem 
identification date resolution date these dates are important 
to keep track how quick the problems have been solved 
status this field represents the service event s actual 
status e g active suspended solved 
priority the priority shows which importance the service 
event has from the provider s perspective the 
importance is derived from the service agreement especially 
the agreed qos parameters service view 
the fields date status and other service events are not 
derived directly from the service model but are necessary 
for the event correlation process 
 application of 
service-oriented event correlation for a 
web hosting scenario 
the leibniz supercomputing center is the joint 
computing center for the munich universities and research 
institutions it also runs the munich scientific network and offers 
related services one of these services is the virtual www 
server a web hosting offer for smaller research institutions 
it currently has approximately customers 
a subservice of the virtual www server is the 
storage service which stores the static and dynamic web pages 
and uses caching techniques for a fast access other 
subservices are dns and ip service when a user accesses a 
hosted web site via one of the lrz s virtual private 
networks the vpn service is also used the resources of the 
virtual www server include a load balancer and 
redundant servers the network connections are also part of the 
resources as well as the apache web server application 
running on the servers figure shows the dependencies of the 
virtual www server 
 customer service management and 
intelligent assistant 
the intelligent assistant that is available at the leibniz 
supercomputing center can currently be used for 
connectivity or performance problems or problems with the lrz 
e-mail service a selection of possible customer problem 
reports for the virtual www server is given in the 
following 
 the hosted web site is not reachable 
 the web site access is too slow 
 the web site contains outdated content 
server 
serverserver 
server 
server server 
server 
server 
server 
outgoing 
connection 
hosting of lrz s 
own pages 
content 
caching 
server 
emergency 
server 
webmail 
server dynamic 
web pages 
static 
web pages 
dns proxyip storage 
resources 
services 
virtual www server 
five redundant servers 
afs 
nfs 
dbload balancer 
figure dependencies of the virtual www 
server 
 the transfer of new content to the lrz does not 
change the provided content 
 the web site looks strange e g caused by problems 
with html version 
this customer reports have to be mapped onto failures 
in resources for e g an unreachable web site different 
root causes are possible like a dns problem connectivity 
problem wrong configuration of the load balancer 
 active probing 
in general active probing can be used for services or 
resources for the service active probing of the virtual www 
server a virtual customer could be installed this customer 
does typical http requests of web sites and compares the 
answer with the known content to check the up-to-dateness 
of a test web site the content could contain a time stamp 
the service active probing could also include the testing of 
subservices e g sending requests to the dns 
the resource active probing performs tests of the resources 
examples are connectivity tests requests to application 
processes and tests of available disk space 
 event correlation for the virtual www 
server 
figure shows the example processing at first a 
customer who takes a look at his hosted web site reports that 
the content that he had changed is not displayed correctly 
this report is transferred to the service management via 
the csm interface an intelligent assistant could be used 
to structure the customer report the service management 
translates the customer report into a service event 
independent from the customer report the service 
provider s own service active probing tries to change the content 
of a test web site because this is not possible a service 
event is issued 
meanwhile a resource event has been reported to the 
event correlator because an access of the content caching 
server to one of the www servers failed as there are no 
other events at the moment the resource event correlation 
 
customer csm 
service 
mgmt 
event 
correlator 
resource 
mgmt 
customer reports 
 web site content 
not up−to−date 
service active 
probing reports 
 web site content 
change not 
possible 
event 
 retrieval of server 
content failed event forward 
resource 
event 
correlation 
service 
event 
correlation 
aggregate 
event 
correlation 
link failure 
report 
event forward 
check www server 
check link 
result display 
link repair 
result display 
result forward 
customer report 
figure example processing of a customer report 
cannot correlate this event to other events at this stage 
it would be possible that the event correlator asks the 
resource management to perform an active probing of related 
resources 
both service events are now transferred to the service 
event correlator and are correlated from the correlation 
of these events it seems likely that either the www server 
itself or the link to the www server is the problem s root 
cause a wrong web site update procedure inside the 
content caching server seems to be less likely as this would only 
explain the customer report and not the service active 
probing result at this stage a service active probing could be 
started but this does not seem to be useful as this 
correlation only deals with the web hosting service and its 
resources and not with other services 
after the separate correlation of both resource and service 
events which can be performed in parallel the aggregate 
event correlator is used to correlate both types of events 
the additional resource event makes it seem much more 
likely that the problems are caused by a broken link to the 
www server or by the www server itself and not by the 
content caching server in this case the event correlator asks 
the resource management to check the link and the www 
server the decision between these two likely error causes 
can not be further automated here 
later the resource management finds out that a broken 
link is the failure s root cause it informs the event correlator 
about this and it can be determined that this explains all 
previous events therefore the event correlation can be 
stopped at this point 
depending on the provider s customer relationship 
management the finding of the root cause and an expected repair 
time could be reported to the customers after the link has 
been repaired it is possible to report this event via the csm 
interface 
even though many details of this event correlation process 
could also be performed differently the example showed an 
important advantage of the service-oriented event 
correlation the relationship between the service provisioning and 
the provider s resources is explicitly modeled this allows a 
mapping of the customer report onto the provider-internal 
resources 
 event correlation for different services 
if a provider like the lrz offers several services the 
serviceoriented event correlation can be used to reveal relationships 
that are not obvious in the first place if the lrz e-mail 
service and its events are viewed in relationship with the 
events for the virtual www server it is possible to identify 
failures in common subservices and resources both services 
depend on the dns which means that customer reports like 
i cannot retrieve new e-mail and the web site of my 
research institute is not available can have a common cause 
e g the dns does not work properly 
 conclusion and future work 
in our paper we showed the need for a service-oriented 
event correlation for an it service provider this new kind 
of event correlation makes it possible to automatically map 
problems with the current service quality onto resource 
failures this helps to find the failure s root cause earlier and 
to reduce costs for sla violations in addition customer 
reports can be linked together and therefore the processing 
effort can be reduced 
to receive these benefits we presented our approach for 
performing the service-oriented event correlation as well as 
a modeling of the necessary correlation information in the 
future we are going to apply our workflow and information 
modeling for services offered by the leibniz supercomputing 
center going further into details 
several issues have not been treated in detail so far e g 
the consequences for the service-oriented event correlation if 
a subservice is offered by another provider if a service does 
not perform properly it has to be determined whether this 
is caused by the provider himself or by the subservice in 
the latter case appropriate information has to be exchanged 
between the providers via the csm interface another issue 
is the use of active probing in the event correlation process 
which can improve the result but can also lead to a 
correlation delay 
another important point is the precise definition of 
dependency which has also been left out by many other 
publications to avoid having to much dependencies in a certain 
situation one could try to check whether the dependencies 
currently exist in case of a download from a web site there 
is only a dependency from the dns subservice at the 
beginning but after the address is resolved a download 
failure is unlikely to have been caused by the dns another 
possibility to reduce the dependencies is to divide a service 
into its possible user interactions e g an e-mail service into 
transactions like get mail sent mail etc and to define the 
dependencies for each user interaction 
acknowledgments 
the authors wish to thank the members of the munich 
network management mnm team for helpful discussions and 
valuable comments on previous versions of the paper the 
mnm team directed by prof dr heinz-gerd hegering is a 
 
group of researchers of the munich universities and the 
leibniz supercomputing center of the bavarian academy of 
sciences its web server is located at wwwmnmteam informatik 
uni-muenchen de 
 references 
 k appleby g goldszmidt and m steinder 
yemanja - a layered event correlation engine for 
multi-domain server farms in proceedings of the 
seventh ifip ieee international symposium on 
integrated network management pages - 
ifip ieee may 
 spectrum aprisma corporation 
http www aprisma com 
 c ensel new approach for automated generation of 
service dependency models in network management 
as a strategy for evolution and development second 
latin american network operation and management 
symposium lanoms ieee august 
 c ensel and a keller an approach for managing 
service dependencies with xml and the resource 
description framework journal of network and 
systems management june 
 m garschhammer r hauck h -g hegering 
b kempter m langer m nerb i radisic 
h roelle and h schmidt towards generic service 
management concepts - a service model based 
approach in proceedings of the seventh ifip ieee 
international symposium on integrated network 
management pages - ifip ieee may 
 b gruschke integrated event management event 
correlation using dependency graphs in proceedings 
of the th ifip ieee international workshop on 
distributed systems operations management 
 dsom ieee ifip october 
 m gupta a neogi m agarwal and g kar 
discovering dynamic dependencies in enterprise 
environments for problem determination in 
proceedings of the th ifip ieee workshop on 
distributed sytems operations and management 
ifip ieee october 
 h -g hegering s abeck and b neumair integrated 
management of networked systems - concepts 
architectures and their operational application 
morgan kaufmann publishers 
 it infrastructure library office of government 
commerce and it service management forum 
http www itil co uk 
 g jakobson and m weissman alarm correlation 
ieee network november 
 g jakobson and m weissman real-time 
telecommunication network management extending 
event correlation with temporal constraints in 
proceedings of the fourth ieee ifip international 
symposium on integrated network management pages 
 - ieee ifip may 
 s kliger s yemini y yemini d ohsie and 
s stolfo a coding approach to event correlation in 
proceedings of the fourth ifip ieee international 
symposium on integrated network management pages 
 - ifip ieee may 
 m langer s loidl and m nerb customer service 
management a more transparent view to your 
subscribed services in proceedings of the th 
ifip ieee international workshop on distributed 
systems operations management dsom 
newark de usa october 
 l lewis a case-based reasoning approach for the 
resolution of faults in communication networks in 
proceedings of the third ifip ieee international 
symposium on integrated network management 
ifip ieee 
 l lewis service level management for enterprise 
networks artech house inc 
 netexpert agilent technologies 
http www agilent com comms oss 
 incharge smarts corporation 
http www smarts com 
 enhanced telecom operations map telemanagement 
forum http www tmforum org 
 verizon communications http www verizon com 
 h wietgrefe k -d tuchs k jobmann g carls 
p froelich w nejdl and s steinfeld using neural 
networks for alarm correlation in cellular phone 
networks in international workshop on applications 
of neural networks to telecommunications 
 iwannt may 
 s yemini s kliger e mozes y yemini and 
d ohsie high speed and robust event correlation 
ieee communiations magazine may 
 
network monitors and contracting systems 
competition and innovation 
paul laskowski john chuang 
uc berkeley 
{paul chuang} sims berkeley edu 
abstract 
today s internet industry suffers from several well-known 
pathologies but none is as destructive in the long term as its 
resistance to evolution rather than introducing new services isps 
are presently moving towards greater commoditization it is 
apparent that the network s primitive system of contracts does not 
align incentives properly in this study we identify the network s 
lack of accountability as a fundamental obstacle to correcting this 
problem employing an economic model we argue that optimal 
routes and innovation are impossible unless new monitoring 
capability is introduced and incorporated with the contracting 
system furthermore we derive the minimum requirements a 
monitoring system must meet to support first-best routing and 
innovation characteristics our work does not constitute a new 
protocol rather we provide practical and specific guidance for the 
design of monitoring systems as well as a theoretical framework to 
explore the factors that influence innovation 
categories and subject descriptors 
c computer-communication networks distributed 
systems j social and behavioral sciences economics 
general terms 
economics theory measurement design legal aspects 
 introduction 
many studies before us have noted the internet s resistance to new 
services and evolution in recent decades numerous ideas have 
been developed in universities implemented in code and even 
written into the routers and end systems of the network only to 
languish as network operators fail to turn them on on a large scale 
the list includes multicast ipv intserv and diffserv lacking 
the incentives just to activate services there seems to be little hope 
of isps devoting adequate resources to developing new ideas in the 
long term this pathology stands out as a critical obstacle to the 
network s continued success ratnasamy shenker and mccanne 
provide extensive discussion in 
on a smaller time scale isps shun new services in favor of cost 
cutting measures thus the network has characteristics of a 
commodity market although in theory isps have a plethora of 
routing policies at their disposal the prevailing strategy is to route in 
the cheapest way possible on one hand this leads directly to 
suboptimal routing more importantly commoditization in the short 
term is surely related to the lack of innovation in the long term 
when the routing decisions of others ignore quality characteristics 
isps are motivated only to lower costs there is simply no reward 
for introducing new services or investing in quality improvements 
in response to these pathologies and others researchers have put 
forth various proposals for improving the situation these can be 
divided according to three high-level strategies the first attempts 
to improve the status quo by empowering end-users clark et al 
suggest that giving end-users control over routing would lead to 
greater service diversity recognizing that some payment mechanism 
must also be provided ratnasamy shenker and mccanne 
postulate a link between network evolution and user-directed 
routing they propose a system of anycast to give end-users 
the ability to tunnel their packets to an isp that introduces a 
desirable protocol the extra traffic to the isp the authors suggest 
will motivate the initial investment 
the second strategy suggests a revision of the contracting system 
this is exemplified by mackie-mason and varian who propose a 
smart market to control access to network resources prices 
are set to the market-clearing level based on bids that users associate 
to their traffic in another direction afergan and wroclawski 
suggest that prices should be explicitly encoded in the routing 
protocols they argue that such a move would improve stability 
and align incentives 
the third high-level strategy calls for greater network 
accountability in this vein argyraki et al propose a system of 
packet obituaries to provide feedback as to which isps drop packets 
 they argue that such feedback would help reveal which isps 
were adequately meeting their contractual obligations unlike the 
first two strategies we are not aware of any previous studies that 
have connected accountability with the pathologies of 
commoditization or lack of innovation 
it is clear that these three strategies are closely linked to each other 
 for example and each argue that giving end-users 
routing control within the current contracting system is 
problematic until today however the relationship between them 
has been poorly understood there is currently little theoretical 
foundation to compare the relative merits of each proposal and a 
particular lack of evidence linking accountability with innovation 
and service differentiation this paper will address both issues 
we will begin by introducing an economic network model that 
relates accountability contracts competition and innovation our 
model is highly stylized and may be considered preliminary it is 
based on a single source sending data to a single destination 
nevertheless the structure is rich enough to expose previously 
unseen features of network behavior we will use our model for 
two main purposes 
first we will use our model to argue that the lack of accountability 
in today s network is a fundamental obstacle to overcoming the 
pathologies of commoditization and lack of innovation in other 
words unless new monitoring capabilities are introduced and 
integrated with the system of contracts the network cannot achieve 
optimal routing and innovation characteristics this result provides 
motivation for the remainder of the paper in which we explore how 
accountability can be leveraged to overcome these pathologies and 
create a sustainable industry we will approach this problem from a 
clean-slate perspective deriving the level of accountability needed 
to sustain an ideal competitive structure 
when we say that today s internet has poor accountability we mean 
that it reveals little information about the behavior - or misbehavior 
- of isps this well-known trait is largely rooted in the network s 
history in describing the design philosophy behind the internet 
protocols clark lists accountability as the least important among 
seven second level goals accordingly accountability 
received little attention during the network s formative years clark 
relates this to the network s military context and finds that had the 
network been designed for commercial development accountability 
would have been a top priority 
argyraki et al conjecture that applying the principles of layering 
and transparency may have led to the network s lack of 
accountability according to these principles end hosts should 
be informed of network problems only to the extent that they are 
required to adapt they notice when packet drops occur so that they 
can perform congestion control and retransmit packets details of 
where and why drops occur are deliberately concealed 
the network s lack of accountability is highly relevant to a 
discussion of innovation because it constrains the system of 
contracts this is because contracts depend upon external 
institutions to function - the judge in the language of incomplete 
contract theory or simply the legal system ultimately if a judge 
cannot verify that some condition holds she cannot enforce a 
contract based on that condition of course the vast majority of 
contracts never end up in court especially when a judge s ruling is 
easily predicted the parties will typically comply with the contract 
terms on their own volition this would not be possible however 
without the judge acting as a last resort 
an institution to support contracts is typically complex but we 
abstract it as follows we imagine that a contract is an algorithm 
that outputs a payment transfer among a set of isps the parties at 
every time this payment is a function of the past and present 
behaviors of the participants but only those that are verifiable 
hence we imagine that a contract only accepts proofs as inputs 
we will call any process that generates these proofs a contractible 
monitor such a monitor includes metering or sensing devices on 
the physical network but it is a more general concept constructing 
a proof of a particular behavior may require readings from various 
devices distributed among many isps the contractible monitor 
includes whatever distributed algorithmic mechanism is used to 
motivate isps to share this private information 
figure demonstrates how our model of contracts fits together we 
make the assumption that all payments are mediated by contracts 
this means that without contractible monitors that attest to say 
latency payments cannot be conditioned on latency 
figure relationship between monitors and contracts 
with this model we may conclude that the level of accountability in 
today s internet only permits best effort contracts nodes cannot 
condition payments on either quality or path characteristics 
is there anything wrong with best-effort contracts the reader 
might wonder why the internet needs contracts at all after all in 
non-network industries traditional firms invest in research and 
differentiate their products all in the hopes of keeping their 
customers and securing new ones one might believe that such 
market forces apply to isps as well we may adopt this as our null 
hypothesis 
null hypothesis market forces are sufficient to maintain service 
diversity and innovation on a network at least to the same extent 
as they do in traditional markets 
there is a popular intuitive argument that supports this hypothesis 
and it may be summarized as follows 
intuitive argument supporting null hypothesis 
 access providers try to increase their quality to get more 
consumers 
 access providers are themselves customers for second hop 
isps and the second hops will therefore try to provide 
highquality service in order to secure traffic from access 
providers access providers try to select high quality transit 
because that increases their quality 
 the process continues through the network giving every 
isp a competitive reason to increase quality 
we are careful to model our network in continuous time in order to 
capture the essence of this argument we can for example specify 
equilibria in which nodes switch to a new next hop in the event of a 
quality drop 
moreover our model allows us to explore any theoretically possible 
punishments against cheaters including those that are costly for 
end-users to administer by contrast customers in the real world 
rarely respond collectively and often simply seek the best deal 
currently offered these constraints limit their ability to punish 
cheaters 
even with these liberal assumptions however we find that we must 
reject our null hypothesis our model will demonstrate that 
identifying a cheating isp is difficult under low accountability 
limiting the threat of market driven punishment we will define an 
index of commoditization and show that it increases without bound 
as data paths grow long furthermore we will demonstrate a 
framework in which an isp s maximum research investment 
decreases hyperbolically with its distance from the end-user 
network 
behavior 
monitor contract 
proof 
payments 
 
to summarize we argue that the internet s lack of accountability 
must be addressed before the pathologies of commoditization and 
lack of innovation can be resolved this leads us to our next topic 
how can we leverage accountability to overcome these pathologies 
we approach this question from a clean-slate perspective instead 
of focusing on incremental improvements we try to imagine how an 
ideal industry would behave then derive the level of accountability 
needed to meet that objective according to this approach we first 
craft a new equilibrium concept appropriate for network 
competition our concept includes the following requirements 
first we require that punishing isps that cheat is done without 
rerouting the path rerouting is likely to prompt end-users to switch 
providers punishing access providers who administer punishments 
correctly next we require that the equilibrium cannot be 
threatened by a coalition of isps that exchanges illicit side 
payments finally we require that the punishment mechanism that 
enforces contracts does not punish innocent nodes that are not in the 
coalition 
the last requirement is somewhat unconventional from an economic 
perspective but we maintain that it is crucial for any reasonable 
solution although isps provide complementary services when they 
form a data path together they are likely to be horizontal 
competitors as well if innocent nodes may be punished an isp 
may decide to deliberately cheat and draw punishment onto itself 
and its neighbors by cheating the isp may save resources thereby 
ensuring that the punishment is more damaging to the other isps 
which probably compete with the cheater directly for some 
customers in the extreme case the cheater may force the other 
isps out of business thereby gaining a monopoly on some routes 
applying this equilibrium concept we derive the monitors needed 
to maintain innovation and optimize routes the solution is 
surprisingly simple contractible monitors must report the quality of 
the rest of the path from each isp to the destination it turns out 
that this is the correct minimum accountability requirement as 
opposed to either end-to-end monitors or hop-by-hop monitors as 
one might initially suspect 
rest of path monitors can be implemented in various ways they 
may be purely local algorithms that listen for packet echoes 
alternately they can be distributed in nature we describe a way to 
construct a rest of path monitor out of monitors for individual isp 
quality and for the data path this requires a mechanism to 
motivate isps to share their monitor outputs with each other the 
rest of path monitor then includes the component monitors and the 
distributed algorithmic mechanism that ensures that information is 
shared as required this example shows that other types of monitors 
may be useful as building blocks but must be combined to form rest 
of path monitors in order to achieve ideal innovation characteristics 
our study has several practical implications for future protocol 
design we show that new monitors must be implemented and 
integrated with the contracting system before the pathologies of 
commoditization and lack of innovation can be overcome 
moreover we derive exactly what monitors are needed to optimize 
routes and support innovation in addition our results provide 
useful input for clean-slate architectural design and we use several 
novel techniques that we expect will be applicable to a variety of 
future research 
the rest of this paper is organized as follows in section we lay 
out our basic network model in section we present a 
lowaccountability network modeled after today s internet we 
demonstrate how poor monitoring causes commoditization and a 
lack of innovation in section we present verifiable monitors and 
show that proofs even without contracts can improve the status 
quo in section we turn our attention to contractible monitors 
we show that rest of path monitors can support competition games 
with optimal routing and innovation we further show that rest of 
path monitors are required to support such competition games we 
continue by discussing how such monitors may be constructed using 
other monitors as building blocks in section we conclude and 
present several directions for future research 
 basic network model 
a source s wants to send data to destination d s and d are nodes 
on a directed acyclic graph with a finite set of intermediate nodes 
{ }nv representing isps all paths lead to d and every 
node not connected to d has at least two choices for next hop 
we will represent quality by a finite dimensional vector space q 
called the quality space each dimension represents a distinct 
network characteristic that end-users care about for example 
latency loss probability jitter and ip version can each be assigned 
to a dimension 
to each node i we associate a vector in the quality space qqi ∈ 
this corresponds to the quality a user would experience if i were the 
only isp on the data path let n 
q∈q be the vector of all node 
qualities 
of course when data passes through multiple nodes their qualities 
combine in some way to yield a path quality we represent this by 
an associative binary operation qqq →× for path 
 nvvv the quality is given by nvvv qqq ∗∗∗ 
 the 
operation reflects the characteristics of each dimension of quality 
for example can act as an addition in the case of latency 
multiplication in the case of loss probability or a 
minimumargument function in the case of security 
when data flows along a complete path from s to d the source and 
destination generally regarded as a single player enjoy utility given 
by a function of the path quality →qu each node along the 
path i experiences some cost of transmission ci 
 game dynamics 
ultimately we are most interested in policies that promote 
innovation on the network in this study we will use innovation in 
a fairly general sense innovation describes any investment by an 
isp that alters its quality vector so that at least one potential data 
path offers higher utility this includes researching a new routing 
algorithm that decreases the amount of jitter users experience it 
also includes deploying a new protocol that supports quality of 
service even more broadly buying new equipment to decrease 
s d 
 
latency may also be regarded as innovation innovation 
may be thought of as the micro-level process by which 
the network evolves 
our analysis is limited in one crucial respect we focus 
on inventions that a single isp can implement to improve 
the end-user experience this excludes technologies that 
require adoption by all isps on the network to function 
because such technologies do not create a competitive 
advantage rewarding them is difficult and may require 
intellectual property or some other market distortion we 
defer this interesting topic to future work 
at first it may seem unclear how a large-scale distributed process 
such as innovation can be influenced by mechanical details like 
networks monitors our model must draw this connection in a 
realistic fashion 
the rate of innovation depends on the profits that potential 
innovators expect in the future the reward generated by an 
invention must exceed the total cost to develop it or the inventor 
will not rationally invest this reward in turn is governed by the 
competitive environment in which the firm operates including the 
process by which firms select prices and agree upon contracts with 
each other of course these decisions depend on how routes are 
established and how contracts determine actual monetary 
exchanges 
any model of network innovation must therefore relate at least three 
distinct processes innovation competition and routing we select 
a game dynamics that makes the relation between these processes as 
explicit as possible this is represented schematically in figure 
the innovation stage occurs first at time − t in this stage each 
agent decides whether or not to make research investments if she 
chooses not to her quality remains fixed if she makes an 
investment her quality may change in some way it is not 
necessary for us to specify how such changes take place the 
agents choices in this stage determine the vector of qualities q 
common knowledge for the rest of the game 
next at time − t agents participate in the competition stage in 
which contracts are agreed upon in today s industry these 
contracts include prices for transit access and peering agreements 
since access is provided on a best-effort basis a transit agreement 
can simply be represented by its price other contracting systems 
we will explore will require more detail 
finally beginning at t firms participate in the routing stage 
other research has already employed repeated games to study 
routing for example repetition reveals interesting effects 
not visible in a single stage game such as informal collusion to 
elevate prices in we use a game in continuous time in order to 
study such properties for example we will later ask whether a 
player will maintain higher quality than her contracts require in the 
hope of keeping her customer base or attracting future customers 
our dynamics reflect the fact that isps make innovation decisions 
infrequently although real firms have multiple opportunities to 
innovate each opportunity is followed by a substantial length of 
time in which qualities are fixed the decision to invest focuses on 
how the firm s new quality will improve the contracts it can enter 
into hence our model places innovation at the earliest stage 
attempting to capture a single investment decision contracting 
decisions are made on an intermediate time scale thus appearing 
next in the dynamics routing decisions are made very frequently 
mainly to maximize immediate profit flows so they appear in the 
last stage 
because of this ordering our model does not allow firms to route 
strategically to affect future innovation or contracting decisions in 
opposition afergan and wroclawski argue that contracts are formed 
in response to current traffic patterns in a feedback loop 
although we are sympathetic to their observation such an addition 
would make our analysis intractable our model is most realistic 
when contracting decisions are infrequent 
throughout this paper our solution concept will be a subgame 
perfect equilibrium spe an spe is a strategy point that is a nash 
equilibrium when restricted to each subgame three important 
subgames have been labeled in figure the innovation game 
includes all three stages the competition game includes only the 
competition stage and the routing stage the routing game includes 
only the routing stage 
an spe guarantees that players are forward-looking this means 
for example that in the competition stage firms must act rationally 
maximizing their expected profits in the routing stage they cannot 
carry out threats they made in the innovation stage if it lowers their 
expected payoff 
our schematic already suggests that the routing game is crucial for 
promoting innovation to support innovation the competition 
game must somehow reward isps with high quality but that 
means that the routing game must tend to route to nodes with high 
quality if the routing game always selects the lowest-cost routes 
for example innovation will not be supported we will support this 
observation with analysis later 
 the routing game 
the routing game proceeds in continuous time with all players 
discounting by a common factor r the outputs from previous 
stages q and the set of contracts are treated as exogenous 
parameters for this game for each time ≥t each node must 
select a next hop to route data to data flows across the resultant 
path causing utility flow to s and d and a flow cost to the nodes on 
the path as described above payment flows are also created based 
on the contracts in place 
relating our game to the familiar repeated prisoners dilemma 
imagine that we are trying to impose a high quality but costly path 
as we argued loosely above such paths must be sustainable in order 
to support innovation each isp on the path tries to maximize her 
own payment net of costs so she may not want to cooperate with 
our plan rather if she can find a way to save on costs at the 
expense of the high quality we desire she will be tempted to do so 
innovation game competition game routing game 
innovation 
stage 
competition 
stage 
routing 
stagequalities 
 q 
contracts 
 prices 
profits 
t - t - t ∈ 
figure game dynamics 
 
analogously to the prisoners dilemma we will call such a decision 
cheating a little more formally 
cheating refers to any action that an isp can take contrary to 
some target strategy point that we are trying to impose that 
enhances her immediate payoff but compromises the quality of 
the data path 
one type of cheating relates to the data path each node on the path 
has to pay the next node to deliver its traffic if the next node offers 
high quality transit we may expect that a lower quality node will 
offer a lower price each node on the path will be tempted to route 
to a cheaper next hop increasing her immediate profits but 
lowering the path quality we will call this type of action cheating 
in route 
another possibility we can model is that a node finds a way to save 
on its internal forwarding costs at the expense of its own quality 
we will call this cheating internally to distinguish it from cheating 
in route for example a node might drop packets beyond the rate 
required for congestion control in order to throttle back tcp flows 
and thus save on forwarding costs alternately a node 
employing quality of service could give high priority packets a 
lower class of service thus saving on resources and perhaps 
allowing itself to sell more high priority service 
if either cheating in route or cheating internally is profitable the 
specified path will not be an equilibrium we assume that cheating 
can never be caught instantaneously rather a cheater can always 
enjoy the payoff from cheating for some positive time which we 
label t this includes the time for other players to detect and react 
to the cheating if the cheater has a contract which includes a 
customer lock-in period t also includes the time until customers 
are allowed to switch to a new isp as we will see later it is 
socially beneficial to decrease t so such lock-in is detrimental to 
welfare 
 pathologies of a 
lowaccountability network 
in order to motivate an exploration of monitoring systems we begin 
in this section by considering a network with a poor degree of 
accountability modeled after today s internet we will show how 
the lack of monitoring necessarily leads to poor routing and 
diminishes the rate of innovation thus the network s lack of 
accountability is a fundamental obstacle to resolving these 
pathologies 
 accountability in the current internet 
first we reflect on what accountability characteristics the present 
internet has argyraki et al point out that end hosts are given 
minimal information about packet drops users know when 
drops occur but not where they occur nor why dropped packets 
may represent the innocent signaling of congestion or as we 
mentioned above they may be a form of cheating internally the 
problem is similar for other dimensions of quality or in fact more 
acute finding an isp that gives high priority packets a lower class 
of service for example is further complicated by the lack of even 
basic diagnostic tools 
in fact it is similarly difficult to identify an isp that cheats in route 
huston notes that internet traffic flows do not always correspond to 
routing information an isp may hand a packet off to a 
neighbor regardless of what routes that neighbor has advertised 
furthermore blocks of addresses are summarized together for 
distant hosts so a destination may not even be resolvable until 
packets are forwarded closer 
one might argue that diagnostic tools like ping and traceroute can 
identify cheaters unfortunately argyraki et al explain that these 
tools only reveal whether probe packets are echoed not the fate of 
past packets thus for example they are ineffective in detecting 
low-frequency packet drops even more fundamentally a 
sophisticated cheater can always spot diagnostic packets and give 
them special treatment 
as a further complication a cheater may assume different aliases 
for diagnostic packets arriving over different routes as we will see 
below this gives the cheater a significant advantage in escaping 
punishment for bad behavior even if the data path is otherwise 
observable 
 modeling low-accountability 
as the above evidence suggests the current industry allows for very 
little insight into the behavior of the network in this section we 
attempt to capture this lack of accountability in our model we 
begin by defining a monitor our model of the way that players 
receive external information about network behavior 
a monitor is any distributed algorithmic mechanism that runs on 
the network graph and outputs to specific nodes informational 
statements about current or past network behavior 
we assume that all external information about network behavior is 
mediated in this way the accountability properties of the internet 
can be represented by the following monitors 
e e end to end a monitor that informs s d about what the 
total path quality is at any time this is the quality they 
experience 
rop rest of path a monitor that informs each node along the 
data path what the quality is for the rest of the path to the 
destination 
prc packets received a monitor that tells nodes how much 
data they accept from each other so that they can charge by 
volume it is important to note however that this information is 
aggregated over many source-destination pairs hence for the 
sake of realism it cannot be used to monitor what the data path is 
players cannot measure the qualities of other single nodes just the 
rest of the path nodes cannot see the path past the next hop this 
last assumption is stricter than needed for our results the critical 
ingredient is that nodes cannot verify that the path avoids a specific 
hop this holds for example if the path is generally visible except 
nodes can use different aliases for different parents similar results 
also hold if alternate paths always converge after some integer 
number m of hops 
it is important to stress that e e and rop are not the contractible 
monitors we described in the introduction - they do not generate 
proofs thus even though a player observes certain information 
she generally cannot credibly share it with another player for 
example if a node after the first hop starts cheating the first hop 
will detect the sudden drop in quality for the rest of the path but the 
first hop cannot make the source believe this observation - the 
 
source will suspect that the first hop was the cheater and fabricated 
the claim against the rest of the path 
typically e e and rop are envisioned as algorithms that run on a 
single node and listen for packet echoes this is not the only way 
that they could be implemented however an alternate strategy is to 
aggregate quality measurements from multiple points in the 
network these measurements can originate in other monitors 
located at various isps the monitor then includes the component 
monitors as well as whatever mechanisms are in place to motivate 
nodes to share information honestly as needed for example if the 
source has monitors that reveal the qualities of individual nodes 
they could be combined with path information to create an rop 
monitor 
since we know that contracts only accept proofs as input we can 
infer that payments in this environment can only depend on the 
number of packets exchanged between players in other words 
contracts are best-effort for the remainder of this section we will 
assume that contracts are also linear - there is a constant payment 
flow so long as a node accepts data and all conditions of the 
contract are met other more complicated tariffs are also possible 
and are typically used to generate lock-in we believe that our 
parameter t is sufficient to describe lock-in effects and we believe 
that the insights in this section apply equally to any tariffs that are 
bounded so that the routing game remains continuous at infinity 
restricting attention to linear contracts allows us to represent some 
node i s contract by its price pi 
because we further know that nodes cannot observe the path after 
the next hop we can infer that contracts exist only between 
neighboring nodes on the graph we will call this arrangement of 
contracts bilateral when a competition game exclusively uses 
bilateral contracts we will call it a bilateral contract competition 
game 
we first focus on the routing game and ask whether a high quality 
route can be maintained even when a low quality route is cheaper 
recall that this is a requirement in order for nodes to have any 
incentive to innovate if nodes tend to route to low price next hops 
regardless of quality we say that the network is commoditized to 
measure this tendency we define an index of commoditization as 
follows 
for a node on the data path i define its quality premium 
minppd ji − where pj is the flow payment to the next hop in 
equilibrium and pmin is the price of the lowest cost next hop 
definition the index of commoditization ci is the average 
over each node on the data path i of i s flow profit as a fraction 
of i s quality premium ijii dpcp −− 
ci ranges from when each node spends all of its potential profit 
on its quality premium to infinite when a node absorbs positive 
profit but uses the lowest price next hop a high value for ci 
implies that nodes are spending little of their money inflow on 
purchasing high quality for the rest of the path as the next claim 
shows this is exactly what happens as the path grows long 
claim if the only monitors are e e rop and prc ∞→ci 
as ∞→n where n is the number of nodes on the data path 
to show that this is true we first need the following lemma which 
will establish the difficulty of punishing nodes in the network 
first a bit of notation recall that a cheater can benefit from its 
actions for t before other players can react when a node 
cheats it can expect a higher profit flow at least until it is caught 
and other players react perhaps by diverting traffic let node i s 
normal profit flow be iπ and her profit flow during cheating be 
some greater value yi we will call the ratio iiy π the 
temptation to cheat 
lemma if the only monitors are e e rop and prc the 
discounted time −nt 
rt 
e 
 
 needed to punish a cheater increases at 
least as fast as the product of the temptations to cheat along the data 
path 
∏ −− 
≥ 
 
 
pathdataon 
 
t 
rt 
i i 
i 
t 
rt 
e 
y 
e 
n 
π 
 
corollary if nodes share a minimum temptation to cheat π y 
the discounted time needed to punish cheating increases at least 
exponentially in the length of the data path n 
−− 
≥ 
 
 
t 
rt 
nt 
rt 
e 
y 
e 
n 
π 
 
since it is the discounted time that increases exponentially the 
actual time increases faster than exponentially if n is so large that 
tn is undefined the given path cannot be maintained in equilibrium 
proof the proof proceeds by induction on the number of nodes on 
the equilibrium data path n for n there is a single node say i 
by cheating the node earns extra profit − 
− 
 
 
t 
rt 
ii ey π if node i 
is then punished until time t the extra profit must be cancelled out 
by the lost profit between time t and t − 
 
t 
t 
rt 
i eπ a little 
manipulation gives −− 
 
 
 
t 
rt 
i 
i 
t 
rt 
e 
y 
e 
π 
 as required 
for n assume for induction that the claim holds for −n the 
source does not know whether the cheater is the first hop or after 
the first hop because the source does not know the data path after 
the first hop it is unable to punish nodes beyond it if it chooses a 
new first hop it might not affect the rest of the data path because 
of this the source must rely on the first hop to punish cheating 
nodes farther along the path the first hop needs discounted time 
∏ − 
 
hopfirstafter 
t 
rt 
i i 
i e 
y 
π 
 to accomplish this by assumption so 
the source must give the first hop this much discounted time in order 
to punish defectors further down the line and the source will expect 
poor quality during this period 
next the source must be protected against a first hop that cheats 
and pretends that the problem is later in the path the first hop can 
 
do this for the full discounted time ∏ − 
 
hopfirstafter 
t 
rt 
i i 
i e 
y 
π 
 so 
the source must punish the first hop long enough to remove the extra 
profit it can make following the same argument as for n we 
can show that the full discounted time is ∏ − 
 
pathdataon 
t 
rt 
i i 
i e 
y 
π 
 
which completes the proof 
the above lemma and its corollary show that punishing cheaters 
becomes more and more difficult as the data path grows long until 
doing so is impossible to capture some intuition behind this result 
imagine that you are an end user and you notice a sudden drop in 
service quality if your data only travels through your access 
provider you know it is that provider s fault you can therefore 
take your business elsewhere at least for some time this threat 
should motivate your provider to maintain high quality 
suppose on the other hand that your data traverses two providers 
when you complain to your isp he responds yes we know your 
quality went down but it s not our fault it s the next isp give us 
some time to punish them and then normal quality will resume if 
your access provider is telling the truth you will want to listen 
since switching access providers may not even route around the 
actual offender thus you will have to accept lower quality service 
for some longer time on the other hand you may want to punish 
your access provider as well in case he is lying this means you 
have to wait longer to resume normal service as more isps are 
added to the path the time increases in a recursive fashion 
with this lemma in hand we can return to prove claim 
proof of claim fix an equilibrium data path of length n label 
the path nodes   n for each node i let i s quality premium be 
 − iii ppd then we have 
 
 
− 
 
− 
 
 
 
− 
 
 
 
 
− − 
−− 
−− 
 
−− 
− 
 
−− 
 
n 
i 
i 
n 
i iii 
iii 
n 
i iii 
ii 
n 
i i 
iii 
c 
g 
npcp 
pcp 
n 
pcp 
pp 
nd 
pcp 
n 
i 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
where gi is node i s temptation to cheat by routing to the lowest 
price next hop lemma tells us that tg 
n 
i 
i ∏ 
 
 where 
 rt 
et − 
− it requires a bit of calculus to show that ic is 
minimized by setting each gi equal to n 
t 
 however as ∞→n 
we have 
→n 
t which shows that ∞→ci 
according to the claim as the data path grows long it increasingly 
resembles a lowest-price path since lowest-price routing does not 
support innovation we may speculate that innovation degrades with 
the length of the data path though we suspect stronger claims are 
possible we can demonstrate one such result by including an extra 
assumption 
available bargain path a competitive market exists for 
lowcost transit such that every node can route to the destination for 
no more than flow payment lp 
claim under the available bargain path assumption if node i a 
distance n from s can invest to alter its quality and the source will 
spend no more than sp for a route including node i s new quality 
then the payment to node i p decreases hyperbolically with n 
 
 s 
n 
l p 
n 
t 
pp 
 
 
− 
 ≤ 
− 
 
where rt 
et − 
− is the bound on the product of temptations 
from the previous claim thus i will spend no more than 
 
 − 
 
− 
s 
n 
l p 
n 
t 
p 
r 
 
on this quality improvement which 
approaches the bargain path s payment 
r 
pl 
 as ∞→n 
the proof is given in the appendix as a node gets farther from the 
source its maximum payment approaches the bargain price pl 
hence the reward for innovation is bounded by the same amount 
large innovations meaning substantially more expensive than 
rpl will not be pursued deep into the network 
claim can alternately be viewed as a lower bound on how much it 
costs to elicit innovation in a network if the source s wants node i 
to innovate it needs to get a motivating payment p to i during the 
routing stage however it must also pay the nodes on the way to i a 
premium in order to motivate them to route properly the claim 
shows that this premium increases with the distance to i until it 
dwarfs the original payment p 
our claims stand in sharp contrast to our null hypothesis from the 
introduction comparing the intuitive argument that supported our 
hypothesis with these claims we can see that we implicitly used an 
oversimplified model of market pressure as either present or not 
as is now clear market pressure relies on the decisions of 
customers but these are limited by the lack of information hence 
competitive forces degrade as the network deepens 
 verifiable monitors 
in this section we begin to introduce more accountability into the 
network recall that in the previous section we assumed that 
players couldn t convince each other of their private information 
what would happen if they could if a monitor s informational 
signal can be credibly conveyed to others we will call it a verifiable 
monitor the monitor s output in this case can be thought of as a 
statement accompanied by a proof a string that can be processed by 
any player to determine that the statement is true 
a verifiable monitor is a distributed algorithmic mechanism that 
runs on the network graph and outputs to specific nodes proofs 
about current or past network behavior 
along these lines we can imagine verifiable counterparts to e e 
and rop we will label these e ev and ropv with these 
monitors each node observes the quality of the rest of the path and 
can also convince other players of these observations by giving 
them a proof 
 
by adding verifiability to our monitors identifying a single cheater 
is straightforward the cheater is the node that cannot produce 
proof that the rest of path quality decreased this means that the 
negative results of the previous section no longer hold for 
example the following lemma stands in contrast to lemma 
lemma with monitors e ev ropv and prc and provided that 
the node before each potential cheater has an alternate next hop that 
isn t more expensive it is possible to enforce any data path in spe 
so long as the maximum temptation is less than what can be deterred 
in finite time 
− 
≤ 
 
 
max 
 
t 
rt 
er 
y 
π 
 
proof this lemma follows because nodes can share proofs to 
identify who the cheater is only that node must be punished in 
equilibrium and the preceding node does not lose any payoff in 
administering the punishment 
with this lemma in mind it is easy to construct counterexamples to 
claim and claim in this new environment 
unfortunately there are at least four reasons not to be satisfied with 
this improved monitoring system the first and weakest reason is 
that the maximum temptation remains finite causing some 
distortion in routes or payments each node along a route must 
extract some positive profit unless the next hop is also the cheapest 
of course if t is small this effect is minimal 
the second and more serious reason is that we have always given 
our source the ability to commit to any punishment real world 
users are less likely to act collectively and may simply search for 
the best service currently offered since punishment phases are 
generally characterized by a drop in quality real world end-users 
may take this opportunity to shop for a new access provider this 
will make nodes less motivated to administer punishments 
the third reason is that lemma does not apply to cheating by 
coalitions a coalition node may pretend to punish its successor 
but instead enjoy a secret payment from the cheating node 
alternately a node may bribe its successor to cheat if the 
punishment phase is profitable and so forth the required 
discounted time for punishment may increase exponentially in the 
number of coalition members just as in the previous section 
the final reason not to accept this monitoring system is that when a 
cheater is punished the path will often be routed around not just the 
offender but around other nodes as well effectively innocent 
nodes will be punished along with the guilty in our abstract model 
this doesn t cause trouble since the punishment falls off the 
equilibrium path the effects are not so benign in the real world 
when isps lie in sequence along a data path they contribute 
complementary services and their relationship is vertical from the 
perspective of other source-destination pairs however these same 
firms are likely to be horizontal competitors because of this a 
node might deliberately cheat in order to trigger punishment for 
itself and its neighbors by cheating the node will save money to 
some extent so the cheater is likely to emerge from the punishment 
phase better off than the innocent nodes this may give the cheater 
a strategic advantage against its competitors in the extreme the 
cheater may use such a strategy to drive neighbors out of business 
and thereby gain a monopoly on some routes 
 contractible monitors 
at the end of the last section we identified several drawbacks that 
persist in an environment with e ev ropv and prc in this 
section we will show how all of these drawbacks can be overcome 
to do this we will require our third and final category of monitor 
a contractible monitor is simply a verifiable monitor that generates 
proofs that can serve as input to a contract thus contractible is 
jointly a property of the monitor and the institutions that must verify 
its proofs contractibility requires that a court 
 can verify the monitor s proofs 
 can understand what the proofs and contracts represent to 
the extent required to police illegal activity 
 can enforce payments among contracting parties 
understanding the agreements between companies has traditionally 
been a matter of reading contracts on paper this may prove to be a 
harder task in a future network setting contracts may plausibly be 
negotiated by machine be numerous even per-flow and be further 
complicated by the many dimensions of quality 
when a monitor together with institutional infrastructure meets 
these criteria we will label it with a subscript c for contractible 
the reader may recall that this is how we labeled the packets 
received monitor prc which allows isps to form contracts with 
per-packet payments similarly e ec and ropc are contractible 
versions of the monitors we are now familiar with 
at the end of the previous section we argued for some desirable 
properties that we d like our solution to have briefly we would 
like to enforce optimal data paths with an equilibrium concept that 
doesn t rely on re-routing for punishment is coalition proof and 
doesn t punish innocent nodes when a coalition cheats we will call 
such an equilibrium a fixed-route coalition-proof 
protect-theinnocent equilibrium 
as the next claim shows ropc allows us to create a system of 
linear price quality contracts under just such an equilibrium 
claim with ropc for any feasible and consistent assignment of 
rest of path qualities to nodes and any corresponding payment 
schedule that yields non-negative payoffs these qualities can be 
maintained with bilateral contracts in a fixed-route coalition-proof 
protect-the-innocent equilibrium 
proof fix any data path consistent with the given rest of path 
qualities select some monetary punishment p large enough to 
prevent any cheating for time t the discounted total payment from 
the source will work let each node on the path enter into a 
contract with its parent which fixes an arbitrary payment schedule 
so long as the rest of path quality is as prescribed when the parent 
node which has ropc submits a proof that the rest of path quality 
is less than expected the contract awards her an instantaneous 
transfer p from the downstream node such proofs can be 
submitted every t for the previous interval 
suppose now that a coalition c decides to cheat the source 
measures a decrease in quality and according to her contract is 
awarded p from the first hop this means that there is a net outflow 
of p from the isps as a whole suppose that node i is not in c in 
order for the parent node to claim p from i it must submit proof that 
the quality of the path starting at i is not as prescribed this means 
 
that there is a cheater after i hence i would also have detected a 
change in quality so i can claim p from the next node on the path 
thus innocent nodes are not punished the sequence of payments 
must end by the destination so the net outflow of p must come from 
the nodes in c this establishes all necessary conditions of the 
equilibrium 
essentially ropc allows for an implementation of price quality 
contracts building upon this result we can construct competition 
games in which nodes offer various qualities to each other at 
specified prices and can credibly commit to meet these 
performance targets even allowing for coalitions and a desire to 
damage other isps 
example define a stackelberg price-quality competition game 
as follows extend the partial order of nodes induced by the graph 
to any complete ordering such that downstream nodes appear 
before their parents in this order each node selects a contract to 
offer to its parents consisting of a rest of path quality and a linear 
price in the routing game each node selects a next hop at every 
time consistent with its advertised rest of path quality the 
stackelberg price-quality competition game can be implemented in 
our model with ropc monitors by using the strategy in the proof 
above it has the following useful property 
claim the stackelberg price-quality competition game yields 
optimal routes in spe 
the proof is given in the appendix this property is favorable from 
an innovation perspective since firms that invest in high quality will 
tend to fall on the optimal path gaining positive payoff in general 
however investments may be over or under rewarded extra 
conditions may be given under which innovation decisions approach 
perfect efficiency for large innovations we omit the full analysis 
here 
example alternately we can imagine that players report their 
private information to a central authority which then assigns all 
contracts for example contracts could be computed to implement 
the cost-minimizing vcg mechanism proposed by feigenbaum et 
al in with ropc monitors we can adapt this mechanism to 
maximize welfare for node i on the optimal path l the net 
payment must equal essentially its contribution to the welfare of s 
d and the other nodes if l is an optimal path in the graph with i 
removed the profit flow to i is 
 
∈≠∈ 
 −− 
 
 
lj 
j 
ijlj 
jll ccququ 
where lq and lq are the qualities of the two paths here price 
quality contracts ensure that nodes report their qualities honestly 
the incentive structure of the vcg mechanism is what motivates 
nodes to report their costs accurately 
a nice feature of this game is that individual innovation decisions 
are efficient meaning that a node will invest in an innovation 
whenever the investment cost is less than the increased welfare of 
the optimal data path unfortunately the source may end up paying 
more than the utility of the path 
notice that with just e ec a weaker version of claim holds 
bilateral price quality contracts can be maintained in an 
equilibrium that is fixed-route and coalition-proof but not 
protectthe-innocent this is done by writing contracts to punish everyone 
on the path when the end to end quality drops if the path length is 
n the first hop pays np to the source the second hop pays pn − 
to the first and so forth this ensures that every node is punished 
sufficiently to make cheating unprofitable for the reasons we gave 
previously we believe that this solution concept is less than ideal 
since it allows for malicious nodes to deliberately trigger 
punishments for potential competitors 
up to this point we have adopted fixed-route coalition-proof 
protect-the-innocent equilibrium as our desired solution concept 
and shown that ropc monitors are sufficient to create some 
competition games that are desirable in terms of service diversity 
and innovation as the next claim will show rest of path 
monitoring is also necessary to construct such games under our 
solution concept 
before we proceed what does it mean for a game to be desirable 
from the perspective of service diversity and innovation we will 
use a very weak assumption essentially that the game is not fully 
commoditized for any node the claim will hold for this entire class 
of games 
definition a competition game is nowhere-commoditized if for 
each node i not adjacent to d there is some assignment of qualities 
and marginal costs to nodes such that the optimal data path includes 
i and i has a positive temptation to cheat 
in the case of linear contracts it is sufficient to require that ∞ ci 
and that every node make positive profit under some assignment of 
qualities and marginal costs 
strictly speaking ropc monitors are not the only way to construct 
these desirable games to prove the next claim we must broaden 
our notion of rest of path monitoring to include the similar ropc 
monitor which attests to the quality starting at its own node 
through the end of the path compare the two monitors below 
ropc gives a node proof that the path quality from the next node 
to the destination is not correct 
ropc gives a node proof that the path quality from that node to 
the destination is correct 
we present a simplified version of this claim by including an 
assumption that only one node on the path can cheat at a time 
 though conspirators can still exchange side payments we will 
discuss the full version after the proof 
claim assume a set of monitors and a nowhere-commoditized 
bilateral contract competition game that always maintains the 
optimal quality in fixed-route coalition-proof protect-the-innocent 
equilibrium with only one node allowed to cheat at a time then 
for each node i not adjacent to d either i has an ropc monitor or 
i s children each have an ropc monitor 
proof first because of the fixed-route assumption punishments 
must be purely monetary 
next when cheating occurs if the payment does not go to the 
source or destination it may go to another coalition member 
rendering it ineffective thus the source must accept some 
monetary compensation net of its normal flow payment when 
cheating occurs since the source only contracts with the first hop 
it must accept this money from the first hop the source s contract 
must therefore distinguish when the path quality is normal from 
when it is lowered by cheating to do so it can either accept proofs 
 
from the source that the quality is lower than required or it can 
accept proofs from the first hop that the quality is correct these 
nodes will not rationally offer the opposing type of proof 
by definition any monitor that gives the source proof that the path 
quality is wrong is an ropc monitor any monitor that gives the 
first hop proof that the quality is correct is a ropc monitor thus 
at least one of these monitors must exist 
by the protect-the-innocent assumption if cheating occurs but the 
first hop is not a cheater she must be able to claim the same size 
reward from the next isp on the path and thus pass on the 
punishment the first hop s contract with the second must then 
distinguish when cheating occurs after the first hop by argument 
similar to that for the source either the first hop has a ropc 
monitor or the second has a ropc monitor this argument can be 
iterated along the entire path to the penultimate node before d 
since the marginal costs and qualities can be arranged to make any 
path the optimal path these statements must hold for all nodes and 
their children which completes the proof 
the two possibilities for monitor correspond to which node has the 
burden of proof in one case the prior node must prove the 
suboptimal quality to claim its reward in the other the subsequent 
node must prove that the quality was correct to avoid penalty 
because the two monitors are similar it seems likely that they 
require comparable costs to implement if submitting the proofs is 
costly it seems natural that nodes would prefer to use the ropc 
monitor placing the burden of proof on the upstream node 
finally we note that it is straightforward to derive the full version of 
the claim which allows for multiple cheaters the only 
complication is that cheaters can exchange side payments which 
makes any money transfers between them redundant because of 
this we have to further generalize our rest of path monitors so they 
are less constrained in the case that there are cheaters on either side 
 implementing monitors 
claim should not be interpreted as a statement that each node must 
compute the rest of path quality locally without input from other 
nodes other monitors besides ropc and ropc can still be used 
loosely speaking as building blocks for instance network 
tomography is concerned with measuring properties of the network 
interior with tools located at the edge using such techniques our 
source might learn both individual node qualities and the data path 
this is represented by the following two monitors 
shopc 
i 
 source-based hop quality a monitor that gives the 
source proof of what the quality of node i is 
spathc source-based path a monitor that gives the source 
proof of what the data path is at any time at least as far as it 
matches the equilibrium path 
with these monitors a punishment mechanism can be designed to 
fulfill the conditions of claim it involves the source sharing the 
proofs it generates with nodes further down the path which use 
them to determine bilateral payments ultimately however the 
proof of claim shows us that each node i s bilateral contracts 
require proof of the rest of path quality this means that node i or 
possibly its children will have to combine the proofs that they 
receive to generate a proof of the rest of path quality thus the 
combined process is itself a rest of path monitor 
what we have done all in all is constructed a rest of path monitor 
using spathc and shopc 
i 
as building blocks our new monitor 
includes both the component monitors and whatever distributed 
algorithmic mechanism exists to make sure nodes share their proofs 
correctly 
this mechanism can potentially involve external institutions for a 
concrete example suppose that when node i suspects it is getting 
poor rest of path quality from its successor it takes the downstream 
node to court during the discovery process the court subpoenas 
proofs of the path and of node qualities from the source ultimately 
there must be some threat to ensure the source complies finally 
for the court to issue a judgment one party or the other must 
compile a proof of what the rest of path quality was hence the 
entire discovery process acts as a rest of path monitor albeit a rather 
costly monitor in this case 
of course mechanisms can be designed to combine these monitors 
at much lower cost typically such mechanisms would call for 
automatic sharing of proofs with court intervention only as a last 
resort we defer these interesting mechanisms to future work 
as an aside intuition might dictate that shopc 
i 
generates more 
information than ropc after all inferring individual node qualities 
seems a much harder problem yet without path information 
shopc 
i 
is not sufficient for our first-best innovation result the 
proof of this demonstrates a useful technique 
claim with monitors e e rop shopc 
i 
and prc and a 
nowhere-commoditized bilateral contract competition game the 
optimal quality cannot be maintained for all assignments of quality 
and marginal cost in fixed-route coalition-proof 
protect-theinnocent equilibrium 
proof because nodes cannot verify the data path they cannot form 
a proof of what the rest of path quality is hence ropc monitors do 
not exist and therefore the requirements of claim cannot hold 
 conclusions and future work 
it is our hope that this study will have a positive impact in at least 
three different ways the first is practical we believe our analysis 
has implications for the design of future monitoring protocols and 
for public policy 
for protocol designers we first provide fresh motivation to create 
monitoring systems we have argued that the poor accountability of 
the internet is a fundamental obstacle to alleviating the pathologies 
of commoditization and lack of innovation unless accountability 
improves these pathologies are guaranteed to remain 
secondly we suggest directions for future advances in monitoring 
we have shown that adding verifiability to monitors allows for 
some improvements in the characteristics of competition at the 
same time this does not present a fully satisfying solution this 
paper has suggested a novel standard for monitors to aspire to - one 
of supporting optimal routes in innovative competition games under 
fixed-route coalition-proof protect-the-innocent equilibrium we 
have shown that under bilateral contracts this specifically requires 
contractible rest of path monitors 
this is not to say that other types of monitors are unimportant we 
included an example in which individual hop quality monitors and a 
path monitor can also meet our standard for sustaining competition 
however in order for this to happen a mechanism must be included 
 
to combine proofs from these monitors to form a proof of rest of 
path quality in other words the monitors must ultimately be 
combined to form contractible rest of path monitors to support 
service differentiation and innovation it may be easier to design rest 
of path monitors directly thereby avoiding the task of designing 
mechanisms for combining component monitors 
as far as policy implications our analysis points to the need for 
legal institutions to enforce contracts based on quality these 
institutions must be equipped to verify proofs of quality and police 
illegal contracting behavior as quality-based contracts become 
numerous and complicated and possibly negotiated by machine 
this may become a challenging task and new standards and 
regulations may have to emerge in response this remains an 
interesting and unexplored area for research 
the second area we hope our study will benefit is that of clean-slate 
architectural design traditionally clean-slate design tends to focus 
on creating effective and elegant networks for a static set of 
requirements thus the approach is often one of engineering 
which tends to neglect competitive effects we agree with 
ratnasamy shenker and mccanne that designing for evolution 
should be a top priority we have demonstrated that the 
network s monitoring ability is critical to supporting innovation as 
are the institutions that support contracting these elements should 
feature prominently in new designs our analysis specifically 
suggests that architectures based on bilateral contracts should 
include contractible rest of path monitoring from a clean-slate 
perspective these monitors can be transparently and fully integrated 
with the routing and contracting systems 
finally the last contribution our study makes is methodological 
we believe that the mathematical formalization we present is 
applicable to a variety of future research questions while a 
significant literature addresses innovation in the presence of 
network effects to the best of our knowledge ours is the first model 
of innovation in a network industry that successfully incorporates 
the actual topological structure as input this allows the discovery 
of new properties such as the weakening of market forces with the 
number of isps on a data path that we observe with 
lowaccountability 
our method also stands in contrast to the typical approach of 
distributed algorithmic mechanism design because this field is 
based on a principle-agent framework contracts are usually 
proposed by the source who is allowed to make a take it or leave it 
offer to network nodes our technique allows contracts to emerge 
from a competitive framework so the source is limited to selecting 
the most desirable contract we believe this is a closer reflection of 
the industry 
based on the insights in this study the possible directions for future 
research are numerous and exciting to some degree contracting 
based on quality opens a pandora s box of pressing questions do 
quality-based contracts stand counter to the principle of network 
neutrality should isps be allowed to offer a choice of contracts at 
different quality levels what anti-competitive behaviors are 
enabled by quality-based contracts can a contracting system 
support optimal multicast trees 
in this study we have focused on bilateral contracts this system 
has seemed natural especially since it is the prevalent system on the 
current network perhaps its most important benefit is that each 
contract is local in nature so both parties share a common familiar 
legal jurisdiction there is no need to worry about who will enforce 
a punishment against another isp on the opposite side of the planet 
nor is there a dispute over whose legal rules to apply in interpreting 
a contract 
although this benefit is compelling it is worth considering other 
systems the clearest alternative is to form a contract between the 
source and every node on the path we may call these source 
contracts source contracting may present surprising advantages 
for instance since isps do not exchange money with each other an 
isp cannot save money by selecting a cheaper next hop 
additionally if the source only has contracts with nodes on the 
intended path other nodes won t even be willing to accept packets 
from this source since they won t receive compensation for carrying 
them this combination seems to eliminate all temptation for a 
single cheater to cheat in route because of this and other 
encouraging features we believe source contracts are a fertile topic 
for further study 
another important research task is to relax our assumption that 
quality can be measured fully and precisely one possibility is to 
assume that monitoring is only probabilistic or suffers from noise 
even more relevant is the possibility that quality monitors are 
fundamentally incomplete a quality monitor can never anticipate 
every dimension of quality that future applications will care about 
nor can it anticipate a new and valuable protocol that an isp 
introduces we may define a monitor space as a subspace of the 
quality space that a monitor can measure qm ⊂ and a 
corresponding monitoring function that simply projects the full 
range of qualities onto the monitor space mqm → 
clearly innovations that leave quality invariant under m are not 
easy to support - they are invisible to the monitoring system in this 
environment we expect that path monitoring becomes more 
important since it is the only way to ensure data reaches certain 
innovator isps further research is needed to understand this 
process 
 acknowledgements 
we would like to thank the anonymous reviewers jens grossklags 
moshe babaioff scott shenker sylvia ratnasamy and hal varian 
for their comments this work is supported in part by the national 
science foundation under itr award ani- 
 references 
 afergan m using repeated games to design 
incentivebased routing systems in proceedings of ieee infocom 
 april 
 afergan m and wroclawski j on the benefits and 
feasibility of incentive based routing infrastructure in acm 
sigcomm workshop on practice and theory of incentives 
in networked systems pins august 
 argyraki k maniatis p cheriton d and shenker s 
providing packet obituaries in third workshop on hot topics 
in networks hotnets november 
 clark d d the design philosophy of the darpa internet 
protocols in proceedings of acm sigcomm 
 
 clark d d wroclawski j sollins k r and braden r 
tussle in cyberspace defining tomorrow s internet in 
proceedings of acm sigcomm august 
 dang-nguyen g and pénard t interconnection agreements 
strategic behaviour and property rights in brousseau e and 
glachant j m eds the economics of contracts theories and 
applications cambridge university press 
 feigenbaum j papadimitriou c sami r and shenker s 
a bgp-based mechanism for lowest-cost routing 
distributed computing pp - 
 huston g interconnection peering and settlements telstra 
australia 
 liu y zhang h gong w and towsley d on the 
interaction between overlay routing and traffic engineering 
in proceedings of ieee infocom 
 mackie-mason j and varian h pricing the internet in 
kahin b and keller j eds public access to the internet 
englewood cliffs nj prentice-hall 
 ratnasamy s shenker s and mccanne s towards an 
evolvable internet architecture in proceeding of acm 
sigcomm 
 shakkottai s and srikant r economics of network pricing 
with multiple isps in proceedings of ieee infocom 
 
 appendix 
proof of claim node i must fall on the equilibrium data path to 
receive any payment let the prices along the data path be 
ppppp ns with marginal costs ncc we may 
assume the prices on the path are greater than lp or the claim 
follows trivially each node along the data path can cheat in route 
by giving data to the bargain path at price no more than lp so 
node j s temptation to cheat is at least 
 − 
− 
≥ 
−− 
−− 
jj 
l 
jjj 
ljj 
pp 
pp 
pcp 
pcp 
 then lemma gives 
 
 
 
− 
− 
− 
− 
− 
− 
⋅⋅ 
− 
− 
− 
− 
≥ 
n 
s 
l 
n 
lll 
p 
pp 
n 
pp 
pp 
pp 
pp 
pp 
pp 
t 
this can be rearranged to give 
 
 s 
n 
l p 
n 
t 
pp 
 
 
− 
 ≤ 
− 
 as required 
the rest of the claim simply recognizes that rp is the greatest 
reward node i can receive for its investment so it will not invest 
sums greater than this 
proof of claim label the nodes n in the order in which 
they select contracts let subgame n be the game that begins with n 
choosing its contract let ln be the set of possible paths restricted to 
nodes n   n that is ln is the set of possible routes from s to 
reach some node that has already moved 
for subgame n define the local welfare over paths nll ∈ and their 
possible next hops nj as follows 
 j 
li 
ipathjl pcqqujlv −− 
∈ 
 
where ql is the quality of path l in the set {n   n} and pathjq and 
pj are the quality and price of the contract j has offered 
for induction assume that subgame n maximizes local welfare 
we show that subgame n does as well if node n selects next hop k 
we can write the following relation 
 nknn knlvpcpknlvnlv π− − 
where n is node n s profit if the path to n is chosen this path is 
chosen whenever nlv is maximal over ln and possible next 
hops if knlv is maximal over ln it is also maximal over the 
paths in ln that don t lead to n this means that node n can choose 
some n small enough so that nlv is maximal over ln so the 
route will lead to k 
conversely if knlv is not maximal over ln either v is greater 
for another of n s next hops in which case n will select that one in 
order to increase n or v is greater for some path in ln that don t 
lead to n in which case nlv cannot be maximal for any 
nonnegative n 
thus we conclude that subgame n maximizes local welfare for the 
initial case observe that this assumption holds for the source 
finally we deduce that subgame which is the entire game 
maximizes local welfare which is equivalent to actual welfare 
hence the stackelberg price-quality game yields an optimal route 
 
stardust a flexible architecture for passive localization in 
wireless sensor networks 
∗ 
radu stoleru pascal vicaire tian he† john a stankovic 
department of computer science university of virginia 
†department of computer science and engineering university of minnesota 
{stoleru pv f} cs virginia edu tianhe cs umn edu stankovic cs virginia edu 
abstract 
the problem of localization in wireless sensor networks 
where nodes do not use ranging hardware remains a 
challenging problem when considering the required location 
accuracy energy expenditure and the duration of the 
localization phase in this paper we propose a framework called 
stardust for wireless sensor network localization based on 
passive optical components in the stardust framework 
sensor nodes are equipped with optical retro-reflectors an 
aerial device projects light towards the deployed sensor 
network and records an image of the reflected light an image 
processing algorithm is developed for obtaining the locations 
of sensor nodes for matching a node id to a location we 
propose a constraint-based label relaxation algorithm we 
propose and develop localization techniques based on four 
types of constraints node color neighbor information 
deployment time for a node and deployment location for a 
node we evaluate the performance of a localization system 
based on our framework by localizing a network of 
sensor nodes deployed in a × ft area the localization 
accuracy ranges from ft to ft while the localization time 
ranges from milliseconds to minutes 
categories and subject descriptors c computer 
communications networks distributed systems c 
 special purpose and application based systems real-time and 
embedded systems 
general terms algorithms measurement performance 
design experimentation 
 introduction 
wireless sensor networks wsn have been envisioned 
to revolutionize the way humans perceive and interact with 
the surrounding environment one vision is to embed tiny 
sensor devices in outdoor environments by aerial 
deployments from unmanned air vehicles the sensor nodes form 
a network and collaborate to compensate for the extremely 
scarce resources available to each of them computational 
power memory size communication capabilities to 
accomplish the mission through collaboration redundancy and 
fault tolerance the wsn is then able to achieve 
unprecedented sensing capabilities 
a major step forward has been accomplished by 
developing systems for several domains military surveillance 
 habitat monitoring and structural monitoring 
even after these successes several research problems remain 
open among these open problems is sensor node 
localization i e how to find the physical position of each sensor 
node despite the attention the localization problem in wsn 
has received no universally acceptable solution has been 
developed there are several reasons for this on one hand 
localization schemes that use ranging are typically high end 
solutions gps ranging hardware consumes energy it is 
relatively expensive if high accuracy is required and poses 
form factor challenges that move us away from the vision 
of dust size sensor nodes ultrasound has a short range and 
is highly directional solutions that use the radio transceiver 
for ranging either have not produced encouraging results if 
the received signal strength indicator is used or are sensitive 
to environment e g multipath on the other hand 
localization schemes that only use the connectivity information 
for inferring location information are characterized by low 
accuracies ≈ ft in controlled environments − ft in 
realistic ones 
to address these challenges we propose a framework for 
wsn localization called stardust in which the 
complexity associated with the node localization is completely 
removed from the sensor node the basic principle of the 
framework is localization through passivity each sensor 
node is equipped with a corner-cube retro-reflector and 
possibly an optical filter a coloring device an aerial 
vehicle projects light onto the deployment area and records 
images containing retro-reflected light beams they appear as 
luminous spots through image processing techniques the 
locations of the retro-reflectors i e sensor nodes is 
deter 
mined for inferring the identity of the sensor node present 
at a particular location the stardust framework develops a 
constraint-based node id relaxation algorithm 
the main contributions of our work are the following we 
propose a novel framework for node localization in wsns 
that is very promising and allows for many future extensions 
and more accurate results we propose a constraint-based 
label relaxation algorithm for mapping node ids to the 
locations and four constraints node connectivity time and 
space which are building blocks for very accurate and very 
fast localization systems we develop a sensor node 
hardware prototype called a sensorball we evaluate the 
performance of a localization system for which we obtain location 
accuracies of − ft with a localization duration ranging 
from milliseconds to minutes we investigate the range 
of a system built on our framework by considering realities 
of physical phenomena that occurs during light propagation 
through the atmosphere 
the rest of the paper is structured as follows section 
is an overview of the state of art the design of the 
stardust framework is presented in section one 
implementation and its performance evaluation are in sections and 
 followed by a suite of system optimization techniques in 
section in section we present our conclusions 
 related work 
we present the prior work in localization in two major 
categories the range-based and the range-free schemes 
the range-based localization techniques have been 
designed to use either more expensive hardware and hence 
higher accuracy or just the radio transceiver ranging 
techniques dependent on hardware are the time-of-flight tof 
and the time-difference-of-arrival tdoa solutions that use 
the radio are based on the received signal strength indicator 
 rssi and more recently on radio interferometry 
the tof localization technique that is most widely used is 
the gps gps is a costly solution for a high accuracy 
localization of a large scale sensor network ahlos employs 
a tdoa ranging technique that requires extensive hardware 
and solves relatively large nonlinear systems of equations 
the cricket location-support system tdoa can achieve 
a location granularity of tens of inches with highly 
directional and short range ultrasound transceivers in the 
location of a sniper is determined in an urban terrain by 
using the tdoa between an acoustic wave and a radio beacon 
the pushpin project uses the tdoa between ultrasound 
pulses and light flashes for node localization the radar 
system uses the rssi to build a map of signal strengths 
as emitted by a set of beacon nodes a mobile node is 
located by the best match in the signal strength space with a 
previously acquired signature in mal a mobile node 
assists in measuring the distances acting as constraints 
between nodes until a rigid graph is generated the localization 
problem is formulated as an on-line state estimation in a 
nonlinear dynamic system a cooperative ranging that 
attempts to achieve a global positioning from distributed local 
optimizations is proposed in a very recent remarkable 
localization technique is based on radio interferometry rips 
 which utilizes two transmitters to create an interfering 
signal the frequencies of the emitters are very close to each 
other thus the interfering signal will have a low frequency 
envelope that can be easily measured the ranging technique 
performs very well the long time required for localization 
and multi-path environments pose significant challenges 
real environments create additional challenges for the 
range based localization schemes these have been 
emphasized by several studies to address these 
challenges and others hardware cost the energy expenditure 
the form factor the small range localization time several 
range-free localization schemes have been proposed sensor 
nodes use primarily connectivity information for inferring 
proximity to a set of anchors in the centroid localization 
scheme a sensor node localizes to the centroid of its 
proximate beacon nodes in apit each node decides its 
position based on the possibility of being inside or outside of 
a triangle formed by any three beacons within node s 
communication range the gradient algorithm leverages 
the knowledge about the network density to infer the average 
one hop length this in turn can be transformed into 
distances to nodes with known locations dv-hop uses the 
hop by hop propagation capability of the network to forward 
distances to landmarks more recently several localization 
schemes that exploit the sensing capabilities of sensor nodes 
have been proposed spotlight creates well controlled 
 in time and space events in the network while the sensor 
nodes detect and timestamp this events from the 
spatiotemporal knowledge for the created events and the temporal 
information provided by sensor nodes nodes spatial 
information can be obtained in a similar manner the lighthouse 
system uses a parallel light beam that is emitted by an 
anchor which rotates with a certain period a sensor node 
detects the light beam for a period of time which is 
dependent on the distance between it and the light emitting device 
many of the above localization solutions target specific 
sets of requirements and are useful for specific applications 
stardust differs in that it addresses a particular demanding 
set of requirements that are not yet solved well stardust is 
meant for localizing air dropped nodes where node 
passiveness high accuracy low cost small form factor and rapid 
localization are all required many military applications have 
such requirements 
 stardust system design 
the design of the stardust system and its name was 
inspired by the similarity between a deployed sensor network 
in which sensor nodes indicate their presence by emitting 
light and the universe consisting of luminous and 
illuminated objects stars galaxies planets etc 
the main difficulty when applying the above ideas to the 
real world is the complexity of the hardware that needs to 
be put on a sensor node so that the emitted light can be 
detected from thousands of feet the energy expenditure for 
producing an intense enough light beam is also prohibitive 
instead what we propose to use for sensor node 
localization is a passive optical element called a retro-reflector 
the most common retro-reflective optical component is a 
corner-cube retroreflector ccr shown in figure a it 
consists of three mutually perpendicular mirrors the 
inter 
 a b 
figure corner-cube retroreflector a and an array of 
ccrs molded in plastic b 
esting property of this optical component is that an incoming 
beam of light is reflected back towards the source of the 
light irrespective of the angle of incidence this is in 
contrast with a mirror which needs to be precisely positioned to 
be perpendicular to the incident light a very common and 
inexpensive implementation of an array of ccrs is the 
retroreflective plastic material used on cars and bicycles for night 
time detection shown in figure b 
in the stardust system each node is equipped with a 
small e g in array of ccrs and the enclosure has 
self-righting capabilities that orient the array of ccrs 
predominantly upwards it is critical to understand that the 
upward orientation does not need to be exact even when large 
angular variations from a perfectly upward orientation are 
present a ccr will return the light in the exact same 
direction from which it came 
in the remaining part of the section we present the 
architecture of the stardust system and the design of its main 
components 
 system architecture 
the envisioned sensor network localization scenario is as 
follows 
 the sensor nodes are released possibly in a controlled 
manner from an aerial vehicle during the night 
 the aerial vehicle hovers over the deployment area and 
uses a strobe light to illuminate it the sensor nodes 
equipped with ccrs and optical filters acting as 
coloring devices have self-righting capabilities and 
retroreflect the incoming strobe light the retro-reflected 
light is either white as the originating source light 
or colored due to optical filters 
 the aerial vehicle records a sequence of two images 
very close in time msec level one image is taken 
when the strobe light is on the other when the strobe 
light is off the acquired images are used for obtaining 
the locations of sensor nodes which appear as luminous 
spots in the image 
 the aerial vehicle executes the mapping of node ids to 
the identified locations in one of the following ways a 
by using the color of a retro-reflected light if a sensor 
node has a unique color b by requiring sensor nodes 
to establish neighborhood information and report it to 
a base station c by controlling the time sequence of 
sensor nodes deployment and recording additional 
imlight emitter 
sensor node i 
transfer function 
φi λ 
ψ λ 
φ ψ λ 
image 
processing 
node id matching 
radio model 
r 
g λ e 
central device 
v 
v 
figure the stardust system architecture 
ages d by controlling the location where a sensor node 
is deployed 
 the computed locations are disseminated to the sensor 
network 
the architecture of the stardust system is shown in 
figure the architecture consists of two main components 
the first is centralized and it is located on a more powerful 
device the second is distributed and it resides on all 
sensor nodes the central device consists of the following the 
light emitter the image processing module the node id 
mapping module and the radio model the distributed 
component of the architecture is the transfer function which 
acts as a filter for the incoming light the aforementioned 
modules are briefly described below 
 light emitter - it is a strobe light capable of producing 
very intense collimated light pulses the emitted light 
is non-monochromatic unlike a laser and it is 
characterized by a spectral density ψ λ a function of the 
wavelength the emitted light is incident on the ccrs 
present on sensor nodes 
 transfer function φ ψ λ - this is a bandpass filter 
for the incident light on the ccr the filter allows a 
portion of the original spectrum to be retro-reflected 
from here on we will refer to the transfer function as 
the color of a sensor node 
 image processing - the image processing module 
acquires high resolution images from these images the 
locations and the colors of sensor nodes are obtained 
if only one set of pictures can be taken i e one 
location of the light emitter image analysis device then the 
map of the field is assumed to be known as well as the 
distance between the imaging device and the field the 
aforementioned assumptions field map and distance to 
it are not necessary if the images can be simultaneously 
taken from different locations it is important to remark 
here that the identity of a node can not be directly 
obtained through image processing alone unless a 
specific characteristic of a sensor node can be identified in 
the image 
 node id matching - this module uses the detected 
locations and through additional techniques e g sensor 
node coloring and connectivity information g λ e 
from the deployed network to uniquely identify the 
sensor nodes observed in the image the connectivity 
information is represented by neighbor tables sent from 
 
algorithm image processing 
 background filtering 
 retro-reflected light recognition through intensity 
filtering 
 edge detection to obtain the location of sensor nodes 
 color identification for each detected sensor node 
each sensor node to the central device 
 radio model - this component provides an estimate of 
the radio range to the node id matching module it 
is only used by node id matching techniques that are 
based on the radio connectivity in the network the 
estimate of the radio range r is based on the sensor node 
density obtained through the image processing 
module and the connectivity information i e g λ e 
the two main components of the stardust architecture 
are the image processing and the node id mapping their 
design and analysis is presented in the sections that follow 
 image processing 
the goal of the image processing algorithm ipa is to 
identify the location of the nodes and their color note that 
ipa does not identify which node fell where but only what 
is the set of locations where the nodes fell 
ipa is executed after an aerial vehicle records two 
pictures one in which the field of deployment is illuminated and 
one when no illuminations is present let pdark be the 
picture of the deployment area taken when no light was emitted 
and plight be the picture of the same deployment area when a 
strong light beam was directed towards the sensor nodes 
the proposed ipa has several steps as shown in 
algorithm the first step is to obtain a third picture pfilter where 
only the differences between pdark and plight remain let us 
assume that pdark has a resolution of n × m where n is the 
number of pixels in a row of the picture while m is the 
number of pixels in a column of the picture then pdark is 
composed of n × m pixels noted pdark i j i ∈ ≤ i ≤ n ≤ 
j ≤ m similarly plight is composed of n × m pixels noted 
plight i j ≤ i ≤ n ≤ j ≤ m 
each pixel p is described by an rgb value where the r 
value is denoted by pr the g value is denoted by pg and 
the b value is denoted by pb ipa then generates the third 
picture pfilter through the following transformations 
pr 
filter i j pr 
light i j −pr 
dark i j 
pg 
filter i j pg 
light i j −pg 
dark i j 
pb 
filter i j pb 
light i j −pb 
dark i j 
 
after this transformation all the features that appeared in 
both pdark and plight are removed from pfilter this simplifies 
the recognition of light retro-reflected by sensor nodes 
the second step consists of identifying the elements 
contained in pfilter that retro-reflect light for this an intensity 
filter is applied to pfilter first ipa converts pfilter into a 
grayscale picture then the brightest pixels are identified and 
used to create preflect this step is eased by the fact that the 
reflecting nodes should appear much brighter than any other 
illuminated object in the picture 
support q λk 
ni 
p 
 
p 
 
pn 
λ 
 
λk 
 
λn 
figure probabilistic label relaxation 
the third step runs an edge detection algorithm on preflect 
to identify the boundary of the nodes present a tool such as 
matlab provides a number of edge detection techniques we 
used the bwboundaries function for the obtained edges the 
location x y in the image of each node is determined by 
computing the centroid of the points constituting its edges 
standard computer graphics techniques are then used 
to transform the d locations of sensor nodes detected in 
multiple images into d sensor node locations the color of 
the node is obtained as the color of the pixel located at x y 
in plight 
 node id matching 
the goal of the node id matching module is to 
obtain the identity node id of a luminous spot in the 
image detected to be a sensor node for this we define v 
{ x y x y xm ym } to be the set of locations of 
the sensor nodes as detected by the image processing 
module and λ {λ λ λm} to be the set of unique node ids 
assigned to the m sensor nodes before deployment from 
here on we refer to node ids as labels 
we model the problem of finding the label λj of a node ni 
as a probabilistic label relaxation problem frequently used 
in image processing understanding in the image processing 
domain scene labeling i e identifying objects in an 
image plays a major role the goal of scene labeling is to 
assign a label to each object detected in an image such that 
an appropriate image interpretation is achieved it is 
prohibitively expensive to consider the interactions among all 
the objects in an image instead constraints placed among 
nearby objects generate local consistencies and through 
iteration global consistencies can be obtained 
the main idea of the sensor node localization through 
probabilistic label relaxation is to iteratively compute the 
probability of each label being the correct label for a 
sensor node by taking into account at each iteration the 
support for a label the support for a label can be understood 
as a hint or proof that a particular label is more likely to be 
the correct one when compared with the other potential 
labels for a sensor node we pictorially depict this main idea 
in figure as shown node ni has a set of candidate 
labels {λ λk} each of the labels has a different value 
for the support function q λk we defer the explanation 
of how the support function is implemented until the 
subsections that follow where we provide four concrete 
techniques formally the algorithm is outlined in algorithm 
where the equations necessary for computing the new 
probability pni λk for a label λk of a node ni are expressed by the 
 
algorithm label relaxation 
 for each sensor node ni do 
 assign equal prob to all possible labels 
 end for 
 repeat 
 converged ← true 
 for each sensor node ni do 
 for each each label λj of ni do 
 compute the support label λj equation 
 end for 
 compute k for the node ni equation 
 for each each label λj do 
 update probability of label λj equation 
 if new prob −old prob ≥ ε then 
 converged ← false 
 end if 
 end for 
 end for 
 until converged true 
following equations 
ps 
ni 
 λk 
 
kni 
ps 
ni 
 λk qs 
ni 
 λk 
where kni is a normalizing constant given by 
kni 
n 
∑ 
k 
ps 
ni 
 λk qs 
ni 
 λk 
and qs 
ni 
 λk is 
qs 
ni 
 λk support for label λk of node ni 
the label relaxation algorithm is iterative and it is 
polynomial in the size of the network number of nodes the 
pseudo-code is shown in algorithm it initializes the 
probabilities associated with each possible label for a node ni 
through a uniform distribution at each iteration s the 
algorithm updates the probability associated with each label by 
considering the support qs 
ni 
 λk for each candidate label of 
a sensor node 
in the sections that follow we describe four different 
techniques for implementing the support function based on 
node coloring radio connectivity the time of deployment 
 time and the location of deployment space while some 
of these techniques are simplistic they are primitives which 
when combined can create powerful localization systems 
these design techniques have different trade-offs which we 
will present in section 
 relaxation with color constraints 
the unique mapping between a sensor node s position 
 identified by the image processing and a label can be 
obtained by assigning a unique color to each sensor node for 
this we define c {c c cn} to be the set of unique 
colors available and m λ → c to be a one-to-one mapping of 
labels to colors this mapping is known prior to the sensor 
node deployment from node manufacturing 
in the case of color constrained label relaxation the 
support for label λk is expressed as follows 
qs 
ni 
 λk 
as a result the label relaxation algorithm algorithm 
consists of the following steps one label is assigned to each 
sensor node lines - of the algorithm implicitly having 
a probability pni λk the algorithm executes a single 
iteration when the support function simply reiterates the 
confidence in the unique labeling 
however it is often the case that unique colors for each 
node will not be available it is interesting to discuss here the 
influence that the size of the coloring space i e c has on 
the accuracy of the localization algorithm several cases are 
discussed below 
 if c no colors are used and the sensor nodes are 
equipped with simple ccrs that reflect back all the 
incoming light i e no filtering and no coloring of the 
incoming light from the image processing system the 
position of sensor nodes can still be obtained since 
all nodes appear white no single sensor node can be 
uniquely identified 
 if c m − then there are enough unique colors for 
all nodes one node remains white i e no coloring the 
problem is trivially solved each node can be identified 
based on its unique color this is the scenario for the 
relaxation with color constraints 
 if c ≥ there are several options for how to 
partition the coloring space if c {c } one possibility is 
to assign the color c to a single node and leave the 
remaining m− sensor nodes white or to assign the color 
c to more than one sensor node one can observe that 
once a color is assigned uniquely to a sensor node in 
effect that sensor node is given the status of anchor 
or node with known location 
it is interesting to observe that there is an entire spectrum 
of possibilities for how to partition the set of sensor nodes 
in equivalence classes where an equivalence class is 
represented by one color in order to maximize the success of the 
localization algorithm one of the goals of this paper is to 
understand how the size of the coloring space and its 
partitioning affect localization accuracy 
despite the simplicity of this method of constraining the 
set of labels that can be assigned to a node we will show that 
this technique is very powerful when combined with other 
relaxation techniques 
 relaxation with connectivity constraints 
connectivity information obtained from the sensor 
network through beaconing can provide additional information 
for locating sensor nodes in order to gather connectivity 
information the following need to occur after deployment 
through beaconing of hello messages sensor nodes build 
their neighborhood tables each node sends its neighbor 
table information to the central device via a base station 
first let us define g λ e to be the weighted 
connectivity graph built by the central device from the received 
neighbor table information in g the edge λi λj has a 
 
λ 
λ 
 
λn 
ni nj 
gi j 
λ 
λ 
 
λn 
pj λ 
pj λ 
 
pj λn 
pi λ 
pi λ 
 
pi λn gi jm 
figure label relaxation with connectivity constraints 
weight gij represented by the number of beacons sent by λj 
and received by λi in addition let r be the radio range of 
the sensor nodes 
the main idea of the connectivity constrained label 
relaxation is depicted in figure in which two nodes ni and 
nj have been assigned all possible labels the confidence in 
each of the candidate labels for a sensor node is represented 
by a probability shown in a dotted rectangle 
it is important to remark that through beaconing and the 
reporting of neighbor tables to the central device a global 
view of all constraints in the network can be obtained it 
is critical to observe that these constraints are among labels 
as shown in figure two constraints exist between nodes ni 
and nj the constraints are depicted by gi j and gi jm the 
number of beacons sent the labels λj and λjm and received 
by the label λi 
the support for the label λk of sensor node ni resulting 
from the interaction i e within radio range with sensor 
node nj is given by 
qs 
ni 
 λk 
m 
∑ 
m 
gλkλm 
ps 
nj 
 λm 
as a result the localization algorithm algorithm 
consists of the following steps all labels are assigned to each 
sensor node lines - of the algorithm and implicitly each 
label has a probability initialized to pni λk λ in each 
iteration the probabilities for the labels of a sensor node are 
updated when considering the interaction with the labels of 
sensor nodes within r it is important to remark that the 
identity of the nodes within r is not known only the candidate 
labels and their probabilities the relaxation algorithm 
converges when during an iteration the probability of no label 
is updated by more than ε 
the label relaxation algorithm based on connectivity 
constraints enforces such constraints between pairs of sensor 
nodes for a large scale sensor network deployment it is not 
feasible to consider all pairs of sensor nodes in the network 
hence the algorithm should only consider pairs of sensor 
nodes that are within a reasonable communication range r 
we assume a circular radio range and a symmetric 
connectivity in the remaining part of the section we propose a 
simple analytical model that estimates the radio range r for 
medium-connected networks less than neighbors per r 
we consider the following to be known the size of the 
deployment field l the number of sensor nodes deployed n 
algorithm localization 
 estimate the radio range r 
 execute the label relaxation algorithm with support 
function given by equation for neighbors less than r 
apart 
 for each sensor node ni do 
 node identity is λk with max prob 
 end for 
and the total number of unidirectional i e not symmetric 
one-hop radio connections in the network k for our 
analysis we uniformly distribute the sensor nodes in a square area 
of length l by using a grid of unit length l 
√ 
n we use the 
substitution u l 
√ 
n to simplify the notation in order to 
distinguish the following cases if u ≤ r ≤ 
√ 
 u each node 
has four neighbors the expected k n if 
√ 
 u ≤ r ≤ u 
each node has eight neighbors the expected k n if 
 u ≤ r ≤ 
√ 
 u each node has twelve neighbors the expected 
k n if 
√ 
 u ≤ r ≤ u each node has twenty neighbors 
 the expected k n 
for a given t k n we take r to be the middle of the 
interval as an example if t then r 
√ 
 u a 
quadratic fitting for r over the possible values of t produces 
the following closed-form solution for the communication 
range r as a function of network connectivity k assuming l 
and n constant 
r k 
l 
√ 
n 
− 
k 
 n 
 
 
k 
 n 
 
we investigate the accuracy of our model in section 
 relaxation with time constraints 
time constraints can be treated similarly with color 
constraints the unique identification of a sensor node can be 
obtained by deploying sensor nodes individually one by one 
and recording a sequence of images the sensor node that is 
identified as new in the last picture it was not identified in 
the picture before last must be the last sensor node dropped 
in a similar manner with color constrained label 
relaxation the time constrained approach is very simple but may 
take too long especially for large scale systems while it 
can be used in practice it is unlikely that only a time 
constrained label relaxation is used as we will see by 
combining constrained-based primitives realistic localization 
systems can be implemented 
the support function for the label relaxation with time 
constraints is defined identically with the color constrained 
relaxation 
qs 
ni 
 λk 
the localization algorithm algorithm consists of the 
following steps one label is assigned to each sensor node 
 lines - of the algorithm and implicitly having a 
probability pni λk the algorithm executes a single iteration 
 
d 
d 
d 
d 
 node 
label- 
label- 
label- 
label- 
 
 
 
 
figure relaxation with space 
constraints 
 
 
 
 
 
 
 
 
 
pdf 
distance d 
σ 
σ 
σ 
figure probability distribution of 
distances 
- 
- 
- 
- 
 
 
 
 
 
x 
- 
- 
- 
- 
 
 
 
 
 
y 
 
 
 
 
 
 
node density 
figure distribution of nodes 
when the support function simply reiterates the confidence 
in the unique labeling 
 relaxation with space constraints 
spatial information related to sensor deployment can also 
be employed as another input to the label relaxation 
algorithm to do that we use two types of locations the node 
location pn and the label location pl the former pn is defined 
as the position of nodes xn yn zn after deployment which 
can be obtained through image processing as mentioned in 
section the latter pl is defined as the location xl yl zl 
where a node is dropped we use dni 
λm 
to denote the 
horizontal distance between the location of the label λm and the 
location of the node ni clearly dni 
λm 
 xn −xl yn −yl 
at the time of a sensor node release the one-to-one 
mapping between the node and its label is known in other words 
the label location is the same as the node location at the 
release time after release the label location information is 
partially lost due to the random factors such as wind and 
surface impact however statistically the node locations are 
correlated with label locations such correlation depends on 
the airdrop methods employed and environments for the 
sake of simplicity let s assume nodes are dropped from the 
air through a helicopter hovering in the air wind can be 
decomposed into three components x y and z only x and 
y affect the horizontal distance a node can travel 
according to we can assume that x and y follow an 
independent normal distribution therefore the absolute value of 
the wind speed follows a rayleigh distribution obviously 
the higher the wind speed is the further a node would land 
away horizontally from the label location if we assume that 
the distance d is a function of the wind speed v 
we can obtain the probability distribution of d under a given 
wind speed distribution without loss of generality we 
assume that d is proportional to the wind speed therefore 
d follows the rayleigh distribution as well as shown in 
figure the spatial-based relaxation is a recursive process 
to assign the probability that a nodes has a certain label by 
using the distances between the location of a node with 
multiple label locations 
we note that the distribution of distance d affects the 
probability with which a label is assigned it is not 
necessarily true that the nearest label is always chosen for example 
if d follows the rayleigh σ distribution we can obtain the 
probability density function pdf of distances as shown 
in figure this figure indicates that the possibility of a 
node to fall vertically is very small under windy conditions 
 σ and that the distance d is affected by the σ the 
spatial distribution of nodes for σ is shown in figure 
strong wind with a high σ value leads to a larger node 
dispersion more formally given a probability density function 
pdf d the support for label λk of sensor node ni can be 
formulated as 
qs 
ni 
 λk pdf dni 
λk 
 
it is interesting to point out two special cases first if all 
nodes are released at once i e only one label location for 
all released nodes the distance d from a node to all labels 
is the same in this case ps 
ni 
 λk ps 
ni 
 λk which indicates 
that we can not use the spatial-based relaxation to recursively 
narrow down the potential labels for a node second if nodes 
are released at different locations that are far away from each 
other we have i if node ni has label λk ps 
ni 
 λk → when 
s → ∞ ii if node ni does not have label λk ps 
ni 
 λk → 
when s → ∞ in this second scenario there are multiple 
labels one label per release hence it is possible to correlate 
release times labels with positions on the ground these 
results indicate that spatial-based relaxation can label the node 
with a very high probability if the physical separation among 
nodes is large 
 relaxation with color and connectivity 
constraints 
one of the most interesting features of the stardust 
architecture is that it allows for hybrid localization solutions to be 
built depending on the system requirements one example 
is a localization system that uses the color and connectivity 
constraints in this scheme the color constraints are used for 
reducing the number of candidate labels for sensor nodes 
to a more manageable value as a reminder in the 
connectivity constrained relaxation all labels are candidate labels 
for each sensor node the color constraints are used in the 
initialization phase of algorithm lines - after the 
initialization the standard connectivity constrained relaxation 
algorithm is used 
for a better understanding of how the label relaxation 
algorithm works we give a concrete example exemplified in 
figure in part a of the figure we depict the data structures 
 
 
 
 
 
 
 
 
 
 
ni nj 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 a 
 
 
 
 
 
 
 
 
 
ni nj 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 b 
figure a step through the algorithm after 
initialization a and after the st iteration for node ni b 
associated with nodes ni and nj after the initialization steps 
of the algorithm lines - as well as the number of beacons 
between different labels as reported by the network through 
g λ e as seen the potential labels shown inside the 
vertical rectangles are assigned to each node node ni can be 
any of the following also depicted in the figure 
are the probabilities associated with each of the labels after 
initialization all probabilities are equal 
part b of figure shows the result of the first iteration 
of the localization algorithm for node ni assuming that node 
nj is the first wi chosen in line of algorithm by using 
equation the algorithm computes the support q λi for 
each of the possible labels for node ni once the q λi s 
are computed the normalizing constant given by equation 
 can be obtained the last step of the iteration is to update 
the probabilities associated with all potential labels of node 
ni as given by equation 
one interesting problem which we explore in the 
performance evaluation section is to assess the impact the 
partitioning of the color set c has on the accuracy of 
localization when the size of the coloring set is smaller than the 
number of sensor nodes as it is the case for our hybrid 
connectivity color constrained relaxation the system designer 
has the option of allowing one node to uniquely have a color 
 acting as an anchor or multiple nodes intuitively by 
assigning one color to more than one node more constraints 
 distributed can be enforced 
 relaxation techniques analysis 
the proposed label relaxation techniques have different 
trade-offs for our analysis of the trade-offs we consider 
the following metrics of interest the localization time 
 duration the energy consumed overhead the network size 
 scale that can be handled by the technique and the 
localization accuracy the parameters of interest are the following 
the number of sensor nodes n the energy spent for one 
aerial drop εd the energy spent in the network for 
collecting and reporting neighbor information εb and the time td 
taken by a sensor node to reach the ground after being 
aerially deployed the cost comparison of the different label 
relaxation techniques is shown in table 
as shown the relaxation techniques based on color and 
space constraints have the lowest localization duration zero 
for all practical purposes the scalability of the color based 
relaxation technique is however limited to the number of 
 a b 
figure sensorball with self-righting capabilities a 
and colored ccrs b 
unique color filters that can be built the narrower the 
transfer function ψ λ the larger the number of unique colors 
that can be created the manufacturing costs however are 
increasing as well the scalability issue is addressed by all 
other label relaxation techniques most notably the time 
constrained relaxation which is very similar to the 
colorconstrained relaxation addresses the scale issue at a higher 
deployment cost 
criteria color connectivity time space 
duration ntb ntd 
overhead εd εd nεb nεd εd 
scale c n n n 
accuracy high low high medium 
table comparison of label relaxation techniques 
 system implementation 
the stardust localization framework depicted in figure 
 is flexible in that it enables the development of new 
localization systems based on the four proposed label 
relaxation schemes or the inclusion of other yet to be invented 
schemes for our performance evaluation we implemented a 
version of the stardust framework namely the one proposed 
in section where the constraints are based on color and 
connectivity 
the central device of the stardust system consists of the 
following the light emitter - we used a 
common-off-theshelf flash light qbeam million candlepower the 
image acquisition was done with a megapixel digital camera 
 sony dsc-s which provided the input to the image 
processing algorithm implemented in matlab 
for sensor nodes we built a custom sensor node called 
sensorball with self-righting capabilities shown in figure 
 a the self-righting capabilities are necessary in order to 
orient the ccr predominantly upwards the ccrs that we 
used were inexpensive plastic molded night time warning 
signs commonly available on bicycles as shown in figure 
 b we remark here the low quality of the ccrs we used 
the reflectivity of each ccr there are tens molded in the 
plastic container is extremely low and each ccr is not built 
with mirrors a reflective effect is achieved by employing 
finely polished plastic surfaces we had colors available 
in addition to the standard ccr which reflects all the 
incoming light white ccr for a slightly higher price ours 
were cents piece better quality ccrs can be employed 
 
figure the field in the dark figure the illuminated field figure the difference figure 
 figure 
higher quality better mirrors would translate in more 
accurate image processing better sensor node detection and 
smaller form factor for the optical component an array of 
ccrs with a smaller area can be used 
the sensor node platform we used was the micaz mote 
the code that runs on each node is a simple application 
which broadcasts beacons and maintains a neighbor 
table containing the percentage of successfully received 
beacons for each neighbor on demand the neighbor table is 
reported to a base station where the node id mapping is 
performed 
 system evaluation 
in this section we present the performance evaluation of 
a system implementation of the stardust localization 
framework the three major research questions that our evaluation 
tries to answer are the feasibility of the proposed framework 
 can sensor nodes be optically detected at large distances 
the localization accuracy of one actual implementation of the 
stardust framework and whether or not atmospheric 
conditions can affect the recognition of sensor nodes in an 
image the first two questions are investigated by evaluating 
the two main components of the stardust framework the 
image processing and the node id matching these 
components have been evaluated separately mainly because of 
lack of adequate facilities we wanted to evaluate the 
performance of the image processing algorithm in a long range 
realistic experimental set-up while the node id matching 
required a relatively large area available for long periods of 
time for connectivity data gathering the third research 
question is investigated through a computer modeling of 
atmospheric phenomena 
for the evaluation of the image processing module we 
performed experiments in a football stadium where we 
deploy sensor nodes in a × grid the distance between the 
central device and the sensor nodes is approximately ft 
the metrics of interest are the number of false positives and 
false negatives in the image processing algorithm 
for the evaluation of the node id mapping component 
we deploy sensor nodes in an × ft flat area of 
a stadium in order to investigate the influence the radio 
connectivity has on localization accuracy we vary the height 
above ground of the deployed sensor nodes two set-ups are 
used one in which the sensor nodes are on the ground and 
the second one in which the sensor nodes are raised inches 
above ground from here on we will refer to these two 
experimental set-ups as the low connectivity and the high 
connectivity networks respectively because when nodes are 
on the ground the communication range is low resulting in 
less neighbors than when the nodes are elevated and have a 
greater communication range the metrics of interest are 
the localization error defined as the distance between the 
computed location and the true location - known from the 
manual placement the percentage of nodes correctly 
localized the convergence of the label relaxation algorithm the 
time to localize and the robustness of the node id mapping 
to errors in the image processing module 
the parameters that we vary experimentally are the 
angle under which images are taken the focus of the camera 
and the degree of connectivity the parameters that we vary 
in simulations subsequent to image acquisition and 
connectivity collection the number of colors the number of 
anchors the number of false positives or negatives as input 
to the node id matching component the distance between 
the imaging device and sensor network i e range 
atmospheric conditions light attenuation coefficient and ccr 
reflectance indicative of its quality 
 image processing 
for the ipa evaluation we deploy sensor nodes in a 
 × grid we take sets of pictures using different 
orientations of the camera and different zooming factors all 
pictures were taken from the same location each set is 
composed of a picture taken in the dark and of a picture taken 
with a light beam pointed at the nodes we process the 
pictures offline using a matlab implementation of ipa since we 
are interested in the feasibility of identifying colored sensor 
nodes at large distance the end result of our ipa is the d 
location of sensor nodes position in the image the 
transformation to d coordinates can be done through standard 
computer graphics techniques 
one set of pictures obtained as part of our experiment is 
shown in figures and the execution of our ipa 
algorithm results in figure which filters out the background 
and figure which shows the output of the edge detection 
step of ipa the experimental results are depicted in 
figure for each set of pictures the graph shows the number 
of false positives the ipa determines that there is a node 
 
figure retroreflectors detected in figure 
 
 
 
 
 
experiment number 
count 
false positives 
false negatives 
figure false positives and negatives for the nodes 
while there is none and the number of false negatives the 
ipa determines that there is no node while there is one in 
about of the cases we obtained perfect results i e no 
false positives and no false negatives in the remaining cases 
we obtained a number of false positives of at most one and 
a number of false negatives of at most two 
we exclude two pairs of pictures from figure in the 
first excluded pair we obtain false positives and in the 
second pair false positives and false negatives by 
carefully examining the pictures we realized that the first pair 
was taken out of focus and that a car temporarily appeared 
in one of the pictures of the second pair the anomaly in 
the second set was due to the fact that we waited too long to 
take the second picture if the pictures had been taken a few 
milliseconds apart the car would have been represented on 
either both or none of the pictures and the ipa would have 
filtered it out 
 node id matching 
we evaluate the node id matching component of our 
system by collecting empirical data connectivity information 
from the outdoor deployment of nodes in the × ft 
area we collect sets of data for the high connectivity 
and low connectivity network deployments off-line we 
investigate the influence of coloring on the metrics of interest 
by randomly assigning colors to the sensor nodes for one 
experimental data set we generate random assignments 
of colors to sensor nodes it is important to observe that for 
the evaluation of the node id matching algorithm color and 
connectivity constrained we simulate the color assignment 
to sensor nodes as mentioned in section the size of the 
coloring space available to us was colors through 
simulations of color assignment not connectivity we are able 
to investigate the influence that the size of the coloring space 
has on the accuracy of localization the value of the 
param 
 
 
 
 
 
 
 
distance feet 
count 
connected 
not connected 
figure the number of existing and missing radio 
connections in the sparse connectivity experiment 
 
 
 
 
 
 
 
 
 
distance feet 
count 
connected 
not connected 
figure the number of existing and missing radio 
connections in the high connectivity experiment 
eter ε used in algorithm was the results presented 
here represent averages over the randomly generated 
colorings and over all experimental data sets 
we first investigate the accuracy of our proposed radio 
model and subsequently use the derived values for the radio 
range in the evaluation of the node id matching component 
 radio model 
from experiments we obtain the average number of 
observed beacons k defined in section for the low 
connectivity network of beacons and for the high 
connectivity network of beacons from our radio model 
 equation we obtain a radio range r ft for the low 
connectivity network and r ft for the high connectivity 
network 
to estimate the accuracy of our simple model we plot 
the number of radio links that exist in the networks and the 
number of links that are missing as functions of the distance 
between nodes the results are shown in figures and 
we define the average radio range r to be the distance over 
which less than of potential radio links are missing 
as shown in figure the radio range is between ft and 
 ft for the higher connectivity network the radio range 
was between ft and ft 
we choose two conservative estimates of the radio range 
 ft for the low connectivity case and ft for the high 
connectivity case which are in good agreement with the values 
predicted by our radio model 
 localization error vs coloring space size 
in this experiment we investigate the effect of the number 
of colors on the localization accuracy for this we randomly 
assign colors from a pool of a given size to the sensor nodes 
 
 
 
 
 
 
 
 
 
 
 
 
number of colors 
localizationerror feet 
r feet 
r feet 
r feet 
figure localization error 
 
 
 
 
 
 
 
 
 
 
 
 
number of colors 
 correctlocalized x 
r feet 
r feet 
r feet 
figure percentage of nodes correctly localized 
we then execute the localization algorithm which uses the 
empirical data the algorithm is run for three different radio 
ranges and ft to investigate its influence on the 
localization error 
the results are depicted in figure localization error 
and figure percentage of nodes correctly localized as 
shown for an estimate of ft for the radio range as 
predicted by our radio model we obtain the smallest 
localization errors as small as ft when enough colors are used 
both figures and confirm our intuition that a larger 
number of colors available significantly decrease the error in 
localization 
the well known fact that relaxation algorithms do not 
always converge was observed during our experiments the 
percentage of successful runs when the algorithm 
converged is depicted in figure as shown in several 
situations the algorithm failed to converge the algorithm 
execution was stopped after iterations per node if the 
algorithm does not converge in a predetermined number of steps 
it will terminate and the label with the highest probability 
will provide the identity of the node it is very probable that 
the chosen label is incorrect since the probabilities of some 
of labels are constantly changing with each iteration the 
convergence of relaxation based algorithms is a well known 
issue 
 localization error vs color uniqueness 
as mentioned in the section a unique color gives a 
sensor node the statute of an anchor a sensor node that is 
an anchor can unequivocally be identified through the image 
processing module in this section we investigate the effect 
unique colors have on the localization accuracy specifically 
we want to experimentally verify our intuition that assigning 
more nodes to a color can benefit the localization accuracy 
by enforcing more constraints as opposed to uniquely 
assigning a color to a single node 
 
 
 
 
 
number of colors 
convergencerate x 
r feet 
r feet 
r feet 
figure convergence error 
 
 
 
 
 
 
 
 
 
 
number of colors 
localizationerror feet 
 anchors 
 anchors 
 anchors 
 anchors 
 anchors 
figure localization error vs number of colors 
for this we fix the number of available colors to either 
 or and vary the number of nodes that are given unique 
colors from up to the maximum number of colors or 
 naturally if we have a maximum number of colors of 
we can assign at most anchors the experimental results 
are depicted in figure localization error and figure 
 percentage of sensor node correctly localized as expected 
the localization accuracy increases with the increase in the 
number of colors available larger coloring space also for 
a given size of the coloring space e g colors available if 
more colors are uniquely assigned to sensor nodes then the 
localization accuracy decreases it is interesting to observe 
that by assigning colors uniquely to nodes the benefit of 
having additional colors is diminished specifically if colors 
are available and all are assigned uniquely the system would 
be less accurately localized error ≈ ft when compared 
to the case of colors and no unique assignments of colors 
 ≈ ft localization error 
the same trend of a less accurate localization can be 
observed in figure which shows the percentage of nodes 
correctly localized i e ft localization error as shown if 
we increase the number of colors that are uniquely assigned 
the percentage of nodes correctly localized decreases 
 localization error vs connectivity 
we collected empirical data for two network deployments 
with different degrees of connectivity high and low in 
order to assess the influence of connectivity on location 
accuracy the results obtained from running our localization 
algorithm are depicted in figure and figure we 
varied the number of colors available and assigned no anchors 
 i e no unique assignments of colors 
in both scenarios as expected localization error decrease 
with an increase in the number of colors it is interesting 
to observe however that the low connectivity scenario 
im 
 
 
 
 
 
 
 
 
 
number of colors 
 correctlocalized x 
 anchors 
 anchors 
 anchors 
 anchors 
 anchors 
figure percentage of nodes correctly localized vs 
number of colors 
 
 
 
 
 
 
 
 
 
 
 
number of colors 
localizationerror feet 
low connectivity 
high connectivity 
figure localization error vs number of colors 
proves the localization accuracy quicker from the additional 
number of colors available when the number of colors 
becomes relatively large twelve for our sensor node 
network both scenarios low and high connectivity have 
comparable localization errors of less that ft the same trend 
of more accurate location information is evidenced by 
figure which shows that the percentage of nodes that are 
localized correctly grows quicker for the low connectivity 
deployment 
 localization error vs image processing 
errors 
so far we investigated the sources for error in 
localization that are intrinsic to the node id matching component 
as previously presented luminous objects can be 
mistakenly detected to be sensor nodes during the location 
detection phase of the image processing module these false 
positives can be eliminated by the color recognition procedure 
of the image processing module more problematic are false 
negatives when a sensor node does not reflect back enough 
light to be detected they need to be handled by the 
localization algorithm in this case the localization algorithm 
is presented with two sets of nodes of different sizes that 
need to be matched one coming from the image processing 
 which misses some nodes and one coming from the 
network with the connectivity information here we assume a 
fully connected network so that all sensor nodes report their 
connectivity information in this experiment we investigate 
how image processing errors false negatives influence the 
localization accuracy 
for this evaluation we ran our localization algorithm with 
empirical data but dropped a percentage of nodes from the 
list of nodes detected by the image processing algorithm we 
artificially introduced false negatives in the image 
process 
 
 
 
 
 
 
 
 
 
 
 
number of colors 
 correctlocalized x 
low connectivity 
high connectivity 
figure percentage of nodes correctly localized 
 
 
 
 
 
 
 
 
 
 false negatives x 
localizationerror feet 
 colors 
 colors 
 colors 
figure impact of false negatives on the localization 
error 
ing the effect of false negatives on localization accuracy is 
depicted in figure as seen in the figure if the number of 
false negatives is the error in position estimation 
doubles when colors are available it is interesting to observe 
that the scenario when more colors are available e g 
colors is being affected more drastically than the scenario with 
less colors e g colors the benefit of having more colors 
available is still being maintained at least for the range of 
colors we investigated through colors 
 localization time 
in this section we look more closely at the duration for 
each of the four proposed relaxation techniques and two 
combinations of them color-connectivity and color-time 
we assume that unique color filters can be manufactured 
that the sensor network is deployed from ft 
 necessary for the time-constrained relaxation and that the time 
required for reporting connectivity grows linearly with an 
initial reporting period of sec as used in a real world 
tracking application the localization duration results as 
presented in table are depicted in figure 
as shown for all practical purposes the time required 
by the space constrained relaxation techniques is sec the 
same applies to the color constrained relaxation for which 
the localization time is sec if the number of colors is 
sufficient considering our assumptions only for a network of 
size the color constrained relaxation works the 
localization duration for all other network sizes and 
is infinite i e unique color assignments to sensor nodes 
can not be made since only colors are unique when 
only color constrained relaxation is used both the 
connectivity constrained and time constrained techniques increase 
linearly with the network size for the time constrained the 
central device deploys sensor nodes one by one recording 
an image after the time a sensor node is expected to reach the 
 
 
 
 
 
 
 
 
color connectivity time space 
colorconenctivity 
color-time 
localization technique 
localizationtime sec 
 nodes 
 nodes 
 nodes 
 nodes 
figure localization time for different 
label relaxation schemes 
 
 
 
 
 
 
 
 
 
 
r feet 
c 
r 
 
 
 
 
 
 
 
 
figure apparent contrast in a 
clear atmosphere 
 
 
 
 
 
 
 
 
 
 
r feet 
c 
r 
 
 
 
 
 
 
 
 
figure apparent contrast in a 
hazing atmosphere 
ground 
it is interesting to notice in figure the improvement in 
the localization time obtained by simply combining the color 
and the connectivity constrained techniques the 
localization duration in this case is identical with the connectivity 
constrained technique 
the combination of color and time constrained 
relaxations is even more interesting for a reasonable 
localization duration of seconds a perfect i e ft localization 
error localization system can be built in this scenario the 
set of sensor nodes is split in batches with each batch 
having a set of unique colors it would be very interesting to 
consider other scenarios where the strength of the space 
constrained relaxation sec for any sensor network size is 
used for improving the other proposed relaxation techniques 
we leave the investigation and rigorous classification of such 
technique combination for future work 
 system range 
in this section we evaluate the feasibility of the 
stardust localization framework when considering the realities 
of light propagation through the atmosphere 
the main factor that determines the range of our system is 
light scattering which redirects the luminance of the source 
into the medium in essence equally affecting the luminosity 
of the target and of the background scattering limits the 
visibility range by reducing the apparent contrast between 
the target and its background approaches zero as the 
distance increases the apparent contrast cr is quantitatively 
expressed by the formula 
cr nt 
r −nb 
r nb 
r 
where nt 
r and nb 
r are the apparent target radiance and 
apparent background radiance at distance r from the light source 
respectively the apparent radiance nt 
r of a target at a 
distance r from the light source is given by 
nt 
r na 
iρte− σr 
πr 
 
where i is the intensity of the light source ρt is the 
target reflectance σ is the spectral attenuation coefficient ≈ 
 km− and ≈ km− for a clear and a hazy 
atmosphere respectively and na is the radiance of the 
atmospheric backscatter and it can be expressed as follows 
na 
gσ i 
 π 
 σrz 
 σr 
e−x 
x 
dx 
where g is a backscatter gain the apparent 
background radiance nb 
r is given by formulas similar with 
equations and where only the target reflectance ρt is 
substituted with the background reflectance ρb it is important 
to remark that when cr reaches its lower limit no increase 
in the source luminance or receiver sensitivity will increase 
the range of the system from equations and it can be 
observed that the parameter which can be controlled and can 
influence the range of the system is ρt the target reflectance 
figures and depict the apparent contrast cr as a 
function of the distance r for a clear and for a hazy 
atmosphere respectively the apparent contrast is investigated for 
reflectance coefficients ρt ranging from to perfect 
reflector for a contrast c of at least as it can be seen in 
figure a range of approximately ft can be achieved 
if the atmosphere is clear the performance dramatically 
deteriorates when the atmospheric conditions are problematic 
as shown in figure a range of up to ft is 
achievable when using highly reflective ccr components 
while our light source million candlepower was 
sufficient for a range of a few hundred feet we remark that there 
exist commercially available light sources million 
candlepower or military million candlepower 
powerful enough for ranges of a few thousand feet 
 stardust system optimizations 
in this section we describe extensions of the proposed 
architecture that can constitute future research directions 
 chained constraint primitives 
in this paper we proposed four primitives for 
constraintbased relaxation algorithms color connectivity time and 
space to demonstrate the power that can be obtained by 
combining them we proposed and evaluated one 
combination of such primitives color and connectivity an 
interesting research direction to pursue could be to chain more than 
two of these primitives an example of such chain is color 
temporal spatial and connectivity other research directions 
could be to use voting scheme for deciding which primitive 
to use or assign different weights to different relaxation 
algorithms 
 
 location learning 
if after several iterations of the algorithm none of the 
label probabilities for a node ni converges to a higher value the 
confidence in our labeling of that node is relatively low it 
would be interesting to associate with a node more than one 
label implicitly more than one location and defer the label 
assignment decision until events are detected in the network 
 if the network was deployed for target tracking 
 localization in rugged environments 
the initial driving force for the stardust localization 
framework was to address the sensor node localization in 
extremely rugged environments canopies dense vegetation 
extremely obstructing environments pose significant 
challenges for sensor nodes localization the hope and our 
original idea was to consider the time period between the aerial 
deployment and the time when the sensor node disappears 
under the canopy by recording the last visible position of a 
sensor node as seen from the aircraft a reasonable estimate 
of the sensor node location can be obtained this would 
require that sensor nodes posses self-righting capabilities 
while in mid-air nevertheless we remark on the suitability 
of our localization framework for rugged non-line-of-sight 
environments 
 conclusions 
stardust solves the localization problem for aerial 
deployments where passiveness low cost small form factor 
and rapid localization are required results show that 
accuracy can be within ft and localization time within 
milliseconds stardust also shows robustness with respect to errors 
we predict the influence the atmospheric conditions can have 
on the range of a system based on the stardust framework 
and show that hazy environments or daylight can pose 
significant challenges 
most importantly the properties of stardust support 
the potential for even more accurate localization solutions 
as well as solutions for rugged non-line-of-sight 
environments 
 references 
 t he s krishnamurthy j a stankovic t abdelzaher l luo 
r stoleru t yan l gu j hui and b krogh an energy-efficient 
surveillance system using wireless sensor networks in mobisys 
 g simon m maroti a ledeczi g balogh b kusy a nadas 
g pap j sallai and k frampton sensor network-based 
countersniper system in sensys 
 a arora p dutta and b bapat a line in the sand a wireless sensor 
network for trage detection classification and tracking in computer 
networks 
 r szewczyk a mainwaring j polastre j anderson and d culler 
an analysis of a large scale habitat monitoring application in acm 
sensys 
 n xu s rangwala k k chintalapudi d ganesan a broad 
r govindan and d estrin a wireless sensor network for structural 
monitoring in acm sensys 
 a savvides c han and m srivastava dynamic fine-grained 
localization in ad-hoc networks of sensors in mobicom 
 n priyantha a chakraborty and h balakrishnan the cricket 
location-support system in mobicom 
 m broxton j lifton and j paradiso localizing a sensor network 
via collaborative processing of global stimuli in ewsn 
 p bahl and v n padmanabhan radar an in-building rf-based user 
location and tracking system in ieee infocom 
 n priyantha h balakrishnan e demaine and s teller 
mobileassisted topology generation for auto-localization in sensor networks 
in ieee infocom 
 p n pathirana a savkin s jha and n bulusu node localization 
using mobile robots in delay-tolerant sensor networks ieee 
transactions on mobile computing 
 c savarese j m rabaey and j beutel locationing in distribued 
ad-hoc wireless sensor networks in icaassp 
 m maroti b kusy g balogh p volgyesi a nadas k molnar 
s dora and a ledeczi radio interferometric geolocation in acm 
sensys 
 k whitehouse a woo c karlof f jiang and d culler the 
effects of ranging noise on multi-hop localization an empirical study 
in ipsn 
 y kwon k mechitov s sundresh w kim and g agha resilient 
localization for sensor networks in outdoor environment uiuc tech 
rep 
 r stoleru and j a stankovic probability grid a location 
estimation scheme for wireless sensor networks in secon 
 n bulusu j heidemann and d estrin gps-less low cost outdoor 
localization for very small devices ieee personal communications 
magazine 
 t he c huang b blum j a stankovic and t abdelzaher 
range-free localization schemes in large scale sensor networks in 
acm mobicom 
 r nagpal h shrobe and j bachrach organizing a global 
coordinate system from local information on an ad-hoc sensor network in 
ipsn 
 d niculescu and b nath ad-hoc positioning system in ieee 
globecom 
 r stoleru t he j a stankovic and d luebke a high-accuracy 
low-cost localization system for wireless sensor networks in acm 
sensys 
 k r¨omer the lighthouse location system for smart dust in 
acm usenix mobisys 
 r y tsai a versatile camera calibration technique for 
highaccuracy d machine vision metrology using off-the-shelf tv cameras 
and lenses ieee jra 
 c l archer and m z jacobson spatial and temporal distributions 
of u s winds and wind power at m derived from measurements 
geophysical research jrnl 
 team for advanced flow simulation and modeling online 
available http www mems rice edu tafsm res 
 k stein r benney t tezduyar v kalro and j leonard -d 
computation of parachute fluid-structure interactions - performance and 
control in aerodynamic decelerator systems conference 
 headquarters department of the army technical manual for 
searchlight infrared an gss- v 
 
pthinc a thin-client architecture 
for mobile wireless web 
joeng kim ricardo a baratto and jason nieh 
department of computer science 
columbia university new york ny usa 
{jk ricardo nieh} cs columbia edu 
abstract 
although web applications are gaining popularity on 
mobile wireless pdas web browsers on these systems can be 
quite slow and often lack adequate functionality to access 
many web sites we have developed pthinc a pda 
thinclient solution that leverages more powerful servers to run 
full-function web browsers and other application logic then 
sends simple screen updates to the pda for display pthinc 
uses server-side screen scaling to provide high-fidelity 
display and seamless mobility across a broad range of different 
clients and screen sizes including both portrait and 
landscape viewing modes pthinc also leverages existing pda 
control buttons to improve system usability and maximize 
available screen resolution for application display we have 
implemented pthinc on windows mobile and evaluated 
its performance on mobile wireless devices our results 
compared to local pda web browsers and other thin-client 
approaches demonstrate that pthinc provides superior web 
browsing performance and is the only pda thin client that 
effectively supports crucial browser helper applications such 
as video playback 
categories and subject descriptors c 
computercommunication-networks distributed systems - client 
server 
general terms design experimentation performance 
 introduction 
the increasing ubiquity of wireless networks and 
decreasing cost of hardware is fueling a proliferation of mobile 
wireless handheld devices both as standalone wireless personal 
digital assistants pda and popular integrated pda cell 
phone devices these devices are enabling new forms of 
mobile computing and communication service providers are 
leveraging these devices to deliver pervasive web access and 
mobile web users already often use these devices to access 
web-enabled information such as news email and localized 
travel guides and maps it is likely that within a few years 
most of the devices accessing the web will be mobile 
users typically access web content by running a web browser 
and associated helper applications locally on the pda 
although native web browsers exist for pdas they deliver 
subpar performance and have a much smaller feature set 
and more limited functionality than their desktop 
computing counterparts as a result pda web browsers are 
often not able to display web content from web sites that 
leverage more advanced web technologies to deliver a richer web 
experience this fundamental problem arises for two 
reasons first because pdas have a completely different 
hardware software environment from traditional desktop 
computers web applications need to be rewritten and customized 
for pdas if at all possible duplicating development costs 
because the desktop application market is larger and more 
mature most development effort generally ends up being 
spent on desktop applications resulting in greater 
functionality and performance than their pda counterparts 
second pdas have a more resource constrained environment 
than traditional desktop computers to provide a smaller 
form factor and longer battery life desktop web browsers 
are large complex applications that are unable to run on a 
pda instead developers are forced to significantly strip 
down these web browsers to provide a usable pda web 
browser thereby crippling pda browser functionality 
thin-client computing provides an alternative approach 
for enabling pervasive web access from handheld devices 
a thin-client computing system consists of a server and a 
client that communicate over a network using a remote 
display protocol the protocol enables graphical displays to be 
virtualized and served across a network to a client device 
while application logic is executed on the server using the 
remote display protocol the client transmits user input to 
the server and the server returns screen updates of the 
applications from the server to the client using a thin-client 
model for mobile handheld devices pdas can become 
simple stateless clients that leverage the remote server 
capabilities to execute web browsers and other helper applications 
the thin-client model provides several important 
benefits for mobile wireless web first standard desktop web 
applications can be used to deliver web content to pdas 
without rewriting or adapting applications to execute on 
a pda reducing development costs and leveraging existing 
software investments second complex web applications can 
be executed on powerful servers instead of running stripped 
down versions on more resource constrained pdas 
providing greater functionality and better performance third 
web applications can take advantage of servers with faster 
networks and better connectivity further boosting 
application performance fourth pdas can be even simpler 
devices since they do not need to perform complex application 
logic potentially reducing energy consumption and 
extend 
ing battery life finally pda thin clients can be essentially 
stateless appliances that do not need to be backed up or 
restored require almost no maintenance or upgrades and do 
not store any sensitive data that can be lost or stolen this 
model provides a viable avenue for medical organizations to 
comply with hipaa regulations while embracing mobile 
handhelds in their day to day operations 
despite these potential advantages thin clients have been 
unable to provide the full range of these benefits in delivering 
web applications to mobile handheld devices existing thin 
clients were not designed for pdas and do not account for 
important usability issues in the context of small form factor 
devices resulting in difficulty in navigating displayed web 
content furthermore existing thin clients are ineffective at 
providing seamless mobility across the heterogeneous mix 
of device display sizes and resolutions while existing thin 
clients can already provide faster performance than native 
pda web browsers in delivering html web content they 
do not effectively support more display-intensive web helper 
applications such as multimedia video which is increasingly 
an integral part of available web content 
to harness the full potential of thin-client computing in 
providing mobile wireless web on pdas we have developed 
pthinc pda thin-client internet computing pthinc 
builds on our previous work on thinc to provide a 
thinclient architecture for mobile handheld devices pthinc 
virtualizes and resizes the display on the server to efficiently 
deliver high-fidelity screen updates to a broad range of 
different clients screen sizes and screen orientations including 
both portrait and landscape viewing modes this enables 
pthinc to provide the same persistent web session across 
different client devices for example pthinc can provide 
the same web browsing session appropriately scaled for 
display on a desktop computer and a pda so that the same 
cookies bookmarks and other meta-data are continuously 
available on both machines simultaneously pthinc s 
virtual display approach leverages semantic information 
available in display commands and client-side video hardware to 
provide more efficient remote display mechanisms that are 
crucial for supporting more display-intensive web 
applications given limited display resolution on pdas pthinc 
maximizes the use of screen real estate for remote display 
by moving control functionality from the screen to readily 
available pda control buttons improving system usability 
we have implemented pthinc on windows mobile and 
demonstrated that it works transparently with existing 
applications window systems and operating systems and does 
not require modifying recompiling or relinking existing 
software we have quantitatively evaluated pthinc against 
local pda web browsers and other thin-client approaches on 
pocket pc devices our experimental results demonstrate 
that pthinc provides superior web browsing performance 
and is the only pda thin client that effectively supports 
crucial browser helper applications such as video playback 
this paper presents the design and implementation of 
pthinc section describes the overall usage model and 
usability characteristics of pthinc section presents the 
design and system architecture of pthinc section presents 
experimental results measuring the performance of pthinc 
on web applications and comparing it against native pda 
browsers and other popular pda thin-client systems 
section discusses related work finally we present some 
concluding remarks 
 pthinc usage model 
pthinc is a thin-client system that consists of a simple 
client viewer application that runs on the pda and a server 
that runs on a commodity pc the server leverages more 
powerful pcs to to run web browsers and other application 
logic the client takes user input from the pda stylus and 
virtual keyboard and sends them to the server to pass to 
the applications screen updates are then sent back from 
the server to the client for display to the user 
when the pthinc pda client is started the user is 
presented with a simple graphical interface where information 
such as server address and port user authentication 
information and session settings can be provided pthinc first 
attempts to connect to the server and perform the 
necessary handshaking once this process has been completed 
pthinc presents the user with the most recent display of 
his session if the session does not exist a new session is 
created existing sessions can be seamlessly continued without 
changes in the session setting or server configuration 
unlike other thin-client systems pthinc provides a user 
with a persistent web session model in which a user can 
launch a session running a web browser and associated 
applications at the server then disconnect from that session 
and reconnect to it again anytime when a user reconnects 
to the session all of the applications continue running where 
the user left off so that the user can continue working as 
though he or she never disconnected the ability to 
disconnect and reconnect to a session at anytime is an important 
benefit for mobile wireless pda users which may have 
intermittent network connectivity 
pthinc s persistent web session model enables a user to 
reconnect to a web session from devices other than the one 
on which the web session was originally initiated this 
provides users with seamless mobility across different devices 
if a user loses his pda he can easily use another pda to 
access his web session furthermore pthinc allows users 
to use non-pda devices to access web sessions as well a 
user can access the same persistent web session on a 
desktop pc as on a pda enabling a user to use the same web 
session from any computer 
pthinc s persistent web session model addresses a key 
problem encountered by mobile web users the lack of a 
common web environment across computers web browsers 
often store important information such as bookmarks cookies 
and history which enable them to function in a much more 
useful manner the problem that occurs when a user moves 
between computers is that this data which is specific to a 
web browser installation cannot move with the user 
furthermore web browsers often need helper applications to 
process different media content and those applications may 
not be consistently available across all computers pthinc 
addresses this problem by enabling a user to remotely use 
the exact same web browser environment and helper 
applications from any computer as a result pthinc can 
provide a common consistent web browsing environment for 
mobile users across different devices without requiring them 
to attempt to repeatedly synchronize different web browsing 
environments across multiple machines 
to enable a user to access the same web session on 
different devices pthinc must provide mechanisms to 
support different display sizes and resolutions toward this end 
pthinc provides a zoom feature that enables a user to 
zoom in and out of a display and allows the display of a web 
 
figure pthinc shortcut keys 
session to be resized to fit the screen of the device being 
used for example if the server is running a web session at 
 × but the client is a pda with a display resolution 
of × pthinc will resize the desktop display to fit 
the full display in the smaller screen of the pda pthinc 
provides the pda user with the option to increase the size 
of the display by zooming in to different parts of the display 
users are often familiar with the general layout of commonly 
visited websites and are able to leverage this resizing 
feature to better navigate through web pages for example 
a user can zoom out of the display to view the entire page 
content and navigate hyperlinks then zoom in to a region 
of interest for a better view 
to enable a user to access the same web session on 
different devices pthinc must also provide mechanisms to 
support different display orientations in a desktop 
environment users are typically accustomed to having displays 
presented in landscape mode where the screen width is larger 
than its height however in a pda environment the choice 
is not always obvious some users may prefer having the 
display in portrait mode as it is easier to hold the device 
in their hands while others may prefer landscape mode in 
order to minimize the amount of side-scrolling necessary 
to view a web page to accommodate pda user 
preferences pthinc provides an orientation feature that enables 
it to seamless rotate the display between landscape and 
portrait mode the landscape mode is particularly useful for 
pthinc users who frequently access their web sessions on 
both desktop and pda devices providing those users with 
the same familiar landscape setting across different devices 
because screen space is a relatively scarce resource on 
pdas pthinc runs in fullscreen mode to maximize the 
screen area available to display the web session to be able 
to use all of the screen on the pda and still allow the user 
to control and interact with it pthinc reuses the typical 
shortcut buttons found on pdas to perform all the 
control functions available to the user the buttons used by 
pthinc do not require any os environment changes they 
are simply intercepted by the pthinc client application 
when they are pressed figure shows how pthinc 
utilizes the shortcut buttons to provide easy navigation and 
improve the overall user experience these buttons are not 
device specific and the layout shown is common to 
widelyused pocketpc devices pthinc provides six shortcuts to 
support its usage model 
 rotate screen the record button on the left edge is 
used to rotate the screen between portrait and 
landscape mode each time the button is pressed 
 zoom out the leftmost button on the bottom front 
is used to zoom out the display of the web session 
providing a bird s eye view of the web session 
 zoom in the second leftmost button on the bottom 
front is used to zoom in the display of the web session 
to more clearly view content of interest 
 directional scroll the middle button on the bottom 
front is used to scroll around the display using a single 
control button in a way that is already familiar to pda 
users this feature is particularly useful when the user 
has zoomed in to a region of the display such that only 
part of the display is visible on the screen 
 show hide keyboard the second rightmost button on 
the bottom front is used to bring up a virtual keyboard 
drawn on the screen for devices which have no physical 
keyboard the virtual keyboard uses standard pda 
os mechanisms providing portability across different 
pda environments 
 close session the rightmost button on the bottom 
front is used to disconnect from the pthinc session 
pthinc uses the pda touch screen stylus and standard 
user interface mechanisms to provide a user interface 
pointand-click metaphor similar to that provided by the mouse 
in a traditional desktop computing environment pthinc 
does not use a cursor since pda environments do not 
provide one instead a user can use the stylus to tap on 
different sections of the touch screen to indicate input focus a 
single tap on the touch screen generates a corresponding 
single click mouse event a double tap on the touch screen 
generates a corresponding double click mouse event pthinc 
provides two-button mouse emulation by using the stylus to 
press down on the screen for one second to generate a right 
mouse click all of these actions are identical to the way 
users already interact with pda applications in the common 
pocketpc environment in web browsing users can click on 
hyperlinks and focus on input boxes by simply tapping on 
the desired screen area of interest unlike local pda web 
browsers and other pda applications pthinc leverages 
more powerful desktop user interface metaphors to enable 
users to manipulate multiple open application windows 
instead of being limited to a single application window at any 
given moment this provides increased browsing flexibility 
beyond what is currently available on pda devices similar 
to a desktop environment browser windows and other 
application windows can be moved around by pressing down 
and dragging the stylus similar to a mouse 
 pthinc system architecture 
pthinc builds on the thinc remote display 
architecture to provide a thin-client system for pdas pthinc 
virtualizes the display at the server by leveraging the video 
device abstraction layer which sits below the window server 
and above the framebuffer this is a well-defined low-level 
device-dependent layer that exposes the video hardware to 
the display system pthinc accomplishes this through a 
simple virtual display driver that intercepts drawing 
commands packetizes and sends them over the network 
 
while other thin-client approaches intercept display 
commands at other layers of the display subsystem pthinc s 
display virtualization approach provides some key benefits 
in efficiently supporting pda clients for example 
intercepting display commands at a higher layer between 
applications and the window system as is done by x requires 
replicating and running a great deal of functionality on the 
pda that is traditionally provided by the desktop window 
system given both the size and complexity of traditional 
window systems attempting to replicate this functionality 
in the restricted pda environment would have proven to 
be a daunting and perhaps unfeasible task furthermore 
applications and the window system often require tight 
synchronization in their operation and imposing a wireless 
network between them by running the applications on the server 
and the window system on the client would significantly 
degrade performance on the other hand intercepting at a 
lower layer by extracting pixels out of the framebuffer as 
they are rendered provides a simple solution that requires 
very little functionality on the pda client but can also 
result in degraded performance the reason is that by the 
time the remote display server attempts to send screen 
updates it has lost all semantic information that may have 
helped it encode efficiently and it must resort to using a 
generic and expensive encoding mechanism on the server 
as well as a potentially expensive decoding mechanism on 
the limited pda client in contrast to both the high and 
low level interception approaches pthinc s approach of 
intercepting at the device driver provides an effective 
balance between client and server simplicity and the ability to 
efficiently encode and decode screen updates 
by using a low-level virtual display approach pthinc 
can efficiently encode application display commands using 
only a small set of low-level commands in a pda 
environment this set of commands provides a crucial component 
in maintaining the simplicity of the client in the 
resourceconstrained pda environment the display commands are 
shown in table and work as follows copy instructs the 
client to copy a region of the screen from its local framebuffer 
to another location this command improves the user 
experience by accelerating scrolling and opaque window 
movement without having to resend screen data from the server 
sfill pfill and bitmap are commands that paint a 
fixed-size region on the screen they are useful for 
accelerating the display of solid window backgrounds desktop 
patterns backgrounds of web pages text drawing and 
certain operations in graphics manipulation programs sfill 
fills a sizable region on the screen with a single color pfill 
replicates a tile over a screen region bitmap performs a 
fill using a bitmap of ones and zeros as a stipple to apply 
a foreground and background color finally raw is used 
to transmit unencoded pixel data to be displayed verbatim 
on a region of the screen this command is invoked as a 
last resort if the server is unable to employ any other 
command and it is the only command that may be compressed 
to mitigate its impact on network bandwidth 
pthinc delivers its commands using a non-blocking 
serverpush update mechanism where as soon as display updates 
are generated on the server they are sent to the client 
clients are not required to explicitly request display 
updates thus minimizing the impact that the typical 
varying network latency of wireless links may have on the 
responsiveness of the system keeping in mind that resource 
command description 
copy copy a frame buffer area to specified 
coordinates 
sfill fill an area with a given pixel color value 
pfill tile an area with a given pixel pattern 
bitmap fill a region using a bit pattern 
raw display raw pixel data at a given location 
table pthinc protocol display commands 
constrained pdas and wireless networks may not be able 
to keep up with a fast server generating a large number of 
updates pthinc is able to coalesce clip and discard 
updates automatically if network loss or congestion occurs or 
the client cannot keep up with the rate of updates this 
type of behavior proves crucial in a web browsing 
environment where for example a page may be redrawn multiple 
times as it is rendered on the fly by the browser in this 
case the pda will only receive and render the final result 
which clearly is all the user is interesting in seeing 
pthinc prioritizes the delivery of updates to the pda 
using a shortest-remaining-size-first srsf preemptive 
update scheduler srsf is analogous to 
shortest-remainingprocessing-time scheduling which is known to be optimal 
for minimizing mean response time in an interactive system 
in a web browsing environment short jobs are associated 
with text and basic page layout components such as the 
page s background which are critical web content for the 
user on the other hand large jobs are often lower priority 
beautifying elements or even worse web page banners 
and advertisements which are of questionable value to the 
user as he or she is browsing the page using srsf pthinc 
is able to maximize the utilization of the relatively scarce 
bandwidth available on the wireless connection between the 
pda and the server 
 display management 
to enable users to just as easily access their web browser 
and helper applications from a desktop computer at home 
as from a pda while on the road pthinc provides a 
resize mechanism to zoom in and out of the display of a web 
session pthinc resizing is completely supported by the 
server not the client the server resamples updates to fit 
within the pdas viewport before they are transmitted over 
the network pthinc uses fant s resampling algorithm to 
resize pixel updates this provides smooth visually 
pleasing updates with properly antialiasing and has only modest 
computational requirements 
pthinc s resizing approach has a number of advantages 
first it allows the pda to leverage the vastly superior 
computational power of the server to use high quality resampling 
algorithms and produce higher quality updates for the pda 
to display second resizing the screen does not translate into 
additional resource requirements for the pda since it does 
not need to perform any additional work finally better 
utilization of the wireless network is attained since rescaling 
the updates reduces their bandwidth requirements 
to enable users to orient their displays on a pda to 
provide a viewing experience that best accommodates user 
preferences and the layout of web pages or applications 
pthinc provides a display rotation mechanism to switch 
between landscape and portrait viewing modes pthinc 
display rotation is completely supported by the client not 
the server pthinc does not explicitly recalculate the 
ge 
ometry of display updates to perform rotation which would 
be computationally expensive instead pthinc simply 
changes the way data is copied into the framebuffer to switch 
between display modes when in portrait mode data is 
copied along the rows of the framebuffer from left to right 
when in landscape mode data is copied along the columns 
of the framebuffer from top to bottom these very fast and 
simple techniques replace one set of copy operations with 
another and impose no performance overhead pthinc 
provides its own rotation mechanism to support a wide range of 
devices without imposing additional feature requirements on 
the pda although some newer pda devices provide native 
support for different orientations this mechanism is not 
dynamic and requires the user to rotate the pda s entire user 
interface before starting the pthinc client windows 
mobile provides native api mechanisms for pda applications 
to rotate their ui on the fly but these mechanisms deliver 
poor performance and display quality as the rotation is 
performed naively and is not completely accurate 
 video playback 
video has gradually become an integral part of the world 
wide web and its presence will only continue to increase 
web sites today not only use animated graphics and flash 
to deliver web content in an attractive manner but also 
utilize streaming video to enrich the web interface users are 
able to view pre-recorded and live newscasts on cnn watch 
sports highlights on espn and even search through large 
collection of videos on google video to allow applications 
to provide efficient video playback interfaces have been 
created in display systems that allow video device drivers to 
expose their hardware capabilities back to the applications 
pthinc takes advantage of these interfaces and its virtual 
device driver approach to provide a virtual bridge between 
the remote client and its hardware and the applications and 
transparently support video playback 
on top of this architecture pthinc uses the yuv 
colorspace to encode the video content which provides a 
number of benefits first it has become increasingly common 
for pda video hardware to natively support yuv and be 
able to perform the colorspace conversion and scaling 
automatically as a result pthinc is able to provide fullscreen 
video playback without any performance hits second the 
use of yuv allows for a more efficient representation of rgb 
data without loss of quality by taking advantage of the 
human eye s ability to better distinguish differences in 
brightness than in color in particular pthinc uses the yv 
format which allows full color rgb data to be encoded 
using just bits per pixel third yuv data is produced 
as one of the last steps of the decoding process of most 
video codecs allowing pthinc to provide video playback 
in a manner that is format independent finally even if the 
pda s video hardware is unable to accelerate playback the 
colorspace conversion process is simple enough that it does 
not impose unreasonable requirements on the pda 
a more concrete example of how pthinc leverages the 
pda video hardware to support video playback can be seen 
in our prototype implementation on the popular dell axim 
x v pda which is equipped with the intel g 
multimedia accelerator in this case pthinc creates an 
offscreen buffer in video memory and writes and reads from 
this memory region data on the yv format when a new 
video frame arrives video data is copied from the buffer to 
figure experimental testbed 
an overlay surface in video memory which is independent 
of the normal surface used for traditional drawing as the 
yv data is put onto the overlay the intel accelerator 
automatically performs both colorspace conversion and scaling 
by using the overlay surface pthinc has no need to redraw 
the screen once video playback is over since the overlapped 
surface is unaffected in addition specific overlay regions 
can be manipulated by leveraging the video hardware for 
example to perform hardware linear interpolation to smooth 
out the frame and display it fullscreen and to do automatic 
rotation when the client runs in landscape mode 
 experimental results 
we have implemented a pthinc prototype that runs the 
client on widely-used windows mobile-based pocket pc 
devices and the server on both windows and linux operating 
systems to demonstrate its effectiveness in supporting 
mobile wireless web applications we have measured its 
performance on web applications we present experimental results 
on different pda devices for two popular web applications 
browsing web pages and playing video content from the web 
we compared pthinc against native web applications 
running locally on the pda to demonstrate the improvement 
that pthinc can provide over the traditional fat-client 
approach we also compared pthinc against three of the 
most widely used thin clients that can run on pdas citrix 
meta-framexp microsoft remote desktop and vnc 
 virtual network computing we follow common 
practice and refer to citrix metaframexp and microsoft remote 
desktop by their respective remote display protocols ica 
 independent computing architecture and rdp remote 
desktop protocol 
 experimental testbed 
we conducted our web experiments using two different 
wireless pocket pc pdas in an isolated wi-fi network 
testbed as shown in figure the testbed consisted of two 
pda client devices a packet monitor a thin-client server 
and a web server except for the pdas all of the other 
machines were ibm netfinity r servers with dual mhz 
intel piii cpus and mb ram and were connected on 
a switched mbps fastethernet network the web server 
used was apache the network emulator was 
nistnet and the packet monitor was ethereal the 
pda clients connected to the testbed through a b 
lucent orinoco ap- wireless access point all experiments 
using the wireless network were conducted within ten feet 
of the access point so we considered the amount of packet 
loss to be negligible in our experiments 
two pocket pc pdas were used to provide results across 
both older less powerful models and newer higher 
performance models the older model was a dell axim x with 
 
client × × depth resize clip 
rdp no yes -bit no yes 
vnc yes yes -bit no no 
ica yes yes -bit yes no 
pthinc yes yes -bit yes no 
table thin-client testbed configuration setting 
a mhz intel xscale pxa cpu and mb ram 
running windows mobile and a dell truemobile 
 ghz compactflash card for wireless networking the 
newer model was a dell axim x v with a mhz intel 
xscale xpa cpu and mb ram running windows 
mobile and integrated b wireless networking the 
x v has an intel g multimedia accelerator with mb 
video memory both pdas are capable of -bit color but 
have different screen sizes and display resolutions the x 
has a inch diagonal screen with × resolution the 
x v has a inch diagonal screen with × 
the four thin clients that we used support different 
levels of display quality as summarized in table the rdp 
client only supports a fixed × display resolution on 
the server with -bit color depth while other platforms 
provide higher levels of display quality to provide a fair 
comparison across all platforms we conducted our experiments 
with thin-client sessions configured for two possible 
resolutions × and × both ica and vnc were 
configured to use the native pda resolution of -bit color 
depth the current pthinc prototype uses -bit color 
directly and the client downsamples updates to the -bit color 
depth available on the pda rdp was configured using only 
 -bit color depth since it does not support any better color 
depth since both pthinc and ica provide the ability to 
view the display resized to fit the screen we measured both 
clients with and without the display resized to fit the pda 
screen each thin client was tested using landscape rather 
than portrait mode when available all systems run on the 
x v could run in landscape mode because the hardware 
provides a landscape mode feature however the x does 
not provide this functionality only pthinc directly 
supports landscape mode so it was the only system that could 
run in landscape mode on both the x and x v 
to provide a fair comparison we also standardized on 
common hardware and operating systems whenever possible 
all of the systems used the netfinity server as the thin-client 
server for the two systems designed for windows servers 
ica and rdp we ran windows server on the server 
for the other systems which support x-based servers vnc 
and pthinc we ran the debian linux unstable 
distribution with the linux kernel on the server we used the 
latest thin-client server versions available on each platform 
at the time of our experiments namely citrix metaframe 
xp server for windows feature release microsoft 
remote desktop built into windows xp and windows 
using rdp and vnc 
 application benchmarks 
we used two web application benchmarks for our 
experiments based on two common application scenarios browsing 
web pages and playing video content from the web since 
many thin-client systems including two of the ones tested 
are closed and proprietary we measured their performance 
in a noninvasive manner by capturing network traffic with 
a packet monitor and using a variant of slow-motion 
benchmarking previously developed to measure thin-client 
performance in pda environments this measurement 
methodology accounts for both the display decoupling that 
can occur between client and server in thin-client systems 
as well as client processing time which may be significant 
in the case of pdas 
to measure web browsing performance we used a web 
browsing benchmark based on the web text page load test 
from the ziff-davis i-bench benchmark suite the 
benchmark consists of javascript controlled load of pages from 
the web server the pages contain both text and 
graphics with pages varying in size the graphics are embedded 
images in gif and jpeg formats the original i-bench 
benchmark was modified for slow-motion benchmarking by 
introducing delays of several seconds between the pages 
using javascript then two tests were run one where 
delays where added between each page and one where pages 
where loaded continuously without waiting for them to be 
displayed on the client in the first test delays were 
sufficiently adjusted in each case to ensure that each page could 
be received and displayed on the client completely without 
temporal overlap in transferring the data belonging to two 
consecutive pages we used the packet monitor to record 
the packet traffic for each run of the benchmark then used 
the timestamps of the first and last packet in the trace to 
obtain our latency measures the packet monitor also 
recorded the amount of data transmitted between the client 
and the server the ratio between the data traffic in the two 
tests yields a scale factor this scale factor shows the loss 
of data between the server and the client due to inability of 
the client to process the data quickly enough the product 
of the scale factor with the latency measurement produces 
the true latency accounting for client processing time 
to run the web browsing benchmark we used mozilla 
firefox running on the thin-client server for the thin 
clients and windows internet explorer ie mobile for 
and mobile for for the native browsers on the x and 
x v pdas respectively in all cases the web browser used 
was sized to fill the entire display region available 
to measure video playback performance we used a video 
benchmark that consisted of playing a s mpeg- video 
clip containing a mix of news and entertainment 
programming at full-screen resolution the video clip is mb and 
consists of x pixel frames with an ideal frame rate 
of frames sec we measured video performance using 
slow-motion benchmarking by monitoring resulting packet 
traffic at two playback rates frames second fps and 
fps and comparing the results to determine playback 
delays and frame drops that occur at fps to measure overall 
video quality for example quality means that all 
video frames were played at real-time speed on the other 
hand quality could mean that half the video data was 
dropped or that the clip took twice as long to play even 
though all of the video data was displayed 
to run the video benchmark we used windows media 
player for windows-based thin-client servers mplayer 
pre for x-based thin-client servers and windows media 
player mobile and mobile for the native video players 
running locally on the x and x v pdas respectively in 
all cases the video player used was sized to fill the entire 
display region available 
 measurements 
figures and show the results of running the web 
brows 
 
 
 
 
pthinc 
resized 
pthincica 
resized 
icavncrdplocal 
latency s 
platform 
axim x x or less 
axim x v x 
axim x x 
axim x v x 
figure browsing benchmark average page latency 
ing benchmark for each platform we show results for up to 
four different configurations two on the x and two on the 
x v depending on whether each configuration was 
supported however not all platforms could support all 
configurations the local browser only runs at the display 
resolution of the pda × or less for the x v and the 
x rdp only runs at × neither platform could 
support × display resolution ica only ran on the 
x and could not run on the x v because it did not work 
on windows mobile 
figure shows the average latency per web page for each 
platform pthinc provides the lowest average web 
browsing latency on both pdas on the x pthinc performs 
up to times better than other thin-client systems and 
times better than the local browser on the x v pthinc 
performs up to times better than other thin-client 
systems and times better than the native browser in fact 
all of the thin clients except vnc outperform the local 
pda browser demonstrating the performance benefits of 
the thin-client approach usability studies have shown that 
web pages should take less than one second to download 
for the user to experience an uninterrupted web browsing 
experience the measurements show that only the thin 
clients deliver subsecond web page latencies in contrast the 
local browser requires more than seconds on average per 
web page the local browser performs worse since it needs 
to run a more limited web browser to process the html 
javascript and do all the rendering using the limited 
capabilities of the pda the thin clients can take advantage of 
faster server hardware and a highly tuned web browser to 
process the web content much faster 
figure shows that rdp is the next fastest platform after 
pthinc however rdp is only able to run at a fixed 
resolution of × and -bit color depth furthermore rdp 
also clips the display to the size of the pda screen so that 
it does not need to send updates that are not visible on the 
pda screen this provides a performance benefit 
assuming the remaining web content is not viewed but degrades 
performance when a user scrolls around the display to view 
other web content rdp achieves its performance with 
significantly lower display quality compared to the other thin 
clients and with additional display clipping not used by other 
systems as a result rdp performance alone does not 
provide a complete comparison with the other platforms in 
contrast pthinc provides the fastest performance while 
at the same time providing equal or better display quality 
than the other systems 
 
 
 
 
 
pthinc 
resized 
pthincica 
resized 
icavncrdplocal 
datasize kb 
platform 
axim x x or less 
axim x v x 
axim x x 
axim x v x 
figure browsing benchmark average page data 
transferred 
since vnc and ica provide similar display quality to 
pthinc these systems provide a more fair comparison of 
different thin-client approaches ica performs worse in part 
because it uses higher-level display primitives that require 
additional client processing costs vnc performs worse in 
part because it loses display data due to its client-pull 
delivery mechanism and because of the client processing costs 
in decompressing raw pixel primitives in both cases their 
performance was limited in part because their pda clients 
were unable to keep up with the rate at which web pages 
were being displayed 
figure also shows measurements for those thin clients 
that support resizing the display to fit the pda screen 
namely ica and pthinc resizing requires additional 
processing which results in slower average web page latencies 
the measurements show that the additional delay incurred 
by ica when resizing versus not resizing is much more 
substantial than for pthinc ica performs resizing on the 
slower pda client in contrast pthinc leverage the more 
powerful server to do resizing reducing the performance 
difference between resizing and not resizing unlike ica 
pthinc is able to provide subsecond web page download 
latencies in both cases 
figure shows the data transferred in kb per page when 
running the slow-motion version of the tests all of the 
platforms have modest data transfer requirements of roughly 
 kb per page or less this is well within the 
bandwidth capacity of wi-fi networks the measurements show 
that the local browser does not transfer the least amount of 
data this is surprising as html is often considered to be 
a very compact representation of content instead rdp is 
the most bandwidth efficient platform largely as a result of 
using only -bit color depth and screen clipping so that it 
does not transfer the entire web page to the client pthinc 
overall has the largest data requirements slightly more than 
vnc this is largely a result of the current pthinc 
prototype s lack of native support for -bit color data in the wire 
protocol however this result also highlights pthinc s 
performance as it is faster than all other systems even while 
transferring more data furthermore as newer pda models 
support full -bit color these results indicate that pthinc 
will continue to provide good web browsing performance 
since display usability and quality are as important as 
performance figures to compare screenshots of the 
different thin clients when displaying a web page in this case 
from the popular bbc news website except for ica all of 
the screenshots were taken on the x v in landscape mode 
 
figure browser screenshot rdp x figure browser screenshot vnc x 
figure browser screenshot ica resized x figure browser screenshot pthinc resized x 
using the maximum display resolution settings for each 
platform given in table the ica screenshot was taken on the 
x since ica does not run on the x v while the 
screenshots lack the visual fidelity of the actual device display 
several observations can be made figure shows that rdp 
does not support fullscreen mode and wastes lots of screen 
space for controls and ui elements requiring the user to 
scroll around in order to access the full contents of the web 
browsing session figure shows that vnc makes better 
use of the screen space and provides better display quality 
but still forces the user to scroll around to view the web 
page due to its lack of resizing support figure shows 
ica s ability to display the full web page given its resizing 
support but that its lack of landscape capability and poorer 
resize algorithm significantly compromise display quality in 
contrast figure shows pthinc using resizing to provide 
a high quality fullscreen display of the full width of the web 
page pthinc maximizes the entire viewing region by 
moving all controls to the pda buttons in addition pthinc 
leverages the server computational power to use a high 
quality resizing algorithm to resize the display to fit the pda 
screen without significant overhead 
figures and show the results of running the video 
playback benchmark for each platform except ica we 
show results for an x and x v configuration ica could 
not run on the x v as noted earlier the measurements 
were done using settings that reflected the environment a 
user would have to access a web session from both a 
desktop computer and a pda as such a × server 
display resolution was used whenever possible and the video 
was shown at fullscreen rdp was limited to × 
display resolution as noted earlier since viewing the entire 
video display is the only really usable option we resized 
the display to fit the pda screen for those platforms that 
supported this feature namely ica and pthinc 
figure shows the video quality for each platform pthinc 
is the only thin client able to provide perfect video playback 
quality similar to the native pda video player all of the 
other thin clients deliver very poor video quality with the 
exception of rdp on the x v which provided unacceptable 
 video quality none of the other systems were even able 
to achieve video quality vnc and ica have the worst 
quality at on the x device 
pthinc s native video support enables superior video 
performance while other thin clients suffer from their 
inability to distinguish video from normal display updates 
they attempt to apply ineffective and expensive 
compression algorithms on the video data and are unable to keep up 
with the stream of updates generated resulting in dropped 
frames or long playback times vnc suffers further from 
its client-pull update model because video frames are 
generated faster than the rate at which the client can process 
and send requests to the server to obtain the next display 
update figure shows the total data transferred during 
 
 
 
 
 
 
 
pthincicavncrdplocal 
videoquality 
platform 
axim x 
axim x v 
figure video benchmark fullscreen video quality 
 
 
 
 
pthincicavncrdplocal 
videodatasize mb 
platform 
axim x 
axim x v 
figure video benchmark fullscreen video data 
video playback for each system the native player is the 
most bandwidth efficient platform sending less than mb 
of data which corresponds to about mbps of bandwidth 
pthinc s video quality requires about mb of data 
which corresponds to a bandwidth usage of less than mbps 
while the other thin clients send less data than thinc 
they do so because they are dropping video data resulting 
in degraded video quality 
figures to compare screenshots of the different thin 
clients when displaying the video clip except for ica all of 
the screenshots were taken on the x v in landscape mode 
using the maximum display resolution settings for each 
platform given in table the ica screenshot was taken on the 
x since ica does not run on the x v figures and 
show that rdp and vnc are unable to display the entire 
video frame on the pda screen rdp wastes screen space 
for ui elements and vnc only shows the top corner of the 
video frame on the screen figure shows that ica 
provides resizing to display the entire video frame but did not 
proportionally resize the video data resulting in strange 
display artifacts in contrast figure shows pthinc using 
resizing to provide a high quality fullscreen display of the 
entire video frame pthinc provides visually more appealing 
video display than rdp vnc or ica 
 related work 
several studies have examined the web browsing 
performance of thin-client computing the ability for 
thin clients to improve web browsing performance on 
wireless pdas was first quantitatively demonstrated in a 
previous study by one of the authors this study 
demonstrated that thin clients can provide both faster web 
browsing performance and greater web browsing functionality 
the study considered a wide range of web content including 
content from medical information systems our work builds 
on this previous study and consider important issues such as 
how usable existing thin clients are in pda environments 
the trade-offs between thin-client usability and performance 
performance across different pda devices and the 
performance of thin clients on common web-related applications 
such as video 
many thin clients have been developed and some have 
pda clients including microsoft s remote desktop 
citrix metraframe xp virtual network computing 
 gotomypc and tarantella these systems 
were first designed for desktop computing and retrofitted 
for pdas unlike pthinc they do not address key 
system architecture and usability issues important for pdas 
this limits their display quality system performance 
available screen space and overall usability on pdas pthinc 
builds on previous work by two of the authors on thinc 
extending the server architecture and introducing a client 
interface and usage model to efficiently support pda devices 
for mobile web applications 
other approaches to improve the performance of mobile 
wireless web browsing have focused on using transcoding 
and caching proxies in conjunction with the fat client model 
 they work by pushing functionality to external 
proxies and using specialized browsing applications on the 
pda device that communicate with the proxy our 
thinclient approach differs fundamentally from these fat-client 
approaches by pushing all web browser logic to the server 
leveraging existing investments in desktop web browsers and 
helper applications to work seamlessly with production 
systems without any additional proxy configuration or web 
browser modifications 
with the emergence of web browsing on small display 
devices web sites have been redesigned using mechanisms like 
wap and specialized native web browsers have been 
developed to tailor the needs of these devices recently opera 
has developed the opera mini web browser which uses 
an approach similar to the thin-client model to provide 
access across a number of mobile devices that would normally 
be incapable of running a web browser instead of requiring 
the device to process web pages it uses a remote server to 
pre-process the page before sending it to the phone 
 conclusions 
we have introduced pthinc a thin-client architecture 
for wireless pdas pthinc provides key architectural and 
usability mechanisms such as server-side screen resizing 
clientside screen rotation using simple copy techniques yuv video 
support and maximizing screen space for display updates 
and leveraging existing pda control buttons for ui 
elements pthinc transparently supports traditional 
desktop browsers and their helper applications on pda devices 
and desktop machines providing mobile users with 
ubiquitous access to a consistent personalized and full-featured 
web environment across heterogeneous devices we have 
implemented pthinc and measured its performance on 
web applications compared to existing thin-client systems 
and native web applications our results on multiple 
mobile wireless devices demonstrate that pthinc delivers web 
browsing performance up to times better than existing 
thin-client systems and times better than a native pda 
browser in addition pthinc is the only pda thin client 
 
figure video screenshot rdp x figure video screenshot vnc x 
figure video screenshot ica resized x 
figure video screenshot pthinc resized x 
that transparently provides full-screen full frame rate video 
playback making web sites with multimedia content 
accessible to mobile web users 
 acknowledgements 
this work was supported in part by nsf itr grants 
ccr and cns- and an ibm sur award 
 references 
 r baratto l kim and j nieh thinc a virtual 
display architecture for thin-client computing in 
proceedings of the th acm symposium on operating 
systems principles sosp oct 
 citrix metaframe http www citrix com 
 b c cumberland g carius and a muir microsoft 
windows nt server terminal server edition 
technical reference microsoft press redmond wa 
 a fox i goldberg s d gribble and d c lee 
experience with top gun wingman a proxy-based 
graphical web browser for the com palmpilot in 
proceedings of middleware lake district england 
september 
 gotomypc http www gotomypc com 
 health insurance portability and accountability act 
http www hhs gov ocr hipaa 
 i-bench version http 
 etestinglabs com benchmarks i-bench i-bench asp 
 a joshi on proxy agents mobility and web access 
mobile networks and applications - 
 j kangasharju y g kwon and a ortega design and 
implementation of a soft caching proxy computer 
networks and isdn systems - - 
 a lai j nieh b bohra v nandikonda a p surana 
and s varshneya improving web browsing on wireless 
pdas using thin-client computing in proceedings of the 
 th international world wide web conference www 
may 
 a maheshwari a sharma k ramamritham and 
p shenoy transquid transcoding and caching proxy for 
heterogenous ecommerce environments in proceedings of 
the th ieee workshop on research issues in data 
engineering ride feb 
 net vnc viewer for pocketpc 
http dotnetvnc sourceforge net 
 j nieh s j yang and n novik measuring thin-client 
performance using slow-motion benchmarking acm 
trans computer systems - feb 
 j nielsen designing web usability new riders 
publishing indianapolis in 
 opera mini browser 
http www opera com products mobile operamini 
 t richardson q stafford-fraser k r wood and 
a hopper virtual network computing ieee internet 
computing jan feb 
 r w scheifler and j gettys the x window system 
acm trans gr - apr 
 sun secure global desktop 
http www sun com software products sgd 
 s j yang j nieh s krishnappa a mohla and 
m sajjadpour web browsing performance of wireless 
thin-client computing in proceedings of the th 
international world wide web conference www may 
 
 
multi-dimensional range queries in sensor networks∗ 
xin li 
† 
young jin kim 
† 
ramesh govindan 
† 
wei hong 
‡ 
abstract 
in many sensor networks data or events are named by 
attributes many of these attributes have scalar values so one 
natural way to query events of interest is to use a 
multidimensional range query an example is list all events 
whose temperature lies between ◦ 
and ◦ 
 and whose 
light levels lie between and such queries are useful 
for correlating events occurring within the network 
in this paper we describe the design of a distributed 
index that scalably supports multi-dimensional range queries 
our distributed index for multi-dimensional data or dim 
uses a novel geographic embedding of a classical index data 
structure and is built upon the gpsr geographic routing 
algorithm our analysis reveals that under reasonable 
assumptions about query distributions dims scale quite well 
with network size both insertion and query costs scale as 
o 
√ 
n in detailed simulations we show that in practice 
the insertion and query costs of other alternatives are 
sometimes an order of magnitude more than the costs of dims 
even for moderately sized network finally experiments on 
a small scale testbed validate the feasibility of dims 
categories and subject descriptors 
c computer communication networks distributed 
systems c special-purpose and application-based 
systems embedded systems 
general terms 
embedded systems sensor networks storage 
 introduction 
in wireless sensor networks data or events will be named 
by attributes or represented as virtual relations in a 
distributed database many of these attributes will 
have scalar values e g temperature and light levels soil 
moisture conditions etc in these systems we argue one 
natural way to query for events of interest will be to use 
multi-dimensional range queries on these attributes for 
example scientists analyzing the growth of marine 
microorganisms might be interested in events that occurred within 
certain temperature and light conditions list all events 
that have temperatures between ◦ 
f and ◦ 
f and light 
levels between and 
such range queries can be used in two distinct ways they 
can help users efficiently drill-down their search for events of 
interest the query described above illustrates this where 
the scientist is presumably interested in discovering and 
perhaps mapping the combined effect of temperature and 
light on the growth of marine micro-organisms more 
importantly they can be used by application software running 
within a sensor network for correlating events and triggering 
actions for example if in a habitat monitoring application 
a bird alighting on its nest is indicated by a certain range 
of thermopile sensor readings and a certain range of 
microphone readings a multi-dimensional range query on those 
attributes enables higher confidence detection of the arrival 
of a flock of birds and can trigger a system of cameras 
in traditional database systems such range queries are 
supported using pre-computed indices indices trade-off some 
initial pre-computation cost to achieve a significantly more 
efficient querying capability for sensor networks we 
assert that a centralized index for multi-dimensional range 
queries may not be feasible for energy-efficiency reasons as 
well as the fact that the access bandwidth to this central 
index will be limited particularly for queries emanating 
from within the network rather we believe there will 
be situations when it is more appropriate to build an 
innetwork distributed data structure for efficiently answering 
multi-dimensional range queries 
in this paper we present just such a data structure that 
we call a dim 
 dims are inspired by classical database 
indices and are essentially embeddings of such indices within 
the sensor network dims leverage two key ideas in-network 
 
distributed index for multi-dimensional data 
 
data centric storage and a novel locality-preserving 
geographic hash section dims trace their lineage to 
datacentric storage systems the underlying mechanism in 
these systems allows nodes to consistently hash an event to 
some location within the network which allows efficient 
retrieval of events building upon this dims use a technique 
whereby events whose attribute values are close are likely 
to be stored at the same or nearby nodes dims then use 
an underlying geographic routing algorithm gpsr to 
route events and queries to their corresponding nodes in an 
entirely distributed fashion 
we discuss the design of a dim presenting algorithms for 
event insertion and querying for maintaining a dim in the 
event of node failure and for making dims robust to data or 
packet loss section we then extensively evaluate dims 
using analysis section simulation section and actual 
implementation section our analysis reveals that 
under reasonable assumptions about query distributions dims 
scale quite well with network size both insertion and query 
costs scale as o 
√ 
n in detailed simulations we show 
that in practice the event insertion and querying costs of 
other alternatives are sometimes an order of magnitude the 
costs of dims even for moderately sized network 
experiments on a small scale testbed validate the feasibility of 
dims section much work remains including efficient 
support for skewed data distributions existential queries 
and node heterogeneity 
we believe that dims will be an essential but perhaps 
not necessarily the only distributed data structure 
supporting efficient queries in sensor networks dims will be part 
of a suite of such systems that enable feature extraction 
simple range querying exact-match queries or 
continuous queries all such systems will likely be 
integrated to a sensor network database system such as 
tinydb application designers could then choose the 
appropriate method of information access for instance 
a fire tracking application would use dim to detect the 
hotspots and would then use mechanisms that enable 
continuous queries to track the spatio-temporal progress 
of the hotspots finally we note that dims are applicable 
not just to sensor networks but to other deeply distributed 
systems embedded networks for home and factory 
automation as well 
 related work 
the basic problem that this paper addresses - 
multidimensional range queries - is typically solved in database 
systems using indexing techniques the database 
community has focused mostly on centralized indices but distributed 
indexing has received some attention in the literature 
indexing techniques essentially trade-off some data 
insertion cost to enable efficient querying indexing has for long 
been a classical research problem in the database 
community our work draws its inspiration from the class 
of multi-key constant branching index structures 
exemplified by k-d trees where k represents the dimensionality 
of the data space our approach essentially represents a 
geographic embedding of such structures in a sensor field 
there is one important difference the classical indexing 
structures are data-dependent as are some indexing schemes 
that use locality preserving hashes and developed in the 
theory literature the index structure is decided 
not only by the data but also by the order in which data 
is inserted our current design is not data dependent 
finally tangentially related to our work is the class of spatial 
indexing systems 
while there has been some work on distributed indexing 
the problem has not been extensively explored there 
exist distributed indices of a restricted kind-those that allow 
exact match or partial prefix match queries examples of 
such systems of course are the internet domain name 
system and the class of distributed hash table dht systems 
exemplified by freenet chord and can our 
work is superficially similar to can in that both construct 
a zone-based overlay atop of the underlying physical 
network the underlying details make the two systems very 
different can s overlay is purely logical while our overlay 
is consistent with the underlying physical topology more 
recent work in the internet context has addressed support 
for range queries in dht systems but it is unclear if 
these directly translate to the sensor network context 
several research efforts have expressed the vision of a 
database interface to sensor networks and there 
are examples of systems that contribute to this vision 
 our work is similar in spirit to this body of 
literature in fact dims could become an important component 
of a sensor network database system such as tinydb 
our work departs from prior work in this area in two 
significant respects unlike these approaches in our work the data 
generated at a node are hashed in general to different 
locations this hashing is the key to scaling multi-dimensional 
range searches in all the other systems described above 
queries are flooded throughout the network and can 
dominate the total cost of the system our work avoids query 
flooding by an appropriate choice of hashing madden et 
al also describe a distributed index called semantic 
routing trees srt this index is used to direct queries 
to nodes that have detected relevant data our work 
differs from srt in three key aspects first srt is built on 
single attributes while dim supports mulitple attributes 
second srt constructs a routing tree based on historical 
sensor readings and therefore works well only for 
slowlychanging sensor values finally in srt queries are issued 
from a fixed node while in dim queries can be issued from 
any node 
a similar differentiation applies with respect to work on 
data-centric routing in sensor networks where data 
generated at a node is assumed to be stored at the node 
and queries are either flooded throughout the network 
or each source sets up a network-wide overlay announcing its 
presence so that mobile sinks can rendezvous with sources 
at the nearest node on the overlay these approaches 
work well for relatively long-lived queries 
finally our work is most close related to data-centric 
storage systems which include geographic hash-tables 
 ghts dimensions and difs in a ght 
data is hashed by name to a location within the network 
enabling highly efficient rendezvous ghts are built upon the 
gpsr protocol and leverage some interesting properties 
of that protocol such as the ability to route to a node nearest 
to a given location we also leverage properties in gpsr as 
we describe later but we use a locality-preserving hash to 
store data enabling efficient multi-dimensional range queries 
dimensions and difs can be thought of as using the 
same set of primitives as ght storage using consistent 
hashing but for different ends dimensions allows 
drill 
down search for features within a sensor network while 
difs allows range queries on a single key in addition to 
other operations 
 the design of dims 
most sensor networks are deployed to collect data from 
the environment in these networks nodes either 
individually or collaboratively will generate events an event 
can generally be described as a tuple of attribute values 
a a · · · ak where each attribute ai represents a 
sensor reading or some value corresponding to a detection 
 e g a confidence level the focus of this paper is the 
design of systems to efficiently answer multi-dimensional range 
queries of the form x − y x − y · · · xk − yk such a 
query returns all events whose attribute values fall into the 
corresponding ranges notice that point queries i e queries 
that ask for events with specified values for each attribute 
are a special case of range queries 
as we have discussed in section range queries can 
enable efficient correlation and triggering within the network 
it is possible to implement range queries by flooding a query 
within the network however as we show in later sections 
this alternative can be inefficient particularly as the system 
scales and if nodes within the network issue such queries 
relatively frequently the other alternative sending all events 
to an external storage node results in the access link being 
a bottleneck especially if nodes within the network issue 
queries shenker et al also make similar arguments with 
respect to data-centric storage schemes in general dims are 
an instance of such schemes 
the system we present in this paper the dim relies upon 
two foundations a locality-preserving geographic hash and 
an underlying geographic routing scheme 
the key to resolving range queries efficiently is data 
locality i e events with comparable attribute values are stored 
nearby the basic insight underlying dim is that data 
locality can be obtained by a locality-preserving geographic 
hash function our geographic hash function finds a 
localitypreserving mapping from the multi-dimensional space 
 described by the set of attributes to a -d geographic space 
this mapping is inspired by k-d trees and is described 
later moreover each node in the network self-organizes 
to claim part of the attribute space for itself we say that 
each node owns a zone so events falling into that space are 
routed to and stored at that node 
having established the mapping and the zone structure 
dims use a geographic routing algorithm previously 
developed in the literature to route events to their corresponding 
nodes or to resolve queries this algorithm gpsr 
essentially enables the delivery of a packet to a node at a 
specified location the routing mechanism is simple when 
a node receives a packet destined to a node at location x it 
forwards the packet to the neighbor closest to x in gpsr 
this is called greedy-mode forwarding when no such 
neighbor exists as when there exists a void in the network the 
node starts the packet on a perimeter mode traversal 
using the well known right-hand rule to circumnavigate voids 
gpsr includes efficient techniques for perimeter traversal 
that are based on graph planarization algorithms amenable 
to distributed implementation 
for all of this to work dims make two assumptions that 
are consistent with the literature first all nodes know 
the approximate geographic boundaries of the network these 
boundaries may either be configured in nodes at the time of 
deployment or may be discovered using a simple protocol 
second each node knows its geographic location node 
locations can be automatically determined by a localization 
system or by other means 
although the basic idea of dims may seem 
straightforward it is challenging to design a completely distributed 
data structure that must be robust to packet losses and 
node failures yet must support efficient query distribution 
and deal with communication voids and obstacles we now 
describe the complete design of dims 
 zones 
the key idea behind dims as we have discussed is a 
geographic locality-preserving hash that maps a multi-attribute 
event to a geographic zone intuitively a zone is a 
subdivision of the geographic extent of a sensor field 
a zone is defined by the following constructive procedure 
consider a rectangle r on the x-y plane intuitively r is 
the bounding rectangle that contains all sensors withing the 
network we call a sub-rectangle z of r a zone if z is 
obtained by dividing r k times k ≥ using a procedure 
that satisfies the following property 
after the i-th division ≤ i ≤ k r is 
partitioned into i 
equal sized rectangles if i is an 
odd even number the i-th division is parallel 
to the y-axis x-axis 
that is the bounding rectangle r is first sub-divided into 
two zones at level by a vertical line that splits r into two 
equal pieces each of these sub-zones can be split into two 
zones at level by a horizontal line and so on we call the 
non-negative integer k the level of zone z i e level z k 
a zone can be identified either by a zone code code z 
or by an address addr z the code code z is a - bit 
string of length level z and is defined as follows if z lies 
in the left half of r the first from the left bit of code z 
is else if z lies in the bottom half of r the second 
bit of code z is else the remaining bits of code z 
are then recursively defined on each of the four quadrants of 
r this definition of the zone code matches the definition 
of zones given above encoding divisions of the sensor field 
geography by bit strings thus in figure the zone in the 
top-right corner of the rectangle r has a zone code of 
note that the zone codes collectively define a zone tree such 
that individual zones are at the leaves of this tree 
the address of a zone z addr z is defined to be the 
centroid of the rectangle defined by z the two representations 
of a zone its code and its address can each be computed 
from the other assuming the level of the zone is known 
two zones are called sibling zones if their zone codes are 
the same except for the last bit for example if code z 
 and code z then z and z are sibling 
zones the sibling subtree of a zone is the subtree rooted 
at the left or right sibling of the zone in the zone tree we 
uniquely define a backup zone for each zone as follows if 
the sibling subtree of the zone is on the left the backup 
zone is the right-most zone in the sibling subtree 
otherwise the backup zone is the left-most zone in the sibling 
subtree for a zone z let p be the first level z − digits 
of code z let backup z be the backup zone of zone z 
if code z p code backup z p ∗ with the most 
number of trailing s ∗ means or occurrences if 
 
code z p code backup z p ∗ with the most 
number of trailing s 
 associating zones with nodes 
our definition of a zone is independent of the actual 
distribution of nodes in the sensor field and only depends upon 
the geographic extent the bounding rectangle of the sensor 
field now we describe how zones are mapped to nodes 
conceptually the sensor field is logically divided into zones 
and each zone is assigned to a single node if the sensor 
network were deployed in a grid-like i e very regular fashion 
then it is easy to see that there exists a k such that each 
node maps into a distinct level-k zone in general however 
the node placements within a sensor field are likely to be less 
regular than the grid for some k some zones may be empty 
and other zones might have more than one node situated 
within them one alternative would have been to choose 
a fixed k for the overall system and then associate nodes 
with the zones they are in and if a zone is empty associate 
the nearest node with it for some definition of nearest 
because it makes our overall query routing system simpler 
we allow nodes in a dim to map to different-sized zones 
to precisely understand the associations between zones 
and nodes we define the notion of zone ownership for any 
given placement of network nodes consider a node a let 
za to be the largest zone that includes only node a and no 
other node then we say that a owns za notice that this 
definition of ownership may leave some sections of the sensor 
field un-associated with a node for example in figure 
the zone does not contain any nodes and would not have 
an owner to remedy this for any empty zone z we define 
the owner to be the owner of backup z in our example 
that empty zone s owner would also be the node that owns 
 its backup zone 
having defined the association between nodes and zones 
the next problem we tackle is given a node placement does 
there exist a distributed algorithm that enables each node 
to determine which zones it owns knowing only the overall 
boundary of the sensor network in principle this should 
be relatively straightforward since each node can simply 
determine the location of its neighbors and apply simple 
geometric methods to determine the largest zone around it 
such that no other node resides in that zone in practice 
however communication voids and obstacles make the 
algorithm much more challenging in particular resolving the 
ownership of zones that do not contain any nodes is 
complicated equally complicated is the case where the zone 
of a node is larger than its communication radius and the 
node cannot determine the boundaries of its zone by local 
communication alone 
our distributed zone building algorithm defers the 
resolution of such zones until when either a query is initiated or 
when an event is inserted the basic idea behind our 
algorithm is that each node tentatively builds up an idea of the 
zone it resides in just by communicating with its neighbors 
 remembering which boundaries of the zone are undecided 
because there is no radio neighbor that can help resolve that 
boundary these undecided boundaries are later resolved 
by a gpsr perimeter traversal when data messages are 
actually routed 
we now describe the algorithm and illustrate it using 
examples in our algorithm each node uses an array bound 
to maintain the four boundaries of the zone it owns 
 rememfigure a network where circles represent sensor 
nodes and dashed lines mark the network boundary 
 
 
 
 
 
 
 
 
figure the zone code and boundaries 
 
 
 
 
 
 
 
figure the corresponding zone tree 
ber that in this algorithm the node only tries to determine 
the zone it resides in not the other zones it might own 
because those zones are devoid of nodes when a node 
starts up each node initializes this array to be the network 
boundary i e initially each node assumes its zone contains 
the whole network the zone boundary algorithm now 
relies upon gpsr s beacon messages to learn the locations of 
neighbors within radio range upon hearing of such a 
neighbor the node calls the algorithm in figure to update its 
zone boundaries and its code accordingly in this algorithm 
we assume that a is the node at which the algorithm is 
executed za is its zone and a is a newly discovered neighbor 
of a procedure contain za a is used to decide if node 
a is located within the current zone boundaries of node a 
using this algorithm then each node can independently 
and asynchronously decide its own tentative zone based on 
the location of its neighbors figure illustrates the results 
of applying this algorithm for the network in figure 
figure describes the corresponding zone tree each zone 
resides at a leaf node and the code of a zone is the path from 
the root to the zone if we represent the branch to the left 
 
build-zone a 
 while contain za a 
 do if length code za mod 
 then new bound ← bound bound 
 if a x new bound 
 then bound ← new bound 
 else bound ← new bound 
 else new bound ← bound bound 
 if a y new bound 
 then bound ← new bound 
 else bound ← new bound 
 update zone code code za 
figure zone boundary determination where a x 
and a y represent the geographic coordinate of node 
a 
insert-event e 
 c ← encode e 
 if contain za c true and is internal true 
 then store e and exit 
 send-message c e 
send-message c m 
 if ∃ neighbor y closer y owner m m true 
 then addr m ← addr y 
 else if length c length code m 
 then update code m and addr m 
 source m ← caller 
 if is owner msg true 
 then owner m ← caller s code 
 send m 
figure inserting an event in a dim procedure 
closer a b m returns true if code a is closer to 
code m than code b source m is used to set the source 
address of message m 
child by and the branch to the right child by this binary 
tree forms the index that we will use in the following event 
and query processing procedures 
we see that the zone sizes are different and depend on 
the local densities and so are the lengths of zone codes for 
different nodes notice that in figure there is an empty 
zone whose code should be in this case if the node in 
zone can only hear the node in zone it sets its 
boundary with the empty zone to undecided because it did 
not hear from any neighboring nodes from that direction 
as we have mentioned before the undecided boundaries are 
resolved using gpsr s perimeter mode when an event is 
inserted or a query sent we describe event insertion in the 
next step 
finally this description does not describe how a node s 
zone codes are adjusted when neighboring nodes fail or new 
nodes come up we return to this in section 
 inserting an event 
in this section we describe how events are inserted into 
a dim there are two algorithms of interest a consistent 
hashing technique for mapping an event to a zone and a 
routing algorithm for storing the event at the appropriate 
zone as we shall see these two algorithms are inter-related 
 hashing an event to a zone 
in section we described a recursive tessellation of 
the geographic extent of a sensor field we now describe 
a consistent hashing scheme for a dim that supports range 
queries on m distinct attributes 
let us denote these attributes a am for simplicity 
assume for now that the depth of every zone in the network 
is k k is a multiple of m and that this value of k is known 
to every node we will relax this assumption shortly 
furthermore for ease of discussion we assume that all attribute 
values have been normalized to be between and 
our hashing scheme assigns a k bit zone code to an event 
as follows for i between and m if ai the i-th 
bit of the zone code is assigned else for i between 
m and m if ai−m or ai−m ∈ the 
i-th bit of the zone is assigned else because the next 
level divisions are at and which divide the ranges 
to and we repeat 
this procedure until all k bits have been assigned as an 
example consider event e for this event the 
 -bit zone code is code za 
essentially our hashing scheme uses the values of the 
attributes in round-robin fashion on the zone tree such as 
the one in figure in order to map an m-attribute event 
to a zone code this is reminiscent of k-d trees but 
is quite different from that data structure zone trees are 
spatial embeddings and do not incorporate the re-balancing 
algorithms in k-d trees 
in our design of dims we do not require nodes to have 
zone codes of the same length nor do we expect a node to 
know the zone codes of other nodes rather suppose the 
encoding node is a and its own zone code is of length ka 
then given an event e node a only hashes e to a zone 
code of length ka we denote the zone code assigned to an 
event e by code e as we describe below as the event is 
routed code e is refined by intermediate nodes this lazy 
evaluation of zone codes allows different nodes to use 
different length zone codes without any explicit coordination 
 routing an event to its owner 
the aim of hashing an event to a zone code is to store the 
event at the node within the network node that owns that 
zone we call this node the owner of the event consider 
an event e that has just been generated at a node a after 
encoding event e node a compares code e with code a 
if the two are identical node a store event e locally 
otherwise node a will attempt to route the event to its owner 
to do this note that code e corresponds to some zone 
z which is a s current guess for the zone at which event e 
should be stored a now invokes gpsr to send a message 
to addr z the centroid of z section the message 
contains the event e code e and the target geographic 
location for storing the event in the message a also marks 
itself as the owner of event e as we will see later the 
guessed zone z the address addr z and the owner of 
e all of them contained in the message will be refined by 
intermediate forwarding nodes 
gpsr now delivers this message to the next hop towards 
addr z from a this next hop node call it b does not 
immediately forward the message rather it attempts to 
com 
dim does not assume that all nodes are homogeneous in 
terms of the sensors they have thus in an m dimensional 
dim a node that does not possess all m sensors can use null 
values for the corresponding readings dim treats null as 
an extreme value for range comparisons as an aside a 
network may have many dim instances running concurrently 
 
pute a new zone code for e to get a new code codenew e 
b will update the code contained in the message and also 
the geographic destination of the message if codenew e is 
longer than the event code in the message in this manner 
as the event wends its way to its owner its zone code gets 
refined now b compares its own code code b against the 
owner code owner e contained in the incoming message 
if code b has a longer match with code e than the 
current owner owner e then b sets itself to be the current 
owner of e meaning that if nobody is eligible to store e 
then b will store the event we shall see how this happens 
next if b s zone code does not exactly match code e b 
will invoke gpsr to deliver e to the next hop 
 resolving undecided zone boundaries during 
insertion 
suppose that some node say c finds itself to be the 
destination or eventual owner of an event e it does so by 
noticing that code code c equals code e after locally 
recomputing a code for e in that case c stores e locally but 
only if all four of c s zone boundaries are decided when 
this condition holds c knows for sure that no other nodes 
have overlapped zones with it in this case we call c an 
internal node 
recall though that because the zone discovery algorithm 
section only uses information from immediate neighbors 
one or more of c s boundaries may be undecided if so c 
assumes that some other nodes have a zone that overlaps 
with its own and sets out to resolve this overlap to do 
this c now sets itself to be the owner of e and continues 
forwarding the message here we rely on gpsr s 
perimeter mode routing to probe around the void that causes the 
undecided boundary since the message starts from c and 
is destined for a geographic location near c gpsr 
guarantees that the message will be delivered back to c if no 
other nodes will update the information in the message if 
the message comes back to c with itself to be the owner c 
infers that it must be the true owner of the zone and stores 
e locally 
if this does not happen there are two possibilities the 
first is that as the event traverses the perimeter some 
intermediate node say b whose zone overlaps with c s marks 
itself to be the owner of the event but otherwise does not 
change the event s zone code this node also recognizes that 
its own zone overlaps with c s and initiates a message 
exchange which causes each of them to appropriately shrink 
their zone 
figures through show an example of this data-driven 
zone shrinking initially both node a and node b have 
claimed the same zone because they are out of radio range 
of each other suppose that a inserts an event e 
a encodes e to and claims itself to be the owner of e 
since a is not an internal node it sends out e looking for 
other owner candidates of e once e gets to node b b will 
see in the message s owner field a s code that is the same as 
its own b then shrinks its zone from to according to 
a s location which is also recorded in the message and send 
a shrink request to a upon receiving this request a also 
shrinks its zone from to 
a second possibility is if some intermediate node changes 
the destination code of e to a more specific value i e 
longer zone code let us label this node d d now tries 
to initiate delivery to the centroid of the new zone this 
a 
b 
 
 
 
 
 
 
 
figure nodes a and b have claimed the same zone 
a 
b 
 
figure an event query message filled arrows 
triggers zone shrinking hollow arrows 
a 
b 
 
 
 
 
 
 
 
figure the zone layout after shrinking now node 
a and b have been mapped to different zones 
might result in a new perimeter walk that returns to d if 
for example d happens to be geographically closest to the 
centroid of the zone however d would not be the owner 
of the event which would still be c in routing to the 
centroid of this zone the message may traverse the perimeter 
and return to d now d notices that c was the original 
owner so it encapsulates the event and directs it to c in 
case that there indeed is another node say x that owns 
an overlapped zone with c x will notice this fact by 
finding in the message the same prefix of the code of one of 
its zones but with a different geographic location from its 
own x will shrink its zone to resolve the overlap if x s 
zone is smaller than or equal to c s zone x will also send 
a shrink request to c once c receives a shrink request 
it will reduce its zone appropriately and fix its undecided 
boundary in this manner the zone formation process is 
resolved on demand in a data-driven way 
 
there are several interesting effects with respect to 
perimeter walking that arise in our algorithm the first is that 
there are some cases where an event insertion might cause 
the entire outer perimeter of the network to be traversed 
 
figure also works as an example where the outer 
perimeter is traversed event e inserted by a will eventually be 
stored in node b before node b stores event e if b s 
nominal radio range does not intersect the network boundary it 
needs to send out e again as a did because b in this case 
is not an internal node but if b s nominal radio range 
intersects the network boundary it then has two choices it 
can assume that there will not be any nodes outside the 
network boundary and so b is an internal node this is an 
aggressive approach on the other hand b can also make 
a conservative decision assuming that there might be some 
other nodes it have not heard of yet b will then force the 
message walking another perimeter before storing it 
in some situations especially for large zones where the 
node that owns a zone is far away from the centroid of the 
owned zone there might exist a small perimeter around the 
destination that does not include the owner of the zone the 
event will end up being stored at a different node than the 
real owner in order to deal with this problem we add an 
extra operation in event forwarding called efficient neighbor 
discovery before invoking gpsr a node needs to check if 
there exists a neighbor who is eligible to be the real owner of 
the event to do this a node c say needs to know the zone 
codes of its neighboring nodes we deploy gpsr s 
beaconing message to piggyback the zone codes for nodes so by 
simply comparing the event s code and neighbor s code a 
node can decide whether there exists a neighbor y which 
is more likely to be the owner of event e c delivers e 
to y which simply follows the decision making procedure 
discussed above 
 summary and pseudo-code 
in summary our event insertion procedure is designed to 
nicely interact with the zone discovery mechanism and the 
event hashing mechanism the latter two mechanisms are 
kept simple while the event insertion mechanism uses lazy 
evaluation at each hop to refine the event s zone code and it 
leverages gpsr s perimeter walking mechanism to fix 
undecided zone boundaries in section we address robustness 
of event insertion to packet loss or to node failures 
figure shows the pseudo-code for inserting and 
forwarding an event e in this pseudo code we have omitted a 
description of the zone shrinking procedure in the pseudo 
code procedure is internal is used to determine if the 
caller is an internal node and procedure is owner is used 
to determine if the caller is more eligible to be the owner of 
the event than is currently claimed owner as recorded in the 
message procedure send-message is used to send either 
an event message or a query message if the message 
destination address has been changed the packet source address 
needs also to be changed in order to avoid being dropped by 
gpsr since gpsr does not allow a node to see the same 
packet in greedy mode twice 
 
this happens less frequently than for ghts where 
inserting an event to a location outside the actual but inside 
the nominal boundary of the network will always invoke an 
external perimeter walk 
 resolving and routing queries 
dims support both point queries 
and range queries 
routing a point query is identical to routing an event thus the 
rest of this section details how range queries are routed 
the key challenge in routing zone queries is brought out 
by the following strawman design if the entire network was 
divided evenly into zones of depth k for some pre-defined 
constant k then the querier the node issuing the query 
could subdivide a given range query into the relevant 
subzones and route individual requests to each of the zones 
this can be inefficient for large range queries and also hard 
to implement in our design where zone sizes are not 
predefined accordingly we use a slightly different technique 
where a range query is initially routed to a zone 
corresponding to the entire range and is then progressively split into 
smaller subqueries we describe this algorithm here 
the first step of the algorithm is to map a range query to 
a zone code prefix conceptually this is easy in a zone tree 
 figure there exists some node which contains the entire 
range query in its sub-tree and none of its children in the 
tree do the initial zone code we choose for the query is the 
zone code corresponding to that tree node and is a prefix of 
the zone codes of all zones note that these zones may not 
be geographically contiguous in the subtree the querier 
computes the zone code of q denoted by code q and then 
starts routing a query to addr code q 
upon receiving a range query q a node a where a is any 
node on the query propagation path divides it into multiple 
smaller sized subqueries if there is an overlap between the 
zone of a zone a and the zone code associated with q 
code q our approach to split a query q into subqueries 
is as follows if the range of q s first attribute contains 
the value a divides q into two sub-queries one of whose 
first attribute ranges from to and the other from to 
 then a decides the half that overlaps with its own zone 
let s call it qa if qa does not exist then a stops splitting 
otherwise it continues splitting using the second attribute 
range and recomputing qa until qa is small enough so 
that it completely falls into zone a and hence a can now 
resolve it for example suppose that node a whose code 
is is to split a range query q − − 
the splitting steps is shown in figure after splitting 
we obtain three smaller queries q − − 
q − − and q − − 
this splitting procedure is illustrated in figure which 
also shows the codes of each subquery after splitting 
a then replies to subquery q with data stored locally 
and sends subqueries q and q using the procedure outlined 
above more generally if node a finds itself to be inside 
the zone subtree that maximally covers q it will send the 
subqueries that resulted from the split otherwise if there 
is no overlap between a and q then a forwards q as is in 
this case q is either the original query or a product of an 
earlier split 
figure describes the pseudo-code for the zone splitting 
algorithm as shown in the above algorithm once a 
subquery has been recognized as belonging to the caller s zone 
procedure resolve is invoked to resolve the subquery and 
send a reply to the querier every query message contains 
 
by point queries we mean the equality condition on all 
indexed keys dim index attributes are not necessarily 
primary keys 
 
the geographic location of its initiator so the corresponding 
reply message can be delivered directly back to the 
initiator finally in the process of query resolution zones might 
shrink similar to shrinkage during inserting we omit this 
in the pseudo code 
 robustness 
until now we have not discussed the impact of node 
failures and packet losses or node arrivals and departures on 
our algorithms packet losses can affect query and event 
insertion and node failures can result in lost data while node 
arrivals and departures can impact the zone structure we 
now discuss how dims can be made robust to these kinds 
of dynamics 
 maintaining zones 
in previous sections we described how the zone discovery 
algorithm could leave zone boundaries undecided these 
undecided boundaries are resolved during insertion or 
querying using the zone shrinking procedure describe above 
when a new node joins the network the zone discovery 
mechanism section will cause neighboring zones to 
appropriately adjust their zone boundaries at this time those 
zones can also transfer to the new node those events they 
store but which should belong to the new node 
before a node turns itself off if this is indeed possible it 
knows that its backup node section will take over its 
zone and will simply send all its events to its backup node 
node deletion may also cause zone expansion in order to 
keep the mapping between the binary zone tree s leaf nodes 
and zones we allow zone expansion to only occur among 
sibling zones section the rule is if zone a s sibling 
zone becomes empty then a can expand its own zone to 
include its sibling zone 
now we turn our attention to node failures node failures 
are just like node deletions except that a failed node does 
not have a chance to move its events to another node but 
how does a node decide if its sibling has failed if the 
sibling is within radio range the absence of gpsr beaconing 
messages can detect this once it detects this the node can 
expand its zone a different approach is needed for 
detecting siblings who are not within radio range these are the 
cases where two nodes own their zones after exchanging a 
shrink message they do not periodically exchange messages 
thereafter to maintain this zone relationship in this case 
we detect the failure in a data-driven fashion with obvious 
efficiency benefits compared to periodic keepalives once a 
node b has failed an event or query message that previously 
should have been owned by the failed node will now be 
delivered to the node a that owns the empty zone left by node 
b a can see this message because a stands right around 
the empty area left by b and is guaranteed to be visited in a 
gpsr perimeter traversal a will set itself to be the owner 
of the message and any node which would have dropped this 
message due to a perimeter loop will redirect the message to 
a instead if a s zone happens to be the sibling of b s zone 
a can safely expand its own zone and notify its expanded 
zone to its neighbors via gpsr beaconing messages 
 preventing data loss from node failure 
the algorithms described above are robust in terms of 
zone formation but node failure can erase data to avoid 
this dims can employ two kinds of replication local 
replication to be resilient to random node failures and mirror 
replication for resilience to concurrent failure of 
geographically contiguous nodes 
mirror replication is conceptually easy suppose an event 
e has a zone code code e then the node that inserts 
e would store two copies of e one at the zone denoted 
by code e and the other at the zone corresponding to the 
one s complement of code e this technique essentially 
creates a mirror dim a querier would need in parallel to 
query both the original dim and its mirror since there is no 
way of knowing if a collection of nodes has failed clearly 
the trade-off here is an approximate doubling of both 
insertion and query costs 
there exists a far cheaper technique to ensure resilience 
to random node failures our local replication technique 
rests on the observation that for each node a there exists 
a unique node which will take over its zone when a fails 
this node is defined as the node responsible for a s zone s 
backup zone see section the basic idea is that a 
replicates each data item it has in this node we call this 
node a s local replica let a s local replica be b often 
b will be a radio neighbor of a and can be detected from 
gpsr beacons sometimes however this is not the case 
and b will have to be explicitly discovered 
we use an explicit message for discovering the local replica 
discovering the local replica is data-driven and uses a 
mechanism similar to that of event insertion node a sends a 
message whose geographic destination is a random nearby 
location chosen by a the location is close enough to a such 
that gpsr will guarantee that the message will delivered 
back to a in addition the message has three fields one for 
the zone code of a code a one for the owner owner a of 
zone a which is set to be empty and one for the geographic 
location of owner a then the packet will be delivered in 
gpsr perimeter mode each node that receives this 
message will compare its zone code and code a in the message 
and if it is more eligible to be the owner of zone a than 
the current owner a recorded in the message it will 
update the field owner a and the corresponding geographic 
location once the packet comes back to a it will know the 
location of its local replica and can start to send replicas 
in a dense sensor network the local replica of a node 
is usually very near to the node either its direct neighbor 
or - hops away so the cost of sending replicas to local 
replication will not dominate the network traffic however 
a node s local replica itself may fail there are two ways to 
deal with this situation periodic refreshes or repeated 
datadriven discovery of local replicas the former has higher 
overhead but more quickly discovers failed replicas 
 robustness to packet loss 
finally the mechanisms for querying and event insertion 
can be easily made resilient to packet loss for event 
insertion a simple ack scheme suffices 
of course queries and responses can be lost as well in 
this case there exists an efficient approach for error 
recovery this rests on the observation that the querier knows 
which zones fall within its query and should have responded 
 we assume that a node that has no data matching a query 
but whose zone falls within the query responds with a 
negative acknowledgment after a conservative timeout the 
querier can re-issue the queries selectively to these zones 
if dim cannot get any answers positive or negative from 
 
 - - 
 - - - - 
 - - 
 - - 
 - - - - 
 
 
 
 
 
 
figure an example of range query splitting 
resolve-range-query q 
 qsub ← nil 
 q qsub ← split-query q 
 if q nil 
 then c ← encode q 
 if contain c code a true 
 then go to step 
 else send-message c q 
 else resolve q 
 if is internal true 
 then absorb q 
 else append q to qsub 
 if qsub nil 
 then for each subquery q ∈ qsub 
 do c ← encode q 
 send-message c q 
figure query resolving algorithm 
certain zones after repeated timeouts it can at least return 
the partial query results to the application together with the 
information about the zones from which data is missing 
 dims an analysis 
in this section we present a simple analytic performance 
evaluation of dims and compare their performance against 
other possible approaches for implementing multi-dimensional 
range queries in sensor networks in the next section we 
validate these analyses using detailed packet-level simulations 
our primary metrics for the performance of a dim are 
average insertion cost measures the average number of 
messages required to insert an event into the network 
average query delivery cost measures the average 
number of messages required to route a query message to 
all the relevant nodes in the network 
it does not measure the number of messages required to 
transmit responses to the querier this latter number 
depends upon the precise data distribution and is the same 
for many of the schemes we compare dims against 
in dims event insertion essentially uses geographic 
routing in a dense n-node network where the likelihood of 
traversing perimeters is small the average event insertion 
cost proportional to 
√ 
n 
on the other hand the query delivery cost depends upon 
the size of ranges specified in the query recall that our 
query delivery mechanism is careful about splitting a query 
into sub-queries doing so only when the query nears the 
zone that covers the query range thus when the querier is 
far from the queried zone there are two components to the 
query delivery cost the first which is proportional to 
√ 
n 
is the cost to deliver the query near the covering zone if 
within this covering zone there are m nodes the message 
delivery cost of splitting the query is proportional to m 
the average cost of query delivery depends upon the 
distribution of query range sizes now suppose that query sizes 
follow some density function f x then the average cost of 
resolve a query can be approximated by 
ê n 
 
xf x dx to 
give some intuition for the performance of dims we 
consider four different forms for f x the uniform distribution 
where a query range encompassing the entire network is as 
likely as a point query a bounded uniform distribution where 
all sizes up to a bound b are equally likely an algebraic 
distribution in which most queries are small but large queries 
are somewhat likely and an exponential distribution where 
most queries are small and large queries are unlikely in all 
our analyses we make the simplifying assumption that the 
size of a query is proportional to the number of nodes that 
can answer that query 
for the uniform distribution p x ∝ c for some constant c 
if each query size from n is equally likely the average 
query delivery cost of uniformly distributed queries is o n 
thus for uniformly distributed queries the performance of 
dims is comparable to that of flooding however for the 
applications we envision where nodes within the network 
are trying to correlate events the uniform distribution is 
highly unrealistic 
somewhat more realistic is a situation where all query 
sizes are bounded by a constant b in this case the average 
cost for resolving such a query is approximately 
ê b 
 
xf x dx 
o b recall now that all queries have to pay an 
approximate cost of o 
√ 
n to deliver the query near the covering 
zone thus if dim limited queries to a size proportional to√ 
n the average query cost would be o 
√ 
n 
the algebraic distribution where f x ∝ x−k 
 for some 
constant k between and has an average query resolution 
cost given by 
ê n 
 
xf x dx o n −k 
 in this case if k 
 the average cost of query delivery is dominated by the 
cost to deliver the query to near the covering zone given by 
o 
√ 
n 
finally for the exponential distribution f x ce−cx 
for 
some constant c and the average cost is just the mean of the 
corresponding distribution i e o for large n 
asymptotically then the cost of the query for the exponential 
distribution is dominated by the cost to deliver the query 
near the covering zone o 
√ 
n 
thus we see that if queries follow either the bounded 
uniform distribution the algebraic distribution or the 
exponential distribution the query cost scales as the insertion 
cost for appropriate choice of constants for the bounded 
uniform and the algebraic distributions 
how well does the performance of dims compare against 
alternative choices for implementing multi-dimensional queries 
a simple alternative is called external storage where all 
events are stored centrally in a node outside the sensor 
network this scheme incurs an insertion cost of o 
√ 
n and 
a zero query cost however as points out such systems 
may be impractical in sensor networks since the access link 
to the external node becomes a hotspot 
a second alternative implementation would store events 
at the node where they are generated queries are flooded 
 
throughout the network and nodes that have matching data 
respond examples of systems that can be used for this 
 although to our knowledge these systems do not implement 
multi-dimensional range queries are directed diffusion 
and tinydb the flooding scheme incurs a zero 
insertion cost but an o n query cost it is easy to show that 
dims outperform flooding as long as the ratio of the number 
of insertions to the number of queries is less than 
√ 
n 
a final alternative would be to use a geographic hash table 
 ght in this approach attribute values are assumed 
to be integers this is actually quite a reasonable 
assumption since attribute values are often quantized and events 
are hashed on some say the first attribute a range query 
is sub-divided into several sub-queries one for each integer 
in the range of the first attribute each sub-query is then 
hashed to the appropriate location the nodes that receive a 
sub-query only return events that match all other attribute 
ranges in this approach which we call ght-r ght s for 
range queries the insertion cost is o 
√ 
n suppose that 
the range of the first attribute contains r discrete values 
then the cost to deliver queries is o r 
√ 
n thus 
asymptotically ght-r s perform similarly to dims in practice 
however the proportionality constants are significantly 
different and dims outperform ght-rs as we shall show 
using detailed simulations 
 dims simulation results 
our analysis gives us some insight into the asymptotic 
behavior of various approaches for multi-dimensional range 
queries in this section we use simulation to compare dims 
against flooding and ght-r this comparison gives us a 
more detailed understanding of these approaches for 
moderate size networks and gives us a nuanced view of the 
mechanistic differences between some of these approaches 
 simulation methodology 
we use ns- for our simulations since dims are 
implemented on top of gpsr we first ported an earlier gpsr 
implementation to the latest version of ns- we modified 
the gpsr module to call our dim implementation when 
it receives any data message in transit or when it is about 
to drop a message because that message traversed the entire 
perimeter this allows a dim to modify message zone codes 
in flight section and determine the actual owner of an 
event or query 
in addition to this we implemented in ns- most of the 
dim mechanisms described in section of those 
mechanisms the only one we did not implement is mirror 
replication we have implemented selective query retransmission 
for resiliency to packet loss but have left the evaluation of 
this mechanism to future work our dim implementation 
in ns- is lines of code 
finally we implemented ght-r our ght-based 
multidimensional range query mechanism in ns- this 
implementation was relatively straightforward given that we had 
ported gpsr and modified gpsr to detect the completion 
of perimeter mode traversals 
using this implementation we conducted a fairly 
extensive evaluation of dim and two alternatives flooding and 
our ght-r for all our experiments we use uniformly 
placed sensor nodes with network sizes ranging from 
nodes to nodes each node has a radio range of m 
for the results presented here each node has on average 
nodes within its nominal radio range we have conducted 
experiments at other node densities they are in agreement 
with the results presented here 
in all our experiments each node first generates events 
on average more precisely for a topology of size n we have 
 n events and each node is equally likely to generate an 
event we have conducted experiments for three different 
event value distributions our uniform event distribution 
generates -dimensional events and for each dimension 
every attribute value is equally likely our normal event 
distribution generates -dimensional events and for each 
dimension the attribute value is normally distributed with a 
mean corresponding to the mid-point of the attribute value 
range the normal event distribution represents a skewed 
data set finally our trace event distribution is a collection 
of -dimensional events obtained from a habitat monitoring 
network as we shall see this represents a fairly skewed 
data set 
having generated events for each simulation we 
generate queries such that on average each node generates 
queries the query sizes are determined using the four size 
distributions we discussed in section uniform 
boundeduniform algebraic and exponential once a query size has 
been determined the location of the query i e the actual 
boundaries of the zone are uniformly distributed for our 
ght-r experiments the dynamic range of the attributes 
had discrete values but we restricted the query range 
for any one attribute to discrete values to allow those 
simulations to complete in reasonable time 
finally using one set of simulations we evaluate the 
efficacy of local replication by turning off random fractions of 
nodes and measuring the fidelity of the returned results 
the primary metrics for our simulations are the average 
query and insertion costs as defined in section 
 results 
although we have examined almost all the combinations 
of factors described above we discuss only the most salient 
ones here for lack of space 
figure plots the average insertion costs for dim and 
ght-r for flooding of course the insertion costs are zero 
dim incurs less per event overhead in inserting events 
 regardless of the actual event distribution figure shows the 
cost for uniformly distributed events the reason for this is 
interesting in ght-r storing almost every event incurs a 
perimeter traversal and storing some events require 
traversing the outer perimeter of the network by contrast in 
dim storing an event incurs a perimeter traversal only when 
a node s boundaries are undecided furthermore an 
insertion or a query in a dim can traverse the outer perimeter 
 section but less frequently than in ghts 
figure plots the average query cost for a bounded 
uniform query size distribution for this graph and the next 
we use a uniform event distribution since the event 
distribution does not affect the query delivery cost for this 
simulation our bound was 
 
th the size of the largest possible 
 
our metrics are chosen so that the exact number of events 
and queries is unimportant for our discussion of course 
the overall performance of the system will depend on the 
relative frequency of events and queries as we discuss in 
section since we don t have realistic ratios for these we 
focus on the microscopic costs rather than on the overall 
system costs 
 
 
 
 
 
 
 
 
 
 
 
 
 
averagecostperinsertion 
network size 
dim 
ght-r 
figure average insertion cost for dim and 
ght 
 
 
 
 
 
 
 
 
fractionofrepliescomparedwithnon-failurecase 
fraction of failed nodes 
no replication 
local replication 
figure local replication performance 
query e g a query of the form − − even for 
this generous query size dims perform quite well almost 
a third the cost of flooding notice however that 
ghtrs incur high query cost since almost any query requires as 
many subqueries as the width of the first attribute s range 
figure plots the average query cost for the exponential 
distribution the average query size for this distribution was 
set to be 
 
th the largest possible query the superior 
scaling of dims is evident in these graphs clearly this is 
the regime in which one might expect dims to perform best 
when most of the queries are small and large queries are 
relatively rare this is also the regime in which one would 
expect to use multi-dimensional range queries to perform 
relatively tight correlations as with the bounded uniform 
distribution ght query cost is dominated by the cost of 
sending sub-queries for dims the query splitting strategy 
works quite well in keep overall query delivery costs low 
figure describes the efficacy of local replication to 
obtain this figure we conducted the following experiment 
on a -node network we inserted a number of events 
uniformly distributed throughout the network then issued 
a query covering the entire network and recorded the 
answers knowing the expected answers for this query we 
then successively removed a fraction f of nodes randomly 
and re-issued the same query the figure plots the fraction 
of expected responses actually received with and without 
replication as the graph shows local replication performs 
well for random failures returning almost of the 
responses when up to of the nodes have failed 
simultaneously 
 in the absence of local replication of course when 
 
in practice the performance of local replication is likely to 
 
 
 
 
 
 
 
 
 
averagecostperqueryinboundedunifdistribution 
network size 
dim 
flooding 
ght-r 
figure average query cost with a bounded 
uniform query distribution 
 
 
 
 
 
 
 
 
 
 
 
averagecostperqueryinexponentialdistribution 
network size 
dim 
flooding 
ght-r 
figure average query cost with an exponential 
query distribution 
 of the nodes fail the response rate is only as one 
would expect 
we note that dims as currently designed are not 
perfect when the data is highly skewed-as it was for our trace 
data set from the habitat monitoring application where most 
of the event values fell into within of the attribute s 
range-a few dim nodes will clearly become the bottleneck 
this is depicted in figure which shows that for dims 
and ght-rs the maximum number of transmissions at any 
network node the hotspots is rather high for less skewed 
data distributions and reasonable query size distributions 
the hotspot curves for all three schemes are comparable 
this is a standard problem that the database indices have 
dealt with by tree re-balancing in our case simpler 
solutions might be possible and we discuss this in section 
however our use of the trace data demonstrates that 
dims work for events which have more than two dimensions 
increasing the number of dimensions does not noticeably 
degrade dims query cost omitted for lack of space 
also omitted are experiments examining the impact of 
several other factors as they do not affect our conclusions 
in any way as we expected dims are comparable in 
performance to flooding when all sizes of queries are equally 
likely for an algebraic distribution of query sizes the 
relative performance is close to that for the exponential 
distribution for normally distributed events the insertion costs 
be much better than this assuming a node and its replica 
don t simultaneously fail often a node will almost always 
detect a replica failure and re-replicate leading to near 
response rates 
 
 
 
 
 
 
 
 
 
maximumhotspotontracedataset 
network size 
dim 
flooding 
ght-r 
figure hotspot usage 
dim 
zone 
manager 
query 
router 
query 
processor 
event 
manager 
event 
router 
gpsr interface event driven thread based 
update 
useuse 
update 
gpsr 
upper interface event driven thread based 
lower interface event driven thread based 
greedy 
forwarding 
perimeter 
forwarding 
beaconing 
neighbor 
list 
manager 
update 
use 
motenic micaradio ip socket b ethernet 
figure software architecture of dim over gpsr 
are comparable to that for the uniform distribution 
finally we note that in all our evaluations we have only 
used list queries those that request all events matching the 
specified range we expect that for summary queries those 
that expect an aggregate over matching events the overall 
cost of dims could be lower because the matching data are 
likely to be found in one or a small number of zones we 
leave an understanding of this to future work also left to 
future work is a detailed understanding of the impact of 
location error on dim s mechanisms recent work has 
examined the impact of imprecise location information on 
other data-centric storage mechanisms such as ghts and 
found that there exist relatively simple fixes to gpsr that 
ameliorate the effects of location error 
 implementation 
we have implemented dims on a linux platform suitable 
for experimentation on pdas and pc- class machines 
to implement dims we had to develop and test an 
independent implementation of gpsr our gpsr implementation 
is full-featured while our dim implementation has most of 
the algorithms discussed in section some of the robustness 
extensions have only simpler variants implemented 
the software architecture of dim gpsr system is shown 
in figure the entire system about lines of code 
is event-driven and multi-threaded the dim subsystem 
consists of six logical components zone management event 
maintenance event routing query routing query 
processing and gpsr interactions the gpsr system is 
implemented as user-level daemon process applications are 
executed as clients for the dim subsystem the gpsr module 
 
 
 
 
 
 
 
 
 
 x x x x 
query size 
average ofreceivedresponses 
perquery 
figure number of events received for different 
query sizes 
 
 
 
 
 
 
 
 
 
 x x x x 
query sizetotalnumberofmessages 
onlyforsendingthequery 
figure query distribution cost 
provides several extensions it exports information about 
neighbors and provides callbacks during packet forwarding 
and perimeter-mode termination 
we tested our implementation on a testbed consisting of 
pc- class machines each of these boxes runs linux and 
uses a mica mote attached through a serial cable for 
communication these boxes are laid out in an office building 
with a total spatial separation of over a hundred feet we 
manually measured the locations of these nodes relative to 
some coordinate system and configured the nodes with their 
location the network topology is approximately a chain 
on this testbed we inserted queries and events from a 
single designated node our events have two attributes which 
span all combinations of the four values 
 sixteen events in all our queries span four sizes returning 
 and events respectively 
figure plots the number of events received for different 
sized queries it might appear that we received fewer events 
than expected but this graph doesn t count the events that 
were already stored at the querier with that adjustment 
the number of responses matches our expectation finally 
figure shows the total number of messages required for 
different query sizes on our testbed 
while these experiments do not reveal as much about the 
performance range of dims as our simulations do they 
nevertheless serve as proof-of-concept for dims our next step 
in the implementation is to port dims to the mica motes 
and integrate them into the tinydb sensor database 
engine on motes 
 
 conclusions 
in this paper we have discussed the design and evaluation 
of a distributed data structure called dim for efficiently 
resolving multi-dimensional range queries in sensor networks 
our design of dims relies upon a novel locality-preserving 
hash inspired by early work in database indexing and is 
built upon gpsr we have a working prototype both of 
gpsr and dim and plan to conduct larger scale 
experiments in the future 
there are several interesting future directions that we 
intend to pursue one is adaptation to skewed data 
distributions since these can cause storage and transmission 
hotspots unlike traditional database indices that re-balance 
trees upon data insertion in sensor networks it might be 
feasible to re-structure the zones on a much larger timescale 
after obtaining a rough global estimate of the data 
distribution another direction is support for node heterogeneity 
in the zone construction process nodes with larger storage 
space assert larger-sized zones for themselves a third is 
support for efficient resolution of existential queries-whether 
there exists an event matching a multi-dimensional range 
acknowledgments 
this work benefited greatly from discussions with fang bian 
hui zhang and other enl lab members as well as from 
comments provided by the reviewers and our shepherd feng 
zhao 
 references 
 j aspnes and g shah skip graphs in proceedings of 
 th annual acm-siam symposium on discrete 
algorithms soda baltimore md january 
 j l bentley multidimensional binary search trees used 
for associative searching communicaions of the acm 
 - 
 p bonnet j e gerhke and p seshadri towards sensor 
database systems in proceedings of the second 
international conference on mobile data management 
hong kong january 
 i clarke o sandberg b wiley and t w hong freenet 
a distributed anonymous information storage and 
retrieval system in designing privacy enhancing 
technologies international workshop on design issues in 
anonymity and unobservability springer new york 
 d comer the ubiquitous b-tree acm computing 
surveys - 
 r a finkel and j l bentley quad trees a data 
structure for retrieval on composite keys acta 
informatica - 
 d ganesan d estrin and j heidemann dimensions 
why do we need a new data handling architecture for 
sensor networks in proceedings of the first workshop on 
hot topics in networks hotnets-i princeton nj 
october 
 a gionis p indyk and r motwani similarity search in 
high dimensions via hashing in proceedings of the th 
vldb conference edinburgh scotland september 
 r govindan j hellerstein w hong s madden 
m franklin and s shenker the sensor network as a 
database technical report - computer science 
department university of southern california september 
 
 b greenstein d estrin r govindan s ratnasamy and 
s shenker difs a distributed index for features in 
sensor networks in proceedings of st ieee international 
workshop on sensor network protocols and applications 
anchorage ak may 
 a guttman r-trees a dynamic index structure for 
spatial searching in proceedings of the acm sigmod 
boston ma june 
 m harren j m hellerstein r huebsch b t loo 
s shenker and i stoica complex queries in dht-based 
peer-to-peer networks in p druschel f kaashoek and 
a rowstron editors proceedings of st international 
workshop on peer-to-peer systems iptps volume 
 of lncs page cambridge ma march 
springer-verlag 
 p indyk and r motwani approximate nearest neighbors 
towards removing the curse of dimensionality in 
proceedings of the th annual acm symposium on 
theory of computing dallas texas may 
 p indyk r motwani p raghavan and s vempala 
locality-preserving hashing in multidimensional spaces in 
proceedings of the th annual acm symposium on 
theory of computing pages - el paso texas 
may acm press 
 c intanagonwiwat r govindan and d estrin directed 
diffusion a scalable and robust communication 
paradigm for sensor networks in proceedings of the sixth 
annual acm ieee international conference on mobile 
computing and networking mobicom boston ma 
august 
 b karp and h t kung gpsr greedy perimeter 
stateless routing for wireless networks in proceedings of 
the sixth annual acm ieee international conference on 
mobile computing and networking mobicom 
boston ma august 
 s madden m franklin j hellerstein and w hong the 
design of an acquisitional query processor for sensor 
networks in proceedings of acm sigcmod san diego 
ca june 
 s madden m j franklin j m hellerstein and w hong 
tag a tiny agregation service for ad-hoc sensor 
networks in proceedings of th annual symposium on 
operating systems design and implementation osdi 
boston ma december 
 s ratnasamy p francis m handley r karp and 
s shenker a scalable content-addressable network in 
proceedings of the acm sigcomm san diego ca 
august 
 s ratnasamy b karp l yin f yu d estrin 
r govindan and s shenker ght a geographic hash 
table for data-centric storage in proceedings of the first 
acm international workshop on wireless sensor 
networks and applications atlanta ga september 
 h samet spatial data structures in w kim editor 
modern database systems the object model 
interoperability and beyond pages - addison 
wesley acm 
 k sead a helmy and r govindan on the effect of 
localization errors on geographic face routing in sensor 
networks in under submission 
 s shenker s ratnasamy b karp r govindan and 
d estrin data-centric storage in sensornets in proc 
acm sigcomm workshop on hot topics in networks 
princeton nj 
 i stoica r morris d karger m f kaashoek and 
h balakrishnan chord a scalable peer-to-peer lookup 
service for internet applications in proceedings of the 
acm sigcomm san diego ca august 
 f ye h luo j cheng s lu and l zhang a two-tier 
data dissemination model for large-scale wireless sensor 
networks in proceedings of the eighth annual acm ieee 
international conference on mobile computing and 
networking mobicom atlanta ga september 
 
adaptive duty cycling for energy harvesting systems 
jason hsu sadaf zahedi aman kansal mani srivastava 
electrical engineering department 
university of california los angeles 
{jasonh kansal szahedi mbs}   ee ucla edu 
vijay raghunathan 
nec labs america 
princeton nj 
vijay nec-labs com 
abstract 
harvesting energy from the environment is feasible in many 
applications to ameliorate the energy limitations in sensor networks in 
this paper we present an adaptive duty cycling algorithm that allows 
energy harvesting sensor nodes to autonomously adjust their duty 
cycle according to the energy availability in the environment the 
algorithm has three objectives namely a achieving energy neutral 
operation i e energy consumption should not be more than the energy 
provided by the environment b maximizing the system performance 
based on an application utility model subject to the above 
energyneutrality constraint and c adapting to the dynamics of the energy 
source at run-time we present a model that enables harvesting sensor 
nodes to predict future energy opportunities based on historical data 
we also derive an upper bound on the maximum achievable 
performance assuming perfect knowledge about the future behavior of 
the energy source our methods are evaluated using data gathered from 
a prototype solar energy harvesting platform and we show that our 
algorithm can utilize up to more environmental energy compared 
to the case when harvesting-aware power management is not used 
categories and subject descriptors 
c computer systems organization computer 
communication networks-distributed systems 
general terms 
algorithms design 
 introduction 
energy supply has always been a crucial issue in designing 
battery-powered wireless sensor networks because the lifetime and 
utility of the systems are limited by how long the batteries are able to 
sustain the operation the fidelity of the data produced by a sensor 
network begins to degrade once sensor nodes start to run out of 
battery power therefore harvesting energy from the environment 
has been proposed to supplement or completely replace battery 
supplies to enhance system lifetime and reduce the maintenance cost 
of replacing batteries periodically 
however metrics for evaluating energy harvesting systems are 
different from those used for battery powered systems 
environmental energy is distinct from battery energy in two ways 
first it is an inexhaustible supply which if appropriately used can 
allow the system to last forever unlike the battery which is a limited 
resource second there is an uncertainty associated with its 
availability and measurement compared to the energy stored in the 
battery which can be known deterministically thus power 
management methods based on battery status are not always 
applicable to energy harvesting systems in addition most power 
management schemes designed for battery-powered systems only 
account for the dynamics of the energy consumers e g cpu radio 
but not the dynamics of the energy supply consequently battery 
powered systems usually operate at the lowest performance level that 
meets the minimum data fidelity requirement in order to maximize 
the system life energy harvesting systems on the other hand can 
provide enhanced performance depending on the available energy 
in this paper we will study how to adapt the performance of the 
available energy profile there exist many techniques to accomplish 
performance scaling at the node level such as radio transmit power 
adjustment dynamic voltage scaling and the use of low 
power modes however these techniques require hardware 
support and may not always be available on resource constrained 
sensor nodes alternatively a common performance scaling 
technique is duty cycling low power devices typically provide at 
least one low power mode in which the node is shut down and the 
power consumption is negligible in addition the rate of duty cycling 
is directly related to system performance metrics such as network 
latency and sampling frequency we will use duty cycle adjustment 
as the primitive performance scaling technique in our algorithms 
 related work 
energy harvesting has been explored for several different types 
of systems such as wearable computers sensor networks 
 etc several technologies to extract energy from the environment 
have been demonstrated including solar motion-based biochemical 
vibration-based and others are being developed 
 while several energy harvesting sensor node platforms 
have been prototyped there is a need for systematic 
power management techniques that provide performance guarantees 
during system operation the first work to take environmental 
energy into account for data routing was followed by 
while these works did demonstrate that environment aware 
decisions improve performance compared to battery aware decisions 
their objective was not to achieve energy neutral operation our 
proposed techniques attempt to maximize system performance while 
maintaining energy-neutral operation 
 system model 
the energy usage considerations in a harvesting system vary 
significantly from those in a battery powered system as mentioned 
earlier we propose the model shown in figure for designing 
energy management methods in a harvesting system the functions 
of the various blocks shown in the figure are discussed below the 
precise methods used in our system to achieve these functions will 
be discussed in subsequent sections 
harvested energy tracking this block represents the mechanisms 
used to measure the energy received from the harvesting device 
such as the solar panel such information is useful for determining 
the energy availability profile and adapting system performance 
based on it collecting this information requires that the node 
hardware be equipped with the facility to measure the power 
generated from the environment and the heliomote platform 
we used for evaluating the algorithms has this capability 
energy generation model for wireless sensor nodes with limited 
storage and processing capabilities to be able to use the harvested 
energy data models that represent the essential components of this 
information without using extensive storage are required the 
purpose of this block is to provide a model for the energy available 
to the system in a form that may be used for making power 
management decisions the data measured by the energy tracking 
block is used here to predict future energy availability a good 
prediction model should have a low prediction error and provide 
predicted energy values for durations long enough to make 
meaningful performance scaling decisions further for energy 
sources that exhibit both long-term and short-term patterns e g 
diurnal and climate variations vs weather patterns for solar energy 
the model must be able to capture both characteristics such a model 
can also use information from external sources such as local weather 
forecast service to improve its accuracy 
energy consumption model it is also important to have detailed 
information about the energy usage characteristics of the system at 
various performance levels for general applicability of our design 
we will assume that only one sleep mode is available we assume 
that the power consumption in the sleep and active modes is known 
it may be noted that for low power systems with more advanced 
capabilities such as dynamic voltage scaling dvs multiple low 
power modes and the capability to shut down system components 
selectively the power consumption in each of the states and the 
resultant effect on application performance should be known to make 
power management decisions 
energy storage model this block represents the model for the 
energy storage technology since all the generated energy may not be 
used instantaneously the harvesting system will usually have some 
energy storage technology storage technologies e g batteries and 
ultra-capacitors are non-ideal in that there is some energy loss while 
storing and retrieving energy from them these characteristics must be 
known to efficiently manage energy usage and storage this block also 
includes the system capability to measure the residual stored energy 
most low power systems use batteries to store energy and provide 
residual battery status this is commonly based on measuring the 
battery voltage which is then mapped to the residual battery energy 
using the known charge to voltage relationship for the battery 
technology in use more sophisticated methods which track the flow of 
energy into and out of the battery are also available 
harvesting-aware power management the inputs provided by the 
previously mentioned blocks are used here to determine the suitable 
power management strategy for the system power management 
could be carried to meet different objectives in different applications 
for instance in some systems the harvested energy may marginally 
supplement the battery supply and the objective may be to maximize 
the system lifetime a more interesting case is when the harvested 
energy is used as the primary source of energy for the system with 
the objective of achieving indefinitely long system lifetime in such 
cases the power management objective is to achieve energy neutral 
operation in other words the system should only use as much 
energy as harvested from the environment and attempt to maximize 
performance within this available energy budget 
 theoretically optimal power 
management 
we develop the following theory to understand the energy 
neutral mode of operation let us define ps t as the energy harvested 
from the environment at time t and the energy being consumed by 
the load at that time is pc t further we model the non-ideal storage 
buffer by its round-trip efficiency η strictly less than and a 
constant leakage power pleak using this notation applying the rule 
of energy conservation leads to the following inequality 
 
 
t t t 
s c c s leakp t p t dt p t p t dt p dtb η 
 
− − − ≥ −∫ ∫ ∫ 
where b is the initial battery level and the function x 
 x if x 
and zero otherwise 
definition ρ σ σ function a non-negative continuous and 
bounded function p t is said to be a ρ σ σ function if and only if 
for any value of finite real number t the following are satisfied 
 
t 
t p t dt tρ σ ρ σ− ≤ ≤ ∫ 
this function can be used to model both energy sources and loads 
if the harvested energy profile ps t is a ρ σ σ function then the 
average rate of available energy over long durations becomes ρ and 
the burstiness is bounded by σ and σ similarly pc t can be modeled 
as a ρ σ function when ρ and σ are used to place an upper bound 
on power consumption the inequality on the right side while there are 
no minimum power consumption constraints 
the condition for energy neutrality equation leads to the 
following theorem based on the energy production consumption and 
energy buffer models discussed above 
theorem energy neutral operation consider 
a harvesting system in which the energy production profile is 
characterized by a ρ σ σ function the load is characterized by 
a ρ σ function and the energy buffer is characterized by 
parameters η for storage efficiency and pleak for leakage power the 
following conditions are sufficient for the system to achieve energy 
neutrality 
ρ ≤ ηρ − pleak 
b ≥ ησ σ 
b ≥ b 
where b is the initial energy stored in the buffer and provides a 
lower bound on the capacity of the energy buffer b the proof is 
presented in our prior work 
to adjust the duty cycle d using our performance scaling 
algorithm we assume the following relation between duty cycle and 
the perceived utility of the system to the user suppose the utility of 
the application to the user is represented by u d when the system 
operates at a duty cycle d then 
min 
 min max 
 max 
 
 
 
u d if d d 
u d k d if d d d 
u d k if d d 
β 
 
 ≤ ≤ 
 
this is a fairly general and simple model and the specific values of 
dmin and dmax may be determined as per application requirements as 
an example consider a sensor node designed to detect intrusion across 
a periphery in this case a linear increase in duty cycle translates into a 
linear increase in the detection probability the fastest and the slowest 
speeds of the intruders may be known leading to a minimum and 
harvested energy 
tracking 
energy consumption 
model 
energy storage model 
 
harvestingaware power 
mangement 
energy generation 
model 
load 
figure system model for an energy harvesting system 
 
maximum sensing delay tolerable which results in the relevant dmax 
and dmin for the sensor node while there may be cases where the 
relationship between utility and duty cycle may be non-linear in this 
paper we restrict our focus on applications that follow this linear 
model in view of the above models for the system components and 
the required performance the objective of our power management 
strategy is adjust the duty cycle d i dynamically so as to maximize 
the total utility u d over a period of time while ensuring energy 
neutral operation for the sensor node 
before discussing the performance scaling methods for harvesting 
aware duty cycle adaptation let us first consider the optimal power 
management strategy that is possible for a given energy generation 
profile for the calculation of the optimal strategy we assume 
complete knowledge of the energy availability profile at the node 
including the availability in the future the calculation of the optimal is 
a useful tool for evaluating the performance of our proposed algorithm 
this is particularly useful for our algorithm since no prior algorithms 
are available to serve as a baseline for comparison 
suppose the time axis is partitioned into discrete slots of duration 
δt and the duty cycle adaptation calculation is carried out over a 
window of nw such time slots we define the following energy profile 
variables with the index i ranging over {   nw} ps i is the power 
output from the harvested source in time slot i averaged over the slot 
duration pc is the power consumption of the load in active mode and 
d i is the duty cycle used in slot i whose value is to be determined 
b i is the residual battery energy at the beginning of slot i following 
this convention the battery energy left after the last slot in the window 
is represented by b nw the values of these variables will depend 
on the choice of d i 
the energy used directly from the harvested source and the energy 
stored and used from the battery must be accounted for differently 
figure shows two possible cases for ps i in a time slot ps i may 
either be less than or higher than pc as shown on the left and right 
respectively when ps i is lower than pc some of the energy used by 
the load comes from the battery while when ps i is higher than pc all 
the energy used is supplied directly from the harvested source the 
crosshatched area shows the energy that is available for storage into 
the battery while the hashed area shows the energy drawn from the 
battery we can write the energy used from the battery in any slot i as 
 { } c cs s sb i b i td i p p i tp i d i td i p i pη η 
 
− δ − − δ − − − 
in equation the first term on the right hand side measures the 
energy drawn from the battery when ps i pc the next term measures 
the energy stored into the battery when the node is in sleep mode and 
the last term measures the energy stored into the battery in active mode 
if ps i pc for energy neutral operation we require the battery at the 
end of the window of nw slots to be greater than or equal to the starting 
battery clearly battery level will go down when the harvested energy 
is not available and the system is operated from stored energy 
however the window nw is judiciously chosen such that over that 
duration we expect the environmental energy availability to complete 
a periodic cycle for instance in the case of solar energy harvesting 
nw could be chosen to be a twenty-four hour duration corresponding 
to the diurnal cycle in the harvested energy this is an approximation 
since an ideal choice of the window size would be infinite but a finite 
size must be used for analytical tractability further the battery level 
cannot be negative at any time and this is ensured by having a large 
enough initial battery level b such that node operation is sustained 
even in the case of total blackout during a window period stating the 
above constraints quantitatively we can express the calculation of the 
optimal duty cycles as an optimization problem below 
 
max 
wn 
i 
d i 
 
∑ 
 { } c s s s cb i b i td i p p i tp i d i td i p i pη η 
 
⎡ ⎤ ⎡ ⎤− δ − − δ − − −⎣ ⎦ ⎣ ⎦ 
 
 b b 
 wb n b ≥ 
min w i { n }d i d≥ ∀ ∈ 
max w i { n }d i d≤ ∀ ∈ 
the solution to the optimization problem yields the duty cycles 
that must be used in every slot and the evolution of residual battery 
over the course of nw slots note that while the constraints above 
contain the non-linear function x 
 the quantities occurring within 
that function are all known constants the variable quantities occur 
only in linear terms and hence the above optimization problem can 
be solved using standard linear programming techniques available 
in popular optimization toolboxes 
 harvesting-aware power 
management 
we now present a practical algorithm for power management that 
may be used for adapting the performance based on harvested energy 
information this algorithm attempts to achieve energy neutral 
operation without using knowledge of the future energy availability 
and maximizes the achievable performance within that constraint 
the harvesting-aware power management strategy consists of 
three parts the first part is an instantiation of the energy generation 
model which tracks past energy input profiles and uses them to 
predict future energy availability the second part computes the 
optimal duty cycles based on the predicted energy and this step 
uses our computationally tractable method to solve the optimization 
problem the third part consists of a method to dynamically adapt 
the duty cycle in response to the observed energy generation profile 
in real time this step is required since the observed energy 
generation may deviate significantly from the predicted energy 
availability and energy neutral operation must be ensured with the 
actual energy received rather than the predicted values 
 energy prediction model 
we use a prediction model based on exponentially weighted 
moving-average ewma the method is designed to exploit the 
diurnal cycle in solar energy but at the same time adapt to the seasonal 
variations a historical summary of the energy generation profile is 
maintained for this purpose while the storage data size is limited to a 
vector length of nw values in order to minimize the memory overheads 
of the power management algorithm the window size is effectively 
infinite as each value in the history window depends on all the 
observed data up to that instant the window size is chosen to be 
hours and each time slot is taken to be minutes as the variation in 
generated power by the solar panel using this setting is less than 
between each adjacent slots this yields nw smaller slot 
durations may be used at the expense of a higher nw 
the historical summary maintained is derived as follows on a 
typical day we expect the energy generation to be similar to the energy 
generation at the same time on the previous days the value of energy 
generated in a particular slot is maintained as a weighted average of the 
energy received in the same time-slot during all observed days the 
weights are exponential resulting in decaying contribution from older 
figure two possible cases for energy calculations 
slot i slot k 
pc 
pc 
p i 
p i 
active sleep 
 
data more specifically the historical average maintained for each slot 
is given by 
 k k kx x xα α− − 
where α is the value of the weighting factor kx is the observed value 
of energy generated in the slot and kx − 
is the previously stored 
historical average in this model the importance of each day relative to 
the previous one remains constant because the same weighting factor 
was used for all days 
the average value derived for a slot is treated as an estimate of 
predicted energy value for the slot corresponding to the subsequent 
day this method helps the historical average values adapt to the 
seasonal variations in energy received on different days one of the 
parameters to be chosen in the above prediction method is the 
parameter α which is a measure of rate of shift in energy pattern over 
time since this parameter is affected by the characteristics of the 
energy and sensor node location the system should have a training 
period during which this parameter will be determined to determine a 
good value of α we collected energy data over days and compared 
the average error of the prediction method for various values of α the 
error based on the different values of α is shown in figure this 
curve suggests an optimum value of α for minimum prediction 
error and this value will be used in the remainder of this paper 
 low-complexity solution 
the energy values predicted for the next window of nw slots are 
used to calculated the desired duty cycles for the next window 
assuming the predicted values match the observed values in the future 
since our objective is to develop a practical algorithm for embedded 
computing systems we present a simplified method to solve the linear 
programming problem presented in section to this end we define 
the sets s and d as follows 
{ } 
{ } 
 
 
s c 
c s 
s i p i p 
d i p p i 
 − ≥ 
 − 
the two sets differ by the condition that whether the node operation 
can be sustained entirely from environmental energy in the case that 
energy produced from the environment is not sufficient battery will be 
discharged to supplement the remaining energy next we sum up both 
sides of over the entire nw window and rewrite it with the new 
notation 
 
 
 
nw nw nw 
i i c s s s s c 
i i d i i i s 
b b td i p p i tp i tp i d i td i p i pη η η 
 ∈ ∈ 
− δ − − δ δ − δ −∑ ∑ ∑ ∑ ∑ 
the term on the left hand side is actually the battery energy used over 
the entire window of nw slots which can be set to for energy neutral 
operation after some algebraic manipulation this yields 
 
 
 
nw 
c 
s s c 
i i d i s 
p 
p i d i p i p d i 
η η ∈ ∈ 
⎛ ⎞⎛ ⎞ 
 − ⎜ ⎟⎜ ⎟ 
⎝ ⎠⎝ ⎠ 
∑ ∑ ∑ 
the term on the left hand side is the total energy received in nw 
slots the first term on the right hand side can be interpreted as the 
total energy consumed during the d slots and the second term is the 
total energy consumed during the s slots we can now replace three 
constraints and in the original problem with restating 
the optimization problem as follows 
 
max 
wn 
i 
d i 
 
∑ 
 
 
 
nw 
c 
s s c 
i i d i s 
p 
p i d i p i p d i 
η η ∈ ∈ 
⎛ ⎞⎛ ⎞ 
 − ⎜ ⎟⎜ ⎟ 
⎝ ⎠⎝ ⎠ 
∑ ∑ ∑ 
min 
max 
d i d { nw 
d i d { nw 
i 
i 
≥ ∀ ∈ 
≤ ∀ ∈ 
this form facilitates a low complexity solution that doesn t require 
a general linear programming solver since our objective is to 
maximize the total system utility it is preferable to set the duty cycle to 
dmin for time slots where the utility per unit energy is the least on the 
other hand we would also like the time slots with the highest ps to 
operate at dmax because of better efficiency of using energy directly 
from the energy source combining these two characteristics we 
define the utility co-efficient for each slot i as follows 
 
 
 
c 
c 
s 
p for i s 
w i p 
p i for i d 
η η 
∈⎧ 
⎪ 
 ⎛ ⎞⎛ ⎞⎨ 
 − ∈⎜ ⎟⎜ ⎟⎪ 
⎝ ⎠⎝ ⎠⎩ 
where w i is a representation of how efficient the energy usage in a 
particular time slot i is a larger w i indicates more system utility per 
unit energy in slot i and vice versa the algorithm starts by assuming 
d i dmin for i {  nw} because of the minimum duty cycle 
requirement and computes the remaining system energy r by 
 
 
 
nw 
c 
s s c 
i i d i s 
p 
r p i d i p i p d i 
η η ∈ ∈ 
⎛ ⎞⎛ ⎞ 
 − − −⎜ ⎟⎜ ⎟ 
⎝ ⎠⎝ ⎠ 
∑ ∑ ∑ 
a negative r concludes that the optimization problem is infeasible 
meaning the system cannot achieve energy neutrality even at the 
minimum duty cycle in this case the system designer is responsible 
for increasing the environment energy availability e g by using larger 
solar panels if r is positive it means the system has excess energy 
that is not being used and this may be allocated to increase the duty 
cycle beyond dmin for some slots since our objective is to maximize 
the total system utility the most efficient way to allocate the excess 
energy is to assign duty cycle dmax to the slots with the highest w i 
so the coefficients w i are arranged in decreasing order and duty 
cycle dmax is assigned to the slots beginning with the largest 
coefficients until the excess energy available r computed by in 
every iteration is insufficient to assign dmax to another slot the 
remaining energy rlast is used to increase the duty cycle to some 
value between dmin and dmax in the slot with the next lower coefficient 
denoting this slot with index j the duty cycle is given by 
d j 
min 
 
 
last c 
last 
s c s 
r p if j d 
dr 
if j s 
p j p p jη 
∈⎧ ⎫ 
⎪ ⎪ 
 ⎨ ⎬ 
∈⎪ ⎪− −⎩ ⎭ 
the above solution to the optimization problem requires only simple 
arithmetic calculations and one sorting step which can be easily 
implemented on an embedded platform as opposed to implementing a 
general linear program solver 
 slot-by-slot continual duty cycle adaptiation 
the observed energy values may vary greatly from the predicted 
ones such as due to the effect of clouds or other sudden changes it is 
thus important to adapt the duty cycles calculated using the predicted 
values to the actual energy measurements in real time to ensure energy 
neutrality denote the initial duty cycle assignments for each time slot i 
computed using the predicted energy values as d i { nw} first 
we compute the difference between predicted power level ps i and 
actual power level observed ps i in every slot i then the excess 
energy in slot i denoted by x can be obtained as follows 
 
 
 
s s s c 
s s s s s c 
p i p i if p i p 
x 
p i p i d i p i p i if p i p 
η 
− ⎧ 
⎪ 
 ⎨ 
− − − − ≤⎪ 
⎩ 
 
 
 
 
 
 
alpha 
avgerror ma 
figure choice of prediction parameter 
 
the upper term accounts for the energy difference when actual 
received energy is more than the power drawn by the load on the 
other hand if the energy received is less than pc we will need to 
account for the extra energy used from the battery by the load which is 
a function of duty cycle used in time slot i and battery efficiency factor 
η when more energy is received than predicted x is positive and that 
excess energy is available for use in the subsequent solutes while if x 
is negative that energy must be compensated from subsequent slots 
case i x in this case we want to reduce the duty cycles used in 
the future slots in order to make up for this shortfall of energy since 
our objective function is to maximize the total system utility we have 
to reduce the duty cycles for time slots with the smallest normalized 
utility coefficient w i this is accomplished by first sorting the 
coefficient w j where j i in decreasing order and then iteratively 
reducing dj to dmin until the total reduction in energy consumption is 
the same as x 
case ii x here we want to increase the duty cycles used in the 
future to utilize this excess energy we received in recent time slot in 
contrast to case i the duty cycles of future time slots with highest 
utility coefficient w i should be increased first in order to maximize 
the total system utility 
suppose the duty cycle is changed by d in slot j define a quantity 
r j d as follows 
⎪ 
⎩ 
⎪ 
⎨ 
⎧ 
 ⎟ 
⎟ 
⎠ 
⎞ 
⎜ 
⎜ 
⎝ 
⎛ 
⎟⎟ 
⎠ 
⎞ 
⎜⎜ 
⎝ 
⎛ 
− 
 ⋅ 
 
lji 
l 
ljl 
ppifp 
p 
d 
ppifp 
djr 
 
 
 
 d 
 
ηη 
the precise procedure to adapt the duty cycle to account for the 
above factors is presented in algorithm this calculation is 
performed at the end of every slot to set the duty cycle for the next 
slot we claim that our duty cycling algorithm is energy neutral 
because an surplus of energy at the previous time slot will always 
translate to additional energy opportunity for future time slots and 
vice versa the claim may be violated in cases of severe energy 
shortages especially towards the end of window for example a large 
deficit in energy supply can t be restored if there is no future energy 
input until the end of the window in such case this offset will be 
carried over to the next window so that long term energy neutrality is 
still maintained 
 evaluation 
our adaptive duty cycling algorithm was evaluated using an actual 
solar energy profile measured using a sensor node called heliomote 
capable of harvesting solar energy this platform not only tracks 
the generated energy but also the energy flow into and out of the 
battery to provide an accurate estimate of the stored energy 
the energy harvesting platform was deployed in a residential area 
in los angeles from the beginning of june through the middle of 
august for a total of days the sensor node used is a mica mote 
running at a fixed duty cycle with an initially full battery battery 
voltage and net current from the solar panels are sampled at a period of 
 seconds the energy generation profile for that duration measured 
by tracking the output current from the solar cell is shown in figure 
both on continuous and diurnal scales we can observe that although 
the energy profile varies from day to day it still exhibits a general 
pattern over several days 
 
 
 
 
 
 
 
 
 
day 
ma 
 
 
 
 
 
 
 
 
 
hour 
ma 
 prediction model 
we first evaluate the performance of the prediction model which 
is judged by the amount of absolute error it made between the 
predicted and actual energy profile figure shows the average error 
of each time slot in ma over the entire days generally the amount 
of error is larger during the day time because that s when the factor of 
weather can cause deviations in received energy while the prediction 
made for night time is mostly correct 
 adaptive duty cycling algorithm 
prior methods to optimize performance while achieving energy 
neutral operation using harvested energy are scarce instead we 
compare the performance of our algorithm against two extremes the 
theoretical optimal calculated assuming complete knowledge about 
future energy availability and a simple approach which attempts to 
achieve energy neutrality using a fixed duty cycle without accounting 
for battery inefficiency 
the optimal duty cycles are calculated for each slot using the 
future knowledge of actual received energy for that slot for the simple 
approach the duty cycle is kept constant within each day and is 
figure solar energy profile left continuous right diurnal 
input d initial duty cycle x excess energy due to error in the 
prediction p predicted energy profile i index of current time slot 
output d updated duty cycles in one or more subsequent slots 
adaptdutycycle 
iteration at each time slot do 
if x 
wsorted w{ nw} sorted in decending order 
q indices of wsorted 
for k to q 
if q k ≤ i or d q k ≥ dmax slot is already passed 
continue 
if r q k dmax − d q k x 
d q k dmax 
x x − r j dmax − d q k 
else 
 x insufficient to increase duty cycle to dmax 
if p q k pl 
d q k d q k x pl 
else 
d q k d q k 
 c s 
x 
p p q kη η − 
if x 
wsorted w{ nw} sorted in ascending order 
q indices of wsorted 
for k to q 
if q k ≤ i or d q k ≤ dmin 
continue 
if r q k dmax − d q k x 
d q k dmin 
x x − r j dmin − d q k 
else 
if p q k pc 
d q k d q k x pc 
else 
d q k d q k 
 c s 
x 
p p q kη η − 
algorithm pseudocode for the duty-cycle adaptation algorithm 
figure average predictor error in ma 
 
 
 
 
 
 
 
 
time h 
abserror ma 
 
computed by taking the ratio of the predicted energy availability and 
the maximum usage and this guarantees that the senor node will never 
deplete its battery running at this duty cycle 
{ 
 s w c 
i nw 
d p i n pη 
∈ 
 ⋅ ⋅∑ 
we then compare the performance of our algorithm to the two 
extremes with varying battery efficiency figure shows the results 
using dmax and dmin the battery efficiency was varied 
from to on the x-axis and solar energy utilizations achieved by 
the three algorithms are shown on the y-axis it shows the fraction of 
net received energy that is used to perform useful work rather than lost 
due to storage inefficiency 
as can be seen from the figure battery efficiency factor has great 
impact on the performance of the three different approaches the three 
approaches all converges to utilization if we have a perfect 
battery η that is energy is not lost by storing it into the batteries 
when battery inefficiency is taken into account both the adaptive and 
optimal approach have much better solar energy utilization rate than 
the simple one additionally the result also shows that our adaptive 
duty cycle algorithm performs extremely close to the optimal 
 
 
 
 
 
 
 
 
eta-batery roundtrip efficiency 
solarenergyutilization 
optimal 
adaptive 
simple 
we also compare the performance of our algorithm with different 
values of dmin and dmax for η which is typical of nimh batteries 
these results are shown in table as the percentage of energy saved 
by the optimal and adaptive approaches and this is the energy which 
would normally be wasted in the simple approach the figures and 
table indicate that our real time algorithm is able to achieve a 
performance very close to the optimal feasible in addition these 
results show that environmental energy harvesting with appropriate 
power management can achieve much better utilization of the 
environmental energy 
dmax 
dmin 
 
 
 
 
 
 
 
 
 
 
 
 
adaptive 
optimal 
 conclusions 
we discussed various issues in power management for systems 
powered using environmentally harvested energy specifically we 
designed a method for optimizing performance subject to the 
constraint of energy neutral operation we also derived a theoretically 
optimal bound on the performance and showed that our proposed 
algorithm operated very close to the optimal the proposals were 
evaluated using real data collected using an energy harvesting sensor 
node deployed in an outdoor environment 
our method has significant advantages over currently used 
methods which are based on a conservative estimate of duty cycle and 
can only provide sub-optimal performance however this work is only 
the first step towards optimal solutions for energy neutral operation it 
is designed for a specific power scaling method based on adapting the 
duty cycle several other power scaling methods such as dvs 
submodule power switching and the use of multiple low power modes are 
also available it is thus of interest to extend our methods to exploit 
these advanced capabilities 
 acknowledgements 
this research was funded in part through support provided by 
darpa under the pac c program the national science foundation 
 nsf under award and the ucla center for embedded 
networked sensing cens any opinions findings conclusions or 
recommendations expressed in this paper are those of the authors and 
do not necessarily reflect the views of darpa nsf or cens 
references 
 r ramanathan and r hain toplogy control of multihop wireless 
networks using transmit power adjustment in proc infocom vol 
 - pp - march 
 t a pering t d burd and r w brodersen the simulation and 
evaluation of dynamic voltage scaling algorithms in proc acm 
islped pp - 
 l benini and g de micheli dynamic power management design 
techniques and cad tools kluwer academic publishers norwell ma 
 
 john kymisis clyde kendall joseph paradiso and neil gershenfeld 
parasitic power harvesting in shoes in iswc pages - ieee 
computer society press october 
 nathan s shenck and joseph a paradiso energy scavenging with 
shoemounted piezoelectrics ieee micro ñ may-june 
 t starner human-powered wearable computing ibm systems journal 
 - 
 mohammed rahimi hardik shah gaurav s sukhatme john 
heidemann and d estrin studying the feasibility of energy harvesting in 
a mobile sensor network in icra 
 chrismelhuish the ecobot project www ias uwe ac uk energy 
autonomy ecobot web page html 
 jan m rabaey m josie ammer julio l da silva jr danny patel and 
shad roundy picoradio supports ad-hoc ultra-low power wireless 
networking ieee computer pages - july 
 joseph a paradiso and mark feldmeier a compact wireless 
selfpowered pushbutton controller in acm ubicomp pages - 
atlanta ga usa september springer-verlag berlin heidelberg 
 se wright ds scott jb haddow andma rosen the upper limit to solar 
energy conversion volume pages - july 
 darpa energy harvesting projects 
http www darpa mil dso trans energy projects html 
 werner weber ambient intelligence industrial research on a visionary 
concept in proceedings of the international symposium on low 
power electronics and design pages - acm press 
 v raghunathan a kansal j hsu j friedman and mb srivastava 
 design considerations for solar energy harvesting wireless embedded 
systems ipsn spots april 
 xiaofan jiang joseph polastre david culler perpetual environmentally 
powered sensor networks ipsn spots april - 
 chulsung park pai h chou and masanobu shinozuka duranode 
wireless networked sensor for structural health monitoring to appear 
in proceedings of the th ieee international conference on sensors 
irvine ca oct - nov 
 aman kansal and mani b srivastava an environmental energy 
harvesting framework for sensor networks in international symposium on 
low power electronicsand design pages - acm press 
 thiemo voigt hartmut ritter and jochen schiller utilizing solar power 
in wireless sensor networks in lcn 
 a kansal j hsu s zahedi and m b srivastava power management 
in energy harvesting sensor networks technical report 
tr-ucla-nesl - networked and embedded systems laboratory ucla 
march 
figure duty cycles achieved with respect to η 
table energy saved by adaptive and optimal approach 
 
a hierarchical process execution support for grid 
computing 
fábio r l cicerre 
institute of computing 
state university of campinas 
campinas brazil 
fcicerre ic unicamp br 
edmundo r m madeira 
institute of computing 
state university of campinas 
campinas brazil 
edmundo ic unicamp br 
luiz e buzato 
institute of computing 
state university of campinas 
campinas brazil 
buzato ic unicamp br 
abstract 
grid is an emerging infrastructure used to share resources 
among virtual organizations in a seamless manner and to 
provide breakthrough computing power at low cost 
nowadays there are dozens of academic and commercial products 
that allow execution of isolated tasks on grids but few 
products support the enactment of long-running processes in a 
distributed fashion in order to address such subject this 
paper presents a programming model and an infrastructure 
that hierarchically schedules process activities using 
available nodes in a wide grid environment their advantages 
are automatic and structured distribution of activities and 
easy process monitoring and steering 
categories and subject descriptors 
c computer-communication networks distributed 
systems-distributed applications 
general terms 
design performance management algorithms 
 introduction 
grid computing is a model for wide-area distributed and 
parallel computing across heterogeneous networks in 
multiple administrative domains this research field aims to 
promote sharing of resources and provides breakthrough 
computing power over this wide network of virtual 
organizations in a seamless manner traditionally as in globus 
 condor-g and legion there is a minimal 
infrastructure that provides data resource sharing computational 
resource utilization management and distributed execution 
specifically considering distributed execution most of the 
existing grid infrastructures supports execution of isolated 
tasks but they do not consider their task interdependencies 
as in processes workflows this deficiency restricts 
better scheduling algorithms distributed execution 
coordination and automatic execution recovery 
there are few proposed middleware infrastructures that 
support process execution over the grid in general they 
model processes by interconnecting their activities through 
control and data dependencies among them webflow 
 emphasizes an architecture to construct distributed 
processes opera-g provides execution recovering and 
steering gridflow focuses on improved scheduling algorithms 
that take advantage of activity dependencies and swindew 
 supports totally distributed execution on peer-to-peer 
networks however such infrastructures contain 
scheduling algorithms that are centralized by process or 
completely distributed but difficult to monitor and control 
 
in order to address such constraints this paper proposes a 
structured programming model for process description and a 
hierarchical process execution infrastructure the 
programming model employs structured control flow to promote 
controlled and contextualized activity execution 
complementary the support infrastructure which executes a process 
specification takes advantage of the hierarchical structure 
of a specified process in order to distribute and schedule 
strong dependent activities as a unit allowing a better 
execution performance and fault-tolerance and providing 
localized communication 
the programming model and the support infrastructure 
named x avantes are under implementation in order to show 
the feasibility of the proposed model and to demonstrate its 
two major advantages to promote widely distributed 
process execution and scheduling but in a controlled 
structured and localized way 
next section describes the programming model and 
section the support infrastructure for the proposed grid 
computing model section demonstrates how the support 
infrastructure executes processes and distributes activities 
related works are presented and compared to the proposed 
model in section the last section concludes this paper 
encompassing the advantages of the proposed hierarchical 
process execution support for the grid computing area and 
lists some future works 
 middleware companion 
processelement 
process activity controller 
 
 
 
 
figure high-level framework of the programming 
model 
 programming model 
the programming model designed for the grid computing 
architecture is very similar to the specified to the business 
process execution language bpel both describe 
processes in xml documents but the former specifies 
processes strictly synchronous and structured and has more 
constructs for structured parallel control the rationale 
behind of its design is the possibility of hierarchically distribute 
the process control and coordination based on structured 
constructs differently from bpel which does not allow 
hierarchical composition of processes 
in the proposed programming model a process is a set of 
interdependent activities arranged to solve a certain 
problem in detail a process is composed of activities 
subprocesses and controllers see figure activities represent 
simple tasks that are executed on behalf of a process 
subprocesses are processes executed in the context of a 
parent process and controllers are control elements used to 
specify the execution order of these activities and 
subprocesses like structured languages controllers can be nested 
and then determine the execution order of other controllers 
data are exchanged among process elements through 
parameters they are passed by value in case of simple 
objects or by reference if they are remote objects shared 
among elements of the same controller or process external 
data can be accessed through data sources such as relational 
databases or distributed objects 
 controllers 
controllers are structured control constructs used to 
define the control flow of processes there are sequential and 
parallel controllers 
the sequential controller types are block switch for 
and while the block controller is a simple sequential 
construct and the others mimic equivalent structured 
programming language constructs similarly the parallel types are 
par parswitch parfor and parwhile they extend the 
respective sequential counterparts to allow parallel execution 
of process elements 
all parallel controller types fork the execution of one or 
more process elements and then wait for each execution to 
finish indeed they contain a fork and a join of execution 
aiming to implement a conditional join all parallel 
controller types contain an exit condition evaluated all time 
that an element execution finishes in order to determine 
when the controller must end 
the parfor and parwhile are the iterative versions of 
the parallel controller types both fork executions while 
the iteration condition is true this provides flexibility to 
determine at run-time the number of process elements to 
execute simultaneously 
when compared to workflow languages the parallel 
controller types represent structured versions of the workflow 
control constructors because they can nest other controllers 
and also can express fixed and conditional forks and joins 
present in such languages 
 process example 
this section presents an example of a prime number search 
application that receives a certain range of integers and 
returns a set of primes contained in this range the whole 
computation is made by a process which uses a parallel 
controller to start and dispatch several concurrent activities 
of the same type in order to find prime numbers the 
portion of the xml document that describes the process and 
activity types is shown below 
 process type name findprimes 
 in parameter type int name min 
 in parameter type int name max 
 in parameter type int name numprimes 
 in parameter type int name numacts 
 body 
 pre code 
setprimes new remotehashset 
parfor setmin getmin 
parfor setmax getmax 
parfor setnumprimes getnumprimes 
parfor setnumacts getnumacts 
parfor setprimes getprimes 
parfor setcounterbegin 
parfor setcounterend getnumacts - 
 pre code 
 parfor name parfor 
 in parameter type int name min 
 in parameter type int name max 
 in parameter type int name numprimes 
 in parameter type int name numacts 
 in parameter 
type remotecollection name primes 
 iterate 
 pre code 
int range 
 getmax -getmin getnumacts 
int minnum range getcounter getmin 
int maxnum minnum range- 
if getcounter getnumacts - 
maxnum getmax 
findprimes setmin minnum 
findprimes setmax maxnum 
findprimes setnumprimes getnumprimes 
findprimes setprimes getprimes 
 pre code 
 activity 
type findprimes name findprimes 
 iterate 
 parfor 
 body 
 out parameter 
type remotecollection name primes 
 process type 
middleware for grid computing 
 activity type name findprimes 
 in parameter type int name min 
 in parameter type int name max 
 in parameter type int name numprimes 
 in parameter 
type remotecollection name primes 
 code 
for int num getmin num getmax num { 
 stop required number of primes was found 
if primes size getnumprimes 
break 
boolean prime true 
for int i i num i { 
if num i { 
prime false 
break 
} 
} 
if prime { 
primes add new integer num 
} 
} 
 code 
 activity type 
firstly a process type that finds prime numbers named 
findprimes is defined it receives through its input 
parameters a range of integers in which prime numbers have 
to be found the number of primes to be returned and the 
number of activities to be executed in order to perform this 
work at the end the found prime numbers are returned as 
a collection through its output parameter 
this process contains a parfor controller aiming to 
execute a determined number of parallel activities it iterates 
from to getnumacts - which determines the number 
of activities starting a parallel activity in each iteration in 
such case the controller divides the whole range of numbers 
in subranges of the same size and in each iteration starts a 
parallel activity that finds prime numbers in a specific 
subrange these activities receive a shared object by reference 
in order to store the prime numbers just found and control 
if the required number of primes has been reached 
finally it is defined the activity type findprimes used 
to find prime numbers in each subrange it receives through 
its input parameters the range of numbers in which it has 
to find prime numbers the total number of prime numbers 
to be found by the whole process and passed by reference 
a collection object to store the found prime numbers 
between its code markers there is a simple code to find prime 
numbers which iterates over the specified range and 
verifies if the current integer is a prime additionally in each 
iteration the code verifies if the required number of primes 
inserted in the primes collection by all concurrent activities 
has been reached and exits if true 
the advantage of using controllers is the possibility of the 
support infrastructure determines the point of execution the 
process is in allowing automatic recovery and monitoring 
and also the capability of instantiating and dispatching 
process elements only when there are enough computing 
resources available reducing unnecessary overhead besides 
due to its structured nature they can be easily composed 
and the support infrastructure can take advantage of this 
in order to distribute hierarchically the nested controllers to 
group server 
group 
java virtual machine 
rmi jdbc 
group manager 
process server 
java virtual machine 
rmi jdbc 
process coordinator 
worker 
java virtual machine 
rmi 
activity manager 
repository 
figure infrastructure architecture 
different machines over the grid allowing enhanced 
scalability and fault-tolerance 
 support infrastructure 
the support infrastructure comprises tools for 
specification and services for execution and monitoring of 
structured processes in highly distributed heterogeneous and 
autonomous grid environments it has services to monitor 
availability of resources in the grid to interpret processes 
and schedule activities and controllers and to execute 
activities 
 infrastructure architecture 
the support infrastructure architecture is composed of 
groups of machines and data repositories which preserves 
its administrative autonomy generally localized machines 
and repositories such as in local networks or clusters form 
a group each machine in a group must have a java virtual 
machine jvm and a java runtime library besides 
a combination of the following grid support services group 
manager gm process coordinator pc and activity 
manager am this combination determines what kind of group 
node it represents a group server a process server or 
simply a worker see figure 
in a group there are one or more group managers but 
only one acts as primary and the others as replicas they 
are responsible to maintain availability information of group 
machines moreover group managers maintain references to 
data resources of the group they use group repositories to 
persist and recover the location of nodes and their 
availability 
to control process execution there are one or more 
process coordinators per group they are responsible to 
instantiate and execute processes and controllers select resources 
and schedule and dispatch activities to workers in order 
to persist and recover process execution and data and also 
load process specification they use group repositories 
finally in several group nodes there is an activity 
manager it is responsible to execute activities in the hosted 
machine on behalf of the group process coordinators and to 
inform the current availability of the associated machine to 
group managers they also have pendent activity queues 
containing activities to be executed 
 inter-group relationships 
in order to model real grid architecture the infrastructure 
must comprise several potentially all local networks like 
internet does aiming to satisfy this intent local groups are 
 middleware companion 
gm 
gm 
gm 
gm 
figure inter-group relationships 
connected to others directly or indirectly through its group 
managers see figure 
each group manager deals with requests of its group 
 represented by dashed ellipses in order to register local 
machines and maintain correspondent availability 
additionally group managers communicate to group managers of 
other groups each group manager exports coarse 
availability information to group managers of adjacent groups and 
also receives requests from other external services to 
furnish detailed availability information in this way if there 
are resources available in external groups it is possible to 
send processes controllers and activities to these groups in 
order to execute them in external process coordinators and 
activity managers respectively 
 process execution 
in the proposed grid architecture a process is specified 
in xml using controllers to determine control flow 
referencing other processes and activities and passing objects to 
their parameters in order to define data flow after specified 
the process is compiled in a set of classes which represent 
specific process activity and controller types at this time 
it can be instantiated and executed by a process coordinator 
 dynamic model 
to execute a specified process it must be instantiated by 
referencing its type on a process coordinator service of a 
specific group also the initial parameters must be passed 
to it and then it can be started 
the process coordinator carries out the process by 
executing the process elements included in its body sequentially 
if the element is a process or a controller the process 
coordinator can choose to execute it in the same machine or to 
pass it to another process coordinator in a remote machine 
if available else if the element is an activity it passes to 
an activity manager of an available machine 
process coordinators request the local group manager to 
find available machines that contain the required service 
process coordinator or activity manager in order to 
execute a process element then it can return a local 
machine a machine in another group or none depending on 
the availability of such resource in the grid it returns an 
external worker activity manager machine if there are no 
available workers in the local group and it returns an 
external process server process coordinator machine if there 
are no available process servers or workers in the local group 
obeying this rule group managers try to find process servers 
in the same group of the available workers 
such procedure is followed recursively by all process 
cogm 
findprimes 
activity 
am 
findprimes 
activity 
am 
findprimes 
activity 
am 
findprimes 
process 
pc 
figure findprimes process execution 
ordinators that execute subprocesses or controllers of a 
process therefore because processes are structured by 
nesting process elements the process execution is automatically 
distributed hierarchically through one or more grid groups 
according to the availability and locality of computing 
resources 
the advantage of this distribution model is wide area 
execution which takes advantage of potentially all grid 
resources and localized communication of process elements 
because strong dependent elements which are under the 
same controller are placed in the same or near groups 
besides it supports easy monitoring and steering due to its 
structured controllers which maintain state and control over 
its inner elements 
 process execution example 
revisiting the example shown in section a process 
type is specified to find prime numbers in a certain range of 
numbers in order to solve this problem it creates a number 
of activities using the parfor controller each activity then 
finds primes in a determined part of the range of numbers 
figure shows an instance of this process type executing 
over the proposed infrastructure a findprimes process 
instance is created in an available process coordinator pc 
which begins executing the parfor controller in each 
iteration of this controller the process coordinator requests 
to the group manager gm an available activity manager 
 am in order to execute a new instance of the findprimes 
activity if there is any am available in this group or in an 
external one the process coordinator sends the activity class 
and initial parameters to this activity manager and requests 
its execution else if no activity manager is available then 
the controller enters in a wait state until an activity manager 
is made available or is created 
in parallel whenever an activity finishes its result is sent 
back to the process coordinator which records it in the 
parfor controller then the controller waits until all 
activities that have been started are finished and it ends at 
this point the process coordinator verifies that there is no 
other process element to execute and finishes the process 
 related work 
there are several academic and commercial products that 
promise to support grid computing aiming to provide 
interfaces protocols and services to leverage the use of widely 
middleware for grid computing 
distributed resources in heterogeneous and autonomous 
networks among them globus condor-g and legion 
 are widely known aiming to standardize interfaces 
and services to grid the open grid services architecture 
 ogsa has been defined 
the grid architectures generally have services that 
manage computing resources and distribute the execution of 
independent tasks on available ones however emerging 
architectures maintain task dependencies and automatically 
execute tasks in a correct order they take advantage of 
these dependencies to provide automatic recovery and 
better distribution and scheduling algorithms 
following such model webflow is a process 
specification tool and execution environment constructed over 
corba that allows graphical composition of activities and 
their distributed execution in a grid environment opera-g 
 like webflow uses a process specification language 
similar to the data flow diagram and workflow languages but 
furnishes automatic execution recovery and limited steering 
of process execution 
the previously referred architectures and others that 
enact processes over the grid have a centralized coordination 
in order to surpass this limitation systems like swindew 
proposed a widely distributed process execution in which 
each node knows where to execute the next activity or join 
activities in a peer-to-peer environment 
in the specific area of activity distribution and scheduling 
emphasized in this work gridflow is remarkable it uses 
a two-level scheduling global and local in the local level 
it has services that predict computing resource utilization 
and activity duration based on this information gridflow 
employs a pert-like technique that tries to forecast the 
activity execution start time and duration in order to better 
schedule them to the available resources 
the architecture proposed in this paper which 
encompasses a programming model and an execution support 
infrastructure is widely decentralized differently from webflow 
and opera-g being more scalable and fault-tolerant but 
like the latter it is designed to support execution recovery 
comparing to swindew the proposed architecture 
contains widely distributed process coordinators which 
coordinate processes or parts of them differently from swindew 
where each node has a limited view of the process only the 
activity that starts next this makes easier to monitor and 
control processes 
finally the support infrastructure breaks the process and 
its subprocesses for grid execution allowing a group to 
require another group for the coordination and execution of 
process elements on behalf of the first one this is 
different from gridflow which can execute a process in at most 
two levels having the global level as the only responsible to 
schedule subprocesses in other groups this can limit the 
overall performance of processes and make the system less 
scalable 
 conclusion and future work 
grid computing is an emerging research field that intends 
to promote distributed and parallel computing over the wide 
area network of heterogeneous and autonomous 
administrative domains in a seamless way similar to what internet 
does to the data sharing there are several products that 
support execution of independent tasks over grid but only a 
few supports the execution of processes with interdependent 
tasks 
in order to address such subject this paper proposes a 
programming model and a support infrastructure that 
allow the execution of structured processes in a widely 
distributed and hierarchical manner this support 
infrastructure provides automatic structured and recursive 
distribution of process elements over groups of available machines 
better resource use due to its on demand creation of 
process elements easy process monitoring and steering due to 
its structured nature and localized communication among 
strong dependent process elements which are placed under 
the same controller these features contribute to better 
scalability fault-tolerance and control for processes execution 
over the grid moreover it opens doors for better scheduling 
algorithms recovery mechanisms and also dynamic 
modification schemes 
the next work will be the implementation of a recovery 
mechanism that uses the execution and data state of 
processes and controllers to recover process execution after 
that it is desirable to advance the scheduling algorithm to 
forecast machine use in the same or other groups and to 
foresee start time of process elements in order to use this 
information to pre-allocate resources and then obtain a 
better process execution performance finally it is 
interesting to investigate schemes of dynamic modification of 
processes over the grid in order to evolve and adapt long-term 
processes to the continuously changing grid environment 
 acknowledgments 
we would like to thank paulo c oliveira from the state 
treasury department of sao paulo for its deeply revision 
and insightful comments 
 references 
 e akarsu g c fox w furmanski and t haupt 
webflow high-level programming environment and 
visual authoring toolkit for high performance 
distributed computing in proceedings of 
supercom puting sc 
 t andrews and f curbera specification business 
process execution language for w eb services v ersion 
 ibm developerworks available at 
 
http www- ibm com developerworks library wsbpel 
 w bausch o pera -g a m icrokernelfor 
com putationalg rids phd thesis swiss federal 
institute of technology zurich 
 t bray and j paoli extensible m arkup language 
 x m l xml core wg w c available at 
http www w org tr rec-xml- 
 j cao s a jarvis s saini and g r nudd 
gridflow workflow management for grid 
computing in proceedings ofthe international 
sym posium on cluster com puting and the g rid 
 ccg rid 
 i foster and c kesselman globus a 
metacomputing infrastructure toolkit intl j 
supercom puter a pplications - 
 i foster c kesselman j m nick and s tuecke 
the physiology ofthe g rid a n o pen g rid services 
a rchitecture for d istributed system s integration 
 middleware companion 
open grid service infrastructure wg global grid 
forum 
 i foster c kesselman and s tuecke the anatomy 
of the grid enabling scalable virtual organization 
the intl journalofh igh perform ance com puting 
a pplications - 
 j frey t tannenbaum m livny i foster and 
s tuecke condor-g a computational management 
agent for multi-institutional grids in proceedings of 
the tenth intl sym posium on h igh perform ance 
d istributed com puting h pd c- ieee 
 a s grimshaw and w a wulf legion - a view 
from feet in proceedings ofthe fifth intl 
sym posium on h igh perform ance d istributed 
com puting ieee 
 t lindholm and f yellin the java v irtualm achine 
specification sun microsystems second edition 
edition 
 b r schulze and e r m madeira grid computing 
with active services concurrency and com putation 
practice and experience journal - 
 j yan y yang and g k raikundalia enacting 
business processes in a decentralised environment 
with p p-based workflow support in proceedings of 
the fourth intl conference on w eb-age inform ation 
m anagem ent w a im 
middleware for grid computing 
demonstration of grid-enabled ensemble kalman filter 
data assimilation methodology for reservoir 
characterization 
ravi vadapalli 
high performance computing center 
texas tech university 
lubbock tx 
 - - - 
ravi vadapalli ttu edu 
ajitabh kumar 
department of petroleum engineering 
texas a m university 
college station tx 
 - - - 
akumar tamu edu 
ping luo 
supercomputing facility 
texas a m university 
college station tx 
 - - - 
pingluo sc tamu edu 
shameem siddiqui 
department of petroleum engineering 
texas tech university 
lubbock tx 
 - - - 
shameem siddiqui ttu edu 
taesung kim 
supercomputing facility 
texas a m university 
college station tx 
 - - - 
tskim sc tamu edu 
abstract 
ensemble kalman filter data assimilation methodology is a 
popular approach for hydrocarbon reservoir simulations in energy 
exploration in this approach an ensemble of geological models 
and production data of oil fields is used to forecast the dynamic 
response of oil wells the schlumberger eclipse software is 
used for these simulations since models in the ensemble do not 
communicate message-passing implementation is a good choice 
each model checks out an eclipse license and therefore 
parallelizability of reservoir simulations depends on the number 
licenses available we have grid-enabled the ensemble kalman 
filter data assimilation methodology for the tigre grid 
computing environment by pooling the licenses and computing 
resources across the collaborating institutions using gridway 
metascheduler and tigre environment the computational 
accuracy can be increased while reducing the simulation runtime 
in this paper we provide an account of our efforts in 
gridenabling the ensemble kalman filter data assimilation 
methodology potential benefits of this approach observations 
and lessons learned will be discussed 
categories and subject descriptors 
c distributed systems distributed applications 
general terms 
algorithms design performance 
 introduction 
grid computing is an emerging collaborative 
computing paradigm to extend institution organization 
specific high performance computing hpc capabilities 
greatly beyond local resources its importance stems from 
the fact that ground breaking research in strategic 
application areas such as bioscience and medicine energy 
exploration and environmental modeling involve strong 
interdisciplinary components and often require intercampus 
collaborations and computational capabilities beyond 
institutional limitations 
the texas internet grid for research and education 
 tigre is a state funded cyberinfrastructure 
development project carried out by five rice a m ttu 
uh and ut austin major university systems - collectively 
called tigre institutions the purpose of tigre is to 
create a higher education grid to sustain and extend 
research and educational opportunities across texas 
tigre is a project of the high performance computing 
across texas hipcat consortium the goal of 
hipcat is to support advanced computational technologies 
to enhance research development and educational 
activities 
the primary goal of tigre is to design and deploy 
state-of-the-art grid middleware that enables integration of 
computing systems storage systems and databases 
visualization laboratories and displays and even 
instruments and sensors across texas the secondary goal 
is to demonstrate the tigre capabilities to enhance 
research and educational opportunities in strategic 
application areas of interest to the state of texas these are 
bioscience and medicine energy exploration and air quality 
modeling vision of the tigre project is to foster 
interdisciplinary and intercampus collaborations identify 
novel approaches to extend academic-government-private 
partnerships and become a competitive model for external 
funding opportunities the overall goal of tigre is to 
support local campus and regional user interests and offer 
avenues to connect with national grid projects such as 
open science grid and teragrid 
within the energy exploration strategic application area 
we have grid-enabled the ensemble kalman filter enkf 
 approach for data assimilation in reservoir modeling and 
demonstrated the extensibility of the application using the 
tigre environment and the gridway metascheduler 
section provides an overview of the tigre environment 
and capabilities application description and the need for 
grid-enabling enkf methodology is provided in section 
the implementation details and merits of our approach are 
discussed in section conclusions are provided in section 
 finally observations and lessons learned are documented 
in section 
 tigre environment 
the tigre grid middleware consists of minimal set of 
components derived from a subset of the virtual data 
toolkit vdt which supports a variety of operating 
systems the purpose of choosing a minimal software stack 
is to support applications at hand and to simplify 
installation and distribution of client server stacks across 
tigre sites additional components will be added as they 
become necessary the pacman packaging and 
distribution mechanism is employed for tigre 
client server installation and management the pacman 
distribution mechanism involves retrieval installation and 
often configuration of the packaged software this 
approach allows the clients to keep current consistent 
versions of tigre software it also helps tigre sites to 
install the needed components on resources distributed 
throughout the participating sites the tigre client server 
stack consists of an authentication and authorization layer 
globus gram -based job submission via web services 
 pre-web services installations are available up on request 
the tools for handling grid proxy generation grid-enabled 
file transfer and grid-enabled remote login are supported 
the pertinent details of tigre services and tools for job 
scheduling and management are provided below 
 certificate authority 
the tigre security infrastructure includes a certificate 
authority ca accredited by the international grid trust 
federation igtf for issuing x user and resource 
grid certificates the texas advanced computing 
center tacc university of texas at austin is the 
tigre s shared ca the tigre institutions serve as 
registration authorities ra for their respective local user 
base for up-to-date information on securing user and 
resource certificates and their installation instructions see 
ref the users and hosts on tigre are identified by 
their distinguished name dn in their x certificate 
provided by the ca a native grid-mapfile that contains a 
list of authorized dns is used to authenticate and authorize 
user job scheduling and management on tigre site 
resources at texas tech university the users are 
dynamically allocated one of the many generic pool 
accounts this is accomplished through the grid user 
management system gums 
 job scheduling and management 
the tigre environment supports gram -based job 
submission via web services the job submission scripts are 
generated using xml the web services gram translates 
the xml scripts into target cluster specific batch schedulers 
such as lsf pbs or sge the high bandwidth file transfer 
protocols such as gridftp are utilized for staging files in 
and out of the target machine the login to remote hosts for 
compilation and debugging is only through gsissh service 
which requires resource authentication through x 
certificates the authentication and authorization of grid 
jobs are managed by issuing grid certificates to both users 
and hosts the certificate revocation lists crl are 
updated on a daily basis to maintain high security standards 
of the tigre grid services the tigre portal 
documentation area provides a quick start tutorial on 
running jobs on tigre 
 metascheduler 
the metascheduler interoperates with the cluster level 
batch schedulers such as lsf pbs in the overall grid 
workflow management in the present work we have 
employed gridway metascheduler - a globus incubator 
project - to schedule and manage jobs across tigre 
the gridway is a light-weight metascheduler that fully 
utilizes globus functionalities it is designed to provide 
efficient use of dynamic grid resources by multiple users 
for grid infrastructures built on top of globus services the 
tigre site administrator can control the resource sharing 
through a powerful built-in scheduler provided by gridway 
or by extending gridway s external scheduling module to 
provide their own scheduling policies application users 
can write job descriptions using gridway s simple and 
direct job template format see section for details or 
standard job submission description language jsdl 
see section for implementation details 
 customer service management system 
a tigre portal was designed and deployed to interface 
users and resource providers it was designed using 
gridport and is maintained by tacc the tigre 
environment is supported by open source tools such as the 
open ticket request system otrs for servicing 
trouble tickets and moinmoin wiki for tigre 
content and knowledge management for education outreach 
and training the links for otrs and wiki are consumed 
by the tigre portal - the gateway for users and 
resource providers the tigre resource status and loads 
are monitored by the grid port information repository 
 gpir service of the gridport toolkit which interfaces 
with local cluster load monitoring service such as ganglia 
the gpir utilizes cron jobs on each resource to gather 
site specific resource characteristics such as jobs that are 
running queued and waiting for resource allocation 
 ensemble kalman filter 
application 
the main goal of hydrocarbon reservoir simulations is to 
forecast the production behavior of oil and gas field 
 denoted as field hereafter for its development and optimal 
management in reservoir modeling the field is divided into 
several geological models as shown in figure for 
accurate performance forecasting of the field it is necessary 
to reconcile several geological models to the dynamic 
response of the field through history matching - 
figure cross-sectional view of the field vertical 
layers correspond to different geological models and the 
nails are oil wells whose historical information will be 
used for forecasting the production behavior 
 figure ref http faculty smu edu zchen research html 
the enkf is a monte carlo method that works with an 
ensemble of reservoir models this method utilizes 
crosscovariances between the field measurements and the 
reservoir model parameters derived from several models 
to estimate prediction uncertainties the geological model 
parameters in the ensemble are sequentially updated with a 
goal to minimize the prediction uncertainties historical 
production response of the field for over years is used in 
these simulations the main advantage of enkf is that it 
can be readily linked to any reservoir simulator and can 
assimilate latest production data without the need to re-run 
the simulator from initial conditions researchers in texas 
are large subscribers of the schlumberger eclipse 
package for reservoir simulations in the reservoir 
modeling each geological model checks out an eclipse 
license the simulation runtime of the enkf methodology 
depends on the number of geological models used number 
of eclipse licenses available production history of the 
field and propagated uncertainties in history matching 
the overall enkf workflow is shown figure 
figure ensemble kaman filter data assimilation 
workflow each site has l licenses 
at start the master control process enkf main 
program reads the simulation configuration file for number 
 n of models and model-specific input files then n 
working directories are created to store the output files at 
the end of iteration the master control process collects the 
output files from n models and post processes 
crosscovariances to estimate the prediction uncertainties 
this information will be used to update models or input 
files for the next iteration the simulation continues until 
the production histories are exhausted 
typical enkf simulation with n and field histories 
of - years in time steps ranging from three months to a 
year takes about three weeks on a serial computing 
environment 
in parallel computing environment there is no 
interprocess communication between the geological models 
in the ensemble however at the end of each simulation 
time-step model-specific output files are to be collected for 
analyzing cross covariances and to prepare next set of 
input files therefore master-slave model in 
messagepassing mpi environment is a suitable paradigm in this 
approach the geological models are treated as slaves and 
are distributed across the available processors the master 
cluster or tigre gridway 
start 
read configuration file 
create n working directories 
create n input files 
model l model model n 
eclipse 
on site a 
eclipse 
on site b 
eclipse 
on site z 
collect n model outputs 
post-process output files 
end 
 
process collects model-specific output files analyzes and 
prepares next set of input files for the simulation since 
each geological model checks out an eclipse license 
parallelizability of the simulation depends on the number of 
licenses available when the available number of licenses is 
less than the number of models in the ensemble one or 
more of the nodes in the mpi group have to handle more 
than one model in a serial fashion and therefore it takes 
longer to complete the simulation 
a petroleum engineering department usually procures 
 - eclipse licenses while at least ten-fold increase in 
the number of licenses would be necessary for industry 
standard simulations the number of licenses can be 
increased by involving several petroleum engineering 
departments that support eclipse package 
since mpi does not scale very well for applications that 
involve remote compute clusters and to get around the 
firewall issues with license servers across administrative 
domains grid-enabling the enkf workflow seems to be 
necessary with this motivation we have implemented 
grid-enabled enkf workflow for the tigre environment 
and demonstrated parallelizability of the application across 
tigre using gridway metascheduler further details are 
provided in the next section 
 implementation details 
to grid-enable the enkf approach we have eliminated 
the mpi code for parallel processing and replaced with n 
single processor jobs or sub-jobs where n is the number 
of geological models in the ensemble these model-specific 
sub-jobs were distributed across tigre sites that support 
eclipse package using the gridway metascheduler 
for each sub-job we have constructed a gridway job 
template that specifies the executable input and output 
files and resource requirements since the tigre compute 
resources are not expected to change frequently we have 
used static resource discovery policy for gridway and the 
sub-jobs were scheduled dynamically across the tigre 
resources using gridway figure represents the sub-job 
template file for the gridway metascheduler 
figure gridway sub-job template 
in figure requirements flag is set to choose the 
resources that satisfy the application requirements in the 
case of enkf application for example we need resources 
that support eclipse package arguments flag 
specifies the model in the ensemble that will invoke 
eclipse at a remote site input files is prepared by 
the enkf main program or master control process and is 
transferred by gridway to the remote site where it is 
untared and is prepared for execution finally 
output files specifies the name and location where the 
output files are to be written 
the command-line features of gridway were used to 
collect and process the model-specific outputs to prepare 
new set of input files this step mimics mpi process 
synchronization in master-slave model at the end of each 
iteration the compute resources and licenses are committed 
back to the pool table shows the sub-jobs in tigre 
grid via gridway using gwps command and for clarity 
only selected columns were shown 
 
user jid dm em name host 
pingluo wrap pend enkf jt antaeus hpcc ttu edu lsf 
pingluo wrap pend enkf jt antaeus hpcc ttu edu lsf 
pingluo wrap actv enkf jt minigar hpcc ttu edu lsf 
pingluo wrap pend enkf jt minigar hpcc ttu edu lsf 
pingluo wrap done enkf jt cosmos tamu edu pbs 
pingluo wrap epil enkf jt cosmos tamu edu pbs 
table job scheduling across tigre using gridway 
metascheduler dm dispatch state em execution state 
jid is the job id and host corresponds to site specific 
cluster and its local batch scheduler 
when a job is submitted to gridway it will go through a 
series of dispatch dm and execution em states for 
dm the states include pend ing prol og wrap per 
epil og and done dm prol means the job has been 
scheduled to a resource and the remote working directory is 
in preparation dm warp implies that gridway is 
executing the wrapper which in turn executes the 
application dm epil implies the job has finished 
running at the remote site and results are being transferred 
back to the gridway server similarly when em pend 
implies the job is waiting in the queue for resource and the 
job is running when em actv for complete list of 
message flags and their descriptions see the documentation 
in ref 
we have demonstrated the grid-enabled enkf runs 
using gridway for tigre environment the jobs are so 
chosen that the runtime doesn t exceed more than a half 
hour the simulation runs involved up to jobs between 
a m and ttu sites with ttu serving licenses for 
resource information see table i 
one of the main advantages of grid-enabled enkf 
simulation is that both the resources and licenses are 
released back to the pool at the end of each simulation time 
step unlike in the case of mpi implementation where 
licenses and nodes are locked until the completion of entire 
simulation however the fact that each sub-job gets 
scheduled independently via gridway could possibly incur 
another time delay caused by waiting in queue for execution 
in each simulation time step such delays are not expected 
executable runforward 
requirements hostname cosmos tamu edu 
hostname antaeus hpcc ttu edu 
hostname minigar hpcc ttu edu 
arguments 
input files in tar 
output files out tar 
in mpi implementation where the node is blocked for 
processing sub-jobs model-specific calculation until the 
end of the simulation there are two main scenarios for 
comparing grid and cluster computing approaches 
scenario i the cluster is heavily loaded the conceived 
average waiting time of job requesting large number of 
cpus is usually longer than waiting time of jobs requesting 
single cpu therefore overall waiting time could be 
shorter in grid approach which requests single cpu for 
each sub-job many times compared to mpi implementation 
that requests large number of cpus at a single time it is 
apparent that grid scheduling is beneficial especially when 
cluster is heavily loaded and requested number of cpus for 
the mpi job is not readily available 
scenario ii the cluster is relatively less loaded or 
largely available it appears the mpi implementation is 
favorable compared to the grid scheduling however 
parallelizability of the enkf application depends on the 
number of eclipse licenses and ideally the number of 
licenses should be equal to the number of models in the 
ensemble therefore if a single institution does not have 
sufficient number of licenses the cluster availability doesn t 
help as much as it is expected 
since the collaborative environment such as tigre can 
address both compute and software resource requirements 
for the enkf application grid-enabled approach is still 
advantageous over the conventional mpi implementation in 
any of the above scenarios 
 conclusions and future work 
tigre is a higher education grid development project 
and its purpose is to sustain and extend research and 
educational opportunities across texas within the energy 
exploration application area we have grid-enabled the mpi 
implementation of the ensemble kalman filter data 
assimilation methodology for reservoir characterization 
this task was accomplished by removing mpi code for 
parallel processing and replacing with single processor jobs 
one for each geological model in the ensemble these 
single processor jobs were scheduled across tigre via 
gridway metascheduler we have demonstrated that by 
pooling licenses across tigre sites more geological 
models can be handled in parallel and therefore conceivably 
better simulation accuracy this approach has several 
advantages over mpi implementation especially when a site 
specific cluster is heavily loaded and or the number licenses 
required for the simulation is more than those available at a 
single site 
towards the future work it would be interesting to 
compare the runtime between mpi and grid 
implementations for the enkf application this effort could 
shed light on quality of service qos of grid environments 
in comparison with cluster computing 
another aspect of interest in the near future would be 
managing both compute and license resources to address 
the job or processor -to-license ratio management 
 observations and lessions 
learned 
the grid-enabling efforts for enkf application have 
provided ample opportunities to gather insights on the 
visibility and promise of grid computing environments for 
application development and support the main issues are 
industry standard data security and qos comparable to 
cluster computing 
since the reservoir modeling research involves 
proprietary data of the field we had to invest substantial 
efforts initially in educating the application researchers on 
the ability of grid services in supporting the industry 
standard data security through role- and privilege-based 
access using x standard 
with respect to qos application researchers expect 
cluster level qos with grid environments also there is a 
steep learning curve in grid computing compared to the 
conventional cluster computing since grid computing is 
still an emerging technology and it spans over several 
administrative domains grid computing is still premature 
especially in terms of the level of qos although it offers 
better data security standards compared to commodity 
clusters 
it is our observation that training and outreach programs 
that compare and contrast the grid and cluster computing 
environments would be a suitable approach for enhancing 
user participation in grid computing this approach also 
helps users to match their applications and abilities grids 
can offer 
in summary our efforts through tigre in grid-enabling 
the enkf data assimilation methodology showed 
substantial promise in engaging petroleum engineering 
researchers through intercampus collaborations efforts are 
under way to involve more schools in this effort these 
efforts may result in increased collaborative research 
educational opportunities and workforce development 
through graduate faculty research programs across tigre 
institutions 
 acknowledgments 
the authors acknowledge the state of texas for supporting 
the tigre project through the texas enterprise fund and 
tigre institutions for providing the mechanism in which 
the authors ravi vadapalli taesung kim and ping luo 
are also participating the authors thank the application 
researchers prof akhil datta-gupta of texas a m 
university and prof lloyd heinze of texas tech 
university for their discussions and interest to exploit the 
tigre environment to extend opportunities in research and 
development 
 references 
 foster i and kesselman c eds the grid blueprint 
for a new computing infrastructure the elsevier series in 
grid computing 
 tigre portal http tigreportal hipcat net 
 vadapalli r sill a dooley r murray m luo p kim 
t huang m thyagaraja k and chaffin d 
demonstration of tigre environment for grid 
enabled suitable applications th 
ieee acm int conf on 
grid computing sept - austin 
 the high performance computing across texas consortium 
http www hipcat net 
 pordes r petravick d kramer b olson d livny m 
roy a avery p blackburn k wenaus t würthwein f 
foster i gardner r wilde m blatecky a mcgee j and 
quick r the open science grid j phys conf series 
http www iop org ej abstract - and 
http www opensciencegrid org 
 reed d a grids the teragrid and beyond 
computer vol no and http www teragrid org 
 evensen g data assimilation the ensemble kalman 
filter springer 
 herrera j huedo e montero r s and llorente i m 
 scientific programming vol no pp - 
 avery p and foster i the griphyn project towards 
petascale virtual data grids technical report 
griphyn- and http vdt cs wisc edu 
 the pacman documentation and installation guide 
http physics bu edu pacman htmls 
 caskey p murray m perez j and sill a case 
studies in identify management for virtual organizations 
educause southwest reg conf feb - austin tx 
http www educause edu ir library pdf swr pdf 
 the grid user management system gums 
https www racf bnl gov facility gums index html 
 thomas m and boisseau j building grid computing 
portals the npaci grid portal toolkit grid computing 
making the global infrastructure a reality chapter 
berman f fox g thomas m boisseau j and hey t 
 eds john wiley and sons ltd chichester 
 open ticket request system http otrs org 
 the moinmoin wiki engine 
http moinmoin wikiwikiweb de 
 vasco d w yoon s and datta-gupta a integrating 
dynamic data into high resolution reservoir models using 
streamline-based analytic sensitivity coefficients society of 
petroleum engineers spe journal 
 emanuel a s and milliken w j history matching 
finite difference models with d streamlines spe 
proc of the annual technical conf and exhibition sept 
 new orleans la 
 nævdal g johnsen l m aanonsen s i and vefring e h 
 reservoir monitoring and continuous model updating 
using ensemble kalman filter spe proc of the 
annual technical conf and exhibition oct - denver 
co 
 jafarpour b and mclaughlin d b history matching 
with an ensemble kalman filter and discrete cosine 
parameterization spe proc of the annual technical 
conf and exhibition nov - anaheim ca 
 li g and reynolds a c an iterative ensemble 
kalman filter for data assimilation spe proc of the 
spe annual technical conf and exhibition nov - 
anaheim ca 
 arroyo-negrete e devagowda d datta-gupta a 
streamline assisted ensemble kalman filter for rapid and 
continuous reservoir model updating proc of the int oil 
gas conf and exhibition spe dec - china 
 eclipse reservoir engineering software 
http www slb com content services software reseng index a 
sp 
tracking immediate predecessors 
in distributed computations 
emmanuelle anceaume jean-michel h´elary michel raynal 
irisa campus beaulieu 
 rennes cedex france 
firstname lastname irisa fr 
abstract 
a distributed computation is usually modeled as a partially 
ordered set of relevant events the relevant events are a 
subset of the primitive events produced by the computation 
an important causality-related distributed computing 
problem that we call the immediate predecessors tracking ipt 
problem consists in associating with each relevant event on 
the fly and without using additional control messages the 
set of relevant events that are its immediate predecessors in 
the partial order so ipt is the on-the-fly computation of 
the transitive reduction i e hasse diagram of the causality 
relation defined by a distributed computation this paper 
addresses the ipt problem it presents a family of 
protocols that provides each relevant event with a timestamp that 
exactly identifies its immediate predecessors the family is 
defined by a general condition that allows application 
messages to piggyback control information whose size can be 
smaller than n the number of processes in that sense 
this family defines message size-efficient ipt protocols 
according to the way the general condition is implemented 
different ipt protocols can be obtained two of them are 
exhibited 
categories and subject descriptors 
c distributed systems 
general terms 
asynchronous distributed computations 
 introduction 
a distributed computation consists of a set of processes 
that cooperate to achieve a common goal a main 
characteristic of these computations lies in the fact that the 
processes do not share a common global memory and 
communicate only by exchanging messages over a 
communication network moreover message transfer delays are finite 
but unpredictable this computation model defines what 
is known as the asynchronous distributed system model it 
is particularly important as it includes systems that span 
large geographic areas and systems that are subject to 
unpredictable loads consequently the concepts tools and 
mechanisms developed for asynchronous distributed systems 
reveal to be both important and general 
causality is a key concept to understand and master the 
behavior of asynchronous distributed systems more 
precisely given two events e and f of a distributed 
computation a crucial problem that has to be solved in a lot of 
distributed applications is to know whether they are causally 
related i e if the occurrence of one of them is a consequence 
of the occurrence of the other the causal past of an event 
e is the set of events from which e is causally dependent 
events that are not causally dependent are said to be 
concurrent vector clocks have been introduced to allow 
processes to track causality and concurrency between the 
events they produce the timestamp of an event produced 
by a process is the current value of the vector clock of the 
corresponding process in that way by associating vector 
timestamps with events it becomes possible to safely decide 
whether two events are causally related or not 
usually according to the problem he focuses on a 
designer is interested only in a subset of the events produced by 
a distributed execution e g only the checkpoint events are 
meaningful when one is interested in determining 
consistent global checkpoints it follows that detecting causal 
dependencies or concurrency on all the events of the 
distributed computation is not desirable in all applications 
 in other words among all the events that may occur 
in a distributed computation only a subset of them are 
relevant in this paper we are interested in the restriction of 
the causality relation to the subset of events defined as being 
the relevant events of the computation 
being a strict partial order the causality relation is 
transitive as a consequence among all the relevant events that 
causally precede a given relevant event e only a subset are 
its immediate predecessors those are the events f such that 
there is no relevant event on any causal path from f to e 
unfortunately given only the vector timestamp associated 
with an event it is not possible to determine which events of 
its causal past are its immediate predecessors this comes 
from the fact that the vector timestamp associated with e 
determines for each process the last relevant event 
belong 
ing to the causal past of e but such an event is not 
necessarily an immediate predecessor of e however some 
applications require to associate with each relevant event only 
the set of its immediate predecessors those applications are 
mainly related to the analysis of distributed computations 
some of those analyses require the construction of the 
lattice of consistent cuts produced by the computation 
it is shown in that the tracking of immediate 
predecessors allows an efficient on the fly construction of this lattice 
more generally these applications are interested in the very 
structure of the causal past in this context the 
determination of the immediate predecessors becomes a major issue 
 additionally in some circumstances this determination 
has to satisfy behavior constraints if the communication 
pattern of the distributed computation cannot be modified 
the determination has to be done without adding control 
messages when the immediate predecessors are used to 
monitor the computation it has to be done on the fly 
we call immediate predecessor tracking ipt the 
problem that consists in determining on the fly and without 
additional messages the immediate predecessors of relevant 
events this problem consists actually in determining the 
transitive reduction hasse diagram of the causality graph 
generated by the relevant events of the computation 
solving this problem requires tracking causality hence using 
vector clocks previous works have addressed the efficient 
implementation of vector clocks to track causal dependence on 
relevant events their aim was to reduce the size of 
timestamps attached to messages an efficient vector clock 
implementation suited to systems with fifo channels is proposed 
in another efficient implementation that does not 
depend on channel ordering property is described in the 
notion of causal barrier is introduced in to reduce 
the size of control information required to implement causal 
multicast however none of these papers considers the 
ipt problem this problem has been addressed for the first 
time to our knowledge in where an ipt protocol 
is described but without correctness proof moreover in 
this protocol timestamps attached to messages are of size 
n this raises the following question which to our 
knowledge has never been answered are there efficient vector 
clock implementation techniques that are suitable for the ipt 
problem 
this paper has three main contributions a positive 
answer to the previous open question the design of a 
family of efficient ipt protocols and a formal 
correctness proof of the associated protocols from a 
methodological point of view the paper uses a top-down approach it 
states abstract properties from which more concrete 
properties and protocols are derived the family of ipt 
protocols is defined by a general condition that allows 
application messages to piggyback control information whose size 
can be smaller than the system size i e smaller than the 
number of processes composing the system in that sense 
this family defines low cost ipt protocols when we 
consider the message size in addition to efficiency the proposed 
approach has an interesting design property namely the 
family is incrementally built in three steps the basic 
vector clock protocol is first enriched by adding to each process 
a boolean vector whose management allows the processes 
to track the immediate predecessor events then a general 
condition is stated to reduce the size of the control 
information carried by messages finally according to the way this 
condition is implemented three ipt protocols are obtained 
the paper is composed of seven sections sections 
introduces the computation model vector clocks and the notion 
of relevant events section presents the first step of the 
construction that results in an ipt protocol in which each 
message carries a vector clock and a boolean array both 
of size n the number of processes section improves 
this protocol by providing the general condition that allows 
a message to carry control information whose size can be 
smaller than n section provides instantiations of this 
condition section provides a simulation study comparing 
the behaviors of the proposed protocols finally section 
concludes the paper due to space limitations proofs of 
lemmas and theorems are omitted they can be found in 
 
 model and vector clock 
 distributed computation 
a distributed program is made up of sequential local 
programs which communicate and synchronize only by 
exchanging messages a distributed computation describes the 
execution of a distributed program the execution of a local 
program gives rise to a sequential process let {p p 
pn} be the finite set of sequential processes of the distributed 
computation each ordered pair of communicating processes 
 pi pj is connected by a reliable channel cij through which 
pi can send messages to pj we assume that each message 
is unique and a process does not send messages to itself 
 
message transmission delays are finite but unpredictable 
moreover channels are not necessarily fifo process speeds 
are positive but arbitrary in other words the underlying 
computation model is asynchronous 
the local program associated with pi can include send 
receive and internal statements the execution of such a 
statement produces a corresponding send receive internal 
event these events are called primitive events let ex 
i 
be the x-th event produced by process pi the sequence 
hi e 
i e 
i ex 
i constitutes the history of pi denoted 
hi let h ∪n 
i hi be the set of events produced by a 
distributed computation this set is structured as a partial 
order by lamport s happened before relation denoted 
hb 
→ and defined as follows ex 
i 
hb 
→ ey 
j if and only if 
 i j ∧ x y local precedence ∨ 
 ∃m ex 
i send m ∧ ey 
j receive m msg prec ∨ 
 ∃ ez 
k ex 
i 
hb 
→ ez 
k ∧ e z 
k 
hb 
→ ey 
j transitive closure 
max ex 
i ey 
j is a partial function defined only when ex 
i and 
ey 
j are ordered it is defined as follows max ex 
i ey 
j ex 
i if 
ey 
j 
hb 
→ ex 
i max ex 
i ey 
j ey 
i if ex 
i 
hb 
→ ey 
j 
clearly the restriction of 
hb 
→ to hi for a given i is a total 
order thus we will use the notation ex 
i ey 
i iff x y 
throughout the paper we will use the following notation 
if e ∈ hi is not the first event produced by pi then pred e 
denotes the event immediately preceding e in the sequence 
hi if e is the first event produced by pi then pred e is 
denoted by ⊥ meaning that there is no such event and 
∀e ∈ hi ⊥ e the partial order bh h 
hb 
→ 
constitutes a formal model of the distributed computation it is 
associated with 
 
this assumption is only in order to get simple protocols 
 
p 
p 
p 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
figure timestamped relevant events and immediate predecessors graph hasse diagram 
 relevant events 
for a given observer of a distributed computation only 
some events are relevant 
 an interesting example 
of what an observation is is the detection of predicates 
on consistent global states of a distributed computation 
 in that case a relevant event corresponds 
to the modification of a local variable involved in the global 
predicate another example is the checkpointing problem 
where a relevant event is the definition of a local checkpoint 
 
the left part of figure depicts a distributed computation 
using the classical space-time diagram in this figure only 
relevant events are represented the sequence of relevant 
events produced by process pi is denoted by ri and r 
∪n 
i ri ⊆ h denotes the set of all relevant events let → 
be the relation on r defined in the following way 
∀ e f ∈ r × r e → f ⇔ e 
hb 
→ f 
the poset r → constitutes an abstraction of the 
distributed computation in the following we consider a 
distributed computation at such an abstraction level 
moreover without loss of generality we consider that the set of 
relevant events is a subset of the internal events if a 
communication event has to be observed a relevant internal event 
can be generated just before a send and just after a receive 
communication event occurred each relevant event is 
identified by a pair process id sequence number see figure 
definition the relevant causal past of an event e ∈ 
h is the partially ordered subset of relevant events f such 
that f 
hb 
→ e it is denoted ↑ e we have ↑ e {f ∈ 
r f 
hb 
→ e} 
note that if e ∈ r then ↑ e {f ∈ r f → e} in 
the computation described in figure we have for the 
event e identified ↑ e { } 
the following properties are immediate consequences of the 
previous definitions let e ∈ h 
cp if e is not a receive event then 
↑ e 
 
 
 
∅ if pred e ⊥ 
↑ pred e ∪ {pred e } if pred e ∈ r 
↑ pred e if pred e ∈ r 
cp if e is a receive event of a message m then 
↑ e 
 
 
 
↑ send m if pred e ⊥ 
↑ pred e ∪ ↑ send m ∪ {pred e } 
if pred e ∈ r 
↑ pred e ∪ ↑ send m if pred e ∈ r 
 
those events are sometimes called observable events 
definition let e ∈ hi for every j such that ↑ e ∩ 
rj ∅ the last relevant event of pj with respect to e is 
lastr e j max{f f ∈↑ e ∩ rj} when ↑ e ∩ rj ∅ 
lastr e j is denoted by ⊥ meaning that there is no such 
event 
let us consider the event e identified in figure we 
have lastr e lastr e lastr e 
 the following properties relate the events lastr e j 
and lastr f j for all the predecessors f of e in the relation 
hb 
→ these properties follow directly from the definitions 
let e ∈ hi 
lr ∀e ∈ hi 
lastr e i 
 
 
 
⊥ if pred e ⊥ 
pred e if pred e ∈ r 
lastr pred e i if pred e ∈ r 
lr if e is not a receipt event ∀j i 
lastr e j lastr pred e j 
lr if e is a receive event of m ∀j i 
lastr e j max lastr pred e j lastr send m j 
 vector clock system 
definition as a fundamental concept associated with the 
causality theory vector clocks have been introduced in 
simultaneously and independently by fidge and mattern 
 a vector clock system is a mechanism that associates 
timestamps with events in such a way that the 
comparison of their timestamps indicates whether the 
corresponding events are or are not causally related and if they are 
which one is the first more precisely each process pi has a 
vector of integers v ci n such that v ci j is the number 
of relevant events produced by pj that belong to the 
current relevant causal past of pi note that v ci i counts the 
number of relevant events produced so far by pi when a 
process pi produces a relevant event e it associates with 
e a vector timestamp whose value denoted e v c is equal 
to the current value of v ci 
vector clock implementation the following 
implementation of vector clocks is based on the observation 
that ∀i ∀e ∈ hi ∀j e v ci j y ⇔ lastr e j ey 
j 
where e v ci is the value of v ci just after the occurrence 
of e this relation results directly from the properties lr 
lr and lr each process pi manages its vector clock 
v ci n according to the following rules 
vc v ci n is initialized to 
vc each time it produces a relevant event e pi increments 
its vector clock entry v ci i v ci i v ci i to 
 
indicate it has produced one more relevant event then 
pi associates with e the timestamp e v c v ci 
vc when a process pi sends a message m it attaches to 
m the current value of v ci let m v c denote this 
value 
vc when pi receives a message m it updates its vector 
clock as follows ∀k v ci k max v ci k m v c k 
 immediate predecessors 
in this section the immediate predecessor tracking 
 ipt problem is stated section then some technical 
properties of immediate predecessors are stated and proved 
 section these properties are used to design the basic 
ipt protocol and prove its correctness section this 
ipt protocol previously presented in without proof is 
built from a vector clock protocol by adding the 
management of a local boolean array at each process 
 the ipt problem 
as indicated in the introduction some applications e g 
analysis of distributed executions detection of 
distributed properties require to determine on-the-fly and 
without additional messages the transitive reduction of the 
relation → i e we must not consider transitive causal 
dependency given two relevant events f and e we say that f 
is an immediate predecessor of e if f → e and there is no 
relevant event g such that f → g → e 
definition the immediate predecessor tracking 
 ipt problem consists in associating with each relevant event 
e the set of relevant events that are its immediate 
predecessors moreover this has to be done on the fly and without 
additional control message i e without modifying the 
communication pattern of the computation 
as noted in the introduction the ipt problem is the 
computation of the hasse diagram associated with the partially 
ordered set of the relevant events produced by a distributed 
computation 
 formal properties of ipt 
in order to design a protocol solving the ipt problem it 
is useful to consider the notion of immediate relevant 
predecessor of any event whether relevant or not first we 
observe that by definition the immediate predecessor on 
pj of an event e is necessarily the lastr e j event 
second for lastr e j to be immediate predecessor of e there 
must not be another lastr e k event on a path between 
lastr e j and e these observations are formalized in the 
following definition 
definition let e ∈ hi the set of immediate 
relevant predecessors of e denoted ip e is the set of the relevant 
events lastr e j j n such that ∀k lastr e j ∈↑ 
 lastr e k 
it follows from this definition that ip e ⊆ {lastr e j j 
 n} ⊂↑ e when we consider figure the graph 
depicted in its right part describes the immediate predecessors 
of the relevant events of the computation defined in its left 
part more precisely a directed edge e f means that the 
relevant event e is an immediate predecessor of the relevant 
event f 
 
the following lemmas show how the set of immediate 
predecessors of an event is related to those of its predecessors 
in the relation 
hb 
→ they will be used to design and prove 
the protocols solving the ipt problem to ease the reading 
of the paper their proofs are presented in appendix a 
the intuitive meaning of the first lemma is the following 
if e is not a receive event all the causal paths arriving at e 
have pred e as next-to-last event see cp so if pred e 
is a relevant event all the relevant events belonging to its 
relevant causal past are separated from e by pred e and 
pred e becomes the only immediate predecessor of e in 
other words the event pred e constitutes a reset w r t 
the set of immediate predecessors of e on the other hand 
if pred e is not relevant it does not separate its relevant 
causal past from e 
lemma if e is not a receive event ip e is equal to 
∅ if pred e ⊥ 
{pred e } if pred e ∈ r 
ip pred e if pred e ∈ r 
the intuitive meaning of the next lemma is as follows if 
e is a receive event receive m the causal paths arriving 
at e have either pred e or send m as next-to-last events 
if pred e is relevant as explained in the previous lemma 
this event hides from e all its relevant causal past and 
becomes an immediate predecessor of e concerning the 
last relevant predecessors of send m only those that are 
not predecessors of pred e remain immediate predecessors 
of e 
lemma let e ∈ hi be the receive event of a message 
m if pred e ∈ ri then ∀j ip e ∩ rj is equal to 
{pred e } if j i 
∅ if lastr pred e j ≥ lastr send m j 
ip send m ∩ rj if lastr pred e j lastr send m j 
the intuitive meaning of the next lemma is the following 
if e is a receive event receive m and pred e is not 
relevant the last relevant events in the relevant causal past of e are 
obtained by merging those of pred e and those of send m 
and by taking the latest on each process so the 
immediate predecessors of e are either those of pred e or those 
of send m on a process where the last relevant events 
of pred e and of send m are the same event f none of 
the paths from f to e must contain another relevant event 
and thus f must be immediate predecessor of both events 
pred e and send m 
lemma let e ∈ hi be the receive event of a message 
m if pred e ∈ ri then ∀j ip e ∩ rj is equal to 
ip pred e ∩ rj if lastr pred e j lastr send m j 
ip send m ∩ rj if lastr pred e j lastr send m j 
ip pred e ∩ip send m ∩rj if lastr pred e j lastr 
 send m j 
 a basic ipt protocol 
the basic protocol proposed here associates with each 
relevant event e an attribute encoding the set ip e of its 
immediate predecessors from the previous lemmas the set 
 
actually this graph is the hasse diagram of the partial 
order associated with the distributed computation 
 
ip e of any event e depends on the sets ip of the events 
pred e and or send m when e receive m hence the 
idea to introduce a data structure allowing to manage the 
sets ips inductively on the poset h 
hb 
→ to take into 
account the information from pred e each process manages 
a boolean array ipi such that ∀e ∈ hi the value of ipi 
when e occurs denoted e ipi is the boolean array 
representation of the set ip e more precisely ∀j ipi j 
 ⇔ lastr e j ∈ ip e as recalled in section the 
knowledge of lastr e j for every e and every j is based 
on the management of vectors v ci thus the set ip e is 
determined in the following way 
ip e {ey 
j e v ci j y ∧ e ipi j j n} 
each process pi updates ipi according to the lemmas 
 and 
 it results from lemma that if e is not a receive event 
the current value of ipi is sufficient to determine e ipi 
it results from lemmas and that if e is a receive 
event e receive m then determining e ipi 
involves information related to the event send m more 
precisely this information involves ip send m and 
the timestamp of send m needed to compare the 
events lastr send m j and lastr pred e j for 
every j so both vectors send m v cj and send m ipj 
 assuming send m produced by pj are attached to 
message m 
 moreover ipi must be updated upon the occurrence 
of each event in fact the value of ipi just after an 
event e is used to determine the value succ e ipi in 
particular as stated in the lemmas the determination 
of succ e ipi depends on whether e is relevant or not 
thus the value of ipi just after the occurrence of event 
e must keep track of this event 
the following protocol previously presented in without 
proof ensures the correct management of arrays v ci as in 
section and ipi according to the lemmas of section 
 the timestamp associated with a relevant event e is 
denoted e ts 
r initialization both v ci n and ipi n are 
initialized to 
r each time it produces a relevant event e 
- pi associates with e the timestamp e ts defined 
as follows e ts { k v ci k ipi k } 
- pi increments its vector clock entry v ci i 
 namely it executes v ci i v ci i 
- pi resets ipi ∀ i ipi ipi i 
r when pi sends a message m to pj it attaches to m 
the current values of v ci denoted m v c and the 
boolean array ipi denoted m ip 
r when it receives a message m from pj pi executes the 
following updates 
∀k ∈ n case 
v ci k m v c k thenv ci k m v c k 
ipi k m ip k 
v ci k m v c k then ipi k min ipi k m ip k 
v ci k m v c k then skip 
endcase 
the proof of the following theorem directly follows from 
lemmas and 
theorem the protocol described in section solves 
the ipt problem for any relevant event e the timestamp 
e ts contains the identifiers of all its immediate 
predecessors and no other event identifier 
 a general condition 
this section addresses a previously open problem 
namely how to solve the ipt problem without requiring each 
application message to piggyback a whole vector clock and 
a whole boolean array first a general condition that 
characterizes which entries of vectors v ci and ipi can be 
omitted from the control information attached to a message 
sent in the computation is defined section it is then 
shown section that this condition is both sufficient and 
necessary 
however this general condition cannot be locally 
evaluated by a process that is about to send a message thus 
locally evaluable approximations of this general condition 
must be defined to each approximation corresponds a 
protocol implemented with additional local data structures in 
that sense the general condition defines a family of ipt 
protocols that solve the previously open problem this issue 
is addressed in section 
 to transmit or not to transmit control 
information 
let us consider the previous ipt protocol section 
rule r shows that a process pj does not systematically 
update each entry v cj k each time it receives a message 
m from a process pi there is no update of v cj k when 
v cj k ≥ m v c k in such a case the value m v c k is 
useless and could be omitted from the control information 
transmitted with m by pi to pj 
similarly some entries ipj k are not updated when a 
message m from pi is received by pj this occurs when 
 v cj k m v c k ∧ m ip k or when v cj k 
m v c k or when m v c k in the latest case as 
m ip k ipi k then no update of ipj k is necessary 
differently some other entries are systematically reset to 
 this occurs when v cj k m v c k ∧ m ip k 
these observations lead to the definition of the condition 
k m k that characterizes which entries of vectors v ci and 
ipi can be omitted from the control information attached 
to a message m sent by a process pi to a process pj 
definition k m k ≡ 
 send m v ci k 
∨ send m v ci k pred receive m v cj k 
∨ 
 
 send m v ci k pred receive m v cj k 
∧ send m ipi k 
 a necessary and sufficient condition 
we show here that the condition k m k is both 
necessary and sufficient to decide which triples of the form 
 k send m v ci k send m ipi k can be omitted in an 
outgoing message m sent by pi to pj a triple attached to 
m will also be denoted k m v c k m ip k due to space 
limitations the proofs of lemma and lemma are given 
in the proof of theorem follows directly from these 
lemmas 
 
lemma sufficiency if k m k is true then the triple 
 k m v c k m ip k is useless with respect to the correct 
management of ipj k and v cj k 
lemma necessity if k m k is false then the triple 
 k m v c k m ip k is necessary to ensure the correct 
management of ipj k and v cj k 
theorem when a process pi sends m to a process pj 
the condition k m k is both necessary and sufficient not to 
transmit the triple k send m v ci k send m ipi k 
 a family of ipt protocols based 
on evaluable conditions 
it results from the previous theorem that if pi could 
evaluate k m k when it sends m to pj this would 
allow us improve the previous ipt protocol in the following 
way in rule r the triple k v ci k ipi k is 
transmitted with m only if ¬k m k moreover rule r is 
appropriately modified to consider only triples carried by m 
however as previously mentioned pi cannot locally 
evaluate k m k when it is about to send m more 
precisely when pi sends m to pj pi knows the exact values of 
send m v ci k and send m ipi k they are the current 
values of v ci k and ipi k but as far as the value of 
pred receive m v cj k is concerned two cases are 
possible case i if pred receive m 
hb 
→ send m then pi can 
know the value of pred receive m v cj k and 
consequently can evaluate k m k case ii if pred receive m 
and send m are concurrent pi cannot know the value of 
pred receive m v cj k and consequently cannot evaluate 
k m k moreover when it sends m to pj whatever the 
case i or ii that actually occurs pi has no way to know 
which case does occur hence the idea to define evaluable 
approximations of the general condition let k m k be 
an approximation of k m k that can be evaluated by a 
process pi when it sends a message m to be correct the 
condition k must ensure that every time pi should 
transmit a triple k v ci k ipi k according to theorem i e 
each time ¬k m k then pi transmits this triple when it 
uses condition k hence the definition of a correct 
evaluable approximation 
definition a condition k locally evaluable by a 
process when it sends a message m to another process is 
correct if ∀ m k ¬k m k ⇒ ¬k m k or equivalently 
∀ m k k m k ⇒ k m k 
this definition means that a protocol evaluating k to 
decide which triples must be attached to messages does not 
miss triples whose transmission is required by theorem 
let us consider the constant condition denoted k 
that is always false i e ∀ m k k m k false this 
trivially correct approximation of k actually corresponds 
to the particular ipt protocol described in section in 
which each message carries a whole vector clock and a 
whole boolean vector the next section presents a better 
approximation of k denoted k 
 a boolean matrix-based evaluable 
condition 
condition k is based on the observation that condition 
k is composed of sub-conditions some of them can be 
pj 
send m 
pi 
v ci k x 
ipi k 
v cj k ≥ x receive m 
figure the evaluable condition k 
locally evaluated while the others cannot more 
precisely k ≡ a ∨ α ∨ β ∧ b where a ≡ send m v ci k 
and b ≡ send m ipi k are locally evaluable 
whereas α ≡ send m v ci k pred receive m v cj k and 
β ≡ send m v ci k pred receive m v cj k are not 
but from easy boolean calculus a∨ α∨β ∧b ⇒ a∨α∨ 
 β ∧ b ≡ k this leads to condition k ≡ a ∨ γ ∧ b where 
γ α ∨ β ≡ send m v ci k ≤ pred receive m v cj k 
i e k ≡ send m v ci k ≤ pred receive m v cj k ∧ 
send m ipi k ∨ send m v ci k 
so pi needs to approximate the predicate send m v ci k 
≤ pred receive m v cj k to be correct this 
approximation has to be a locally evaluable predicate ci j k such that 
when pi is about to send a message m to pj ci j k ⇒ 
 send m v ci k ≤ pred receive m v cj k informally 
that means that when ci j k holds the local context of 
pi allows to deduce that the receipt of m by pj will not 
lead to v cj k update pj knows as much as pi about 
pk hence the concrete condition k is the following 
k ≡ send m v ci k ∨ ci j k ∧ send m ipi k 
let us now examine the design of such a predicate 
 denoted ci first the case j i can be ignored since it is 
assumed section that a process never sends a 
message to itself second in the case j k the relation 
send m v ci j ≤ pred receive m v cj j is always true 
because the receipt of m by pj cannot update v cj j thus 
∀j i ci j j must be true now let us consider the case 
where j i and j k figure suppose that there exists 
an event e receive m with e send m m sent by 
pj and piggybacking the triple k m v c k m ip k and 
m v c k ≥ v ci k hence m v c k receive m v ci k 
as v cj k cannot decrease this means that as long as v ci k 
does not increase for every message m sent by pi to pj we 
have the following send m v ci k receive m v ci k 
send m v cj k ≤ receive m v cj k i e ci j k must 
remain true in other words once ci j k is true the only 
event of pi that could reset it to false is either the receipt 
of a message that increases v ci k or if k i the 
occurrence of a relevant event that increases v ci i similarly 
once ci j k is false the only event that can set it to true is 
the receipt of a message m from pj piggybacking the triple 
 k m v c k m ip k with m v c k ≥ v ci k 
in order to implement the local predicates ci j k each 
process pi is equipped with a boolean matrix mi as in 
such that m j k ⇔ ci j k it follows from the 
previous discussion that this matrix is managed according to the 
following rules note that its i-th line is not significant case 
j i and that its diagonal is always equal to 
m initialization ∀ j k mi j k is initialized to 
 
m each time it produces a relevant event e pi resets 
the ith column of its matrix ∀j i mi j i 
m when pi sends a message no update of mi occurs 
m when it receives a message m from pj pi executes the 
following updates 
∀ k ∈ n case 
v ci k m v c k then ∀ i j k mi k 
mi j k 
v ci k m v c k then mi j k 
v ci k m v c k then skip 
endcase 
the following lemma results from rules m -m the 
theorem that follows shows that condition k m k is correct 
 both are proved in 
lemma ∀i ∀m sent by pi to pj ∀k we have 
send m mi j k ⇒ 
send m v ci k ≤ pred receive m v cj k 
theorem let m be a message sent by pi to pj let 
k m k ≡ send m mi j k ∧ send m ipi k 
 ∨ send m v ci k we have k m k ⇒ k m k 
 resulting ipt protocol 
the complete text of the ipt protocol based on the 
previous discussion follows 
rm initialization 
- both v ci n and ipi n are set to 
and ∀ j k mi j k is set to 
rm each time it produces a relevant event e 
- pi associates with e the timestamp e ts defined 
as follows e ts { k v ci k ipi k } 
- pi increments its vector clock entry v ci i 
 namely it executes v ci i v ci i 
- pi resets ipi ∀ i ipi ipi i 
- pi resets the ith column of its boolean matrix 
∀j i mi j i 
rm when pi sends a message m to pj it attaches to m the 
set of triples each made up of a process id an integer 
and a boolean { k v ci k ipi k mi j k ∨ 
ipi k ∧ v ci k } 
rm when pi receives a message m from pj it executes the 
following updates 
∀ k m v c k m ip k carried by m 
case 
v ci k m v c k then v ci k m v c k 
ipi k m ip k 
∀ i j k mi k 
 
actually the value of this column remains constant after 
its first update in fact ∀j mi j i can be set to only upon 
the receipt of a message from pj carrying the value v cj i 
 see r but as mj i i pj does not send v cj i to 
pi so it is possible to improve the protocol by executing 
this reset of the column mi ∗ i only when pi produces 
its first relevant event 
mi j k 
v ci k m v c k then ipi k min ipi k m ip k 
mi j k 
v ci k m v c k then skip 
endcase 
 a tradeoff 
the condition k m k shows that a triple has not to be 
transmitted when mi j k ∧ ipi k ∨ v ci k 
 let us first observe that the management of ipi k 
is governed by the application program more precisely 
the ipt protocol does not define which are the 
relevant events it has only to guarantee a correct 
management of ipi k differently the matrix mi does not belong 
to the problem specification it is an auxiliary variable of 
the ipt protocol which manages it so as to satisfy the 
following implication when pi sends m to pj mi j k 
 ⇒ pred receive m v cj k ≥ send m v ci k the 
fact that the management of mi is governed by the protocol 
and not by the application program leaves open the 
possibility to design a protocol where more entries of mi are equal 
to this can make the condition k m k more often 
satisfied 
and can consequently allow the protocol to transmit 
less triples 
we show here that it is possible to transmit less triples 
at the price of transmitting a few additional boolean 
vectors the previous ipt matrix-based protocol section 
is modified in the following way the rules rm and 
rm are replaced with the modified rules rm and rm 
 mi ∗ k denotes the kth column of mi 
rm when pi sends a message m to pj it attaches to m 
the following set of -uples each made up of a 
process id an integer a boolean and a boolean vector 
{ k v ci k ipi k mi ∗ k mi j k ∨ ipi k 
 ∧ v ci k } 
rm when pi receives a message m from pj it executes the 
following updates 
∀ k m v c k m ip k m m n k carried by m 
case 
v ci k m v c k then v ci k m v c k 
ipi k m ip k 
∀ i mi k m m k 
v ci k m v c k then ipi k min ipi k m ip k 
∀ i mi k 
max mi k m m k 
v ci k m v c k then skip 
endcase 
similarly to the proofs described in it is possible to 
prove that the previous protocol still satisfies the 
property proved in lemma namely ∀i ∀m sent by pi to pj 
∀k we have send m mi j k ⇒ send m v ci k ≤ 
pred receive m v cj k 
 
let us consider the previously described protocol section 
 where the value of each matrix entry mi j k is always 
equal to the reader can easily verify that this setting 
correctly implements the matrix moreover k m k is then 
always false it actually coincides with k k m which 
corresponds to the case where whole vectors have to be 
transmitted with each message 
 
intuitively the fact that some columns of matrices m are 
attached to application messages allows a transitive 
transmission of information more precisely the relevant history 
of pk known by pj is transmitted to a process pi via a causal 
sequence of messages from pj to pi in contrast the 
protocol described in section used only a direct transmission of 
this information in fact as explained section the 
predicate c locally implemented by the matrix m was based on 
the existence of a message m sent by pj to pi piggybacking 
the triple k m v c k m ip k and m v c k ≥ v ci k 
i e on the existence of a direct transmission of information 
 by the message m 
the resulting ipt protocol defined by the rules rm 
rm rm and rm uses the same condition k m k 
as the previous one it shows an interesting tradeoff between 
the number of triples k v ci k ipi k whose transmission 
is saved and the number of boolean vectors that have to 
be additionally piggybacked it is interesting to notice that 
the size of this additional information is bounded while each 
triple includes a non-bounded integer namely a vector clock 
value 
 experimental study 
this section compares the behaviors of the previous 
protocols this comparison is done with a simulation study 
ipt denotes the protocol presented in section that 
uses the condition k m k which is always equal to false 
ipt denotes the protocol presented in section that uses 
the condition k m k where messages carry triples 
finally ipt denotes the protocol presented in section that 
also uses the condition k m k but where messages carry 
additional boolean vectors 
this section does not aim to provide an in-depth 
simulation study of the protocols but rather presents a general 
view on the protocol behaviors to this end it compares 
ipt and ipt with regard to ipt more precisely for 
ipt the aim was to evaluate the gain in terms of triples 
 k v ci k ipi k not transmitted with respect to the 
systematic transmission of whole vectors as done in ipt for 
ipt the aim was to evaluate the tradeoff between the 
additional boolean vectors transmitted and the number of saved 
triples the behavior of each protocol was analyzed on a set 
of programs 
 simulation parameters 
the simulator provides different parameters enabling to 
tune both the communication and the processes features 
these parameters allow to set the number of processes for 
the simulated computation to vary the rate of 
communication send receive events and to alter the time duration 
between two consecutive relevant events moreover to be 
independent of a particular topology of the underlying 
network a fully connected network is assumed internal events 
have not been considered 
since the presence of the triples k v ci k ipi k 
piggybacked by a message strongly depends on the frequency at 
which relevant events are produced by a process different 
time distributions between two consecutive relevant events 
have been implemented e g normal uniform and poisson 
distributions the senders of messages are chosen 
according to a random law to exhibit particular configurations 
of a distributed computation a given scenario can be 
provided to the simulator message transmission delays follow 
a standard normal distribution finally the last parameter 
of the simulator is the number of send events that occurred 
during a simulation 
 parameter settings 
to compare the behavior of the three ipt protocols we 
performed a large number of simulations using different 
parameters setting we set to the number of processes 
participating to a distributed computation the number of 
communication events during the simulation has been set to 
 the parameter λ of the poisson time distribution λ 
is the average number of relevant events in a given time 
interval has been set so that the relevant events are generated 
at the beginning of the simulation with the uniform time 
distribution a relevant event is generated in the average 
every communication events the location parameter of 
the standard normal time distribution has been set so that 
the occurrence of relevant events is shifted around the third 
part of the simulation experiment 
as noted previously the simulator can be fed with a 
given scenario this allows to analyze the worst case scenarios 
for ipt and ipt these scenarios correspond to the case 
where the relevant events are generated at the maximal 
frequency i e each time a process sends or receives a message 
it produces a relevant event 
finally the three ipt protocols are analyzed with the 
same simulation parameters 
 simulation results 
the results are displayed on the figures a- d these 
figures plot the gain of the protocols in terms of the number 
of triples that are not transmitted y axis with respect to 
the number of communication events x axis from these 
figures we observe that whatever the time distribution 
followed by the relevant events both ipt and ipt exhibit 
a behavior better than ipt i e the total number of 
piggybacked triples is lower in ipt and ipt than in ipt 
even in the worst case see figure d 
let us consider the worst scenario in that case the gain 
is obtained at the very beginning of the simulation and lasts 
as long as it exists a process pj for which ∀k v cj k 
in that case the condition ∀k k m k is satisfied as soon 
as ∃k v cj k both ipt and ipt behave as ipt 
 the shape of the curve becomes flat since the condition 
k m k is no longer satisfied 
figure a shows that during the first events of the 
simulation the slope of curves ipt and ipt are steep the 
same occurs in figure d that depicts the worst case 
scenario then the slope of these curves decreases and remains 
constant until the end of the simulation in fact as soon as 
v cj k becomes greater than the condition ¬k m k 
reduces to mi j k ∨ ipi k 
figure b displays an interesting feature it considers λ 
 as the relevant events are taken only during the very 
beginning of the simulation this figure exhibits a very steep 
slope as the other figures the figure shows that as soon as 
no more relevant events are taken on average of the 
triples are not piggybacked by the messages this shows 
the importance of matrix mi furthermore ipt benefits 
from transmitting additional boolean vectors to save triple 
transmissions the figures a- c show that the average 
gain of ipt with respect to ipt is close to 
finally figure c underlines even more the importance 
 
of matrix mi when very few relevant events are taken 
ipt and ipt turn out to be very efficient indeed this 
figure shows that very quickly the gain in number of triples 
that are saved is very high actually of the triples are 
saved 
 lessons learned from the simulation 
of course all simulation results are consistent with the 
theoretical results ipt is always better than or equal to 
ipt and ipt is always better than ipt the simulation 
results teach us more 
 the first lesson we have learnt concerns the matrix mi 
its use is quite significant but mainly depends on the time 
distribution followed by the relevant events on the one 
hand when observing figure b where a large number of 
relevant events are taken in a very short time ipt can save 
up to of the triples however we could have 
expected a more sensitive gain of ipt since the boolean vector 
ip tends to stabilize to when no relevant events 
are taken in fact as discussed in section the 
management of matrix mi within ipt does not allow a transitive 
transmission of information but only a direct transmission 
of this information this explains why some columns of mi 
may remain equal to while they could potentially be equal 
to differently as ipt benefits from transmitting 
additional boolean vectors providing a transitive transmission 
information it reaches a gain of 
on the other hand when very few relevant events are 
taken in a large period of time see figure c the behavior of 
ipt and ipt turns out to be very efficient since the 
transmission of up to of the triples is saved this comes from 
the fact that very quickly the boolean vector ipi tends to 
stabilize to and that matrix mi contains very few 
 since very few relevant events have been taken thus a 
direct transmission of the information is sufficient to quickly 
get matrices mi equal to 
 the second lesson concerns ipt more precisely the 
tradeoff between the additional piggybacking of boolean 
vectors and the number of triples whose transmission is saved 
with n adding booleans to a triple does not 
substantially increases its size the figures a- c exhibit the 
number of triples whose transmission is saved the average 
gain in number of triples of ipt with respect to ipt is 
about 
 conclusion 
this paper has addressed an important causality-related 
distributed computing problem namely the immediate 
predecessors tracking problem it has presented a family of 
protocols that provide each relevant event with a timestamp 
that exactly identify its immediate predecessors the 
family is defined by a general condition that allows application 
messages to piggyback control information whose size can 
be smaller than n the number of processes in that sense 
this family defines message size-efficient ipt protocols 
according to the way the general condition is implemented 
different ipt protocols can be obtained three of them have 
been described and analyzed with simulation experiments 
interestingly it has also been shown that the efficiency of 
the protocols measured in terms of the size of the control 
information that is not piggybacked by an application 
message depends on the pattern defined by the communication 
events and the relevant events 
last but not least it is interesting to note that if one is not 
interested in tracking the immediate predecessor events the 
protocols presented in the paper can be simplified by 
suppressing the ipi booleans vectors but keeping the boolean 
matrices mi the resulting protocols that implement a 
vector clock system are particularly efficient as far as the 
size of the timestamp carried by each message is concerned 
interestingly this efficiency is not obtained at the price of 
additional assumptions such as fifo channels 
 references 
 anceaume e h´elary j -m and raynal m tracking 
immediate predecessors in distributed computations res 
report irisa univ rennes france 
 baldoni r prakash r raynal m and singhal m 
efficient ∆-causal broadcasting journal of computer 
systems science and engineering - 
 chandy k m and lamport l distributed snapshots 
determining global states of distributed systems acm 
transactions on computer systems - 
 diehl c jard c and rampon j -x reachability analysis 
of distributed executions proc tapsoft 
springer-verlag lncs pp - 
 fidge c j timestamps in message-passing systems that 
preserve partial ordering proc th australian 
computing conference pp - 
 fromentin e jard c jourdan g -v and raynal m 
on-the-fly analysis of distributed computations ipl 
 - 
 fromentin e and raynal m shared global states in 
distributed computations jcss - 
 fromentin e raynal m garg v k and tomlinson a 
on-the-fly testing of regular patterns in distributed 
computations proc icpp vol - 
 garg v k principles of distributed systems kluwer 
academic press pages 
 h´elary j -m most´efaoui a netzer r h b and raynal 
m communication-based prevention of useless 
ckeckpoints in distributed computations distributed 
computing - 
 h´elary j -m melideo g and raynal m tracking 
causality in distributed systems a suite of efficient 
protocols proc sirocco carleton university press 
pp - l aquila italy june 
 h´elary j -m netzer r and raynal m consistency issues 
in distributed checkpoints ieee tse 
 - 
 hurfin m mizuno m raynal m and singhal m efficient 
distributed detection of conjunction of local predicates 
in asynch computations ieee tse - 
 lamport l time clocks and the ordering of events in a 
distributed system comm acm - 
 marzullo k and sabel l efficient detection of a class of 
stable properties distributed computing - 
 mattern f virtual time and global states of distributed 
systems proc int conf parallel and distributed 
algorithms cosnard quinton raynal robert eds 
north-holland pp - 
 prakash r raynal m and singhal m an adaptive 
causal ordering algorithm suited to mobile computing 
environment jpdc - 
 raynal m and singhal s logical time capturing 
causality in distributed systems ieee computer 
 - 
 singhal m and kshemkalyani a an efficient 
implementation of vector clocks ipl - 
 wang y m consistent global checkpoints that contain 
a given set of local checkpoints ieee toc 
 - 
 
 
 
 
 
 
 
 
 
gaininnumberoftriples 
communication events number 
ipt 
ipt 
ipt 
relevant events 
 a the relevant events follow a uniform distribution 
 ratio 
- 
 
 
 
 
 
 
 
 
 
 
 
 
gaininnumberoftriples 
communication events number 
ipt 
ipt 
ipt 
relevant events 
 b the relevant events follow a poisson distribution 
 λ 
 
 
 
 
 
 
 
 
 
 
 
 
gaininnumberoftriples 
communication events number 
ipt 
ipt 
ipt 
relevant events 
 c the relevant events follow a normal distribution 
 
 
 
 
 
 
 
 
 
 
 
gaininnumberoftriples 
communication events number 
ipt 
ipt 
ipt 
relevant events 
 d for each pi pi takes a relevant event and 
broadcast to all processes 
figure experimental results 
 
shooter localization and weapon classification with 
soldier-wearable networked sensors 
peter volgyesi gyorgy balogh andras nadas christopher b nash akos ledeczi 
institute for software integrated systems vanderbilt university 
nashville tn usa 
akos ledeczi vanderbilt edu 
abstract 
the paper presents a wireless sensor network-based mobile 
countersniper system a sensor node consists of a 
helmetmounted microphone array a cots micaz mote for 
internode communication and a custom sensorboard that 
implements the acoustic detection and time of arrival toa 
estimation algorithms on an fpga a -axis compass provides 
self orientation and bluetooth is used for communication 
with the soldier s pda running the data fusion and the user 
interface the heterogeneous sensor fusion algorithm can 
work with data from a single sensor or it can fuse toa or 
angle of arrival aoa observations of muzzle blasts and 
ballistic shockwaves from multiple sensors the system 
estimates the trajectory the range the caliber and the weapon 
type the paper presents the system design and the results 
from an independent evaluation at the us army aberdeen 
test center the system performance is characterized by 
 degree trajectory precision and over caliber estimation 
accuracy for all shots and close to weapon estimation 
accuracy for out of guns tested 
categories and subject descriptors 
c computer-communications networks 
distributed systems j computers in other systems 
military 
general terms design measurement performance 
 introduction 
the importance of countersniper systems is underscored 
by the constant stream of news reports coming from the 
middle east in october cnn reported on a new 
tactic employed by insurgents a mobile sniper team moves 
around busy city streets in a car positions itself at a good 
standoff distance from dismounted us military personnel 
takes a single well-aimed shot and immediately melts in the 
city traffic by the time the soldiers can react they are 
gone a countersniper system that provides almost 
immediate shooter location to every soldier in the vicinity would 
provide clear benefits to the warfigthers 
our team introduced pinptr the first sensor 
networkbased countersniper system in the system is 
based on potentially hundreds of inexpensive sensor nodes 
deployed in the area of interest forming an ad-hoc multihop 
network the acoustic sensors measure the time of arrival 
 toa of muzzle blasts and ballistic shockwaves pressure 
waves induced by the supersonic projectile send the data to 
a base station where a sensor fusion algorithm determines 
the origin of the shot pinptr is characterized by high 
precision m average d accuracy for shots originating within 
or near the sensor network and degree bearing precision 
for both azimuth and elevation and accuracy in range 
estimation for longer range shots the truly unique 
characteristic of the system is that it works in such reverberant 
environments as cluttered urban terrain and that it can 
resolve multiple simultaneous shots at the same time this 
capability is due to the widely distributed sensing and the 
unique sensor fusion approach the system has been 
tested several times in us army mout military 
operations in urban terrain facilities 
the obvious disadvantage of such a system is its static 
nature once the sensors are distributed they cover a 
certain area depending on the operation the deployment may 
be needed for an hour or a month but eventually the area 
looses its importance it is not practical to gather and reuse 
the sensors especially under combat conditions even if the 
sensors are cheap it is still a waste and a logistical problem 
to provide a continuous stream of sensors as the operations 
move from place to place as it is primarily the soldiers that 
the system protects a natural extension is to mount the 
sensors on the soldiers themselves while there are 
vehiclemounted countersniper systems available commercially 
we are not aware of a deployed system that protects 
dismounted soldiers a helmet-mounted system was developed 
in the mid s by bbn but it was not continued beyond 
the darpa program that funded it 
 
to move from a static sensor network-based solution to a 
highly mobile one presents significant challenges the sensor 
positions and orientation need to be constantly monitored 
as soldiers may work in groups of as little as four people 
the number of sensors measuring the acoustic phenomena 
may be an order of magnitude smaller than before 
moreover the system should be useful to even a single soldier 
finally additional requirements called for caliber estimation 
and weapon classification in addition to source localization 
the paper presents the design and evaluation of our 
soldierwearable mobile countersniper system it describes the 
hardware and software architecture including the custom sensor 
board equipped with a small microphone array and 
connected to a cots micaz mote special emphasis is 
paid to the sensor fusion technique that estimates the 
trajectory range caliber and weapon type simultaneously the 
results and analysis of an independent evaluation of the 
system at the us army aberdeen test center are also 
presented 
 approach 
the firing of a typical military rifle such as the ak 
or m produces two distinct acoustic phenomena the 
muzzle blast is generated at the muzzle of the gun and 
travels at the speed sound the supersonic projectile generates 
an acoustic shockwave a kind of sonic boom the 
wavefront has a conical shape the angle of which depends on the 
mach number the speed of the bullet relative to the speed 
of sound 
the shockwave has a characteristic shape resembling a 
capital n the rise time at both the start and end of the 
signal is very fast under μsec the length is determined by 
the caliber and the miss distance the distance between the 
trajectory and the sensor it is typically a few hundred μsec 
once a trajectory estimate is available the shockwave length 
can be used for caliber estimation 
our system is based on four microphones connected to 
a sensorboard the board detects shockwaves and muzzle 
blasts and measures their toa if at least three acoustic 
channels detect the same event its aoa is also computed 
if both the shockwave and muzzle blast aoa are available 
a simple analytical solution gives the shooter location as 
shown in section as the microphones are close to each 
other typically - we cannot expect very high precision 
also this method does not estimate a trajectory in fact an 
infinite number of trajectory-bullet speed pairs satisfy the 
observations however the sensorboards are also connected 
to cots micaz motes and they share their aoa and toa 
measurements as well as their own location and orientation 
with each other using a multihop routing service a 
hybrid sensor fusion algorithm then estimates the trajectory 
the range the caliber and the weapon type based on all 
available observations 
the sensorboard is also bluetooth capable for 
communication with the soldier s pda or laptop computer a wired 
usb connection is also available the sensorfusion 
algorithm and the user interface get their data through one of 
these channels 
the orientation of the microphone array at the time of 
detection is provided by a -axis digital compass currently 
the system assumes that the soldier s pda is gps-capable 
and it does not provide self localization service itself 
however the accuracy of gps is a few meters degrading the 
figure acoustic sensorboard mote assembly 
 
overall accuracy of the system refer to section for an 
analysis the latest generation sensorboard features a texas 
instruments cc- radio enabling the high-precision radio 
interferometric self localization approach we have developed 
separately however we leave the integration of the two 
technologies for future work 
 hardware 
since the first static version of our system in the 
sensor nodes have been built upon the uc berkeley crossbow 
mica product line although rudimentary acoustic 
signal processing can be done on these microcontroller-based 
boards they do not provide the required computational 
performance for shockwave detection and angle of arrival 
measurements where multiple signals from different 
microphones need to be processed in parallel at a high sampling 
rate our rd generation sensorboard is designed to be used 
with micaz motes-in fact it has almost the same size as 
the mote itself see figure 
the board utilizes a powerful xilinx xc s fpga 
chip with various standard peripheral ip cores multiple soft 
processor cores and custom logic for the acoustic detectors 
 figure the onboard flash mb and psram mb 
modules allow storing raw samples of several acoustic events 
which can be used to build libraries of various acoustic 
signatures and for refining the detection cores off-line also the 
external memory blocks can store program code and data 
used by the soft processor cores on the fpga 
the board supports four independent analog channels 
sampled at up to ms s million samples per seconds these 
channels featuring an electret microphone panasonic 
wm pnt amplifiers with controllable gain - db and 
a -bit serial adc analog devices ad reside on 
separate tiny boards which are connected to the main 
sensorboard with ribbon cables this partitioning enables the 
use of truly different audio channels eg slower sampling 
frequency different gain or dynamic range and also results 
in less noisy measurements by avoiding long analog signal 
paths 
the sensor platform offers a rich set of interfaces and can 
be integrated with existing systems in diverse ways an 
rs port and a bluetooth bluegiga wt wireless link 
with virtual uart emulation are directly available on the 
board and provide simple means to connect the sensor to 
pcs and pdas the mote interface consists of an i 
c bus 
along with an interrupt and gpio line the latter one is used 
 
figure block diagram of the sensorboard 
for precise time synchronization between the board and the 
mote the motes are equipped with ieee 
compliant radio transceivers and support ad-hoc wireless 
networking among the nodes and to from the base station the 
sensorboard also supports full-speed usb transfers with 
custom usb dongles for uploading recorded audio samples 
to the pc the on-board jtag chain-directly accessible 
through a dedicated connector-contains the fpga part 
and configuration memory and provides in-system 
programming and debugging facilities 
the integrated honeywell hmr digital compass 
module provides heading pitch and roll information with ◦ 
accuracy which is essential for calculating and combining 
directional estimates of the detected events 
due to the complex voltage requirements of the fpga 
the power supply circuitry is implemented on the 
sensorboard and provides power both locally and to the mote we 
used a quad pack of rechargeable aa batteries as the power 
source although any other configuration is viable that meets 
the voltage requirements the fpga core v and i o 
 v voltages are generated by a highly efficient buck 
switching regulator the fpga configuration v and a 
separate v power net are fed by low current ldos the 
latter one is used to provide independent power to the mote 
and to the bluetooth radio the regulators-except the last 
one-can be turned on off from the mote or through the 
bluetooth radio via gpio lines to save power 
the first prototype of our system employed sensor 
nodes some of these nodes were mounted on military kevlar 
helmets with the microphones directly attached to the 
surface at about cm separation as shown in figure a the 
rest of the nodes were mounted in plastic enclosures 
 figure b with the microphones placed near the corners of 
the boxes to form approximately cm× cm rectangles 
 software architecture 
the sensor application relies on three subsystems 
exploiting three different computing paradigms as they are shown 
in figure although each of these execution models suit 
their domain specific tasks extremely well this diversity 
 a b 
figure sensor prototypes mounted on a kevlar 
helmet a and in a plastic box on a tripod b 
presents a challenge for software development and system 
integration the sensor fusion and user interface 
subsystem is running on pdas and were implemented in java 
the sensing and signal processing tasks are executed by an 
fpga which also acts as a bridge between various wired 
and wireless communication channels the ad-hoc internode 
communication time synchronization and data sharing are 
the responsibilities of a microcontroller based radio module 
similarly the application employs a wide variety of 
communication protocols such as bluetooth and ieee 
wireless links as well as optional uarts i 
c and or usb 
buses 
soldier 
operated device 
 pda laptop 
fpga 
sensor board 
mica radio 
module 
 ghz wireless link 
radio control 
message routing 
acoustic event encoder 
sensor time synch 
network time synch remote control 
time 
stamping 
interrupts 
virtual 
register 
interface 
c 
o 
o 
r 
d 
i 
n 
a 
t 
o 
r 
a 
n 
a 
l 
o 
g 
c 
h 
a 
n 
n 
e 
l 
s compass 
picoblaze 
comm 
interface 
picoblaze 
wt bluetooth radio 
mote if i c interrupts 
usb psram 
u 
a 
r 
t 
u 
a 
r 
t 
mb 
det 
sw 
det 
rec 
bluetooth link 
user 
interface 
sensor 
fusion 
location 
engine gps 
message dis- assemblersensor 
control 
figure software architecture diagram 
the sensor fusion module receives and unpacks raw 
measurements time stamps and feature vectors from the 
sensorboard through the bluetooth link also it fine tunes 
the execution of the signal processing cores by setting 
parameters through the same link note that measurements 
from other nodes along with their location and orientation 
information also arrive from the sensorboard which acts as 
a gateway between the pda and the sensor network the 
handheld device obtains its own gps location data and 
di 
rectly receives orientation information through the 
sensorboard the results of the sensor fusion are displayed on the 
pda screen with low latency since the application is 
implemented in pure java it is portable across different pda 
platforms 
the border between software and hardware is 
considerably blurred on the sensor board the ip 
cores-implemented in hardware description languages hdl on the 
reconfigurable fpga fabric-closely resemble hardware 
building blocks however some of them-most notably the soft 
processor cores-execute true software programs the 
primary tasks of the sensor board software are acquiring 
data samples from the analog channels processing 
acoustic data detection and providing access to the results 
and run-time parameters through different interfaces 
as it is shown in figure a centralized virtual register 
file contains the address decoding logic the registers for 
storing parameter values and results and the point to point 
data buses to and from the peripherals thus it effectively 
integrates the building blocks within the sensorboard and 
decouples the various communication interfaces this 
architecture enabled us to deploy the same set of sensors in a 
centralized scenario where the ad-hoc mote network using 
the i 
c interface collected and forwarded the results to a 
base station or to build a decentralized system where the 
local pdas execute the sensor fusion on the data obtained 
through the bluetooth interface and optionally from other 
sensors through the mote interface the same set of 
registers are also accessible through a uart link with a terminal 
emulation program also because the low-level interfaces 
are hidden by the register file one can easily add replace 
these with new ones eg the first generation of motes 
supported a standard μp interface bus on the sensor connector 
which was dropped in later designs 
the most important results are the time stamps of the 
detected events these time stamps and all other timing 
information parameters acoustic event features are based 
on a mhz clock and an internal timer on the fpga the 
time conversion and synchronization between the sensor 
network and the board is done by the mote by periodically 
requesting the capture of the current timer value through a 
dedicated gpio line and reading the captured value from 
the register file through the i 
c interface based on the the 
current and previous readings and the corresponding mote 
local time stamps the mote can calculate and maintain the 
scaling factor and offset between the two time domains 
the mote interface is implemented by the i 
c slave ip 
core and a thin adaptation layer which provides a data and 
address bus abstraction on top of it the maximum 
effective bandwidth is kbps through this interface the 
fpga contains several uart cores as well for 
communicating with the on-board bluetooth module for 
controlling the digital compass and for providing a wired rs 
link through a dedicated connector the control status and 
data registers of the uart modules are available through 
the register file the higher level protocols on these lines are 
implemented by xilinx picoblaze microcontroller cores 
and corresponding software programs one of them provides 
a command line interface for test and debug purposes while 
the other is responsible for parsing compass readings by 
default they are connected to the rs port and to the 
on-board digital compass line respectively however they 
can be rewired to any communication interface by changing 
the register file base address in the programs e g the 
command line interface can be provided through the bluetooth 
channel 
two of the external interfaces are not accessible through 
the register file a high speed usb link and the sram 
interface are tied to the recorder block the usb module 
implements a simple fifo with parallel data lines connected to an 
external ft r usb device controller the ram driver 
implements data read write cycles with correct timing and 
is connected to the on-board pseudo sram these 
interfaces provide mb s effective bandwidth for downloading 
recorded audio samples for example 
the data acquisition and signal processing paths exhibit 
clear symmetry the same set of ip cores are instantiated 
four times i e the number of acoustic channels and run 
independently the signal paths meet only just before 
the register file each of the analog channels is driven by 
a serial a d core for providing a mhz serial clock and 
shifting in -bit data samples at ms s and a digital 
potentiometer driver for setting the required gain each channel 
has its own shockwave and muzzle blast detector which are 
described in section the detectors fetch run-time 
parameter values from the register file and store their results there 
as well the coordinator core constantly monitors the 
detection results and generates a mote interrupt promptly upon 
full detection or after a reasonable timeout after partial 
detection 
the recorder component is not used in the final 
deployment however it is essential for development purposes for 
refining parameter values for new types of weapons or for 
other acoustic sources this component receives the 
samples from all channels and stores them in circular buffers in 
the psram device if the signal amplitude on one of the 
channels crosses a predefined threshold the recorder 
component suspends the sample collection with a predefined delay 
and dumps the contents of the buffers through the usb link 
the length of these buffers and delays the sampling rate 
the threshold level and the set of recorded channels can be 
 re configured run-time through the register file note that 
the core operates independently from the other signal 
processing modules therefore it can be used to validate the 
detection results off-line 
the fpga cores are implemented in vhdl the picoblaze 
programs are written in assembly the complete 
configuration occupies of the resources slices of the fpga and 
the maximum clock speed is mhz which is safely higher 
than the speed used with the actual device mhz 
the micaz motes are responsible for distributing 
measurement data across the network which drastically 
improves the localization and classification results at each node 
besides a robust radio mac layer the motes require two 
essential middleware services to achieve this goal the 
messages need to be propagated in the ad-hoc multihop network 
using a routing service we successfully integrated the 
directed flood-routing framework dfrf in our 
application apart from automatic message aggregation and 
efficient buffer management the most unique feature of dfrf 
is its plug-in architecture which accepts custom routing 
policies routing policies are state machines that govern 
how received messages are stored resent or discarded 
example policies include spanning tree routing broadcast 
geographic routing etc different policies can be used for 
different messages concurrently and the application is able to 
 
change the underlying policies at run-time eg because of 
the changing rf environment or power budget in fact we 
switched several times between a simple but lavish broadcast 
policy and a more efficient gradient routing on the field 
correlating toa measurements requires a common time 
base and precise time synchronization in the sensor network 
the routing integrated time synchronization rits 
protocol relies on very accurate mac-layer time-stamping 
to embed the cumulative delay that a data message accrued 
since the time of the detection in the message itself that 
is at every node it measures the time the message spent 
there and adds this to the number in the time delay slot of 
the message right before it leaves the current node every 
receiving node can subtract the delay from its current time 
to obtain the detection time in its local time reference the 
service provides very accurate time conversion few μs per 
hop error which is more than adequate for this application 
note that the motes also need to convert the sensorboard 
time stamps to mote time as it is described earlier 
the mote application is implemented in nesc and is 
running on top of tinyos with its kb ram and 
 kb program space rom requirement it easily fits on 
the micaz motes 
 detection algorithm 
there are several characteristics of acoustic shockwaves 
and muzzle blasts which distinguish their detection and 
signal processing algorithms from regular audio applications 
both events are transient by their nature and present very 
intense stimuli to the microphones this is increasingly 
problematic with low cost electret microphones-designed 
for picking up regular speech or music although 
mechanical damping of the microphone membranes can mitigate 
the problem this approach is not without side effects the 
detection algorithms have to be robust enough to handle 
severe nonlinear distortion and transitory oscillations since 
the muzzle blast signature closely follows the shockwave 
signal and because of potential automatic weapon bursts it is 
extremely important to settle the audio channels and the 
detection logic as soon as possible after an event also 
precise angle of arrival estimation necessitates high sampling 
frequency in the mhz range and accurate event detection 
moreover the detection logic needs to process multiple 
channels in parallel channels on our existing hardware 
these requirements dictated simple and robust algorithms 
both for muzzle blast and shockwave detections instead of 
using mundane energy detectors-which might not be able 
to distinguish the two different events-the applied 
detectors strive to find the most important characteristics of the 
two signals in the time-domain using simple state machine 
logic the detectors are implemented as independent ip 
cores within the fpga-one pair for each channel the 
cores are run-time configurable and provide detection event 
signals with high precision time stamps and event specific 
feature vectors although the cores are running 
independently and in parallel a crude local fusion module integrates 
them by shutting down those cores which missed their events 
after a reasonable timeout and by generating a single 
detection message towards the mote at this point the mote can 
read and forward the detection times and features and is 
responsible to restart the cores afterwards 
the most conspicuous characteristics of an acoustic 
shockwave see figure a are the steep rising edges at the 
be 
- 
- 
- 
- 
- 
 
 
 
 
 
 
shockwave m 
time µs 
amplitude 
 
 
 
 
 
len 
 a 
s t - s t-d e 
tstart 
 t 
s t - s t-d e 
s t - s t-d e 
t - t start lmin 
s t - s t-d e 
len t - tstart 
idle 
 
first edge done 
 
second edge 
 
first edge 
 
found 
 
t - tstart 
≥ lmax 
t - tstart 
≥ lmax 
 b 
figure shockwave signal generated by a × 
 mm nato projectile a and the state machine 
of the detection algorithm b 
ginning and end of the signal also the length of the n-wave 
is fairly predictable-as it is described in section -and is 
relatively short - μs the shockwave detection core 
is continuously looking for two rising edges within a given 
interval the state machine of the algorithm is shown in 
figure b the input parameters are the minimum 
steepness of the edges d e and the bounds on the length of 
the wave lmin lmax the only feature calculated by the 
core is the length of the observed shockwave signal 
in contrast to shockwaves the muzzle blast signatures are 
characterized by a long initial period - ms where the first 
half period is significantly shorter than the second half 
due to the physical limitations of the analog circuitry 
described at the beginning of this section irregular oscillations 
and glitches might show up within this longer time window 
as they can be clearly seen in figure a therefore the real 
challenge for the matching detection core is to identify the 
first and second half periods properly the state machine 
 figure b does not work on the raw samples directly 
but is fed by a zero crossing zc encoder after the initial 
triggering the detector attempts to collect those zc 
segments which belong to the first period positive amplitude 
while discarding too short in our terminology garbage 
segments-effectively implementing a rudimentary low-pass 
filter in the zc domain after it encounters a sufficiently 
long negative segment it runs the same collection logic for 
the second half period if too much garbage is discarded 
in the collection phases the core resets itself to prevent the 
 false detection of the halves from completely different 
periods separated by rapid oscillation or noise finally if the 
constraints on the total length and on the length ratio hold 
the core generates a detection event along with the actual 
length amplitude and energy of the period calculated 
concurrently the initial triggering mechanism is based on two 
amplitude thresholds one static but configurable 
amplitude level and a dynamically computed one the latter one 
is essential to adapt the sensor to different ambient noise 
environments and to temporarily suspend the muzzle blast 
detector after a shock wave event oscillations in the analog 
section or reverberations in the sensor enclosure might 
otherwise trigger false muzzle blast detections the dynamic 
noise level is estimated by a single pole recursive low-pass 
filter cutoff   khz on the fpga 
 
 
- 
- 
- 
- 
- 
 
 
 
 
 
 
time µs 
amplitude 
muzzle blast m 
 
 
 
 
len 
 
len 
 a 
idle 
 
second zc 
 
pending zc 
 
first zc 
 
found 
 
amplitude 
threshold 
long 
positive zc 
long 
negative zc 
valid 
full period 
max 
garbage 
wrong sign 
garbage 
collect 
first period 
garbage 
collect 
first period 
garbage 
 b 
figure muzzle blast signature a produced by an 
m assault rifle and the corresponding detection 
logic b 
the detection cores were originally implemented in java 
and evaluated on pre-recorded signals because of much faster 
test runs and more convenient debugging facilities later 
on they were ported to vhdl and synthesized using the 
xilinx ise tool suite the functional equivalence between 
the two implementations were tested by vhdl test benches 
and python scripts which provided an automated way to 
exercise the detection cores on the same set of pre-recorded 
signals and to compare the results 
 sensor fusion 
the sensor fusion algorithm receives detection messages 
from the sensor network and estimates the bullet trajectory 
the shooter position the caliber of the projectile and the 
type of the weapon the algorithm consists of well separated 
computational tasks outlined below 
 compute muzzle blast and shockwave directions of 
arrivals for each individual sensor see 
 compute range estimates this algorithm can 
analytically fuse a pair of shockwave and muzzle blast aoa 
estimates see 
 compute a single trajectory from all shockwave 
measurements see 
 if trajectory available compute range see 
else compute shooter position first and then trajectory 
based on it see 
 if trajectory available compute caliber see 
 if caliber available compute weapon type see 
we describe each step in the following sections in detail 
 direction of arrival 
the first step of the sensor fusion is to calculate the 
muzzle blast and shockwave aoa-s for each sensorboard each 
sensorboard has four microphones that measure the toa-s 
since the microphone spacing is orders of magnitude smaller 
than the distance to the sound source we can approximate 
the approaching sound wave front with a plane far field 
assumption 
let us formalize the problem for microphones first let 
p p and p be the position of the microphones ordered by 
time of arrival t t t first we apply a simple 
geometry validation step the measured time difference between 
two microphones cannot be larger than the sound 
propagation time between the two microphones 
 ti − tj pi − pj c ε 
where c is the speed of sound and ε is the maximum 
measurement error if this condition does not hold the 
corresponding detections are discarded let v x y z be the 
normal vector of the unknown direction of arrival we also 
use r x y z the vector from p to p and r x y z 
the vector from p to p let s consider the projection of 
the direction of the motion of the wave front v to r 
divided by the speed of sound c this gives us how long it 
takes the wave front to propagate form p to p 
vr c t − t 
the same relationship holds for r and v 
vr c t − t 
we also know that v is a normal vector 
vv 
moving from vectors to coordinates using the dot product 
definition leads to a quadratic system 
xx yy zz c t − t 
xx yy zz c t − t 
x 
 y 
 z 
 
we omit the solution steps here as they are 
straightforward but long there are two solutions if the source is on 
the p p p plane the two solutions coincide we use the 
fourth microphone s measurement-if there is one-to 
eliminate one of them otherwise both solutions are considered 
for further processing 
 muzzle-shock fusion 
u 
v 
 tp 
 tp 
tp 
 p′ 
bullet trajectory 
figure section plane of a shot at p and two 
sensors at p and at p one sensor detects the 
muzzle blast s the other the shockwave s time and 
direction of arrivals 
consider the situation in figure a shot was fired from 
p at time t both p and t are unknown we have one muzzle 
blast and one shockwave detections by two different sensors 
 
with aoa and hence toa information available the 
muzzle blast detection is at position p with time t and aoa 
u the shockwave detection is at p with time t and aoa 
v u and v are normal vectors it is shown below that these 
measurements are sufficient to compute the position of the 
shooter p 
let p be the point on the extended shockwave cone 
surface where pp is perpendicular to the surface note that 
pp is parallel with v since p is on the cone surface which 
hits p a sensor at p would detect the same shockwave 
time of arrival t the cone surface travels at the speed of 
sound c so we can express p using p 
p p cv t − t 
p can also be expressed from p 
p p cu t − t 
yielding 
p cu t − t p cv t − t 
p p is perpendicular to v 
 p − p v 
yielding 
 p cu t − t − cv t − t − p v 
containing only one unknown t one obtains 
t 
 p −p v 
c 
 uvt −t 
uv− 
 
from here we can calculate the shoter position p 
let s consider the special single sensor case where p p 
 one sensor detects both shockwave and muzzle blast aoa 
in this case 
t uvt −t 
uv− 
 
since u and v are not used separately only uv the absolute 
orientation of the sensor can be arbitrary we still get t which 
gives us the range 
here we assumed that the shockwave is a cone which is 
only true for constant projectile speeds in reality the angle 
of the cone slowly grows the surface resembles one half of 
an american football the decelerating bullet results in a 
smaller time difference between the shockwave and the 
muzzle blast detections because the shockwave generation slows 
down with the bullet a smaller time difference results in a 
smaller range so the above formula underestimates the true 
range however it can still be used with a proper 
deceleration correction function we leave this for future work 
 trajectory estimation 
danicki showed that the bullet trajectory and speed can 
be computed analytically from two independent shockwave 
measurements where both toa and aoa are measured 
the method gets more sensitive to measurement errors as 
the two shockwave directions get closer to each other in 
the special case when both directions are the same the 
trajectory cannot be computed in a real world application 
the sensors are typically deployed on a plane approximately 
in this case all sensors located on one side of the 
trajectory measure almost the same shockwave aoa to avoid 
this error sensitivity problem we consider shockwave 
measurement pairs only if the direction of arrival difference is 
larger than a certain threshold 
we have multiple sensors and one sensor can report two 
different directions when only three microphones detect the 
shockwave hence we typically have several trajectory 
candidates i e one for each aoa pair over the threshold we 
applied an outlier filtering and averaging method to fuse 
together the shockwave direction and time information and 
come up with a single trajectory assume that we have 
n individual shockwave aoa measurements let s take all 
possible unordered pairs where the direction difference is 
above the mentioned threshold and compute the trajectory 
for each this gives us at most n n− 
 
trajectories a 
trajectory is represented by one point pi and the normal vector 
vi where i is the trajectory index we define the distance 
of two trajectories as the dot product of their normal 
vectors 
d i j vivj 
for each trajectory a neighbor set is defined 
n i {j d i j r} 
where r is a radius parameter the largest neighbor set is 
considered to be the core set c all other trajectories are 
outliers the core set can be found in o n 
 time the 
trajectories in the core set are then averaged to get the final 
trajectory 
it can happen that we cannot form any sensor pairs 
because of the direction difference threshold it means all 
sensors are on the same side of the trajectory in this case 
we first compute the shooter position described in the next 
section that fixes p making v the only unknown to find 
v in this case we use a simple high resolution grid search 
and minimize an error function based on the shockwave 
directions 
we have made experiments to utilize the measured 
shockwave length in the trajectory estimation there are some 
promising results but it needs further research 
 shooter position estimation 
the shooter position estimation algorithm aggregates the 
following heterogenous information generated by earlier 
computational steps 
 trajectory 
 muzzle blast toa at a sensor 
 muzzle blast aoa at a sensor which is effectively a 
bearing estimate to the shooter and 
 range estimate at a sensor when both shockwave and 
muzzle blast aoa are available 
some sensors report only toa some has bearing 
estimate s also and some has range estimate s as well 
depending on the number of successful muzzle blast and shockwave 
detections by the sensor for an example refer to figure 
note that a sensor may have two different bearing and range 
estimates detections gives two possible aoa-s for 
muzzle blast i e bearing and or shockwave furthermore the 
combination of two different muzzle blast and shockwave 
aoa-s may result in two different ranges 
 
 rrvvt ′′ 
 vt 
 vvt ′ 
 t 
 t 
 t 
bullet trajectory 
shooter position 
figure example of heterogenous input data for 
the shooter position estimation algorithm all 
sensors have toa measurements t t t t t one 
sensor has a single bearing estimate v one sensor has 
two possible bearings v v and one sensor has two 
bearing and two range estimates v v r r 
in a multipath environment these detections will not only 
contain gaussian noise but also possibly large errors due to 
echoes it has been showed in our earlier work that a similar 
problem can be solved efficiently with an interval arithmetic 
based bisection search algorithm the basic idea is to 
define a discrete consistency function over the area of 
interest and subdivide the space into d boxes for any given 
 d box this function gives the number of measurements 
supporting the hypothesis that the shooter was within that 
box the search starts with a box large enough to contain 
the whole area of interest then zooms in by dividing and 
evaluating boxes the box with the maximum consistency 
is divided until the desired precision is reached 
backtracking is possible to avoid getting stuck in a local maximum 
this approach has been shown to be fast enough for 
online processing note however that when the trajectory 
has already been calculated in previous steps the search 
needs to be done only on the trajectory making it orders of 
magnitude faster 
next let us describe how the consistency function is 
calculated in detail consider b a three dimensional box we 
would like to compute the consistency value of first we 
consider only the toa information if one sensor has 
multiple toa detections we use the average of those times so 
one sensor supplies at most one toa estimate for each 
toa we can calculate the corresponding time of the shot 
since the origin is assumed to be in box b since it is a box 
and not a single point this gives us an interval for the shot 
time the maximum number of overlapping time intervals 
gives us the value of the consistency function for b for a 
detailed description of the consistency function and search 
algorithm refer to 
here we extend the approach the following way we 
modify the consistency function based on the bearing and range 
data from individual sensors a bearing estimate supports 
b if the line segment starting from the sensor with the 
measured direction intersects the b box a range supports b 
if the sphere with the radius of the range and origin of the 
sensor intersects b instead of simply checking whether the 
position specified by the corresponding bearing-range pairs 
falls within b this eliminates the sensor s possible 
orientation error the value of the consistency function is 
incremented by one for each bearing and range estimate that is 
consistent with b 
 caliber estimation 
the shockwave signal characteristics has been studied 
before by whitham he showed that the shockwave period 
t is related to the projectile diameter d the length l the 
perpendicular miss distance b from the bullet trajectory to 
the sensor the mach number m and the speed of sound c 
t mb 
c m − 
d 
l ≈ d 
c 
 mb 
l 
 
 
 
 
 
 
 
 
 
miss distance m shockwavelength microseconds 
 cal 
 mm 
 mm 
figure shockwave length and miss distance 
relationship each data point represents one 
sensorboard after an aggregation of the individual 
measurements of the four acoustic channels three 
different caliber projectiles have been tested 
shots sensors 
to illustrate the relationship between miss distance and 
shockwave length here we use all shots with three 
different caliber projectiles fired during the evaluation during 
the evaluation we used data obtained previously using a few 
practice shots per weapon sensors microphones by 
sensor measured the shockwave length for each sensor 
we considered the shockwave length estimation valid if at 
least three out of four microphones agreed on a value with 
at most microsecond variance this filtering leads to a 
 report rate per sensor and gets rid of large 
measurement errors the experimental data is shown in figure 
whitham s formula suggests that the shockwave length for a 
given caliber can be approximated with a power function of 
the miss distance with a exponent best fit functions 
on our data are 
 cal t b 
 mm t b 
 mm t b 
to evaluate a shot we take the caliber whose 
approximation function results in the smallest rms error of the 
filtered sensor readings this method has less than 
caliber estimation error when an accurate trajectory estimate 
is available in other words caliber estimation only works 
if enough shockwave detections are made by the system to 
compute a trajectory 
 
 weapon estimation 
we analyzed all measured signal characteristics to find 
weapon specific information unfortunately we concluded 
that the observed muzzle blast signature is not characteristic 
enough of the weapon for classification purposes the 
reflections of the high energy muzzle blast from the environment 
have much higher impact on the muzzle blast signal shape 
than the weapon itself shooting the same weapon from 
different places caused larger differences on the recorded signal 
than shooting different weapons from the same place 
 
 
 
 
 
 
 
 
 
 
 
range m 
speed m s 
ak- 
m 
figure ak and m bullet deceleration 
measurements both weapons have the same caliber 
data is approximated using simple linear regression 
 
 
 
 
 
 
 
 
 
 
 
 
range m 
speed m s 
m 
m 
m 
figure m m and m bullet deceleration 
measurements all weapons have the same caliber 
data is approximated using simple linear regression 
however the measured speed of the projectile and its 
caliber showed good correlation with the weapon type this 
is because for a given weapon type and ammunition pair 
the muzzle velocity is nearly constant in figures and 
 we can see the relationship between the range and the 
measured bullet speed for different calibers and weapons 
in the supersonic speed range the bullet deceleration can 
be approximated with a linear function in case of the 
 mm caliber the two tested weapons ak m can 
be clearly separated figure unfortunately this is not 
necessarily true for the mm caliber the m with its 
higher muzzle speed can still be well classified but the m 
and m weapons seem practically undistinguishable 
 figure however this may be partially due to the limited 
number of practice shots we were able to take before the 
actual testing began more training data may reveal better 
separation between the two weapons since their published 
muzzle velocities do differ somewhat 
the system carries out weapon classification in the 
following manner once the trajectory is known the speed can be 
calculated for each sensor based on the shockwave geometry 
to evaluate a shot we choose the weapon type whose 
deceleration function results in the smallest rms error of the 
estimated range-speed pairs for the estimated caliber class 
 results 
an independent evaluation of the system was carried out 
by a team from nist at the us army aberdeen test center 
in april the experiment was setup on a shooting 
range with mock-up wooden buildings and walls for 
supporting elevated shooter positions and generating multipath 
effects figure shows the user interface with an aerial 
photograph of the site sensor nodes were deployed on 
surveyed points in an approximately × m area there 
were five fixed targets behind the sensor network several 
firing positions were located at each of the firing lines at 
 and meters these positions were known 
to the evaluators but not to the operators of the system 
six different weapons were utilized ak and m 
firing mm projectiles m m and m with mm 
ammunition and the caliber m 
note that the sensors remained static during the test the 
primary reason for this is that nobody is allowed downrange 
during live fire tests utilizing some kind of remote 
control platform would have been too involved for the limited 
time the range was available for the test the experiment 
therefore did not test the mobility aspect of the system 
during the one day test there were shots fired the 
results are summarized in table the system detected all 
shots successfully since a ballistic shockwave is a unique 
acoustic phenomenon it makes the detection very robust 
there were no false positives for shockwaves but there were 
a handful of false muzzle blast detections due to parallel 
tests of artillery at a nearby range 
shooter local- caliber trajectory trajectory distance no 
range ization accu- azimuth distance error of 
 m rate racy error deg error m m shots 
 
 
 
 
all 
table summary of results fusing all available 
sensor observations all shots were successfully 
detected so the detection rate is omitted localization 
rate means the percentage of shots that the sensor 
fusion was able to estimate the trajectory of the 
caliber accuracy rate is relative to the shots localized 
and not all the shots because caliber estimation 
requires the trajectory the trajectory error is broken 
down to azimuth in degrees and the actual distance 
of the shooter from the trajectory the distance 
error shows the distance between the real shooter 
position and the estimated shooter position as such 
it includes the error caused by both the trajectory 
and that of the range estimation note that the 
traditional bearing and range measures are not good 
ones for a distributed system such as ours because 
of the lack of a single reference point 
 
figure the user interface of the system 
showing the experimental setup the sensor nodes 
are labeled by their id and marked by dark circles 
the targets are black squares marked t- through 
t- the long white arrows point to the shooter 
position estimated by each sensor where it is 
missing the corresponding sensor did not have enough 
detections to measure the aoa of either the 
muzzle blast the shockwave or both the thick black 
line and large circle indicate the estimated 
trajectory and the shooter position as estimated by fusing 
all available detections from the network this shot 
from the -meter line at target t- was localized 
almost perfectly by the sensor network the caliber 
and weapon were also identified correctly out of 
 nodes were able to estimate the location alone 
their bearing accuracy is within a degree while the 
range is off by less than in the worst case 
the localization rate characterizes the system s ability to 
successfully estimate the trajectory of shots since caliber 
estimation and weapon classification relies on the trajectory 
non-localized shots are not classified either there were 
shots out of that were not localized the reason for 
missed shots is the trajectory ambiguity problem that occurs 
when the projectile passes on one side of all the sensors in 
this case two significantly different trajectories can generate 
the same set of observations see and also section 
instead of estimating which one is more likely or displaying 
both possibilities we decided not to provide a trajectory at 
all it is better not to give an answer other than a shot 
alarm than misleading the soldier 
localization accuracy is broken down to trajectory 
accuracy and range estimation precision the angle of the 
estimated trajectory was better than degree except for the 
 m range since the range should not affect trajectory 
estimation as long as the projectile passes over the network 
we suspect that the slightly worse angle precision for m 
is due to the hurried shots we witnessed the soldiers took 
near the end of the day this is also indicated by another 
datapoint the estimated trajectory distance from the 
actual targets has an average error of m for m shots 
 m for m shots and m for all but m shots 
as the distance between the targets and the sensor network 
was fixed this number should not show a × improvement 
just because the shooter is closer 
since the angle of the trajectory itself does not 
characterize the overall error-there can be a translation 
alsotable also gives the distance of the shooter from the 
estimated trajectory these indicate an error which is about 
 - of the range to put this into perspective a 
trajectory estimate for a m shot will very likely go through or 
very near the window the shooter is located at again we 
believe that the disproportionally larger errors at m are 
due to human errors in aiming as the ground truth was 
obtained by knowing the precise location of the shooter and 
the target any inaccuracy in the actual trajectory directly 
adds to the perceived error of the system 
we call the estimation of the shooter s position on the 
calculated trajectory range estimation due to the lack of a 
better term the range estimates are better than 
accurate from m and for m however this goes 
to or worse for longer distances we did not have a 
facility to test system before the evaluation for ranges 
beyond m during the evaluation we ran into the 
problem of mistaking shockwave echoes for muzzle blasts these 
echoes reached the sensors before the real muzzle blast for 
long range shots only since the projectile travels - × faster 
than the speed of sound so the time between the shockwave 
 and its possible echo from nearby objects and the muzzle 
blast increases with increasing ranges this resulted in 
underestimating the range since the system measured shorter 
times than the real ones since the evaluation we finetuned 
the muzzle blast detection algorithm to avoid this problem 
distance m ak m m m m m -m 
 m 
 m 
 m 
 m 
all 
table weapon classification results the 
percentages are relative to the number of shots localized and 
not all shots as the classification algorithm needs to 
know the trajectory and the range note that the 
difference is small there were shots localized 
out of the total 
the caliber and weapon estimation accuracy rates are 
based on the shots that were successfully localized note 
that there was a single shot that was falsely classified by the 
caliber estimator the overall weapon classification 
accuracy does not seem impressive but if we break it down 
to the six different weapons tested the picture changes 
dramatically as shown in table for four of the weapons 
 ak m m and m the classification rate is 
almost there were only two shots out of approximately 
 that were missed the m and m proved to be too 
similar and they were mistaken for each other most of the 
time one possible explanation is that we had only a limited 
number of test shots taken with these weapons right before 
the evaluation and used the wrong deceleration 
approximation function either this or a similar mistake was made 
 
since if we simply used the opposite of the system s answer 
where one of these weapons were indicated the accuracy 
would have improved x if we consider these two weapons 
a single weapon class then the classification accuracy for it 
becomes 
note that the ak and m have the same caliber 
 mm just as the m m and m do mm 
that is the system is able to differentiate between weapons 
of the same caliber we are not aware of any system that 
classifies weapons this accurately 
 single sensor performance 
as was shown previously a single sensor alone is able 
to localize the shooter if it can determine both the muzzle 
blast and the shockwave aoa that is it needs to measure 
the toa of both on at least three acoustic channels while 
shockwave detection is independent of the range-unless the 
projectile becomes subsonic- the likelihood of muzzle blast 
detection beyond meters is not enough for consistently 
getting at least three per sensor node for aoa estimation 
hence we only evaluate the single sensor performance for 
the shots that were taken from and m note that 
we use the same test data as in the previous section but we 
evaluate individually for each sensor 
table summarizes the results broken down by the ten 
sensors utilized since this is now not a distributed system 
the results are given relative to the position of the given 
sensor that is a bearing and range estimate is provided note 
that many of the common error sources of the networked 
system do not play a role here time synchronization is 
not applicable the sensor s absolute location is irrelevant 
 just as the relative location of multiple sensors the 
sensor s orientation is still important though there are several 
disadvantages of the single sensor case compared to the 
networked system there is no redundancy to compensate for 
other errors and to perform outlier rejection the 
localization rate is markedly lower and a single sensor alone is not 
able to estimate the caliber or classify the weapon 
sensor id 
loc rate 
bearing deg 
range m 
table single sensor accuracy for shots fired 
from and meters localization rate refers to 
the percentage of shots the given sensor alone was 
able to localize the bearing and range values are 
average errors they characterize the accuracy of 
localization from the given sensor s perspective 
the data indicates that the performance of the sensors 
varied significantly especially considering the localization 
rate one factor has to be the location of the given 
sensor including how far it was from the firing lines and how 
obstructed its view was also the sensors were hand-built 
prototypes utilizing nowhere near production quality 
packaging mounting in light of these factors the overall 
average bearing error of degrees and range error of m 
with a microphone spacing of less than cm are excellent 
we believe that professional manufacturing and better 
microphones could easily achieve better performance than the 
best sensor in our experiment localization rate and 
 m range error 
interestingly the largest error in range was a huge m 
clearly due to some erroneous detection yet the largest 
bearing error was less than degrees which is still a good 
indication for the soldier where to look 
the overall localization rate over all single sensors was 
 while for m shots only this jumped to note 
that the firing range was prepared to simulate an urban 
area to some extent there were a few single- and two-storey 
wooden structures built both in and around the sensor 
deployment area and the firing lines hence not all sensors had 
line-of-sight to all shooting positions we estimate that 
of the sensors had obstructed view to the shooter on 
average hence we can claim that a given sensor had about 
chance of localizing a shot within m since the sensor 
deployment area was m deep m shots correspond to 
actual distances between and m again we 
emphasize that localization needs at least three muzzle blast and 
three shockwave detections out of a possible four for each per 
sensor the detection rate for single sensors-corresponding 
to at least one shockwave detection per sensor-was 
practically 
 
 
 
 
 
 
 
 
 
 
 
 
number of sensors 
percentageofshots 
figure histogram showing what fraction of the 
 shots taken from and meters were 
localized by at most how many individual sensors alone 
 of the shots were missed by every single 
sensor i e none of them had both muzzle blast and 
shockwave aoa detections note that almost all 
of these shots were still accurately localized by the 
networked system i e the sensor fusion using all 
available observations in the sensor network 
it would be misleading to interpret these results as the 
system missing half the shots as soldiers never work alone 
and the sensor node is relatively cheap to afford having 
every soldier equipped with one we also need to look at the 
overall detection rates for every shot figure shows the 
histogram of the percentage of shots vs the number of 
individual sensors that localized it of shots were not 
localized by any sensor alone but was localized by at 
least one sensor out of ten 
 error sources 
in this section we analyze the most significant sources of 
error that affect the performance of the networked shooter 
localization and weapon classification system in order to 
correlate the distributed observations of the acoustic events 
the nodes need to have a common time and space reference 
hence errors in the time synchronization node localization 
and node orientation all degrade the overall accuracy of the 
system 
 
our time synchronization approach yields errors 
significantly less than microseconds as the sound travels 
about cm in that time time synchronization errors have a 
negligible effect on the system 
on the other hand node location and orientation can have 
a direct effect on the overall system performance notice 
that to analyze this we do not have to resort to 
simulation instead we can utilize the real test data gathered at 
aberdeen but instead of using the real sensor locations 
known very accurately and the measured and calibrated 
almost perfect node orientations we can add error terms to 
them and run the sensor fusion this exactly replicates how 
the system would have performed during the test using the 
imprecisely known locations and orientations 
another aspect of the system performance that can be 
evaluated this way is the effect of the number of available 
sensors instead of using all ten sensors in the data fusion 
we can pick any subset of the nodes to see how the accuracy 
degrades as we decrease the number of nodes 
the following experiment was carried out the number 
of sensors were varied from to in increments of each 
run picked the sensors randomly using a uniform 
distribution at each run each node was randomly moved to a 
new location within a circle around its true position with 
a radius determined by a zero-mean gaussian distribution 
finally the node orientations were perturbed using a 
zeromean gaussian distribution each combination of 
parameters were generated times and utilized for all shots 
the results are summarized in figure there is one d 
barchart for each of the experiment sets with the given fixed 
number of sensors the x-axis shows the node location error 
that is the standard deviation of the corresponding 
gaussian distribution that was varied between and meters 
the y-axis shows the standard deviation of the node 
orientation error that was varied between and degrees the 
z-axis is the resulting trajectory azimuth error note that 
the elevation angles showed somewhat larger errors than the 
azimuth since all the sensors were in approximately a 
horizontal plane and only a few shooter positions were out of the 
same plane and only by m or so the test was not sufficient 
to evaluate this aspect of the system 
there are many interesting observation one can make by 
analyzing these charts node location errors in this range 
have a small effect on accuracy node orientation errors on 
the other hand noticeably degrade the performance still 
the largest errors in this experiment of degrees for 
sensors and degrees for sensors are still very good 
note that as the location and orientation errors increase 
and the number of sensors decrease the most significantly 
affected performance metric is the localization rate see 
table for a summary successful localization goes down 
from almost to when we go from sensors to 
 even without additional errors this is primarily caused 
by geometry for a successful localization the bullet needs 
to pass over the sensor network that is at least one sensor 
should be on the side of the trajectory other than the rest 
of the nodes this is a simplification for illustrative 
purposes if all the sensors and the trajectory are not coplanar 
localization may be successful even if the projectile passes 
on one side of the network see section as the 
numbers of sensors decreased in the experiment by randomly 
selecting a subset the probability of trajectories abiding by 
this rule decreased this also means that even if there are 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
azimutherror degree 
position error m 
orientation error 
 degree 
 sensors 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
azimutherror degree 
position error m 
orientation error 
 degree 
 sensors 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
azimutherror degree 
position error m 
orientation error 
 degree 
 sensors 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
azimutherror degree 
position error m 
orientation error 
 degree 
 sensors 
figure the effect of node localization and 
orientation errors on azimuth accuracy with and 
 nodes note that the chart for nodes is almost 
identical for the -node case hence it is omitted 
 
many sensors i e soldiers but all of them are right next to 
each other the localization rate will suffer however when 
the sensor fusion does provide a result it is still accurate 
even with few available sensors and relatively large 
individual errors a very few consistent observation lead to good 
accuracy as the inconsistent ones are discarded by the 
algorithm this is also supported by the observation that for 
the cases with the higher number of sensors or the 
localization rate is hardly affected by even large errors 
errors sensors 
 m deg 
 m deg 
 m deg 
 m deg 
 m deg 
table localization rate as a function of the 
number of sensors used the sensor node location and 
orientation errors 
one of the most significant observations on figure and 
table is that there is hardly any difference in the data for 
 and sensors this means that there is little advantage 
of adding more nodes beyond sensors as far as the accuracy 
is concerned 
the speed of sound depends on the ambient temperature 
the current prototype considers it constant that is typically 
set before a test it would be straightforward to employ 
a temperature sensor to update the value of the speed of 
sound periodically during operation note also that wind 
may adversely affect the accuracy of the system the sensor 
fusion however could incorporate wind speed into its 
calculations it would be more complicated than temperature 
compensation but could be done 
other practical issues also need to be looked at before a 
real world deployment silencers reduce the muzzle blast 
energy and hence the effective range the system can 
detect it at however silencers do not effect the shockwave 
and the system would still detect the trajectory and caliber 
accurately the range and weapon type could not be 
estimated without muzzle blast detections subsonic weapons 
do not produce a shockwave however this is not of great 
significance since they have shorter range lower accuracy 
and much less lethality hence their use is not widespread 
and they pose less danger in any case 
another issue is the type of ammunition used irregular 
armies may use substandard even hand manufactured 
bullets this effects the muzzle velocity of the weapon for 
weapon classification to work accurately the system would 
need to be calibrated with the typical ammunition used by 
the given adversary 
 related work 
acoustic detection and recognition has been under 
research since the early fifties the area has a close 
relevance to the topic of supersonic flow mechanics fansler 
analyzed the complex near-field pressure waves that occur 
within a foot of the muzzle blast fansler s work gives a 
good idea of the ideal muzzle blast pressure wave without 
contamination from echoes or propagation effects 
experiments with greater distances from the muzzle were 
conducted by stoughton the measurements of the ballistic 
shockwaves using calibrated pressure transducers at known 
locations measured bullet speeds and miss distances of 
 meters for mm and mm projectiles were made 
results indicate that ground interaction becomes a problem 
for miss distances of meters or larger 
another area of research is the signal processing of gunfire 
acoustics the focus is on the robust detection and length 
estimation of small caliber acoustic shockwaves and 
muzzle blasts possible techniques for classifying signals as 
either shockwaves or muzzle blasts includes short-time fourier 
transform stft the smoothed pseudo wigner-ville 
distribution spwvd and a discrete wavelet transformation 
 dwt joint time-frequency jtf spectrograms are used 
to analyze the typical separation of the shockwave and 
muzzle blast transients in both time and frequency mays 
concludes that the dwt is the best method for classifying 
signals as either shockwaves or muzzle blasts because it works 
well and is less expensive to compute than the spwvd 
the edges of the shockwave are typically well defined and 
the shockwave length is directly related to the bullet 
characteristics a paper by sadler compares two shockwave 
edge detection methods a simple gradient-based detector 
and a multi-scale wavelet detector it also demonstrates how 
the length of the shockwave as determined by the edge 
detectors can be used along with whithams equations to 
estimate the caliber of a projectile note that the available 
computational performance on the sensor nodes the limited 
wireless bandwidth and real-time requirements render these 
approaches infeasible on our platform 
a related topic is the research and development of 
experimental and prototype shooter location systems researchers 
at bbn have developed the bullet ears system which has 
the capability to be installed in a fixed position or worn by 
soldiers the fixed system has tetrahedron shaped 
microphone arrays with meter spacing the overall system 
consists of two to three of these arrays spaced to 
meters from each other the soldier-worn system has 
microphones as well as a gps antenna and orientation 
sensors mounted on a helmet there is a low speed rf 
connection from the helmet to the processing body an extensive 
test has been conducted to measure the performance of both 
type of systems the fixed systems performance was one 
order of magnitude better in the angle calculations while their 
range performance where matched the angle accuracy of 
the fixed system was dominantly less than one degree while 
it was around five degrees for the helmet mounted one the 
range accuracy was around percent for both of the 
systems the problem with this and similar centralized 
systems is the need of the one or handful of microphone arrays 
to be in line-of-sight of the shooter a sensor networked 
based solution has the advantage of widely distributed 
sensing for better coverage multipath effect compensation and 
multiple simultaneous shot resolution this is especially 
important for operation in acoustically reverberant urban 
areas note that bbn s current vehicle-mounted system 
called boomerang a modified version of bullet ears 
is currently used in iraq 
the company shotspotter specializes in law enforcement 
systems that report the location of gunfire to police within 
seconds the goal of the system is significantly different 
than that of military systems shotspotter reports m 
typical accuracy which is more than enough for police to 
 
respond they are also manufacturing experimental soldier 
wearable and uav mounted systems for military use 
but no specifications or evaluation results are publicly 
available 
 conclusions 
the main contribution of this work is twofold first the 
performance of the overall distributed networked system is 
excellent most noteworthy are the trajectory accuracy of 
one degree the correct caliber estimation rate of well over 
 and the close to weapon classification rate for of 
the weapons tested the system proved to be very robust 
when increasing the node location and orientation errors and 
decreasing the number of available sensors all the way down 
to a couple the key factor behind this is the sensor fusion 
algorithm s ability to reject erroneous measurements it is 
also worth mentioning that the results presented here 
correspond to the first and only test of the system beyond m 
and with six different weapons we believe that with the 
lessons learned in the test a consecutive field experiment 
could have showed significantly improved results especially 
in range estimation beyond m and weapon classification 
for the remaining two weapons that were mistaken for each 
other the majority of the times during the test 
second the performance of the system when used in 
standalone mode that is when single sensors alone provided 
localization was also very good while the overall 
localization rate of per sensor for shots up to m could be 
improved the bearing accuracy of less than a degree and 
the average range error are remarkable using the 
handmade prototypes of the low-cost nodes note that of 
the shots were successfully localized by at least one of the 
ten sensors utilized in standalone mode 
we believe that the technology is mature enough that 
a next revision of the system could be a commercial one 
however important aspects of the system would still need 
to be worked on we have not addresses power 
management yet a current node runs on aa batteries for about 
 hours of continuous operation a deployable version of 
the sensor node would need to be asleep during normal 
operation and only wake up when an interesting event occurs 
an analog trigger circuit could solve this problem however 
the system would miss the first shot instead the acoustic 
channels would need to be sampled and stored in a circular 
buffer the rest of the board could be turned off when 
a trigger wakes up the board the acoustic data would be 
immediately available experiments with a previous 
generation sensor board indicated that this could provide a x 
increase in battery life other outstanding issues include 
weatherproof packaging and ruggedization as well as 
integration with current military infrastructure 
 references 
 bbn technologies website http www bbn com 
 e danicki acoustic sniper localization archives of 
acoustics - 
 g l duckworth et al fixed and wearable acoustic 
counter-sniper systems for law enforcement in e m 
carapezza and d b law editors proc spie vol 
 p - pages - jan 
 k fansler description of muzzle blast by modified 
scaling models shock and vibration - 
 d gay p levis r von behren m welsh 
e brewer and d culler the nesc language a 
holistic approach to networked embedded systems 
proceedings of programming language design and 
implementation pldi june 
 j hill r szewczyk a woo s hollar d culler and 
k pister system architecture directions for networked 
sensors in proc of asplos nov 
 b kus´y g balogh p v¨olgyesi j sallai a n´adas 
a l´edeczi m mar´oti and l meertens node-density 
independent localization information processing in 
sensor networks ipsn spots track apr 
 a l´edeczi a n´adas p v¨olgyesi g balogh 
b kus´y j sallai g pap s d´ora k moln´ar 
m mar´oti and g simon countersniper system for 
urban warfare acm transactions on sensor 
networks - nov 
 m mar´oti directed flood-routing framework for 
wireless sensor networks in proceedings of the th 
acm ifip usenix international conference on 
middleware pages - new york ny usa 
springer-verlag new york inc 
 b mays shockwave and muzzle blast classification 
via joint time frequency and wavelet analysis 
technical report army research lab adelphi md 
 - sept 
 tinyos hardware platforms 
http tinyos net scoop special hardware 
 crossbow micaz mpr radio module 
http www xbow com products productsdetails 
aspx sid 
 picoblaze user resources 
http www xilinx com ipcenter processor 
central picoblaze picoblaze user resources htm 
 b m sadler t pham and l c sadler optimal 
and wavelet-based shock wave detection and 
estimation acoustical society of america journal 
 - aug 
 j sallai b kus´y a l´edeczi and p dutta on the 
scalability of routing-integrated time synchronization 
 rd european workshop on wireless sensor networks 
 ewsn feb 
 shotspotter website http 
 www shotspotter com products military html 
 g simon m mar´oti a l´edeczi g balogh b kus´y 
a n´adas g pap j sallai and k frampton sensor 
network-based countersniper system in sensys 
proceedings of the nd international conference on 
embedded networked sensor systems pages - new 
york ny usa acm press 
 r stoughton measurements of small-caliber ballistic 
shock waves in air acoustical society of america 
journal - aug 
 b a weiss c schlenoff m shneier and a virts 
technology evaluations and performance metrics for 
soldier-worn sensors for assist in performance metrics 
for intelligent systems workshop aug 
 g whitham flow pattern of a supersonic projectile 
communications on pure and applied mathematics 
 
 
a point-distribution index and its application to 
sensor-grouping in wireless sensor networks 
yangfan zhou haixuan yang michael r lyu edith c -h ngai 
department of computer science and engineering 
the chinese university of hong kong 
hong kong china 
{yfzhou hxyang lyu chngai} cse cuhk edu hk 
abstract 
we propose ι a novel index for evaluation of point-distribution 
ι is the minimum distance between each pair of points 
normalized by the average distance between each pair of points we find 
that a set of points that achieve a maximum value of ι result in 
a honeycomb structure we propose that ι can serve as a good 
index to evaluate the distribution of the points which can be 
employed in coverage-related problems in wireless sensor networks 
 wsns to validate this idea we formulate a general 
sensorgrouping problem for wsns and provide a general sensing model 
we show that locally maximizing ι at sensor nodes is a good 
approach to solve this problem with an algorithm called 
maximizingι node-deduction mind simulation results verify that mind 
outperforms a greedy algorithm that exploits sensor-redundancy we 
design this demonstrates a good application of employing ι in 
coverage-related problems for wsns 
categories and subject descriptors 
c computer - communication networks network 
architecture and design c special-purpose and application-based 
systems realtime and embedded systems 
general terms 
theory algorithms design verification performance 
 introduction 
a wireless sensor network wsn consists of a large number of 
in-situ battery-powered sensor nodes a wsn can collect the data 
about physical phenomena of interest there are many 
potential applications of wsns including environmental monitoring and 
surveillance etc 
in many application scenarios wsns are employed to conduct 
surveillance tasks in adverse or even worse in hostile working 
environments one major problem caused is that sensor nodes are 
subjected to failures therefore fault tolerance of a wsn is 
critical 
one way to achieve fault tolerance is that a wsn should contain 
a large number of redundant nodes in order to tolerate node 
failures it is vital to provide a mechanism that redundant nodes can be 
working in sleeping mode i e major power-consuming units such 
as the transceiver of a redundant sensor node can be shut off to 
save energy and thus to prolong the network lifetime redundancy 
should be exploited as much as possible for the set of sensors that 
are currently taking charge in the surveillance work of the network 
area 
we find that the minimum distance between each pair of points 
normalized by the average distance between each pair of points 
serves as a good index to evaluate the distribution of the points we 
call this index denoted by ι the normalized minimum distance if 
points are moveable we find that maximizing ι results in a 
honeycomb structure the honeycomb structure poses that the coverage 
efficiency is the best if each point represents a sensor node that 
is providing surveillance work employing ι in coverage-related 
problems is thus deemed promising 
this enlightens us that maximizing ι is a good approach to 
select a set of sensors that are currently taking charge in the 
surveillance work of the network area to explore the effectiveness of 
employing ι in coverage-related problems we formulate a 
sensorgrouping problem for high-redundancy wsns an algorithm called 
maximizing-ι node-deduction mind is proposed in which 
redundant sensor nodes are removed to obtain a large ι for each set of 
sensors that are currently taking charge in the surveillance work of 
the network area we also introduce another greedy solution called 
incremental coverage quality algorithm icqa for this problem 
which serves as a benchmark to evaluate mind 
the main contribution of this paper is twofold first we 
introduce a novel index ι for evaluation of point-distribution we show 
that maximizing ι of a wsn results in low redundancy of the 
network second we formulate a general sensor-grouping problem 
for wsns and provide a general sensing model with the mind 
algorithm we show that locally maximizing ι among each sensor 
node and its neighbors is a good approach to solve this problem 
this demonstrates a good application of employing ι in 
coveragerelated problems 
the rest of the paper is organized as follows in section we 
introduce our point-distribution index ι we survey related work 
and formulate a sensor-grouping problem together with a general 
sensing model in section section investigates the application 
of ι in this grouping problem we propose mind for this problem 
 
and introduce icqa as a benchmark in section we present 
our simulation results in which mind and icqa are compared 
section provides conclusion remarks 
 the normalized minimum distance 
ι a point-distribution index 
suppose there are n points in a euclidean space ω the 
coordinates of these points are denoted by xi i n 
it may be necessary to evaluate how the distribution of these 
points is there are many metrics to achieve this goal for 
example the mean square error from these points to their mean value 
can be employed to calculate how these points deviate from their 
mean i e their central in resource-sharing evaluation the global 
fairness index gfi is often employed to measure how even the 
resource distributes among these points when xi represents the 
amount of resource that belong to point i in wsns gfi is usually 
used to calculate how even the remaining energy of sensor nodes 
is 
when n is larger than and the points do not all overlap that 
points all overlap means xi xj ∀ i j n we propose 
a novel index called the normalized minimum distance namely ι 
to evaluate the distribution of the points ι is the minimum distance 
between each pair of points normalized by the average distance 
between each pair of points it is calculated by 
ι 
min xi − xj 
µ 
 ∀ i j n and i j 
where xi − xj denotes the euclidean distance between point 
i and point j in ω the min · function calculates the minimum 
distance between each pair of points and µ is the average distance 
between each pair of points which is 
µ 
 
pn 
i 
pn 
j j i xi − xj 
n n − 
 
ι measures how well the points separate from one another 
obviously ι is in interval ι is equal to if and only if n is equal 
to and these three points forms an equilateral triangle ι is equal 
to zero if any two points overlap ι is a very interesting value of a 
set of points if we consider each xi ∀i n is a variable in 
ω how these n points would look like if ι is maximized 
an algorithm is implemented to generate the topology in which 
ι is locally maximized the algorithm can be found in we 
consider a -dimensional space we select n 
and perform this algorithm in order to avoid that the algorithm 
converge to local optimum we select different random seeds to 
generate the initial points for time and obtain the best one 
that results in the largest ι when the algorithm converges figure 
demonstrates what the resulting topology looks like when n 
as an example 
suppose each point represents a sensor node if the sensor 
coverage model is the boolean coverage model and 
the coverage radius of each node is the same it is exciting to see 
that this topology results in lowest redundancy because the vonoroi 
diagram formed by these nodes a vonoroi diagram formed by 
a set of nodes partitions a space into a set of convex polygons such 
that points inside a polygon are closest to only one particular node 
is a honeycomb-like structure 
 
this enlightens us that ι may be employed to solve problems 
related to sensor-coverage of an area in wsns it is desirable 
 
this is how base stations of a wireless cellular network are 
deployed and why such a network is called a cellular one 
 
 
 
 
 
 
 
 
 
 
x 
y 
figure node number ι 
that the active sensor nodes that are performing surveillance task 
should separate from one another under the constraint that the 
sensing area should be covered the more each node separates from 
the others the less the redundancy of the coverage is ι indicates 
the quality of such separation it should be useful for approaches 
on sensor-coverage related problems 
in our following discussions we will show the effectiveness of 
employing ι in sensor-grouping problem 
 the sensor-grouping problem 
in many application scenarios to achieve fault tolerance a wsn 
contains a large number of redundant nodes in order to tolerate 
node failures a node sleeping-working schedule scheme is 
therefore highly desired to exploit the redundancy of working sensors 
and let as many nodes as possible sleep 
much work in the literature is on this issue yan et al 
introduced a differentiated service in which a sensor node finds out 
its responsible working duration with cooperation of its neighbors 
to ensure the coverage of sampled points ye et al developed 
peas in which sensor nodes wake up randomly over time probe 
their neighboring nodes and decide whether they should begin to 
take charge of surveillance work xing et al exploited a 
probabilistic distributed detection model with a protocol called 
coordinating grid co-grid wang et al designed an approach called 
coverage configuration protocol ccp which introduced the 
notion that the coverage degree of intersection-points of the 
neighboring nodes sensing-perimeters indicates the coverage of a convex 
region in our recent work we also provided a sleeping 
configuration protocol namely sscp in which sleeping eligibility 
of a sensor node is determined by a local voronoi diagram sscp 
can provide different levels of redundancy to maintain different 
requirements of fault tolerance 
the major feature of the aforementioned protocols is that they 
employ online distributed and localized algorithms in which a 
sensor node determines its sleeping eligibility and or sleeping time 
based on the coverage requirement of its sensing area with some 
information provided by its neighbors 
another major approach for sensor node sleeping-working 
scheduling issue is to group sensor nodes sensor nodes in a network are 
divided into several disjoint sets each set of sensor nodes are able 
to maintain the required area surveillance work the sensor nodes 
are scheduled according to which set they belong to these sets 
work successively only one set of sensor nodes work at any time 
we call the issue sensor-grouping problem 
the major advantage of this approach is that it avoids the 
overhead caused by the processes of coordination of sensor nodes to 
make decision on whether a sensor node is a candidate to sleep or 
 
work and how long it should sleep or work such processes should 
be performed from time to time during the lifetime of a network in 
many online distributed and localized algorithms the large 
overhead caused by such processes is the main drawback of the 
online distributed and localized algorithms on the contrary roughly 
speaking this approach groups sensor nodes in one time and 
schedules when each set of sensor nodes should be on duty it does not 
require frequent decision-making on working sleeping eligibility 
 
in by slijepcevic et al the sensing area is divided into 
regions sensor nodes are grouped with the most-constrained 
leastconstraining algorithm it is a greedy algorithm in which the 
priority of selecting a given sensor is determined by how many 
uncovered regions this sensor covers and the redundancy caused by 
this sensor in by cardei et al disjoint sensor sets are 
modeled as disjoint dominating sets although maximum dominating 
sets computation is np-complete the authors proposed a 
graphcoloring based algorithm cardei et al also studied similar problem 
in the domain of covering target points in the np-completeness 
of the problem is proved and a heuristic that computes the sets are 
proposed these algorithms are centralized solutions of 
sensorgrouping problem 
however global information e g the location of each in-network 
sensor node of a large scale wsn is also very expensive to 
obtained online also it is usually infeasible to obtain such 
information before sensor nodes are deployed for example sensor nodes 
are usually deployed in a random manner and the location of each 
in-network sensor node is determined only after a node is deployed 
the solution of sensor-grouping problem should only base on 
locally obtainable information of a sensor node that is to say nodes 
should determine which group they should join in a fully 
distributed way here locally obtainable information refers to a node s 
local information and the information that can be directly obtained 
from its adjacent nodes i e nodes within its communication range 
in subsection we provide a general problem formulation of 
the sensor-grouping problem distributed-solution requirement is 
formulated in this problem it is followed by discussion in 
subsection on a general sensing model which serves as a given 
condition of the sensor-grouping problem formulation 
to facilitate our discussions the notations in our following 
discussions are described as follows 
 n the number in-network sensor nodes 
 s j j m the jth set of sensor nodes where m 
is the number of sets 
 l i i n the physical location of node i 
 φ the area monitored by the network i e the sensing area 
of the network 
 r the sensing radius of a sensor node we assume that 
a sensor node can only be responsible to monitor a circular 
area centered at the node with a radius equal to r this is 
a usual assumption in work that addresses sensor-coverage 
related problems we call this circular area the sensing area 
of a node 
 problem formulation 
we assume that each sensor node can know its approximate 
physical location the approximate location information is obtainable 
if each sensor node carries a gps receiver or if some localization 
algorithms are employed e g 
 
note that if some nodes die a re-grouping process might also be 
performed to exploit the remaining nodes in a set of sensor nodes 
how to provide this mechanism is beyond the scope of this paper 
and yet to be explored 
problem given 
 the set of each sensor node i s sensing neighbors n i and 
the location of each member in n i 
 a sensing model which quantitatively describes how a point 
p in area φ is covered by sensor nodes that are responsible to 
monitor this point we call this quantity the coverage quality 
of p 
 the coverage quality requirement in φ denoted by s when 
the coverage of a point is larger than this threshold we say 
this point is covered 
for each sensor node i make a decision on which group s j it 
should join so that 
 area φ can be covered by sensor nodes in each set s j 
 m the number of sets s j is maximized 
in this formulation we call sensor nodes within a circular area 
centered at a sensor node i with a radius equal to · r the sensing 
neighbors of node i this is because sensors nodes in this area 
together with node i may be cooperative to ensure the coverage of 
a point inside node i s sensing area 
we assume that the communication range of a sensor node is 
larger than · r which is also a general assumption in work that 
addresses sensor-coverage related problems that is to say the first 
given condition in problem is the information that can be obtained 
directly from a node s adjacent nodes it is therefore locally 
obtainable information the last two given conditions in this problem 
formulation can be programmed into a node before it is deployed 
or by a node-programming protocol e g during network 
runtime therefore the given conditions can all be easily obtained by 
a sensor-grouping scheme with fully distributed implementation 
we reify this problem with a realistic sensing model in next 
subsection 
 a general sensing model 
as wsns are usually employed to monitor possible events in a 
given area it is therefore a design requirement that an event 
occurring in the network area must may be successfully detected by 
sensors 
this issue is usually formulated as how to ensure that an event 
signal omitted in an arbitrary point in the network area can be 
detected by sensor nodes obviously a sensing model is required to 
address this problem so that how a point in the network area is 
covered can be modeled and quantified thus the coverage quality of 
a wsn can be evaluated 
different applications of wsns employ different types of 
sensors which surely have widely different theoretical and physical 
characteristics therefore to fulfill different application 
requirements different sensing models should be constructed based on the 
characteristics of the sensors employed 
a simple theoretical sensing model is the boolean sensing model 
 boolean sensing model assumes that a sensor 
node can always detect an event occurring in its responsible 
sensing area but most sensors detect events according to the signal 
strength sensed event signals usually fade in relation to the 
physical distance between an event and the sensor the larger the 
distance the weaker the event signals that can be sensed by the sensor 
which results in a reduction of the probability that the event can be 
successfully detected by the sensor 
as in wsns event signals are usually electromagnetic acoustic 
or photic signals they fade exponentially with the increasing of 
 
their transmit distance specifically the signal strength e d of an 
event that is received by a sensor node satisfies 
e d 
α 
dβ 
 
where d is the physical distance from the event to the sensor node 
α is related to the signal strength omitted by the event and β is 
signal fading factor which is typically a positive number larger than 
or equal to usually α and β are considered as constants 
based on this notion to be more reasonable researchers propose 
collaborative sensing model to capture application requirements 
area coverage can be maintained by a set of collaborative sensor 
nodes for a point with physical location l the point is considered 
covered by the collaboration of i sensors denoted by k ki if 
and only if the following two equations holds 
∀j i l kj − l r 
c l 
ix 
j 
 e l kj − l s 
c l is regarded as the coverage quality of location l in the 
network area 
however we notice that defining the sensibility as the sum of the 
sensed signal strength by each collaborative sensor implies a very 
special application applications must employ the sum of the 
signal strength to achieve decision-making to capture generally 
realistic application requirement we modify the definition described 
in equation the model we adopt in this paper is described in 
details as follows 
we consider the probability p l kj that an event located at l 
can be detected by sensor kj is related to the signal strength sensed 
by kj formally 
p l kj γe d 
δ 
 l kj − l β 
 
where γ is a constant and δ γα is a constant too normalizes 
the distance to a proper scale and the item is to avoid infinite 
value of p l kj 
the probability that an event located at l can be detected by any 
collaborative sensors that satisfied equation is 
p l − 
iy 
j 
 − p l kj 
as the detection probability p l reasonably determines how 
an event occurring at location l can be detected by the networks it 
is a good measure of the coverage quality of location l in a wsn 
specifically equation is modified to 
c l p l 
 − 
iy 
j 
 − 
δ 
 l kj − l β 
 s 
to sum it up we consider a point at location l is covered if 
equation and hold 
 maximizing-ι node-deduction 
algorithm for sensor-grouping 
problem 
before we process to introduce algorithms to solve the sensor 
grouping problem let us define the margin denoted by θ of an 
area φ monitored by the network as the band-like marginal area 
of φ and all the points on the outer perimeter of θ is ρ distance 
away from all the points on the inner perimeter of θ ρ is called the 
margin length 
in a practical network sensor nodes are usually evenly deployed 
in the network area obviously the number of sensor nodes that 
can sense an event occurring in the margin of the network is smaller 
than the number of sensor nodes that can sense an event occurring 
in other area of the network based on this consideration in our 
algorithm design we ensure the coverage quality of the network 
area except the margin the information on φ and ρ is 
networkbased each in-network sensor node can be pre-programmed or 
on-line informed about φ and ρ and thus calculate whether a point 
in its sensing area is in the margin or not 
 maximizing-ι node-deduction algorithm 
the node-deduction process of our maximizing-ι node-deduction 
algorithm mind is simple a node i greedily maximizes ι of the 
sub-network composed by itself its ungrouped sensing neighbors 
and the neighbors that are in the same group of itself under the 
constraint that the coverage quality of its sensing area should be 
ensured node i deletes nodes in this sub-network one by one the 
candidate to be pruned satisfies that 
 it is an ungrouped node 
 the deletion of the node will not result in uncovered-points 
inside the sensing area of i 
a candidate is deleted if the deletion of the candidate results in 
largest ι of the sub-network compared to the deletion of other 
candidates this node-deduction process continues until no candidate 
can be found then all the ungrouped sensing neighbors that are 
not deleted are grouped into the same group of node i we call the 
sensing neighbors that are in the same group of node i the group 
sensing neighbors of node i we then call node i a finished node 
meaning that it has finished the above procedure and the sensing 
area of the node is covered those nodes that have not yet finished 
this procedure are called unfinished nodes 
the above procedure initiates at a random-selected node that is 
not in the margin the node is grouped to the first group it 
calculates the resulting group sensing neighbors of it based on the above 
procedure it informs these group sensing neighbors that they are 
selected in the group then it hands over the above procedure to 
an unfinished group sensing neighbors that is the farthest from 
itself this group sensing neighbor continues this procedure until no 
unfinished neighbor can be found then the first group is formed 
 algorithmic description of this procedure can be found at 
after a group is formed another random-selected ungrouped 
node begins to group itself to the second group and initiates the 
above procedure in this way groups are formed one by one when 
a node that involves in this algorithm found out that the coverage 
quality if its sensing area except what overlaps the network margin 
cannot be ensured even if all its ungrouped sensing neighbors are 
grouped into the same group as itself the algorithm stops mind 
is based on locally obtainable information of sensor nodes it is 
a distributed algorithm that serves as an approximate solution of 
problem 
 incremental coverage quality algorithm 
a benchmark for mind 
to evaluate the effectiveness of introducing ι in the sensor-group 
problem another algorithm for sensor-group problem called 
incremental coverage quality algorithm icqa is designed our aim 
 
is to evaluate how an idea i e mind based on locally maximize 
ι performs 
in icqa a node-selecting process is as follows a node i 
greedily selects an ungrouped sensing neighbor in the same group as 
itself one by one and informs the neighbor it is selected in the group 
the criterion is 
 the selected neighbor is responsible to provide surveillance 
work for some uncovered parts of node i s sensing area i e 
the coverage quality requirement of the parts is not fulfilled 
if this neighbor is not selected 
 the selected neighbor results in highest improvement of the 
coverage quality of the neighbor s sensing area 
the improvement of the coverage quality mathematically should 
be the integral of the the improvements of all points inside the 
neighbor s sensing area a numerical approximation is employed 
to calculate this improvement details are presented in our 
simulation study 
this node-selecting process continues until the sensing area of 
node i is entirely covered in this way node i s group sensing 
neighbors are found the above procedure is handed over as what 
mind does and new groups are thus formed one by one and 
the condition that icqa stops is the same as mind icqa is also 
based on locally obtainable information of sensor nodes icqa is 
also a distributed algorithm that serves as an approximate solution 
of problem 
 simulation results 
to evaluate the effectiveness of employing ι in sensor-grouping 
problem we build simulation surveillance networks we employ 
mind and icqa to group the in-network sensor nodes we 
compare the grouping results with respect to how many groups both 
algorithms find and how the performance of the resulting groups 
are 
detailed settings of the simulation networks are shown in table 
 in simulation networks sensor nodes are randomly deployed in 
a uniform manner in the network area 
table the settings of the simulation networks 
area of sensor field m m 
ρ m 
r m 
α β γ and and 
s 
for evaluating the coverage quality of the sensing area of a node 
we divide the sensing area of a node into several regions and regard 
the coverage quality of the central point in each region as a 
representative of the coverage quality of the region this is a numerical 
approximation larger number of such regions results in better 
approximation as sensor nodes are with low computational 
capacity there is a tradeoff between the number of such regions and the 
precision of the resulting coverage quality of the sensing area of a 
node in our simulation study we set this number for 
evaluating the improvement of coverage quality in icqa we sum up all 
the improvements at each region-center as the total improvement 
 number of groups formed by mind and 
icqa 
we set the total in-network node number to different values and 
let the networks perform mind and icqa for each n 
simulations run with several random seeds to generate different networks 
results are averaged figure shows the group numbers found in 
networks with different n s 
 
 
 
 
 
 
 
 
 
 
 
 
total in−network node number 
totalnumberofgroupsfound 
icqa 
mmnp 
figure the number of groups found by mind and icqa 
we can see that mind always outperforms icqa in terms of 
the number of groups formed obviously the larger the number of 
groups can be formed the more the redundancy of each group is 
exploited this output shows that an approach like mind that aim 
to maximize ι of the resulting topology can exploits redundancy 
well 
as an example in case that n the results of five 
networks are listed in table 
table the grouping results of five networks with n 
net mind icqa mind icqa 
group number group number average ι average ι 
 
 
 
 
 
the difference between the average ι of the groups in each 
network shows that groups formed by mind result in topologies with 
larger ι s it demonstrates that ι is good indicator of redundancy in 
different networks 
 the performance of the resulting groups 
although mind forms more groups than icqa does which 
implies longer lifetime of the networks another importance 
consideration is how these groups formed by mind and icqa perform 
we let events randomly occur in the network area except 
the margin we compare how many events happen at the locations 
where the quality is less than the requirement s when each 
resulting group is conducting surveillance work we call the 
number of such events the failure number of group figure shows 
the average failure numbers of the resulting groups when different 
node numbers are set 
we can see that the groups formed by mind outperform those 
formed by icqa because the groups formed by mind result in 
lower failure numbers this further demonstrates that mind is a 
good approach for sensor-grouping problem 
 
 
 
 
 
 
 
 
 
total in−network node number 
averagefailurenumbers 
icqa 
mmnp 
figure the failure numbers of mind and icqa 
 conclusion 
this paper proposes ι a novel index for evaluation of 
pointdistribution ι is the minimum distance between each pair of points 
normalized by the average distance between each pair of points 
we find that a set of points that achieve a maximum value of ι 
result in a honeycomb structure we propose that ι can serve as a 
good index to evaluate the distribution of the points which can be 
employed in coverage-related problems in wireless sensor networks 
 wsns we set out to validate this idea by employing ι to a 
sensorgrouping problem we formulate a general sensor-grouping 
problem for wsns and provide a general sensing model with an 
algorithm called maximizing-ι node-deduction mind we show that 
maximizing ι at sensor nodes is a good approach to solve this 
problem simulation results verify that mind outperforms a greedy 
algorithm that exploits sensor-redundancy we design in terms of the 
number and the performance of the groups formed this 
demonstrates a good application of employing ι in coverage-related 
problems 
 acknowledgement 
the work described in this paper was substantially supported by 
two grants rgc project no cuhk e and ugc project 
no aoe e- of the hong kong special administrative 
region china 
 references 
 i akyildiz w su y sankarasubramaniam and e cayirci 
a survey on wireless sensor networks ieee 
communications magazine - 
 f aurenhammer vononoi diagram - a survey of a 
fundamental geometric data structure acm computing 
surveys - september 
 n bulusu j heidemann and d estrin gps-less low-cost 
outdoor localization for very small devices ieee personal 
communication october 
 m cardei and d -z du improving wireless sensor network 
lifetime through power aware organization acm wireless 
networks may 
 m cardei d maccallum x cheng m min x jia d li 
and d -z du wireless sensor networks with energy efficient 
organization journal of interconnection networks - 
december 
 m cardei and j wu coverage in wireless sensor networks 
in handbook of sensor networks eds m ilyas and i 
magboub crc press 
 x chen and m r lyu a sensibility-based sleeping 
configuration protocol for dependable wireless sensor 
networks cse technical report the chinese university of 
hong kong 
 r jain w hawe and d chiu a quantitative measure of 
fairness and discrimination for resource allocation in shared 
computer systems technical report dec-tr- 
september 
 s s kulkarni and l wang mnp multihop network 
reprogramming service for sensor networks in proc of the 
 th international conference on distributed computing 
systems icdcs june 
 b liu and d towsley a study on the coverage of 
large-scale sensor networks in proc of the st ieee 
international conference on mobile ad-hoc and sensor 
systems fort lauderdale fl october 
 a mainwaring j polastre r szewczyk d culler and 
j anderson wireless sensor networks for habitat 
monitoring in proc of the acm international workshop on 
wireless sensor networks and applications 
 s megerian f koushanfar g qu g veltri and 
m potkonjak explosure in wirless sensor networks theory 
and pratical solutions wireless networks 
 s slijepcevic and m potkonjak power efficient 
organization of wireless sensor networks in proc of the 
ieee international conference on communications icc 
volume helsinki finland june 
 d tian and n d georganas a node scheduling scheme for 
energy conservation in large wireless sensor networks 
wireless communications and mobile computing 
 - may 
 x wang g xing y zhang c lu r pless and c gill 
integrated coverage and connectivity configuration in 
wireless sensor networks in proc of the st acm 
international conference on embedded networked sensor 
systems sensys los angeles ca november 
 g xing c lu r pless and j a o´ sullivan co-grid an 
efficient converage maintenance protocol for distributed 
sensor networks in proc of the rd international 
symposium on information processing in sensor networks 
 ipsn berkeley ca april 
 t yan t he and j a stankovic differentiated 
surveillance for sensor networks in proc of the st acm 
international conference on embedded networked sensor 
systems sensys los angeles ca november 
 f ye g zhong j cheng s lu and l zhang peas a 
robust energy conserving protocol for long-lived sensor 
networks in proc of the rd international conference on 
distributed computing systems icdcs providence rhode 
island may 
 y zhou h yang and m r lyu a point-distribution index 
and its application in coverage-related problems cse 
technical report the chinese university of hong kong 
 
 
cenwits a sensor-based loosely coupled search and 
rescue system using witnesses 
jyh-how huang 
department of computer 
science 
university of colorado 
campus box 
boulder co - 
huangjh cs colorado edu 
saqib amjad 
department of computer 
science 
university of colorado 
campus box 
boulder co - 
saqib amjad colorado edu 
shivakant mishra 
department of computer 
science 
university of colorado 
campus box 
boulder co - 
mishras cs colorado edu 
abstract 
this paper describes the design implementation and 
evaluation of a search and rescue system called cenwits cenwits 
uses several small commonly-available rf-based sensors 
and a small number of storage and processing devices it is 
designed for search and rescue of people in emergency 
situations in wilderness areas a key feature of cenwits is that 
it does not require a continuously connected sensor network 
for its operation it is designed for an intermittently 
connected network that provides only occasional connectivity 
it makes a judicious use of the combined storage capability 
of sensors to filter organize and store important 
information combined battery power of sensors to ensure that the 
system remains operational for longer time periods and 
intermittent network connectivity to propagate information 
to a processing center a prototype of cenwits has been 
implemented using berkeley mica motes the paper 
describes this implementation and reports on the performance 
measured from it 
categories and subject descriptors 
c computer-communication networks distributed 
systems 
general terms 
algorithms design experimentation 
 introduction 
search and rescue of people in emergency situation in a 
timely manner is an extremely important service it has 
been difficult to provide such a service due to lack of timely 
information needed to determine the current location of a 
person who may be in an emergency situation with the 
emergence of pervasive computing several systems 
 have been developed over the last few years 
that make use of small devices such as cell phones 
sensors etc all these systems require a connected network 
via satellites gsm base stations or mobile devices this 
requirement severely limits their applicability particularly 
in remote wilderness areas where maintaining a connected 
network is very difficult 
for example a gsm transmitter has to be in the range 
of a base station to transmit as a result it cannot operate 
in most wilderness areas while a satellite transmitter is 
the only viable solution in wilderness areas it is typically 
expensive and cumbersome furthermore a line of sight is 
required to transmit to satellite and that makes it 
infeasible to stay connected in narrow canyons large cities with 
skyscrapers rain forests or even when there is a roof or some 
other obstruction above the transmitter e g in a car an 
rf transmitter has a relatively smaller range of 
transmission so while an in-situ sensor is cheap as a single unit it is 
expensive to build a large network that can provide 
connectivity over a large wilderness area in a mobile environment 
where sensors are carried by moving people power-efficient 
routing is difficult to implement and maintain over a large 
wilderness area in fact building an adhoc sensor network 
using only the sensors worn by hikers is nearly impossible 
due to a relatively small number of sensors spread over a 
large wilderness area 
in this paper we describe the design implementation 
and evaluation of a search and rescue system called 
cenwits connection-less sensor-based tracking system 
using witnesses cenwits is comprised of mobile in-situ 
sensors that are worn by subjects people wild animals or 
in-animate objects access points ap that collect 
information from these sensors and gps receivers and location 
points lp that provide location information to the 
sensors a subject uses gps receivers when it can connect to 
a satellite and lps to determine its current location the 
key idea of cenwits is that it uses a concept of witnesses 
to convey a subject s movement and location information 
to the outside world this averts a need for maintaining a 
connected network to transmit location information to the 
outside world in particular there is no need for 
expensive gsm or satellite transmitters or maintaining an adhoc 
network of in-situ sensors in cenwits 
 
cenwits employs several important mechanisms to 
address the key problem of resource constraints low signal 
strength low power and limited memory in sensors in 
particular it makes a judicious use of the combined 
storage capability of sensors to filter organize and store 
important information combined battery power of sensors to 
ensure that the system remains operational for longer time 
periods and intermittent network connectivity to propagate 
information to a processing center 
the problem of low signal strengths short range rf 
communication is addressed by avoiding a need for maintaining 
a connected network instead cenwits propagates the 
location information of sensors using the concept of witnesses 
through an intermittently connected network as a result 
this system can be deployed in remote wilderness areas as 
well as in large urban areas with skyscrapers and other tall 
structures also this makes cenwits cost-effective a 
subject only needs to wear light-weight and low-cost sensors 
that have gps receivers but no expensive gsm or satellite 
transmitters furthermore since there is no need for a 
connected sensor network there is no need to deploy sensors in 
very large numbers 
the problem of limited battery life and limited memory of 
a sensor is addressed by incorporating the concepts of groups 
and partitions groups and partitions allow sensors to stay 
in sleep or receive modes most of the time using groups and 
partitions the location information collected by a sensor can 
be distributed among several sensors thereby reducing the 
amount of memory needed in one sensor to store that 
information in fact cenwits provides an adaptive tradeoff 
between memory and power consumption of sensors each 
sensor can dynamically adjust its power and memory 
consumption based on its remaining power or available memory 
it has amply been noted that the strength of sensor 
networks comes from the fact that several sensor nodes can 
be distributed over a relatively large area to construct a 
multihop network this paper demonstrates that important 
large-scale applications can be built using sensors by 
judiciously integrating the storage communication and 
computation capabilities of sensors the paper describes 
important techniques to combine memory transmission and 
battery power of many sensors to address resource constraints 
in the context of a search and rescue application however 
these techniques are quite general we discuss several other 
sensor-based applications that can employ these techniques 
while cenwits addresses the general location tracking 
and reporting problem in a wide-area network there are 
two important differences from the earlier work done in this 
area first unlike earlier location tracking solutions 
cenwits does not require a connected network second unlike 
earlier location tracking solutions cenwits does not aim for 
a very high accuracy of localization instead the main goal 
is to provide an approximate small area where search and 
rescue efforts can be concentrated 
the rest of this paper is organized as follows in section 
 we overview some of the recent projects and technologies 
related to movement and location tracking and search and 
rescue systems in section we describe the overall 
architecture of cenwits and provide a high-level description of 
its functionality in the next section section we discuss 
power and memory management in cenwits to simplify 
our presentation we will focus on a specific application of 
tracking lost injured hikers in all these sections in 
section we describe a prototype implementation of cenwits 
and present performance measured from this 
implementation we discuss how the ideas of cenwits can be used 
to build several other applications in section finally in 
section we discuss some related issues and conclude the 
paper 
 related work 
a survey of location systems for ubiquitous computing 
is provided in a location tracking system for adhoc 
sensor networks using anchor sensors as reference to gain 
location information and spread it out to outer node is 
proposed in most location tracking systems in adhoc 
sensor networks are for benefiting geographic-aware routing 
they don t fit well for our purposes the well-known 
active badge system lets a user carry a badge around 
an infrared sensor in the room can detect the presence of 
a badge and determine the location and identification of 
the person this is a useful system for indoor environment 
where gps doesn t work locationing using devices 
is probably the cheapest solution for indoor position 
tracking because of the popularity and low cost of 
devices several business solutions based on this technology 
have been developed 
a system that combines two mature technologies and is 
viable in suburban area where a user can see clear sky and 
has gsm cellular reception at the same time is currently 
available this system receives gps signal from a 
satellite and locates itself draws location on a map and sends 
location information through gsm network to the others 
who are interested in the user s location 
a very simple system to monitor children consists an rf 
transmitter and a receiver the system alarms the holder 
of the receiver when the transmitter is about to run out of 
range 
personal locater beacons plb has been used for avalanche 
rescuing for years a skier carries an rf transmitter that 
emits beacons periodically so that a rescue team can find 
his her location based on the strength of the rf signal 
luxury version of plb combines a gps receiver and a 
cospassarsat satellite transmitter that can transmit user s 
location in latitude and longitude to the rescue team whenever 
an accident happens however the device either is turned 
on all the time resulting in fast battery drain or must be 
turned on after the accident to function 
another related technology in widespread use today is the 
onstar system typically used in several luxury cars 
in this system a gps unit provides position information 
and a powerful transmitter relays that information via 
satellite to a customer service center designed for emergencies 
the system can be triggered either by the user with the push 
of a button or by a catastrophic accident once the system 
has been triggered a human representative attempts to gain 
communication with the user via a cell phone built as an 
incar device if contact cannot be made emergency services 
are dispatched to the location provided by gps like plbs 
this system has several limitations first it is heavy and 
expensive it requires a satellite transmitter and a connected 
network if connectivity with either the gps network or a 
communication satellite cannot be maintained the system 
fails unfortunately these are common obstacles 
encountered in deep canyons narrow streets in large cities parking 
garages and a number of other places 
 
the lifetch system uses gps receiver board combined 
with a gsm gprs transmitter and an rf transmitter in 
one wireless sensor node called intelligent communication 
unit icu an icu first attempts to transmit its location 
to a control center through gsm gprs network if that 
fails it connects with other icus adhoc network to 
forward its location information until the information reaches 
an icu that has gsm gprs reception this icu then 
transmits the location information of the original icu via 
the gsm gprs network 
zebranet is a system designed to study the moving 
patterns of zebras it utilizes two protocols history-based 
protocol and flooding protocol history-based protocol is 
used when the zebras are grazing and not moving around 
too much while this might be useful for tracking zebras 
it s not suitable for tracking hikers because two hikers are 
most likely to meet each other only once on a trail in 
the flooding protocol a node dumps its data to a neighbor 
whenever it finds one and doesn t delete its own copy until 
it finds a base station without considering routing loops 
packet filtering and grouping the size of data on a node will 
grow exponentially and drain the power and memory of a 
sensor node with in a short time instead cenwits uses a 
four-phase hand-shake protocol to ensure that a node 
transmits only as much information as the other node is willing 
to receive while zebranet is designed for a big group of 
sensors moving together in the same direction with same 
speed cenwits is designed to be used in the scenario where 
sensors move in different directions at different speeds 
delay tolerant network architecture addresses some 
important problems in challenged resource-constrained 
networks while this work is mainly concerned with 
interoperability of challenged networks some problems related 
to occasionally-connected networks are similar to the ones 
we have addressed in cenwits 
among all these systems luxury plb and lifetch are 
designed for location tracking in wilderness areas however 
both of these systems require a connected network luxury 
plb requires the user to transmit a signal to a satellite 
while lifetch requires connection to gsm gprs network 
luxury plb transmits location information only when an 
accident happens however if the user is buried in the snow 
or falls into a deep canyon there is almost no chance for the 
signal to go through and be relayed to the rescue team this 
is because satellite transmission needs line of sight 
furthermore since there is no known history of user s location it is 
not possible for the rescue team to infer the current location 
of the user another disadvantage of luxury plb is that a 
satellite transmitter is very expensive costing in the range 
of lifetch attempts to transmit the location 
information by gsm gprs and adhoc sensor network that uses 
aodv as the routing protocol however having a cellular 
reception in remote areas in wilderness areas e g 
american national parks is unlikely furthermore it is extremely 
unlikely that icus worn by hikers will be able to form an 
adhoc network in a large wilderness area this is because 
the hikers are mobile and it is very unlikely to have several 
icus placed dense enough to forward packets even on a very 
popular hike route 
cenwits is designed to address the limitations of systems 
such as luxury plb and lifetch it is designed to 
provide hikers skiers and climbers who have their activities 
mainly in wilderness areas a much higher chance to convey 
their location information to a control center it is not 
reliant upon constant connectivity with any communication 
medium rather it communicates information along from 
user to user finally arriving at a control center unlike 
several of the systems discussed so far it does not require that 
a user s unit is constantly turned on in fact it can discover 
a victim s location even if the victim s sensor was off at the 
time of accident and has remained off since then cenwits 
solves one of the greatest problems plaguing modern search 
and rescue systems it has an inherent on-site storage 
capability this means someone within the network will have 
access to the last-known-location information of a victim 
and perhaps his bearing and speed information as well 
figure hiker a and hiker b are are not in the 
range of each other 
 cenwits 
we describe cenwits in the context of locating lost injured 
hikers in wilderness areas each hiker wears a sensor mica 
motes in our prototype equipped with a gps receiver and 
an rf transmitter each sensor is assigned a unique id and 
maintains its current location based on the signal received by 
its gps receiver it also emits beacons periodically when 
any two sensors are in the range of one another they record 
the presence of each other witness information and also 
exchange the witness information they recorded earlier the 
key idea here is that if two sensors come with in range of 
each other at any time they become each other s witnesses 
later on if the hiker wearing one of these sensors is lost the 
other sensor can convey the last known witnessed location 
of the lost hiker furthermore by exchanging the witness 
information that each sensor recorded earlier the witness 
information is propagated beyond a direct contact between 
two sensors 
to convey witness information to a processing center or to 
a rescue team access points are established at well-known 
locations that the hikers are expected to pass through e g 
at the trail heads trail ends intersection of different trails 
scenic view points resting areas and so on whenever a 
sensor node is in the vicinity of an access point all witness 
information stored in that sensor is automatically dumped 
to the access point access points are connected to a 
processing center via satellite or some other network 
 the 
witness information is downloaded to the processing center 
from various access points at regular intervals in case 
connection to an access point is lost the information from that 
 
a connection is needed only between access points and a 
processing center there is no need for any connection 
between different access points 
 
access point can be downloaded manually e g by uavs 
to estimate the speed location and direction of a hiker at 
any point in time all witness information of that hiker that 
has been collected from various access points is processed 
figure hiker a and hiker b are in the range 
of each other a records the presence of b and b 
records the presence of a a and b become each 
other s witnesses 
figure hiker a is in the range of an access 
point it uploads its recorded witness information 
and clears its memory 
an example of how cenwits operates is illustrated in 
figures and first hikers a and b are on two close 
trails but out of range of each other figure this is 
a very common scenario during a hike for example on a 
popular four-hour hike a hiker might run into as many as 
 other hikers this accounts for one encounter every 
minutes on average a slow hiker can go mile feet 
per hour thus in minutes a slow hiker can go as far as 
 feet this implies that if we were to put hikers on a 
 -hour one-way hike evenly the range of each sensor node 
should be at least feet for them to communicate with 
one another continuously the signal strength starts 
dropping rapidly for two mica nodes to communicate with each 
other when they are feet away and is completely lost 
when they are feet away from each other so for the 
sensors to form a sensor network on a -hour hiking trail 
there should be at least hikers scattered evenly clearly 
this is extremely unlikely in fact in a -hour less-popular 
hiking trail one might only run into say five other hikers 
cenwits takes advantage of the fact that sensors can 
communicate with one another and record their presence given 
a walking speed of one mile per hour feet per minute 
and mica range of about feet for non-line-of-sight radio 
transmission two hikers have about minutes to 
discover the presence of each other and exchange their 
witness information we therefore design our system to have 
each sensor emit a beacon every one-and-a-half minute in 
figure hiker b s sensor emits a beacon when a is in range 
this triggers a to exchange data with b a communicates 
the following information to b my id is a i saw c at 
pm at ◦ 
 ◦ 
 i saw e at 
pm at ◦ 
 ◦ 
 b then replies 
with my id is b i saw k at am at ◦ 
 
 ◦ 
 in addition a records i saw b at 
pm at ◦ 
 ◦ 
 and b records i 
saw a at pm at ◦ 
 ◦ 
 
b goes on his way to overnight camping while a heads 
back to trail head where there is an ap which emits beacon 
every seconds to avoid missing any hiker a dumps all 
witness information it has collected to the access point this 
is shown in figure 
 witness information storage 
a critical concern is that there is limited amount of 
memory available on motes kb sdram memory kb 
flash memory and - kb eeprom so it is important 
to organize witness information efficiently cenwits stores 
witness information at each node as a set of witness records 
 format is shown in figure 
 b 
node id record time x y location time hop count 
 b b b b 
figure format of a witness record 
when two nodes i and j encounter each other each node 
generates a new witness record in the witness record 
generated by i node id is j record time is the current time 
in i s clock x y are the coordinates of the location of i 
that i recorded most recently either from satellite or an 
lp location time is the time when the this location was 
recorded and hop count is 
each node is assigned a unique node id when it enters a 
trail in our current prototype we have allocated one byte 
for node id although this can be increased to two or more 
bytes if a large number of hikers are expected to be present 
at the same time we can represent time in bits to a 
second precision so we have allocated bytes each for record 
time and location time the circumference of the earth 
is approximately km if we use a -bit number to 
represent both longitude and latitude the precision we get 
is 
 meter inches which is 
quite precise for our needs so we have allocated bytes 
each for x and y coordinates of the location of a node in 
fact a foot precision can be achieved by using only bits 
 location point and location inference 
although a gps receiver provides an accurate location 
information it has it s limitation in canyons and rainy 
forests a gps receiver does not work when there is a 
heavy cloud cover gps users have experienced inaccuracy 
in the reported location as well unfortunately a lot of 
hiking trails are in dense forests and canyons and it s not that 
uncommon to rain after hikers start hiking to address this 
cenwits incorporates the idea of location points lp a 
location point can update a sensor node with its current 
location whenever the node is near that lp lps are placed at 
different locations in a wilderness area where gps receivers 
don t work an lp is a very simple device that emits 
prerecorded location information at some regular time interval 
it can be placed in difficult-to-reach places such as deep 
canyons and dense rain forests by simply dropping them 
from an airplane lps allow a sensor node to determine 
its current location more accurately however they are not 
 
an essential requirement of cenwits if an lp runs out of 
power the cenwits will continue to work correctly 
figure gps receiver not working correctly 
sensors then have to rely on lp to provide coordination 
in figure b cannot get gps reception due to bad 
weather it then runs into a on the trail who doesn t have 
gps reception either their sensors record the presence of 
each other after minutes a is in range of an lp that 
provides an accurate location information to a when a 
returns to trail head and uploads its data figure the 
system can draw a circle centered at the lp from which a 
fetched location information for the range of encounter 
location of a and b by overlapping this circle with the trail 
map two or three possible location of encounter can be 
inferred thus when a rescue is required the possible location 
of b can be better inferred see figures and 
figure a is back to trail head it reports the 
time of encounter with b to ap but no location 
information to ap 
figure b is still missing after sunset cenwits 
infers the last contact point and draws the circle of 
possible current locations based on average hiking 
speed 
cenwits requires that the clocks of different sensor nodes 
be loosely synchronized with one another such a 
synchronization is trivial when gps coverage is available in 
addition sensor nodes in cenwits synchronize their clocks 
whenever they are in the range of an ap or an lp the 
figure based on overlapping landscape b might 
have hiked to wrong branch and fallen off a cliff hot 
rescue areas can thus be determined 
synchronization accuracy cenwits needs is of the order of a 
second or so as long as the clocks are synchronized with in 
one second range whether a met b at or 
doesn t matter in the ordering of witness events and 
inferring the path 
 memory and power management 
cenwits employs several important mechanisms to 
conserve power and memory it is important to note while 
current sensor nodes have limited amount of memory 
future sensor nodes are expected to have much more memory 
with this in mind the main focus in our design is to 
provide a tradeoff between the amount of memory available and 
amount of power consumption 
 memory management 
the size of witness information stored at a node can get 
very large this is because the node may come across several 
other nodes during a hike and may end up accumulating a 
large amount of witness information over time to address 
this problem cenwits allows a node to pro-actively free up 
some parts of its memory periodically this raises an 
interesting question of when and which witness record should be 
deleted from the memory of a node cenwits uses three 
criteria to determine this record count hop count and record 
gap 
record count refers to the number of witness records with 
same node id that a node has stored in its memory a node 
maintains an integer parameter max record count 
it stores at most max record count witness records 
of any node 
every witness record has a hop count field that stores the 
number times hops this record has been transferred since 
being created initially this field is set to whenever a 
node receives a witness record from another node it 
increments the hop count of that record by a node maintains 
an integer parameter called max hop count it keeps 
only those witness records in its memory whose hop count 
is less than max hop count the max hop count 
parameter provides a balance between two conflicting goals 
 to ensure that a witness record has been propagated to 
and thus stored at as many nodes as possible so that it has 
a high probability of being dumped at some ap as quickly 
as possible and to ensure that a witness record is stored 
only at a few nodes so that it does not clog up too much of 
the combined memory of all sensor nodes we chose to use 
hop count instead of time-to-live to decide when to drop a 
packet the main reason for this is that the probability of 
a packet reaching an ap goes up as the hop count adds up 
for example when the hop count if for a specific record 
 
the record is in at least sensor nodes on the other hand 
if we discard old records without considering hop count 
there is no guarantee that the record is present in any other 
sensor node 
record gap refers to the time difference between the record 
times of two witness records with the same node id to 
save memory a node n ensures the the record gap between 
any two witness records with the same node id is at least 
min record gap for each node id i n stores the 
witness record with the most recent record time rti the witness 
with most recent record time that is at least min record gap 
time units before rti and so on until the record count limit 
 max record count is reached 
when a node is tight in memory it adjusts the three 
parameters max record count max hop count and 
min record gap to free up some memory it 
decrements max record count and max hop count 
and increments min record gap it then first erases 
all witness records whose hop count exceeds the reduced 
max hop count value and then erases witness records 
to satisfy the record gap criteria also when a node has 
extra memory space available e g after dumping its witness 
information at an access point it resets max record count 
max hop count and min record gap to some 
predefined values 
 power management 
an important advantage of using sensors for tracking 
purposes is that we can regulate the behavior of a sensor node 
based on current conditions for example we mentioned 
earlier that a sensor should emit a beacon every minute 
given a hiking speed of mile hour however if a user is 
moving feet sec a beacon should be emitted every 
seconds if a user is not moving at all a beacon can be 
emitted every minutes in the night a sensor can be put 
into sleep mode to save energy when a user is not likely to 
move at all for a relatively longer period of time if a user 
is active for only eight hours in a day we can put the sensor 
into sleep mode for the other hours and thus save rd 
of the energy 
in addition a sensor node can choose to not send any 
beacons during some time intervals for example suppose 
hiker a has communicated its witness information to three 
other hikers in the last five minutes if it is running low 
on power it can go to receive mode or sleep mode for the 
next ten minutes it goes to receive mode if it is still willing 
to receive additional witness information from hikers that it 
encounters in the next ten minutes it goes to sleep mode if 
it is extremely low on power 
the bandwidth and energy limitations of sensor nodes 
require that the amount of data transferred among the nodes 
be reduced to minimum it has been observed that in some 
scenarios instructions could be executed for the same 
energy cost of sending a bit m by radio to reduce 
the amount of data transfer cenwits employs a handshake 
protocol that two nodes execute when they encounter one 
another the goal of this protocol is to ensure that a node 
transmits only as much witness information as the other 
node is willing to receive this protocol is initiated when 
a node i receives a beacon containing the node id of the 
sender node j and i has not exchanged witness information 
with j in the last δ time units assume that i j the 
protocol consists of four phases see figure 
 phase i node i sends its receive constraints and the 
number of witness records it has in its memory 
 phase ii on receiving this message from i j sends its 
receive constraints and the number of witness records 
it has in its memory 
 phase iii on receiving the above message from j i 
sends its witness information filtered based on receive 
constraints received in phase ii 
 phase iv after receiving the witness records from 
i j sends its witness information filtered based on 
receive constraints received in phase i 
j 
 constaints witness info size 
 constaints witness info size 
 filtered witness info 
 filtered witness info 
i j 
j 
j 
i 
i 
i 
figure four-phase hand shake protocol i j 
receive constraints are a function of memory and power 
in the most general case they are comprised of the three 
parameters record count hop count and record gap used 
for memory management if i is low on memory it specifies 
the maximum number of records it is willing to accept from 
j similarly i can ask j to send only those records that 
have hop count value less than max hop count − 
finally i can include its min record gap value in its 
receive constraints note that the handshake protocol is 
beneficial to both i and j they save memory by receiving 
only as much information as they are willing to accept and 
conserve energy by sending only as many witness records as 
needed 
it turns out that filtering witness records based on 
min record gap is complex it requires that the 
witness records of any given node be arranged in an order sorted 
by their record time values maintaining this sorted order is 
complex in memory because new witness records with the 
same node id can arrive later that may have to be inserted 
in between to preserve the sorted order for this reason the 
receive constraints in the current cenwits prototype do not 
include record gap 
suppose i specifies a hop count value of in this case 
j checks the hop count field of every witness record before 
sending them if the hop count value is greater than the 
record is not transmitted 
 groups and partitions 
to further reduce communication and increase the 
lifetime of our system we introduce the notion of groups the 
idea is based on the concept of abstract regions presented 
in a group is a set of n nodes that can be defined 
in terms of radio connectivity geographic location or other 
properties of nodes all nodes within a group can 
communicate directly with one another and they share information 
to maintain their view of the external world at any point 
in time a group has exactly one leader that communicates 
 
with external nodes on behalf of the entire group a group 
can be static meaning that the group membership does not 
change over the period of time or it could be dynamic in 
which case nodes can leave or join the group to make our 
analysis simple and to explain the advantages of group we 
first discuss static groups 
a static group is formed at the start of a hiking trail or ski 
slope suppose there are five family members who want to 
go for a hike in the rocky mountain national park before 
these members start their hike each one of them is given 
a sensor node and the information is entered in the system 
that the five nodes form a group each group member is 
given a unique id and every group member knows about 
other members of the group the group as a whole is also 
assigned an id to distinguish it from other groups in the 
system 
figure a group of five people node is the 
group leader and it is communicating on behalf of 
the group with an external node all other 
 shown in a lighter shade are in sleep mode 
as the group moves through the trail it exchanges 
information with other nodes or groups that it comes across at 
any point in time only one group member called the leader 
sends and receives information on behalf of the group and 
all other n − group members are put in the sleep mode 
 see figure it is this property of groups that saves 
us energy the group leadership is time-multiplexed among 
the group members this is done to make sure that a single 
node does not run out of battery due to continuous exchange 
of information thus after every t seconds the leadership 
is passed on to another node called the successor and the 
leader now an ordinary member is put to sleep since 
energy is dear we do not implement an extensive election 
algorithm for choosing the successor instead we choose the 
successor on the basis of node id the node with the next 
highest id in the group is chosen as the successor the last 
node of course chooses the node with the lowest id as its 
successor 
we now discuss the data storage schemes for groups 
memory is a scarce resource in sensor nodes and it is therefore 
important that witness information be stored efficiently among 
group members efficient data storage is not a trivial task 
when it comes to groups the tradeoff is between simplicity 
of the scheme and memory savings a simpler scheme 
incurs lesser energy cost as compared to a more sophisticated 
scheme but offers lesser memory savings as well this is 
because in a more complicated scheme the group members 
have to coordinate to update and store information after 
considering a number of different schemes we have come 
to a conclusion that there is no optimal storage scheme for 
groups the system should be able to adapt according to 
its requirements if group members are low on battery then 
the group can adapt a scheme that is more energy efficient 
similarly if the group members are running out of memory 
they can adapt a scheme that is more memory efficient we 
first present a simple scheme that is very energy efficient but 
does not offer significant memory savings we then present 
an alternate scheme that is much more memory efficient 
as already mentioned a group can receive information 
only through the group leader whenever the leader comes 
across an external node e it receives information from that 
node and saves it in our first scheme when the timeslot for 
the leader expires the leader passes this new information it 
received from e to its successor this is important because 
during the next time slot if the new leader comes across 
another external node it should be able to pass 
information about all the external nodes this group has witnessed 
so far thus the information is fully replicated on all nodes 
to maintain the correct view of the world 
our first scheme does not offer any memory savings but is 
highly energy efficient and may be a good choice when the 
group members are running low on battery except for the 
time when the leadership is switched all n − members are 
asleep at any given time this means that a single member 
is up for t seconds once every n∗t seconds and therefore has 
to spend approximately only nth 
of its energy thus if 
there are members in a group we save energy which 
is huge more energy can be saved by increasing the group 
size 
we now present an alternate data storage scheme that 
aims at saving memory at the cost of energy in this scheme 
we divide the group into what we call partitions partitions 
can be thought of as subgroups within a group each 
partition must have at least two nodes in it the nodes within a 
partition are called peers each partition has one peer 
designated as partition leader the partition leader stays in 
receive mode at all times while all others peers a partition stay 
in the sleep mode partition leadership is time-multiplexed 
among the peers to make sure that a single node does not 
run out of battery like before a group has exactly one 
leader and the leadership is time-multiplexed among 
partitions the group leader also serves as the partition leader 
for the partition it belongs to see figure 
in this scheme all partition leaders participate in 
information exchange whenever a group comes across an external 
node e every partition leader receives all witness 
information but it only stores a subset of that information after 
filtering information is filtered in such a way that each 
partition leader has to store only b k bytes of data where 
k is the number of partitions and b is the total number 
of bytes received from e similarly when a group wants to 
send witness information to e each partition leader sends 
only b k bytes that are stored in the partition it belongs 
to however before a partition leader can send information 
it must switch from receive mode to send mode also 
partition leaders must coordinate with one another to ensure 
that they do not send their witness information at the same 
time i e their message do not collide all this is achieved 
by having the group leader send a signal to every partition 
leader in turn 
 
figure the figure shows a group of eight nodes 
divided into four partitions of nodes each node 
 is the group leader whereas nodes and are 
partition leaders all other nodes are in the sleep 
mode 
since the partition leadership is time-multiplexed it is 
important that any information received by the partition 
leader p be passed on to the next leader p this has 
to be done to make sure that p has all the information 
that it might need to send when it comes across another 
external node during its timeslot one way of achieving this 
is to wake p up just before p s timeslot expires and then 
have p transfer information only to p an alternate is to 
wake all the peers up at the time of leadership change and 
then have p broadcast the information to all peers each 
peer saves the information sent by p and then goes back 
to sleep in both cases the peers send acknowledgement to 
the partition leader after receiving the information in the 
former method only one node needs to wake up at the time 
of leadership change but the amount of information that 
has to be transmitted between the nodes increases as time 
passes in the latter case all nodes have to be woken up at 
the time of leadership change but small piece of information 
has to be transmitted each time among the peers since 
communication is much more expensive than bringing the 
nodes up we prefer the second method over the first one 
a group can be divided into partitions in more than one 
way for example suppose we have a group of six members 
we can divide this group into three partitions of two peers 
each or two partitions with three peers each the choice 
once again depends on the requirements of the system a 
few big partitions will make the system more energy efficient 
this is because in this configuration a greater number of 
nodes will stay in sleep mode at any given point in time 
on the other hand several small partitions will make the 
system memory efficient since each node will have to store 
lesser information see figure 
a group that is divided into partitions must be able to 
readjust itself when a node leaves or runs out of battery 
this is crucial because a partition must have at least two 
nodes at any point in time to tolerate failure of one node 
for example in figure a if node or node dies the 
partition is left with only one node later on if that single 
node in the partition dies all witness information stored in 
that partition will be lost we have devised a very simple 
protocol to solve this problem we first explain how 
partifigure the figure shows two different ways of 
partitioning a group of six nodes in a a group 
is divided into three partitions of two nodes node 
 is the group leader nodes and are partition 
leaders and nodes and are in sleep mode in 
 b the group is divided into two partitions of three 
nodes node is the group leader node is the 
partition leader and nodes and are in sleep 
mode 
tions are adjusted when a peer dies and then explain what 
happens if a partition leader dies 
suppose node in figure a dies when node the 
partition leader sends information to node it does not 
receive an acknowledgement from it and concludes that node 
 has died 
 at this point node contacts other partition 
leaders nodes using a broadcast message and 
informs them that one of its peers has died upon hearing 
this each partition leader informs node i the number of 
nodes in its partition ii a candidate node that node can 
take if the number of nodes in its partition is greater than 
 and iii the amount of witness information stored in its 
partition upon hearing from every leader node chooses 
the candidate node from the partition with maximum 
number must be greater than of peers and sends a message 
back to all leaders node then sends data to its new peer 
to make sure that the information is replicated within the 
partition 
however if all partitions have exactly two nodes then 
node must join another partition it chooses the partition 
that has the least amount of witness information to join it 
sends its witness information to the new partition leader 
witness information and membership update is propagated 
to all peers during the next partition leadership change 
we now consider the case where the partition leader dies 
if this happens then we wait for the partition leadership to 
change and for the new partition leader to eventually find 
out that a peer has died once the new partition leader finds 
out that it needs more peers it proceeds with the protocol 
explained above however in this case we do lose 
information that the previous partition leader might have received 
just before it died this problem can be solved by 
implementing a more rigorous protocol but we have decided to 
give up on accuracy to save energy 
our current design uses time-division multiplexing to 
schedule wakeup and sleep modes in the sensor nodes however 
recent work on radio wakeup sensors can be used to do 
this scheduling more efficiently we plan to incorporate radio 
wakeup sensors in cenwits when the hardware is mature 
 
the algorithm to conclude that a node has died can be 
made more rigorous by having the partition leader query 
the suspected node a few times 
 
 system evaluation 
a sensor is constrained in the amount of memory and 
power in general the amount of memory needed and power 
consumption depends on a variety of factors such as node 
density number of hiker encounters and the number of 
access points in this section we provide an estimate of how 
long the power of a mica mote will last under certain 
assumtions 
first we assume that each sensor node carries about 
witness records on encountering another hiker a sensor 
node transmits witness records and receives new 
witness records since each record is bytes long it will take 
 seconds to transmit records and another 
seconds to receive records over a bps link the power 
consumption of mica due to cpu processing 
transmission and reception are approximately ma ma and 
 ma per hour respectively and the capacity of an 
alkaline battery is mah 
since the radio module of mica is half-duplex and 
assuming that the cpu is always active when a node is awake 
power consumption due to transmission is 
ma per hour and due to reception is ma per 
hour so average power consumtion due to transmission 
and reception is ma per hour 
given that the capacity of an alkaline battery is 
mah a battery should last for hours of 
transmission and reception an encounter between two 
hikers results in exchange of about witness records that takes 
about seconds as calculated above thus a single 
alkaline battery can last for ∗ ∗ hiker 
encounters 
assuming that a node emits a beacon every seconds 
and a hiker encounter occurs everytime a beacon is emitted 
 worst-case scenario a single alkaline battery will last for 
 ∗ ∗ ∗ ∗ days since a mica 
is equipped with two batteries a mica sensor can remain 
operation for about two months notice that this 
calculation is preliminary because it assumes that hikers are active 
 hours of the day and a hiker encounter occurs every 
seconds in a more realistic scenario power is expected to 
last for a much longer time period also this time period 
will significantly increase when groups of hikers are moving 
together 
finally the lifetime of a sensor running on two 
batteries can definitely be increased significantly by using energy 
scavenging techniques and energy harvesting techniques 
 
 prototype implementation 
we have implemented a prototype of cenwits on mica 
sensor mhz running mantis os b one of the sensor 
is equipped with mts ca gps module which is capable 
of barometric pressure and two-axis acceleration sensing in 
addition to gps location tracking we use sirf the serial 
communication protocol to control gps module sirf has 
a rich command set but we record only x and y coordinates 
a witness record is bytes long when a node starts up it 
stores its current location and emits a beacon 
periodicallyin the prototype a node emits a beacon every minute 
we have conducted a number of experiments with this 
prototype a detailed report on these experiments with the 
raw data collected and photographs of hikers access points 
etc is available at http csel cs colorado edu ∼huangjh 
cenwits index htm here we report results from three of 
them in all these experiments there are three access points 
 a b and c where nodes dump their witness information 
these access points also provide location information to the 
nodes that come with in their range we first show how 
cenwits can be used to determine the hiking trail a hiker is most 
likely on and the speed at which he is hiking and identify 
hot search areas in case he is reported missing next we 
show the results of power and memory management 
techniques of cenwits in conserving power and memory of a 
sensor node in one of our experiments 
 locating lost hikers 
the first experiment is called direct contact it is a very 
simple experiment in which a single hiker starts from a 
goes to b and then c and finally returns to a see figure 
 the goal of this experiment is to illustrate that 
cenwits can deduce the trail a hiker takes by processing witness 
information 
figure direct contact experiment 
node id record x y location hop 
time time count 
 
 
 
 
 
 
 
table witness information collected in the direct 
contact experiment 
the witness information dumped at the three access points 
was then collected and processed at a control center part 
of the witness information collected at the control center is 
shown in table the x y locations in this table 
correspond to the location information provided by access points 
a b and c a is located at b is located at 
and c is located at three encounter points 
 between hiker and the three access points extracted from 
 
this witness information are shown in figure shown in 
rectangular boxes for example a at means came in 
contact with a at time using this information we can 
infer the direction in which hiker was moving and speed at 
which he was moving furthermore given a map of hiking 
trails in this area it is clearly possible to identify the hiking 
trail that hiker took 
the second experiment is called indirect inference this 
experiment is designed to illustrate that the location 
direction and speed of a hiker can be inferred by cenwits even 
if the hiker never comes in the range of any access point it 
illustrates the importance of witness information in search 
and rescue applications in this experiment there are three 
hikers and hiker takes a trail that goes along 
access points a and b while hiker takes trail that goes along 
access points c and b hiker takes a trail that does not 
come in the range of any access points however this hiker 
meets hiker and during his hike this is illustrated in 
figure 
figure indirect inference experiment 
node id record x y location hop 
time time count 
 
 
 
 
 
 
table witness information collected from hiker 
in indirect inference experiment 
part of the witness information collected at the control 
center from access points a b and c is shown in tables 
 and there are some interesting data in these tables 
for example the location time in some witness records is 
not the same as the record time this means that the node 
that generated that record did not have its most up-to-date 
location at the encounter time for example when hikers 
 and meet at time the last recorded location time of 
node id record x y location hop 
time time count 
 
 
 
 
 
 
 
 
table witness information collected from hiker 
in indirect inference experiment 
hiker is recorded at time so node generates 
a witness record with record time location and 
location time in fact the last two records in table 
have as their location this has happened because 
these witness records were generate by hiker during his 
encounter with at time and until this time hiker 
 hadn t come in contact with any location points 
interestingly a more accurate location information of 
and encounter or and encounter can be computed by 
process the witness information at the control center it 
took units of time for hiker to go from a to b 
 assuming a constant hiking speed and a relatively 
straight-line hike it can be computed that at time hiker 
 must have been at location thus is a more 
accurate location of encounter between and 
finally our third experiment called identifying hot search 
areas is designed to determine the trail a hiker has taken 
and identify hot search areas for rescue after he is reported 
missing there are six hikers and in this 
experiment figure shows the trails that hikers 
 and took along with the encounter points obtained 
from witness records collected at the control center for 
brevity we have not shown the entire witness information 
collected at the control center this information is available 
at http csel cs colorado edu ∼huangjh cenwits index htm 
figure identifying hot search area experiment 
 without hiker 
 
now suppose hiker is reported missing at time to 
determine the hot search areas the witness records of hiker 
 are processed to determine the trail he is most likely on 
the speed at which he had been moving direction in which 
he had been moving and his last known location based on 
this information and the hiking trail map hot search areas 
are identified the hiking trail taken by hiker as inferred 
by cenwits is shown by a dotted line and the hot search 
areas identified by cenwits are shown by dark lines inside 
the dotted circle in figure 
figure identifying hot search area experiment 
 with hiker 
 results of power and memory 
management 
the witness information shown in tables and has 
not been filtered using the three criteria described in 
section for example the witness records generated by at 
record times and see table have all been 
generated due a single contact between access point c and node 
 by applying the record gap criteria two of these three 
records will be erased similarly the witness records 
generated by at record times and see table have 
all been generated due a single contact between access point 
a and node again by applying the record gap criteria 
two of these three records will be erased our experiments 
did not generate enough data to test the impact of record 
count or hop count criteria 
to evaluate the impact of these criteria we simulated 
cenwits to generate a significantly large number of records for a 
given number of hikers and access points we generated 
witness records by having the hikers walk randomly we applied 
the three criteria to measure the amount of memory savings 
in a sensor node the results are shown in table the 
number of hikers in this simulation was and the number 
of access points was the number of witness records 
reported in this table is an average number of witness records 
a sensor node stored at the time of dump to an access point 
these results show that the three memory management 
criteria significantly reduces the memory consumption of 
sensor nodes in cenwits for example they can reduce 
max min max of 
record record hop witness 
count gap count records 
 
 
 
 
 
 
 
 
table impact of memory management techniques 
the memory consumption by up to however these 
results are premature at present for two reasons they 
are generated via simulation of hikers walking at random 
and it is not clear what impact the erasing of witness 
records has on the accuracy of inferred location hot search 
areas of lost hikers in our future work we plan to undertake 
a major study to address these two concerns 
 other applications 
in addition to the hiking in wilderness areas cenwits can 
be used in several other applications e g skiing climbing 
wild life monitoring and person tracking since cenwits 
relies only on intermittent connectivity it can take advantage 
of the existing cheap and mature technologies and thereby 
make tracking cheaper and fairly accurate since cenwits 
doesn t rely on keeping track of a sensor holder all time 
but relies on maintaining witnesses the system is relatively 
cheaper and widely applicable for example there are some 
dangerous cliffs in most ski resorts but it is too expensive 
for a ski resort to deploy a connected wireless sensor network 
through out the mountain using cenwits we can deploy 
some sensors at the cliff boundaries these boundary 
sensors emit beacons quite frequently e g every second and 
so can record presence of skiers who cross the boundary and 
fall off the cliff ski patrols can cruise the mountains every 
hour and automatically query the boundary sensor when in 
range using pdas if a pda shows that a skier has been 
close to the boundary sensor the ski patrol can use a long 
range walkie-talkie to query control center at the resort base 
to check the witness record of the skier if there is no 
witness record after the recorded time in the boundary sensor 
there is a high chance that a rescue is needed 
in wildlife monitoring a very popular method is to attach 
a gps receiver on the animals to collect data either a 
satellite transmitter is used or the data collector has to 
wait until the gps receiver brace falls off after a year or so 
and then search for the gps receiver gps transmitters are 
very expensive e g the one used in geese tracking is 
each also it is not yet known if continuous radio signal 
is harmful to the birds furthermore a gps transmitter is 
quite bulky and uncomfortable and as a result birds always 
try to get rid of it using cenwits not only can we record 
the presence of wildlife we can also record the behavior 
of wild animals e g lions might follow the migration of 
deers cenwits does nor require any bulky and expensive 
satellite transmitters nor is there a need to wait for a year 
and search for the braces cenwits provides a very simple 
and cost-effective solution in this case also access points 
 
can be strategically located e g near a water source to 
increase chances of collecting up-to-date data in fact the 
access points need not be statically located they can be 
placed in a low-altitude plane e g a uav and be flown over 
a wilderness area to collect data from wildlife 
in large cities cenwits can be used to complement gps 
since gps doesn t work indoor and near skyscrapers if a 
person a is reported missing and from the witness records 
we find that his last contacts were c and d we can trace 
an approximate location quickly and quite efficiently 
 discussion and future work 
this paper presents a new search and rescue system called 
cenwits that has several advantages over the current search 
and rescue systems these advantages include a 
looselycoupled system that relies only on intermittent network 
connectivity power and storage efficiency and low cost it 
solves one of the greatest problems plaguing modern search 
and rescue systems it has an inherent on-site storage 
capability this means someone within the network will have 
access to the last-known-location information of a victim 
and perhaps his bearing and speed information as well it 
utilizes the concept of witnesses to propagate information 
infer current possible location and speed of a subject and 
identify hot search and rescue areas in case of emergencies 
a large part of cenwits design focuses on addressing the 
power and memory limitations of current sensor nodes in 
fact power and memory constraints depend on how much 
weight of sensor node a hiker is willing to carry and the 
cost of these sensors an important goal of cenwits is build 
small chips that can be implanted in hiking boots or ski 
jackets this goal is similar to the avalanche beacons that 
are currently implanted in ski jackets we anticipate that 
power and memory will continue to be constrained in such 
an environment 
while the paper focuses on the development of a search 
and rescue system it also provides some innovative 
systemlevel ideas for information processing in a sensor network 
system 
we have developed and experimented with a basic 
prototype of cenwits at present future work includes 
developing a more mature prototype addressing important issues 
such as security privacy and high availability there are 
several pressing concerns regarding security privacy and 
high availability in cenwits for example an adversary 
can sniff the witness information to locate endangered 
animals females children etc he may inject false information 
in the system an individual may not be comfortable with 
providing his her location and movement information even 
though he she is definitely interested in being located in a 
timely manner at the time of emergency in general people 
in hiking community are friendly and usually trustworthy 
so a bullet-proof security is not really required however 
when cenwits is used in the context of other applications 
security requirements may change since the sensor nodes 
used in cenwits are fragile they can fail in fact the nature 
and level of security privacy and high availability support 
needed in cenwits strongly depends on the application for 
which it is being used and the individual subjects involved 
accordingly we plan to design a multi-level support for 
security privacy and high availability in cenwits 
so far we have experimented with cenwits in a very 
restricted environment with a small number of sensors our 
next goal is to deploy this system in a much larger and more 
realistic environment in particular discussions are currenly 
underway to deploy cenwits in the rocky mountain and 
yosemite national parks 
 references 
 -based tracking system 
http www pangonetworks com locator htm 
 brent geese http www wwt org uk brent 
 the onstar system http www onstar com 
 personal locator beacons with gps receiver and 
satellite transmitter http www aeromedix com 
 personal tracking using gps and gsm system 
http www ulocate com trimtrac html 
 rf based kid tracking system 
http www ion-kids com 
 f alessio performance measurements with motes 
technology mswim 
 p bahl and v n padmanabhan radar an 
in-building rf-based user location and tracking 
system ieee infocom 
 k fall a delay-tolerant network architecture for 
challenged internets in sigcomm 
 l gu and j stankovic radio triggered wake-up 
capability for sensor networks in real-time 
applications symposium 
 j hightower and g borriello location systems for 
ubiquitous computing ieee computer 
 w jaskowski k jedrzejek b nyczkowski and 
s skowronek lifetch life saving system csidc 
 p juang h oki y wang m martonosi l peh 
and d rubenstein energy-efficient computing for 
wildlife tracking design tradeoffs and early 
experiences with zebranet in asplos 
 k kansal and m srivastava energy harvesting aware 
power management in wireless sensor networks a 
systems perspective 
 g j pottie and w j kaiser embedding the 
internet wireless integrated network sensors 
communications of the acm may 
 s roundy p k wright and j rabaey a study of 
low-level vibrations as a power source for wireless 
sensor networks computer communications 
 
 c savarese j m rabaey and j beutel locationing 
in distributed ad-hoc wireless sensor networks 
icassp 
 v shnayder m hempstead b chen g allen and 
m welsh simulating the power consumption of 
large-scale sensor network applications in sensys 
 
 r want and a hopper active badges and personal 
interactive computing objects ieee transactions of 
consumer electronics 
 m welsh and g mainland programming sensor 
networks using abstract regions first usenix acm 
symposium on networked systems design and 
implementation nsdi 
 
msp multi-sequence positioning of wireless sensor nodes∗ 
ziguo zhong 
computer science and engineering 
university of minnesota 
zhong cs umn edu 
tian he 
computer science and engineering 
university of minnesota 
tianhe cs umn edu 
abstract 
wireless sensor networks have been proposed for use in 
many location-dependent applications most of these need to 
identify the locations of wireless sensor nodes a challenging 
task because of the severe constraints on cost energy and 
effective range of sensor devices to overcome limitations in 
existing solutions we present a multi-sequence positioning msp 
method for large-scale stationary sensor node localization in 
outdoor environments the novel idea behind msp is to 
reconstruct and estimate two-dimensional location information for 
each sensor node by processing multiple one-dimensional node 
sequences easily obtained through loosely guided event 
distribution starting from a basic msp design we propose four 
optimizations which work together to increase the localization 
accuracy we address several interesting issues such as 
incomplete partial node sequences and sequence flip found in the 
mirage test-bed we built we have evaluated the msp system 
through theoretical analysis extensive simulation as well as 
two physical systems an indoor version with micaz motes 
and an outdoor version with micaz motes this 
evaluation demonstrates that msp can achieve an accuracy within 
one foot requiring neither additional costly hardware on 
sensor nodes nor precise event distribution it also provides a nice 
tradeoff between physical cost anchors and soft cost events 
while maintaining localization accuracy 
categories and subject descriptors 
c computer communications networks 
distributed systems 
general terms 
algorithms measurement design performance 
experimentation 
 introduction 
although wireless sensor networks wsn have shown 
promising prospects in various applications researchers 
still face several challenges for massive deployment of such 
networks one of these is to identify the location of 
individual sensor nodes in outdoor environments because of 
unpredictable flow dynamics in airborne scenarios it is not currently 
feasible to localize sensor nodes during massive uva-based 
deployment on the other hand geometric information is 
indispensable in these networks since users need to know where 
events of interest occur e g the location of intruders or of a 
bomb explosion 
previous research on node localization falls into two 
categories range-based approaches and range-free approaches 
range-based approaches compute per-node 
location information iteratively or recursively based on 
measured distances among target nodes and a few anchors which 
precisely know their locations these approaches generally 
require costly hardware e g gps and have limited 
effective range due to energy constraints e g ultrasound-based 
tdoa although range-based solutions can be 
suitably used in small-scale indoor environments they are 
considered less cost-effective for large-scale deployments on the 
other hand range-free approaches do not 
require accurate distance measurements but localize the node 
based on network connectivity proximity information 
unfortunately since wireless connectivity is highly influenced by the 
environment and hardware calibration existing solutions fail 
to deliver encouraging empirical results or require substantial 
survey and calibration on a case-by-case basis 
realizing the impracticality of existing solutions for the 
large-scale outdoor environment researchers have recently 
proposed solutions e g spotlight and lighthouse 
for sensor node localization using the spatiotemporal 
correlation of controlled events i e inferring nodes locations based 
on the detection time of controlled events these solutions 
demonstrate that long range and high accuracy localization can 
be achieved simultaneously with little additional cost at 
sensor nodes these benefits however come along with an 
implicit assumption that the controlled events can be precisely 
distributed to a specified location at a specified time we argue 
that precise event distribution is difficult to achieve especially 
at large scale when terrain is uneven the event distribution 
device is not well calibrated and its position is difficult to 
maintain e g the helicopter-mounted scenario in 
to address these limitations in current approaches in this 
paper we present a multi-sequence positioning msp method 
 
for large-scale stationary sensor node localization in 
deployments where an event source has line-of-sight to all sensors 
the novel idea behind msp is to estimate each sensor node s 
two-dimensional location by processing multiple easy-to-get 
one-dimensional node sequences e g event detection order 
obtained through loosely-guided event distribution 
this design offers several benefits first compared to a 
range-based approach msp does not require additional costly 
hardware it works using sensors typically used by sensor 
network applications such as light and acoustic sensors both of 
which we specifically consider in this work second compared 
to a range-free approach msp needs only a small number of 
anchors theoretically as few as two so high accuracy can be 
achieved economically by introducing more events instead of 
more anchors and third compared to spotlight msp does not 
require precise and sophisticated event distribution an 
advantage that significantly simplifies the system design and reduces 
calibration cost 
this paper offers the following additional intellectual 
contributions 
 we are the first to localize sensor nodes using the concept 
of node sequence an ordered list of sensor nodes sorted 
by the detection time of a disseminated event we 
demonstrate that making full use of the information embedded 
in one-dimensional node sequences can significantly 
improve localization accuracy interestingly we discover 
that repeated reprocessing of one-dimensional node 
sequences can further increase localization accuracy 
 we propose a distribution-based location estimation 
strategy that obtains the final location of sensor nodes using 
the marginal probability of joint distribution among 
adjacent nodes within the sequence this new algorithm 
outperforms the widely adopted centroid estimation 
 to the best of our knowledge this is the first work to 
improve the localization accuracy of nodes by adaptive 
events the generation of later events is guided by 
localization results from previous events 
 we evaluate line-based msp on our new mirage test-bed 
and wave-based msp in outdoor environments through 
system implementation we discover and address several 
interesting issues such as partial sequence and sequence 
flips to reveal msp performance at scale we provide 
analytic results as well as a complete simulation study 
all the simulation and implementation code is available 
online at http www cs umn edu ∼zhong msp 
the rest of the paper is organized as follows section 
briefly surveys the related work section presents an 
overview of the msp localization system in sections and 
basic msp and four advanced processing methods are 
introduced section describes how msp can be applied in a wave 
propagation scenario section discusses several 
implementation issues section presents simulation results and section 
reports an evaluation of msp on the mirage test-bed and an 
outdoor test-bed section concludes the paper 
 related work 
many methods have been proposed to localize wireless 
sensor devices in the open air most of these can be 
classified into two categories range-based and range-free 
localization range-based localization systems such as gps 
cricket ahlos aoa robust 
quadrilaterals and sweeps are based on fine-grained 
point-topoint distance estimation or angle estimation to identify 
pernode location constraints on the cost energy and hardware 
footprint of each sensor node make these range-based 
methods undesirable for massive outdoor deployment in addition 
ranging signals generated by sensor nodes have a very limited 
effective range because of energy and form factor concerns 
for example ultrasound signals usually effectively propagate 
 - feet using an on-board transmitter consequently 
these range-based solutions require an undesirably high 
deployment density although the received signal strength 
indicator rssi related methods were once considered 
an ideal low-cost solution the irregularity of radio 
propagation seriously limits the accuracy of such systems the 
recently proposed rips localization system superimposes 
two rf waves together creating a low-frequency envelope that 
can be accurately measured this ranging technique performs 
very well as long as antennas are well oriented and 
environmental factors such as multi-path effects and background noise 
are sufficiently addressed 
range-free methods don t need to estimate or measure 
accurate distances or angles instead anchors or controlled-event 
distributions are used for node localization range-free 
methods can be generally classified into two types anchor-based 
and anchor-free solutions 
 for anchor-based solutions such as centroid apit 
 serloc gradient and aps the main 
idea is that the location of each node is estimated based on 
the known locations of the anchor nodes different anchor 
combinations narrow the areas in which the target nodes 
can possibly be located anchor-based solutions normally 
require a high density of anchor nodes so as to achieve 
good accuracy in practice it is desirable to have as few 
anchor nodes as possible so as to lower the system cost 
 anchor-free solutions require no anchor nodes instead 
external event generators and data processing platforms 
are used the main idea is to correlate the event detection 
time at a sensor node with the known space-time 
relationship of controlled events at the generator so that detection 
time-stamps can be mapped into the locations of sensors 
spotlight and lighthouse work in this fashion 
in spotlight the event distribution needs to be 
precise in both time and space precise event distribution 
is difficult to achieve without careful calibration 
especially when the event-generating devices require certain 
mechanical maneuvers e g the telescope mount used in 
spotlight all these increase system cost and reduce 
localization speed stardust which works much faster 
uses label relaxation algorithms to match light spots 
reflected by corner-cube retro-reflectors ccr with sensor 
nodes using various constraints label relaxation 
algorithms converge only when a sufficient number of robust 
constraints are obtained due to the environmental impact 
on rf connectivity constraints however stardust is less 
accurate than spotlight 
in this paper we propose a balanced solution that avoids 
the limitations of both anchor-based and anchor-free solutions 
unlike anchor-based solutions msp allows a flexible 
tradeoff between the physical cost anchor nodes with the soft 
 
 
a 
b 
 
 
 
 
target nodeanchor node 
 a b 
 b a 
 a b 
 ab 
 
 
 
 
 
 b 
 c d 
 a 
event 
node sequence generated by event 
event 
node sequence generated by event 
node sequence generated by event 
node sequence generated by event 
event event 
figure the msp system overview 
cost localization events msp uses only a small number of 
anchors theoretically as few as two unlike anchor-free 
solutions msp doesn t need to maintain rigid time-space 
relationships while distributing events which makes system design 
simpler more flexible and more robust to calibration errors 
 system overview 
msp works by extracting relative location information from 
multiple simple one-dimensional orderings of nodes 
figure a shows a layout of a sensor network with anchor nodes 
and target nodes target nodes are defined as the nodes to be 
localized briefly the msp system works as follows first 
events are generated one at a time in the network area e g 
ultrasound propagations from different locations laser scans 
with diverse angles as each event propagates as shown in 
figure a each node detects it at some particular time 
instance for a single event we call the ordering of nodes which 
is based on the sequential detection of the event a node 
sequence each node sequence includes both the targets and the 
anchors as shown in figure b second a multi-sequence 
processing algorithm helps to narrow the possible location of 
each node to a small area figure c finally a 
distributionbased estimation method estimates the exact location of each 
sensor node as shown in figure d 
figure shows that the node sequences can be obtained 
much more economically than accurate pair-wise distance 
measurements between target nodes and anchor nodes via 
ranging methods in addition this system does not require a rigid 
time-space relationship for the localization events which is 
critical but hard to achieve in controlled event distribution 
scenarios e g spotlight 
for the sake of clarity in presentation we present our system 
in two cases 
 ideal case in which all the node sequences obtained 
from the network are complete and correct and nodes are 
time-synchronized 
 realistic deployment in which i node sequences can 
be partial incomplete ii elements in sequences could 
flip i e the order obtained is reversed from reality and 
 iii nodes are not time-synchronized 
to introduce the msp algorithm we first consider a simple 
straight-line scan scenario then we describe how to 
implement straight-line scans as well as other event types such as 
sound wave propagation 
 
a 
 
 
 
 
b 
c 
 
 
 
 
straight-line scan 
straight-linescan 
 
 
 a 
 
c 
 
 
 
 
b 
 
 
 
c 
 
 a 
b 
 
target node 
anchor node 
figure obtaining multiple node sequences 
 basic msp 
let us consider a sensor network with n target nodes and 
m anchor nodes randomly deployed in an area of size s the 
top-level idea for basic msp is to split the whole sensor 
network area into small pieces by processing node sequences 
because the exact locations of all the anchors in a node sequence 
are known all the nodes in this sequence can be divided into 
o m parts in the area 
in figure we use numbered circles to denote target nodes 
and numbered hexagons to denote anchor nodes basic msp 
uses two straight lines to scan the area from different directions 
treating each scan as an event all the nodes react to the event 
sequentially generating two node sequences for vertical scan 
 the node sequence is a c b as shown 
outside the right boundary of the area in figure for 
horizontal scan the node sequence is c a b 
as shown under the bottom boundary of the area in figure 
since the locations of the anchor nodes are available the 
anchor nodes in the two node sequences actually split the area 
vertically and horizontally into parts as shown in figure 
to extend this process suppose we have m anchor nodes and 
perform d scans from different angles obtaining d node 
sequences and dividing the area into many small parts 
obviously the number of parts is a function of the number of 
anchors m the number of scans d the anchors location as well as 
the slop k for each scan line according to the pie-cutting 
theorem the area can be divided into o m d parts when 
m and d are appropriately large the polygon for each target 
node may become sufficiently small so that accurate 
estimation can be achieved we emphasize that accuracy is affected 
not only by the number of anchors m but also by the number 
of events d in other words msp provides a tradeoff between 
the physical cost of anchors and the soft cost of events 
algorithm depicts the computing architecture of basic 
msp each node sequence is processed within line to for 
each node getboundaries in line searches for the 
predecessor and successor anchors in the sequence so as to 
determine the boundaries of this node then in line updatemap 
shrinks the location area of this node according to the newly 
obtained boundaries after processing all sequences centroid 
estimation line set the center of gravity of the final 
polygon as the estimated location of the target node 
basic msp only makes use of the order information 
between a target node and the anchor nodes in each sequence 
actually we can extract much more location information from 
 
algorithm basic msp process 
output the estimated location of each node 
 repeat 
 getoneunprocessedseqence 
 repeat 
 getonenodefromsequenceinorder 
 getboundaries 
 updatemap 
 until all the target nodes are updated 
 until all the node sequences are processed 
 repeat 
 getoneunestimatednode 
 centroidestimation 
 until all the target nodes are estimated 
each sequence section will introduce advanced msp in 
which four novel optimizations are proposed to improve the 
performance of msp significantly 
 advanced msp 
four improvements to basic msp are proposed in this 
section the first three improvements do not need additional 
sensing and communication in the networks but require only 
slightly more off-line computation the objective of all these 
improvements is to make full use of the information embedded 
in the node sequences the results we have obtained 
empirically indicate that the implementation of the first two methods 
can dramatically reduce the localization error and that the third 
and fourth methods are helpful for some system deployments 
 sequence-based msp 
as shown in figure each scan line and m anchors splits 
the whole area into m parts each target node falls into 
one polygon shaped by scan lines we noted that in basic msp 
only the anchors are used to narrow down the polygon of each 
target node but actually there is more information in the node 
sequence that we can made use of 
let s first look at a simple example shown in figure the 
previous scans narrow the locations of target node and node 
 into two dashed rectangles shown in the left part of figure 
then a new scan generates a new sequence with 
knowledge of the scan s direction it is easy to tell that node is 
located to the left of node thus we can further narrow the 
location area of node by eliminating the shaded part of node 
 s rectangle this is because node is located on the right of 
node while the shaded area is outside the lower boundary of 
node similarly the location area of node can be narrowed 
by eliminating the shaded part out of node s right boundary 
we call this procedure sequence-based msp which means that 
the whole node sequence needs to be processed node by node 
in order specifically sequence-based msp follows this exact 
processing rule 
 
 
 
 
 
lower boundary of upper boundary of 
lower boundary of upper boundary of 
new sequence 
new upper boundary of 
new lower boundary of 
eventpropagation 
figure rule illustration in sequence based msp 
algorithm sequence-based msp process 
output the estimated location of each node 
 repeat 
 getoneunprocessedseqence 
 repeat 
 getonenodebyincreasingorder 
 computelowbound 
 updatemap 
 until the last target node in the sequence 
 repeat 
 getonenodebydecreasingorder 
 computeupbound 
 updatemap 
 until the last target node in the sequence 
 until all the node sequences are processed 
 repeat 
 getoneunestimatednode 
 centroidestimation 
 until all the target nodes are estimated 
elimination rule along a scanning direction the lower 
boundary of the successor s area must be equal to or larger 
than the lower boundary of the predecessor s area and the 
upper boundary of the predecessor s area must be equal to or 
smaller than the upper boundary of the successor s area 
in the case of figure node is the successor of node 
and node is the predecessor of node according to the 
elimination rule node s lower boundary cannot be smaller 
than that of node and node s upper boundary cannot exceed 
node s upper boundary 
algorithm illustrates the pseudo code of sequence-based 
msp each node sequence is processed within line to the 
sequence processing contains two steps 
step line to compute and modify the lower 
boundary for each target node by increasing order in the node 
sequence each node s lower boundary is determined by the 
lower boundary of its predecessor node in the sequence thus 
the processing must start from the first node in the sequence 
and by increasing order then update the map according to the 
new lower boundary 
step line to compute and modify the upper 
boundary for each node by decreasing order in the node sequence 
each node s upper boundary is determined by the upper 
boundary of its successor node in the sequence thus the processing 
must start from the last node in the sequence and by 
decreasing order then update the map according to the new upper 
boundary 
after processing all the sequences for each node a polygon 
bounding its possible location has been found then 
center-ofgravity-based estimation is applied to compute the exact 
location of each node line to 
an example of this process is shown in figure the third 
scan generates the node sequence b c a in 
addition to the anchor split lines because nodes and come 
after node in the sequence node and s polygons could 
be narrowed according to node s lower boundary the lower 
right-shaded area similarly the shaded area in node s 
rectangle could be eliminated since this part is beyond node s 
upper boundary indicated by the dotted line similar 
eliminating can be performed for node as shown in the figure 
 
 
a 
 
 
 
 
b 
c 
 
 
 
 
straight-line scan 
straight-linescan 
straight-line scan 
target node 
anchor node 
figure sequence-based msp example 
 
a 
 
 
 
 
b 
c 
 
 
 
 
straight-line scan 
straight-linescan 
straight-line scan 
reprocessing scan 
target node 
anchor node 
figure iterative msp reprocessing scan 
from above we can see that the sequence-based msp 
makes use of the information embedded in every sequential 
node pair in the node sequence the polygon boundaries of 
the target nodes obtained in prior could be used to further split 
other target nodes areas our evaluation in sections and 
shows that sequence-based msp considerably enhances system 
accuracy 
 iterative msp 
sequence-based msp is preferable to basic msp because it 
extracts more information from the node sequence in fact 
further useful information still remains in sequence-based msp 
a sequence processed later benefits from information produced 
by previously processed sequences e g the third scan in 
figure however the first several sequences can hardly benefit 
from other scans in this way inspired by this phenomenon 
we propose iterative msp the basic idea of iterative msp is 
to process all the sequences iteratively several times so that the 
processing of each single sequence can benefit from the results 
of other sequences 
to illustrate the idea more clearly figure shows the results 
of three scans that have provided three sequences now if we 
process the sequence a c b obtained from 
scan again we can make progress as shown in figure 
the reprocessing of the node sequence provides information 
in the way an additional vertical scan would from 
sequencebased msp we know that the upper boundaries of nodes and 
 along the scan direction must not extend beyond the upper 
boundary of node therefore the grid parts can be eliminated 
 a central of gravity b joint distribution 
 
 
 
 
 
 
 
 
 
 
 
figure example of joint distribution estimation 
  
vm 
ap 
vm 
ap 
vm 
ap 
vm 
ap 
combine 
m 
ap 
figure idea of dbe msp for each node 
for the nodes and node respectively as shown in figure 
from this example we can see that iterative processing of the 
sequence could help further shrink the polygon of each target 
node and thus enhance the accuracy of the system 
the implementation of iterative msp is straightforward 
process all the sequences multiple times using sequence-based 
msp like sequence-based msp iterative msp introduces no 
additional event cost in other words reprocessing does not 
actually repeat the scan physically evaluation results in 
section will show that iterative msp contributes noticeably to 
a lower localization error empirical results show that after 
iterations improvements become less significant in summary 
iterative processing can achieve better performance with only 
a small computation overhead 
 distribution-based estimation 
after determining the location area polygon for each node 
estimation is needed for a final decision previous research 
mostly applied the center of gravity cog method 
 which minimizes average error if every node is 
independent of all others cog is the statistically best solution in 
msp however each node may not be independent for 
example two neighboring nodes in a certain sequence could have 
overlapping polygon areas in this case if the marginal 
probability of joint distribution is used for estimation better 
statistical results are achieved 
figure shows an example in which node and node are 
located in the same polygon if cog is used both nodes are 
localized at the same position figure a however the node 
sequences obtained from two scans indicate that node should 
be to the left of and above node as shown in figure b 
the high-level idea of distribution-based estimation 
proposed for msp which we call dbe msp is illustrated in 
figure the distributions of each node under the ith scan for the 
ith node sequence are estimated in node vmap i which is a 
data structure for remembering the marginal distribution over 
scan i then all the vmaps are combined to get a single map 
and weighted estimation is used to obtain the final location 
for each scan all the nodes are sorted according to the gap 
which is the diameter of the polygon along the direction of the 
scan to produce a second gap-based node sequence then 
the estimation starts from the node with the smallest gap this 
is because it is statistically more accurate to assume a uniform 
distribution of the node with smaller gap for each node 
processed in order from the gap-based node sequence either if 
 
pred node s area 
predecessor node exists 
conditional distribution 
based on pred node s area 
alone uniformly distributed 
succ node s area 
successor node exists 
conditional distribution 
based on succ node s area 
succ node s area 
both predecessor and successor 
nodes exist conditional distribution 
based on both of them 
pred node s area 
figure four cases in dbe process 
no neighbor node in the original event-based node sequence 
shares an overlapping area or if the neighbors have not been 
processed due to bigger gaps a uniform distribution uniform 
is applied to this isolated node the alone case in figure 
if the distribution of its neighbors sharing overlapped areas has 
been processed we calculate the joint distribution for the node 
as shown in figure there are three possible cases 
depending on whether the distribution of the overlapping predecessor 
and or successor nodes have has already been estimated 
the estimation s strategy of starting from the most accurate 
node smallest gap node reduces the problem of estimation 
error propagation the results in the evaluation section indicate 
that applying distribution-based estimation could give 
statistically better results 
 adaptive msp 
so far all the enhancements to basic msp focus on 
improving the multi-sequence processing algorithm given a fixed set 
of scan directions all these enhancements require only more 
computing time without any overhead to the sensor nodes 
obviously it is possible to have some choice and optimization on 
how events are generated for example in military situations 
artillery or rocket-launched mini-ultrasound bombs can be used 
for event generation at some selected locations in adaptive 
msp we carefully generate each new localization event so as 
to maximize the contribution of the new event to the refinement 
of localization based on feedback from previous events 
figure depicts the basic architecture of adaptive msp 
through previous localization events the whole map has been 
partitioned into many small location areas the idea of 
adaptive msp is to generate the next localization event to achieve 
best-effort elimination which ideally could shrink the location 
area of individual node as much as possible 
we use a weighted voting mechanism to evaluate candidate 
localization events every node wants the next event to split its 
area evenly which would shrink the area fast therefore every 
node votes for the parameters of the next event e g the scan 
angle k of the straight-line scan since the area map is 
maintained centrally the vote is virtually done and there is no need 
for the real sensor nodes to participate in it after gathering all 
the voting results the event parameters with the most votes win 
the election there are two factors that determine the weight of 
each vote 
 the vote for each candidate event is weighted according 
to the diameter d of the node s location area nodes with 
bigger location areas speak louder in the voting because 
map partitioned by the localization events 
diameter of each 
area 
candidate 
localization events 
evaluation 
trigger next 
localization evet 
figure basic architecture of adaptive msp 
 
 
diameter d 
 
 
 k 
 
 k 
 
 k 
 
 k 
 
 k 
 
 k 
 
 k 
 k 
 k 
 
 k 
 k 
 k 
weight 
el 
small 
i 
opt 
i 
j 
ii 
j 
i 
s 
s 
dkkdfkweight 
arg 
 ⋅ ∆ 
 
 
opt 
k 
target node 
anchor node 
center of gravity 
node s area 
figure candidate slops for node at anchor 
overall system error is reduced mostly by splitting the 
larger areas 
 the vote for each candidate event is also weighted 
according to its elimination efficiency for a location area which 
is defined as how equally in size or in diameter an event 
can cut an area in other words an optimal scan event 
cuts an area in the middle since this cut shrinks the area 
quickly and thus reduces localization uncertainty quickly 
combining the above two aspects the weight for each vote 
is computed according to the following equation 
weight k 
j 
i f di △ k 
j 
i k 
opt 
i 
k 
j 
i is node i s jth supporting parameter for next event 
generation di is diameter of node i s location area △ k 
j 
i k 
opt 
i is the 
distance between k 
j 
i and the optimal parameter k 
opt 
i for node i 
which should be defined to fit the specific application 
figure presents an example for node s voting for the 
slopes of the next straight-line scan in the system there 
are a fixed number of candidate slopes for each scan e g 
k k k k the location area of target node is shown 
in the figure the candidate events k 
 k 
 k 
 k 
 k 
 k 
 are 
evaluated according to their effectiveness compared to the optimal 
ideal event which is shown as a dotted line with appropriate 
weights computed according to equation for this 
specific example as is illustrated in the right part of figure 
f di △ k 
j 
i kopt 
i is defined as the following equation 
weight kj 
i f di △ kj 
i kopt 
i di · 
ssmall 
slarge 
 
ssmall and slarge are the sizes of the smaller part and larger 
part of the area cut by the candidate line respectively in this 
case node votes for the candidate lines that do not cross its 
area since ssmall 
we show later that adaptive msp improves localization 
accuracy in wsns with irregularly shaped deployment areas 
 
 overhead and msp complexity analysis 
this section provides a complexity analysis of the msp 
design we emphasize that msp adopts an asymmetric design in 
which sensor nodes need only to detect and report the events 
they are blissfully oblivious to the processing methods 
proposed in previous sections in this section we analyze the 
computational cost on the node sequence processing side where 
resources are plentiful 
according to algorithm the computational complexity of 
basic msp is o d · n · s and the storage space required is 
o n · s where d is the number of events n is the number of 
target nodes and s is the area size 
according to algorithm the computational complexity of 
both sequence-based msp and iterative msp is o c·d ·n ·s 
where c is the number of iterations and c for 
sequencebased msp and the storage space required is o n ·s both the 
computational complexity and storage space are equal within a 
constant factor to those of basic msp 
the computational complexity of the distribution-based 
estimation dbe msp is greater the major overhead comes 
from the computation of joint distributions when both 
predecessor and successor nodes exit in order to compute the 
marginal probability msp needs to enumerate the locations of 
the predecessor node and the successor node for example 
if node a has predecessor node b and successor node c then 
the marginal probability pa x y of node a s being at location 
 x y is 
pa x y ∑ 
i 
∑ 
j 
∑ 
m 
∑ 
n 
 
nb a c 
·pb i j ·pc m n 
nb a c is the number of valid locations for a satisfying the 
sequence b a c when b is at i j and c is at m n 
pb i j is the available probability of node b s being located 
at i j pc m n is the available probability of node c s 
being located at m n a naive algorithm to compute equation 
 has complexity o d · n · s however since the marginal 
probability indeed comes from only one dimension along the 
scanning direction e g a line the complexity can be reduced 
to o d · n · s after algorithm optimization in addition the 
final location areas for every node are much smaller than the 
original field s therefore in practice dbe msp can be 
computed much faster than o d ·n ·s 
 wave propagation example 
so far the description of msp has been solely in the 
context of straight-line scan however we note that msp is 
conceptually independent of how the event is propagated as long 
as node sequences can be obtained clearly we can also 
support wave-propagation-based events e g ultrasound 
propagation air blast propagation which are polar coordinate 
equivalences of the line scans in the cartesian coordinate system 
this section illustrates the effects of msp s implementation in 
the wave propagation-based situation for easy modelling we 
have made the following assumptions 
 the wave propagates uniformly in all directions 
therefore the propagation has a circle frontier surface since 
msp does not rely on an accurate space-time relationship 
a certain distortion in wave propagation is tolerable if any 
directional wave is used the propagation frontier surface 
can be modified accordingly 
 
 
 
 
target node 
anchor node 
previous event location 
a 
 
center of gravity 
 
 
 
b 
 
c 
a line of preferred locations for next event 
figure example of wave propagation situation 
 under the situation of line-of-sight we allow obstacles to 
reflect or deflect the wave reflection and deflection are 
not problems because each node reacts only to the first 
detected event those reflected or deflected waves come 
later than the line-of-sight waves the only thing the 
system needs to maintain is an appropriate time interval 
between two successive localization events 
 we assume that background noise exists and therefore we 
run a band-pass filter to listen to a particular wave 
frequency this reduces the chances of false detection 
the parameter that affects the localization event generation 
here is the source location of the event the different 
distances between each node and the event source determine the 
rank of each node in the node sequence using the node 
sequences the msp algorithm divides the whole area into many 
non-rectangular areas as shown in figure in this figure 
the stars represent two previous event sources the previous 
two propagations split the whole map into many areas by those 
dashed circles that pass one of the anchors each node is 
located in one of the small areas since sequence-based msp 
iterative msp and dbe msp make no assumptions about the 
type of localization events and the shape of the area all three 
optimization algorithms can be applied for the wave 
propagation scenario 
however adaptive msp needs more explanation figure 
illustrates an example of nodes voting for next event source 
locations unlike the straight-line scan the critical parameter 
now is the location of the event source because the distance 
between each node and the event source determines the rank of 
the node in the sequence in figure if the next event breaks 
out along near the solid thick gray line which perpendicularly 
bisects the solid dark line between anchor c and the center of 
gravity of node s area the gray area the wave would reach 
anchor c and the center of gravity of node s area at roughly 
the same time which would relatively equally divide node s 
area therefore node prefers to vote for the positions around 
the thick gray line 
 practical deployment issues 
for the sake of presentation until now we have described 
msp in an ideal case where a complete node sequence can be 
obtained with accurate time synchronization in this section 
we describe how to make msp work well under more realistic 
conditions 
 
 incomplete node sequence 
for diverse reasons such as sensor malfunction or natural 
obstacles the nodes in the network could fail to detect 
localization events in such cases the node sequence will not be 
complete this problem has two versions 
 anchor nodes are missing in the node sequence 
if some anchor nodes fail to respond to the localization 
events then the system has fewer anchors in this case 
the solution is to generate more events to compensate for 
the loss of anchors so as to achieve the desired accuracy 
requirements 
 target nodes are missing in the node sequence 
there are two consequences when target nodes are 
missing first if these nodes are still be useful to sensing 
applications they need to use other backup localization 
approaches e g centroid to localize themselves with help 
from their neighbors who have already learned their own 
locations from msp secondly since in advanced msp 
each node in the sequence may contribute to the overall 
system accuracy dropping of target nodes from sequences 
could also reduce the accuracy of the localization thus 
proper compensation procedures such as adding more 
localization events need to be launched 
 localization without time synchronization 
in a sensor network without time synchronization support 
nodes cannot be ordered into a sequence using timestamps for 
such cases we propose a listen-detect-assemble-report 
protocol which is able to function independently without time 
synchronization 
listen-detect-assemble-report requires that every node 
listens to the channel for the node sequence transmitted from its 
neighbors then when the node detects the localization event 
it assembles itself into the newest node sequence it has heard 
and reports the updated sequence to other nodes figure 
 a illustrates an example for the listen-detect-assemble-report 
protocol for simplicity in this figure we did not differentiate 
the target nodes from anchor nodes a solid line between two 
nodes stands for a communication link suppose a straight line 
scans from left to right node detects the event and then it 
broadcasts the sequence into the network node and node 
 receive this sequence when node detects the event node 
 adds itself into the sequence and broadcasts the 
sequence propagates in the same direction with the scan as shown 
in figure a finally node obtains a complete sequence 
 
in the case of ultrasound propagation because the event 
propagation speed is much slower than that of radio the 
listendetect-assemble-report protocol can work well in a situation 
where the node density is not very high for instance if the 
distance between two nodes along one direction is meters 
the m s sound needs ms to propagate from one node 
to the other while normally the communication data rate is 
 kbps in the wsn e g cc it takes only about 
 ∼ ms to transmit an assembled packet for one hop 
one problem that may occur using the 
listen-detectassemble-report protocol is multiple partial sequences as 
shown in figure b two separate paths in the network may 
result in two sequences that could not be further combined in 
this case since the two sequences can only be processed as 
separate sequences some order information is lost therefore the 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 a 
 b 
 c 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
event propagation 
event propagation 
event propagation 
figure node sequence without time synchronization 
accuracy of the system would decrease 
the other problem is the sequence flip problem as shown 
in figure c because node and node are too close to 
each other along the scan direction they detect the scan 
almost simultaneously due to the uncertainty such as media 
access delay two messages could be transmitted out of order 
for example if node sends out its report first then the order 
of node and node gets flipped in the final node sequence 
the sequence flip problem would appear even in an accurately 
synchronized system due to random jitter in node detection if 
an event arrives at multiple nodes almost simultaneously a 
method addressing the sequence flip is presented in the next 
section 
 sequence flip and protection band 
sequence flip problems can be solved with and without 
time synchronization we firstly start with a scenario 
applying time synchronization existing solutions for time 
synchronization can easily achieve sub-millisecond-level 
accuracy for example ftsp achieves µs microsecond 
average error for a two-node single-hop case therefore we 
can comfortably assume that the network is synchronized with 
maximum error of µs however when multiple nodes are 
located very near to each other along the event propagation 
direction even when time synchronization with less than ms 
error is achieved in the network sequence flip may still occur 
for example in the sound wave propagation case if two nodes 
are less than meters apart the difference between their 
detection timestamp would be smaller than millisecond 
we find that sequence flip could not only damage system 
accuracy but also might cause a fatal error in the msp algorithm 
figure illustrates both detrimental results in the left side of 
figure a suppose node and node are so close to each 
other that it takes less than ms for the localization event to 
propagate from node to node now unfortunately the node 
sequence is mistaken to be so node is expected to be 
located to the right of node such as at the position of the 
dashed node according to the elimination rule in 
sequencebased msp the left part of node s area is cut off as shown in 
the right part of figure a this is a potentially fatal error 
because node is actually located in the dashed area which has 
been eliminated by mistake during the subsequent 
eliminations introduced by other events node s area might be cut off 
completely thus node could consequently be erased from the 
map even in cases where node still survives its area actually 
does not cover its real location 
 
 
 
 
 
lower boundary of upper boundary of 
flipped sequence fatal elimination error 
eventpropagation 
 
fatal error 
 
 
 
 
 
lower boundary of upper boundary of 
flipped sequence safe elimination 
eventpropagation 
 
new lower boundary of 
 
b 
 a 
 b 
b protection band 
figure sequence flip and protection band 
another problem is not fatal but lowers the localization 
accuracy if we get the right node sequence node has a 
new upper boundary which can narrow the area of node as in 
figure due to the sequence flip node loses this new upper 
boundary 
in order to address the sequence flip problem especially to 
prevent nodes from being erased from the map we propose 
a protection band compensation approach the basic idea of 
protection band is to extend the boundary of the location area 
a little bit so as to make sure that the node will never be erased 
from the map this solution is based on the fact that nodes 
have a high probability of flipping in the sequence if they are 
near to each other along the event propagation direction if 
two nodes are apart from each other more than some distance 
say b they rarely flip unless the nodes are faulty the width 
of a protection band b is largely determined by the maximum 
error in system time synchronization and the localization event 
propagation speed 
figure b presents the application of the protection band 
instead of eliminating the dashed part in figure a for node 
 the new lower boundary of node is set by shifting the 
original lower boundary of node to the left by distance b in this 
case the location area still covers node and protects it from 
being erased in a practical implementation supposing that the 
ultrasound event is used if the maximum error of system time 
synchronization is ms two nodes might flip with high 
probability if the timestamp difference between the two nodes is 
smaller than or equal to ms accordingly we set the 
protection band b as m the distance sound can propagate within 
 millisecond by adding the protection band we reduce the 
chances of fatal errors although at the cost of localization 
accuracy empirical results obtained from our physical test-bed 
verified this conclusion 
in the case of using the listen-detect-assemble-report 
protocol the only change we need to make is to select the protection 
band according to the maximum delay uncertainty introduced 
by the mac operation and the event propagation speed to 
bound mac delay at the node side a node can drop its report 
message if it experiences excessive mac delay this converts 
the sequence flip problem to the incomplete sequence problem 
which can be more easily addressed by the method proposed in 
section 
 simulation evaluation 
our evaluation of msp was conducted on three platforms 
 i an indoor system with micaz motes using straight-line 
scan ii an outdoor system with micaz motes using sound 
wave propagation and iii an extensive simulation under 
various kinds of physical settings 
in order to understand the behavior of msp under 
numerous settings we start our evaluation with simulations 
then we implemented basic msp and all the advanced 
msp methods for the case where time synchronization is 
available in the network the simulation and 
implementation details are omitted in this paper due to space 
constraints but related documents are provided online at 
http www cs umn edu ∼zhong msp full implementation and 
evaluation of system without time synchronization are yet to be 
completed in the near future 
in simulation we assume all the node sequences are perfect 
so as to reveal the performance of msp achievable in the 
absence of incomplete node sequences or sequence flips in our 
simulations all the anchor nodes and target nodes are assumed 
to be deployed uniformly the mean and maximum errors are 
averaged over runs to obtain high confidence for legibility 
reasons we do not plot the confidence intervals in this paper 
all the simulations are based on the straight-line scan example 
we implement three scan strategies 
 random scan the slope of the scan line is randomly 
chosen at each time 
 regular scan the slope is predetermined to rotate 
uniformly from degree to degrees for example if the 
system scans times then the scan angles would be 
 and 
 adaptive scan the slope of each scan is determined 
based on the localization results from previous scans 
we start with basic msp and then demonstrate the 
performance improvements one step at a time by adding i 
sequencebased msp ii iterative msp iii dbe msp and iv adaptive 
msp 
 performance of basic msp 
the evaluation starts with basic msp where we compare the 
performance of random scan and regular scan under different 
configurations we intend to illustrate the impact of the number 
of anchors m the number of scans d and target node density 
 number of target nodes n in a fixed-size region on the 
localization error table shows the default simulation parameters 
the error of each node is defined as the distance between the 
estimated location and the real position we note that by 
default we only use three anchors which is considerably fewer 
than existing range-free solutions 
impact of the number of scans in this experiment we 
compare regular scan with random scan under a different number 
of scans from to in steps of the number of anchors 
table default configuration parameters 
parameter description 
field area × grid unit 
scan type regular default random scan 
anchor number default 
scan times default 
target node number default 
statistics error mean max 
random seeds runs 
 
 
 
 
 
 
 
 
 
 
 
 
mean error and max error vs scan time 
scan time 
error max error of random scan 
max error of regular scan 
mean error of random scan 
mean error of regular scan 
 a error vs number of scans 
 
 
 
 
 
 
 
 
mean error and max error vs anchor number 
anchor number 
error 
max error of random scan 
max error of regular scan 
mean error of random scan 
mean error of regular scan 
 b error vs anchor number 
 
 
 
 
 
 
 
 
mean error and max error vs target node number 
target node number 
error 
max error of random scan 
max error of regular scan 
mean error of random scan 
mean error of regular scan 
 c error vs number of target nodes 
figure evaluation of basic msp under random and regular scans 
 
 
 
 
 
 
 
 
 
basic msp vs sequence based msp ii 
scan time 
error 
max error of basic msp 
max error of seq msp 
mean error of basic msp 
mean error of seq msp 
 a error vs number of scans 
 
 
 
 
 
 
 
 
 
 
 
 
basic msp vs sequence based msp i 
anchor number 
error 
max error of basic msp 
max error of seq msp 
mean error of basic msp 
mean error of seq msp 
 b error vs anchor number 
 
 
 
 
 
 
 
 
 
 
 
 
basic msp vs sequence based msp iii 
target node number 
error 
max error of basic msp 
max error of seq msp 
mean error of basic msp 
mean error of seq msp 
 c error vs number of target nodes 
figure improvements of sequence-based msp over basic msp 
is by default figure a indicates the following i as 
the number of scans increases the localization error decreases 
significantly for example localization errors drop more than 
 from scans to scans ii statistically regular scan 
achieves better performance than random scan under identical 
number of scans however the performance gap reduces as 
the number of scans increases this is expected since a large 
number of random numbers converges to a uniform 
distribution figure a also demonstrates that msp requires only 
a small number of anchors to perform very well compared to 
existing range-free solutions 
impact of the number of anchors in this experiment we 
compare regular scan with random scan under different 
number of anchors from to in steps of the results shown in 
figure b indicate that i as the number of anchor nodes 
increases the localization error decreases and ii 
statistically regular scan obtains better results than random scan with 
identical number of anchors by combining figures a 
and b we can conclude that msp allows a flexible tradeoff 
between physical cost anchor nodes and soft cost 
 localization events 
impact of the target node density in this experiment we 
confirm that the density of target nodes has no impact on the 
accuracy which motivated the design of sequence-based msp 
in this experiment we compare regular scan with random scan 
under different number of target nodes from to in steps 
of results in figure c show that mean localization 
errors remain constant across different node densities however 
when the number of target nodes increases the average 
maximum error increases 
summary from the above experiments we can conclude that 
in basic msp regular scan are better than random scan under 
different numbers of anchors and scan events this is because 
regular scans uniformly eliminate the map from different 
directions while random scans would obtain sequences with 
redundant overlapping information if two scans choose two similar 
scanning slopes 
 improvements of sequence-based msp 
this section evaluates the benefits of exploiting the order 
information among target nodes by comparing sequence-based 
msp with basic msp in this and the following sections 
regular scan is used for straight-line scan event generation the 
purpose of using regular scan is to keep the scan events and 
the node sequences identical for both sequence-based msp and 
basic msp so that the only difference between them is the 
sequence processing procedure 
impact of the number of scans in this experiment we 
compare sequence-based msp with basic msp under different 
number of scans from to in steps of figure a 
indicates significant performance improvement in sequence-based 
msp over basic msp across all scan settings especially when 
the number of scans is large for example when the number 
of scans is errors in sequence-based msp are about 
of that of basic msp we conclude that sequence-based msp 
performs extremely well when there are many scan events 
impact of the number of anchors in this experiment we 
use different number of anchors from to in steps of as 
seen in figure b the mean error and maximum error of 
sequence-based msp is much smaller than that of basic msp 
especially when there is limited number of anchors in the 
system e g anchors the error rate was almost halved by 
using sequence-based msp this phenomenon has an interesting 
explanation the cutting lines created by anchor nodes are 
exploited by both basic msp and sequence-based msp so as the 
 
 
 
 
 
 
 
 
 
 
 
 
 
basic msp vs iterative msp 
iterative times 
error 
max error of iterative seq msp 
mean error of iterative seq msp 
max error of basic msp 
mean error of basic msp 
figure improvements of iterative msp 
 
 
 
 
 
 
 
 
 
 
 
 
dbe vs non−dbe 
error 
cumulativedistrubutioinfunctions cdf 
mean error cdf of dbe msp 
mean error cdf of non−dbe msp 
max error cdf of dbe msp 
max error cdf of non−dbe msp 
figure improvements of dbe msp 
 
 
 
 
 
 
 
 
 
adaptive msp for by 
target node number 
error 
max error of regualr scan 
max error of adaptive scan 
mean error of regualr scan 
mean error of adaptive scan 
 a adaptive msp for by field 
 
 
 
 
 
 
 
 
 
 
 
 
mean error cdf at different angle steps in adaptive scan 
mean error 
cumulativedistrubutioinfunctions cdf 
 degree angle step adaptive 
 degree angle step adaptive 
 degree angle step adaptive 
 degree step regular scan 
 b impact of the number of candidate events 
figure the improvements of adaptive msp 
number of anchor nodes increases anchors tend to dominate 
the contribution therefore the performance gaps lessens 
impact of the target node density figure c 
demonstrates the benefits of exploiting order information among 
target nodes since sequence-based msp makes use of the 
information among the target nodes having more target nodes 
contributes to the overall system accuracy as the number of 
target nodes increases the mean error and maximum error of 
sequence-based msp decreases clearly the mean error in 
basic msp is not affected by the number of target nodes as shown 
in figure c 
summary from the above experiments we can conclude that 
exploiting order information among target nodes can improve 
accuracy significantly especially when the number of events is 
large but with few anchors 
 iterative msp over sequence-based msp 
in this experiment the same node sequences were processed 
iteratively multiple times in figure the two single marks 
are results from basic msp since basic msp doesn t perform 
iterations the two curves present the performance of 
iterative msp under different numbers of iterations c we note that 
when only a single iteration is used this method degrades to 
sequence-based msp therefore figure compares the three 
methods to one another 
figure shows that the second iteration can reduce the 
mean error and maximum error dramatically after that the 
performance gain gradually reduces especially when c 
this is because the second iteration allows earlier scans to 
exploit the new boundaries created by later scans in the first 
iteration such exploitation decays quickly over iterations 
 dbe msp over iterative msp 
figure in which we augment iterative msp with 
distribution-based estimation dbe msp shows that dbe 
msp could bring about statistically better performance 
figure presents cumulative distribution localization errors in 
general the two curves of the dbe msp lay slightly to the left 
of that of non-dbe msp which indicates that dbe msp has 
a smaller statistical mean error and averaged maximum error 
than non-dbe msp we note that because dbe is augmented 
on top of the best solution so far the performance 
improvement is not significant when we apply dbe on basic msp 
methods the improvement is much more significant we omit 
these results because of space constraints 
 improvements of adaptive msp 
this section illustrates the performance of adaptive msp 
over non-adaptive msp we note that feedback-based 
adaptation can be applied to all msp methods since it affects only 
the scanning angles but not the sequence processing in this 
experiment we evaluated how adaptive msp can improve the 
best solution so far the default angle granularity step for 
adaptive searching is degrees 
impact of area shape first if system settings are regular 
the adaptive method hardly contributes to the results for a 
square area regular the performance of adaptive msp and 
regular scans are very close however if the shape of the area 
is not regular adaptive msp helps to choose the appropriate 
localization events to compensate therefore adaptive msp 
can achieve a better mean error and maximum error as shown 
in figure a for example adaptive msp improves 
localization accuracy by when the number of target nodes is 
 
impact of the target node density figure a shows that 
when the node density is low adaptive msp brings more 
benefit than when node density is high this phenomenon makes 
statistical sense because the law of large numbers tells us that 
node placement approaches a truly uniform distribution when 
the number of nodes is increased adaptive msp has an edge 
 
figure the mirage test-bed line scan figure the -node outdoor experiments wave 
when layout is not uniform 
impact of candidate angle density figure b shows that 
the smaller the candidate scan angle step the better the 
statistical performance in terms of mean error the rationale is clear 
as wider candidate scan angles provide adaptive msp more 
opportunity to choose the one approaching the optimal angle 
 simulation summary 
starting from basic msp we have demonstrated 
step-bystep how four optimizations can be applied on top of each other 
to improve localization performance in other words these 
optimizations are compatible with each other and can jointly 
improve the overall performance we note that our simulations 
were done under assumption that the complete node sequence 
can be obtained without sequence flips in the next section we 
present two real-system implementations that reveal and 
address these practical issues 
 system evaluation 
in this section we present a system implementation of msp 
on two physical test-beds the first one is called mirage a 
large indoor test-bed composed of six -foot by -foot boards 
illustrated in figure each board in the system can be used 
as an individual sub-system which is powered controlled and 
metered separately three hitachi cp-x projectors 
connected through a matorx triplehead go graphics expansion 
box are used to create an ultra-wide integrated display on six 
boards figure shows that a long tilted line is generated by 
the projectors we have implemented all five versions of msp 
on the mirage test-bed running micaz motes unless 
mentioned otherwise the default setting is anchors and scans at 
the scanning line speed of feet s in all of our graphs each 
data point represents the average value of trials in the 
outdoor system a dell a speaker is used to generate khz 
sound as shown in figure we place micaz motes in the 
backyard of a house since the location is not completely open 
sound waves are reflected scattered and absorbed by various 
objects in the vicinity causing a multi-path effect in the 
system evaluation simple time synchronization mechanisms are 
applied on each node 
 indoor system evaluation 
during indoor experiments we encountered several 
realworld problems that are not revealed in the simulation first 
sequences obtained were partial due to misdetection and 
message losses second elements in the sequences could flip due 
to detection delay uncertainty in media access or error in time 
synchronization we show that these issues can be addressed 
by using the protection band method described in section 
 on scanning speed and protection band 
in this experiment we studied the impact of the scanning 
speed and the length of protection band on the performance of 
the system in general with increasing scanning speed nodes 
have less time to respond to the event and the time gap between 
two adjacent nodes shrinks leading to an increasing number of 
partial sequences and sequence flips 
figure shows the node flip situations for six scans with 
distinct angles under different scan speeds the x-axis shows 
the distance between the flipped nodes in the correct node 
sequence y-axis shows the total number of flips in the six scans 
this figure tells us that faster scan brings in not only 
increasing number of flips but also longer-distance flips that require 
wider protection band to prevent from fatal errors 
figure a shows the effectiveness of the protection band 
in terms of reducing the number of unlocalized nodes when 
we use a moderate scan speed feet s the chance of flipping 
is rare therefore we can achieve feet mean accuracy 
 figure b with feet maximum error figure c with 
increasing speeds the protection band needs to be set to a larger 
value to deal with flipping interesting phenomena can be 
observed in figures on one hand the protection band can 
sharply reduce the number of unlocalized nodes on the other 
hand protection bands enlarge the area in which a target would 
potentially reside introducing more uncertainty thus there is 
a concave curve for both mean and maximum error when the 
scan speed is at feet s 
 on msp methods and protection band 
in this experiment we show the improvements resulting 
from three different methods figure a shows that a 
protection band of feet is sufficient for the scan speed of 
 feet s figures b and c show clearly that iterative 
msp with adaptation achieves best performance for 
example figures b shows that when we set the protection band 
at feet iterative msp achieves feet accuracy which 
is more accurate than the basic design similarly 
figures b and c show the double-edged effects of 
protection band on the localization accuracy 
 
 
 
 
 flip distribution for scans at line speed of feet s 
flips 
node distance in the ideal node sequence 
 
 
 
 
 flip distribution for scans at line speed of feet s 
flips 
 
 
 
 
 flip distribution for scans at line speed of feet s 
flips 
figure number of flips for different scan speeds 
 
 
 
 
 
 
 
 
 
 
 
 
 
unlocalized node number line scan at different speed 
protection band in feet 
unlocalizednodenumber 
scan line speed feet s 
scan line speed feet s 
scan line speed feet s 
 a number of unlocalized nodes 
 
 
 
 
 
 
 
 
 
mean error line scan at different speed 
protection band in feet 
error infeet 
scan line speed feet s 
scan line speed feet s 
scan line speed feet s 
 b mean localization error 
 
 
 
 
 
 
 
max error line scan at different speed 
protection band in feet 
error infeet 
scan line speed feet s 
scan line speed feet s 
scan line speed feet s 
 c max localization error 
figure impact of protection band and scanning speed 
 
 
 
 
 
 
 
 
 
 
 
 
unlocalized node number scan line speed feet s 
protection band in feet 
numberofunlocalizednodeoutof 
unlocalized node of basic msp 
unlocalized node of sequence based msp 
unlocalized node of iterative msp 
 a number of unlocalized nodes 
 
 
 
 
 
 
 
 
 
mean error scan line speed feet s 
protection band in feet 
error infeet 
mean error of basic msp 
mean error of sequence based msp 
mean error of iterative msp 
 b mean localization error 
 
 
 
 
 
 
 
max error scan line speed feet s 
protection band in feet 
error infeet 
max error of basic msp 
max error of sequence based msp 
max error of iterative msp 
 c max localization error 
figure impact of protection band under different msp methods 
 
 
 
 
 
 
 
unlocalized node number protection band feet 
anchor number 
unlocalizednodenumber 
 scan events at speed feet s 
 scan events at speed feet s 
 scan events at speed feet s 
 a number of unlocalized nodes 
 
 
 
 
 
 
 
 
 
 
mean error protection band feet 
anchor number 
error infeet 
mean error of scan events at speed feet s 
mean error of scan events at speed feet s 
mean error of scan events at speed feet s 
 b mean localization error 
 
 
 
 
 
 
 
 
 
 
 
 
max error protection band feet 
anchor number 
error infeet 
max error of scan events at speed feet s 
max error of scan events at speed feet s 
max error of scan events at speed feet s 
 c max localization error 
figure impact of the number of anchors and scans 
 on number of anchors and scans 
in this experiment we show a tradeoff between hardware 
cost anchors with soft cost events figure a shows that 
with more cutting lines created by anchors the chance of 
unlocalized nodes increases slightly we note that with a feet 
protection band the percentage of unlocalized nodes is very 
small e g in the worst-case with anchors only out of 
nodes are not localized due to flipping figures b and c 
show the tradeoff between number of anchors and the number 
of scans obviously with the number of anchors increases the 
error drops significantly with anchors we can achieve a 
localization accuracy as low as ∼ feet which is nearly a 
 improvement similarly with increasing number of scans 
the accuracy drops significantly as well we can observe about 
 across all anchor settings when we increase the number of 
scans from to for example with only anchors we can 
achieve -foot accuracy with scans 
 outdoor system evaluation 
the outdoor system evaluation contains two parts i 
effective detection distance evaluation which shows that the 
node sequence can be readily obtained and ii sound 
propagation based localization which shows the results of 
wavepropagation-based localization 
 effective detection distance evaluation 
we firstly evaluate the sequence flip phenomenon in wave 
propagation as shown in figure motes were placed as 
five groups in front of the speaker four nodes in each group 
at roughly the same distances to the speaker the gap between 
each group is set to be and feet respectively in four 
experiments figure shows the results the x-axis in each 
subgraph indicates the group index there are four nodes in each 
group bars the y-axis shows the detection rank order 
of each node in the node sequence as distance between each 
group increases number of flips in the resulting node sequence 
 
figure wave detection 
 
 
 
 
 
 
 feet group distance 
rank 
group index 
 
 
 
 
 
 
 feet group distance 
rank 
group index 
 
 
 
 
 
 
 feet group distance 
rank 
group index 
 
 
 
 
 
 
 feet group distance 
rank 
group index 
figure ranks vs distances 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
y-dimension feet 
x-dimension feet 
node 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
y-dimension feet 
x-dimension feet 
anchor 
figure localization error sound 
decreases for example in the -foot distance subgraph there 
are quite a few flips between nodes in adjacent and even 
nonadjacent groups while in the -foot subgraph flips between 
different groups disappeared in the test 
 sound propagation based localization 
as shown in figure motes are placed as a grid 
including rows with feet between each row and columns with 
 feet between each column six khz acoustic wave 
propagation events are generated around the mote grid by a speaker 
figure shows the localization results using iterative msp 
 times iterative processing with a protection band of feet 
the average error of the localization results is feet and the 
maximum error is feet with one un-localized node 
we found that sequence flip in wave propagation is more 
severe than that in the indoor line-based test this is expected 
due to the high propagation speed of sound currently we use 
micaz mote which is equipped with a low quality 
microphone we believe that using a better speaker and more events 
the system can yield better accuracy despite the hardware 
constrains the msp algorithm still successfully localized most of 
the nodes with good accuracy 
 conclusions 
in this paper we present the first work that exploits the 
concept of node sequence processing to localize sensor nodes we 
demonstrated that we could significantly improve localization 
accuracy by making full use of the information embedded in 
multiple easy-to-get one-dimensional node sequences we 
proposed four novel optimization methods exploiting order and 
marginal distribution among non-anchor nodes as well as the 
feedback information from early localization results 
importantly these optimization methods can be used together and 
improve accuracy additively the practical issues of partial 
node sequence and sequence flip were identified and addressed 
in two physical system test-beds we also evaluated 
performance at scale through analysis as well as extensive 
simulations results demonstrate that requiring neither costly 
hardware on sensor nodes nor precise event distribution msp can 
achieve a sub-foot accuracy with very few anchor nodes 
provided sufficient events 
 references 
 cc data sheet avaiable at http www chipcon com 
 p bahl and v n padmanabhan radar an in-building rf-based user 
location and tracking system in ieee infocom 
 m broxton j lifton and j paradiso localizing a sensor network via 
collaborative processing of global stimuli in ewsn 
 n bulusu j heidemann and d estrin gps-less low cost outdoor 
localization for very small devices ieee personal communications 
magazine 
 d culler d estrin and m srivastava overview of sensor networks 
ieee computer magazine 
 j elson l girod and d estrin fine-grained network time 
synchronization using reference broadcasts in osdi 
 d k goldenberg p bihler m gao j fang b d anderson a morse 
and y yang localization in sparse networks using sweeps in 
mobicom 
 t he c huang b m blum j a stankovic and t abdelzaher 
rangefree localization schemes in large-scale sensor networks in 
mobicom 
 b kusy p dutta p levis m mar a ledeczi and d culler elapsed 
time on arrival a simple and versatile primitive for canonical time 
synchronization services international journal of ad-hoc and 
ubiquitous computing 
 l lazos and r poovendran serloc secure range-independent 
localization for wireless sensor networks in wise 
 m maroti b kusy g balogh p volgyesi a nadas k molnar 
s dora and a ledeczi radio interferometric geolocation in 
sensys 
 m maroti b kusy g simon and a ledeczi the flooding time 
synchronization protocol in sensys 
 d moore j leonard d rus and s teller robust distributed network 
localization with noise range measurements in sensys 
 r nagpal and d coore an algorithm for group formation in an 
amorphous computer in pdcs 
 d niculescu and b nath ad-hoc positioning system in globecom 
 
 d niculescu and b nath ad-hoc positioning system aps using 
aoa in infocom 
 n b priyantha a chakraborty and h balakrishnan the cricket 
location-support system in mobicom 
 k r¨omer the lighthouse location system for smart dust in mobisys 
 
 a savvides c c han and m b srivastava dynamic fine-grained 
localization in ad-hoc networks of sensors in mobicom 
 r stoleru t he j a stankovic and d luebke a high-accuracy 
low-cost localization system for wireless sensor networks in sensys 
 
 r stoleru p vicaire t he and j a stankovic stardust a flexible 
architecture for passive localization in wireless sensor networks in 
sensys 
 e w weisstein plane division by lines mathworld wolfram com 
 b h wellenhoff h lichtenegger and j collins global positions 
system theory and practice fourth edition springer verlag 
 k whitehouse the design of calamari an ad-hoc localization system 
for sensor networks in university of california at berkeley 
 z zhong msp evaluation and implementation report avaiable at 
http www cs umn edu ∼zhong msp 
 g zhou t he and j a stankovic impact of radio irregularity on 
wireless sensor networks in mobisys 
 
an evaluation of availability latency in carrier-based 
vehicular ad-hoc networks 
shahram 
ghandeharizadeh 
dept of computer science 
univ of southern california 
los angeles ca usa 
shahram usc edu 
shyam kapadia 
dept of computer science 
univ of southern california 
los angeles ca usa 
kapadia usc edu 
bhaskar krishnamachari 
dept of computer science 
dept of electrical engineering 
univ of southern california 
los angeles ca usa 
bkrishna usc edu 
abstract 
on-demand delivery of audio and video clips in peer-to-peer 
vehicular ad-hoc networks is an emerging area of research our target 
environment uses data carriers termed zebroids where a mobile 
device carries a data item on behalf of a server to a client thereby 
minimizing its availability latency in this study we quantify the 
variation in availability latency with zebroids as a function of a rich 
set of parameters such as car density storage per device repository 
size and replacement policies employed by zebroids using 
analysis and extensive simulations we gain novel insights into the 
design of carrier-based systems significant improvements in latency 
can be obtained with zebroids at the cost of a minimal overhead 
these improvements occur even in scenarios with lower accuracy 
in the predictions of the car routes two particularly surprising 
findings are a naive random replacement policy employed by 
the zebroids shows competitive performance and latency 
improvements obtained with a simplified instantiation of zebroids are 
found to be robust to changes in the popularity distribution of the 
data items 
categories and subject descriptors c distributed systems 
client server 
general terms algorithms performance design 
experimentation 
 introduction 
technological advances in areas of storage and wireless 
communications have now made it feasible to envision on-demand delivery 
of data items for e g video and audio clips in vehicular 
peer-topeer networks in prior work ghandeharizadeh et al 
introduce the concept of vehicles equipped with a 
car-to-car-peer-topeer device termed automata for in-vehicle entertainment the 
notable features of an automata include a mass storage device 
offering hundreds of gigabytes gb of storage a fast processor and 
several types of networking cards even with today s gb disk 
drives a repository of diverse entertainment content may exceed 
the storage capacity of a single automata such repositories 
constitute the focus of this study to exchange data we assume each 
automata is configured with two types of networking cards a 
low-bandwidth networking card with a long radio-range in the 
order of miles that enables an automata device to communicate with 
a nearby cellular or wimax station a high-bandwidth 
networking card with a limited radio-range in the order of hundreds of feet 
the high bandwidth connection supports data rates in the 
order of tens to hundreds of megabits per second and represents the 
ad-hoc peer to peer network between the vehicles this is 
labelled as the data plane and is employed to exchange data items 
between devices the low-bandwidth connection serves as the 
control plane enabling automata devices to exchange meta-data with 
one or more centralized servers this connection offers bandwidths 
in the order of tens to hundreds of kilobits per second the 
centralized servers termed dispatchers compute schedules of data 
delivery along the data plane using the provided meta-data these 
schedules are transmitted to the participating vehicles using the 
control plane the technical feasibility of such a two-tier 
architecture is presented in with preliminary results to demonstrate the 
bandwidth of the control plane is sufficient for exchange of control 
information needed for realizing such an application 
in a typical scenario an automata device presents a passenger 
with a list of data items 
 showing both the name of each data item 
and its availability latency the latter denoted as δ is defined as 
the earliest time at which the client encounters a copy of its 
requested data item a data item is available immediately when it 
resides in the local storage of the automata device serving the 
request due to storage constraints an automata may not store the 
entire repository in this case availability latency is the time from 
when the user issues a request until when the automata encounters 
another car containing the referenced data item the terms car and 
automata are used interchangeably in this study 
the availability latency for an item is a function of the current 
location of the client its destination and travel path the mobility 
model of the automata equipped vehicles the number of replicas 
constructed for the different data items and the placement of data 
item replicas across the vehicles a method to improve the 
availability latency is to employ data carriers which transport a replica 
of the requested data item from a server car containing it to a client 
that requested it these data carriers are termed  zebroids 
selection of zebroids is facilitated by the two-tiered architecture 
the control plane enables centralized information gathering at a 
dispatcher present at a base-station 
some examples of control 
in 
without loss of generality the term data item might be either 
traditional media such as text or continuous media such as an audio or 
video clip 
 
there may be dispatchers deployed at a subset of the base-stations 
for fault-tolerance and robustness dispatchers between 
basestations may communicate via the wired infrastructure 
 
formation are currently active requests travel path of the clients and 
their destinations and paths of the other cars for each client 
request the dispatcher may choose a set of z carriers that collaborate 
to transfer a data item from a server to a client z-relay zebroids 
here z is the number of zebroids such that ≤ z n where 
n is the total number of cars when z there are no carriers 
requiring a server to deliver the data item directly to the client 
otherwise the chosen relay team of z zebroids hand over the data item 
transitively to one another to arrive at the client thereby reducing 
availability latency see section for details to increase 
robustness the dispatcher may employ multiple relay teams of z-carriers 
for every request this may be useful in scenarios where the 
dispatcher has lower prediction accuracy in the information about the 
routes of the cars finally storage constraints may require a zebroid 
to evict existing data items from its local storage to accommodate 
the client requested item 
in this study we quantify the following main factors that affect 
availability latency in the presence of zebroids i data item 
repository size ii car density iii storage capacity per car iv client 
trip duration v replacement scheme employed by the zebroids 
and vi accuracy of the car route predictions for a significant 
subset of these factors we address some key questions pertaining to 
use of zebroids both via analysis and extensive simulations 
our main findings are as follows a naive random replacement 
policy employed by the zebroids shows competitive performance 
in terms of availability latency with such a policy substantial 
improvements in latency can be obtained with zebroids at a minimal 
replacement overhead in more practical scenarios where the 
dispatcher has inaccurate information about the car routes zebroids 
continue to provide latency improvements a surprising result is 
that changes in popularity of the data items do not impact the 
latency gains obtained with a simple instantiation of z-relay zebroids 
called one-instantaneous zebroids see section this study 
suggests a number of interesting directions to be pursued to gain 
better understanding of design of carrier-based systems that 
improve availability latency 
related work replication in mobile ad-hoc networks has been 
a widely studied topic however none of these 
studies employ zebroids as data carriers to reduce the latency of the 
client s requests several novel and important studies such as 
zebranet daknet data mules message ferries 
and seek and focus have analyzed factors impacting 
intermittently connected networks consisting of data carriers similar in 
spirit to zebroids factors considered by each study are dictated by 
their assumed environment and target application a novel 
characteristic of our study is the impact on availability latency for a 
given database repository of items a detailed description of 
related works can be obtained in 
the rest of this paper is organized as follows section 
provides an overview of the terminology along with the factors that 
impact availability latency in the presence of zebroids section 
describes how the zebroids may be employed section provides 
details of the analysis methodology employed to capture the 
performance with zebroids section describes the details of the 
simulation environment used for evaluation section enlists the key 
questions examined in this study and answers them via analysis 
and simulations finally section presents brief conclusions and 
future research directions 
 overview and terminology 
table summarizes the notation of the parameters used in the 
paper below we introduce some terminology used in the paper 
assume a network of n automata-equipped cars each with 
storage capacity of α bytes the total storage capacity of the 
system is st n ·α there are t data items in the database each with 
database parameters 
t number of data items 
si size of data item i 
fi frequency of access to data item i 
replication parameters 
ri normalized frequency of access to data item i 
ri number of replicas for data item i 
n characterizes a particular replication scheme 
δi average availability latency of data item i 
δagg aggregate availability latency δagg t 
j δj · fj 
automata system parameters 
g number of cells in the map d-torus 
n number of automata devices in the system 
α storage capacity per automata 
γ trip duration of the client automata 
st total storage capacity of the automata system st n · α 
table terms and their definitions 
size si the frequency of access to data item i is denoted as fi 
with t 
j fj let the trip duration of the client automata 
under consideration be γ 
we now define the normalized frequency of access to the data 
item i denoted by ri as 
ri 
 fi n 
t 
j fj n 
 ≤ n ≤ ∞ 
the exponent n characterizes a particular replication technique 
a square-root replication scheme is realized when n 
this serves as the base-line for comparison with the case when 
zebroids are deployed ri is normalized to a value between and 
 the number of replicas for data item i denoted as ri is ri 
min n max ri·n·α 
si 
 this captures the case when at least 
one copy of every data item must be present in the ad-hoc network 
at all times in cases where a data item may be lost from the ad-hoc 
network this equation becomes ri min n max ri·n·α 
si 
 
in this case a request for the lost data item may need to be satisfied 
by fetching the item from a remote server 
the availability latency for a data item i denoted as δi is defined 
as the earliest time at which a client automata will find the first 
replica of the item accessible to it if this condition is not satisfied 
then we set δi to γ this indicates that data item i was not available 
to the client during its journey note that since there is at least one 
replica in the system for every data item i by setting γ to a large 
value we ensure that the client s request for any data item i will be 
satisfied however in most practical circumstances γ may not be 
so large as to find every data item 
we are interested in the availability latency observed across all 
data items hence we augment the average availability latency 
for every data item i with its fi to obtain the following weighted 
availability latency δagg metric δagg t 
i δi · fi 
next we present our solution approach describing how zebroids 
are selected 
 solution approach 
 zebroids 
when a client references a data item missing from its local 
storage the dispatcher identifies all cars with a copy of the data item 
as servers next the dispatcher obtains the future routes of all cars 
for a finite time duration equivalent to the maximum time the client 
is willing to wait for its request to be serviced using this 
information the dispatcher schedules the quickest delivery path from any 
of the servers to the client using any other cars as intermediate 
carriers hence it determines the optimal set of forwarding decisions 
 
that will enable the data item to be delivered to the client in the 
minimum amount of time note that the latency along the quickest 
delivery path that employs a relay team of z zebroids is similar to 
that obtained with epidemic routing under the assumptions of 
infinite storage and no interference 
a simple instantiation of z-relay zebroids occurs when z 
and the client s request triggers a transfer of a copy of the requested 
data item from a server to a zebroid in its vicinity such a 
zebroid is termed one-instantaneous zebroid in some cases the 
dispatcher might have inaccurate information about the routes of 
the cars hence a zebroid scheduled on the basis of this inaccurate 
information may not rendezvous with its target client to minimize 
the likelihood of such scenarios the dispatcher may schedule 
multiple zebroids this may incur additional overhead due to redundant 
resource utilization to obtain the same latency improvements 
the time required to transfer a data item from a server to a 
zebroid depends on its size and the available link bandwidth with 
small data items it is reasonable to assume that this transfer time is 
small especially in the presence of the high bandwidth data plane 
large data items may be divided into smaller chunks enabling the 
dispatcher to schedule one or more zebroids to deliver each chunk 
to a client in a timely manner this remains a future research 
direction 
initially number of replicas for each data item replicas might be 
computed using equation this scheme computes the number 
of data item replicas as a function of their popularity it is static 
because number of replicas in the system do not change and no 
replacements are performed hence this is referred to as the 
 nozebroids environment we quantify the performance of the various 
replacement policies with reference to this base-line that does not 
employ zebroids 
one may assume a cold start phase where initially only one or 
few copies of every data item exist in the system many storage 
slots of the cars may be unoccupied when the cars encounter one 
another they construct new replicas of some selected data items to 
occupy the empty slots the selection procedure may be to choose 
the data items uniformly at random new replicas are created as 
long as a car has a certain threshold of its storage unoccupied 
eventually majority of the storage capacity of a car will be 
exhausted 
 carrier-based replacement policies 
the replacement policies considered in this paper are reactive 
since a replacement occurs only in response to a request issued for a 
certain data item when the local storage of a zebroid is completely 
occupied it needs to replace one of its existing items to carry the 
client requested data item for this purpose the zebroid must 
select an appropriate candidate for eviction this decision process 
is analogous to that encountered in operating system paging where 
the goal is to maximize the cache hit ratio to prevent disk access 
delay the carrier-based replacement policies employed in our 
study are least recently used lru least frequently used 
 lfu and random where a eviction candidate is chosen 
uniformly at random we have considered local and global variants 
of the lru lfu policies which determine whether local or global 
knowledge of contents of the cars known at the dispatcher is used 
for the eviction decision at a zebroid see for more details 
the replacement policies incur the following overheads first 
the complexity associated with the implementation of a policy 
second the bandwidth used to transfer a copy of a data item from a 
server to the zebroid third the average number of replacements 
incurred by the zebroids note that in the no-zebroids case neither 
overhead is incurred 
the metrics considered in this study are aggregate availability 
latency δagg percentage improvement in δagg with zebroids as 
compared to the no-zebroids case and average number of replacements 
incurred per client request which is an indicator of the overhead 
incurred by zebroids 
note that the dispatchers with the help of the control plane may 
ensure that no data item is lost from the system in other words 
at least one replica of every data item is maintained in the ad-hoc 
network at all times in such cases even though a car may meet a 
requesting client earlier than other servers if its local storage 
contains data items with only a single copy in the system then such a 
car is not chosen as a zebroid 
 analysis methodology 
here we present the analytical evaluation methodology and some 
approximations as closed-form equations that capture the 
improvements in availability latency that can be obtained with both 
oneinstantaneous and z-relay zebroids first we present some 
preliminaries of our analysis methodology 
 let n be the number of cars in the network performing a d 
random walk on a 
√ 
g× 
√ 
g torus an additional car serves 
as a client yielding a total of n cars such a mobility 
model has been used widely in the literature chiefly 
because it is amenable to analysis and provides a baseline 
against which performance of other mobility models can be 
compared moreover this class of markovian mobility 
models has been used to model the movements of vehicles 
 
 we assume that all cars start from the stationary distribution 
and perform independent random walks although for sparse 
density scenarios the independence assumption does hold it 
is no longer valid when n approaches g 
 let the size of data item repository of interest be t also 
data item i has ri replicas this implies ri cars identified as 
servers have a copy of this data item when the client requests 
item i 
all analysis results presented in this section are obtained 
assuming that the client is willing to wait as long as it takes for its request 
to be satisfied unbounded trip duration γ ∞ with the random 
walk mobility model on a d-torus there is a guarantee that as 
long as there is at least one replica of the requested data item in the 
network the client will eventually encounter this replica 
extensions to the analysis that also consider finite trip durations can 
be obtained in 
consider a scenario where no zebroids are employed in this 
case the expected availability latency for the data item is the 
expected meeting time of the random walk undertaken by the client 
with any of the random walks performed by the servers aldous et 
al show that the the meeting time of two random walks in such 
a setting can be modelled as an exponential distribution with the 
mean c c · g · log g where the constant c for g ≥ 
the meeting time or equivalently the availability latency δi for 
the client requesting data item i is the time till it encounters any of 
these ri replicas for the first time this is also an exponential 
distribution with the following expected value note that this formulation 
is valid only for sparse cases when g ri δi cglogg 
ri 
the aggregate availability latency without employing zebroids is 
then this expression averaged over all data items weighted by their 
frequency of access 
δagg no − zeb 
t 
i 
fi · c · g · log g 
ri 
 
t 
i 
fi · c 
ri 
 
 
 one-instantaneous zebroids 
recall that with one-instantaneous zebroids for a given request 
a new replica is created on a car in the vicinity of the server 
provided this car meets the client earlier than any of the ri servers 
moreover this replica is spawned at the time step when the client 
issues the request let nc 
i be the expected total number of nodes 
that are in the same cell as any of the ri servers then we have 
nc 
i n − ri · − − 
 
g 
 ri 
 
in the analytical model we assume that nc 
i new replicas are 
created so that the total number of replicas is increased to ri nc 
i 
the availability latency is reduced since the client is more likely to 
meet a replica earlier the aggregated expected availability latency 
in the case of one-instantaneous zebroids is then given by 
δagg zeb 
t 
i 
fi · c · g · log g 
ri nc 
i 
 
t 
i 
fi · c 
ri nc 
i 
 
note that in obtaining this expression for ease of analysis we 
have assumed that the new replicas start from random locations 
in the torus not necessarily from the same cell as the original ri 
servers it thus treats all the nc 
i carriers independently just like 
the ri original servers as we shall show below by comparison 
with simulations this approximation provides an upper-bound on 
the improvements that can be obtained because it results in a lower 
expected latency at the client 
it should be noted that the procedure listed above will yield a 
similar latency to that employed by a dispatcher employing 
oneinstantaneous zebroids see section since the dispatcher is 
aware of all future car movements it would only transfer the 
requested data item on a single zebroid if it determines that the 
zebroid will meet the client earlier than any other server this selected 
zebroid is included in the nc 
i new replicas 
 z-relay zebroids 
to calculate the expected availability latency with z-relay 
zebroids we use a coloring problem analog similar to an approach 
used by spyropoulos et al details of the procedure to obtain 
a closed-form expression are given in the aggregate 
availability latency δagg with z-relay zebroids is given by 
δagg zeb 
t 
i 
 fi · 
c 
n 
· 
 
n − ri 
· 
 n · log 
n 
ri 
− log n − ri 
 simulation methodology 
the simulation environment considered in this study comprises 
of vehicles such as cars that carry a fraction of the data item 
repository a prediction accuracy parameter inherently provides a certain 
probabilistic guarantee on the confidence of the car route 
predictions known at the dispatcher a value of implies that the 
exact routes of all cars are known at all times a value for this 
parameter indicates that the routes predicted for the cars will match 
the actual ones with probability note that this probability is 
spread across the car routes for the entire trip duration we now 
provide the preliminaries of the simulation study and then describe 
the parameter settings used in our experiments 
 similar to the analysis methodology the map used is a d 
torus a markov mobility model representing a unbiased d 
random walk on the surface of the torus describes the 
movement of the cars across this torus 
 each grid cell is a unique state of this markov chain in each 
time slot every car makes a transition from a cell to any of 
its neighboring cells the transition is a function of the 
current location of the car and a probability transition matrix 
q qij where qij is the probability of transition from state 
i to state j only automata equipped cars within the same 
cell may communicate with each other 
 the parameters γ δ have been discretized and expressed in 
terms of the number of time slots 
 an automata device does not maintain more than one replica 
of a data item this is because additional replicas occupy 
storage without providing benefits 
 either one-instantaneous or z-relay zebroids may be employed 
per client request for latency improvement 
 unless otherwise mentioned the prediction accuracy 
parameter is assumed to be this is because this study 
aims to quantify the effect of a large number of parameters 
individually on availability latency 
here we set the size of every data item si to be α represents 
the number of storage slots per automata each storage slot stores 
one data item γ represents the duration of the client s journey in 
terms of the number of time slots hence the possible values of 
availability latency are between and γ δ is defined as the number 
of time slots after which a client automata device will encounter a 
replica of the data item for the first time if a replica for the data 
item requested was encountered by the client in the first cell then 
we set δ if δ γ then we set δ γ indicating that no copy 
of the requested data item was encountered by the client during its 
entire journey in all our simulations for illustration we consider a 
 × d-torus with γ set to our experiments indicate that the 
trends in the results scale to maps of larger size 
we simulated a skewed distribution of access to the t data items 
that obeys zipf s law with a mean of this distribution is 
shown to correspond to sale of movie theater tickets in the united 
states we employ a replication scheme that allocates replicas 
for a data item as a function of the square-root of the frequency of 
access of that item the square-root replication scheme is shown 
to have competitive latency performance over a large parameter 
space the data item replicas are distributed uniformly across 
the automata devices this serves as the base-line no-zebroids 
case the square-root scheme also provides the initial replica 
distribution when zebroids are employed note that the replacements 
performed by the zebroids will cause changes to the data item replica 
distribution requests generated as per the zipf distribution are 
issued one at a time the client car that issues the request is chosen 
in a round-robin manner after a maximum period of γ the latency 
encountered by this request is recorded 
in all the simulation results each point is an average of 
requests moreover the confidence intervals determined for 
the results are quite tight for the metrics of latency and 
replacement overhead hence we only present them for the metric that 
captures the percentage improvement in latency with respect to the 
no-zebroids case 
 results 
in this section we describe our evaluation results where the 
following key questions are addressed with a wide choice of 
replacement schemes available for a zebroid what is their effect on 
availability latency a more central question is do zebroids provide 
 
 
 
 
 
 
 
number of cars 
aggregate availability latency δ 
agg 
 
lru global 
lfu global 
lru local 
lfu local 
random 
figure figure shows the availability latency when 
employing one-instantaneous zebroids as a function of n α values 
when the total storage in the system is kept fixed st 
significant improvements in availability latency what is the 
associated overhead incurred in employing these zebroids what 
happens to these improvements in scenarios where a dispatcher may 
have imperfect information about the car routes what inherent 
trade-offs exist between car density and storage per car with 
regards to their combined as well as individual effect on availability 
latency in the presence of zebroids we present both simple 
analysis and detailed simulations to provide answers to these questions 
as well as gain insights into design of carrier-based systems 
 how does a replacement scheme employed 
by a zebroid impact availability latency 
for illustration we present  scale-up experiments where 
oneinstantaneous zebroids are employed see figure by scale-up 
we mean that α and n are changed proportionally to keep the total 
system storage st constant here t and st we 
choose the following values of n α { 
 } the figure indicates that a random replacement scheme 
shows competitive performance this is because of several reasons 
recall that the initial replica distribution is set as per the 
squareroot replication scheme the random replacement scheme does not 
alter this distribution since it makes replacements blind to the 
popularity of a data item however the replacements cause dynamic 
data re-organization so as to better serve the currently active 
request moreover the mobility pattern of the cars is random hence 
the locations from which the requests are issued by clients are also 
random and not known a priori at the dispatcher these findings 
are significant because a random replacement policy can be 
implemented in a simple decentralized manner 
the lru-global and lfu-global schemes provide a latency 
performance that is worse than random this is because these 
policies rapidly develop a preference for the more popular data items 
thereby creating a larger number of replicas for them during 
eviction the more popular data items are almost never selected as a 
replacement candidate consequently there are fewer replicas for 
the less popular items hence the initial distribution of the data 
item replicas changes from square-root to that resembling linear 
replication the higher number of replicas for the popular data 
items provide marginal additional benefits while the lower number 
of replicas for the other data items hurts the latency performance of 
these global policies the lfu-local and lru-local schemes have 
similar performance to random since they do not have enough history 
of local data item requests we speculate that the performance of 
these local policies will approach that of their global variants for a 
large enough history of data item requests on account of the 
competitive performance shown by a random policy for the remainder 
of the paper we present the performance of zebroids that employ a 
random replacement policy 
 do zebroids provide significant 
improvements in availability latency 
we find that in many scenarios employing zebroids provides 
substantial improvements in availability latency 
 analysis 
we first consider the case of one-instantaneous zebroids 
figure a shows the variation in δagg as a function of n for t 
and α with a × torus using equation both the x and y 
axes are drawn to a log-scale figure b show the improvement 
in δagg obtained with one-instantaneous zebroids in this case only 
the x-axis is drawn to a log-scale for illustration we assume that 
the t data items are requested uniformly 
initially when the network is sparse the analytical approximation 
for improvements in latency with zebroids obtained from 
equations and closely matches the simulation results however as 
n increases the sparseness assumption for which the analysis is 
valid namely n g is no longer true hence the two curves 
rapidly diverge the point at which the two curves move away from 
each other corresponds to a value of δagg ≤ moreover as 
mentioned earlier the analysis provides an upper bound on the latency 
improvements as it treats the newly created replicas given by nc 
i 
independently however these nc 
i replicas start from the same cell 
as one of the server replicas ri finally the analysis captures a 
oneshot scenario where given an initial data item replica distribution 
the availability latency is computed the new replicas created do 
not affect future requests from the client 
on account of space limitations here we summarize the 
observations in the case when z-relay zebroids are employed the 
interested reader can obtain further details in similar observations 
like the one-instantaneous zebroid case apply since the simulation 
and analysis curves again start diverging when the analysis 
assumptions are no longer valid however the key observation here is that 
the latency improvement with z-relay zebroids is significantly 
better than the one-instantaneous zebroids case especially for lower 
storage scenarios this is because in sparse scenarios the 
transitive hand-offs between the zebroids creates higher number of 
replicas for the requested data item yielding lower availability latency 
moreover it is also seen that the simulation validation curve for the 
improvements in δagg with z-relay zebroids approaches that of the 
one-instantaneous zebroid case for higher storage higher n 
values this is because one-instantaneous zebroids are a special case 
of z-relay zebroids 
 simulation 
we conduct simulations to examine the entire storage spectrum 
obtained by changing car density n or storage per car α to also 
capture scenarios where the sparseness assumptions for which the 
analysis is valid do not hold we separate the effect of n and α 
by capturing the variation of n while keeping α constant case 
 and vice-versa case both with z-relay and one-instantaneous 
zebroids here we set the repository size as t figure 
captures case mentioned above similar trends are observed with 
case a complete description of those results are available in 
with figure b keeping α constant initially increasing car 
density has higher latency benefits because increasing n introduces 
more zebroids in the system as n is further increased ω reduces 
because the total storage in the system goes up consequently the 
number of replicas per data item goes up thereby increasing the 
 
number of servers hence the replacement policy cannot find a 
zebroid as often to transport the requested data item to the client 
earlier than any of the servers on the other hand the increased 
number of servers benefits the no-zebroids case in bringing δagg 
down the net effect results in reduction in ω for larger values of 
n 
 
 
 
 
 
 
 
− 
 
 
 
 
 
 
number of cars 
no−zebroidsanal 
no−zebroids 
sim 
one−instantaneous 
anal 
one−instantaneoussim 
aggregate availability latency δagg 
 
 a δagg 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
number of cars 
 improvement in δagg 
wrt no−zebroids ω 
analytical upper−bound 
simulation 
 b ω 
figure figure shows the latency performance with 
oneinstantaneous zebroids via simulations along with the 
analytical approximation for a × torus with t 
the trends mentioned above are similar to that obtained from the 
analysis however somewhat counter-intuitively with relatively 
higher system storage z-relay zebroids provide slightly lower 
improvements in latency as compared to one-instantaneous zebroids 
we speculate that this is due to the different data item replica 
distributions enforced by them note that replacements performed by 
the zebroids cause fluctuations in these replica distributions which 
may effect future client requests we are currently exploring 
suitable choices of parameters that can capture these changing replica 
distributions 
 what is the overhead incurred with 
improvements in latency with zebroids 
we find that the improvements in latency with zebroids are 
obtained at a minimal replacement overhead per client request 
 analysis 
with one-instantaneous zebroids for each client request a 
maximum of one zebroid is employed for latency improvement hence 
the replacement overhead per client request can amount to a 
maximum of one recall that to calculate the latency with one-instantaneous 
 
 
 
 
 
 
 
 
number of cars 
aggregate availability latency δagg 
 
no−zebroids 
one−instantaneous 
z−relays 
 a 
 
 
 
 
 
 
 
 
number of cars 
 improvement in δagg 
wrt no−zebroids ω 
one−instantaneous 
z−relays 
 b 
figure figure depicts the latency performance with both 
one-instantaneous and z-relay zebroids as a function of the car 
density when α and t 
zebroids nc 
i new replicas are created in the same cell as the servers 
now a replacement is only incurred if one of these nc 
i newly 
created replicas meets the client earlier than any of the ri servers 
let xri and xnc 
i 
respectively be random variables that capture 
the minimum time till any of the ri and nc 
i replicas meet the client 
since xri and xnc 
i 
are assumed to be independent by the property 
of exponentially distributed random variables we have 
overhead request · p xnc 
i 
 xri · p xri ≤ xnc 
i 
 
 
overhead request 
ri 
c 
ri 
c 
 
nc 
i 
c 
 
ri 
ri nc 
i 
 
recall that the number of replicas for data item i ri is a function 
of the total storage in the system i e ri k·n ·α where k satisfies 
the constraint ≤ ri ≤ n using this along with equation we 
get 
overhead request − 
g 
g n · − k · α 
 
now if we keep the total system storage n · α constant since 
g and t are also constant increasing n increases the replacement 
overhead however if n ·α is constant then increasing n causes α 
 
 
 
 
 
 
 
 
 
 
 
 
 
number of cars 
one−instantaneous 
zebroids 
average number of replacements per request 
 n α 
 n α 
 n α 
 n α 
figure figure captures replacement overhead when 
employing one-instantaneous zebroids as a function of n α 
values when the total storage in the system is kept fixed st 
 
to go down this implies that a higher replacement overhead is 
incurred for higher n and lower α values moreover when ri n 
this means that every car has a replica of data item i hence no 
zebroids are employed when this item is requested yielding an 
overhead request for this item as zero next we present 
simulation results that validate our analysis hypothesis for the overhead 
associated with deployment of one-instantaneous zebroids 
 simulation 
figure shows the replacement overhead with one-instantaneous 
zebroids when n α are varied while keeping the total system 
storage constant the trends shown by the simulation are in agreement 
with those predicted by the analysis above however the total 
system storage can be changed either by varying car density n or 
storage per car α on account of similar trends here we present 
the case when α is kept constant and n is varied figure we 
refer the reader to for the case when α is varied and n is held 
constant 
we present an intuitive argument for the behavior of the 
perrequest replacement overhead curves when the storage is extremely 
scarce so that only one replica per data item exists in the automata 
network the number of replacements performed by the zebroids is 
zero since any replacement will cause a data item to be lost from 
the system the dispatcher ensures that no data item is lost from 
the system at the other end of the spectrum if storage becomes 
so abundant that α t then the entire data item repository can 
be replicated on every car the number of replacements is again 
zero since each request can be satisfied locally a similar scenario 
occurs if n is increased to such a large value that another car with 
the requested data item is always available in the vicinity of the 
client however there is a storage spectrum in the middle where 
replacements by the scheduled zebroids result in improvements in 
δagg see figure 
moreover we observe that for sparse storage scenarios the higher 
improvements with z-relay zebroids are obtained at the cost of a 
higher replacement overhead when compared to the one-instantaneous 
zebroids case this is because in the former case each of these z 
zebroids selected along the lowest latency path to the client needs 
to perform a replacement however the replacement overhead is 
still less than indicating that on an average less than one 
replacement per client request is needed even when z-relay zebroids are 
employed 
 
 
 
 
 
 
 
 
 
 
 
 
number of cars 
z−relays 
one−instantaneous 
average number of replacements per request 
figure figure shows the replacement overhead with 
zebroids for the cases when n is varied keeping α 
 
 
 
 
 
 
 
 
 
 
prediction percentage 
no−zebroids n 
one−instantaneous n 
z−relays n 
no−zebroids n 
one−instantaneous n z−relays n 
aggregate availability latency δagg 
 
figure figure shows δagg for different car densities as a 
function of the prediction accuracy metric with α and t 
 
 what happens to the availability latency 
with zebroids in scenarios with 
inaccuracies in the car route predictions 
we find that zebroids continue to provide improvements in 
availability latency even with lower accuracy in the car route 
predictions we use a single parameter p to quantify the accuracy of the 
car route predictions 
 analysis 
since p represents the probability that a car route predicted by the 
dispatcher matches the actual one hence the latency with zebroids 
can be approximated by 
δerr 
agg p · δagg zeb − p · δagg no − zeb 
δerr 
agg p · δagg zeb − p · 
c 
ri 
 
expressions for δagg zeb can be obtained from equations 
 one-instantaneous or z-relay zebroids 
 simulation 
figure shows the variation in δagg as a function of this route 
prediction accuracy metric we observe a smooth reduction in the 
 
improvement in δagg as the prediction accuracy metric reduces for 
zebroids that are scheduled but fail to rendezvous with the client 
due to the prediction error we tag any such replacements made by 
the zebroids as failed it is seen that failed replacements gradually 
increase as the prediction accuracy reduces 
 under what conditions are the 
improvements in availability latency with zebroids 
maximized 
surprisingly we find that the improvements in latency obtained 
with one-instantaneous zebroids are independent of the input 
distribution of the popularity of the data items 
 analysis 
the fractional difference labelled ω in the latency between the 
no-zebroids and one-instantaneous zebroids is obtained from 
equations and as 
ω 
t 
i 
fi·c 
ri 
− t 
i 
fi·c 
ri n−ri · − − 
g ri 
 
t 
i 
fi·c 
ri 
 
here c c·g·log g this captures the fractional improvement 
in the availability latency obtained by employing one-instantaneous 
zebroids let α making the total storage in the system st 
n assuming the initial replica distribution is as per the 
squareroot replication scheme we have ri 
√ 
fi·n 
t 
j 
√ 
fj 
 hence we get 
fi 
k 
·r 
i 
n where k t 
j fj using this along with the 
approximation − x n 
 − n · x for small x we simplify the 
above equation to get ω − 
t 
i 
ri 
 
n−ri 
g 
t 
i ri 
in order to determine when the gains with one-instantaneous 
zebroids are maximized we can frame an optimization problem as 
follows maximize ω subject to t 
i ri st 
theorem with a square-root replication scheme 
improvements obtained with one-instantaneous zebroids are independent 
of the input popularity distribution of the data items see for 
proof 
 simulation 
we perform simulations with two different frequency 
distribution of data items uniform and zipfian with mean 
similar latency improvements with one-instantaneous zebroids are 
obtained in both cases this result has important implications in 
cases with biased popularity toward certain data items the 
aggregate improvements in latency across all data item requests still 
remain the same even in scenarios where the frequency of access 
to the data items changes dynamically zebroids will continue to 
provide similar latency improvements 
 conclusions and 
future research directions 
in this study we examined the improvements in latency that can 
be obtained in the presence of carriers that deliver a data item from 
a server to a client we quantified the variation in availability 
latency as a function of a rich set of parameters such as car density 
storage per car title database size and replacement policies 
employed by zebroids 
below we summarize some key future research directions we 
intend to pursue to better reflect reality we would like to validate the 
observations obtained from this study with some real world 
simulation traces of vehicular movements for example using 
corsim this will also serve as a validation for the utility of the 
markov mobility model used in this study we are currently 
analyzing the performance of zebroids on a real world data set 
comprising of an ad-hoc network of buses moving around a small 
neighborhood in amherst zebroids may also be used for delivery 
of data items that carry delay sensitive information with a certain 
expiry extensions to zebroids that satisfy such application 
requirements presents an interesting future research direction 
 acknowledgments 
this research was supported in part by an annenberg fellowship and nsf 
grants numbered cns- nets noss cns- career 
and iis- 
 references 
 federal highway administration corridor simulation version 
http www ops fhwa dot gov trafficanalysistools cors im htm 
 d aldous and j fill reversible markov chains and random walks 
on graphs under preparation 
 a bar-noy i kessler and m sidi mobile users to update or not 
to update in ieee infocom 
 j burgess b gallagher d jensen and b levine maxprop 
routing for vehicle-based disruption-tolerant networking in ieee 
infocom april 
 e cohen and s shenker replication strategies in unstructured 
peer-to-peer networks in sigcomm 
 a dan d dias r mukherjee d sitaram and r tewari buffering 
and caching in large-scale video servers in compcon 
 s ghandeharizadeh s kapadia and b krishnamachari pavan a 
policy framework for content availabilty in vehicular ad-hoc 
networks in vanet new york ny usa acm press 
 s ghandeharizadeh s kapadia and b krishnamachari 
comparison of replication strategies for content availability in 
c p networks in mdm may 
 s ghandeharizadeh s kapadia and b krishnamachari an 
evaluation of availability latency in carrier-based vehicular ad-hoc 
networks technical report department of computer science 
university of southern california ceng- - 
 s ghandeharizadeh and b krishnamachari c p a peer-to-peer 
network for on-demand automobile information services in globe 
ieee 
 t hara effective replica allocation in ad-hoc networks for 
improving data accessibility in ieee infocom 
 h hayashi t hara and s nishio a replica allocation method 
adapting to topology changes in ad-hoc networks in dexa 
 p juang h oki y wang m martonosi l peh and d rubenstein 
energy-efficient computing for wildlife tracking design tradeoffs 
and early experiences with zebranet sigarch comput archit 
news 
 a pentland r fletcher and a hasson daknet rethinking 
connectivity in developing nations computer - 
 f sailhan and v issarny cooperative caching in ad-hoc networks 
in mdm 
 r shah s roy s jain and w brunette data mules modeling and 
analysis of a three-tier architecture for sparse sensor networks 
elsevier ad-hoc networks journal september 
 t spyropoulos k psounis and c raghavendra single-copy 
routing in intermittently connected mobile networks in secon 
april 
 a tanenbaum modern operating systems nd edition chapter 
section prentice hall 
 a vahdat and d becker epidemic routing for partially-connected 
ad-hoc networks technical report department of computer science 
duke university 
 w zhao m ammar and e zegura a message ferrying approach 
for data delivery in sparse mobile ad-hoc networks in mobihoc 
pages - new york ny usa acm press 
 m zonoozi and p dassanayake user mobility modeling and 
characterization of mobility pattern ieee journal on selected 
areas in communications - september 
 
evaluating opportunistic routing protocols 
with large realistic contact traces 
libo song and david f kotz 
institute for security technology studies ists 
department of computer science dartmouth college hanover nh usa 
abstract 
traditional mobile ad-hoc network manet routing protocols 
assume that contemporaneous end-to-end communication paths exist 
between data senders and receivers in some mobile ad-hoc 
networks with a sparse node population an end-to-end 
communication path may break frequently or may not exist at any time many 
routing protocols have been proposed in the literature to address the 
problem but few were evaluated in a realistic opportunistic 
network setting we use simulation and contact traces derived from 
logs in a production network to evaluate and compare five 
existing protocols direct-delivery epidemic random prophet and 
link-state as well as our own proposed routing protocol we show 
that the direct delivery and epidemic routing protocols suffer either 
low delivery ratio or high resource usage and other protocols make 
tradeoffs between delivery ratio and resource usage 
categories and subject descriptors 
c computer systems organization computer 
communication networks-distributed systems 
general terms 
performance design 
 introduction 
mobile opportunistic networks are one kind of delay-tolerant 
network dtn delay-tolerant networks provide service 
despite long link delays or frequent link breaks long link delays 
happen in networks with communication between nodes at a great 
distance such as interplanetary networks link breaks are caused 
by nodes moving out of range environmental changes interference 
from other moving objects radio power-offs or failed nodes for 
us mobile opportunistic networks are those dtns with sparse node 
population and frequent link breaks caused by power-offs and the 
mobility of the nodes 
mobile opportunistic networks have received increasing interest 
from researchers in the literature these networks include mobile 
sensor networks wild-animal tracking networks 
pocketswitched networks and transportation networks we 
expect to see more opportunistic networks when the 
one-laptopper-child olpc project starts rolling out inexpensive 
laptops with wireless networking capability for children in developing 
countries where often no infrastructure exits opportunistic 
networking is one promising approach for those children to exchange 
information 
one fundamental problem in opportunistic networks is how to 
route messages from their source to their destination mobile 
opportunistic networks differ from the internet in that disconnections 
are the norm instead of the exception in mobile opportunistic 
networks communication devices can be carried by people 
vehicles or animals some devices can form a small mobile 
ad-hoc network when the nodes move close to each other but a 
node may frequently be isolated from other nodes note that 
traditional internet routing protocols and ad-hoc routing protocols such 
as aodv or dsdv assume that a contemporaneous 
endto-end path exists and thus fail in mobile opportunistic networks 
indeed there may never exist an end-to-end path between two given 
devices 
in this paper we study protocols for routing messages between 
wireless networking devices carried by people we assume that 
people send messages to other people occasionally using their 
devices when no direct link exists between the source and the 
destination of the message other nodes may relay the message to the 
destination each device represents a unique person it is out of the 
scope of this paper when a device maybe carried by multiple 
people each message is destined for a specific person and thus for 
a specific node carried by that person although one person may 
carry multiple devices we assume that the sender knows which 
device is the best to receive the message we do not consider 
multicast or geocast in this paper 
many routing protocols have been proposed in the literature 
few of them were evaluated in realistic network settings or even in 
realistic simulations due to the lack of any realistic people 
mobility model random walk or random way-point mobility models are 
often used to evaluate the performance of those routing protocols 
although these synthetic mobility models have received extensive 
interest by mobile ad-hoc network researchers they do not 
reflect people s mobility patterns realising the limitations of 
using random mobility models in simulations a few researchers have 
studied routing protocols in mobile opportunistic networks with 
realistic mobility traces chaintreau et al theoretically analyzed 
the impact of routing algorithms over a model derived from a 
realistic mobility data set su et al simulated a set of routing 
 
protocols in a small experimental network those studies help 
researchers better understand the theoretical limits of opportunistic 
networks and the routing protocol performance in a small network 
 - nodes 
deploying and experimenting large-scale mobile opportunistic 
networks is difficult we too resort to simulation instead of 
using a complex mobility model to mimic people s mobility patterns 
we used mobility traces collected in a production wireless 
network at dartmouth college to drive our simulation our 
messagegeneration model however was synthetic 
to the best of our knowledge we are the first to simulate the 
effect of routing protocols in a large-scale mobile opportunistic 
network using realistic contact traces derived from real traces of 
a production network with more than users 
using realistic contact traces we evaluate the performance of 
three naive routing protocols direct-delivery epidemic and 
random and two prediction-based routing protocols prophet 
and link-state we also propose a new prediction-based 
routing protocol and compare it to the above in our evaluation 
 routing protocol 
a routing protocol is designed for forwarding messages from one 
node source to another node destination any node may 
generate messages for any other node and may carry messages destined 
for other nodes in this paper we consider only messages that are 
unicast single destination 
dtn routing protocols could be described in part by their 
transfer probability and replication probability that is when one node 
meets another node what is the probability that a message should 
be transfered and if so whether the sender should retain its copy 
two extremes are the direct-delivery protocol and the epidemic 
protocol the former transfers with probability when the node 
meets the destination for others and no replication the latter 
uses transfer probability for all nodes and unlimited replication 
both these protocols have their advantages and disadvantages all 
other protocols are between the two extremes 
first we define the notion of contact between two nodes then 
we describe five existing protocols before presenting our own 
proposal 
a contact is defined as a period of time during which two nodes 
have the opportunity to communicate although we are aware that 
wireless technologies differ we assume that a node can reliably 
detect the beginning and end time of a contact with nearby nodes 
a node may be in contact with several other nodes at the same time 
the contact history of a node is a sequence of contacts with other 
nodes node i has a contact history hi j for each other node j 
which denotes the historical contacts between node i and node j 
we record the start and end time for each contact however the last 
contacts in the node s contact history may not have ended 
 direct delivery protocol 
in this simple protocol a message is transmitted only when the 
source node can directly communicate with the destination node 
of the message in mobile opportunistic networks however the 
probability for the sender to meet the destination may be low or 
even zero 
 epidemic routing protocol 
the epidemic routing protocol floods messages into the 
network the source node sends a copy of the message to every node 
that it meets the nodes that receive a copy of the message also 
send a copy of the message to every node that they meet 
eventually a copy of the message arrives at the destination of the message 
this protocol is simple but may use significant resources 
excessive communication may drain each node s battery quickly 
moreover since each node keeps a copy of each message storage is not 
used efficiently and the capacity of the network is limited 
at a minimum each node must expire messages after some amount 
of time or stop forwarding them after a certain number of hops 
after a message expires the message will not be transmitted and will 
be deleted from the storage of any node that holds the message 
an optimization to reduce the communication cost is to 
transfer index messages before transferring any data message the 
index messages contain ids of messages that a node currently holds 
thus by examining the index messages a node only transfers 
messages that are not yet contained on the other nodes 
 random routing 
an obvious approach between the above two extremes is to 
select a transfer probability between and to forward messages at 
each contact we use a simple replication strategy that allows only 
the source node to make replicas and limits the replication to a 
specific number of copies the message has some chance of 
being transferred to a highly mobile node and thus may have a better 
chance to reach its destination before the message expires 
 prophet protocol 
prophet is a probabilistic routing protocol using history 
of past encounters and transitivity to estimate each node s delivery 
probability for each other node when node i meets node j the 
delivery probability of node i for j is updated by 
pij − pij p pij 
where p is an initial probability a design parameter for a given 
network lindgren et al chose as did we in our 
evaluation when node i does not meet j for some time the delivery 
probability decreases by 
pij αk 
pij 
where α is the aging factor α and k is the number of time 
units since the last update 
the prophet protocol exchanges index messages as well as 
delivery probabilities when node i receives node j s delivery 
probabilities node i may compute the transitive delivery probability 
through j to z with 
piz piz − piz pijpjzβ 
where β is a design parameter for the impact of transitivity we 
used β as did lindgren 
 link-state protocol 
su et al use a link-state approach to estimate the weight of 
each path from the source of a message to the destination they 
use the median inter-contact duration or exponentially aged 
intercontact duration as the weight on links the exponentially aged 
inter-contact duration of node i and j is computed by 
wij αwij − α i 
where i is the new inter-contact duration and α is the aging factor 
nodes share their link-state weights when they can communicate 
with each other and messages are forwarded to the node that have 
the path with the lowest link-state weight 
 
 timely-contact probability 
we also use historical contact information to estimate the 
probability of meeting other nodes in the future but our method differs 
in that we estimate the contact probability within a period of time 
for example what is the contact probability in the next hour 
neither prophet nor link-state considers time in this way 
one way to estimate the timely-contact probability is to use the 
ratio of the total contact duration to the total time however this 
approach does not capture the frequency of contacts for example 
one node may have a long contact with another node followed by 
a long non-contact period a third node may have a short contact 
with the first node followed by a short non-contact period using 
the above estimation approach both examples would have similar 
contact probability in the second example however the two nodes 
have more frequent contacts 
we design a method to capture the contact frequency of mobile 
nodes for this purpose we assume that even short contacts are 
sufficient to exchange messages 
the probability for node i to meet node j is computed by the 
following procedure we divide the contact history hi j into a 
sequence of n periods of δt starting from the start time t of the 
first contact in history hi j to the current time we number each 
of the n periods from to n − then check each period if node 
i had any contact with node j during a given period m which is 
 t mδt t m δt we set the contact status im to be 
 otherwise the contact status im is the probability p 
 
ij that 
node i meets node j in the next δt can be estimated as the average 
of the contact status in prior intervals 
p 
 
ij 
 
n 
n− x 
m 
im 
to adapt to the change of contact patterns and reduce the storage 
space for contact histories a node may discard old history contacts 
in this situation the estimate would be based on only the retained 
history 
the above probability is the direct contact probability of two 
nodes we are also interested in the probability that we may be 
able to pass a message through a sequence of k nodes we define 
the k-order probability inductively 
p 
 k 
ij p 
 
ij 
x 
α 
p 
 
iα p 
 k− 
αj 
where α is any node other than i or j 
 our routing protocol 
we first consider the case of a two-hop path that is with only 
one relay node we consider two approaches either the receiving 
neighbor decides whether to act as a relay or the source decides 
which neighbors to use as relay 
 receiver decision 
whenever a node meets other nodes they exchange all their 
messages or as above index messages if the destination of a 
message is the receiver itself the message is delivered otherwise if 
the probability of delivering the message to its destination through 
this receiver node within δt is greater than or equal to a certain 
threshold the message is stored in the receiver s storage to forward 
 
in our simulation however we accurately model the 
communication costs and some short contacts will not succeed in transfer of 
all messages 
to the destination if the probability is less than the threshold the 
receiver discards the message notice that our protocol replicates 
the message whenever a good-looking relay comes along 
 sender decision 
to make decisions a sender must have the information about its 
neighbors contact probability with a message s destination 
therefore meta-data exchange is necessary 
when two nodes meet they exchange a meta-message 
containing an unordered list of node ids for which the sender of the 
metamessage has a contact probability greater than the threshold 
after receiving a meta-message a node checks whether it has 
any message that destined to its neighbor or to a node in the node 
list of the neighbor s meta-message if it has it sends a copy of the 
message 
when a node receives a message if the destination of the 
message is the receiver itself the message is delivered otherwise the 
message is stored in the receiver s storage for forwarding to the 
destination 
 multi-node relay 
when we use more than two hops to relay a message each node 
needs to know the contact probabilities along all possible paths to 
the message destination 
every node keeps a contact probability matrix in which each cell 
pij is a contact probability between to nodes i and j each node 
i computes its own contact probabilities row i with other nodes 
using equation whenever the node ends a contact with other 
nodes each row of the contact probability matrix has a version 
number the version number for row i is only increased when node i 
updates the matrix entries in row i other matrix entries are updated 
through exchange with other nodes when they meet 
when two nodes i and j meet they first exchange their contact 
probability matrices node i compares its own contact matrix with 
node j s matrix if node j s matrix has a row l with a higher version 
number then node i replaces its own row l with node j s row l 
likewise node j updates its matrix after the exchange the two 
nodes will have identical contact probability matrices 
next if a node has a message to forward the node estimates 
its neighboring node s order-k contact probability to contact the 
destination of the message using equation if p 
 k 
ij is above a 
threshold or if j is the destination of the message node i will send 
a copy of the message to node j 
all the above effort serves to determine the transfer probability 
when two nodes meet the replication decision is orthogonal to 
the transfer decision in our implementation we always replicate 
although prophet and link-state do no replication as 
described we added replication to those protocols for better 
comparison to our protocol 
 evaluation results 
we evaluate and compare the results of direct delivery epidemic 
random prophet link-state and timely-contact routing 
protocols 
 mobility traces 
we use real mobility data collected at dartmouth college 
dartmouth college has collected association and disassociation 
messages from devices on its wireless network wireless users since 
spring each message records the wireless card mac 
address the time of association disassociation and the name of the 
access point we treat each unique mac address as a node for 
 
more information about dartmouth s network and the data 
collection see previous studies 
our data are not contacts in a mobile ad-hoc network we can 
approximate contact traces by assuming that two users can 
communicate with each other whenever they are associated with the same 
access point chaintreau et al used dartmouth data traces and 
made the same assumption to theoretically analyze the impact of 
human mobility on opportunistic forwarding algorithms this 
assumption may not be accurate 
but it is a good first approximation 
in our simulation we imagine the same clients and same mobility 
in a network with no access points since our campus has full wifi 
coverage we assume that the location of access points had little 
impact on users mobility 
we simulated one full month of trace data november 
taken from crawdad with users although 
predictionbased protocols require prior contact history to estimate each node s 
delivery probability our preliminary results show that the 
performance improvement of warming-up over one month of trace was 
marginal therefore for simplicity we show the results of all 
protocols without warming-up 
 simulator 
we developed a custom simulator 
since we used contact traces 
derived from real mobility data we did not need a mobility model 
and omitted physical and link-layer details for node discovery we 
were aware that the time for neighbor discovery in different 
wireless technologies vary from less than one seconds to several 
seconds furthermore connection establishment also takes time such 
as dhcp in our simulation we assumed the nodes could discover 
and connect each other instantly when they were associated with a 
same ap to accurately model communication costs however we 
simulated some mac-layer behaviors such as collision 
the default settings of the network of our simulator are listed in 
table using the values recommended by other papers 
the message probability was the probability of generating 
messages as described in section the default transmission 
bandwidth was mb s when one node tried to transmit a message it 
first checked whether any nearby node was transmitting if it was 
the node backed off a random number of slots each slot was 
millisecond and the maximum number of backoff slots was the 
size of messages was uniformly distributed between bytes and 
 bytes the hop count limit hcl was the maximum number 
of hops before a message should stop forwarding the time to live 
 ttl was the maximum duration that a message may exist before 
expiring the storage capacity was the maximum space that a node 
can use for storing messages for our routing method we used a 
default prediction window δt of hours and a probability 
threshold of the replication factor r was not limited by default so 
the source of a message transferred the messages to any other node 
that had a contact probability with the message destination higher 
than the probability threshold 
 message generation 
after each contact event in the contact trace we generated a 
message with a given probability we choose a source node and a 
des 
two nodes may not have been able to directly communicate while 
they were at two far sides of an access point or two nodes may 
have been able to directly communicate if they were between two 
adjacent access points 
 
we tried to use a general network simulator ns which was 
extremely slow when simulating a large number of mobile nodes in 
our case more than nodes and provided unnecessary detail 
in modeling lower-level network protocols 
table default settings of the simulation 
parameter default value 
message probability 
bandwidth mb s 
transmission slot millisecond 
max backoff slots 
message size - bytes 
hop count limit hcl unlimited 
time to live ttl unlimited 
storage capacity unlimited 
prediction window δt hours 
probability threshold 
contact history length 
replication always 
aging factor α prophet 
initial probability p prophet 
transitivity impact β prophet 
 
 
 
 
 
 
 
 
numberofoccurrence 
hour 
movements 
contacts 
figure movements and contacts duration each hour 
tination node randomly using a uniform distribution across nodes 
seen in the contact trace up to the current time when there were 
more contacts during a certain period there was a higher likelihood 
that a new message was generated in that period this correlation 
is not unreasonable since there were more movements during the 
day than during the night and so the number of contacts figure 
shows the statistics of the numbers of movements and the numbers 
of contacts during each hour of the day summed across all users 
and all days the plot shows a clear diurnal activity pattern the 
activities reached lowest around am and peaked between pm and 
 pm we assume that in some applications network traffic exhibits 
similar patterns that is people send more messages during the day 
too 
messages expire after a ttl we did not use proactive methods 
to notify nodes the delivery of messages so that the messages can 
be removed from storage 
 metrics 
we define a set of metrics that we use in evaluating routing 
protocols in opportunistic networks 
 delivery ratio the ratio of the number of messages delivered 
to the number of total messages generated 
 message transmissions the total number of messages 
transmitted during the simulation across all nodes 
 
 meta-data transmissions the total number of meta-data units 
transmitted during the simulation across all nodes 
 message duplications the number of times a message copy 
occurred due to replication 
 delay the duration between a message s generation time and 
the message s delivery time 
 storage usage the max and mean of maximum storage bytes 
used across all nodes 
 results 
here we compare simulation results of the six routing protocols 
 
 
 
 
unlimited 
deliveryratio 
message time-to-live ttl hour 
direct 
random 
prediction 
state 
prophet 
epidemic 
figure delivery ratio log scale the direct and random 
protocols for one-hour ttl had delivery ratios that were too 
low to be visible in the plot 
figure shows the delivery ratio of all the protocols with 
different ttls in all the plots in the paper prediction stands for our 
method state stands for the link-state protocol and prophet 
represents prophet although we had users in the 
network the direct-delivery and random protocols had low delivery 
ratios note the log scale even for messages with an unlimited 
lifetime only out of messages were delivered during this 
one-month simulation the delivery ratio of epidemic routing was 
the best the three prediction-based approaches had low delivery 
ratio compared to epidemic routing although our method was 
slightly better than the other two the advantage was marginal 
the high delivery ratio of epidemic routing came with a price 
excessive transmissions figure shows the number of message 
data transmissions the number of message transmissions of 
epidemic routing was more than times higher than for the 
predictionbased routing protocols obviously the direct delivery protocol 
had the lowest number of message transmissions - the number of 
message delivered among the three prediction-based methods 
the prophet transmitted fewer messages but had comparable 
delivery-ratio as seen in figure 
figure shows that epidemic and all prediction-based methods 
had substantial meta-data transmissions though epidemic routing 
had relatively more with shorter ttls because epidemic 
protocol transmitted messages at every contact in turn more nodes had 
messages that required meta-data transmission during contact the 
direct-delivery and random protocols had no meta-data 
transmissions 
in addition to its message transmissions and meta-data 
transmissions the epidemic routing protocol also had excessive message 
 
 
 
 
 
 
 e 
 e 
 e 
unlimited 
numberofmessagetransmitted 
message time-to-live ttl hour 
direct 
random 
prediction 
state 
prophet 
epidemic 
figure message transmissions log scale 
 
 
 
 
 
 
 e 
 e 
 e 
unlimited 
numberofmeta-datatransmissions 
message time-to-live ttl hour 
direct 
random 
prediction 
state 
prophet 
epidemic 
figure meta-data transmissions log scale direct and 
random protocols had no meta-data transmissions 
duplications spreading replicas of messages over the network 
figure shows that epidemic routing had one or two orders more 
duplication than the prediction-based protocols recall that the 
directdelivery and random protocols did not replicate thus had no data 
duplications 
figure shows both the median and mean delivery delays all 
protocols show similar delivery delays in both mean and median 
measures for medium ttls but differ for long and short ttls 
with a -hour ttl or unlimited ttl epidemic routing had the 
shortest delays the direct-delivery had the longest delay for 
unlimited ttl but it had the shortest delay for the one-hour ttl 
the results seem contrary to our intuition the epidemic routing 
protocol should be the fastest routing protocol since it spreads 
messages all over the network indeed the figures show only the delay 
time for delivered messages for direct delivery random and the 
probability-based routing protocols relatively few messages were 
delivered for short ttls so many messages expired before they 
could reach their destination those messages had infinite delivery 
delay and were not included in the median or mean measurements 
for longer ttls more messages were delivered even for the 
directdelivery protocol the statistics of longer ttls for comparison are 
more meaningful than those of short ttls 
since our message generation rate was low the storage usage 
was also low in our simulation figure shows the maximum 
and average of maximum volume in kbytes of messages stored 
 
 
 
 
 
 
 
 e 
 e 
 e 
unlimited 
numberofmessageduplications 
message time-to-live ttl hour 
direct 
random 
prediction 
state 
prophet 
epidemic 
figure message duplications log scale direct and random 
protocols had no message duplications 
 
 
 
 
 
unlimited unlimited 
delay minute 
message time-to-live ttl hour 
direct 
random 
prediction 
state 
prophet 
epidemic 
mean delaymedian delay 
figure median and mean delays log scale 
in each node the epidemic routing had the most storage usage 
the message time-to-live parameter was the big factor affecting the 
storage usage for epidemic and prediction-based routing protocols 
we studied the impact of different parameters of our 
predictionbased routing protocol our prediction-based protocol was 
sensitive to several parameters such as the probability threshold and the 
prediction window δt figure shows the delivery ratios when 
we used different probability thresholds the leftmost value 
is the value used for the other plots a higher probability threshold 
limited the transfer probability so fewer messages were delivered 
it also required fewer transmissions as shown in figure with 
a larger prediction window we got a higher contact probability 
thus for the same probability threshold we had a slightly higher 
delivery ratio as shown in figure and a few more transmissions 
as shown in figure 
 related work 
in addition to the protocols that we evaluated in our simulation 
several other opportunistic network routing protocols have been 
proposed in the literature we did not implement and evaluate these 
routing protocols because either they require domain-specific 
information location information assume certain mobility 
patterns present orthogonal approaches to other 
routing protocols 
 
 
 
 
 
 
unlimited unlimited 
storageusage kb 
message time-to-live ttl hour 
direct 
random 
prediction 
state 
prophet 
epidemic 
mean of maximummax of maximum 
figure max and mean of maximum storage usage across all 
nodes log scale 
 
 
 
 
 
 
 
deliveryratio 
probability threshold 
figure probability threshold impact on delivery ratio of 
timely-contact routing 
lebrun et al propose a location-based delay-tolerant 
network routing protocol their algorithm assumes that every node 
knows its own position and the destination is stationary at a known 
location a node forwards data to a neighbor only if the 
neighbor is closer to the destination than its own position our protocol 
does not require knowledge of the nodes locations and learns their 
contact patterns 
leguay et al use a high-dimensional space to represent a 
mobility pattern then routes messages to nodes that are closer to 
the destination node in the mobility pattern space location 
information of nodes is required to construct mobility patterns 
musolesi et al propose an adaptive routing protocol for 
intermittently connected mobile ad-hoc networks they use a kalman 
filter to compute the probability that a node delivers messages this 
protocol assumes group mobility and cloud connectivity that is 
nodes move as a group and among this group of nodes a 
contemporaneous end-to-end connection exists for every pair of nodes when 
two nodes are in the same connected cloud dsdv routing is 
used 
network coding also draws much interest from dtn research 
erasure-coding explores coding algorithms to reduce 
message replicas the source node replicates a message m times then 
uses a coding scheme to encode them in one big message 
after replicas are encoded the source divides the big message into k 
 
 
 
 
 
 
 
 
 
 
numberofmessagetransmitted million 
probability threshold 
figure probability threshold impact on message 
transmission of timely-contact routing 
 
 
 
 
 
 
 
deliveryratio 
prediction window hour 
figure prediction window impact on delivery ratio of 
timely-contact routing semi-log scale 
blocks of the same size and transmits a block to each of the first k 
encountered nodes if m of the blocks are received at the 
destination the message can be restored where m k in a uniformly 
distributed mobility scenario the delivery probability increases 
because the probability that the destination node meets m relays is 
greater than it meets k relays given m k 
 summary 
we propose a prediction-based routing protocol for 
opportunistic networks we evaluate the performance of our protocol using 
realistic contact traces and compare to five existing routing 
protocols 
our simulation results show that direct delivery had the 
lowest delivery ratio the fewest data transmissions and no meta-data 
transmission or data duplication direct delivery is suitable for 
devices that require an extremely low power consumption the 
random protocol increased the chance of delivery for messages 
otherwise stuck at some low mobility nodes epidemic routing delivered 
the most messages the excessive transmissions and data 
duplication however consume more resources than portable devices may 
be able to provide 
none of these protocols direct-delivery random and epidemic 
routing are practical for real deployment of opportunistic networks 
 
 
 
 
 
 
 
 
 
numberofmessagetransmitted million 
prediction window hour 
figure prediction window impact on message transmission 
of timely-contact routing semi-log scale 
because they either had an extremely low delivery ratio or had an 
extremely high resource consumption the prediction-based 
routing protocols had a delivery ratio more than times better than 
that for direct-delivery and random routing and fewer 
transmissions and less storage usage than epidemic routing they also had 
fewer data duplications than epidemic routing 
all the prediction-based routing protocols that we have 
evaluated had similar performance our method had a slightly higher 
delivery ratio but more transmissions and higher storage usage 
there are many parameters for prediction-based routing protocols 
however and different parameters may produce different results 
indeed there is an opportunity for some adaptation for example 
high priority messages may be given higher transfer and 
replication probabilities to increase the chance of delivery and reduce the 
delay or a node with infrequent contact may choose to raise its 
transfer probability 
we only studied the impact of predicting peer-to-peer contact 
probability for routing in unicast messages in some applications 
context information such as location may be available for the 
peers one may also consider other messaging models for 
example where messages are sent to a location such that every node at 
that location will receive a copy of the message location 
prediction may be used to predict nodes mobility and to choose as 
relays those nodes moving toward the destined location 
research on routing in opportunistic networks is still in its early 
stage many other issues of opportunistic networks such as 
security and privacy are mainly left open we anticipate studying these 
issues in future work 
 acknowledgement 
this research is a project of the center for mobile 
computing and the institute for security technology studies at dartmouth 
college it was supported by docomo labs usa the 
crawdad archive at dartmouth college funded by nsf cri award 
 nsf infrastructure award eia- and by grant 
number -dd-bx- awarded by the bureau of justice 
assistance points of view or opinions in this document are those of 
the authors and do not represent the official position or policies of 
any sponsor 
 references 
 john burgess brian gallagher david jensen and brian neil 
levine maxprop routing for vehicle-based 
 
disruption-tolerant networks in proceedings of the th 
ieee international conference on computer 
communications infocom april 
 scott burleigh adrian hooke leigh torgerson kevin fall 
vint cerf bob durst keith scott and howard weiss 
delay-tolerant networking an approach to interplanetary 
internet ieee communications magazine - 
june 
 tracy camp jeff boleng and vanessa davies a survey of 
mobility models for ad-hoc network research wireless 
communication mobile computing wcmc special 
issue on mobile ad-hoc networking research trends and 
applications - 
 andrew campbell shane eisenman nicholas lane 
emiliano miluzzo and ronald peterson people-centric 
urban sensing in ieee wireless internet conference august 
 
 augustin chaintreau pan hui jon crowcroft christophe 
diot richard gass and james scott impact of human 
mobility on the design of opportunistic forwarding 
algorithms in proceedings of the th ieee international 
conference on computer communications infocom 
april 
 kevin fall a delay-tolerant network architecture for 
challenged internets in proceedings of the conference 
on applications technologies architectures and protocols 
for computer communications sigcomm august 
 tristan henderson david kotz and ilya abyzov the 
changing usage of a mature campus-wide wireless network 
in proceedings of the th annual international conference 
on mobile computing and networking mobicom pages 
 - september 
 pan hui augustin chaintreau james scott richard gass 
jon crowcroft and christophe diot pocket switched 
networks and human mobility in conference environments 
in acm sigcomm workshop on delay tolerant 
networking pages - august 
 ravi jain dan lelescu and mahadevan balakrishnan 
model t an empirical model for user registration patterns in 
a campus wireless lan in proceedings of the th annual 
international conference on mobile computing and 
networking mobicom pages - 
 sushant jain mike demmer rabin patra and kevin fall 
using redundancy to cope with failures in a delay tolerant 
network in proceedings of the conference on 
applications technologies architectures and protocols for 
computer communications sigcomm pages - 
august 
 philo juang hidekazu oki yong wang margaret 
martonosi li-shiuan peh and daniel rubenstein 
energy-efficient computing for wildlife tracking design 
tradeoffs and early experiences with zebranet in the tenth 
international conference on architectural support for 
programming languages and operating systems october 
 
 david kotz and kobby essien analysis of a campus-wide 
wireless network wireless networks - 
 david kotz tristan henderson and ilya abyzov 
crawdad data set dartmouth campus 
http crawdad cs dartmouth edu dartmouth campus 
december 
 jason lebrun chen-nee chuah dipak ghosal and michael 
zhang knowledge-based opportunistic forwarding in 
vehicular wireless ad-hoc networks in ieee vehicular 
technology conference pages - may 
 jeremie leguay timur friedman and vania conan 
evaluating mobility pattern space routing for dtns in 
proceedings of the th ieee international conference on 
computer communications infocom april 
 anders lindgren avri doria and olov schelen 
probabilistic routing in intermittently connected networks in 
workshop on service assurance with partial and intermittent 
resources sapir pages - 
 mirco musolesi stephen hailes and cecilia mascolo 
adaptive routing for intermittently connected mobile ad-hoc 
networks in ieee international symposium on a world of 
wireless mobile and multimedia networks pages - 
june extended version 
 olpc one laptop per child project http laptop org 
 c e perkins and p bhagwat highly dynamic 
destination-sequenced distance-vector routing dsdv for 
mobile computers computer communication review pages 
 - october 
 c e perkins and e m royer ad-hoc on-demand distance 
vector routing in ieee workshop on mobile computing 
systems and applications pages - february 
 libo song david kotz ravi jain and xiaoning he 
evaluating next-cell predictors with extensive wi-fi mobility 
data ieee transactions on mobile computing 
 - december 
 jing su ashvin goel and eyal de lara an empirical 
evaluation of the student-net delay tolerant network in 
international conference on mobile and ubiquitous systems 
 mobiquitous july 
 amin vahdat and david becker epidemic routing for 
partially-connected ad-hoc networks technical report 
cs- - duke university july 
 yong wang sushant jain margaret martonosia and kevin 
fall erasure-coding based routing for opportunistic 
networks in acm sigcomm workshop on delay tolerant 
networking pages - august 
 yu wang and hongyi wu dft-msn the delay fault tolerant 
mobile sensor network for pervasive information gathering 
in proceedings of the th ieee international conference on 
computer communications infocom april 
 
consistency-preserving caching of dynamic 
database content∗ 
niraj tolia and m satyanarayanan 
carnegie mellon university 
{ntolia satya} cs cmu edu 
abstract 
with the growing use of dynamic web content generated from 
relational databases traditional caching solutions for throughput and 
latency improvements are ineffective we describe a middleware 
layer called ganesh that reduces the volume of data transmitted 
without semantic interpretation of queries or results it achieves 
this reduction through the use of cryptographic hashing to detect 
similarities with previous results these benefits do not require 
any compromise of the strict consistency semantics provided by the 
back-end database further ganesh does not require modifications 
to applications web servers or database servers and works with 
closed-source applications and databases using two benchmarks 
representative of dynamic web sites measurements of our 
prototype show that it can increase end-to-end throughput by as much 
as twofold for non-data intensive applications and by as much as 
tenfold for data intensive ones 
categories and subject descriptors 
c computer-communication networks distributed 
systems h database management systems 
general terms 
design performance 
 introduction 
an increasing fraction of web content is dynamically generated 
from back-end relational databases even when database content 
remains unchanged temporal locality of access cannot be exploited 
because dynamic content is not cacheable by web browsers or by 
intermediate caching servers such as akamai mirrors in a 
multitiered architecture each web request can stress the wan link 
between the web server and the database this causes user 
experience to be highly variable because there is no caching to 
insulate the client from bursty loads previous attempts in caching 
dynamic database content have generally weakened transactional 
semantics or required application modifications 
we report on a new solution that takes the form of a 
databaseagnostic middleware layer called ganesh ganesh makes no effort 
to semantically interpret the contents of queries or their results 
instead it relies exclusively on cryptographic hashing to detect 
similarities with previous results hash-based similarity detection has 
seen increasing use in distributed file systems for 
improving performance on low-bandwidth networks however these 
techniques have not been used for relational databases unlike 
previous approaches that use generic methods to detect similarity 
ganesh exploits the structure of relational database results to yield 
superior performance improvement 
one faces at least three challenges in applying hash-based 
similarity detection to back-end databases first previous work in this 
space has traditionally viewed storage content as uninterpreted bags 
of bits with no internal structure this allows hash-based 
techniques to operate on long contiguous runs of data for maximum 
effectiveness in contrast relational databases have rich internal 
structure that may not be as amenable to hash-based similarity 
detection second relational databases have very tight integrity and 
consistency constraints that must not be compromised by the use 
of hash-based techniques third the source code of commercial 
databases is typically not available this is in contrast to previous 
work which presumed availability of source code 
our experiments show that ganesh while conceptually simple 
can improve performance significantly at bandwidths 
representative of today s commercial internet on benchmarks modeling 
multitiered web applications the throughput improvement was as high 
as tenfold for data-intensive workloads for workloads that were 
not data-intensive throughput improvements of up to twofold were 
observed even when bandwidth was not a constraint ganesh had 
low overhead and did not hurt performance our experiments also 
confirm that exploiting the structure present in database results is 
crucial to this performance improvement 
 background 
 dynamic content generation 
as the world wide web has grown many web sites have 
decentralized their data and functionality by pushing them to the edges 
of the internet today ebusiness systems often use a three-tiered 
architecture consisting of a front-end web server an application 
server and a back-end database server figure illustrates this 
architecture the first two tiers can be replicated close to a 
concentration of clients at the edge of the internet this improves user 
experience by lowering end-to-end latency and reducing exposure 
www track performance and scalability session scalable systems for dynamic content 
 
back-end database 
server 
front-end web and 
application servers 
figure multi-tier architecture 
to backbone traffic congestion it can also increase the availability 
and scalability of web services 
content that is generated dynamically from the back-end database 
cannot be cached in the first two tiers while databases can be 
easily replicated in a lan this is infeasible in a wan because of 
the difficult task of simultaneously providing strong consistency 
availability and tolerance to network partitions as a result 
databases tend to be centralized to meet the strong consistency 
requirements of many ebusiness applications such as banking 
finance and online retailing thus the back-end database is 
usually located far from many sets of first and second-tier nodes 
in the absence of both caching and replication wan bandwidth 
can easily become a limiting factor in the performance and 
scalability of data-intensive applications 
 hash-based systems 
ganesh s focus is on efficient transmission of results by 
discovering similarities with the results of previous queries as sql queries 
can generate large results hash-based techniques lend themselves 
well to the problem of efficiently transferring these large results 
across bandwidth constrained links 
the use of hash-based techniques to reduce the volume of data 
transmitted has emerged as a common theme of many recent 
storage systems as discussed in section these techniques rely 
on some basic assumptions cryptographic hash functions are 
assumed to be collision-resistant in other words it is 
computationally intractable to find two inputs that hash to the same output the 
functions are also assumed to be one-way that is finding an 
input that results in a specific output is computationally infeasible 
menezes et al provide more details about these assumptions 
the above assumptions allow hash-based systems to assume that 
collisions do not occur hence they are able to treat the hash of a 
data item as its unique identifier a collection of data items 
effectively becomes content-addressable allowing a small hash to serve 
as a codeword for a much larger data item in permanent storage or 
network transmission 
the assumption that collisions are so rare as to be effectively 
non-existent has recently come under fire however as 
explained by black we believe that these issues do not form a 
concern for ganesh all communication is between trusted parts 
of the system and an adversary has no way to force ganesh to 
accept invalid data further ganesh does not depend critically on any 
specific hash function while we currently use sha- replacing it 
with a different hash function would be simple there would be 
no impact on performance as stronger hash functions e g 
sha only add a few extra bytes and the generated hashes are still 
orders of magnitude smaller than the data items they represent no 
re-hashing of permanent storage is required since ganesh only uses 
hashing on volatile data 
 design and implementation 
ganesh exploits redundancy in the result stream to avoid 
transmitting result fragments that are already present at the query site 
redundancy can arise naturally in many different ways for 
example a query repeated after a certain interval may return a different 
result because of updates to the database however there may be 
significant commonality between the two results as another 
example a user who is refining a search may generate a sequence 
of queries with overlapping results when ganesh detects 
redundancy it suppresses transmission of the corresponding result 
fragments instead it transmits a much smaller digest of those 
fragments and lets the query site reconstruct the result through hash 
lookup in a cache of previous results in effect ganesh uses 
computation at the edges to reduce internet communication 
our description of ganesh focuses on four aspects we first 
explain our approach to detecting similarity in query results next 
we discuss how the ganesh architecture is completely invisible to 
all components of a multi-tier system we then describe ganesh s 
proxy-based approach and the dataflow for detecting similarity 
 detecting similarity 
one of the key design decisions in ganesh is how similarity is 
detected there are many potential ways to decompose a result into 
fragments the optimal way is of course the one that results in the 
smallest possible object for transmission for a given query s results 
finding this optimal decomposition is a difficult problem because 
of the large space of possibilities and because the optimal choice 
depends on many factors such as the contents of the query s result 
the history of recent results and the cache management algorithm 
when an object is opaque the use of rabin fingerprints 
to detect common data between two objects has been successfully 
shown in the past by systems such as lbfs and casper 
rabin fingerprinting uses a sliding window over the data to 
compute a rolling hash assuming that the hash function is uniformly 
distributed a chunk boundary is defined whenever the lower order 
bits of the hash value equal some predetermined value the 
number of lower order bits used defines the average chunk size these 
sub-divided chunks of the object become the unit of comparison for 
detecting similarity between different objects 
as the locations of boundaries found by using rabin fingerprints 
is stochastically determined they usually fail to align with any 
structural properties of the underlying data the algorithm 
therefore deals well with in-place updates insertions and deletions 
however it performs poorly in the presence of any reordering of data 
figure shows an example where two results a and b 
consisting of three rows have the same data but have different sort 
attributes in the extreme case rabin fingerprinting might be unable 
to find any similar data due to the way it detects chunk boundaries 
fortunately ganesh can use domain specific knowledge for more 
precise boundary detection the information we exploit is that a 
query s result reflects the structure of a relational database where 
all data is organized as tables and rows it is therefore simple to 
check for similarity with previous results at two granularities first 
the entire result and then individual rows the end of a row in a 
result serves as a natural chunk boundary it is important to note that 
using the tabular structure in results only involves shallow 
interpretation of the data ganesh does not perform any deeper semantic 
interpretation such as understanding data types result schema or 
integrity constraints 
tuning rabin fingerprinting for a workload can also be difficult 
if the average chunk size is too large chunks can span multiple 
result rows however selecting a smaller average chunk size 
increases the amount of metadata required to the describe the results 
www track performance and scalability session scalable systems for dynamic content 
 
figure rabin fingerprinting vs ganesh s chunking 
this in turn would decrease the savings obtained via its use 
rabin fingerprinting also needs two computationally-expensive passes 
over the data once to determine chunk boundaries and one again to 
generate cryptographic hashes for the chunks ganesh only needs 
a single pass for hash generation as the chunk boundaries are 
provided by the data s natural structure 
the performance comparison in section shows that ganesh s 
row-based algorithm outperforms rabin fingerprinting given that 
previous work has already shown that rabin fingerprinting 
performs better than gzip we do not compare ganesh to 
compression algorithms in this paper 
 transparency 
the key factor influencing our design was the need for ganesh 
to be completely transparent to all components of a typical 
ebusiness system web servers application servers and database servers 
without this ganesh stands little chance of having a significant 
real-world impact requiring modifications to any of the above 
components would raise the barrier for entry of ganesh into an 
existing system and thus reduce its chances of adoption preserving 
transparency is simplified by the fact that ganesh is purely a 
performance enhancement not a functionality or usability enhancement 
we chose agent interposition as the architectural approach to 
realizing our goal this approach relies on the existence of a compact 
programming interface that is already widely used by target 
software it also relies on a mechanism to easily add new code without 
disrupting existing module structure 
these conditions are easily met in our context because of the 
popularity of java as the programming language for ebusiness 
systems the java database connectivity jdbc api allows 
java applications to access a wide variety of databases and even 
other tabular data repositories such as flat files access to these 
data sources is provided by jdbc drivers that translate between 
the jdbc api and the database communication mechanism 
figure a shows how jdbc is typically used in an application 
as the jdbc interface is standardized one can substitute one 
jdbc driver for another without application modifications the 
jdbc driver thus becomes the natural module to exploit for code 
interposition as shown in figure b the native jdbc driver is 
replaced with a ganesh jdbc driver that presents the same 
standardized interface the ganesh driver maintains an in-memory 
cache of result fragments from previous queries and performs 
reassembly of results at the database we add a new process called 
the ganesh proxy this proxy which can be shared by multiple 
front-end nodes consists of two parts code to detect similarity 
in result fragments and the original native jdbc driver that 
communicates with the database the use of a proxy at the database 
makes ganesh database-agnostic and simplifies prototyping and 
experimentation ganesh is thus able to work with a wide range 
of databases and applications requiring no modifications to either 
 proxy-based caching 
the native jdbc driver shown in figure a is a lightweight 
code component supplied by the database vendor its main 
funcclient 
database 
web and 
application server 
native jdbc driver 
wan 
 a native architecture 
client 
database 
ganesh proxy 
native jdbc driver 
wan 
web and 
application server 
ganesh jdbc driver 
 b ganesh s interposition-based architecture 
figure native vs ganesh architecture 
tion is to mediate communication between the application and the 
remote database it forwards queries buffers entire results and 
responds to application requests to view parts of results 
the ganesh jdbc driver shown in figure b presents the 
application with an interface identical to that provided by the native 
driver it provides the ability to reconstruct results from compact 
hash-based descriptions sent by the proxy to perform this 
reconstruction the driver maintains an in-memory cache of 
recentlyreceived results this cache is only used as a source of result 
fragments in reconstructing results no attempt is made by the ganesh 
driver or proxy to track database updates the lack of cache 
consistency does not hurt correctness as a description of the results is 
always fetched from the proxy - at worst there will be no 
performance benefit from using ganesh stale data will simply be paged 
out of the cache over time 
the ganesh proxy accesses the database via the native jdbc 
driver which remains unchanged between figures a and b 
the database is thus completely unaware of the existence of the 
proxy the proxy does not examine any queries received from 
the ganesh driver but passes them to the native driver instead 
the proxy is responsible for inspecting database output received 
from the native driver detecting similar results and generating 
hash-based encodings of these results whenever enough similarity 
is found while this architecture does not decrease the load on a 
database as mentioned earlier in section it is much easier to 
replicate databases for scalability in a lan than in a wan 
to generate a hash-based encoding the proxy must be aware of 
what result fragments are available in the ganesh driver s cache 
one approach is to be optimistic and to assume that all result 
fragments are available this will result in the smallest possible initial 
transmission of a result however in cases where there is little 
overlap with previous results the ganesh driver will have to make 
many calls to the proxy during reconstruction to fetch missing 
result fragments to avoid this situation the proxy loosely tracks the 
state of the ganesh driver s cache since both components are 
under our control it is relatively simple to do this without resorting 
to gray-box techniques or explicit communication for maintaining 
cache coherence instead the proxy simulates the ganesh driver s 
cache management algorithm and uses this to maintain a list of 
hashes for which the ganesh driver is likely to possess the result 
fragments in case of mistracking there will be no loss of 
correctness but there will be extra round-trip delays to fetch the missing 
fragments if the client detects loss of synchronization with the 
proxy it can ask the proxy to reset the state shared between them 
also note that the proxy does not need to keep the result fragments 
themselves only their hashes this allows the proxy to remain 
scalable even when it is shared by many front-end nodes 
www track performance and scalability session scalable systems for dynamic content 
 
object output stream 
convert resultset 
object input stream 
convert resultset 
all data 
recipe 
resultset 
all data 
resultset 
network 
ganesh proxy ganesh jdbc driver 
result 
set 
recipe 
result 
set 
yes 
yes 
no 
no 
ganeshinputstream 
ganeshoutputstream 
figure dataflow for result handling 
 encoding and decoding results 
the ganesh proxy receives database output as java objects from 
the native jdbc driver it examines this output to see if a java 
object of type resultset is present the jdbc interface uses 
this data type to store results of database queries if a resultset 
object is found it is shrunk as discussed below all other java 
objects are passed through unmodified 
as discussed in section the proxy uses the row boundaries 
defined in the resultset to partition it into fragments 
consisting of single result rows all resultset objects are converted 
into objects of a new type called reciperesultset we use 
the term recipe for this compact description of a database 
result because of its similarity to a file recipe in the casper file 
system the conversion replaces each result fragment that is 
likely to be present in the ganesh driver s cache by a sha- hash 
of that fragment previously unseen result fragments are retained 
verbatim the proxy also retains hashes for the new result 
fragments as they will be present in the driver s cache in the future 
note that the proxy only caches hashes for result fragments and 
does not cache recipes 
the proxy constructs a reciperesultset by checking for 
similarity at the entire result and then the row level if the entire 
result is predicted to be present in the ganesh driver s cache the 
reciperesultset is simply a single hash of the entire result 
otherwise it contains hashes for those rows predicted to be present 
in that cache all other rows are retained verbatim if the proxy 
estimates an overall space savings it will transmit the 
reciperesultset otherwise the original resultset is transmitted 
the reciperesultset objects are transformed back into 
resultset objects by the ganesh driver figure illustrates 
resultset handling at both ends each sha- hash found in a 
reciperesultset is looked up in the local cache of result 
fragments on a hit the hash is replaced by the corresponding 
fragment on a miss the driver contacts the ganesh proxy to fetch the 
fragment all previously unseen result fragments that were retained 
verbatim by the proxy are hashed and added to the result cache 
there should be very few misses if the proxy has accurately 
tracked the ganesh driver s cache state a future optimization would 
be to batch the fetch of missing fragments this would be valuable 
when there are many small missing fragments in a high-latency 
wan once the transformation is complete the fully reconstructed 
resultset object is passed up to the application 
 experimental validation 
three questions follow from the goals and design of ganesh 
 first can performance can be improved significantly by 
exploiting similarity across database results 
benchmark dataset details 
 users stories 
bboard gb comments 
auction gb users items 
table benchmark dataset details 
 second how important is ganesh s structural similarity 
detection relative to rabin fingerprinting s similarity detection 
 third is the overhead of the proxy-based design acceptable 
our evaluation answers these question through controlled 
experiments with the ganesh prototype this section describes the 
benchmarks used our evaluation procedure and the experimental setup 
results of the experiments are presented in sections and 
 benchmarks 
our evaluation is based on two benchmarks that have been 
widely used by other researchers to evaluate various aspects of 
multi-tier and ebusiness architectures the first 
benchmark bboard is modeled after slashdot a technology-oriented 
news site the second benchmark auction is modeled after 
ebay an online auction site in both benchmarks most content is 
dynamically generated from information stored in a database 
details of the datasets used can be found in table 
 the bboard benchmark 
the bboard benchmark also known as rubbos 
models slashdot a popular technology-oriented web site slashdot 
aggregates links to news stories and other topics of interest found 
elsewhere on the web the site also serves as a bulletin board by 
allowing users to comment on the posted stories in a threaded 
conversation form it is not uncommon for a story to gather hundreds 
of comments in a matter of hours the bboard benchmark is 
similar to the site and models the activities of a user including 
readonly operations such as browsing the stories of the day browsing 
story categories and viewing comments as well as write operations 
such as new user registration adding and moderating comments 
and story submission 
the benchmark consists of three different phases a short 
warmup phase a runtime phase representing the main body of the 
workload and a short cool-down phase in this paper we only report 
results from the runtime phase the warm-up phase is important 
in establishing dynamic system state but measurements from that 
phase are not significant for our evaluation the cool-down phase 
is solely for allowing the benchmark to shut down 
the warm-up runtime and cool-down phases are and 
minutes respectively the number of simulated clients were 
 and the benchmark is available in a java servlets 
and php version and has different datasets we evaluated ganesh 
using the java servlets version and the expanded dataset 
the bboard benchmark defines two different workloads the 
first the authoring mix consists of read-only operations and 
 read-write operations the second the browsing mix 
contains only read-only operations and does not update the database 
 the auction benchmark 
the auction benchmark also known as rubis models 
ebay the online auction site the ebay web site is used to buy 
and sell items via an auction format the main activities of a user 
include browsing selling or bidding for items modeling the 
activities on this site this benchmark includes read-only activities such 
as browsing items by category and by region as well as read-write 
www track performance and scalability session scalable systems for dynamic content 
 
netem 
router ganesh 
proxy 
clients web and 
application server 
database 
server 
figure experimental setup 
activities such as bidding for items buying and selling items and 
leaving feedback 
as with bboard the benchmark consists of three different phases 
the warm-up runtime and cool-down phases for this experiment 
are and minutes respectively we tested ganesh with 
four client configurations where the number of test clients was set 
to and the benchmark is available in a 
enterprise java bean ejb java servlets and php version and has 
different datasets we evaluated ganesh with the java servlets 
version and the expanded dataset 
the auction benchmark defines two different workloads the 
first the bidding mix consists of read-only operations and 
 read-write operations the second the browsing mix 
contains only read-only operations and does not update the database 
 experimental procedure 
both benchmarks involve a synthetic workload of clients 
accessing a web server the number of clients emulated is an 
experimental parameter each emulated client runs an instance of the 
benchmark in its own thread using a matrix to transition between 
different benchmark states the matrix defines a stochastic model 
with probabilities of transitioning between the different states that 
represent typical user actions an example transition is a user 
logging into the auction system and then deciding on whether to 
post an item for sale or bid on active auctions each client also 
models user think time between requests the think time is 
modeled as an exponential distribution with a mean of seconds 
we evaluate ganesh along two axes number of clients and wan 
bandwidth higher loads are especially useful in understanding 
ganesh s performance when the cpu or disk of the database server 
or proxy is the limiting factor a previous study has shown that 
approximately of the wide-area internet bottlenecks observed 
had an available bandwidth under mb s based on this work 
we focus our evaluation on the wan bandwidth of mb s with 
 ms of round-trip latency representative of severely constrained 
network paths and mb s with ms of round-trip latency 
representative of a moderately constrained network path we also report 
ganesh s performance at mb s with no added round-trip 
latency this bandwidth representative of an unconstrained network 
is especially useful in revealing any potential overhead of ganesh 
in situations where wan bandwidth is not the limiting factor for 
each combination of number of clients and wan bandwidth we 
measured results from the two configurations listed below 
 native this configuration corresponds to figure a 
native avoids ganesh s overhead in using a proxy and 
performing java object serialization 
 ganesh this configuration corresponds to figure b for 
a given number of clients and wan bandwidth comparing 
these results to the corresponding native results gives the 
performance benefit due to the ganesh middleware system 
the metric used to quantify the improvement in throughput is 
the number of client requests that can be serviced per second the 
metric used to quantify ganesh s overhead is the average response 
time for a client request for all of the experiments the ganesh 
driver used by the application server used a cache size of 
items 
 the proxy was effective in tracking the ganesh driver s 
cache state for all of our experiments the miss rate on the driver 
never exceeded 
 experimental setup 
the experimental setup used for the benchmarks can be seen in 
figure all machines were ghz pentium s with 
hyperthreading enabled with the exception of the database server all 
machines had gb of sdram and ran the fedora core linux 
distribution the database server had gb of sdram 
we used apache s tomcat as both the application server that 
hosted the java servlets and the web server both benchmarks 
used java servlets to generate the dynamic content the database 
server used the open source mysql database for the native jdbc 
drivers we used the connector j drivers provided by mysql the 
application server used sun s java virtual machine as the runtime 
environment for the java servlets the sysstat tool was used to 
monitor the cpu network disk and memory utilization on all 
machines 
the machines were connected by a switched gigabit ethernet 
network as shown in figure the front-end web and 
application server was separated from the proxy and database server by a 
netem router this router allowed us to control the bandwidth 
and latency settings on the network the netem router is a 
standard pc with two network cards running the linux traffic control 
and network emulation software the bandwidth and latency 
constraints were only applied to the link between the application server 
and the database for the native case and between the application 
server and the proxy for the ganesh case there is no 
communication between the application server and the database with ganesh 
as all data flows through the proxy as our focus was on the wan 
link between the application server and the database there were no 
constraints on the link between the simulated test clients and the 
web server 
 throughput and response time 
in this section we address the first question raised in section 
can performance can be improved significantly by exploiting 
similarity across database results to answer this question we use 
results from the bboard and auction benchmarks we use 
two metrics to quantify the performance improvement obtainable 
through the use of ganesh throughput from the perspective of the 
web server and average response time from the perspective of the 
client throughput is measured in terms of the number of client 
requests that can be serviced per second 
 bboard results and analysis 
 authoring mix 
figures a and b present the average number of requests 
serviced per second and the average response time for these requests 
as perceived by the clients for bboard s authoring mix 
as figure a shows native easily saturates the mb s link 
at clients the native solution delivers requests sec with an 
average response time of seconds native s throughput drops 
with an increase in test clients as clients timeout due to 
congestion at the application server usability studies have shown that 
response times above seconds cause the user to move on to 
 
as java lacks a sizeof operator java caches therefore limit 
their size based on the number of objects the size of cache dumps 
taken at the end of the experiments never exceeded mb 
www track performance and scalability session scalable systems for dynamic content 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
requests sec 
native ganesh 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
avg resp time sec 
native ganesh 
note logscale 
 a throughput authoring mix b response time authoring mix 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
requests sec 
native ganesh 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
avg resp time sec 
native ganesh 
note logscale 
 c throughput browsing mix d response time browsing mix 
mean of three trials the maximum standard deviation for throughput and response time was and of the corresponding mean 
figure bboard benchmark - throughput and average response time 
other tasks based on these numbers increasing the 
number of test clients makes the native system unusable ganesh at 
 mb s however delivers a twofold improvement with test 
clients and a fivefold improvement at clients ganesh s 
performance drops slightly at and clients as the network is 
saturated compared to native figure b shows that ganesh s 
response times are substantially lower with sub-second response 
times at clients 
figure a also shows that for and test clients ganesh 
at mb s has the same throughput and average response time as 
native at mb s only at and clients does native at 
mb s deliver higher throughput than ganesh at mb s 
comparing both ganesh and native at mb s we see that 
ganesh is no longer bandwidth constrained and delivers up to a 
twofold improvement over native at test clients as ganesh 
does not saturate the network with higher test client configurations 
at test clients its average response time is seconds rather 
than native s seconds 
as expected there are no visible gains from ganesh at the higher 
bandwidth of mb s where the network is no longer the 
bottleneck ganesh however still tracks native in terms of throughput 
 browsing mix 
figures c and d present the average number of requests 
serviced per second and the average response time for these requests 
as perceived by the clients for bboard s browsing mix 
regardless of the test client configuration figure c shows 
that native s throughput at mb s is limited to reqs sec ganesh 
at mb s with test clients delivers more than a sixfold 
increase in throughput the improvement increases to over a 
elevenfold increase at test clients before ganesh saturates the 
network further figure d shows that native s average response 
time of seconds at test clients make the system unusable 
these high response times further increase with the addition of test 
clients even with the test client configuration ganesh 
delivers an acceptable average response time of seconds 
due to the data-intensive nature of the browsing mix ganesh at 
 mb s surprisingly performs much better than native at mb s 
further as shown in figure d while the average response time 
for native at mb s is acceptable at test clients it is unusable 
with test clients with an average response time of seconds 
like the mb s case this response time increases with the addition 
of extra test clients 
ganesh at mb s and both native and ganesh at mb s are 
not bandwidth limited however performance plateaus out after 
 test clients due to the database cpu being saturated 
 filter variant 
we were surprised by the native performance from the bboard 
benchmark at the bandwidth of mb s native performance was 
lower than what we had expected it turned out the benchmark 
code that displays stories read all the comments associated with 
the particular story from the database and only then did some 
postprocessing to select the comments to be displayed while this is 
exactly the behavior of slashcode the code base behind the 
slashdot web site we decided to modify the benchmark to perform some 
pre-filtering at the database this modified benchmark named the 
filter variant models a developer who applies optimizations at the 
sql level to transfer less data in the interests of brevity we only 
briefly summarize the results from the authoring mix 
for the authoring mix at test clients at mb s figure a 
shows that native s throughput increase by when compared 
to the original benchmark while ganesh s improvement is smaller 
at native s performance drops above clients as the test 
clients time out due to high response times the most significant 
gain for native is seen at mb s at test clients when 
compared to the original benchmark native sees a improvement 
in throughput and a reduction in average response time while 
www track performance and scalability session scalable systems for dynamic content 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
requests sec 
native ganesh 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
avg resp time sec 
native ganesh 
note logscale 
 a throughput authoring mix b response time authoring mix 
mean of three trials the maximum standard deviation for throughput and response time was and of the corresponding mean 
figure bboard benchmark - filter variant - throughput and average response time 
ganesh sees no improvement when compared to the original it still 
processes more requests sec than native thus while the 
optimizations were more helpful to native ganesh still delivers an 
improvement in performance 
 auction results and analysis 
 bidding mix 
figures a and b present the average number of requests 
serviced per second and the average response time for these requests 
as perceived by the clients for auction s bidding mix as 
mentioned earlier the bidding mix consists of a mixture of read and 
write operations 
the auction benchmark is not as data intensive as bboard 
therefore most of the gains are observed at the lower bandwidth 
of mb s figure a shows that the increase in throughput due 
to ganesh ranges from at test clients to with 
test clients as seen in figure b the average response times for 
ganesh are significantly lower than native ranging from a decrease 
of at test clients to at test clients 
figure a also shows that with a fourfold increase of 
bandwidth from mb s to mb s native is no longer bandwidth 
constrained and there is no performance difference between ganesh 
and native with the higher test client configurations we did 
observe that the bandwidth used by ganesh was lower than native 
ganesh might still be useful in these non-constrained scenarios if 
bandwidth is purchased on a metered basis similar results are seen 
for the mb s scenario 
 browsing mix 
for auction s browsing mix figures c and d present the 
average number of requests serviced per second and the average 
response time for these requests as perceived by the clients 
again most of the gains are observed at lower bandwidths at 
mb s native and ganesh deliver similar throughput and response 
times with test clients while the throughput for both remains 
the same at test clients figure d shows that ganesh s 
average response time is lower than native native saturates the 
link at clients and adding extra test clients only increases the 
average response time ganesh regardless of the test client 
configuration is not bandwidth constrained and maintains the same 
response time at test clients figure c shows that ganesh s 
throughput is almost twice that of native 
at the higher bandwidths of and mb s neither ganesh 
nor native is bandwidth limited and deliver equivalent throughput 
and response times 
benchmark orig size ganesh size rabin size 
selectsort mb mb mb 
selectsort mb mb mb 
table similarity microbenchmarks 
 structural vs rabin similarity 
in this section we address the second question raised in 
section how important is ganesh s structural similarity detection 
relative to rabin fingerprinting-based similarity detecting to 
answer this question we used microbenchmarks and the bboard and 
auction benchmarks as ganesh always performed better than 
rabin fingerprinting we only present a subset of the results here in 
the interests of brevity 
 microbenchmarks 
two microbenchmarks show an example of the effects of data 
reordering on rabin fingerprinting algorithm in the first 
microbenchmark selectsort a query with a specified sort order selects 
 mb of data spread over approximately k rows the 
query is then repeated with a different sort attribute while the 
same number of rows and the same data is returned the order of 
rows is different in such a scenario one would expect a large 
amount of similarity to be detected between both results as 
table shows ganesh s row-based algorithm achieves a 
reduction while the rabin fingerprinting algorithm with the average 
chunk size parameter set to kb only achieves a reduction 
the reason as shown earlier in figure is that with rabin 
fingerprinting the spans of data between two consecutive boundaries 
usually cross row boundaries with the order of the rows changing 
in the second result and the rabin fingerprints now spanning 
different rows the algorithm is unable to detect significant similarity 
the small gain seen is mostly for those single rows that are large 
enough to be broken into multiple chunks 
selectsort another micro-benchmark executed the same queries 
but increased the minimum chunk size of the rabin fingerprinting 
algorithm as can be seen in table even the small gain from the 
previous microbenchmark disappears as the minimum chunk size 
was greater than the average row size while one can partially 
address these problems by dynamically varying the parameters of the 
rabin fingerprinting algorithm this can be computationally 
expensive especially in the presence of changing workloads 
 application benchmarks 
we ran the bboard benchmark described in section on 
two versions of ganesh the first with rabin fingerprinting used as 
www track performance and scalability session scalable systems for dynamic content 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
requests sec 
native ganesh 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
avg resp time sec 
native ganesh 
note logscale 
 a throughput bidding mix b response time bidding mix 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
requests sec 
native ganesh 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
avg resp time sec 
native ganesh 
note logscale 
 c throughput browsing mix d response time browsing mix 
mean of three trials the maximum standard deviation for throughput and response time was and of the corresponding mean 
figure auction benchmark - throughput and average response time 
the chunking algorithm and the second with ganesh s row-based 
algorithm rabin s results for the browsing mix are normalized to 
ganesh s results and presented in figure 
as figure a shows at mb s independent of the test client 
configuration rabin significantly underperforms ganesh this 
happens because of a combination of two reasons first as outlined 
in section rabin finds less similarity as it does not exploit 
the result s structural information second this benchmark 
contained some queries that generated large results in this case 
rabin with a small average chunk size generated a large number of 
objects that evicted other useful data from the cache in contrast 
ganesh was able to detect these large rows and correspondingly 
increase the size of the chunks this was confirmed as cache 
statistics showed that ganesh s hit ratio was roughly three time that of 
rabin throughput measurements at mb s were similar with 
the exception of rabin s performance with test clients in this 
case ganesh was not network limited and in fact the throughput 
was the same as clients at mb s rabin however took 
advantage of the bandwidth increase from to mb s to deliver a 
slightly better performance at mb s rabin s throughput was 
almost similar to ganesh as bandwidth was no longer a bottleneck 
the normalized response time presented in figure b shows 
similar trends at and mb s the addition of test clients 
decreases the normalized response time as ganesh s average response 
time increases faster than rabin s however at no point does rabin 
outperform ganesh note that at and clients at mb s 
rabin does have a higher overhead even when it is not bandwidth 
constrained as mentioned in section this is due to the fact that 
rabin has to hash each resultset twice the overhead 
disappears with and clients as the database cpu is saturated 
and limits the performance of both ganesh and rabin 
 proxy overhead 
in this section we address the third question raised in section 
is the overhead of ganesh s proxy-based design acceptable to 
answer this question we concentrate on its performance at the higher 
bandwidths our evaluation in section showed that ganesh when 
compared to native can deliver a substantial throughput 
improvement at lower bandwidths it is only at higher bandwidths that 
latency measured by the average response time for a client request 
and throughput measured by the number of client requests that can 
be serviced per second overheads would be visible 
looking at the authoring mix of the original bboard 
benchmark there are no visible gains from ganesh at mb s ganesh 
however still tracks native in terms of throughput while the 
average response time is higher for ganesh the absolute difference is 
in between and seconds and would be imperceptible to 
the end-user the browsing mix shows an even smaller difference 
in average response times the results from the filter variant of the 
bboard benchmarks are similar even for the auction 
benchmark the difference between native and ganesh s response time at 
 mb s was never greater than seconds the only exception 
to the above results was seen in the filter variant of the bboard 
benchmark where ganesh at test clients added seconds 
to the average response time thus even for much faster networks 
where the wan link is not the bottleneck ganesh always delivers 
throughput equivalent to native while some extra latency is added 
by the proxy-based design it is usually imperceptible 
 related work 
to the best of our knowledge ganesh is the first system that 
combines the use of hash-based techniques with caching of database 
results to improve throughput and response times for applications 
with dynamic content we also believe that it is also the first 
system to demonstrate the benefits of using structural information for 
www track performance and scalability session scalable systems for dynamic content 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
norm throughput 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 mb s mb s mb s 
test clients 
norm responsetime 
 a normalized throughput higher is better b normalized response time higher is worse 
for throughput a normalized result greater than implies that rabin is better for response time a normalized result greater than implies 
that ganesh is better mean of three trials the maximum standard deviation for throughput and response time was and of 
the corresponding mean 
figure normalized comparison of ganesh vs rabin - bboard browsing mix 
detecting similarity in this section we first discuss alternative 
approaches to caching dynamic content and then examine other uses 
of hash-based primitives in distributed systems 
 caching dynamic content 
at the database layer a number of systems have advocated 
middletier caching where parts of the database are replicated at the edge or 
server these systems either cache entire tables in what 
is essentially a replicated database or use materialized views from 
previous query replies they require tight integration with the 
back-end database to ensure a time bound on the propagation of 
updates these systems are also usually targeted towards 
workloads that do not require strict consistency and can tolerate stale 
data further unlike ganesh some of these mid-tier caching 
solutions suffer from the complexity of having to participate in 
query planing and distributed query processing 
gao et al propose using a distributed object replication 
architecture where the data store s consistency requirements are 
adapted on a per-application basis these solutions require 
substantial developer resources and detailed understanding of the 
application being modified while systems that attempt to automate 
the partitioning and replication of an application s database 
exist they do not provide full transaction semantics in 
comparison ganesh does not weaken any of the semantics provided by 
the underlying database 
recent work in the evaluation of edge caching options for 
dynamic web sites has suggested that without careful planning 
employing complex offloading strategies can hurt performance 
instead the work advocates for an architecture in which all tiers 
except the database should be offloaded to the edge our evaluation of 
ganesh has shown that it would benefit these scenarios to improve 
database scalability c-jdbc sss and ganymed 
also advocate the use of an interposition-based architecture to 
transparently cluster and replicate databases at the middleware level 
the approaches of these architectures and ganesh are 
complementary and they would benefit each other 
moving up to the presentation layer there has been widespread 
adoption of fragment-based caching which improves cache 
utilization by separately caching different parts of generated web 
pages while fragment-based caching works at the edge a recent 
proposal has proposed moving web page assembly to the clients to 
optimize content delivery while ganesh is not used at the 
presentation layer the same principles have been applied in duplicate 
transfer detection to increase web cache efficiency as well as 
for web access across bandwidth limited links 
 hash-based systems 
the past few years have seen the emergence of many systems 
that exploit hash-based techniques at the heart of all these 
systems is the idea of detecting similarity in data without requiring 
interpretation of that data this simple yet elegant idea relies on 
cryptographic hashing as discussed earlier in section successful 
applications of this idea span a wide range of storage systems 
examples include peer-to-peer backup of personal computing files 
storage-efficient archiving of data and finding similar files 
spring and wetherall apply similar principles at the network 
level using synchronized caches at both ends of a network link 
duplicated data is replaced by smaller tokens for transmission and 
then restored at the remote end this and other hash-based systems 
such as the casper and lbfs filesystems and layer- 
bandwidth optimizers such as riverbed and peribit use rabin 
fingerprinting to discover spans of commonality in data this 
approach is especially useful when data items are modified in-place 
through insertions deletions and updates however as section 
shows the performance of this technique can show a dramatic drop 
in the presence of data reordering ganesh instead uses row 
boundaries as dividers for detecting similarity 
the most aggressive use of hash-based techniques is by systems 
that use hashes as the primary identifiers for objects in persistent 
storage storage systems such as cfs and past that 
have been built using distributed hash tables fall into this category 
single instance storage and venti are other examples of 
such systems as discussed in section the use of cryptographic 
hashes for addressing persistent data represents a deeper level of 
faith in their collision-resistance than that assumed by ganesh if 
time reveals shortcomings in the hash algorithm the effort involved 
in correcting the flaw is much greater in ganesh it is merely a 
matter of replacing the hash algorithm 
 conclusion 
the growing use of dynamic web content generated from 
relational databases places increased demands on wan bandwidth 
traditional caching solutions for bandwidth and latency reduction 
are often ineffective for such content this paper shows that the 
impact of wan accesses to databases can be substantially reduced 
through the ganesh architecture without any compromise of the 
database s strict consistency semantics the essence of the ganesh 
architecture is the use of computation at the edges to reduce 
communication through the internet ganesh is able to use 
cryptographic hashes to detect similarity with previous results and send 
www track performance and scalability session scalable systems for dynamic content 
 
compact recipes of results rather than full results our design uses 
interposition to achieve complete transparency clients application 
servers and database servers are all unaware of ganesh s presence 
and require no modification 
our experimental evaluation confirms that ganesh while 
conceptually simple can be highly effective in improving throughput 
and response time our results also confirm that exploiting the 
structure present in database results to detect similarity is crucial 
to this performance improvement 
 references 
 akella a seshan s and shaikh a an empirical 
evaluation of wide-area internet bottlenecks in proc rd 
acm sigcomm conference on internet measurement 
 miami beach fl usa oct pp - 
 altinel m bornh ¨ovd c krishnamurthy s 
mohan c pirahesh h and reinwald b cache 
tables paving the way for an adaptive database cache in 
proc of th vldb berlin germany pp - 
 altinel m luo q krishnamurthy s mohan 
c pirahesh h lindsay b g woo h and 
brown l dbcache database caching for web application 
servers in proc acm sigmod pp - 
 amiri k park s tewari r and padmanabhan 
s dbproxy a dynamic data cache for web applications in 
proc ieee international conference on data engineering 
 icde mar 
 black j compare-by-hash a reasoned analysis in proc 
 usenix annual technical conference boston ma 
may pp - 
 bolosky w j corbin s goebel d and 
douceur j r single instance storage in windows 
in proc th usenix windows systems symposium seattle 
wa aug pp - 
 brewer e a lessons from giant-scale services ieee 
internet computing - 
 broder a glassman s manasse m and 
zweig g syntactic clustering of the web in proc th 
international www conference 
 cecchet e chanda a elnikety s 
marguerite j and zwaenepoel w performance 
comparison of middleware architectures for generating 
dynamic web content in proc fourth acm ifip usenix 
international middleware conference rio de janeiro 
brazil june 
 cecchet e marguerite j and zwaenepoel w 
c-jdbc flexible database clustering middleware in proc 
 usenix annual technical conference boston ma 
june 
 cox l p murray c d and noble b d pastiche 
making backup cheap and easy in osdi symposium on 
operating systems design and implementation 
 dabek f kaashoek m f karger d morris 
r and stoica i wide-area cooperative storage with 
cfs in th acm symposium on operating systems 
principles banff canada oct 
 druschel p and rowstron a past a large-scale 
persistent peer-to-peer storage utility in hotos viii schloss 
elmau germany may pp - 
 edge side includes http www esi org 
 gao l dahlin m nayate a zheng j and 
iyengar a application specific data replication for edge 
services in www proc twelfth international 
conference on world wide web pp - 
 hemminger s netem - emulating real networks in the lab 
in proc linux conference australia canberra 
australia apr 
 henson v an analysis of compare-by-hash in proc th 
workshop on hot topics in operating systems hotos ix 
 may pp - 
 jmob benchmarks http jmob objectweb org 
 labrinidis a and roussopoulos n balancing 
performance and data freshness in web database servers in 
proc th vldb conference sept 
 larson p -a goldstein j and zhou j 
transparent mid-tier database caching in sql server in proc 
 acm sigmod pp - 
 manber u finding similar files in a large file system in 
proc usenix winter technical conference san 
fransisco ca - pp - 
 manjhi a ailamaki a maggs b m mowry 
t c olston c and tomasic a simultaneous 
scalability and security for data-intensive web applications 
in proc acm sigmod june pp - 
 menezes a j vanstone s a and oorschot p 
c v handbook of applied cryptography crc press 
 miller r b response time in man-computer 
conversational transactions in proc afips fall joint 
computer conference pp - 
 mogul j c chan y m and kelly t design 
implementation and evaluation of duplicate transfer 
detection in http in proc first symposium on networked 
systems design and implementation san francisco ca 
mar 
 muthitacharoen a chen b and mazieres d a 
low-bandwidth network file system in proc th acm 
symposium on operating systems principles banff canada 
oct 
 pfeifer d and jakschitsch h method-based 
caching in multi-tiered server applications in proc fifth 
international symposium on distributed objects and 
applications catania sicily italy nov 
 plattner c and alonso g ganymed scalable 
replication for transactional web applications in proc th 
acm ifip usenix international conference on 
middleware pp - 
 quinlan s and dorward s venti a new approach 
to archival storage in proc fast conference on file 
and storage technologies 
 rabin m fingerprinting by random polynomials in 
harvard university center for research in computing 
technology technical report tr- - 
 rabinovich m xiao z douglis f and 
kalmanek c moving edge side includes to the real edge 
- the clients in proc th usenix symposium on internet 
technologies and systems seattle wa mar 
 reese g database programming with jdbc and java 
 st ed o reilly june 
 rhea s liang k and brewer e value-based web 
caching in proc twelfth international world wide web 
conference may 
 sivasubramanian s alonso g pierre g and 
van steen m globedb autonomic data replication for 
web applications in www proc th international 
world-wide web conference may 
 spring n t and wetherall d a 
protocol-independent technique for eliminating redundant 
network traffic in proc of acm sigcomm aug 
 tolia n harkes j kozuch m and 
satyanarayanan m integrating portable and distributed 
storage in proc rd usenix conference on file and 
storage technologies san francisco ca mar 
 tolia n kozuch m satyanarayanan m karp 
b perrig a and bressoud t opportunistic use of 
content addressable storage for distributed file systems in 
proc usenix annual technical conference san 
antonio tx june pp - 
 yuan c chen y and zhang z evaluation of edge 
caching offloading for dynamic content delivery in www 
 proc twelfth international conference on world wide 
web pp - 
www track performance and scalability session scalable systems for dynamic content 
 
