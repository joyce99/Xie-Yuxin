exchanging reputation values among heterogeneous 
agent reputation models an experience on art testbed 
anarosa a f brandão 
 laurent vercouter 
 sara casare 
and jaime sichman 
 
laboratório de técnicas inteligentes - ep usp 
av prof luciano gualberto trav - são paulo - brazil 
 
anarosabrandao gmail com {sara casare jaime sichman} poli usp br 
 
ecole nationale supérieure des mines de saint-etienne 
 cours fauriel saint-etienne cedex france 
laurent vercouter emse fr 
abstract 
in open mas it is often a problem to achieve agents 
interoperability the heterogeneity of its components turns the 
establishment of interaction or cooperation among them into a 
non trivial task since agents may use different internal models 
and the decision about trust other agents is a crucial condition to 
the formation of agents cooperation in this paper we propose the 
use of an ontology to deal with this issue we experiment this idea 
by enhancing the art reputation model with semantic data 
obtained from this ontology this data is used during interaction 
among heterogeneous agents when exchanging reputation values 
and may be used for agents that use different reputation models 
categories and subject descriptors 
i distributed artificial intelligence multiagent systems 
general terms 
design experimentation standardization 
 introduction 
open multiagent systems mas are composed of autonomous 
distributed agents that may enter and leave the agent society at 
their will because open systems have no centralized control over 
the development of its parts since agents are considered as 
autonomous entities we cannot assume that there is a way to 
control their internal behavior these features are interesting to 
obtain flexible and adaptive systems but they also create new risks 
about the reliability and the robustness of the system solutions to 
this problem have been proposed by the way of trust models 
where agents are endowed with a model of other agents that 
allows them to decide if they can or cannot trust another agent 
such trust decision is very important because it is an essential 
condition to the formation of agents cooperation the trust 
decision processes use the concept of reputation as the basis of a 
decision reputation is a subject that has been studied in several 
works with different approaches but also with 
different semantics attached to the reputation concept casare and 
sichman proposed a functional ontology of reputation 
 fore and some directions about how it could be used to allow 
the interoperability among different agent reputation models this 
paper describes how the fore can be applied to allow 
interoperability among agents that have different reputation 
models an outline of this approach is sketched in the context of a 
testbed for the experimentation and comparison of trust models 
the art testbed 
 the functional ontology of 
reputation fore 
in the last years several computational models of reputation have 
been proposed as an example of research 
produced in the mas field we refer to three of them a cognitive 
reputation model a typology of reputation and the 
reputation model used in the regret system each model 
includes its own specific concepts that may not exist in other 
models or exist with a different name for instance image and 
reputation are two central concepts in the cognitive reputation 
model these concepts do not exist in the typology of reputation 
or in the regret model in the typology of reputation we can find 
some similar concepts such as direct reputation and indirect 
reputation but there are some slight semantic differences in the 
same way the regret model includes four kinds of reputation 
 direct witness neighborhood and system that overlap with the 
concepts of other models but that are not exactly the same 
the functional ontology of reputation fore was defined as a 
common semantic basis that subsumes the concepts of the main 
reputation models the fore includes as its kernel the following 
concepts reputation nature roles involved in reputation formation 
and propagation information sources for reputation evaluation of 
reputation and reputation maintenance the ontology concept 
reputationnature is composed of concepts such as 
individualreputation groupreputation and productreputation 
reputation formation and propagation involves several roles 
played by the entities or agents that participate in those processes 
the ontology defines the concepts reputationprocess and 
reputationrole moreover reputation can be classified according 
to the origin of beliefs and opinions that can derive from several 
sources the ontology defines the concept reputationtype which 
can be primaryreputation or secondaryreputation 
primaryreputation is composed of concepts observedreputation 
and directreputation and the concept secondaryreputation is 
composed of concepts such as propagatedreputation and 
collectivereputation more details about the fore can be found 
on 
 mapping the agent reputation 
models to the fore 
visser et al suggest three different ways to support semantic 
integration of different sources of information a centralized 
approach where each source of information is related to one 
common domain ontology a decentralized approach where every 
source of information is related to its own ontology and a hybrid 
approach where every source of information has its own ontology 
and the vocabulary of these ontologies are related to a common 
ontology this latter organizes the common global vocabulary in 
order to support the source ontologies comparison casare and 
sichman used the hybrid approach to show that the fore 
serves as a common ontology for several reputation models 
therefore considering the ontologies which describe the agent 
reputation models we can define a mapping between these 
ontologies and the fore whenever the ontologies use a common 
vocabulary also the information concerning the mappings 
between the agent reputation models and the fore can be directly 
inferred by simply classifying the resulting ontology from the 
integration of a given reputation model ontology and the fore in 
an ontology tool with reasoning engine 
for instance a mapping between the cognitive reputation model 
ontology and the fore relates the concepts image and reputation 
to primaryreputation and secondaryreputation from fore 
respectively also a mapping between the typology of 
reputation and the fore relates the concepts direct reputation 
and indirect reputation to primaryreputation and 
secondaryreputation from fore respectively nevertheless the 
concepts direct trust and witness reputation from the regret 
system reputation model are mapped to primaryreputation and 
propagatedreputation from fore since propagatedreputation is 
a sub-concept of secondaryreputation it can be inferred that 
witness reputation is also mapped to secondaryreputation 
 experimental scenarios using 
the art testbed 
to exemplify the use of mappings from last section we define a 
scenario where several agents are implemented using different 
agent reputation models this scenario includes the agents 
interaction during the simulation of the game defined by art 
in order to describe the ways interoperability is possible between 
different trust models using the fore 
 the art testbed 
the art testbed provides a simulation engine on which several 
agents using different trust models may run the simulation 
consists in a game where the agents have to decide to trust or not 
other agents the game s domain is art appraisal in which agents 
are required to evaluate the value of paintings based on 
information exchanged among other agents during agents 
interaction the information can be an opinion transaction when 
an agent asks other agents to help it in its evaluation of a painting 
or a reputation transaction when the information required is 
about the reputation of another agent a target for a given era 
more details about the art testbed can be found in 
the art common reputation model was enhanced with semantic 
data obtained from fore a general agent architecture for 
interoperability was defined to allow agents to reason about 
the information received from reputation interactions this 
architecture contains two main modules the reputation mapping 
module rmm which is responsible for mapping concepts 
between an agent reputation model and fore and the reputation 
reasoning module rrm which is responsible for deal with 
information about reputation according to the agent reputation 
model 
 reputation transaction scenarios 
while including the fore to the art common reputation model 
we have incremented it to allow richer interactions that involve 
reputation transaction in this section we describe scenarios 
concerning reputation transactions in the context of art testbed 
but the first is valid for any kind of reputation transaction and the 
second is specific for the art domain 
 general scenario 
suppose that agents a b and c are implemented according to the 
aforementioned general agent architecture with the enhanced art 
common reputation model using different reputation models 
agent a uses the typology of reputation model agent b uses the 
cognitive reputation model and agent c uses the regret system 
model consider the interaction about reputation where agents a 
and b receive from agent c information about the reputation of 
agent y a big picture of this interaction is showed in figure 
regret 
ontology 
 y value 
witnessreputation 
c 
typol 
ontology 
 y value 
propagatedreputation 
a 
cogmod 
ontology 
 y value 
reputation 
b 
 y value 
propagatedreputation 
 y value 
propagatedreputation 
regret 
ontology 
 y value 
witnessreputation 
c 
regret 
ontology 
 y value 
witnessreputation 
regret 
ontology 
 y value 
witnessreputation 
 y value 
witnessreputation 
c 
typol 
ontology 
 y value 
propagatedreputation 
a 
typol 
ontology 
 y value 
propagatedreputation 
typol 
ontology 
 y value 
propagatedreputation 
 y value 
propagatedreputation 
a 
cogmod 
ontology 
 y value 
reputation 
b 
cogmod 
ontology 
 y value 
reputation 
cogmod 
ontology 
 y value 
reputation 
 y value 
reputation 
b 
 y value 
propagatedreputation 
 y value 
propagatedreputation 
 y value 
propagatedreputation 
 y value 
propagatedreputation 
figure interaction about reputation 
the information witness reputation from agent c is treated by its 
rmm and is sent as propagatedreputation to both agents the 
corresponding information in agent a reputation model is 
propagated reputation and in agent b reputation model is 
reputation the way agents a and b make use of the information 
depends on their internal reputation model and their rrm 
implementation 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 art scenario 
considering the same agents a and b and the art appraisal domain 
of art another interesting scenario describes the following 
situation agent a asks to agent b information about agents it 
knows that have skill on some specific painting era in this case 
agent a wants information concerning the direct reputation agent 
b has about agents that have skill on an specific era such as 
cubism following the same steps of the previous scenario agent 
a message is prepared in its rrm using information from its 
internal model a big picture of this interaction is in figure 
typol 
ontology 
 agent value 
skill cubism 
reputation directreputation 
a 
 agent value skill cubism 
reputation primaryreputation 
cogmod 
ontology 
 agent value 
skill cubism 
reputation image 
b 
typol 
ontology 
 agent value 
skill cubism 
reputation directreputation 
a 
 agent value skill cubism 
reputation primaryreputation 
cogmod 
ontology 
 agent value 
skill cubism 
reputation image 
b 
figure interaction about specific types of reputation values 
agent b response to agent a is processed in its rrm and it is 
composed of tuples agent value cubism image where 
the pair agent value is composed of all agents and associated 
reputation values whose agent b knows their expertise about 
cubism by its own opinion this response is forwarded to the 
rmm in order to be translated to the enriched common model and 
to be sent to agent a after receiving the information sent by 
agent b agent a processes it in its rmm and translates it to its 
own reputation model to be analyzed by its rrm 
 conclusion 
in this paper we present a proposal for reducing the 
incompatibility between reputation models by using a general 
agent architecture for reputation interaction which relies on a 
functional ontology of reputation fore used as a globally 
shared reputation model a reputation mapping module allows 
agents to translate information from their internal reputation 
model into the shared model and vice versa the art testbed has 
been enriched to use the ontology during agent transactions some 
scenarios were described to illustrate our proposal and they seem 
to be a promising way to improve the process of building 
reputation just using existing technologies 
 acknowledgments 
anarosa a f brandão is supported by cnpq brazil grant 
 - and jaime sichman is partially supported by 
cnpq brazil grants - - and 
 - laurent vercouter was partially supported by 
fapesp grant - 
 references 
 agha g a abstracting interaction patterns a 
programming paradigm for open distributed systems in 
 eds e najm and j -b stefani formal methods for open 
object-based distributed systems ifip transactions 
chapman hall 
 casare s and sichman j s towards a functional ontology 
of reputation in proc of the th 
intl joint conference on 
autonomous agents and multi agent systems aamas 
utrecht the netherlands v pp - 
 casare s and sichman j s using a functional ontology of 
reputation to interoperate different agent reputation 
models journal of the brazilian computer society 
 pp - 
 castelfranchi c and falcone r principles of trust in mas 
cognitive anatomy social importance and quantification in 
proceedings of icmas paris pp - 
 conte r and paolucci m reputation in artificial societies 
social beliefs for social order kluwer publ 
 fullam k klos t muller g sabater j topol z 
barber s rosenchein j vercouter l and voss m a 
specification of the agent reputation and trust art testbed 
experimentation and competition for trust in agent societies 
in proc of the th 
intl joint conf on autonomous agents 
and multiagent systems aamas acm - 
 mui l halberstadt a mohtashemi m notions of 
reputation in multi-agents systems a review in proc of 
 st intl joint conf on autonomous agents and multi-agent 
systems aamas bologna italy - 
 muller g and vercouter l decentralized monitoring of 
agent communication with a reputation model in trusting 
agents for trusting electronic societies lncs 
pp - 
 sabater j and sierra c regret reputation in gregarious 
societies in müller j et al eds proc of the th 
intl conf 
on autonomous agents canada acm - 
 sabater j and sierra c review on computational trust 
and reputation models in artificial intelligence review 
kluwer acad publ v n pp - 
 vercouter l casare s sichman j and brandão a an 
experience on reputation models interoperability based on a 
functional ontology in proc of the th 
ijcai hyderabad 
india pp - 
 visser u stuckenschmidt h wache h and vogele t 
enabling technologies for inter-operability in in u visser 
and h pundt eds workshop on the th intl symp of 
computer science for environmental protection bonn 
germany pp - 
 yu b and singh m p an evidential model of distributed 
reputation management in proc of the st intl joint conf 
on autonomous agents and multi-agent systems aamas 
 bologna italy part pp - 
 zacharia g and maes p trust management through 
reputation mechanisms in applied artificial intelligence 
 pp - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a unified and general framework for 
argumentation-based negotiation 
leila amgoud 
irit - cnrs 
 route de narbonne 
 toulouse france 
amgoud irit fr 
yannis dimopoulos 
university of cyprus 
 kallipoleos str 
po box cyprus 
yannis cs ucy ac cy 
pavlos moraitis 
paris-descartes university 
 rue des saints-pères 
 paris cedex france 
 
pavlos math-info univparis fr 
abstract 
this paper proposes a unified and general framework for 
argumentation-based negotiation in which the role of 
argumentation is formally analyzed the framework makes it 
possible to study the outcomes of an argumentation-based 
negotiation it shows what an agreement is how it is related 
to the theories of the agents when it is possible and how 
this can be attained by the negotiating agents in this case 
it defines also the notion of concession and shows in which 
situation an agent will make one as well as how it influences 
the evolution of the dialogue 
categories and subject descriptors 
i deduction and theorem proving 
nonmonotonic reasoning and belief revision 
 i distributed artificial intelligence intelligent 
agents 
general terms 
human factors theory 
 introduction 
roughly speaking negotiation is a process aiming at 
finding some compromise or consensus between two or several 
agents about some matters of collective agreement such 
as pricing products allocating resources or choosing 
candidates negotiation models have been proposed for the 
design of systems able to bargain in an optimal way with 
other agents for example buying or selling products in 
ecommerce 
different approaches to automated negotiation have been 
investigated including game-theoretic approaches which 
usually assume complete information and unlimited 
computation capabilities heuristic-based approaches which try 
to cope with these limitations and argumentation-based 
approaches which emphasize the 
importance of exchanging information and explanations between 
negotiating agents in order to mutually influence their 
behaviors e g an agent may concede a goal having a small 
priority and consequently the outcome of the dialogue 
indeed the two first types of settings do not allow for the 
addition of information or for exchanging opinions about offers 
integrating argumentation theory in negotiation provides a 
good means for supplying additional information and also 
helps agents to convince each other by adequate arguments 
during a negotiation dialogue indeed an offer supported 
by a good argument has a better chance to be accepted by 
an agent and can also make him reveal his goals or give 
up some of them the basic idea behind an 
argumentationbased approach is that by exchanging arguments the 
theories of the agents i e their mental states may evolve and 
consequently the status of offers may change for instance 
an agent may reject an offer because it is not acceptable for 
it however the agent may change its mind if it receives a 
strong argument in favor of this offer 
several proposals have been made in the literature for 
modeling such an approach however the work is still 
preliminary some researchers have mainly focused on relating 
argumentation with protocols they have shown how and 
when arguments in favor of offers can be computed and 
exchanged others have emphasized on the decision making 
problem in the authors argued that selecting an offer 
to propose at a given step of the dialogue is a decision 
making problem they have thus proposed an 
argumentationbased decision model and have shown how such a model 
can be related to the dialogue protocol 
in most existing works there is no deep formal analysis 
of the role of argumentation in negotiation dialogues it is 
not clear how argumentation can influence the outcome of 
the dialogue moreover basic concepts in negotiation such 
as agreement i e optimal solutions or compromise and 
concession are neither defined nor studied 
this paper aims to propose a unified and general framework 
for argumentation-based negotiation in which the role of 
argumentation is formally analyzed and where the existing 
systems can be restated in this framework a negotiation 
dialogue takes place between two agents on a set o of offers 
whose structure is not known the goal of a negotiation is to 
find among elements of o an offer that satisfies more or less 
 
 - - - - rps c ifaamas 
the preferences of both agents each agent is supposed to 
have a theory represented in an abstract way a theory 
consists of a set a of arguments whose structure and origin are 
not known a function specifying for each possible offer in o 
the arguments of a that support it a non specified conflict 
relation among the arguments and finally a preference 
relation between the arguments the status of each argument is 
defined using dung s acceptability semantics consequently 
the set of offers is partitioned into four subsets acceptable 
rejected negotiable and non-supported offers we show how 
an agent s theory may evolve during a negotiation dialogue 
we define formally the notions of concession compromise 
and optimal solution then we propose a protocol that 
allows agents i to exchange offers and arguments and ii to 
make concessions when necessary we show that dialogues 
generated under such a protocol terminate and even reach 
optimal solutions when they exist 
this paper is organized as follows section introduces the 
logical language that is used in the rest of the paper 
section defines the agents as well as their theories in section 
 we study the properties of these agents theories 
section defines formally an argumentation-based negotiation 
shows how the theories of agents may evolve during a 
dialogue and how this evolution may influence the outcome of 
the dialogue two kinds of outcomes optimal solution and 
compromise are defined and we show when such outcomes 
are reached section illustrates our general framework 
through some examples section compares our formalism 
with existing ones section concludes and presents some 
perspectives due to lack of space the proofs are not 
included these last are in a technical report that we will 
make available online at some later time 
 the logical language 
in what follows l will denote a logical language and ≡ 
is an equivalence relation associated with it 
from l a set o {o on} of n offers is identified such 
that oi oj ∈ o such that oi ≡ oj this means that the 
offers are different offers correspond to the different 
alternatives that can be exchanged during a negotiation dialogue 
for instance if the agents try to decide the place of their 
next meeting then the set o will contain different towns 
different arguments can be built from l the set args l 
will contain all those arguments by argument we mean a 
reason in believing or of doing something in it has been 
argued that the selection of the best offer to propose at a 
given step of the dialogue is a decision problem in it has 
been shown that in an argumentation-based approach for 
decision making two kinds of arguments are distinguished 
arguments supporting choices or decisions and arguments 
supporting beliefs moreover it has been acknowledged that 
the two categories of arguments are formally defined in 
different ways and they play different roles indeed an 
argument in favor of a decision built both on an agent s 
beliefs and goals tries to justify the choice whereas an 
argument in favor of a belief built only from beliefs tries 
to destroy the decision arguments in particular the beliefs 
part of those decision arguments consequently in a 
negotiation dialogue those two kinds of arguments are generally 
exchanged between agents in what follows the set args l 
is then divided into two subsets a subset argso l of 
arguments supporting offers and a subset argsb l of arguments 
supporting beliefs thus args l argso l ∪ argsb l 
as in in what follows we consider that the structure of 
the arguments is not known 
since the knowledge bases from which arguments are built 
may be inconsistent the arguments may be conflicting too 
in what follows those conflicts will be captured by the 
relation rl thus rl ⊆ args l × args l three assumptions 
are made on this relation first the arguments supporting 
different offers are conflicting the idea behind this 
assumption is that since offers are exclusive an agent has to choose 
only one at a given step of the dialogue note that the 
relation rl is not necessarily symmetric between the 
arguments of argsb l the second hypothesis says that 
arguments supporting the same offer are also conflicting the 
idea here is to return the strongest argument among these 
arguments the third condition does not allow an argument 
in favor of an offer to attack an argument supporting a 
belief this avoids wishful thinking formally 
definition rl ⊆ args l × args l is a conflict 
relation among arguments such that 
 ∀a a ∈ argso l s t a a a rl a 
 a ∈ argso l and a ∈ argsb l such that a rl a 
note that the relation rl is not symmetric this is due 
to the fact that arguments of argsb l may be conflicting 
but not necessarily in a symmetric way in what follows we 
assume that the set args l of arguments is finite and each 
argument is attacked by a finite number of arguments 
 negotiating agents theories and 
reasoning models 
in this section we define formally the negotiating agents 
i e their theories as well as the reasoning model used by 
those agents in a negotiation dialogue 
 negotiating agents theories 
agents involved in a negotiation dialogue called 
negotiating agents are supposed to have theories in this paper the 
theory of an agent will not refer as usual to its mental states 
 i e its beliefs desires and intentions however it will be 
encoded in a more abstract way in terms of the arguments 
owned by the agent a conflict relation among those 
arguments a preference relation between the arguments and a 
function that specifies which arguments support offers of the 
set o we assume that an agent is aware of all the 
arguments of the set args l the agent is even able to express 
a preference between any pair of arguments this does not 
mean that the agent will use all the arguments of args l 
but it encodes the fact that when an agent receives an 
argument from another agent it can interpret it correctly and it 
can also compare it with its own arguments similarly each 
agent is supposed to be aware of the conflicts between 
arguments this also allows us to encode the fact that an agent 
can recognize whether the received argument is in conflict 
or not with its arguments however in its theory only the 
conflicts between its own arguments are considered 
definition negotiating agent theory let o 
be a set of n offers a negotiating agent theory is a tuple 
a f r def such that 
 a ⊆ args l 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 f o → a 
s t ∀i j with i j f oi ∩ f oj ∅ 
let ao ∪f oi with i n 
 ⊆ args l × args l is a partial preorder denoting 
a preference relation between arguments 
 r ⊆ rl such that r ⊆ a × a 
 def ⊆ a × a such that ∀ a b ∈ a a defeats b denoted 
a def b iff 
- a r b and 
- not b a 
the function f returns the arguments supporting offers in 
o in it has been argued that any decision may have 
arguments supporting it called arguments pro and 
arguments against it called arguments cons moreover these 
two types of arguments are not necessarily conflicting for 
simplicity reasons in this paper we consider only arguments 
pro moreover we assume that an argument cannot 
support two distinct offers however it may be the case that an 
offer is not supported at all by arguments thus f oi may 
be empty 
example let o {o o o } be a set of offers the 
following theory is the theory of agent i 
 a {a a a a } 
 f o {a } f o {a } f o ∅ thus ao 
{a a } 
 { a a a a a a a a } 
 r {a a a a a a a a } 
 def { a a a a } 
from the above definition of agent theory the following hold 
property 
 def ⊆ r 
 ∀a a ∈ f oi a r a 
 the reasoning model 
from the theory of an agent one can define the 
argumentation system used by that agent for reasoning about the 
offers and the arguments i e for computing the status of 
the different offers and arguments 
definition argumentation system let a f 
 r def be the theory of an agent the argumentation 
system of that agent is the pair a def 
in different acceptability semantics have been introduced 
for computing the status of arguments these are based 
on two basic concepts defence and conflict-free defined as 
follows 
definition defence conflict-free let s ⊆ a 
 s defends an argument a iff each argument that defeats 
a is defeated by some argument in s 
 s is conflict-free iff there exist no a a in s such that 
a def a 
definition acceptability semantics let s be 
a conflict-free set of arguments and let t a 
→ a 
be a 
function such that t s {a a is defended by s} 
 s is a complete extension iff s t s 
 s is a preferred extension iff s is a maximal w r t set 
⊆ complete extension 
 s is a grounded extension iff it is the smallest w r t 
set ⊆ complete extension 
let e ex denote the different extensions under a given 
semantics 
note that there is only one grounded extension it 
contains all the arguments that are not defeated and those 
arguments that are defended directly or indirectly by 
nondefeated arguments 
theorem let a def the argumentation system 
defined as shown above 
 it may have x ≥ preferred extensions 
 the grounded extensions is s i≥ 
t ∅ 
note that when the grounded extension or the preferred 
extension is empty this means that there is no acceptable 
offer for the negotiating agent 
example in example there is one preferred 
extension e {a a a } 
now that the acceptability semantics is defined we are ready 
to define the status of any argument 
definition argument status let a def be an 
argumentation system and e ex its extensions under a 
given semantics let a ∈ a 
 a is accepted iff a ∈ ei ∀ei with i x 
 a is rejected iff ei such that a ∈ ei 
 a is undecided iff a is neither accepted nor rejected 
this means that a is in some extensions and not in 
others 
note that a {a a is accepted} ∪ {a a is rejected} ∪ {a a 
is undecided} 
example in example the arguments a a and a 
are accepted whereas the argument a is rejected 
as said before agents use argumentation systems for 
reasoning about offers in a negotiation dialogue agents propose 
and accept offers that are acceptable for them and reject 
bad ones in what follows we will define the status of an 
offer according to the status of arguments one can define 
four statuses of the offers as follows 
definition offers status let o ∈ o 
 the offer o is acceptable for the negotiating agent iff 
∃ a ∈ f o such that a is accepted oa {oi ∈ o 
such that oi is acceptable} 
 the offer o is rejected for the negotiating agent iff ∀ 
a ∈ f o a is rejected or {oi ∈ o such that oi is 
rejected} 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 the offer o is negotiable iff ∀ a ∈ f o a is undecided 
on {oi ∈ o such that oi is negotiable} 
 the offer o is non-supported iff it is neither 
acceptable nor rejected or negotiable ons {oi ∈ o such 
that oi is non-supported offers} 
example in example the two offers o and o are 
acceptable since they are supported by accepted arguments 
whereas the offer o is non-supported since it has no 
argument in its favor 
from the above definitions the following results hold 
property let o ∈ o 
 o oa ∪ or ∪ on ∪ ons 
 the set oa may contain more than one offer 
from the above partition of the set o of offers a preference 
relation between offers is defined let ox and oy be two 
subsets of o ox oy means that any offer in ox is 
preferred to any offer in the set oy we can write also for two 
offers oi oj oi oj iff oi ∈ ox oj ∈ oy and ox oy 
definition preference between offers let o 
be a set of offers and oa or on ons its partition oa 
on ons or 
example in example we have o o and o o 
however o and o are indifferent 
 the structure of negotiation 
theories 
in this section we study the properties of the system 
developed above we first show that in the particular case 
where a ao ie all of the agent s arguments refer to 
offers the corresponding argumentation system will return 
at least one non-empty preferred extension 
theorem let a def an argumentation system such 
that a ao then the system returns at least one 
extension e such that e ≥ 
we now present some results that demonstrate the 
importance of indifference in negotiating agents and more 
specifically its relation to acceptable outcomes we first show that 
the set oa may contain several offers when their 
corresponding accepted arguments are indifferent w r t the preference 
relation 
theorem let o o ∈ o o o ∈ oa iff ∃ a ∈ 
f o ∃ a ∈ f o such that a and a are accepted and 
are indifferent w r t i e a b and b a 
we now study acyclic preference relations that are defined 
formally as follows 
definition acyclic relation a relation r on 
a set a is acyclic if there is no sequence a a an ∈ a 
with n such that ai ai ∈ r and an a ∈ r with 
 ≤ i n 
note that acyclicity prohibits pairs of arguments a b such 
that a b and b a ie an acyclic preference relation 
disallows indifference 
theorem let a be a set of arguments r the 
attacking relation of a defined as r ⊆ a × a and an acyclic 
relation on a then for any pair of arguments a b ∈ a 
such that a b ∈ r either a b ∈ def or b a ∈ def or 
both 
the previous result is used in the proof of the following 
theorem that states that acyclic preference relations 
sanction extensions that support exactly one offer 
theorem let a be a set of arguments and an 
acyclic relation on a if e is an extension of a def 
then e ∩ ao 
an immediate consequence of the above is the following 
property let a be a set of arguments such that a 
ao if the relation on a is acyclic then each extension 
ei of a def ei 
another direct consequence of the above theorem is that 
in acyclic preference relations arguments that support offers 
can participate in only one preferred extension 
theorem let a be a set of arguments and an 
acyclic relation on a then the preferred extensions of 
a def are pairwise disjoint w r t arguments of ao 
using the above results we can prove the main theorem of 
this section that states that negotiating agents with acyclic 
preference relations do not have acceptable offers 
theorem let a f r def be a negotiating 
agent such that a ao and is an acyclic relation then 
the set of accepted arguments w r t a def is emtpy 
consequently the set of acceptable offers oa is empty as well 
 argumentation-based negotiation 
in this section we define formally a protocol that 
generates argumentation-based negotiation dialogues between 
two negotiating agents p and c the two agents 
negotiate about an object whose possible values belong to a set 
o this set o is supposed to be known and the same for 
both agents for simplicity reasons we assume that this 
set does not change during the dialogue the agents are 
equipped with theories denoted respectively ap 
 fp 
 p 
 
rp 
 defp 
 and ac 
 fc 
 c 
 rc 
 defc 
 note that the 
two theories may be different in the sense that the agents 
may have different sets of arguments and different 
preference relations worst yet they may have different 
arguments in favor of the same offers moreover these theories 
may evolve during the dialogue 
 evolution of the theories 
before defining formally the evolution of an agent s theory 
let us first introduce the notion of dialogue moves or moves 
for short 
definition move a move is a tuple mi pi 
ai oi ti such that 
 pi ∈ {p c} 
 ai ∈ args l ∪ θ 
 
in what follows θ denotes the fact that no argument or no 
offer is given 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 oi ∈ o ∪ θ 
 ti ∈ n∗ 
is the target of the move such that ti i 
the function player resp argument offer target 
returns the player of the move i e pi resp the argument 
of a move i e ai the offer oi and the target of the move 
ti let m denote the set of all the moves that can be built 
from {p c} arg l o 
note that the set m is finite since arg l and o are 
assumed to be finite let us now see how an agent s theory 
evolves and why the idea is that if an agent receives an 
argument from another agent it will add the new argument 
to its theory moreover since an argument may bring new 
information for the agent thus new arguments can emerge 
let us take the following example 
example suppose that an agent p has the following 
propositional knowledge base σp {x y → z} from this 
base one cannot deduce z let s assume that this agent 
receives the following argument {a a → y} that justifies y 
it is clear that now p can build an argument say {a a → 
y y → z} in favor of z 
in a similar way if a received argument is in conflict with the 
arguments of the agent i then those conflicts are also added 
to its relation ri 
 note that new conflicts may arise between 
the original arguments of the agent and the ones that emerge 
after adding the received arguments to its theory those new 
conflicts should also be considered as a direct consequence 
of the evolution of the sets ai 
and ri 
 the defeat relation 
defi 
is also updated 
the initial theory of an agent i i e its theory before the 
dialogue starts is denoted by ai 
 fi 
 i 
 ri 
 defi 
 with 
i ∈ {p c} besides in this paper we suppose that the 
preference relation i 
of an agent does not change during 
the dialogue 
definition theory evolution let m mt 
 mj be a sequence of moves the theory of an agent i at 
a step t is ai 
t fi 
t i 
t ri 
t defi 
t such that 
 ai 
t ai 
 ∪ {ai i t ai argument mi } ∪ 
a with a ⊆ args l 
 fi 
t o → ai 
t 
 i 
t i 
 
 ri 
t ri 
 ∪ { ai aj ai argument mi 
aj argument mj i j ≤ t and ai rl aj} ∪ r with 
r ⊆ rl 
 defi 
t ⊆ ai 
t × ai 
t 
the above definition captures the monotonic aspect of an 
argument indeed an argument cannot be removed 
however its status may change an argument that is accepted 
at step t of the dialogue by an agent may become rejected 
at step t i consequently the status of offers also change 
thus the sets oa or on and ons may change from one 
step of the dialogue to another that means for example 
that some offers could move from the set oa to the set or 
and vice-versa note that in the definition of rt the 
relation rl is used to denote a conflict between exchanged 
arguments the reason is that such a conflict may not be 
in the set ri 
of the agent i thus in order to recognize 
such conflicts we have supposed that the set rl is known 
to the agents this allows us to capture the situation where 
an agent is able to prove an argument that it was unable 
to prove before by incorporating in its beliefs some 
information conveyed through the exchange of arguments with 
another agent this unknown at the beginning of the 
dialogue argument could give to this agent the possibility to 
defeat an argument that it could not by using its initial 
arguments this could even lead to a change of the status of 
these initial arguments and this change would lead to the 
one of the associated offers status 
in what follows oi 
t x denotes the set of offers of type x 
where x ∈ {a n r ns} of the agent i at step t of the 
dialogue in some places we can use for short the notation oi 
t 
to denote the partition of the set o at step t for agent i 
note that we have not oi 
t x ⊆ oi 
t x 
 the notion of agreement 
as said in the introduction negotiation is a process aiming 
at finding an agreement about some matters by agreement 
one means a solution that satisfies to the largest possible 
extent the preferences of both agents in case there is no such 
solution we say that the negotiation fails in what follows 
we will discuss the different kinds of solutions that may be 
reached in a negotiation the first one is the optimal 
solution an optimal solution is the best offer for both agents 
formally 
definition optimal solution let o be a set 
of offers and o ∈ o the offer o is an optimal solution at 
a step t ≥ iff o ∈ op 
t a ∩ oc 
t a 
such a solution does not always exist since agents may have 
conflicting preferences thus agents make concessions by 
proposing accepting less preferred offers 
definition concession let o ∈ o be an offer 
the offer o is a concession for an agent i iff o ∈ oi 
x such 
that ∃oi 
y ∅ and oi 
y oi 
x 
during a negotiation dialogue agents exchange first their 
most preferred offers and if these last are rejected they 
make concessions in this case we say that their best offers 
are no longer defendable in an argumentation setting this 
means that the agent has already presented all its arguments 
supporting its best offers and it has no counter argument 
against the ones presented by the other agent formally 
definition defendable offer let ai 
t fi 
t i 
t 
ri 
t defi 
t be the theory of agent i at a step t of the 
dialogue let o ∈ o such that ∃j ≤ t with player mj i and 
offer mj o the offer o is defendable by the agent i iff 
 ∃a ∈ fi 
t o and k ≤ t s t argument mk a or 
 ∃a ∈ at 
\fi 
t o s t a defi 
t b with 
- argument mk b k ≤ t and player mk i 
- l ≤ t argument ml a 
the offer o is said non-defendable otherwise and ndi 
t is the 
set of non-defendable offers of agent i at a step t 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 negotiation dialogue 
now that we have shown how the theories of the agents 
evolve during a dialogue we are ready to define formally 
an argumentation-based negotiation dialogue for that 
purpose we need to define first the notion of a legal 
continuation 
definition legal move a move m is a legal 
continuation of a sequence of moves m ml iff j k l 
such that 
 offer mj offer mk and 
 player mj player mk 
the idea here is that if the two agents present the same 
offer then the dialogue should terminate and there is no 
longer possible continuation of the dialogue 
definition argumentation-based negotiation 
an argumentation-based negotiation dialogue d between two 
agents p and c is a non-empty sequence of moves m ml 
such that 
 pi p iff i is even and pi c iff i is odd 
 player m p argument m θ offer m θ 
and target m 
 ∀ mi if offer mi θ then offer mi oj ∀ oj ∈ 
o\ o 
player mi 
i r ∪ nd 
player mi 
i 
 ∀i l mi is a legal continuation of m mi− 
 target mi mj such that j i and player mi 
player mj 
 if argument mi θ then 
- if offer mi θ then argument mi ∈ f offer mi 
- if offer mi θ then argument mi def 
player mi 
i 
argument target mi 
 i j ≤ l such that mi mj 
 m ∈ m such that m is a legal continuation of m ml 
let d be the set of all possible dialogues 
the first condition says that the two agents take turn the 
second condition says that agent p starts the negotiation 
dialogue by presenting an offer note that in the first turn 
we suppose that the agent does not present an argument 
this assumption is made for strategical purposes indeed 
arguments are exchanged as soon as a conflict appears the 
third condition ensures that agents exchange their best 
offers but never the rejected ones this condition takes also 
into account the concessions that an agent will have to make 
if it was established that a concession is the only option for 
it at the current state of the dialogue of course as we 
have shown in a previous section an agent may have several 
good or acceptable offers in this case the agent chooses 
one of them randomly the fourth condition ensures that 
the moves are legal this condition allows to terminate the 
dialogue as soon as an offer is presented by both agents 
the fifth condition allows agents to backtrack the sixth 
 
the first move has no target 
condition says that an agent may send arguments in favor 
of offers and in this case the offer should be stated in the 
same move an agent can also send arguments in order to 
defeat arguments of the other agent the next condition 
prevents repeating the same move this is useful for 
avoiding loops the last condition ensures that all the possible 
legal moves have been presented 
the outcome of a negotiation dialogue is computed as 
follows 
definition dialogue outcome let d m 
ml be a argumentation-based negotiation dialogue the 
outcome of this dialogue denoted outcome is outcome d 
offer ml iff ∃j l s t offer ml offer mj and 
player ml player mj otherwise outcome d θ 
note that when outcome d θ the negotiation fails and 
no agreement is reached by the two agents however if 
outcome d θ the negotiation succeeds and a solution 
that is either optimal or a compromise is found 
theorem ∀di ∈ d the argumentation-based 
negotiation di terminates 
the above result is of great importance since it shows that 
the proposed protocol avoids loops and dialogues terminate 
another important result shows that the proposed protocol 
ensures to reach an optimal solution if it exists formally 
theorem completeness let d m ml be 
a argumentation-based negotiation dialogue if ∃t ≤ l such 
that op 
t a ∩ oc 
t a ∅ then outcome d ∈ op 
t a ∩ oc 
t a 
we show also that the proposed dialogue protocol is sound 
in the sense that if a dialogue returns a solution then that 
solution is for sure a compromise in other words that 
solution is a common agreement at a given step of the 
dialogue we show also that if the negotiation fails then there 
is no possible solution 
theorem soundness let d m ml be a 
argumentation-based negotiation dialogue 
 if outcome d o o θ then ∃t ≤ l such that o ∈ 
op 
t x ∩ oc 
t y with x y ∈ {a n ns} 
 if outcome d θ then ∀t ≤ l op 
t x ∩ oc 
t y ∅ ∀ 
x y ∈ {a n ns} 
a direct consequence of the above theorem is the following 
property let d m ml be a 
argumentationbased negotiation dialogue if outcome d θ then ∀t ≤ l 
 op 
t r oc 
t a ∪ oc 
t n ∪ oc 
t ns and 
 oc 
t r op 
t a ∪ op 
t n ∪ op 
t ns 
 illustrative examples 
in this section we will present some examples in order to 
illustrate our general framework 
example no argumentation let o {o o } 
be the set of all possible offers let p and c be two agents 
equipped with the same theory a f r def such that 
a ∅ f o f o ∅ ∅ r ∅ def ∅ in 
this case it is clear that the two offers o and o are 
nonsupported the proposed protocol see definition will 
generate one of the following dialogues 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
p m p θ o 
c m c θ o 
this dialogue ends with o as a compromise note that this 
solution is not considered as optimal since it is not an 
acceptable offer for the agents 
p m p θ o 
c m c θ o 
p m p θ o 
this dialogue ends with o as a compromise 
p m p θ o 
c m c θ o 
this dialogue also ends with o as a compromise the last 
possible dialgue is the following that ends with o as a 
compromise 
p m p θ o 
c m c θ o 
p m p θ o 
note that in the above example since there is no exchange 
of arguments the theories of both agents do not change let 
us now consider the following example 
example static theories let o {o o } be 
the set of all possible offers the theory of agent p is ap 
 
fp 
 p 
 rp 
 defp 
such that ap 
 {a a } fp 
 o 
{a } fp 
 o {a } p 
 { a a } rp 
 { a a a a } 
defp 
 {a a } the argumentation system ap 
 defp 
of 
this agent will return a as an accepted argument and a as 
a rejected one consequently the offer o is acceptable and 
o is rejected 
the theory of agent c is ac 
 fc 
 c 
 rc 
 defc 
such 
that ac 
 {a a } fc 
 o {a } fc 
 o {a } c 
 
{ a a } rc 
 { a a a a } defc 
 {a a } the 
argumentation system ac 
 defc 
of this agent will return 
a as an accepted argument and a as a rejected one 
consequently the offer o is acceptable and o is rejected 
the only possible dialogues that may take place between 
the two agents are the following 
p m p θ o 
c m c θ o 
p m p a o 
c m c a o 
the second possible dialogue is the following 
p m p θ o 
c m c a o 
p m p a o 
c m c θ o 
both dialogues end with failure note that in both dialogues 
the theories of both agents do not change the reason is that 
the exchanged arguments are already known to both agents 
the negotiation fails because the agents have conflicting 
preferences 
let us now consider an example in which argumentation will 
allow agents to reach an agreement 
example dynamic theories let o {o o } be 
the set of all possible offers the theory of agent p is ap 
 
fp 
 p 
 rp 
 defp 
such that ap 
 {a a } fp 
 o 
 {a } fp 
 o {a } p 
 { a a a a } rp 
 
{ a a a a } defp 
 { a a } the argumentation 
system ap 
 defp 
of this agent will return a as an accepted 
argument and a as a rejected one consequently the offer 
o is acceptable and o is rejected 
the theory of agent c is ac 
 fc 
 c 
 rc 
 defc 
such 
that ac 
 {a a a } fc 
 o {a } fc 
 o {a } 
c 
 { a a a a } rc 
 { a a a a a a } 
defc 
 { a a a a } the argumentation system 
ac 
 defc 
of this agent will return a and a as accepted 
arguments and a as a rejected one consequently the offer 
o is acceptable and o is rejected 
the following dialogue may take place between the two 
agents 
p m p θ o 
c m c θ o 
p m p a o 
c m c a θ 
c m p θ o 
at step of the dialogue the agent p receives the 
argument a from p thus its theory evolves as follows ap 
 {a a a } rp 
 { a a a a a a } defp 
 
{ a a a a } at this step the argument a which was 
accepted will become rejected and the argument a which 
was at the beginning of the dialogue rejected will become 
accepted thus the offer o will be acceptable for the agent 
whereas o will become rejected at this step the offer o 
is acceptable for both agents thus it is an optimal solution 
the dialogue ends by returning this offer as an outcome 
 related work 
argumentation has been integrated in negotiation 
dialogues at the early nineties by sycara in that work the 
author has emphasized the advantages of using 
argumentation in negotiation dialogues and a specific framework has 
been introduced in the different types of arguments 
that are used in a negotiation dialogue such as threats and 
rewards have been discussed moreover a particular 
framework for negotiation have been proposed in 
different other frameworks have been proposed even if all these 
frameworks are based on different logics and use different 
definitions of arguments they all have at their heart an 
exchange of offers and arguments however none of those 
proposals explain when arguments can be used within a 
negotiation and how they should be dealt with by the agent 
that receives them thus the protocol for handling 
arguments was missing another limitation of the above 
frameworks is the fact that the argumentation frameworks they 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
use are quite poor since they use a very simple 
acceptability semantics in a negotiation framework that fills the 
gap has been suggested a protocol that handles the 
arguments was proposed however the notion of concession 
is not modeled in that framework and it is not clear what 
is the status of the outcome of the dialogue moreover it 
is not clear how an agent chooses the offer to propose at a 
given step of the dialogue in the authors have 
focused mainly on this decision problem they have proposed 
an argumentation-based decision framework that is used by 
agents in order to choose the offer to propose or to accept 
during the dialogue in that work agents are supposed to 
have a beliefs base and a goals base 
our framework is more general since it does not impose 
any specific structure for the arguments the offers or the 
beliefs the negotiation protocol is general as well thus this 
framework can be instantiated in different ways by creating 
in such manner different specific argumentation-based 
negotiation frameworks all of them respecting the same 
properties our framework is also a unified one because frameworks 
like the ones presented above can be represented within this 
framework for example the decision making mechanism 
proposed in for the evaluation of arguments and 
therefore of offers which is based on a priority relation between 
mutually attacked arguments can be captured by the 
relation defeat proposed in our framework this relation takes 
simultaneously into account the attacking and preference 
relations that may exist between two arguments 
 conclusions and future work 
in this paper we have presented a unified and general 
framework for argumentation-based negotiation like any 
other argumentation-based negotiation framework as it is 
evoked in e g our framework has all the advantages 
that argumentation-based negotiation approaches present 
when related to the negotiation approaches based either on 
game theoretic models see e g or heuristics this 
work is a first attempt to formally define the role of 
argumentation in the negotiation process more precisely for the 
first time it formally establishes the link that exists between 
the status of the arguments and the offers they support it 
defines the notion of concession and shows how it influences 
the evolution of the negotiation it determines how the 
theories of agents evolve during the dialogue and performs an 
analysis of the negotiation outcomes it is also the first time 
where a study of the formal properties of the negotiation 
theories of the agents as well as of an argumentative 
negotiation dialogue is presented 
our future work concerns several points a first point is 
to relax the assumption that the set of possible offers is the 
same to both agents indeed it is more natural to assume 
that agents may have different sets of offers during a 
negotiation dialogue these sets will evolve arguments in favor 
of the new offers may be built from the agent theory thus 
the set of offers will be part of the agent theory another 
possible extension of this work would be to allow agents 
to handle both arguments pro and cons offers this is 
more akin to the way human take decisions considering 
both types of arguments will refine the evaluation of the 
offers status in the proposed model a preference relation 
between offers is defined on the basis of the partition of the 
set of offers this preference relation can be refined for 
instance among the acceptable offers one may prefer the 
offer that is supported by the strongest argument in 
different criteria have been proposed for comparing decisions 
our framework can thus be extended by integrating those 
criteria another interesting point to investigate is that of 
considering negotiation dialogues between two agents with 
different profiles by profile we mean the criterion used by 
an agent to compare its offers 
 references 
 l amgoud s belabbes and h prade towards a 
formal framework for the search of a consensus 
between autonomous agents in proceedings of the th 
international joint conference on autonomous agents 
and multi-agents systems pages - 
 l amgoud s parsons and n maudet arguments 
dialogue and negotiation in proceedings of the th 
european conference on artificial intelligence 
 l amgoud and h prade reaching agreement 
through argumentation a possibilistic approach in 
th international conference on the principles of 
knowledge representation and reasoning kr 
 
 l amgoud and h prade explaining qualitative 
decision under uncertainty by argumentation in st 
national conference on artificial intelligence 
aaai pages - 
 p m dung on the acceptability of arguments and its 
fundamental role in nonmonotonic reasoning logic 
programming and n-person games artificial 
intelligence - 
 n r jennings p faratin a r lumuscio 
s parsons and c sierra automated negotiation 
prospects methods and challenges international 
journal of group decision and negotiation 
 a kakas and p moraitis adaptive agent negotiation 
via argumentation in proceedings of the th 
international joint conference on autonomous agents 
and multi-agents systems pages - 
 s kraus k sycara and a evenchik reaching 
agreements through argumentation a logical model 
and implementation artificial intelligence - 
 
 s parsons and n r jennings negotiation through 
argumentation-a preliminary report in proceedings 
of the nd international conference on multi agent 
systems pages - 
 i rahwan s d ramchurn n r jennings 
p mcburney s parsons and e sonenberg 
argumentation-based negotiation knowledge 
engineering review - 
 j rosenschein and g zlotkin rules of encounter 
designing conventions for automated negotiation 
among computers mit press cambridge 
massachusetts 
 k sycara persuasive argumentation in negotiation 
theory and decision - 
 f tohm´e negotiation and defeasible reasons for 
choice in proceedings of the stanford spring 
symposium on qualitative preferences in deliberation 
and practical reasoning pages - 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
modular interpreted systems 
wojciech jamroga 
department of informatics 
clausthal university of technology germany 
wjamroga in tu-clausthal de 
thomas ågotnes 
department of computer engineering 
bergen university college norway 
tag hib no 
abstract 
we propose a new class of representations that can be used for 
modeling and model checking temporal strategic and epistemic 
properties of agents and their teams our representations borrow the 
main ideas from interpreted systems of halpern fagin et al 
however they are also modular and compact in the way concurrent 
programs are we also mention preliminary results on model checking 
alternating-time temporal logic for this natural class of models 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligencemultiagent systems i artificial intelligence knowledge 
representation formalisms and methods-modal logic 
general terms 
theory 
 introduction 
the logical foundations of multi-agent systems have received 
much attention in recent years logic has been used to represent 
and reason about e g knowledge time cooperation and 
strategic ability lately an increasing amount of research has 
focused on higher level representation languages for models of such 
logics motivated mainly by the need for compact representations 
and for representations that correspond more closely to the actual 
systems which are modeled multi-agent systems are open systems 
in the sense that agents interact with an environment only partially 
known in advance thus we need representations of models of 
multi-agent systems which are modular in the sense that a 
component such as an agent can be replaced removed or added without 
major changes to the representation of the whole model however 
as we argue in this paper few existing representation languages are 
both modular compact and computationally grounded on the one 
hand and allow for representing properties of both knowledge and 
strategic ability on the other 
in this paper we present a new class of representations for 
models of open multi-agent systems which are modular compact and 
come with an implicit methodology for modeling and designing 
actual systems 
the structure of the paper is as follows first in section we 
present the background of our work - that is logics that combine 
time knowledge and strategies more precisely modal logics that 
combine branching time knowledge and strategies under 
incomplete information we start with computation tree logic ctl then 
we add knowledge ctlk and then we discuss two variants of 
alternating-time temporal logic atl one for the perfect and one 
for the imperfect information case the semantics of logics like the 
ones presented in section are usually defined over explicit models 
 kripke structures that enumerate all possible global states of the 
system however enumerating these states is one of the things one 
mostly wants to avoid because there are too many of them even 
for simple systems thus we usually need representations that are 
more compact another reason for using a more specialized class of 
models is that general kripke structures do not always give enough 
help in terms of methodology both at the stage of design nor at 
implementation this calls for a semantics which is more grounded in 
the sense that the correspondence between elements of the model 
and the entities that are modeled is more immediate in section 
we present an overview of representations that have been used for 
modeling and model checking systems in which time action and 
possibly knowledge are important we mention especially 
representations used for theoretical analysis we point out that the 
compact and or grounded representations of temporal models do not 
play their role in a satisfactory way when agents strategies are 
considered finally in section we present our framework of 
modular interpreted systems mis and show where it fits in the 
picture we conclude with a somewhat surprising hypothesis that 
model checking ability under imperfect information for mis can be 
computationally cheaper than model checking perfect information 
until now almost all complexity results were distinctly in favor of 
perfect information strategies and the others were indifferent 
 logics of time knowledge and 
strategic ability 
first we present the logics ctl ctlk atl and atlir that are 
the starting point of our study 
 branching time ctl 
computation tree logic ctl includes operators for temporal 
properties of systems i e path quantifier e there is a path 
together with temporal operators f in the next state always 
from now on and u until 
every occurrence of a temporal 
operator is immediately preceded by exactly one path quantifier 
 this variant of the language is sometimes called vanilla ctl 
let π be a set of atomic propositions with a typical element p 
ctl formulae ϕ are defined as follows 
ϕ p ¬ϕ ϕ ∧ ϕ e fϕ e ϕ eϕ u ϕ 
the semantics of ctl is based on kripke models m st r π 
which include a nonempty set of states st a state transition relation 
r ⊆ st × st and a valuation of propositions π π → p st 
a path λ in m refers to a possible behavior or computation of 
system m and can be represented as an infinite sequence of states 
q q q such that qirqi for every i we denote 
the ith state in λ by λ i a q-path is a path that starts in q 
interpretation of a formula in a state q in model m is defined as follows 
m q p iff q ∈ π p 
m q ¬ϕ iff m q ϕ 
m q ϕ ∧ ψ iff m q ϕ and m q ψ 
m q e fϕ iff there is a q-path λ such that m λ ϕ 
m q e ϕ iff there is a q-path λ such that m λ i ϕ for 
every i ≥ 
m q eϕ u ψ iff there is a q-path λ and i ≥ such that 
m λ i ψ and m λ j ϕ for every ≤ j i 
 adding knowledge ctlk 
ctlk is a straightforward combination of ctl and standard 
epistemic logic let agt { k} be a set of agents with 
a typical element a epistemic logic uses operators for representing 
agents knowledge kaϕ is read as agent a knows that ϕ models 
of ctlk extend models of ctl with epistemic indistinguishability 
relations ∼a⊆ st × st one per agent we assume that all ∼a are 
equivalences the semantics of epistemic operators is defined as 
follows 
m q kaϕ iff m q ϕ for every q such that q ∼a q 
note that when talking about agents knowledge we 
implicitly assume that agents may have imperfect information about the 
actual current state of the world otherwise the notion of 
knowledge would be trivial this does not have influence on the way 
we model evolution of a system as a single unit but it will become 
important when particular agents and their strategies come to the 
fore 
 agents and their strategies atl 
alternating-time temporal logic atl is a logic for 
reasoning about temporal and strategic properties of open computational 
systems multi-agent systems in particular the language of atl 
consists of the following formulae 
ϕ p ¬ϕ ϕ ∧ ϕ a fϕ a ϕ a ϕ u ϕ 
where a ⊆ agt informally a ϕ says that agents a have a 
collective strategy to enforce ϕ it should be noted that the ctl path 
quantifiers a e can be expressed with ∅ agt respectively 
the semantics of atl is defined in so called concurrent game 
structures cgss a cgs is a tuple 
m agt st act d o π π 
 
additional operators a for every path and sometime in 
the future are defined in the usual way 
consisting of a set agt { k} of agents set st of states 
valuation of propositions π π → p st set act of atomic 
actions function d agt × st → p act indicates the actions 
available to agent a ∈ agt in state q ∈ st finally o is a 
deterministic transition function which maps a state q ∈ st and an 
action profile α αk ∈ actk 
 αi ∈ d i q to another state 
q o q α αk 
definition a memoryless strategy of agent a is a function 
sa st → act such that sa q ∈ d a q 
a collective strategy sa 
for a team a ⊆ agt specifies an individual strategy for each agent 
a ∈ a finally the outcome of strategy sa in state q is defined as 
the set of all computations that may result from executing sa from 
q on 
out q sa {λ q q q q q and for every i 
there exists αi− 
 αi− 
k such that αi− 
a sa a qi− 
for each a ∈ a αi− 
a ∈ d a qi− for each a ∈ a and 
o qi− αi− 
 αi− 
k qi} 
the semantics of cooperation modalities is as follows 
m q a fϕ iff there is a collective strategy sa such that 
for every λ ∈ out q sa we have m λ ϕ 
m q a ϕ iff there exists sa such that for every λ ∈ 
out q sa we have m λ i for every i ≥ 
m q a ϕ u ψ iff there exists sa such that for every λ ∈ 
out q sa there is a i ≥ for which m λ i ψ and 
m λ j ϕ for every ≤ j i 
 agents with imperfect information atlir 
as atl does not include incomplete information in its scope it 
can be seen as a logic for reasoning about agents who always have 
complete knowledge about the current state of the whole system 
atlir includes the same formulae as atl except that the 
cooperation modalities are presented with a subscript a ir indicates 
that they address agents with imperfect information and imperfect 
recall formally the recursive definition of atlir formulae is 
ϕ p ¬ϕ ϕ ∧ ϕ a ir 
fϕ a ir ϕ a irϕ u ϕ 
models of atlir concurrent epistemic game structures cegs 
can be defined as tuples m agt st act d o ∼ ∼k π π 
where agt st act d o π π is a cgs and ∼ ∼k are 
epistemic equivalence relations it is required that agents have the 
same choices in indistinguishable states q ∼a q implies d a q 
d a q atlir restricts the strategies that can be used by agents 
to uniform strategies i e functions sa st → act such that 
sa q ∈ d a q and if q ∼a q then sa q sa q a collective 
strategy is uniform if it contains only uniform individual strategies 
again the function out q sa returns the set of all paths that may 
result from agents a executing collective strategy sa from state q 
the semantics of atlir formulae can be defined as follows 
m q a ir 
fϕ iff there is a uniform collective strategy sa 
such that for every a ∈ a q such that q ∼a q and λ ∈ 
out sa q we have m λ ϕ 
 
this is a deviation from the original semantics of atl where 
strategies assign agents choices to sequences of states which 
suggests that agents can by definition recall the whole history of each 
game while the choice of one or another notion of strategy affects 
the semantics of the full atl 
∗ 
 and most atl extensions e g for 
games with imperfect information it should be pointed out that 
both types of strategies yield equivalent semantics for pure atl 
 cf 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
m q a ir ϕ iff there exists sa such that for every a ∈ a 
q such that q ∼a q and λ ∈ out sa q we have m λ i 
for every i ≥ 
m q a irϕ u ψ iff there exist sa such that for every a ∈ a 
q such that q ∼a q and λ ∈ out sa q there is i ≥ for 
which m λ i ψ and m λ j ϕ for every ≤ j i 
that is a irϕ holds iff a have a uniform collective strategy such 
that for every path that can possibly result from execution of the 
strategy according to at least one agent from a ϕ is the case 
 models and model checking 
in this section we present and discuss various existing 
representations of systems that can be used for modeling and model 
checking we believe that the two most important points of 
reference are in this case the modeling formalism i e the logic 
and the semantics we use and the phenomenon or more 
generally the domain we are going to model to which we will often 
refer as the real world our aim is a representation which is 
reasonably close to the real world i e it is sufficiently compact and 
grounded and still not too far away from the formalism so that 
it e g easily allows for theoretical analysis of computational 
problems we begin with discussing the merits of explicit 
modelsin our case these are transition systems concurrent game structures 
and cegss presented in the previous section 
 explicit models 
obviously an advantage of explicit models is that they are very 
close to the semantics of our logics simply because they are the 
semantics on the other hand they are in many ways difficult to 
use to describe an actual system 
 exponential size temporal models usually have an 
exponential number of states with respect to any higher-level 
description e g boolean variables n-ary attributes etc also their 
size is exponential in the number of processes or agents 
if the evolution of a system results from joint synchronous 
or asynchronous actions of several active entities for 
cgss the situation is even worse here also the number of 
transitions is exponential even if we fix the number of states 
in practice this means that such representations are very 
seldom scalable 
 explicit models include no modularity states in a model 
refer to global states of the system transitions in the model 
correspond to global transitions as well i e they represent 
 in an atomic way everything that may happen in one single 
step regardless of who has done it to whom and in what 
way 
 logics like atl are often advertised as frameworks for 
modeling and reasoning about open computational systems 
ideally one would like the elements of such a system to have 
as little interdependencies as possible so that they can be 
plugged in and out without much hassle for instance when 
we want to test various designs or implementations of the 
active component in the case of a multi-agent system the 
 
another class of atl models alternating transition systems 
represent transitions in a more succinct way while we still have 
exponentially many states in an ats the number of transitions is 
simply quadratic wrt to states like for ctl models 
unfortunately ats are even less modular and harder to design than 
concurrent game structures and they cannot be easily extended to handle 
incomplete information cf 
need is perhaps even more obvious we do not only need 
to re-plug various designs of a single agent in the overall 
architecture we usually also need to change e g increase 
the number of agents acting in a given environment without 
necessarily changing the design of the whole system 
unfortunately atl models are anything but open in this sense 
theoretical complexity results for explicit models are as follows 
model checking ctl and ctlk is p-complete and can be done in 
time o ml where m is the number of transitions in the model 
and l is the length of the formula alternatively it can be done 
in time o n 
l where n is the number of states model checking 
atl is p-complete wrt m l and δp 
 -complete wrt n k l k being 
the number of agents model checking atlir is δp 
 
 complete wrt m l and δp 
 -complete wrt n k l 
 compressed representations 
explicit representation of all states and transitions is inefficient 
in many ways an alternative is to represent the state transition 
space in a symbolic way 
such models offer some hope for feasible model checking 
properties of open multi-agent systems although it is well known that 
they are compact only in a fraction of all cases 
for us however 
they are insufficient for another reason they are merely optimized 
representations of explicit models thus they are neither more 
open nor better grounded they were meant to optimize 
implementation rather than facilitate design or modeling methodology 
 interpreted systems 
interpreted systems are held by many as a prime example 
of computationally grounded models of distributed systems an 
interpreted system can be defined as a tuple is st stk stenv r π 
st stk are local state spaces of agents k and stenv is the 
set of states of the environment the set of global states is defined 
as st st × × stk × stenv r ⊆ st × st is a transition relation 
and π π → p st while the transition relation encapsulates 
the possible evolution of the system over time the epistemic 
dimension is defined by the local components of each global state 
q qk qenv ∼i q qk qenv iff qi qi 
it is easy to see that such a representation is modular and 
compact as far as we are concerned with states moreover it gives a 
natural grounded approach to knowledge and suggests an 
intuitive methodology for modeling epistemic states unfortunately 
the way transitions are represented in interpreted systems is neither 
compact nor modular nor grounded the temporal aspect of the 
system is given by a joint transition function exactly like in 
explicit models this is not without a reason if we separate activities 
of the agents too much we cannot model interaction in the 
framework any more and interaction is the most interesting thing here 
but the bottom line is that the temporal dimension of an interpreted 
system has exponential representation and it is almost as difficult 
to plug components in and out of an interpreted system as for an 
ordinary ctl or atl model since the local activity of an agent is 
completely merged with his interaction with the rest of the system 
 concurrent programs 
the idea of concurrent programs has been long known in the 
literature on distributed systems here we use the formulation 
from a concurrent program p is composed of k 
concurrent processes each described by a labeled transition system pi 
sti acti ri πi πi where sti is the set of local states of process 
 
representation r of an explicit model m is compact if the size of 
r is logarithmic with respect to the size of m 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
i acti is the set of local actions ri ⊆ sti ×acti ×sti is a transition 
relation and πi πi are the set of local propositions and their 
valuation the behavior of program p is given by the product 
automaton of p pk under the assumption that processes work 
asynchronously actions are interleaved and synchronization is obtained 
through common action names 
concurrent programs have several advantages first of all they 
are modular and compact they allow for local modeling of 
components - much more so than interpreted systems not only states 
but also actions are local here moreover they allow for 
representing explicit interaction between local transitions of reactive 
processes like willful communication and synchronization on the 
other hand they do not allow for representing implicit 
incidental or not entirely benevolent interaction between processes for 
example if we want to represent the act of pushing somebody the 
pushed object must explicitly execute an action of being pushed 
which seems somewhat ridiculous side effects of actions are also 
not easy to model still this is a minor complaint in the context 
of ctl because for temporal logics we are only interested in the 
flow of transitions and not in the underlying actions for temporal 
reasoning about k asynchronous processes with no implicit 
interaction concurrent programs seem just about perfect 
the situation is different when we talk about autonomous 
proactive components like agents acting together cooperatively or 
adversely in a common environment - and we want to address 
their strategies and abilities now particular actions are no less 
important than the resulting transitions actions may influence other 
agents local states without their consent they may have side 
effects on other agents states etc passing messages and or calling 
procedures is by no means the only way of interaction between 
agents moreover the availability of actions to an agent should 
not depend on the actions that will be executed by other agents at 
the same time - these are the outcome states that may depend on 
these actions finally we would often like to assume that agents act 
synchronously in particular all agents play simultaneously in 
concurrent game structures but assuming synchrony and autonomy 
of actions synchronization can no longer be a means of 
coordination 
to sum up we need a representation which is very much like 
concurrent programs but allows for modeling agents that play 
synchronously and which enables modeling more sophisticated 
interaction between agents actions the first postulate is easy to satisfy 
as we show in the following section the second will be addressed 
in section 
we note that model checking ctl against concurrent programs 
is pspace-complete in the number of local states and the length 
of the formula 
 synchronous cp and simple reactive 
modules 
the semantics of atl is based on synchronous models where 
availability of actions does not depend on the actions currently 
executed by the other players a slightly different variant of 
concurrent programs can be defined via synchronous product of programs 
so that all agents play simultaneously 
unfortunately under such 
interpretation no direct interaction between agents actions can be 
modeled at all 
definition a synchronous concurrent program consists of 
k concurrent processes pi sti acti ri πi πi with the 
follow 
the concept is not new of course and has already existed in folk 
knowledge although we failed to find an explicit definition in the 
literature 
ing unfolding to a cgs agt { k} st 
qk 
i sti act 
sk 
i acti d i q qk {αi qi αi qi ∈ ri for some qi ∈ 
sti} o q qk α αk q qk such that qi αi qi ∈ 
ri for every i π 
sk 
i πi and π p πi p for p ∈ πi 
we note that the simple reactive modules srml from can 
be seen as a particular implementation of synchronous concurrent 
programs 
definition a srml system is a tuple σ π m mk 
where σ { k} is a set of modules or agents π is a 
set of boolean variables and for each i ∈ σ we have mi 
ctri initi updatei where ctri ⊆ π sets initi and updatei consist 
of guarded commands of the form φ v ψ vk ψk 
where every vj ∈ ctri and φ ψ ψk are propositional 
formulae over π it is required that ctr ctrk partitions π 
the idea is that agent i controls the variables ctri the init guarded 
commands are used to initialize the controlled variables while the 
update guarded commands can change their values in each round 
a guarded command is enabled if the guard φ is true in the current 
state of the system in each round an enabled update guarded 
command is executed each ψj is evaluated against the current state of 
the system and its logical value is assigned to vj several guarded 
commands being enabled at the same time model non-deterministic 
choice model checking atl for srml has been proved 
exptimecomplete in the size of the model and the length of the formula 
 concurrent epistemic programs 
concurrent programs both asynchronous and synchronous can 
be used to encode epistemic relations too - exactly in the same 
way as interpreted systems do that is when unfolding a 
concurrent program to a model of ctlk or atlir we define that 
q qk ∼i q qk iff qi qi model checking ctlk 
against concurrent epistemic programs is pspace-complete 
srml can be also interpreted in the same way then we would 
assume that every agent can see only the variables he controls 
concurrent epistemic programs are modular and have a grounded 
semantics they are usually compact albeit not always for 
example an agent with perfect information will always blow up the size 
of such a program still they inherit all the problems of 
concurrent programs with perfect information discussed in section 
limited interaction between components availability of local 
actions depending on the actual transition etc the problems were 
already important for agents with perfect information but they 
become even more crucial when agents have only limited knowledge 
of the current situation one of the most important applications of 
logics that combine strategic and epistemic properties is 
verification of communication protocols e g in the context of security 
now we may want to e g check agents ability to pass an 
information between them without letting anybody else intercept the 
message the point is that the action of intercepting is by definition 
enabled we just look for a protocol in which the transition of 
successful interception is never carried out so availability of actions 
must be independent of the actions chosen by the other agents under 
incomplete information on the other hand interaction is arguably 
the most interesting feature of multi-agent systems and it is really 
hard to imagine models of strategic-epistemic logics in which it is 
not possible to represent communication 
 reactive modules 
reactive modules can be seen as a refinement of 
concurrent epistemic programs primarily used by the mocha model 
checker but they are much more powerful expressive and 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
grounded we have already mentioned a very limited variant of 
rml i e srml the vocabulary of rml is very close to 
implementations in terms of general computational systems the 
modules are essentially collections of variables states are just 
valuations of variables events actions are variable updates however 
the sets of variables controlled by different agents can overlap 
they can change over time etc moreover reactive modules 
support incomplete information through observability of variables 
although it is not the main focus of rml again the relationship 
between sets of observable variables and to sets of controlled 
variables is mostly left up to the designer of a system agents can act 
synchronously as well as asynchronously 
to sum up rml define a powerful framework for modeling 
distributed systems with various kinds of synchrony and asynchrony 
however we believe that there is still a need for a simpler and 
slightly more abstract class of representations first the 
framework of rml is technically complicated involving a number 
auxiliary concepts and their definitions second it is not always 
convenient to represent all that is going on in a multi-agent system 
as reading and or writing from to program variables this view 
of a multi-agent system is arguably close to its computer 
implementation but usually rather distant from the real world 
domainhence the need for a more abstract and more conceptually flexible 
framework third the separation of the local complexity and the 
complexity of interaction is not straightforward our new proposal 
more in the spirit of interpreted systems takes these observations 
as the starting point the proposed framework is presented in 
section 
 modular interpreted systems 
the idea behind distributed systems multi-agent systems even 
more so is that we deal with several loosely coupled components 
where most of the processing goes on inside components i e 
locally and only a small fraction of the processing occurs between 
the components interaction is crucial which makes concurrent 
programs an insufficient modeling tool but it usually consumes 
much less of the agent s resources than local computations which 
makes the explicit transition tables of cgs cegs and interpreted 
systems an overkill modular interpreted systems proposed here 
extrapolate the modeling idea behind interpreted systems in a way 
that allows for a tight control of the interaction complexity 
definition a modular interpreted system mis is defined 
as a tuple 
s agt env act in 
where agt {a ak} is a set of agents env is the environment 
act is a set of actions and in is a set of symbols called interaction 
alphabet each agent has the following internal structure 
ai sti di outi ini oi πi πi where 
 sti is a set of local states 
 di sti → p act defines local availability of actions for 
convenience of the notation we additionally define the set of 
situated actions as di { qi α qi ∈ sti α ∈ di qi } 
 outi ini are interaction functions outi di → in refers to 
the influence that a given situated action of agent ai may 
possibly have on the external world and ini sti ×ink 
→ in 
translates external manifestations of the other agents and 
the environment into the impression that they make on 
ai s transition function depending on the local state of ai 
 oi di × in → sti is a deterministic local transition 
function 
 πi is a set of local propositions of agent ai where we require 
that πi and πj are disjunct when i j and 
 πi πi → p sti is a valuation of these propositions 
the environment env stenv outenv inenv oenv πenv πenv has the 
same structure as an agent except that it does not perform actions 
and that thus outenv stenv → in and oenv stenv × in → stenv 
within our framework we assume that every action is executed 
by an actor that is an agent as a consequence every actor is 
explicitly represented in a mis as an agent just like in the case of 
cgs and cegs the environment on the other hand represents the 
 passive context of agents actions in practice it serves to capture 
the aspects of the global state that are not observable by any of the 
agents 
the input functions ini seem to be the fragile spots here when 
given explicitly as tables they have size exponential wrt the 
number of agents and linear wrt the size of in however we can 
use e g a construction similar to the one from to represent 
interaction functions more compactly 
definition implicit input function for state q ∈ sti is given 
by a sequence ϕ η ϕn ηn where each ηj ∈ in is an 
interaction symbol and each ϕj is a boolean combination of 
propositions ˆηi 
 with η ∈ in ˆηi 
stands for η is the symbol currently 
generated by agent i the input function is now defined as 
follows ini q k env ηj iff j is the lowest index such that 
{ˆ 
 ˆk 
k ˆenv 
env} ϕj it is required that ϕn ≡ so that the 
mapping is effective 
remark every ini can be encoded as an implicit input 
function with each ϕj being of polynomial size with respect to the 
number of interaction symbols cf 
note that for some domains the mis representation of a system 
requires exponentially many symbols in the interaction alphabet in 
in such a case the problem is inherent to the domain and ini will 
have size exponential wrt the number of agents 
 representing agent systems with mis 
let stg 
qk 
i sti ×stenv be the set of all possible global states 
generated by a modular interpreted system s 
definition the unfolding of a mis s for initial states q ⊆ 
stg to a cegs cegs s q agt st π π act d o ∼ ∼k 
is defined as follows 
 agt { k} and act act 
 st is the set of global states from stg which are reachable 
from some state in q via the transition relation defined by o 
 below 
 π 
sk 
i πi ∪ πenv 
 for each q q qk qenv ∈ st and i k env 
we define q ∈ π p iff p ∈ πi and qi ∈ πi p 
 d i q di qi for global state q q qk qenv 
 the transition function is constructed as follows let q 
q qk qenv ∈ st and α α αk be an action 
profile s t αi ∈ d i q we define inputi q α 
ini 
` 
qi out q α outi− qi− αi− outi qi αi 
 outk qk αk outenv qenv 
´ 
for each agent i k 
and inputenv q α inenv 
` 
qenv out q α outk qk αk 
´ 
 
then o q α o q α input q α 
ok qk αk inputk q α oenv qenv inputenv q α 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 for each i k q qk qenv ∼i q qk qenv iff 
qi qi 
remark note that miss can be used as representations of 
cgss too in that case epistemic relations ∼i are simply omitted in 
the unfolding we denote the unfolding of a mis s for initial states 
q into a cgs by cgs s q 
propositions and state that modular interpreted systems can 
be used as representations for explicit models of multi-agent 
systems on the other hand these representations are not always 
compact as demonstrated by propositions and 
proposition for every cegs m there is a mis sm 
and a 
set of global states q of sm 
such that cegs sm 
 q is isomorphic to 
m 
proof let m { k} st act d o π π ∼ ∼k 
be a cegs we construct a mis sm 
 {a ak} env act in 
with agents ai sti di outi ini oi πi πi and environment env 
stenv outenv inenv oenv πenv πenv plus a set q ⊆ stg of global 
states as follows 
 in act ∪ st ∪ actk− 
× st 
 sti { q ∼i q ∈ st} for ≤ i ≤ k i e sti is the set of i s 
indistinguishability classes in m 
 stenv st 
 di q ∼i d i q for ≤ i ≤ k this is well-defined since 
d i q d i q whenever q ∼i q 
 outi q ∼i αi αi for ≤ i ≤ k outenv q q 
 ini q ∼i α αi− αi αk qenv 
α αi− αi αk qenv for i ∈ { k} 
inenv q α αk α αk 
ini x and inenv x are arbitrary for other arguments x 
 oi q ∼i αi α αi− αi αk qenv 
 o qenv α αk ∼i for ≤ i ≤ k and αi ∈ di q ∼i 
oenv q α αk o q α αk 
oi and oenv are arbitrary for other arguments 
 πi ∅ for ≤ i ≤ k and πenv π 
 πenv p π p 
 q { q ∼ q ∼k q q ∈ st} 
let m cegs sm 
 q agt st act d o π π ∼ ∼k 
we argue that m and m are isomorphic by establishing a 
oneto-one correspondence between the respective sets of states and 
showing that the other parts of the structures agree on 
corresponding states 
first we show that for any ˆq q ∼ q ∼k q ∈ q 
and any α α αk such that αi ∈ d i ˆq we have 
o ˆq α q ∼ q ∼k q where q o q α 
let ˆq o ˆq α now for any i inputi ˆq α ini q ∼i 
out q ∼ α outi− q ∼i− αi− outi q ∼i αi 
 outk q ∼k αk outenv q ini q ∼i α αi− αi 
 
this shows another difference between the environment and the 
agents the environment does not possess knowledge 
 
we say that two cegs are isomorphic if they only differ in the 
names of states and or actions 
 αk q α αi− αi αk q similarly we get 
that inputenv ˆq α α αk thus we get that o ˆq α 
o q ∼ α input ˆq α ok q ∼k αk inputk ˆq α 
oenv q inputenv ˆq α o q α αk ∼ 
 o q α αk ∼k o q α αk thus ˆq 
 q ∼ q ∼k q for q o q α αk which completes 
the proof of 
we now argue that st q clearly q ⊆ st let ˆq ∈ st 
we must show that ˆq ∈ q the argument is on induction on the 
length of the least o path from q to ˆq the base case ˆq ∈ q is 
immediate for the inductive step ˆq o ˆq α for some ˆq ∈ q 
and then we have that ˆq ∈ q by thus st q 
now we have a one-to-one correspondence between st and st 
r ∈ st corresponds to r ∼ r ∼k r ∈ st it remains to 
be shown that the other parts of the structures m and m agree on 
corresponding states 
 agt agt 
 act act 
 π 
sk 
i πi ∪ πenv π 
 for p ∈ π π q ∼ q ∼k q ∈ π p iff q ∈ 
πenv p iff q ∈ π p same valuations at corresponding states 
 d i q ∼ q ∼k q di q ∼i d i q 
 it follows immediately from and the fact that q st 
that o q ∼ q ∼k q α r ∼ r ∼k r 
iff o q α r transitions on the same joint action in 
corresponding states lead to corresponding states 
 q ∼ q ∼k q ∼i r ∼ r ∼k r iff q ∼i 
 r ∼i iff q ∼i r the accessibility relations relate 
corresponding states which completes the proof 
corollary for every cegs m there is an atlir-equivalent 
mis s with initial states q that is for every state q in m there is 
a state q in cegs s q satisfying exactly the same atlir formulae 
and vice versa 
proposition for every cgs m there is a mis sm 
and a set 
of global states q of sm 
such that cgs sm 
 q is isomorphic to m 
proof let m agt st act d o π π be given now let 
ˆm agt st act d o π π ∼ ∼k for some arbitrary 
accessibility relations ∼i over st by proposition there exists a mis 
s 
ˆm 
with global states q such that ˆm cegs s 
ˆm 
 q is isomorphic 
to ˆm let m be the cgs obtained by removing the accessibility 
relations from ˆm clearly m is isomorphic to m 
corollary for every cgs m there is an atl-equivalent 
mis s with initial states q that is for every state q in m there is a 
state q in cgs s q satisfying exactly the same atl formulae and 
vice versa 
proposition the local state spaces in a mis are not 
always compact with respect to the underlying concurrent epistemic 
game structure 
proof take a cegs m in which agent i has always perfect 
information about the current global state of the system when 
constructing a modular interpreted system s such that m cegs s q 
we have that sti must be isomorphic with st 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the above property is a part of the interpreted systems heritage 
the next proposition stems from the fact that explicit models and 
interpreted systems allow for intensive interaction between agents 
proposition the size of in in s is in general exponential 
with respect to the number of local states and local actions this is 
the case even when epistemic relations are not relevant i e when 
s is taken as a representation of an ordinary cgs 
proof consider a cgs m with agents agt { k} global 
states st 
qk 
i {qi 
 qi 
i} and actions act { } all enabled 
everywhere the transition function is defined as 
o q 
j 
 qk 
jk 
 α αk q 
l 
 qk 
lk 
 where li ji α 
 αk mod i note that m can be represented as a modular 
interpreted system with succinct local state spaces sti {qi 
 qi 
i} 
still the current actions of all agents are relevant to determine the 
resulting local transition of agent i 
we will call items in outi ini the interaction layer of a 
modular interpreted system s the other elements of s constitute the local 
layer of the mis in this paper we are ultimately interested in model 
checking complexity with respect to the size of the local layer to 
this end we will assume that the size of interaction layer is 
polynomial in the number of local states and actions note that by 
propositions and not every explicit model submits to compact 
representation with a mis still as we declared at the beginning of 
section we are mainly interested in a modeling framework for 
systems of loosely coupled components where interaction is 
essential but most processing is done locally anyway more 
importantly the framework of mis allows for separating the interaction 
of agents from their local structure to a larger extent moreover we 
can control and measure the complexity of each layer in a finer way 
than before first we can try to abstract from the complexity of a 
layer e g like in this paper by assuming that the other layer is kept 
within certain complexity bounds second we can also measure 
separately the interaction complexity of different agents 
 modular interpreted systems vs simple 
reactive modules 
in this section we show that simple reactive modules are as we 
already suggested a specific and somewhat limited 
implementation of modular interpreted systems first we define our quite 
strong notion of equivalence of representations 
definition two representations are equivalent if they 
unfold to isomorphic concurrent epistemic game structures they are 
cgs-equivalent if they unfold to the same cgs 
proposition for any srml there is a cgs-equivalent mis 
proof consider an srml r with k modules and n variables 
we construct s agt act in with agt {a ak} act 
{ n ⊥ ⊥n} and in 
sk 
i sti × sti the local state 
spaces sti will be defined in a moment let us assume without loss 
of generality that ctri {x xr} also we consider all guarded 
commands of i to be of type γi ψ ψ xi or γ⊥ 
i ψ ψ 
xi ⊥ now agent ai in s has the following components sti 
p ctri i e local states of ai are valuations of variables controlled 
by i di qi { r ⊥ ⊥r} outi qi α qi qi 
ini qi q q qi− qi− qi qi qk qk 
{xi ∈ ctri q qk 
w 
γi ψ 
ψ} {xi ∈ ctri q qk 
w 
γ⊥ 
i ψ 
ψ} to define local transitions we consider three cases if 
t f ∅ no update is enabled then oi qi α t f qi for 
every action α if t ∅ we take any arbitrary ˆx ∈ t and 
define oi qi j t f qi ∪ {xj} if xj ∈ t and qi ∪ {ˆx} otherwise 
oi qi ⊥j t f qi \ {xj} if xj ∈ f and qi ∪ {ˆx} otherwise 
moreover if t ∅ f we take any arbitrary ˆx ∈ f and 
define oi qi j t f qi ∪ {xj} if xj ∈ t and qi \ {ˆx} otherwise 
oi qi ⊥j t f qi \{xj} if xj ∈ f and qi \{ˆx} otherwise finally 
πi ctri and qi ∈ πi xj iff xj ∈ qi 
the above construction shows that srml have more compact 
representation of states than mis ri local variables of agent i give 
rise to ri 
local states in a way reactive modules both simple 
and full are two-level representations first the system is 
represented as a product of modules next each module can be seen 
as a product of its variables together with their update operations 
note however that specification of updates with respect to a single 
variable in an srml may require guarded commands of total length 
o 
pk 
i ri 
 thus the representation of transitions in srml is in 
the worst case no more compact than in mis despite the two-level 
structure of srml we observe finally that mis are more general 
because in srml the current actions of other agents have no 
influence on the outcome of agent i s current action although the 
outcome can be influenced by other agents current local states 
 model checking modular interpreted 
systems 
one of our main aims was to study the complexity of symbolic 
model checking atlir in a meaningful way following the 
reviewers remarks we state our complexity results only as conjectures 
preliminary proofs can be found in 
conjecture model checking atl for modular interpreted 
systems is exptime-complete 
conjecture model checking atlir for the class of 
modular interpreted systems is pspace-complete 
a summary of complexity results for model checking 
temporal and strategic logics with and without epistemic component 
is given in the table below the table presents completeness 
results for various models and settings of input parameters symbols 
n k m stand for the number of states agents and transitions in an 
explicit model l is the length of the formula and nlocal is the 
number of local states in a concurrent program or modular interpreted 
system the new results conjectured in this paper are printed in 
italics note that the result for model checking atl against modular 
interpreted systems is an extension of the result from 
m l n k l nlocal k l 
ctl p p pspace 
ctlk p p pspace 
atl p δp 
 exptime 
atlir δp 
 δp 
 pspace 
if we are right then the results for atl and atlir form an 
intriguing pattern when we compare model checking agents with 
perfect vs imperfect information the first problem appears to be 
much easier against explicit models measured with the number of 
transitions next we get the same complexity class against explicit 
models measured with the number of states and agents finally 
model checking imperfect information turns out to be easier than 
model checking perfect information for modular interpreted 
systems why can it be so 
first a mis unfolds into cegs and cgs in a different way in 
the first case the mis is assumed to encode the epistemic relations 
explicitly which makes it explode when we model agents with 
perfect or almost perfect information in the latter case the epistemic 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
aspect is ignored which gives some extra room for encoding the 
transition relation more efficiently another crucial factor is the 
number of available strategies relative to the size of input 
parameters the number of all strategies is exponential in the number 
of global states for uniform strategies there are usually much less 
of them but still exponentially many in general thus the fact that 
perfect information strategies can be synthesized incrementally has 
a substantial impact on the complexity of the problem however 
measured in terms of local states and agents the number of all 
strategies is doubly exponential while there are only 
exponentially many uniform strategies - which settles the results in favor of 
imperfect information 
 conclusions 
we have presented a new class of representations for open 
multiagent systems our representations called modular interpreted 
systems are modular in the sense that components can be changed 
replaced removed or added with as little changes to the whole 
representation as possible more compact than traditional explicit 
representations and grounded in the sense that the correspondences 
between the primitives of the model and the entities being 
modeled are more immediate giving a methodology for designing and 
implementing systems we also conjecture that the complexity of 
model checking strategic ability for our representations is higher if 
we assume perfect information than if we assume imperfect 
information 
the solutions proposed in this paper are not necessarily 
perfect for example the impression functions ini seem to be the 
main source of non-modularity in mis and can be perhaps 
improved but we believe them to be a step in the right direction 
we also do not mean to claim that our representations should 
replace more elaborate modeling languages like promela or reactive 
modules we only suggest that there is a need for compact modular 
and reasonably grounded models that are more expressive than 
concurrent epistemic programs and still allow for easier theoretical 
analysis than reactive modules we also suggest that mis might be 
better suited for modeling simple multi-agent domains especially 
for human-oriented as opposed to computer-oriented design 
 acknowledgments 
we thank the anonymous reviewers and andrzej tarlecki for 
their helpful remarks thomas ågotnes work on this paper was 
supported by grants v and s from the 
research council of norway 
 references 
 r alur and t a henzinger reactive modules formal 
methods in system design - 
 r alur t a henzinger and o kupferman 
alternating-time temporal logic lecture notes in 
computer science - 
 r alur t a henzinger and o kupferman 
alternating-time temporal logic journal of the acm 
 - 
 r alur t a henzinger f y c mang s qadeer s k 
rajamani and s tasiran mocha user manual in 
proceedings of cav volume of lecture notes in 
computer science pages - 
 e m clarke e a emerson and a p sistla automatic 
verification of finite-state concurrent systems using temporal 
logic specifications acm transactions on programming 
languages and systems - 
 e a emerson and j y halpern sometimes and not never 
revisited on branching versus linear time temporal logic in 
proceedings of the annual acm symposium on principles of 
programming languages pages - 
 r fagin j y halpern y moses and m y vardi 
reasoning about knowledge mit press cambridge ma 
 
 m franceschet a montanari and m de rijke model 
checking for combined logics in proceedings of the rd 
international conference on temporal logic ictl 
 v goranko and w jamroga comparing semantics of logics 
for multi-agent systems synthese - 
 j y halpern reasoning about knowledge a survey in 
d m gabbay c j hogger and j a robinson editors the 
handbook of logic in artificial intelligence and logic 
programming volume iv pages - oxford university 
press 
 j y halpern and r fagin modelling knowledge and action 
in distributed systems distributed computing 
 - 
 w jamroga and j dix do agents make model checking 
explode computationally in m p˘echou˘cek p petta and 
l z varga editors proceedings of ceemas volume 
 of lecture notes in computer science pages - 
springer verlag 
 w jamroga and j dix model checking abilities of agents 
a closer look submitted 
 w jamroga and t ågotnes modular interpreted systems a 
preliminary report technical report ifi- - clausthal 
university of technology 
 o kupferman m y vardi and p wolper an 
automata-theoretic approach to branching-time model 
checking journal of the acm - 
 f laroussinie n markey and g oreiby expressiveness 
and complexity of atl technical report lsv- - cnrs 
 ens cachan france 
 k l mcmillan symbolic model checking an approach to 
the state explosion problem kluwer academic publishers 
 
 k l mcmillan applying sat methods in unbounded 
symbolic model checking in proceedings of cav 
volume of lecture notes in computer science pages 
 - 
 w penczek and a lomuscio verifying epistemic properties 
of multi-agent systems via bounded model checking in 
proceedings of aamas pages - new york ny 
usa acm press 
 f raimondi and a lomuscio the complexity of symbolic 
model checking temporal-epistemic logics in l czaja 
editor proceedings of cs p 
 p y schobbens alternating-time logic with imperfect 
recall electronic notes in theoretical computer science 
 
 w van der hoek a lomuscio and m wooldridge on the 
complexity of practical atl model checking in p stone and 
g weiss editors proceedings of aamas pages 
 - 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
an agent-based approach for privacy-preserving 
recommender systems 
richard cissée 
dai-labor tu berlin 
franklinstrasse 
 berlin 
richard cissee dai-labor de 
sahin albayrak 
dai-labor tu berlin 
franklinstrasse 
 berlin 
sahin albayrak dai-labor de 
abstract 
recommender systems are used in various domains to 
generate personalized information based on personal user data 
the ability to preserve the privacy of all participants is an 
essential requirement of the underlying information filtering 
architectures because the deployed recommender systems 
have to be accepted by privacy-aware users as well as 
information and service providers existing approaches neglect 
to address privacy in this multilateral way 
we have developed an approach for privacy-preserving 
recommender systems based on multi-agent system 
technology which enables applications to generate 
recommendations via various filtering techniques while preserving the 
privacy of all participants we describe the main modules of 
our solution as well as an application we have implemented 
based on this approach 
categories and subject descriptors 
h information storage and retrieval 
information search and retrieval-information filtering i 
 artificial intelligence distributed artificial 
intelligence-multiagent systems 
general terms 
management security human factors standardization 
 introduction 
information filtering if systems aim at countering 
information overload by extracting information that is 
relevant for a given user out of a large body of information 
available via an information provider in contrast to 
information retrieval ir systems where relevant information 
is extracted based on search queries if architectures 
generate personalized information based on user profiles 
containing for each given user personal data preferences and 
rated items the provided body of information is usually 
structured and collected in provider profiles filtering 
techniques operate on these profiles in order to generate 
recommendations of items that are probably relevant for a given 
user or in order to determine users with similar interests 
or both depending on the respective goal the resulting 
systems constitute recommender systems matchmaker 
systems or a combination thereof 
the aspect of privacy is an essential issue in all if systems 
generating personalized information obviously requires the 
use of personal data according to surveys indicating major 
privacy concerns of users in the context of recommender 
systems and e-commerce in general users can be 
expected to be less reluctant to provide personal information 
if they trust the system to be privacy-preserving with regard 
to personal data similar considerations also apply to the 
information provider who may want to control the 
dissemination of the provided information and to the provider of the 
filtering techniques who may not want the details of the 
utilized filtering algorithms to become common knowledge a 
privacy-preserving if system should therefore balance these 
requirements and protect the privacy of all parties involved 
in a multilateral way while addressing general requirements 
regarding performance security and quality of the 
recommendations as well as described in the following section 
there are several approaches with similar goals but none of 
these provide a generic approach in which the privacy of all 
parties is preserved 
we have developed an agent-based approach for 
privacypreserving if which has been utilized for realizing a 
combined recommender matchmaker system as part of an 
application supporting users in planning entertainment-related 
activities in this paper we focus on the recommender 
system functionality our approach is based on multi-agent 
system mas technology because fundamental features of 
agents such as autonomy adaptability and the ability to 
communicate are essential requirements of our approach in 
other words the realized approach does not merely 
constitute a solution for privacy-preserving if within a mas 
context but rather utilizes a mas architecture in order to 
realize a solution for privacy-preserving if which could not 
be realized easily otherwise 
the paper is structured as follows section describes 
related work section describes the general ideas of our 
approach in section we describe essential details of the 
 
 - - - - rps c ifaamas 
modules of our approach and their implementation in 
section we evaluate the approach mainly via the realized 
application section concludes the paper with an outlook 
and outlines further work 
 related work 
there is a large amount of work in related areas such as 
private information retrieval privacy-preserving data 
mining and other privacy-preserving protocols 
most of which is based on secure multi-party computation 
 we have ruled out secure multi-party computation 
approaches mainly because of their complexity and because 
the algorithm that is computed securely is not considered to 
be private in these approaches 
various enforcement mechanisms have been suggested 
that are applicable in the context of privacy-preserving 
information filtering such as enterprise privacy policies 
or hippocratic databases both of which annotate user 
data with additional meta-information specifying how the 
data is to be handled on the provider side these approaches 
ultimately assume that the provider actually intends to 
protect the privacy of the user data and offer support for this 
task but they are not intended to prevent the provider 
from acting in a malicious manner trusted computing as 
specified by the trusted computing group aims at 
realizing trusted systems by increasing the security of open 
systems to a level comparable with the level of security that 
is achievable in closed systems it is based on a 
combination of tamper-proof hardware and various software 
components some example applications including peer-to-peer 
networks distributed firewalls and distributed computing 
in general are listed in 
there are some approaches for privacy-preserving 
recommender systems based on distributed collaborative filtering 
in which recommendations are generated via a public model 
aggregating the distributed user profiles without containing 
explicit information about user profiles themselves this 
is achieved via secure multi-party computation or via 
random perturbation of the user data in 
various approaches are integrated within a single architecture 
in an agent-based approach is described in which user 
agents representing similar users are discovered via a 
transitive traversal of user agents privacy is preserved through 
pseudonymous interaction between the agents and through 
adding obfuscating data to personal information more 
recent related approaches are described in 
in an agent-based architecture for privacy-preserving 
demographic filtering is described which may be 
generalized in order to support other kinds of filtering techniques 
while in some aspects similar to our approach this 
architecture addresses at least two aspects inadequately namely 
the protection of the filter against manipulation attempts 
and the prevention of collusions between the filter and the 
provider 
 privacy-preserving 
information filtering 
we identify three main abstract entities participating in 
an information filtering process within a distributed system 
a user entity a provider entity and a filter entity whereas 
in some applications the provider and filter entities 
explicitly trust each other because they are deployed by the same 
party our solution is applicable more generically because it 
does not require any trust between the main abstract 
entities in this paper we focus on aspects related to the 
information filtering process itself and omit all aspects 
related to information collection and processing i e the stages 
in which profiles are generated and maintained mainly 
because these stages are less critical with regard to privacy as 
they involve fewer different entities 
 requirements 
our solution aims at meeting the following requirements 
with regard to privacy 
 user privacy no linkable information about user 
profiles should be acquired permanently by any other 
entity or external party including other user entities 
single user profile items however may be acquired 
permanently if they are unlinkable i e if they 
cannot be attributed to a specific user or linked to other 
user profile items temporary acquisition of private 
information is permitted as well sets of 
recommendations may be acquired permanently by the provider 
but they should not be linkable to a specific user 
these concessions simplify the resulting protocol and 
allow the provider to obtain recommendations and 
single unlinkable user profile items and thus to determine 
frequently requested information and optimize the 
offered information accordingly 
 provider privacy no information about provider 
profiles with the exception of the recommendations 
should be acquired permanently by other entities or 
external parties again temporary acquisition of 
private information is permitted additionally the 
propagation of provider information is entirely under the 
control of the provider thus the provider is enabled 
to prevent misuse such as the automatic large-scale 
extraction of information 
 filter privacy details of the algorithms applied by 
the filtering techniques should not be acquired 
permanently by any other entity or external party general 
information about the algorithm may be provided by 
the filter entity in order to help other entities to reach 
a decision on whether to apply the respective filtering 
technique 
in addition general requirements regarding the quality 
of the recommendations as well as security aspects 
performance and broadness of the resulting system have to be 
addressed as well while minor trade-offs may be acceptable 
the resulting system should reach a level similar to regular 
recommender systems with regard to these requirements 
 outline of the solution 
the basic idea for realizing a protocol fulfilling these 
privacy-related requirements in recommender systems is 
implied by allowing the temporary acquisition of private 
information see for the original approach user and 
provider entity both propagate the respective profile data to 
the filter entity the filter entity provides the 
recommendations and subsequently deletes all private information thus 
fulfilling the requirement regarding permanent acquisition 
of private information 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the entities whose private information is propagated have 
to be certain that the respective information is actually 
acquired temporarily only trust in this regard may be 
established in two main ways 
 trusted software the respective entity itself is trusted 
to remove the respective information as specified 
 trusted environment the respective entity operates 
in an environment that is trusted to control the 
communication and life cycle of the entity to an extent 
that the removal of the respective information may 
be achieved regardless of the attempted actions of the 
entity itself additionally the environment itself is 
trusted not to act in a malicious manner e g it is 
trusted not to acquire and propagate the respective 
information itself 
in both cases trust may be established in various ways 
reputation-based mechanisms additional trusted third 
parties certifying entities or environments or trusted 
computing mechanisms may be used our approach is based on a 
trusted environment realized via trusted computing 
mechanisms because we see this solution as the most generic 
and realistic approach this decision is discussed briefly in 
section 
we are now able to specify the abstract information 
filtering protocol as shown in figure the filter entity deploys a 
temporary filter entity tfe operating in a trusted 
environment the user entity deploys an additional relay entity 
operating in the same environment through mechanisms 
provided by this environment the relay entity is able to 
control the communication of the tfe and the provider entity 
is able to control the communication of both relay entity 
and the tfe thus it is possible to ensure that the 
controlled entities are only able to propagate recommendations 
but no other private information in the first stage steps 
 to of figure the relay entity establishes control of 
the tfe and thus prevents it from propagating user profile 
information user profile data is propagated without 
participation of the provider entity from the user entity to the 
tfe via the relay entity in the second stage steps to 
 of figure the provider entity establishes control of 
both relay and tfe and thus prevents them from 
propagating provider profile information provider profile data is 
propagated from the provider entity to the tfe via the 
relay entity in the third stage steps to of figure 
the tfe returns the recommendations via the relay entity 
and the controlled entities are terminated taken together 
these steps ensure that all private information is acquired 
temporarily only by the other main entities the problems 
of determining acceptable queries on the provider profile and 
ensuring unlinkability of the recommendations are discussed 
in the following section 
our approach requires each entity in the distributed 
architecture to have the following five main abilities the ability 
to perform certain well-defined tasks such as carrying out a 
filtering process with a high degree of autonomy i e largely 
independent of other entities e g because the respective 
entity is not able to communicate in an unrestricted manner 
the ability to be deployable dynamically in a well-defined 
environment the ability to communicate with other entities 
the ability to achieve protection against external 
manipulation attempts and the ability to control and restrict the 
communication of other entities 
figure the abstract privacy-preserving 
information filtering protocol all communication across 
the environments indicated by dashed lines is 
prevented with the exception of communication with 
the controlling entity 
mas architectures are an ideal solution for realizing a 
distributed system characterized by these features because 
they provide agents constituting entities that are actually 
characterized by autonomy mobility and the ability to 
communicate as well as agent platforms as environments 
providing means to realize the security of agents in this 
context the issue of malicious hosts i e hosts attacking 
agents has to be addressed explicitly furthermore existing 
mas architectures generally do not allow agents to control 
the communication of other agents it is possible however 
to expand a mas architecture and to provide designated 
agents with this ability for these reasons our solution is 
based on a fipa -compliant mas architecture the 
entities introduced above are mapped directly to agents and 
the trusted environment in which they exist is realized in 
the form of agent platforms 
in addition to the mas architecture itself which is 
assumed as given our solution consists of the following five 
main modules 
 the controller module described in section 
provides functionality for controlling the communication 
capabilities of agents 
 the transparent persistence module facilitates 
the use of different data storage mechanisms and 
provides a uniform interface for accessing persistent 
information which may be utilized for monitoring critical 
interactions involving potentially private information 
e g as part of queries its description is outside the 
scope of this paper 
 the recommender module details of which are 
described in section provides recommender system 
functionality 
 the matchmaker module provides matchmaker 
system functionality it additionally utilizes social 
aspects of mas technology its description is outside the 
scope of this paper 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 finally a separate module described in section 
provides exemplary filtering techniques in order 
to show that various restrictions imposed on filtering 
techniques by our approach may actually be fulfilled 
the trusted environment introduced above encompasses 
the mas architecture itself and the controller module 
which have to be trusted to act in a non-malicious manner 
in order to rule out the possibility of malicious hosts 
 main modules and 
implementation 
in this section we describe the main modules of our 
approach and outline the implementation while we have 
chosen a specific architecture for the implementation the 
specification of the module is applicable to any fipa-compliant 
mas architecture a module basically encompasses 
ontologies functionality provided by agents via agent services and 
internal functionality throughout this paper {m}kx 
denotes a message m encrypted via a non-specified symmetric 
encryption scheme with a secret key kx used for 
encryption and decryption which is initially known only to 
participant x a key kxy is a key shared by participants 
x and y a cryptographic hash function is used at 
various points of the protocol i e a function returning a hash 
value h x for given data x that is both preimage-resistant 
and collision-resistant 
 we denote a set of hash values for 
a data set x {x xn} as h x {h x h xn } 
whereas h x denotes a single hash value of a data set 
 controller module 
as noted above the ability to control the communication 
of agents is generally not a feature of existing mas 
architectures 
but at the same time a central requirement of our 
approach for privacy-preserving information filtering the 
required functionality cannot be realized based on regular 
agent services or components because an agent on a 
platform is usually not allowed to interfere with the actions of 
other agents in any way therefore we add additional 
infrastructure providing the required functionality to the mas 
architecture itself resulting in an agent environment with 
extended functionality and responsibilities 
controlling the communication capabilities of an agent is 
realized by restricting via rules in a manner similar to a 
firewall but with the consent of the respective agent its 
incoming and outgoing communication to specific platforms 
or agents on external platforms as well as other possible 
communication channels such as the file system consent 
is required because otherwise the overall security would be 
compromised as attackers could arbitrarily block various 
communication channels our approach does not require 
controlling the communication between agents on the same 
platform and therefore this aspect is not addressed 
consequently all rules addressing communication capabilities 
have to be enforced across entire platforms because 
otherwise a controlled agent could just use a non-controlled agent 
 
in the implementation we have used the advanced 
encryption standard aes as the symmetric encryption scheme 
and sha- as the cryptographic hash function 
 
a recent survey on agent environments concludes that 
aspects related to agent environments are often neglected 
and does not indicate any existing work in this particular 
area 
on the same platform as a relay for communicating with 
agents residing on external platforms various agent services 
provide functionality for adding and revoking control of 
platforms including functionality required in complex scenarios 
where controlled agents in turn control further platforms 
the implementation of the actual control mechanism 
depends on the actual mas architecture in our 
implementation we have utilized methods provided via the java 
security manager as part of the java security model thus 
the supervisor agent is enabled to define custom security 
policies thereby granting or denying other agents access to 
resources required for communication with other agents as 
well as communication in general such as files or sockets for 
tcp ip-based communication 
 recommender module 
the recommender module is mainly responsible for 
carrying out information filtering processes according to the 
protocol described in table the participating entities are 
realized as agents and the interactions as agent services we 
assume that mechanisms for secure agent communication are 
available within the respective mas architecture two 
issues have to be addressed in this module the relevant parts 
of the provider profile have to be retrieved without 
compromising the user s privacy and the recommendations have to 
be propagated in a privacy-preserving way 
our solution is based on a threat model in which no main 
abstract entity may safely assume any other abstract entity 
to act in an honest manner each entity has to assume that 
other entities may attempt to obtain private information 
either while following the specified protocol or even by 
deviating from the protocol according to we classify the 
former case as honest-but-curious behavior as an example 
the tfe may propagate recommendations as specified but 
may additionally attempt to propagate private information 
and the latter case as malicious behavior as an example the 
filter may attempt to propagate private information instead 
of the recommendations 
 retrieving the provider profile 
as outlined above the relay agent relays data between 
the tfe agent and the provider agent these agents are 
not allowed to communicate directly because the tfe agent 
cannot be assumed to act in an honest manner unlike the 
user profile which is usually rather small the provider 
profile is often too voluminous to be propagated as a whole 
efficiently a typical example is a user profile containing 
ratings of about movies while the provider profile 
contains some movies retrieving only the relevant part 
of the provider profile however is problematic because it 
has to be done without leaking sensitive information about 
the user profile therefore the relay agent has to analyze all 
queries on the provider profile and reject potentially critical 
queries such as queries containing a set of user profile items 
because the propagation of single unlinkable user profile 
items is assumed to be uncritical we extend the 
information filtering protocol as follows the relevant parts of the 
provider profile are retrieved based on single anonymous 
interactions between the relay and the provider if the mas 
architecture used for the implementation does not provide 
an infrastructure for anonymous agent communication this 
feature has to be provided explicitly the most 
straightforward way is to use additional relay agents deployed via 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
table the basic information filtering protocol 
with participants u user agent p provider 
agent f tfe agent r relay agent based on the 
abstract protocol shown in figure up denotes the 
user profile with up {up upn} pp denotes the 
provider profile and rec denotes the set of 
recommendations with rec {rec recm} 
phase sender → message or action 
step receiver 
 r → f establish control 
 u → r up 
 r → f up 
 p → r f establish control 
 p → r pp 
 r → f pp 
 f → r rec 
 r → p rec 
 p → u rec 
 r → f terminate f 
 p → r terminate r 
the main relay agent and used once for a single anonymous 
interaction obviously unlinkability is only achieved if 
multiple instances of the protocol are executed simultaneously 
between the provider and different users because agents 
on controlled platforms are unable to communicate 
anonymously with the respective controlling agent control has to 
be established after the anonymous interactions have been 
completed to prevent the uncontrolled relay agents from 
propagating provider profile data the respective data is 
encrypted and the key is provided only after control has been 
established therefore the second phase of the protocol 
described in table is replaced as described in table 
additionally the relay agent may allow other interactions 
as long as no user profile items are used within the queries 
in this case the relay agent has to ensure that the provider 
does not obtain any information exceeding the information 
deducible via the recommendations themselves the 
clusterbased filtering technique described in section is an 
example for a filtering technique operating in this manner 
 recommendation propagation 
the propagation of the recommendations is even more 
problematic mainly because more participants are involved 
recommendations have to be propagated from the tfe 
agent via the relay and provider agent to the user agent no 
participant should be able to alter the recommendations or 
use them for the propagation of private information 
therefore every participant in this chain has to obtain and verify 
the recommendations in unencrypted form prior to the next 
agent in the chain i e the relay agent has to verify the 
recommendations before the provider obtains them and so on 
therefore the final phase of the protocol described in table 
 is replaced as described in table it basically consists of 
two parts step to and step to step each of 
which provide a solution for a problem related to the 
prisoners problem in which two participants the prisoners 
intend to exchange a message via a third untrusted 
participant the warden who may read the message but must not 
be able to alter it in an undetectable manner there are 
various solutions for protocols addressing the prisoners 
probtable the updated second stage of the 
information filtering protocol with definitions as above ppq 
is the part of the provider profile pp returned as the 
result of the query q 
phase sender → message or action 
step receiver 
repeat to ∀ up ∈ up 
 f → r q up a query based on up 
 r anon 
→ p q up r remains anonymous 
 p → r anon 
{ppq up }kp 
 p → r f establish control 
 p → r kp 
 r → f ppq up 
table the updated final stage of the information 
filtering protocol with definitions as above 
phase sender → message or action 
step receiver 
 f → r rec {h rec }kpf 
 r → p h kr {{h rec }kpf }kr 
 p → r kp f 
 r → p kr 
repeat ∀ rec ∈ rec 
 r → p {rec}kurrec 
repeat ∀ rec ∈ rec 
 p → u h kprec {{rec}kurrec 
}kprec 
repeat to ∀ rec ∈ rec 
 u → p kurrec 
 p → u kprec 
 u → f terminate f 
 p → u terminate u 
lem the more obvious of these however such as protocols 
based on the use of digital signatures introduce additional 
threats e g via the possibility of additional subliminal 
channels in order to minimize the risk of possible threats 
we have decided to use a protocol that only requires a 
symmetric encryption scheme 
the first part of the final phase is carried out as follows 
in order to prevent the relay from altering 
recommendations they are propagated by the filter together with an 
encrypted hash in step thus the relay is able to verify 
the recommendations before they are propagated further 
the relay however may suspect the data propagated as 
the encrypted hash to contain private information instead 
of the actual hash value therefore the encrypted hash is 
encrypted again and propagated together with a hash on 
the respective key in step in step the key kp f 
is revealed to the relay allowing the relay to validate the 
encrypted hash in step the key kr is revealed to the 
provider allowing the provider to decrypt the data received 
in step and thus to obtain h rec propagating the 
hash of the key kr prevents the relay from altering the 
recommendations to rec after step which would be 
undetectable otherwise because the relay could choose a key 
kr so that {{h rec }kpf }kr {{h rec }kpf }kr 
 
the encryption scheme used for encrypting the hash has to 
be secure against known-plaintext attacks because 
otherwise the relay may be able to obtain kp f after step and 
subsequently alter the recommendations in an undetectable 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
way additionally the encryption scheme must not be 
commutative for similar reasons 
the remaining protocol steps are interactions between 
relay provider and user agent the interactions of step 
to step ensure via the same mechanisms used in step 
 to that the provider is able to analyze the 
recommendations before the user obtains them but at the same 
time prevent the provider from altering the 
recommendations additionally the recommendations are not processed 
at once but rather one at a time to prevent the provider 
from withholding all recommendations 
upon completion of the protocol both user and provider 
have obtained a set of recommendations if the user wants 
these recommendations to be unlinkable to himself the user 
agent has to carry out the entire protocol anonymously 
again the most straightforward way to achieve this is to use 
additional relay agents deployed via the user agent which are 
used once for a single information filtering process 
 exemplary filtering techniques 
the filtering technique applied by the tfe agent cannot 
be chosen freely all collaboration-based approaches such 
as collaborative filtering techniques based on the profiles 
of a set of users are not applicable because the provider 
profile does not contain user profile data unless this data 
has been collected externally instead these approaches 
are realized via the matchmaker module which is outside 
the scope of this paper learning-based approaches are not 
applicable because the tfe agent cannot propagate any 
acquired data to the filter which effectively means that the 
filter is incapable of learning filtering techniques that are 
actually applicable are feature-based approaches such as 
content-based filtering in which profile items are compared 
via their attributes and knowledge-based filtering in which 
domain-specific knowledge is applied in order to match user 
and provider profile items an overview of different classes 
and hybrid combinations of filtering techniques is given in 
 we have implemented two generic content-based 
filtering approaches that are applicable within our approach 
a direct content-based filtering technique based on the 
class of item-based top-n recommendation algorithms is 
used in cases where the user profile contains items that are 
also contained in the provider profile in a preprocessing 
stage i e prior to the actual information filtering processes 
a model is generated containing the k most similar items for 
each provider profile item while computationally rather 
complex this approach is feasible because it has to be done 
only once and it is carried out in a privacy-preserving way 
via interactions between the provider agent and a tfe 
agent the resulting model is stored by the provider agent 
and can be seen as an additional part of the provider 
profile in the actual information filtering process the k most 
similar items are retrieved for each single user profile item 
via queries on the model as described in section this 
is possible in a privacy-preserving way via anonymous 
communication recommendations are generated by selecting 
the n most frequent items from the result sets that are not 
already contained within the user profile 
as an alternative approach applicable when the user 
profile contains information in addition to provider profile 
items we provide a cluster-based approach in which provider 
profile items are clustered in a preprocessing stage via an 
agglomerative hierarchical clustering approach each cluster is 
represented by a centroid item and the cluster elements are 
either sub-clusters or on the lowest level the items 
themselves in the information filtering stage the relevant items 
are retrieved by descending through the cluster hierarchy in 
the following manner the cluster items of the highest level 
are retrieved independent of the user profile by 
comparing these items with the user profile data the most relevant 
sub-clusters are determined and retrieved in a subsequent 
iteration this process is repeated until the lowest level 
is reached which contains the items themselves as 
recommendations throughout the process user profile items are 
never propagated to the provider as such the 
information deducible about the user profile does not exceed the 
information deducible via the recommendations themselves 
 because essentially only a chain of cluster centroids leading 
to the recommendations is retrieved and therefore it is not 
regarded as privacy-critical 
 implementation 
we have implemented the approach for privacy-preserving 
if based on jiac iv a fipa-compliant mas 
architecture jiac iv integrates fundamental aspects of 
autonomous agents regarding pro-activeness intelligence 
communication capabilities and mobility by providing a scalable 
component-based architecture additionally jiac iv offers 
components realizing management and security 
functionality and provides a methodology for agent-oriented 
software engineering jiac iv stands out among mas 
architectures as the only security-certified architecture since it 
has been certified by the german federal office for 
information security according to the eal of the common criteria 
for information technology security standard jiac iv 
offers several security features in the areas of access control 
for agent services secure communication between agents 
and low-level security based on java security policies 
and thus provides all security-related functionality required 
for our approach 
we have extended the jiac iv architecture by adding the 
mechanisms for communication control described in section 
 regarding the issue of malicious hosts we currently 
assume all providers of agent platforms to be trusted we 
are additionally developing a solution that is actually based 
on a trusted computing infrastructure 
 evaluation 
for the evaluation of our approach we have examined 
whether and to which extent the requirements mainly 
regarding privacy performance and quality are actually met 
privacy aspects are directly addressed by the modules and 
protocols described above and therefore not evaluated 
further here performance is a critical issue mainly because of 
the overhead caused by creating additional agents and agent 
platforms for controlling communication and by the 
additional interactions within the recommender module 
overall a single information filtering process takes about ten 
times longer than a non-privacy-preserving information 
filtering process leading to the same results which is a 
considerable overhead but still acceptable under certain conditions 
as described in the following section 
 the smart event assistant 
as a proof of concept and in order to evaluate 
performance and quality under real-life conditions we have 
ap the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure the smart event assistant a 
privacypreserving recommender system supporting users 
in planning entertainment-related activities 
plied our approach within the smart event assistant a 
mas-based recommender system which integrates various 
personalized services for entertainment planning in 
different german cities such as a restaurant finder and a movie 
finder additional services such as a calendar a 
routing service and news services complement the information 
services an intelligent day planner integrates all 
functionality by providing personalized recommendations for the 
various information services based on the user s preferences 
and taking into account the location of the user as well as 
the potential venues all services are accessible via mobile 
devices as well 
 figure shows a screenshot of the 
intelligent day planner s result dialog the smart event 
assistant is entirely realized as a mas system providing among 
other functionality various filter agents and different service 
provider agents which together with the user agents utilize 
the functionality provided by our approach 
recommendations are generated in two ways a push 
service delivers new recommendations to the user in regular 
intervals e g once per day via email or sms because the 
user is not online during these interactions they are less 
critical with regard to performance and the protracted 
duration of the information filtering process is acceptable in 
this case recommendations generated for the intelligent 
day planner however have to be delivered with very little 
latency because the process is triggered by the user who 
expects to receive results promptly in this scenario the 
overall performance is substantially improved by setting up 
the relay agent and the tfe agent oﬄine i e prior to the 
user s request and by an efficient retrieval of the relevant 
 
the smart event assistant may be accessed online via 
http www smarteventassistant de 
table complexity of typical privacy-preserving 
 pp vs non-privacy-preserving npp filtering 
processes in the realized application in the 
nonprivacy-preserving version an agent retrieves the 
profiles directly and propagates the result to a 
provider agent 
scenario push day planning 
version npp pp npp pp 
profile size retrieved total amount of items 
user 
provider 
elapsed time in filtering process in seconds 
setup n a n a oﬄine 
database access 
profile propagation n a n a 
filtering algorithm 
result propagation 
complete time 
part of the provider profile because the user is only 
interested in items such as movies available within a certain 
time period and related to specific locations such as 
screenings at cinemas in a specific city the relevant part of the 
provider profile is usually small enough to be propagated 
entirely because these additional parameters are not seen 
as privacy-critical as they are not based on the user 
profile but rather constitute a short-term information need 
the relevant part of the provider profile may be propagated 
as a whole with no need for complex interactions taken 
together these improvements result in a filtering process 
that takes about three times as long as the respective 
nonprivacy-preserving filtering process which we regard as an 
acceptable trade-off for the increased level of privacy table 
 shows the results of the performance evaluation in more 
detail in these scenarios a direct content-based filtering 
technique similar to the one described in section is 
applied because equivalent filtering techniques have been 
applied successfully in regular recommender systems there 
are no negative consequences with regard to the quality of 
the recommendations 
 alternative approaches 
as described in section our solution is based on 
trusted computing there are more straightforward ways 
to realize privacy-preserving if e g by utilizing a 
centralized architecture in which the privacy-preserving 
providerside functionality is realized as trusted software based on 
trusted computing however we consider these approaches 
to be unsuitable because they are far less generic whenever 
some part of the respective software is patched upgraded or 
replaced the entire system has to be analyzed again in order 
to determine its trustworthiness a process that is 
problematic in itself due to its complexity in our solution only a 
comparatively small part of the overall system is based on 
trusted computing because agent platforms can be utilized 
for a large variety of tasks and because we see trusted 
computing as the most promising approach to realize secure and 
trusted agent environments it seems reasonable to assume 
that these respective mechanisms will be generally available 
in the future independent of specific solutions such as the 
one described here 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 conclusion further work 
we have developed an agent-based approach for 
privacypreserving recommender systems by utilizing 
fundamental features of agents such as autonomy adaptability and 
the ability to communicate by extending the capabilities 
of agent platform managers regarding control of agent 
communication by providing a privacy-preserving protocol for 
information filtering processes and by utilizing suitable 
filtering techniques we have been able to realize an approach 
which actually preserves privacy in information filtering 
architectures in a multilateral way as a proof of concept we 
have used the approach within an application supporting 
users in planning entertainment-related activities 
we envision various areas of future work to achieve 
complete user privacy the protocol should be extended in order 
to keep the recommendations themselves private as well 
generally the feedback we have obtained from users of the 
smart event assistant indicates that most users are 
indifferent to privacy in the context of entertainment-related 
personal information therefore we intend to utilize the 
approach to realize a recommender system in a more 
privacysensitive domain such as health or finance which would 
enable us to better evaluate user acceptance 
 acknowledgments 
we would like to thank our colleagues andreas rieger 
and nicolas braun who co-developed the smart event 
assistant the smart event assistant is based on a project 
funded by the german federal ministry of education and 
research under grant no ak and a project funded 
by the german federal ministry of economics and labour 
under grant no md 
 references 
 r agrawal j kiernan r srikant and y xu 
hippocratic databases in th int l conf on very 
large databases vldb hong kong 
 r agrawal and r srikant privacy-preserving data 
mining in proc of the acm sigmod conference on 
management of data pages - acm press 
may 
 e a¨ımeur g brassard j m fernandez and f s 
mani onana privacy-preserving demographic 
filtering in sac proceedings of the acm 
symposium on applied computing pages - new 
york ny usa acm press 
 m bawa r bayardo jr and r agrawal 
privacy-preserving indexing of documents on the 
network in proc of the vldb 
 r burke hybrid recommender systems survey and 
experiments user modeling and user-adapted 
interaction - 
 j canny collaborative filtering with privacy in 
ieee symposium on security and privacy pages 
 - 
 b chor o goldreich e kushilevitz and m sudan 
private information retrieval in ieee symposium on 
foundations of computer science pages - 
 r ciss´ee an architecture for agent-based 
privacy-preserving information filtering in proceedings 
of the th international workshop on trust privacy 
deception and fraud in agent systems 
 m deshpande and g karypis item-based top-n 
recommendation algorithms acm trans inf syst 
 - 
 l foner political artifacts and personal privacy the 
yenta multi-agent distributed matchmaking system 
phd thesis mit 
 foundation for intelligent physical agents fipa 
abstract architecture specification version l 
 s fricke k bsufka j keiser t schmidt 
r sesseler and s albayrak agent-based telematic 
services and telecom applications communications of 
the acm april 
 t garfinkel m rosenblum and d boneh flexible 
os support and applications for trusted computing in 
proceedings of hotos-ix may 
 t geissler and o kroll-peters applying security 
standards to multi agent systems in aamas 
workshop safety security in multiagent systems 
 o goldreich s micali and a wigderson how to 
play any mental game in proc of stoc pages 
 - new york ny usa acm press 
 s jha l kruger and p mcdaniel privacy 
preserving clustering in esorics volume 
of lncs springer 
 g karjoth m schunter and m waidner the 
platform for enterprise privacy practices 
privacy-enabled management of customer data in 
pet volume of lncs springer 
 h link j saia t lane and r a laviolette the 
impact of social networks on multi-agent recommender 
systems in proc of the workshop on cooperative 
multi-agent learning ecml pkdd 
 b n miller j a konstan and j riedl pocketlens 
toward a personal recommender system acm trans 
inf syst - 
 h polat and w du svd-based collaborative filtering 
with privacy in proc of sac pages - 
new york ny usa acm press 
 t schmidt advanced security infrastructure for 
multi-agent-applications in the telematic area phd 
thesis technische universit¨at berlin 
 g j simmons the prisoners problem and the 
subliminal channel in d chaum editor proc of 
crypto pages - plenum press 
 m teltzrow and a kobsa impacts of user privacy 
preferences on personalized systems a comparative 
study in designing personalized user experiences in 
ecommerce pages - 
 d weyns h parunak f michel t holvoet and 
j ferber environments for multiagent systems 
state-of-the-art and research challenges in 
environments for multiagent systems volume of 
lncs springer 
 j wohltorf r ciss´ee and a rieger berlintainment 
an agent-based context-aware entertainment planning 
system ieee communications magazine 
 m wooldridge and n r jennings intelligent agents 
theory and practice knowledge engineering review 
 - 
 a yao protocols for secure computation in proc of 
ieee fogs pages - 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a framework for agent-based distributed machine 
learning and data mining 
jan tozicka 
gerstner laboratory 
czech technical university 
technick a prague 
czech republic 
tozicka labe felk cvut cz 
michael rovatsos 
school of informatics 
the university of edinburgh 
edinburgh eh le 
united kingdom 
mrovatso inf ed ac uk 
michal pechoucek 
gerstner laboratory 
czech technical university 
technick a prague 
czech republic 
pechouc labe felk cvut cz 
abstract 
this paper proposes a framework for agent-based 
distributed machine learning and data mining based on i 
the exchange of meta-level descriptions of individual 
learning processes among agents and ii online reasoning about 
learning success and learning progress by learning agents 
we present an abstract architecture that enables agents to 
exchange models of their local learning processes and 
introduces a number of different methods for integrating these 
processes this allows us to apply existing agent 
interaction mechanisms to distributed machine learning tasks 
thus leveraging the powerful coordination methods available 
in agent-based computing and enables agents to engage in 
meta-reasoning about their own learning decisions we 
apply this architecture to a real-world distributed clustering 
application to illustrate how the conceptual framework can 
be used in practical systems in which different learners may 
be using different datasets hypotheses and learning 
algorithms we report on experimental results obtained using 
this system review related work on the subject and discuss 
potential future extensions to the framework 
general terms 
theory 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence-multiagent systems 
 introduction 
in the areas of machine learning and data mining cf 
 for overviews it has long been recognised that 
parallelisation and distribution can be used to improve learning 
performance various techniques have been suggested in 
this respect ranging from the low-level integration of 
independently derived learning hypotheses e g combining 
different classifiers to make optimal classification decisions 
 model averaging of bayesian classifiers or 
consensusbased methods for integrating different clusterings to 
the high-level combination of learning results obtained by 
heterogeneous learning agents using meta-learning e g 
 
all of these approaches assume homogeneity of agent 
design all agents apply the same learning algorithm and or 
agent objectives all agents are trying to cooperatively solve 
a single global learning problem therefore the techniques 
they suggest are not applicable in societies of autonomous 
learners interacting in open systems in such systems 
learners agents may not be able to integrate their datasets or 
learning results because of different data formats and 
representations learning algorithms or legal restrictions that 
prohibit such integration and cannot always be 
guaranteed to interact in a strictly cooperative fashion discovered 
knowledge and collected data might be economic assets that 
should only be shared when this is deemed profitable 
malicious agents might attempt to adversely influence others 
learning results etc 
examples for applications of this kind abound many 
distributed learning domains involve the use of sensitive data 
and prohibit the exchange of this data e g exchange of 
patient data in distributed brain tumour diagnosis - 
however they may permit the exchange of local learning 
hypotheses among different learners in other areas training 
data might be commercially valuable so that agents would 
only make it available to others if those agents could 
provide something in return e g in remote ship surveillance 
and tracking where the different agencies involved are 
commercial service providers furthermore agents might 
have a vested interest in negatively affecting other agents 
learning performance an example for this is that of 
fraudulent agents on ebay which may try to prevent 
reputationlearning agents from the construction of useful models for 
detecting fraud 
viewing learners as autonomous self-directed agents is 
the only appropriate view one can take in modelling these 
distributed learning environments the agent metaphor 
becomes a necessity as oppossed to preferences for scalability 
dynamic data selection interactivity which can also 
be achieved through non-agent distribution and 
parallelisation in principle 
despite the autonomy and self-directedness of learning 
agents many of these systems exhibit a sufficient overlap 
in terms of individual learning goals so that beneficial 
cooperation might be possible if a model for flexible 
interaction between autonomous learners was available that allowed 
agents to 
 exchange information about different aspects of their 
own learning mechanism at different levels of detail 
without being forced to reveal private information that 
should not be disclosed 
 decide to what extent they want to share information 
about their own learning processes and utilise 
information provided by other learners and 
 reason about how this information can best be used to 
improve their own learning performance 
our model is based on the simple idea that autonomous 
learners should maintain meta-descriptions of their own 
learning processes see also in order to be able to 
exchange information and reason about them in a rational way 
 i e with the overall objective of improving their own 
learning results our hypothesis is a very simple one 
if we can devise a sufficiently general abstract 
view of describing learning processes we will be 
able to utilise the whole range of methods for i 
rational reasoning and ii communication and 
coordination offered by agent technology so as to 
build effective autonomous learning agents 
to test this hypothesis we introduce such an abstract 
architecture section and implement a simple concrete 
instance of it in a real-world domain section we report 
on empirical results obtained with this implemented system 
that demonstrate the viability of our approach section 
finally we review related work section and conclude 
with a summary discussion of our approach and outlook to 
future work on the subject section 
 abstract architecture 
our framework is based on providing formal meta-level 
descriptions of learning processes i e representations of all 
relevant components of the learning machinery used by a 
learning agent together with information about the state of 
the learning process 
to ensure that this framework is sufficiently general we 
consider the following general description of a learning 
problem 
given data d ⊆ d taken from an instance space 
d a hypothesis space h and an unknown 
target function c ∈ h 
 derive a function h ∈ h that 
approximates c as well as possible according to 
some performance measure g h → q where q 
is a set of possible levels of learning performance 
 
by requiring this we are ensuring that the learning problem 
can be solved in principle using the given hypothesis space 
this very broad definition includes a number of components 
of a learning problem for which more concrete specifications 
can be provided if we want to be more precise for the cases 
of classification and clustering for example we can further 
specify the above as follows learning data can be described 
in both cases as d ×n 
i ai where ai is the domain of 
the ith attribute and the set of attributes is a { n} 
for the hypothesis space we obtain 
h ⊆ {h h d → { }} 
in the case of classification i e a subset of the set of all 
possible classifiers the nature of which depends on the 
expressivity of the learning algorithm used and 
h ⊆ {h h d → n h is total with range { k}} 
in the case of clustering i e a subset of all sets of possible 
cluster assignments that map data points to a finite number 
of clusters numbered to k for classification g might be 
defined in terms of the numbers of false negatives and false 
positives with respect to some validation set v ⊆ d and 
clustering might use various measures of cluster validity to 
evaluate the quality of a current hypothesis so that q r 
in both cases but other sets of learning quality levels can 
be imagined 
next we introduce a notion of learning step which 
imposes a uniform basic structure on all learning processes that 
are supposed to exchange information using our framework 
for this we assume that each learner is presented with a 
finite set of data d d dk in each step this is an 
ordered set to express that the order in which the samples are 
used for training matters and employs a training update 
function f h × d∗ 
→ h which updates h given a series of 
samples d dk in other words one learning step always 
consists of applying the update function to all samples in d 
exactly once we define a learning step as a tuple 
l d h f g h 
where we require that h ⊆ h and h ∈ h 
the intuition behind this definition is that each learning 
step completely describes one learning iteration as shown 
in figure in step t the learner updates the current 
hypothesis ht− with data dt and evaluates the resulting new 
hypothesis ht according to the current performance measure 
gt such a learning step is equivalent to the following steps 
of computation 
 train the algorithm on all samples in d once i e 
calculate ft ht− dt ht 
 calculate the quality gt of the resulting hypothesis 
gt ht 
we denote the set of all possible learning steps by l for 
ease of notation we denote the components of any l ∈ l by 
d l h l f l and g l respectively the reason why such 
learning step specifications use a subset h of h instead of 
h itself is that learners often have explicit knowledge about 
which hypotheses are effectively ruled out by f given h in 
the future if this is not the case we can still set h h 
a learning process is a finite non-empty sequence 
l l → l → → ln 
of learning steps such that 
∀ ≤ i n h li f li h li d li 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
training function 
ht 
performance measure solution quality 
qtgtft 
training set 
dt 
hypothesis 
hypothesis 
ht− 
figure a generic model of a learning step 
i e the only requirement the transition relation →⊆ l × l 
makes is that the new hypothesis is the result of training the 
old hypothesis on all available sample data that belongs to 
the current step we denote the set of all possible learning 
processes by l ignoring for ease of notation the fact that 
this set depends on h d and the spaces of possible 
training and evaluation functions f and g the performance 
trace associated with a learning process l is the sequence 
q qn ∈ qn 
where qi g li h li i e the sequence 
of quality values calculated by the performance measures of 
the individual learning steps on the respective hypotheses 
such specifications allow agents to provide a 
selfdescription of their learning process however in 
communication among learning agents it is often useful to 
provide only partial information about one s internal learning 
process rather than its full details e g when advertising 
this information in order to enter information exchange 
negotiations with others for this purpose we will assume 
that learners describe their internal state in terms of sets of 
learning processes in the sense of disjunctive choice which 
we call learning process descriptions lpds rather than by 
giving precise descriptions about a single concrete learning 
process 
this allows us to describe properties of a learning 
process without specifying its details exhaustively as an 
example the set {l ∈ l ∀l l i d l ≤ } describes all 
processes that have a training set of at most 
samples where all the other elements are arbitrary likewise 
{l ∈ l ∀l l i d l {d}} is equivalent to just providing 
information about a single sample {d} and no other details 
about the process this can be useful to model for 
example data received from the environment therefore we use 
℘ l that is the set of all lpds as the basis for 
designing content languages for communication in the protocols 
we specify below 
in practice the actual content language chosen will of 
course be more restricted and allow only for a special type 
of subsets of l to be specified in a compact way and its 
choice will be crucial for the interactions that can occur 
between learning agents for our examples below we simply 
assume explicit enumeration of all possible elements of the 
respective sets and function spaces d h etc extended by 
the use of wildcard symbols ∗ so that our second example 
above would become {d} ∗ ∗ ∗ ∗ 
 learning agents 
in our framework a learning agent is essentially a 
metareasoning function that operates on information about 
learning processes and is situated in an environment co-inhabited 
by other learning agents this means that it is not only 
capable of meta-level control on how to learn but in doing 
so it can take information into account that is provided by 
other agents or the environment although purely 
cooperative or hybrid cases are possible for the purposes of this 
paper we will assume that agents are purely self-interested 
and that while there may be a potential for cooperation 
considering how agents can mutually improve each others 
learning performance there is no global mechanism that can 
enforce such cooperative behaviour 
formally speaking an agent s learning function is a 
function which given a set of histories of previous learning 
processes of oneself and potentially of learning processes about 
which other agents have provided information and outputs 
a learning step which is its next learning action in the 
most general sense our learning agent s internal learning 
process update can hence be viewed as a function 
λ ℘ l → l × ℘ l 
which takes a set of learning histories of oneself and others 
as inputs and computes a new learning step to be executed 
while updating the set of known learning process histories 
 e g by appending the new learning action to one s own 
learning process and leaving all information about others 
learning processes untouched note that in λ {l ln} 
 l {l ln } some elements li of the input learning process 
set may be descriptions of new learning data received from 
the environment 
the λ-function can essentially be freely chosen by the 
agent as long as one requirement is met namely that the 
learning data that is being used always stems from what 
has been previously observed more formally 
∀{l ln} ∈ ℘ l λ {l ln} l {l ln } 
⇒ 
„ 
d l ∪ 
 
l li j 
d l 
 
⊆ 
 
l li j 
d l 
i e whatever λ outputs as a new learning step and updated 
set of learning histories it cannot invent new data it has 
to work with the samples that have been made available 
to it earlier in the process through the environment or from 
other agents and it can of course re-train on previously used 
data 
the goal of the agent is to output an optimal learning 
step in each iteration given the information that it has one 
possibility of specifying this is to require that 
∀{l ln} ∈ ℘ l λ {l ln} l {l ln } 
⇒ l arg max 
l ∈l 
g l h l 
but since it will usually be unrealistic to compute the 
optimal next learning step in every situation it is more useful 
 
note that our outlook is not only different from common 
cooperative models of distributed machine learning and data 
mining but also delineates our approach from multiagent 
learning systems in which agents learn about other agents 
 i e the learning goal itself is not affected by agents 
behaviour in the environment 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
i j dj hj fj gj hj 
di 
pd→d 
 di dj 
 
 
pd→d 
kd→d 
 di dj 
 n a 
hi 
 
 
 n a 
fi 
 
 
 n a 
gi 
 
 n a 
pg→h 
 gi hj 
 
 
pg→h 
kg→h 
 gi hj 
hi 
 
 n a 
 
table matrix of integration functions for 
messages sent from learner i to j 
to simply use g l h l as a running performance measure 
to evaluate how well the agent is performing 
this is too abstract and unspecific for our purposes while 
it describes what agents should do transform the settings 
for the next learning step in an optimal way it doesn t 
specify how this can be achieved in practice 
 integrating learning process information 
to specify how an agent s learning process can be affected 
by integrating information received from others we need to 
flesh out the details of how the learning steps it will perform 
can be modified using incoming information about learning 
processes described by other agents this includes the 
acquisition of new learning data from the environment as a 
special case in the most general case we can specify this 
in terms of the potential modifications to the existing 
information about learning histories that can be performed using 
new information for ease of presentation we will assume 
that agents are stationary learning processes that can only 
record the previously executed learning step and only 
exchange information about this one individual learning step 
 our model can be easily extended to cater for more complex 
settings 
let lj dj hj fj gj hj be the current state of 
agent j when receiving a learning process description li 
di hi fi gi hi from agent i for the time being we 
assume that this is a specific learning step and not a more 
vague disjunctive description of properties of the 
learning step of i considering all possible interactions at an 
abstract level we basically obtain a matrix of 
possibilities for modifications of j s learning step specification as 
shown in table in this matrix each entry specifies 
a family of integration functions pc→c 
 pc→c 
kc→c 
where 
c c ∈ {d h f g h} and which define how agent j s 
component cj will be modified using the information ci provided 
about the same or a different component of i s learning 
step by applying pc→c 
r ci cj for some r ∈ { kc→c } 
to put it more simply the collections of p-functions an agent 
j uses specifies how it will modify its own learning behaviour 
using information obtained from i 
for the diagonal of this matrix which contains the most 
common ways of integrating new information in one s own 
learning model obvious ways of modifying one s own 
learning process include replacing cj by ci or ignoring ci 
altogether more complex subtle forms of learning process 
integration include 
 modification of dj append di to dj filter out all 
elements from dj which also appear in di append 
di to dj discarding all elements with attributes 
outside ranges which affect gj or those elements already 
correctly classified by hj 
 modification of hi use the union intersection of hi 
and hj alternatively discard elements of hj that are 
inconsistent with dj in the process of intersection or 
union or filter out elements that cannot be obtained 
using fj unless fj is modified at the same time 
 modification of fj modify parameters or background 
knowledge of fj using information about fi assess 
their relevance by simulating previous learning steps 
on dj using gj and discard those that do not help 
improve own performance 
 modification of hj combine hj with hi using say 
logical or mathematical operators make the use of hi 
contingent on a pre-integration assessment of its quality 
using own data dj and gj 
while this list does not include fully fledged concrete 
integration operations for learning processes it is indicative of 
the broad range of interactions between individual agents 
learning processes that our framework enables 
note that the list does not include any modifications to 
gj this is because we do not allow modifications to the 
agent s own quality measure as this would render the model 
of rational learning action useless if the quality measure 
is relative and volatile we cannot objectively judge learning 
performance also note that some of the above examples 
require consulting other elements of lj than those appearing 
as arguments of the p-operations we omit these for ease 
of notation but emphasise that information-rich operations 
will involve consulting many different aspects of lj 
apart from operations along the diagonal of the matrix 
more exotic integration operations are conceivable that 
combine information about different components in theory 
we could fill most of the matrix with entries for them but 
for lack of space we list only a few examples 
 modification of dj using fi pre-process samples in 
fi e g to achieve intermediate representations that fj 
can be applied to 
 modification of dj using hi filter out samples from 
dj that are covered by hi and build hj using fj only 
on remaining samples 
 modification of hj using fi filter out hypotheses from 
hj that are not realisable using fi 
 modification of hj using gi if hj is composed of several 
sub-components filter out those sub-components that 
do not perform well according to gi 
 
finally many messages received from others describing 
properties of their learning processes will contain 
information about several elements of a learning step giving rise to 
yet more complex operations that depend on which kinds of 
information are available 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure screenshot of our simulation system 
displaying online vessel tracking data for the north sea 
region 
 application example 
 domain description 
as an illustration of our framework we present an 
agentbased data mining system for clustering-based surveillance 
using ais automatic identification system data in 
our application domain different commercial and 
governmental agencies track the journeys of ships over time 
using ais data which contains structured information 
automatically provided by ships equipped with shipborne 
mobile ais stations to shore stations other ships and aircrafts 
this data contains the ship s identity type position course 
speed navigational status and other safety-related 
information figure shows a screenshot of our simulation system 
it is the task of ais agencies to detect anomalous 
behaviour so as to alarm police coastguard units to further 
investigate unusual potentially suspicious behaviour such 
behaviour might include things such as deviation from the 
standard routes between the declared origin and destination 
of the journey unexpected close encounters between 
different vessels on sea or unusual patterns in the choice of 
destination over multiple journeys taking the type of 
vessel and reported freight into account while the reasons for 
such unusual behaviour may range from pure coincidence or 
technical problems to criminal activity such as smuggling 
piracy terrorist military attacks it is obviously useful to 
pre-process the huge amount of vessel tracking data that 
is available before engaging in further analysis by human 
experts 
to support this automated pre-processing task software 
used by these agencies applies clustering methods in order 
to identify outliers and flag those as potentially suspicious 
entities to the human user however many agencies active 
in this domain are competing enterprises and use their 
 partially overlapping but distinct datasets and learning 
hypotheses models as assets and hence cannot be expected 
to collaborate in a fully cooperative way to improve 
overall learning results considering that this is the reality of 
the domain in the real world it is easy to see that a 
framework like the one we have suggested above might be useful 
to exploit the cooperation potential that is not exploited by 
current systems 
 agent-based distributed learning system 
design 
to describe a concrete design for the ais domain we need 
to specify the following elements of the overall system 
 the datasets and clustering algorithms available to 
individual agents 
 the interaction mechanism used for exchanging 
descriptions of learning processes and 
 the decision mechanism agents apply to make learning 
decisions 
regarding our agents are equipped with their own private 
datasets in the form of vessel descriptions learning samples 
are represented by tuples containing data about individual 
vessels in terms of attributes a { n} including things 
such as width length etc with real-valued domains ai 
r for all i 
in terms of learning algorithm we consider clustering 
with a fixed number of k clusters using the k-means and 
k-medoids clustering algorithms fixed meaning that 
the learning algorithm will always output k clusters 
however we allow agents to change the value of k over different 
learning cycles this means that the hypothesis space can 
be defined as h { c ck ci ∈ r a 
} i e the set of all 
possible sets of k cluster centroids in a -dimensional 
euclidean space for each hypothesis h c ck and any 
data point d ∈ ×n 
i ai given domain ai for the ith 
attribute of each sample the assignment to clusters is given 
by 
c c ck d arg min 
 ≤j≤k 
 d − cj 
i e d is assigned to that cluster whose centroid is closest to 
the data point in terms of euclidean distance 
for evaluation purposes each dataset pertaining to a 
particular agent i is initially split into a training set di and a 
validation vi then we generate a set of fake vessels fi 
such that fi vi these two sets assess the agent s 
ability to detect suspicious vessels for this we assign a 
confidence value r h d to every ship d 
r h d 
 
 d − cc h d 
where c h d is the index of the nearest centroid based 
on this measure we classify any vessel in fi ∪ vi as fake if 
its r-value is below the median of all the confidences r h d 
for d ∈ fi ∪ vi with this we can compute the quality 
gi h ∈ r as the ratio between all correctly classified vessels 
and all vessels in fi ∪ vi 
as concerns we use a simple contract-net protocol 
 cnp based hypothesis trading mechanism before 
each learning iteration agents issue publicly broadcasted 
calls-for-proposals cfps advertising their own numerical 
model quality in other words the initiator of a cnp 
describes its own current learning state as ∗ ∗ ∗ gi h ∗ 
where h is their current hypothesis model we assume that 
agents are sincere when advertising their model quality but 
note that this quality might be of limited relevance to other 
agents as they may specialise on specific regions of the data 
space not related to the test set of the sender of the cfp 
subsequently some agents may issue bids in which they 
advertise in turn the quality of their own model if the 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
bids if any are accepted by the initiator of the protocol 
who issued the cfp the agents exchange their hypotheses 
and the next learning iteration ensues 
to describe what is necessary for we have to specify 
 i under which conditions agents submit bids in response 
to a cfp ii when they accept bids in the cnp negotiation 
process and iii how they integrate the received 
information in their own learning process concerning i and ii 
we employ a very simple rule that is identical in both cases 
let g be one s own model quality and g that advertised by 
the cfp or highest bid respectively if g g we respond 
to the cfp accept the bid else respond to the cfp 
 accept the bid with probability p g g and ignore reject it 
else if two agents make a deal they exchange their 
learning hypotheses models in our experiments g and g are 
calculated by an additional agent that acts as a global 
validation mechanism for all agents in a more realistic setting a 
comparison mechanism for different g functions would have 
to be provided 
as for iii each agent uses a single model merging 
operator taken from the following two classes of operators hj is 
the receiver s own model and hi is the provider s model 
 ph→h 
 hi hj 
- m-join the m best clusters in terms of coverage 
of dj from hypothesis hi are appended to hj 
- m-select the set of the m best clusters in terms 
of coverage of dj from the union hi ∪hj is chosen 
as a new model unlike m-join this method does 
not prefer own clusters over others 
 ph→d 
 hi dj 
- m-filter the m best clusters as above from 
hi are identified and appended to a new model 
formed by using those samples not covered by 
these clusters applying the own learning 
algorithm fj 
whenever m is large enough to encompass all clusters we 
simply write join or filter for them in section we analyse 
the performance of each of these two classes for different 
choices of m 
it is noteworthy that this agent-based distributed data 
mining system is one of the simplest conceivable instances 
of our abstract architecture while we have previously 
applied it also to a more complex market-based architecture 
using inductive logic programming learners in a transport 
logistics domain we believe that the system described 
here is complex enough to illustrate the key design decisions 
involved in using our framework and provides simple 
example solutions for these design issues 
 experimental results 
figure shows results obtained from simulations with 
three learning agents in the above system using the k-means 
and k-medoids clustering methods respectively we 
partition the total dataset of ships into three disjoint sets of 
 samples each and assign each of these to one learning 
agent the single agent is learning from the whole dataset 
the parameter k is set to as this is the optimal value for 
the total dataset according to the davies-bouldin index 
for m-select we assume m k which achieves a constant 
figure performance results obtained for different 
integration operations in homogeneous learner 
societies using the k-means top and k-medoids 
 bottom methods 
model size for m-join and m-filter we assume m to 
limit the extent to which models increase over time 
during each experiment the learning agents receive ship 
descriptions in batches of samples between these 
batches there is enough time to exchange the models among 
the agents and recompute the models if necessary each 
ship is described using width length draught and speed 
attributes with the goal of learning to detect which vessels 
have provided fake descriptions of their own properties the 
validation set contains real and randomly generated 
fake ships to generate sufficiently realistic properties for 
fake ships their individual attribute values are taken from 
randomly selected ships in the validation set so that each 
fake sample is a combination of attribute values of several 
existing ships 
in these experiments we are mainly interested in 
investigating whether a simple form of knowledge sharing between 
self-interested learning agents could improve agent 
performance compared to a setting of isolated learners thereby 
we distinguish between homogeneous learner societies where 
all agents use the same clustering algorithm and 
heterogeneous ones where different agents use different algorithms 
as can be seen from the performance plots in figure 
 homogeneous case and heterogeneous case two agents 
use the same method and one agent uses the other this is 
clearly the case for the unrestricted join and filter 
integration operations m k in both cases this is quite natural 
as these operations amount to sharing all available model 
knowledge among agents under appropriate constraints 
depending on how beneficial the exchange seems to the agents 
we can see that the quality of these operations is very close 
to the single agent that has access to all training data 
for the restricted m k m-join m-filter and m-select 
methods we can also observe an interesting distinction 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure performance results obtained for 
different integration operations in heterogeneous societies 
with the majority of learners using the k-means 
 top and k-medoids bottom methods 
namely that these perform similarly to the isolated learner 
case in homogeneous agent groups but better than isolated 
learners in more heterogeneous societies this suggests that 
heterogeneous learners are able to benefit even from rather 
limited knowledge sharing and this is what using a rather 
small m amounts to given that k while this is 
not always true for homogeneous agents this nicely 
illustrates how different learning or data mining algorithms can 
specialise on different parts of the problem space and then 
integrate their local results to achieve better individual 
performance 
apart from these obvious performance benefits 
integrating partial learning results can also have other advantages 
the m-filter operation for example decreases the number 
of learning samples and thus can speed up the learning 
process the relative number of filtered examples measured in 
our experiments is shown in the following table 
k-means k-medoids 
filtering - - 
m-filtering - - 
the overall conclusion we can draw from these initial 
experiments with our architecture is that since a very 
simplistic application of its principles has proven capable of 
improving the performance of individual learning agents it is 
worthwhile investigating more complex forms of 
information exchange about learning processes among autonomous 
learners 
 related work 
we have already mentioned work on distributed 
 nonagent machine learning and data mining in the 
introductory chapter so in this section we shall restrict ourselves to 
approaches that are more closely related to our outlook on 
distributed learning systems 
very often approaches that are allegedly agent-based 
completely disregard agent autonomy and prescribe local 
decision-making procedures a priori a typical example for 
this type of system is the one suggested by caragea et al 
which is based on a distributed support-vector machine 
approach where agents incrementally join their datasets 
together according to a fixed distributed algorithm a similar 
example is the work of weiss where groups of 
classifier agents learn to organise their activity so as to optimise 
global system behaviour 
the difference between this kind of collaborative 
agentbased learning systems and our own framework is that 
these approaches assume a joint learning goal that is pursued 
collaboratively by all agents 
many approaches rely heavily on a homogeneity 
assumption plaza and ontanon suggest methods for 
agentbased intelligent reuse of cases in case-based reasoning but 
is only applicable to societies of homogeneous learners and 
coined towards a specific learning method an 
agentbased method for integrating distributed cluster analysis 
processes using density estimation is presented by klusch 
et al which is also specifically designed for a 
particular learning algorithm the same is true of which 
both present market-based mechanisms for aggregating the 
output of multiple learning agents even though these 
approaches consider more interesting interaction mechanisms 
among learners 
a number of approaches for sharing learning data 
have also been proposed grecu and becker suggest an 
exchange of learning samples among agents and ghosh et 
al is a step in the right direction in terms of revealing 
only partial information about one s learning process as it 
deals with limited information sharing in distributed 
clustering 
papyrus is a system that provides a markup language 
for meta-description of data hypotheses and intermediate 
results and allows for an exchange of all this information 
among different nodes however with a strictly cooperative 
goal of distributing the load for massively distributed data 
mining tasks 
the male system was a very early multiagent 
learning system in which agents used a blackboard approach to 
communicate their hypotheses agents were able to critique 
each others hypotheses until agreement was reached 
however all agents in this system were identical and the system 
was strictly cooperative 
the animals system was used to simulate 
multistrategy learning by combining two or more learning 
techniques represented by heterogeneous agents in order to 
overcome weaknesses in the individual algorithms yet it was 
also a strictly cooperative system 
as these examples show and to the best of our knowledge 
there have been no previous attempts to provide a 
framework that can accommodate both independent and 
heterogeneous learning agents and this can be regarded as the main 
contribution of our work 
 conclusion 
in this paper we outlined a generic abstract framework 
for distributed machine learning and data mining this 
framework constitutes to our knowledge the first attempt 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
to capture complex forms of interaction between 
heterogeneous and or self-interested learners in an architecture that 
can be used as the foundation for implementing systems that 
use complex interaction and reasoning mechanisms to enable 
agents to inform and improve their learning abilities with 
information provided by other learners in the system 
provided that all agents engage in a sufficiently similar learning 
activity 
to illustrate that the abstract principles of our 
architecture can be turned into concrete computational systems 
we described a market-based distributed clustering system 
which was evaluated in the domain of vessel tracking for 
purposes of identifying deviant or suspicious behaviour 
although our experimental results only hint at the potential 
of using our architecture they underline that what we are 
proposing is feasible in principle and can have beneficial 
effects even in its most simple instantiation 
yet there is a number of issues that we have not addressed 
in the presentation of the architecture and its empirical 
evaluation firstly we have not considered the cost of 
communication and made the implicit assumption that the required 
communication comes for free this is of course 
inadequate if we want to evaluate our method in terms of the 
total effort required for producing a certain quality of 
learning results secondly we have not experimented with agents 
using completely different learning algorithms e g symbolic 
and numerical in systems composed of completely different 
agents the circumstances under which successful information 
exchange can be achieved might be very different from those 
described here and much more complex communication and 
reasoning methods may be necessary to achieve a useful 
integration of different agents learning processes finally more 
sophisticated evaluation criteria for such distributed 
learning architectures have to be developed to shed some light 
on what the right measures of optimality for autonomously 
reasoning and communicating agents should be 
these issues together with a more systematic and 
thorough investigation of advanced interaction and 
communication mechanisms for distributed collaborating and 
competing agents will be the subject of our future work on the 
subject 
acknowledgement we gratefully acknowledge the 
support of the presented research by army research 
laboratory project n - - and office for naval research 
project n - - - 
 references 
 http www aislive com 
 http www healthagents com 
 s bailey r grossman h sivakumar and 
a turinsky papyrus a system for data mining over 
local and wide area clusters and super-clusters in 
proc of the conference on supercomputing 
 e bauer and r kohavi an empirical comparison of 
voting classification algorithms bagging boosting 
and variants machine learning 
 p berkhin survey of clustering data mining 
techniques technical report accrue software 
 d caragea a silvescu and v honavar agents that 
learn from distributed dynamic data sources in 
proc of the workshop on learning agents 
 n chawla and s e abd l o hall creating 
ensembles of classifiers in proceedings of icdm 
pages - san jose ca usa 
 d dash and g f cooper model averaging for 
prediction with discrete bayesian networks journal 
of machine learning research - 
 d l davies and d w bouldin a cluster 
separation measure ieee transactions on pattern 
analysis and machine intelligence - 
 p edwards and w davies a heterogeneous 
multi-agent learning system in proceedings of the 
special interest group on cooperating knowledge 
based systems pages - 
 j ghosh a strehl and s merugu a consensus 
framework for integrating distributed clusterings 
under limited knowledge sharing in nsf workshop 
on next generation data mining - 
 d l grecu and l a becker coactive learning for 
distributed data mining in proceedings of kdd- 
pages - new york ny august 
 m klusch s lodi and g moro agent-based 
distributed data mining the kdec scheme in 
agentlink number in lncs springer 
 t m mitchell machine learning pages - 
mcgraw-hill new york 
 s ontanon and e plaza recycling data for 
multi-agent learning in proc of icml- 
 l panait and s luke cooperative multi-agent 
learning the state of the art autonomous agents 
and multi-agent systems - 
 b park and h kargupta distributed data mining 
algorithms systems and applications in n ye 
editor data mining handbook pages - 
 f j provost and d n hennessy scaling up 
distributed machine learning with cooperation in 
proc of aaai- pages - aaai press 
 s sian extending learning to multiple agents issues 
and a model for multi-agent machine learning 
 ma-ml in y kodratoff editor machine 
learningewsl- pages - springer-verlag 
 r smith the contract-net protocol high-level 
communication and control in a distributed problem 
solver ieee transactions on computers 
c- - 
 s j stolfo a l prodromidis s tselepis w lee 
d w fan and p k chan jam java agents for 
meta-learning over distributed databases in proc of 
the kdd- pages - usa 
 j toˇziˇcka m jakob and m pˇechouˇcek 
market-inspired approach to collaborative learning 
in cooperative information agents x cia 
volume of lncs pages - springer 
 y z wei l moreau and n r jennings 
recommender systems a market-based design in 
proceedings of aamas- pages - 
 g weiß a multiagent perspective of parallel and 
distributed machine learning in proceedings of 
agents pages - 
 g weiss and p dillenbourg what is multi in 
multi-agent learning collaborative-learning 
cognitive and computational approaches - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
approximate and online multi-issue negotiation 
shaheen s fatima 
department of 
computer science 
university of liverpool 
liverpool l bx uk 
shaheen csc liv ac uk 
michael wooldridge 
department of 
computer science 
university of liverpool 
liverpool l bx uk 
mjw csc liv ac uk 
nicholas r jennings 
school of electronics and 
computer science 
university of southampton 
southampton so bj uk 
nrj ecs soton ac uk 
abstract 
this paper analyzes bilateral multi-issue negotiation between 
selfinterested autonomous agents the agents have time constraints in 
the form of both deadlines and discount factors there are m 
issues for negotiation where each issue is viewed as a pie of size 
one the issues are indivisible i e individual issues cannot be 
split between the parties each issue must be allocated in its 
entirety to either agent here different agents value different issues 
differently thus the problem is for the agents to decide how to 
allocate the issues between themselves so as to maximize their 
individual utilities for such negotiations we first obtain the 
equilibrium strategies for the case where the issues for negotiation are 
known a priori to the parties then we analyse their time 
complexity and show that finding the equilibrium offers is an np-hard 
problem even in a complete information setting in order to 
overcome this computational complexity we then present negotiation 
strategies that are approximately optimal but computationally 
efficient and show that they form an equilibrium we also analyze the 
relative error i e the difference between the true optimum and the 
approximate the time complexity of the approximate equilibrium 
strategies is o nm 
 where n is the negotiation deadline and 
the relative error finally we extend the analysis to online 
negotiation where different issues become available at different time points 
and the agents are uncertain about their valuations for these issues 
specifically we show that an approximate equilibrium exists for 
online negotiation and show that the expected difference between 
the optimum and the approximate is o 
√ 
m these approximate 
strategies also have polynomial time complexity 
categories and subject descriptors 
i distributed artificial intelligence multiagent systems 
general terms 
algorithms design theory 
 introduction 
negotiation is a key form of interaction in multiagent systems it 
is a process in which disputing agents decide how to divide the 
gains from cooperation since this decision is made jointly by the 
agents themselves each party can only obtain what 
the other is prepared to allow them now the simplest form of 
negotiation involves two agents and a single issue for example 
consider a scenario in which a buyer and a seller negotiate on the 
price of a good to begin the two agents are likely to differ on the 
price at which they believe the trade should take place but through 
a process of joint decision-making they either arrive at a price that 
is mutually acceptable or they fail to reach an agreement since 
agents are likely to begin with different prices one or both of them 
must move toward the other through a series of offers and counter 
offers in order to obtain a mutually acceptable outcome however 
before the agents can actually perform such negotiations they must 
decide the rules for making offers and counter offers that is they 
must set the negotiation protocol on the basis of this protocol 
each agent chooses its strategy i e what offers it should make 
during the course of negotiation given this context this work 
focuses on competitive scenarios with self-interested agents for 
such cases each participant defines its strategy so as to maximise 
its individual utility 
however in most bilateral negotiations the parties involved need 
to settle more than one issue for this case the issues may be 
divisible or indivisible for the former the problem for the agents 
is to decide how to split each issue between themselves for 
the latter the individual issues cannot be divided an issue in its 
entirety must be allocated to either of the two agents since the 
agents value different issues differently they must come to terms 
about who will take which issue to date most of the existing 
work on multi-issue negotiation has focussed on the former case 
 however in many real-world settings the 
issues are indivisible hence our focus here is on negotiation for 
indivisible issues such negotiations are very common in 
multiagent systems for example consider the case of task allocation 
between two agents there is a set of tasks to be carried out and 
different agents have different preferences for the tasks the tasks 
cannot be partitioned a task must be carried out by one agent the 
problem then is for the agents to negotiate about who will carry out 
which task 
a key problem in the study of multi-issue negotiation is to 
determine the equilibrium strategies an equally important problem 
especially in the context of software agents is to find the time 
complexity of computing the equilibrium offers however such 
computational issues have so far received little attention as we will 
show this is mainly due to the fact that existing work describe in 
section has mostly focused on negotiation for divisible issues 
 
 - - - - rps c ifaamas 
and finding the equilibrium for this case is computationally easier 
than that for the case of indivisible issues our primary objective is 
therefore to answer the computational questions for the latter case 
for the types of situations that are commonly faced by agents in 
real-world contexts thus we consider negotiations in which there 
is incomplete information and time constraints incompleteness of 
information on the part of negotiators is a common feature of most 
practical negotiations also agents typically have time constraints 
in the form of both deadlines and discount factors deadlines are an 
essential element since negotiation cannot go on indefinitely rather 
it must end within a reasonable time limit likewise discount 
factors are essential since the goods may be perishable or their value 
may decline due to inflation moreover the strategic behaviour of 
agents with deadlines and discount factors differs from those 
without see for single issue bargaining without deadlines and 
 for bargaining with deadlines and discount factors in the 
context of divisible issues 
given this we consider indivisible issues and first analyze the 
strategic behaviour of agents to obtain the equilibrium strategies 
for the case where all the issues for negotiation are known a priori 
to both agents for this case we show that the problem of finding 
the equilibrium offers is np-hard even in a complete information 
setting then in order to overcome the problem of time 
complexity we present strategies that are approximately optimal but 
computationally efficient and show that they form an equilibrium we 
also analyze the relative error i e the difference between the true 
optimum and the approximate the time complexity of the 
approximate equilibrium strategies is o nm 
 where n is the 
negotiation deadline and the relative error finally we extend the 
analysis to online negotiation where different issues become 
available at different time points and the agents are uncertain about their 
valuations for these issues specifically we show that an 
approximate equilibrium exists for online negotiation and show that the 
expected difference between the optimum and the approximate is 
o 
√ 
m these approximate strategies also have polynomial time 
complexity 
in so doing our contribution lies in analyzing the computational 
complexity of the above multi-issue negotiation problem and 
finding the approximate and online equilibria no previous work has 
determined these equilibria since software agents have limited 
computational resources our results are especially relevant to such 
resource bounded agents 
the remainder of the paper is organised as follows we begin by 
giving a brief overview of single-issue negotiation in section in 
section we obtain the equilibrium for multi-issue negotiation and 
show that finding equilibrium offers is an np-hard problem we 
then present an approximate equilibrium and evaluate its 
approximation error section analyzes online multi-issue negotiation 
section discusses the related literature and section concludes 
 single-issue negotiation 
we adopt the single issue model of because this is a model 
where during negotiation the parties are allowed to make offers 
from a set of discrete offers since our focus is on indivisible issues 
 i e parties are allowed to make one of two possible offers zero 
or one our scenario fits in well with hence we use this basic 
single issue model and extend it to multiple issues before doing 
so we give an overview of this model and its equilibrium strategies 
there are two strategic agents a and b each agent has time 
constraints in the form of deadlines and discount factors the two 
agents negotiate over a single indivisible issue i this issue is a 
 pie of size and the agents want to determine who gets the pie 
there is a deadline i e a number of rounds by which negotiation 
must end let n ∈ n 
denote this deadline the agents use an 
alternating offers protocol as the one of rubinstein which 
proceeds through a series of time periods one of the agents say 
a starts negotiation in the first time period i e t by making 
an offer xi or to b agent b can either accept or reject the 
offer if it accepts negotiation ends in an agreement with a getting 
xi and b getting yi − xi otherwise negotiation proceeds to 
the next time period in which agent b makes a counter-offer this 
process of making offers continues until one of the agents either 
accepts an offer or quits negotiation resulting in a conflict thus 
there are three possible actions an agent can take during any time 
period accept the last offer make a new counter-offer or quit the 
negotiation 
an essential feature of negotiations involving alternating offers 
is that the agents utilities decrease with time specifically 
the decrease occurs at each step of offer and counteroffer this 
decrease is represented with a discount factor denoted δi ≤ 
for both 
agents 
let xt 
i yt 
i denote the offer made at time period t where xt 
i and 
yt 
i denote the share for agent a and b respectively then for a given 
pie the set of possible offers is 
{ xt 
i yt 
i xt 
i or yt 
i or and xt 
i yt 
i } 
at time t if a and b receive a share of xt 
i and yt 
i respectively then 
their utilities are 
ua 
i xt 
i t 
j 
xt 
i × δt− 
if t ≤ n 
 otherwise 
ub 
i yt 
i t 
j 
yt 
i × δt− 
if t ≤ n 
 otherwise 
the conflict utility i e the utility received in the event that no deal 
is struck is zero for both agents 
for the above setting the agents reason as follows in order to 
determine what to offer at t we let a b denote a s 
 b s equilibrium offer for the first time period let agent a denote 
the first mover i e at t a proposes to b who should get the 
pie to begin consider the case where the deadline for both agents 
is n if b accepts the division occurs as agreed if not neither 
agent gets anything since n is the deadline here a is in a 
powerful position and is able to propose to keep percent of the 
pie and give nothing to b 
 since the deadline is n b accepts 
this offer and agreement takes place in the first time period 
now consider the case where the deadline is n in order to 
decide what to offer in the first round a looks ahead to t and 
reasons backwards agent a reasons that if negotiation proceeds 
to the second round b will take percent of the pie by offering 
 and leave nothing for a thus in the first time period if a 
offers b anything less than the whole pie b will reject the offer 
hence during the first time period agent a offers agent b 
accepts this and an agreement occurs in the first time period 
in general if the deadline is n negotiation proceeds as follows 
as before agent a decides what to offer in the first round by 
looking ahead as far as t n and then reasoning backwards agent a s 
 
having a different discount factor for different agents only makes 
the presentation more involved without leading to any changes in 
the analysis of the strategic behaviour of the agents or the time 
complexity of finding the equilibrium offers hence we have a single 
discount factor for both agents 
 
it is possible that b may reject such a proposal however 
irrespective of whether b accepts or rejects the proposal it gets zero utility 
 because the deadline is n thus we assume that b accepts 
a s offer 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
offer for t depends on who the offering agent is for the last 
time period this in turn depends on whether n is odd or even 
since a makes an offer at t and the agents use the alternating 
offers protocol the offering agent for the last time period is b if n 
is even and it is a if n is odd thus depending on whether n is odd 
or even a makes the following offer at t 
a 
j 
offer if odd n 
accept if b s turn 
b 
j 
offer if even n 
accept if a s turn 
agent b accepts this offer and negotiation ends in the first time 
period note that the equilibrium outcome depends on who makes 
the first move since we have two agents and either of them could 
move first we get two possible equilibrium outcomes 
on the basis of the above equilibrium for single-issue 
negotiation with complete information we first obtain the equilibrium for 
multiple issues and then show that computing these offers is a hard 
problem we then present a time efficient approximate equilibrium 
 multi-issue negotiation 
we first analyse the complete information setting this section 
forms the base which we extend to the case of information 
uncertainty in section 
here a and b negotiate over m indivisible issues these 
issues are m distinct pies and the agents want to determine how 
to distribute the pies between themselves let s { m} 
denote the set of m pies as before each pie is of size let the 
discount factor for issue c where ≤ c ≤ m be δc ≤ 
for each issue let n denote each agent s deadline in the offer for 
time period t where ≤ t ≤ n agent a s b s share for each of 
the m issues is now represented as an m element vector xt 
∈ bm 
 yt 
∈ bm 
 where b denotes the set { } thus if agent a s share 
for issue c at time t is xt 
c then agent b s share is yt 
c −xt 
c the 
shares for a and b are together represented as the package xt 
 yt 
 
as is traditional in multi-issue utility theory we define an agent s 
cumulative utility using the standard additive form the 
functions ua 
 bm 
× bm 
× n 
→ r and ub 
 bm 
× bm 
× n 
→ r 
give the cumulative utilities for a and b respectively at time t these 
are defined as follows 
ua 
 xt 
 yt 
 t 
 
σm 
c ka 
c ua 
c xt 
c t if t ≤ n 
 otherwise 
 
ub 
 xt 
 yt 
 t 
 
σm 
c kb 
cub 
c yt 
c t if t ≤ n 
 otherwise 
 
where ka 
∈ nm 
 denotes an m element vector of constants for 
agent a and kb 
∈ nm 
 that for b here n denotes the set of 
positive integers these vectors indicate how the agents value different 
issues for example if ka 
c ka 
c then agent a values issue c 
more than issue c likewise for agent b in other words the m 
issues are perfect substitutes i e all that matters to an agent is its 
total utility for all the m issues and not that for any subset of them 
in all the settings we study the issues will be perfect substitutes 
to begin each agent has complete information about all negotiation 
parameters i e n m ka 
c kb 
c and δc for ≤ c ≤ m 
now multi-issue negotiation can be done using different 
procedures broadly speaking there are three key procedures for 
negotiating multiple issues 
 the package deal procedure where all the issues are settled 
together as a bundle 
 the sequential procedure where the issues are discussed one 
after another and 
 the simultaneous procedure where the issues are discussed in 
parallel 
between these three procedures the package deal is known to 
generate pareto optimal outcomes hence we adopt it here we 
first give a brief description of the procedure and then determine 
the equilibrium strategies for it 
 the package deal procedure 
in this procedure the agents use the same protocol as for 
singleissue negotiation described in section however an offer for the 
package deal includes a proposal for each issue under negotiation 
thus for m issues an offer includes m divisions one for each 
issue agents are allowed to either accept a complete offer i e all 
m issues or reject a complete offer an agreement can therefore 
take place either on all m issues or on none of them 
as per the single-issue negotiation an agent decides what to 
offer by looking ahead and reasoning backwards however since an 
offer for the package deal includes a share for all the m issues the 
agents can now make tradeoffs across the issues in order to 
maximise their cumulative utilities 
for ≤ c ≤ m the equilibrium offer for issue c at time t is 
denoted as at 
c bt 
c where at 
c and bt 
c denote the shares for agent a 
and b respectively we denote the equilibrium package at time t 
as at 
 bt 
 where at 
∈ bm 
 bt 
∈ bm 
 is an m element vector 
that denotes a s b s share for each of the m issues also for 
 ≤ c ≤ m δc is the discount factor for issue c the symbols 
and denote m element vectors of zeroes and ones respectively 
note that for ≤ t ≤ n at 
c bt 
c i e the sum of the agents 
shares at time t for each pie is one finally for time period t for 
 ≤ t ≤ n we let a t respectively b t denote the equilibrium 
strategy for agent a respectively b 
 equilibrium strategies 
as mentioned in section the package deal allows agents to make 
tradeoffs we let tradeoffa tradeoffb denote agent a s b s 
function for making tradeoffs we let p denote a set of parameters 
to the procedure tradeoffa tradeoffb where p {ka 
 kb 
 δ m} 
given this the following theorem characterises the equilibrium for 
the package deal procedure 
theorem for the package deal procedure the following 
strategies form a nash equilibrium the equilibrium strategy for 
t n is 
a n 
j 
offer if a s turn 
accept if b s turn 
b n 
j 
offer if b s turn 
accept if a s turn 
for all preceding time periods t n if xt 
 yt 
 denotes the 
offer made at time t then the equilibrium strategies are defined as 
follows 
a t 
 
 
 
offer tradeoffa p ub t t if a s turn 
if ua 
 xt 
 yt 
 t ≥ ua t accept 
else reject if b s turn 
b t 
 
 
 
offer tradeoffb p ua t t if b s turn 
if ub 
 xt 
 yt 
 t ≥ ub t accept 
else reject if a s turn 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
where ua t ua 
 at 
 bt 
 t and ub t ub 
 at 
 
bt 
 t 
proof we look ahead to the last time period i e t n and 
then reason backwards to begin if negotiation reaches the 
deadline n then the agent whose turn it is takes everything and leaves 
nothing for its opponent hence we get the strategies a n and 
b n as given in the statement of the theorem 
in all the preceding time periods t n the offering agent 
proposes a package that gives its opponent a cumulative utility equal 
to what the opponent would get from its own equilibrium offer for 
the next time period during time period t either a or b could 
be the offering agent consider the case where a makes an offer 
at t the package that a offers at t gives b a cumulative utility 
of ub 
 at 
 bt 
 t however since there is more than one 
issue there is more than one package that gives b this cumulative 
utility from among these packages a offers the one that maximises 
its own cumulative utility because it is a utility maximiser thus 
the problem for a is to find the package at 
 bt 
 so as to 
maximize 
mx 
c 
ka 
c − bt 
c δt− 
c 
such that 
mx 
c 
bt 
ckb 
c ≥ ub t 
bt 
c or for ≤ c ≤ m 
where ub t δt− 
c ka 
c and kb 
c are constants and bt 
c ≤ c ≤ m 
is a variable 
assume that the function tradeoffa takes parameters p ub t 
and t to solve the maximisation problem given in equation and 
returns the corresponding package if there is more than one 
package that solves equation then tradeoffa returns any one of 
them because agent a gets equal utility from all such packages 
and so does agent b the function tradeoffb for agent b is 
analogous to that for a 
on the other hand the equilibrium strategy for the agent that 
receives an offer is as follows for time period t let b denote the 
receiving agent then b accepts xt 
 yt 
 if ub t ≤ ub 
 xt 
 yt 
 t 
otherwise it rejects the offer because it can get a higher utility in 
the next time period the equilibrium strategy for a as receiving 
agent is defined analogously 
in this way we reason backwards and obtain the offers for the 
first time period thus we get the equilibrium strategies a t and 
b t given in the statement of the theorem 
the following example illustrates how the agents make tradeoffs 
using the above equilibrium strategies 
example assume there are m issues for negotiation 
the deadline for both issues is n and the discount factor for 
both issues for both agents is δ let ka 
 ka 
 
kb 
 and kb 
 let agent a be the first mover by 
using backward reasoning a knows that if negotiation reaches the 
second time period which is the deadline then b will get a 
hundred percent of both the issues this gives b a cumulative utility of 
ub thus in the first time period if b gets 
anything less than a utility of it will reject a s offer so at t 
a offers the package where it gets issue and b gets issue this 
gives a cumulative utility of to a and to b agent b accepts the 
package and an agreement takes place in the first time period 
the maximization problem in equation can be viewed as the - 
knapsack problem 
 in the - knapsack problem we have a set 
 
note that for the case of divisible issues this is the fractional 
knapof m items where each item has a profit and a weight there is a 
knapsack with a given capacity the objective is to fill the knapsack 
with items so as to maximize the cumulative profit of the items in 
the knapsack this problem is analogous to the negotiation problem 
we want to solve i e the maximization problem of equation 
since ka 
c and δt− 
c are constants maximizing 
pm 
c ka 
c −bt 
c δt− 
c 
is the same as minimizing 
pm 
c ka 
c bt 
c hence equation can be 
written as 
minimize 
mx 
c 
ka 
c bt 
c 
such that 
mx 
c 
bt 
ckb 
c ≥ ub t 
bt 
c or for ≤ c ≤ m 
equation is a minimization version of the standard - knapsack 
problem 
with m items where ka 
c represents the profit for item c 
kb 
c the weight for item c and ub t the knapsack capacity 
example was for two issues and so it was easy to find the 
equilibrium offers but in general it is not computationally easy to 
find the equilibrium offers of theorem the following theorem 
proves this 
theorem for the package deal procedure the problem of 
finding the equilibrium offers given in theorem is np-hard 
proof finding the equilibrium offers given in theorem 
requires solving the - knapsack problem given in equation since 
the - knapsack problem is np-hard the problem of finding 
equilibrium for the package deal is also np-hard 
 approximate equilibrium 
researchers in the area of algorithms have found time efficient 
methods for computing approximate solutions to - knapsack 
problems hence we use these methods to find a solution to our 
negotiation problem at this stage we would like to point out the 
main difference between solving the - knapsack problem and 
solving our negotiation problem the - knapsack problem 
involves decision making by a single agent regarding which items to 
place in the knapsack on the other hand our negotiation problem 
involves two players and they are both strategic hence in our case 
it is not enough to just find an approximate solution to the knapsack 
problem we must also show that such an approximation forms an 
equilibrium 
the traditional approach for overcoming the computational 
complexity in finding an equilibrium has been to use an approximate 
equilibrium see for example in this approach a strategy 
profile is said to form an approximate nash equilibrium if neither 
agent can gain more than the constant by deviating hence our 
aim is to use the solution to the - knapsack problem proposed 
in and show that it forms an approximate equilibrium to our 
negotiation problem before doing so we give a brief overview of 
the key ideas that underlie approximation algorithms 
there are two key issues in the design of approximate algorithms 
 
sack problem the factional knapsack problem is computationally 
easy it can be solved in time polynomial in the number of items in 
the knapsack problem in contrast the - knapsack problem 
is computationally hard 
 
note that for the standard - knapsack problem the weights 
profits and the capacity are positive integers however a - knapsack 
problem with fractions and non positive values can easily be 
transformed to one with positive integers in time linear in m using the 
methods given in 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 the quality of their solution and 
 the time taken to compute the approximation 
the quality of an approximate algorithm is determined by 
comparing its performance to that of the optimal algorithm and measuring 
the relative error the relative error is defined as z−z∗ 
 z∗ 
where z is the approximate solution and z∗ 
the optimal one in 
general we are interested in finding approximate algorithms whose 
relative error is bounded from above by a certain constant i e 
 z − z∗ 
 z∗ 
≤ 
regarding the second issue of time complexity we are interested in 
finding fully polynomial approximation algorithms an 
approximation algorithm is said to be fully polynomial if for any it finds 
a solution satisfying equation in time polynomially bounded by 
size of the problem for the - knapsack problem the problem size 
is equal to the number of items and by 
for the - knapsack problem ibarra and kim presented a 
fully polynomial approximation method this method is based on 
dynamic programming it is a parametric method that takes as a 
parameter and for any finds a heuristic solution z with 
relative error at most such that the time and space complexity grow 
polynomially with the number of items m and more 
specifically the space and time complexity are both o m 
 and hence 
polynomial in m and see for the detailed approximation 
algorithm and proof of time and space complexity 
since the ibarra and kim method is fully polynomial we use it to 
solve our negotiation problem this is done as follows for agent 
a let aprx-tradeoffa p ub t t denote a procedure that 
returns an approximate solution to equation using the ibarra and 
kim method the procedure aprx-tradeoffb p ua t t for 
agent b is analogous 
for ≤ c ≤ m the approximate equilibrium offer for issue c 
at time t is denoted as ¯at 
c ¯bt 
c where ¯at 
c and ¯bt 
c denote the shares 
for agent a and b respectively we denote the equilibrium package 
at time t as ¯at 
 ¯bt 
 where ¯at 
∈ bm 
 ¯bt 
∈ bm 
 is an m element 
vector that denotes a s b s share for each of the m issues also 
as before for ≤ c ≤ m δc is the discount factor for issue c 
note that for ≤ t ≤ n ¯at 
c ¯bt 
c i e the sum of the agents 
shares at time t for each pie is one finally for time period t for 
 ≤ t ≤ n we let ¯a t respectively ¯b t denote the approximate 
equilibrium strategy for agent a respectively b the following 
theorem uses this notation and characterizes an approximate 
equilibrium for multi-issue negotiation 
theorem for the package deal procedure the following 
strategies form an approximate nash equilibrium the 
equilibrium strategy for t n is 
¯a n 
j 
offer if a s turn 
accept if b s turn 
¯b n 
j 
offer if b s turn 
accept if a s turn 
for all preceding time periods t n if xt 
 yt 
 denotes the 
offer made at time t then the equilibrium strategies are defined as 
follows 
¯a t 
 
 
 
offer aprx-tradeoffa p ub t t if a s turn 
if ua 
 xt 
 yt 
 t ≥ ua t accept 
else reject if b s turn 
¯b t 
 
 
 
offer aprx-tradeoffb p ua t t if b s turn 
if ub 
 xt 
 yt 
 t ≥ ub t accept 
else reject if a s turn 
where ua t ua 
 ¯at 
 ¯bt 
 t and ub t ub 
 ¯at 
 
¯bt 
 t an agreement takes place at t 
proof as in the proof for theorem we use backward 
reasoning we first obtain the strategies for the last time period t n 
it is straightforward to get these strategies the offering agent gets 
a hundred percent of all the issues 
then for t n − the offering agent must solve the 
maximization problem of equation by substituting t n− in it for agent 
a b this is done by approx-tradeoffa approx-tradeoffb 
these two functions are nothing but the ibarra and kim s 
approximation method for solving the - knapsack problem these two 
functions take as a parameter and use the ibarra and kim s 
approximation method to return a package that approximately 
maximizes equation thus the relative error for these two functions 
is the same as that for ibarra and kim s method i e it is at most 
where is given in equation 
assume that a is the offering agent for t n − agent a must 
offer a package that gives b a cumulative utility equal to what it 
would get from its own approximate equilibrium offer for the next 
time period i e ub 
 ¯at 
 ¯bt 
 t where ¯at 
 ¯bt 
 is the 
approximate equilibrium package for the next time period recall 
that for the last time period the offering agent gets a hundred 
percent of all the issues since a is the offering agent for t n − 
and the agents use the alternating offers protocol it is b s turn at 
t n thus ub 
 ¯at 
 ¯bt 
 t is equal to b s cumulative 
utility from receiving a hundred percent of all the issues using this 
utility as the capacity of the knapsack a uses approx-tradeoffa 
and obtains the approximate equilibrium package for t n − 
on the other hand if b is the offering agent at t n − it uses 
approx-tradeoffb to obtain the approximate equilibrium 
package 
in the same way for t n − the offering agent say a uses 
approx-tradeoffa to find an approximate equilibrium package 
that gives b a utility of ub 
 ¯at 
 ¯bt 
 t by reasoning 
backwards we obtain the offer for time period t if a b is the 
offering agent it proposes the offer approx-tradeoffa p ub 
 approx-tradeoffb p ua the receiving agent 
accepts the offer this is because the relative error in its cumulative 
utility from the offer is at most an agreement therefore takes 
place in the first time period 
theorem the time complexity of finding the approximate 
equilibrium offer for the first time period is o nm 
 
proof the time complexity of approx-tradeoffa and 
approxtradeoffb is the same as the time complexity of the ibarra and 
kim method i e o m 
 in order to find the equilibrium 
offer for the first time period using backward reasoning 
approxtradeoffa or approx- tradeoffb is invoked n times hence 
the time complexity of finding the approximate equilibrium offer 
for the first time period is o nm 
 
this analysis was done in a complete information setting 
however an extension of this analysis to an incomplete information 
setting where the agents have probability distributions over some 
uncertain parameter is straightforward as long as the negotiation is 
done offline i e the agents know their preference for each 
individual issue before negotiation begins for instance consider the case 
where different agents have different discount factors and each 
agent is uncertain about its opponent s discount factor although it 
knows its own this uncertainty is modelled with a probability 
distribution over the possible values for the opponent s discount factor 
and having this distribution as common knowledge to the agents 
all our analysis for the complete information setting still holds for 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
this incomplete information setting except for the fact that an agent 
must now use the given probability distribution and find its 
opponent s expected utility instead of its actual utility hence instead of 
analyzing an incomplete information setting for offline negotiation 
we focus on online multi-issue negotiation 
 online multi-issue negotiation 
we now consider a more general and arguably more realistic 
version of multi-issue negotiation where the agents are uncertain about 
the issues they will have to negotiate about in future in this setting 
when negotiating an issue the agents know that they will negotiate 
more issues in the future but they are uncertain about the details of 
those issues as before let m be the total number of issues that are 
up for negotiation the agents have a probability distribution over 
the possible values of ka 
c and kb 
c for ≤ c ≤ m let ka 
c and kb 
c be 
uniformly distributed over this probability distribution n 
and m are common knowledge to the agents however the agents 
come to know ka 
c and kb 
c only just before negotiation for issue c 
begins once the agents reach an agreement on issue c it cannot be 
re-negotiated 
this scenario requires online negotiation since the agents must 
make decisions about an issue prior to having the information about 
the future issues we first give a brief introduction to online 
problems and then draw an analogy between the online knapsack 
problem and the negotiation problem we want to solve 
in an online problem data is given to the algorithm 
incrementally one unit at a time the online algorithm must also 
produce the output incrementally after seeing i units of input it must 
output the ith unit of output since decisions about the output are 
made with incomplete knowledge about the entire input an 
online algorithm often cannot produce an optimal solution such an 
algorithm can only approximate the performance of the optimal 
algorithm that sees all the inputs in advance in the design of online 
algorithms the main aim is to achieve a performance that is close 
to that of the optimal offline algorithm on each input an online 
algorithm is said to be stochastic if it makes decisions on the basis of 
the probability distributions for the future inputs the performance 
of stochastic online algorithms is assessed in terms of the expected 
difference between the optimum and the approximate solution 
 denoted e z∗ 
m −zm where z∗ 
m is the optimal and zm the approximate 
solution note that the subscript m is used to indicate the fact that 
this difference depends on m 
we now describe the protocol for online negotiation and then 
obtain an approximate equilibrium the protocol is defined as 
follows let agent a denote the first mover since we focus on the 
package deal procedure the first mover is the same for all the m 
issues 
step for c the agents are given the values of ka 
c and kb 
c 
these two values are now common 
knowledge 
step the agents settle issue c using the alternating offers 
protocol described in section negotiation for issue c must end 
within n time periods from the start of negotiation on the 
issue if an agreement is not reached within this time then 
negotiation fails on this and on all remaining issues 
step the above steps are repeated for issues c m 
negotiation for issue c ≤ c ≤ m begins in the time 
period following an agreement on issue c − 
 
we assume common knowledge because it simplifies exposition 
however if ka 
c kb 
c is a s b s private knowledge then our analysis 
will still hold but now an agent must find its opponent s expected 
utility on the basis of the p d fs for ka 
c and kb 
c 
thus during time period t the problem for the offering agent say 
a is to find the optimal offer for issue c on the basis of ka 
c and 
kb 
c and the probability distribution for ka 
i and kb 
i c i ≤ m 
in order to solve this online negotiation problem we draw analogy 
with the online knapsack problem before doing so however we 
give a brief overview of the online knapsack problem 
in the online knapsack problem there are m items the agent 
must examine the m items one at a time according to the order they 
are input i e as their profit and size coefficients become known 
hence the algorithm is required to decide whether or not to 
include each item in the knapsack as soon as its weight and profit 
become known without knowledge concerning the items still to be 
seen except for their total number note that since the agents have 
a probability distribution over the weights and profits of the future 
items this is a case of stochastic online knapsack problem our 
online negotiation problem is analogous to the online knapsack 
problem this analogy is described in detail in the proof for theorem 
again researchers in algorithms have developed time efficient 
approximate solutions to the online knapsack problem hence 
we use this solution and show that it forms an equilibrium 
the following theorem characterizes an approximate equilibrium 
for online negotiation here the agents have to choose a 
strategy without knowing the features of the future issues because of 
this information incompleteness the relevant equilibrium solution 
is that of a bayes nash equilibrium bne in which each agent 
plays the best response to the other agents with respect to their 
expected utilities however finding an agent s bne strategy is 
analogous to solving the online - knapsack problem also the 
online knapsack can only be solved approximately hence 
the relevant equilibrium solution concept is approximate bne see 
 for example the following theorem finds this equilibrium 
using procedures online- tradeoffa and online-tradeoffb 
which are defined in the proof of the theorem for a given time 
period we let zm denote the approximately optimal solution 
generated by online-tradeoffa or online-tradeoffb and z∗ 
m 
the actual optimum 
theorem for the package deal procedure the following 
strategies form an approximate bayes nash equilibrium the 
equilibrium strategy for t n is 
a n 
j 
offer if a s turn 
accept if b s turn 
b n 
j 
offer if b s turn 
accept if a s turn 
for all preceding time periods t n if xt 
 yt 
 denotes the 
offer made at time t then the equilibrium strategies are defined as 
follows 
a t 
 
 
 
offer online-tradeoffa p ub t t if a s turn 
if ua 
 xt 
 yt 
 t ≥ ua t accept 
else reject if b s turn 
b t 
 
 
 
offer online-tradeoffb p ua t t if b s turn 
if ub 
 xt 
 yt 
 t ≥ ub t accept 
else reject if a s turn 
where ua t ua 
 ¯at 
 ¯bt 
 t and ub t ub 
 ¯at 
 
¯bt 
 t an agreement on issue c takes place at t c for a 
given time period the expected difference between the solution 
generated by the optimal strategy and that by the approximate strategy 
is e z∗ 
m − zm o 
√ 
m 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
proof as in theorem we find the equilibrium offer for time 
period t using backward induction let a be the offering agent 
for t for all the m issues consider the last time period t n 
 recall from step of the online protocol that n is the deadline for 
completing negotiation on the first issue since the first mover is 
the same for all the issues and the agents make offers alternately 
the offering agent for t n is also the same for all the m issues 
assume that b is the offering agent for t n as in section 
the offering agent for t n gets a hundred percent of all the m 
issues since b is the offering agent for t n his utility for this 
time period is 
ub n kb 
 δn− 
 
mx 
i 
δ 
i n− 
i 
recall that ka 
i and kb 
i for c i ≤ m are not known to the 
agents hence the agents can only find their expected utilities from 
the future issues on the basis of the probability distribution 
functions for ka 
i and kb 
i however during the negotiation for issue c 
the agents know ka 
c but not kb 
c see step of the online protocol 
hence a computes ub n as follows agent b s utility from issue 
c is kb 
 δn− 
 which is the first term of equation then 
on the basis of the probability distribution functions for ka 
i and 
kb 
i agent a computes b s expected utility from each future issue i 
as δ 
i n− 
i since ka 
i and kb 
i are uniformly distributed on 
thus b s expected cumulative utility from these m − c issues is 
 
pm 
i δ 
i n− 
i which is the second term of equation 
now in order to decide what to offer for issue c the offering 
agent for t n − i e agent a must solve the following online 
knapsack problem 
maximize σm 
i ka 
i − ¯bt 
i δn− 
i 
such that σm 
i kb 
i 
¯bt 
i ≥ ub n 
¯bt 
i or for ≤ i ≤ m 
the only variables in the above maximization problem are ¯bt 
i now 
maximizing σm 
i ka 
i −¯bt 
i δn− 
i is the same as minimizing σm 
i ka 
i 
¯bt 
i 
since δn− 
i and ka 
i are constants thus we write equation as 
minimize σm 
i ka 
i 
¯bt 
i 
such that σm 
i kb 
i 
¯bt 
i ≥ ub n 
¯bt 
i or for ≤ i ≤ m 
the above optimization problem is analogous to the online - 
knapsack problem an algorithm to solve the online knapsack 
problem has already proposed in this algorithm is called the 
fixed-choice online algorithm it has time complexity linear in the 
number of items m in the knapsack problem we use this to solve 
our online negotiation problem thus our online-tradeoffa 
algorithm is nothing but the fixed-choice online algorithm and 
therefore has the same time complexity as the latter this algorithm takes 
the values of ka 
i and kb 
i one at a time and generates an approximate 
solution to the above knapsack problem the expected difference 
between the optimum and approximate solution is e z∗ 
m − zm 
o 
√ 
m see for the detailed fixed-choice online 
algorithm and a proof for e z∗ 
m − zm o 
√ 
m 
the fixed-choice online algorithm of is a generalization of 
the basic greedy algorithm for the offline knapsack problem the 
idea behind it is as follows a threshold value is determined on the 
basis of the information regarding weights and profits for the - 
knapsack problem the method then includes into the knapsack all 
items whose profit density profit density of an item is its profit per 
unit weight exceeds the threshold until either the knapsack is filled 
or all the m items have been considered 
in more detail the algorithm online-tradeoffa works as 
follows it first gets the values of ka 
 and kb 
 and finds ¯bt 
c since we 
have a - knapsack problem ¯bt 
c can be either zero or one now if 
¯bt 
c for t n then ¯bt 
c must be one for ≤ t n i e a must 
offer ¯bt 
c at t if ¯bt 
c for t n but a offers ¯bt 
c 
at t then agent b gets less utility than what it expects from a s 
offer and rejects the proposal thus if ¯bt 
c for t n then the 
optimal strategy for a is to offer ¯bt 
c at t agent b accepts 
the offer thus negotiation on the first issue starts at t and an 
agreement on it is also reached at t 
in the next time period i e t negotiation proceeds to the 
next issue the deadline for the second issue is n time periods from 
the start of negotiation on the issue for c the algorithm 
online-tradeoffa is given the values of ka 
 and kb 
 and finds ¯bt 
c 
as described above agent offers bc at t and b accepts thus 
negotiation on the second issue starts at t and an agreement 
on it is also reached at t 
this process repeats for the remaining issues c m 
thus each issue is agreed upon in the same time period in which 
it starts as negotiation for the next issue starts in the following 
time period see step of the online protocol agreement on issue 
i occurs at time t i 
on the other hand if b is the offering agent at t he uses 
the algorithm online-tradeoffb which is defined analogously 
thus irrespective of who makes the first move all the m issues are 
settled at time t m 
theorem the time complexity of finding the approximate 
equilibrium offers of theorem is linear in m 
proof the time complexity of online-tradeoffa and 
onlinetradeoffb is the same as the time complexity of the fixed-choice 
online algorithm of since the latter has time complexity linear 
in m the time complexity of online-tradeoffa and 
onlinetradeoffb is also linear in m 
it is worth noting that for the - knapsack problem the lower 
bound on the expected difference between the optimum and the 
solution found by any online algorithm is ω thus it follows 
that this lower bound also holds for our negotiation problem 
 related work 
work on multi-issue negotiation can be divided into two main types 
that for indivisible issues and that for divisible issues we first 
describe the existing work for the case of divisible issues since 
schelling first noted that the outcome of negotiation depends 
on the choice of negotiation procedure much research effort has 
been devoted to the study of different procedures for negotiating 
multiple issues however most of this work has focussed on the 
sequential procedure for this procedure a key issue is the 
negotiation agenda here the term agenda refers to the order in which 
the issues are negotiated the agenda is important because each 
agent s cumulative utility depends on the agenda if we change the 
agenda then these utilities change hence the agents must decide 
what agenda they will use now the agenda can be decided before 
negotiating the issues such an agenda is called exogenous or it 
may be decided during the process of negotiation such an agenda 
is called endogenous for instance fershtman analyze 
sequential negotiation with exogenous agenda a number of researchers 
have also studied negotiations with an endogenous agenda 
in contrast to the above work that mainly deals with sequential 
negotiation studies the equilibrium for the package deal 
procedure however all the above mentioned work differs from ours in 
that we focus on indivisible issues while others focus on the case 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
where each issue is divisible specifically no previous work has 
determined an approximate equilibrium for multi-issue negotiation 
or for online negotiation 
existing work for the case of indivisible issues has mostly dealt 
with task allocation problems for tasks that cannot be partioned 
to a group of agents the problem of task allocation has been 
previously studied in the context of coalitions involving more than 
two agents for example analyze the problem for the case 
where the agents act so as to maximize the benefit of the system 
as a whole in contrast our focus is on two agents where both of 
them are self-interested and want to maximize their individual 
utilities on the other hand focus on the use of contracts for task 
allocation to multiple self interested agents but this work concerns 
finding ways of decommitting contracts after the initial allocation 
has been done so as to improve an agent s utility in contrast our 
focuses on negotiation regarding who will carry out which task 
finally online and approximate mechanisms have been studied 
in the context of auctions but not for bilateral negotiations 
 which is the focus of our work 
 conclusions 
this paper has studied bilateral multi-issue negotiation between 
self-interested autonomous agents with time constraints the issues 
are indivisible and different agents value different issues 
differently thus the problem is for the agents to decide how to allocate 
the issues between themselves so as to maximize their individual 
utilities specifically we first showed that finding the equilibrium 
offers is an np-hard problem even in a complete information 
setting we then presented approximately optimal negotiation 
strategies and showed that they form an equilibrium these strategies 
have polynomial time complexity we also analysed the difference 
between the true optimum and the approximate optimum finally 
we extended the analysis to online negotiation where the issues 
become available at different time points and the agents are uncertain 
about the features of these issues specifically we showed that an 
approximate equilibrium exists for online negotiation and analysed 
the approximation error these approximate strategies also have 
polynomial time complexity 
there are several interesting directions for future work first 
for online negotiation we assumed that the constants ka 
c and kb 
c are 
both uniformly distributed it will be interesting to analyze the case 
where ka 
c and kb 
c have other possibly different probability 
distributions apart from this we treated the number of issues as being 
common knowledge to the agents in future it will be interesting 
to treat the number of issues as uncertain 
 references 
 g ausiello p crescenzi g gambosi v kann 
a marchetti-spaccamela and m protasi complexity and 
approximation combinatorial optimization problems and 
their approximability properties springer 
 m bac and h raff issue-by-issue negotiations the role of 
information and time preference games and economic 
behavior - 
 a borodin and r el-yaniv online computation and 
competitive analysis cambridge university press 
 s j brams fair division from cake cutting to dispute 
resolution cambridge university press 
 l a busch and i j horstman bargaining frictions 
bargaining procedures and implied costs in multiple-issue 
bargaining economica - 
 s s fatima m wooldridge and n r jennings 
multi-issue negotiation with deadlines journal of artificial 
intelligence research - 
 c fershtman the importance of the agenda in bargaining 
games and economic behavior - 
 f glover a multiphase dual algorithm for the zero-one 
integer programming problem operations research 
 - 
 m t hajiaghayi r kleinberg and d c parkes adaptive 
limited-supply online auctions in acm conference on 
electronic commerce acmec- pages - new 
york 
 o h ibarra and c e kim fast approximation algorithms 
for the knapsack and sum of subset problems journal of 
acm - 
 r inderst multi-issue bargaining with endogenous agenda 
games and economic behavior - 
 r keeney and h raiffa decisions with multiple 
objectives preferences and value trade-offs new york 
john wiley 
 s kraus strategic negotiation in multi-agent environments 
the mit press cambridge massachusetts 
 d lehman l i o callaghan and y shoham truth 
revelation in approximately efficient combinatorial auctions 
journal of the acm - 
 a lomuscio m wooldridge and n r jennings a 
classification scheme for negotiation in electronic commerce 
international journal of group decision and negotiation 
 - 
 a marchetti-spaccamela and c vercellis stochastic online 
knapsack problems mathematical programming 
 - 
 s martello and p toth knapsack problems algorithms and 
computer implementations john wiley and sons 
 m j osborne and a rubinstein a course in game theory 
the mit press 
 h raiffa the art and science of negotiation harvard 
university press cambridge usa 
 j s rosenschein and g zlotkin rules of encounter mit 
press 
 a rubinstein perfect equilibrium in a bargaining model 
econometrica - january 
 t sandholm and v lesser levelled commitment contracts 
and strategic breach games and economic behavior 
special issue on ai and economics - 
 t sandholm and n vulkan bargaining with deadlines in 
aaai- pages - orlando fl 
 t c schelling an essay on bargaining american economic 
review - 
 o shehory and s kraus methods for task allocation via 
agent coalition formation artificial intelligence journal 
 - - 
 s singh v soni and m wellman computing approximate 
bayes nash equilibria in tree games of incomplete 
information in proceedings of the acm conference on 
electronic commerce acm-ec pages - new york 
may 
 i stahl bargaining theory economics research institute 
stockholm school of economics stockholm 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
distributed agent-based air traffic flow management 
kagan tumer 
oregon state university 
 rogers hall 
corvallis or usa 
kagan tumer oregonstate edu 
adrian agogino 
ucsc nasa ames research center 
mailstop - 
moffett field ca usa 
adrian email arc nasa gov 
abstract 
air traffic flow management is one of the fundamental 
challenges facing the federal aviation administration faa 
today the faa estimates that in alone there were 
over hours of delays at a cost to the industry in 
excess of three billion dollars finding reliable and adaptive 
solutions to the flow management problem is of paramount 
importance if the next generation air transportation 
systems are to achieve the stated goal of accommodating three 
times the current traffic volume this problem is 
particularly complex as it requires the integration and or 
coordination of many factors including new data e g changing 
weather info potentially conflicting priorities e g 
different airlines limited resources e g air traffic controllers 
and very heavy traffic volume e g over flights over 
the us airspace 
in this paper we use facet - an air traffic flow simulator 
developed at nasa and used extensively by the faa and 
industry - to test a multi-agent algorithm for traffic flow 
management an agent is associated with a fix a specific 
location in d space and its action consists of setting the 
separation required among the airplanes going though that 
fix agents use reinforcement learning to set this separation 
and their actions speed up or slow down traffic to manage 
congestion our facet based results show that agents 
receiving personalized rewards reduce congestion by up to 
over agents receiving a global reward and by up to over 
a current industry approach monte carlo estimation 
categories and subject descriptors 
i computing methodologies artificial 
intelligencemultiagent systems 
general terms 
algorithms performance 
 introduction 
the efficient safe and reliable management of our ever 
increasing air traffic is one of the fundamental challenges 
facing the aerospace industry today on a typical day more 
than commercial flights operate within the us airspace 
 in order to efficiently and safely route this air traffic 
current traffic flow control relies on a centralized 
hierarchical routing strategy that performs flow projections ranging 
from one to six hours as a consequence the system is 
slow to respond to developing weather or airport conditions 
leading potentially minor local delays to cascade into large 
regional congestions in weather routing decisions 
and airport conditions caused delays accounting for 
 hours of delays the total cost of these delays was 
estimated to exceed three billion dollars by industry 
furthermore as the traffic flow increases the current 
procedures increase the load on the system the airports and 
the air traffic controllers more aircraft per region 
without providing any of them with means to shape the traffic 
patterns beyond minor reroutes the next generation air 
transportation systems ngats initiative aims to address 
this issues and not only account for a threefold increase in 
traffic but also for the increasing heterogeneity of aircraft 
and decreasing restrictions on flight paths unlike many 
other flow problems where the increasing traffic is to some 
extent absorbed by improved hardware e g more servers 
with larger memories and faster cpus for internet routing 
the air traffic domain needs to find mainly algorithmic 
solutions as the infrastructure e g number of the airports will 
not change significantly to impact the flow problem there 
is therefore a strong need to explore new distributed and 
adaptive solutions to the air flow control problem 
an adaptive multi-agent approach is an ideal fit to this 
naturally distributed problem where the complex interaction 
among the aircraft airports and traffic controllers renders a 
pre-determined centralized solution severely suboptimal at 
the first deviation from the expected plan though a truly 
distributed and adaptive solution e g free flight where 
aircraft can choose almost any path offers the most potential 
in terms of optimizing flow it also provides the most 
radical departure from the current system as a consequence a 
shift to such a system presents tremendous difficulties both 
in terms of implementation e g scheduling and airport 
capacity and political fallout e g impact on air traffic 
controllers in this paper we focus on agent based system that 
can be implemented readily in this approach we assign an 
 
 - - - - rps c ifaamas 
agent to a fix a specific location in d because aircraft 
flight plans consist of a sequence of fixes this 
representation allows localized fixes or agents to have direct impact 
on the flow of air traffic 
 in this approach the agents 
actions are to set the separation that approaching aircraft 
are required to keep this simple agent-action pair allows 
the agents to slow down or speed up local traffic and allows 
agents to a have significant impact on the overall air traffic 
flow agents learn the most appropriate separation for their 
location using a reinforcement learning rl algorithm 
in a reinforcement learning approach the selection of the 
agent reward has a large impact on the performance of the 
system in this work we explore four different agent reward 
functions and compare them to simulating various changes 
to the system and selecting the best solution e g 
equivalent to a monte-carlo search the first explored reward 
consisted of the system reward the second reward was a 
personalized agent reward based on collectives 
the last two rewards were personalized rewards based on 
estimations to lower the computational burden of the 
reward computation all three personalized rewards aim to 
align agent rewards with the system reward and ensure that 
the rewards remain sensitive to the agents actions 
previous work in this domain fell into one of two distinct 
categories the first principles based modeling approaches 
used by domain experts and the algorithmic 
approaches explored by the learning and or agents 
community though our approach comes from the second 
category we aim to bridge the gap by using facet to test 
our algorithms a simulator introduced and widely used i e 
over organizations and users by work in the first 
category 
the main contribution of this paper is to present a 
distributed adaptive air traffic flow management algorithm that 
can be readily implemented and test that algorithm using 
facet in section we describe the air traffic flow problem 
and the simulation tool facet in section we present 
the agent-based approach focusing on the selection of the 
agents and their action space along with the agents learning 
algorithms and reward structures in section we present 
results in domains with one and two congestions explore 
different trade-offs of the system objective function discuss 
the scaling properties of the different agent rewards and 
discuss the computational cost of achieving certain levels of 
performance finally in section we discuss the 
implications of these results and provide and map the required work 
to enable the faa to reach its stated goal of increasing the 
traffic volume by threefold 
 air traffic flow management 
with over flights operating within the united states 
airspace on an average day the management of traffic flow 
is a complex and demanding problem not only are there 
concerns for the efficiency of the system but also for 
fairness e g different airlines adaptability e g developing 
weather patterns reliability and safety e g airport 
management in order to address such issues the management 
of this traffic flow occurs over four hierarchical levels 
 separation assurance - minute decisions 
 
we discuss how flight plans with few fixes can be handled 
in more detail in section 
 regional flow minutes to hours 
 national flow - hours and 
 dynamic airspace configuration hours to year 
because of the strict guidelines and safety concerns 
surrounding aircraft separation we will not address that control 
level in this paper similarly because of the business and 
political impact of dynamic airspace configuration we will 
not address the outermost flow control level either instead 
we will focus on the regional and national flow management 
problems restricting our impact to decisions with time 
horizons between twenty minutes and eight hours the proposed 
algorithm will fit between long term planning by the faa 
and the very short term decisions by air traffic controllers 
the continental us airspace consists of regional centers 
 handling - flights on a given day and sectors 
 handling - flights the flow control problem has to 
address the integration of policies across these sectors and 
centers account for the complexity of the system e g over 
 public use airports and air traffic controllers 
and handle changes to the policies caused by weather 
patterns two of the fundamental problems in addressing the 
flow problem are i modeling and simulating such a large 
complex system as the fidelity required to provide reliable 
results is difficult to achieve and ii establishing the method 
by which the flow management is evaluated as directly 
minimizing the total delay may lead to inequities towards 
particular regions or commercial entities below we discuss 
how we addressed both issues namely we present facet 
a widely used simulation tool and discuss our system 
evaluation function 
figure facet screenshot displaying traffic 
routes and air flow statistics 
 facet 
facet future atm concepts evaluation tool a physics 
based model of the us airspace was developed to accurately 
model the complex air traffic flow problem it is based on 
propagating the trajectories of proposed flights forward in 
time facet can be used to either simulate and display air 
traffic a hour slice with flights takes minutes to 
simulate on a ghz gb ram computer or provide rapid 
statistics on recorded data d trajectories for flights 
including sectors airports and fix statistics in seconds 
on the same computer facet is extensively used by 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the faa nasa and industry over organizations and 
 users 
facet simulates air traffic based on flight plans and 
through a graphical user interface allows the user to analyze 
congestion patterns of different sectors and centers figure 
 facet also allows the user to change the flow patterns 
of the aircraft through a number of mechanisms including 
metering aircraft through fixes the user can then observe 
the effects of these changes to congestion in this paper 
agents use facet directly through batch mode where 
agents send scripts to facet asking it to simulate air 
traffic based on metering orders imposed by the agents the 
agents then produce their rewards based on receive feedback 
from facet about the impact of these meterings 
 system evaluation 
the system performance evaluation function we select 
focuses on delay and congestion but does not account for 
fairness impact on different commercial entities instead it 
focuses on the amount of congestion in a particular sector and 
on the amount of measured air traffic delay the linear 
combination of these two terms gives the full system evaluation 
function g z as a function of the full system state z more 
precisely we have 
g z − − α b z αc z 
where b z is the total delay penalty for all aircraft in the 
system and c z is the total congestion penalty the 
relative importance of these two penalties is determined by the 
value of α and we explore various trade-offs based on α in 
section 
the total delay b is a sum of delays over a set of sectors 
s and is given by 
b z 
x 
s∈s 
bs z 
where 
bs z 
x 
t 
θ t − τs kt s t − τs 
where ks t is the number of aircraft in sector s at a 
particular time τs is a predetermined time and θ · is the 
step function that equals when its argument is greater or 
equal to zero and has a value of zero otherwise intuitively 
bs z provides the total number of aircraft that remain in 
a sector s past a predetermined time τs and scales their 
contribution to count by the amount by which they are late 
in this manner bs z provides a delay factor that not only 
accounts for all aircraft that are late but also provides a 
scale to measure their lateness this definition is based 
on the assumption that most aircraft should have reached 
the sector by time τs and that aircraft arriving after this 
time are late in this paper the value of τs is determined by 
assessing aircraft counts in the sector in the absence of any 
intervention or any deviation from predicted paths 
similarly the total congestion penalty is a sum over the 
congestion penalties over the sectors of observation s 
c z 
x 
s∈s 
cs z 
where 
cs z a 
x 
t 
θ ks t − cs eb ks t−cs 
 
where a and b are normalizing constants and cs is the 
capacity of sector s as defined by the faa intuitively cs z 
penalizes a system state where the number of aircraft in a 
sector exceeds the faas official sector capacity each sector 
capacity is computed using various metrics which include the 
number of air traffic controllers available the exponential 
penalty is intended to provide strong feedback to return the 
number of aircraft in a sector to below the faa mandated 
capacities 
 agent based air traffic flow 
the multi agent approach to air traffic flow management 
we present is predicated on adaptive agents taking 
independent actions that maximize the system evaluation function 
discussed above to that end there are four critical 
decisions that need to be made agent selection agent action 
set selection agent learning algorithm selection and agent 
reward structure selection 
 agent selection 
selecting the aircraft as agents is perhaps the most 
obvious choice for defining an agent that selection has the 
advantage that agent actions can be intuitive e g change 
of flight plan increase or decrease speed and altitude and 
offer a high level of granularity in that each agent can have 
its own policy however there are several problems with 
that approach first there are in excess of aircraft 
in a given day leading to a massively large multi-agent 
system second as the agents would not be able to sample their 
state space sufficiently learning would be prohibitively slow 
as an alternative we assign agents to individual ground 
locations throughout the airspace called fixes each agent is 
then responsible for any aircraft going through its fix fixes 
offer many advantages as agents 
 their number can vary depending on need the 
system can have as many agents as required for a given 
situation e g agents coming live around an area 
with developing weather conditions 
 because fixes are stationary collecting data and 
matching behavior to reward is easier 
 because aircraft flight plans consist of fixes agent will 
have the ability to affect traffic flow patterns 
 they can be deployed within the current air traffic 
routing procedures and can be used as tools to help air 
traffic controllers rather than compete with or replace 
them 
figure shows a schematic of this agent based system 
agents surrounding a congestion or weather condition affect 
the flow of traffic to reduce the burden on particular regions 
 agent actions 
the second issue that needs to be addressed is 
determining the action set of the agents again an obvious choice 
may be for fixes to bid on aircraft affecting their flight 
plans though appealing from a free flight perspective that 
approach makes the flight plans too unreliable and 
significantly complicates the scheduling problem e g arrival at 
airports and the subsequent gate assignment process 
instead we set the actions of an agent to determining 
the separation distance between aircraft that aircraft have 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
to maintain when going through the agent s fix this is 
known as setting the miles in trail or mit when an agent 
sets the mit value to d aircraft going towards its fix are 
instructed to line up and keep d miles of separation though 
aircraft will always keep a safe distance from each other 
regardless of the value of d when there are many aircraft 
going through a fix the effect of issuing higher mit values 
is to slow down the rate of aircraft that go through the fix 
by increasing the value of d an agent can limit the amount 
of air traffic downstream of its fix reducing congestion at 
the expense of increasing the delays upstream 
figure schematic of agent architecture the 
agents corresponding to fixes surrounding a 
possible congestion become live and start setting new 
separation times 
 agent learning 
the objective of each agent is to learn the best values of 
d that will lead to the best system performance g in this 
paper we assume that each agent will have a reward 
function and will aim to maximize its reward using its own 
reinforcement learner though alternatives such as 
evolving neuro-controllers are also effective for complex 
delayed-reward problems relatively sophisticated 
reinforcement learning systems such as temporal difference may have 
to be used however due to our agent selection and agent 
action set the air traffic congestion domain modeled in this 
paper only needs to utilize immediate rewards as a 
consequence simple table-based immediate reward reinforcement 
learning is used our reinforcement learner is equivalent to 
an -greedy q-learner with a discount rate of at every 
episode an agent takes an action and then receives a reward 
evaluating that action after taking action a and receiving 
reward r an agent updates its q table which contains its 
estimate of the value for taking that action as follows 
q a − l q a l r 
where l is the learning rate at every time step the agent 
chooses the action with the highest table value with 
probability − and chooses a random action with probability 
 in the experiments described in this paper α is equal 
to and is equal to the parameters were chosen 
experimentally though system performance was not overly 
sensitive to these parameters 
 agent reward structure 
the final issue that needs to be addressed is selecting the 
reward structure for the learning agents the first and most 
direct approach is to let each agent receive the system 
performance as its reward however in many domains such 
a reward structure leads to slow learning we will 
therefore also set up a second set of reward structures based on 
agent-specific rewards given that agents aim to maximize 
their own rewards a critical task is to create good agent 
rewards or rewards that when pursued by the agents lead 
to good overall system performance in this work we focus 
on difference rewards which aim to provide a reward that is 
both sensitive to that agent s actions and aligned with the 
overall system reward 
 difference rewards 
consider difference rewards of the form 
di ≡ g z − g z − zi ci 
where zi is the action of agent i all the components of 
z that are affected by agent i are replaced with the fixed 
constant ci 
 
 
in many situations it is possible to use a ci that is 
equivalent to taking agent i out of the system intuitively this 
causes the second term of the difference reward to 
evaluate the performance of the system without i and therefore 
d evaluates the agent s contribution to the system 
performance there are two advantages to using d first because 
the second term removes a significant portion of the impact 
of other agents in the system it provides an agent with 
a cleaner signal than g this benefit has been dubbed 
learnability agents have an easier time learning in 
previous work second because the second term does not 
depend on the actions of agent i any action by agent i that 
improves d also improves g this term which measures the 
amount of alignment between two rewards has been dubbed 
factoredness in previous work 
 estimates of difference rewards 
though providing a good compromise between aiming 
for system performance and removing the impact of other 
agents from an agent s reward one issue that may plague d 
is computational cost because it relies on the computation 
of the counterfactual term g z − zi ci i e the system 
performance without agent i it may be difficult or 
impossible to compute particularly when the exact mathematical 
form of g is not known let us focus on g functions in the 
following form 
g z gf f z 
where gf is non-linear with a known functional form and 
f z 
x 
i 
fi zi 
where each fi is an unknown non-linear function we 
assume that we can sample values from f z enabling us to 
compute g but that we cannot sample from each fi zi 
 
this notation uses zero padding and vector addition rather 
than concatenation to form full state vectors from partial 
state vectors the vector zi in our notation would be ziei 
in standard vector notation where ei is a vector with a value 
of in the ith component and is zero everywhere else 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
in addition we assume that gf is much easier to compute 
than f z or that we may not be able to even compute 
f z directly and must sample it from a black box 
computation this form of g matches our system evaluation 
in the air traffic domain when we arrange agents so that 
each aircraft is typically only affected by a single agent each 
agent s impact of the counts of the number of aircraft in a 
sector kt s will be mostly independent of the other agents 
these values of kt s are the f z s in our formulation and 
the penalty functions form gf note that given aircraft 
counts the penalty functions gf can be easily computed 
in microseconds while aircraft counts f can only be 
computed by running facet taking on the order of seconds 
to compute our counterfactual g z − zi ci we need to 
compute 
gf f z − zi ci gf 
 
  
x 
j i 
fj zj fi ci 
 
a 
 gf f z − fi zi fi ci 
unfortunately we cannot compute this directly as the values 
of fi zi are unknown however if agents take actions 
independently it does not observe how other agents act before 
taking its own action we can take advantage of the linear 
form of f z in the fis with the following equality 
e f−i z−i zi e f−i z−i ci 
where e f−i z−i zi is the expected value of all of the fs 
other than fi given the value of zi and e f−i z−i ci is the 
expected value of all of the fs other than fi given the value 
of zi is changed to ci we can then estimate f z − zi ci 
f z − fi zi fi ci f z − fi zi fi ci 
 e f−i z−i ci − e f−i z−i zi 
 f z − e fi zi zi e fi ci ci 
 e f−i z−i ci − e f−i z−i zi 
 f z − e f z zi e f z ci 
therefore we can evaluate di g z − g z − zi ci as 
dest 
i gf f z − gf f z − e f z zi e f z ci 
leaving us with the task of estimating the values of e f z zi 
and e f z ci these estimates can be computed by 
keeping a table of averages where we average the values of the 
observed f z for each value of zi that we have seen this 
estimate should improve as the number of samples increases to 
improve our estimates we can set ci e z and if we make 
the mean squared approximation of f e z ≈ e f z then 
we can estimate g z − g z − zi ci as 
dest 
i gf f z − gf f z − e f z zi e f z 
this formulation has the advantage in that we have more 
samples at our disposal to estimate e f z than we do to 
estimate e f z ci 
 simulation results 
in this paper we test the performance of our agent based 
air traffic optimization method on a series of simulations 
using the facet air traffic simulator in all experiments 
we test the performance of five different methods the first 
method is monte carlo estimation where random policies 
are created with the best policy being chosen the other 
four methods are agent based methods where the agents are 
maximizing one of the following rewards 
 the system reward g z as define in equation 
 the difference reward di z assuming that agents 
can calculate counterfactuals 
 estimation to the difference reward dest 
i z where 
agents estimate the counterfactual using e f z zi 
and e f z ci 
 estimation to the difference reward dest 
i z where 
agents estimate the counterfactual using e f z zi 
and e f z 
these methods are first tested on an air traffic domain with 
 aircraft where of the aircraft are going through 
a single point of congestion over a four hour simulation 
agents are responsible for reducing congestion at this single 
point while trying to minimize delay the methods are then 
tested on a more difficult problem where a second point of 
congestion is added with the remaining aircraft going 
through this second point of congestion 
in all experiments the goal of the system is to maximize 
the system performance given by g z with the parameters 
a b τs equal to minutes and τs equal to 
 minutes these values of τ are obtained by examining 
the time at which most of the aircraft leave the sectors when 
no congestion control is being performed except where 
noted the trade-off between congestion and lateness α is 
set to in all experiments to make the agent results 
comparable to the monte carlo estimation the best policies 
chosen by the agents are used in the results all results are 
an average of thirty independent trials with the differences 
in the mean σ 
√ 
n shown as error bars though in most 
cases the error bars are too small to see 
figure performance on single congestion 
problem with aircraft agents and α 
 single congestion 
in the first experiment we test the performance of the five 
methods when there is a single point of congestion with 
twenty agents this point of congestion is created by setting 
up a series of flight plans that cause the number of aircraft in 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the sector of interest to be significantly more than the 
number allowed by the faa the results displayed in figures 
 and show the performance of all five algorithms on two 
different system evaluations in both cases the agent based 
methods significantly outperform the monte carlo method 
this result is not surprising since the agent based methods 
intelligently explore their space where as the monte carlo 
method explores the space randomly 
figure performance on single congestion 
problem with aircraft agents and α 
among the agent based methods agents using difference 
rewards perform better than agents using the system 
reward again this is not surprising since with twenty agents 
an agent directly trying to maximize the system reward has 
difficulty determining the effect of its actions on its own 
reward even if an agent takes an action that reduces 
congestion and lateness other agents at the same time may 
take actions that increase congestion and lateness causing 
the agent to wrongly believe that its action was poor in 
contrast agents using the difference reward have more 
influence over the value of their own reward therefore when an 
agent takes a good action the value of this action is more 
likely to be reflected in its reward 
this experiment also shows that estimating the difference 
reward is not only possible but also quite effective when 
the true value of the difference reward cannot be computed 
while agents using the estimates do not achieve as high of 
results as agents using the true difference reward they still 
perform significantly better than agents using the system 
reward note however that the benefit of the estimated 
difference rewards are only present later in learning earlier 
in learning the estimates are poor and agents using the 
estimated difference rewards perform no better then agents 
using the system reward 
 two congestions 
in the second experiment we test the performance of the 
five methods on a more difficult problem with two points of 
congestion on this problem the first region of congestion is 
the same as in the previous problem and the second region 
of congestion is added in a different part of the country 
the second congestion is less severe than the first one so 
agents have to form different policies depending which point 
of congestion they are influencing 
figure performance on two congestion problem 
with aircraft agents and α 
figure performance on two congestion problem 
with aircraft agents and α 
the results displayed in figure show that the relative 
performance of the five methods is similar to the single 
congestion case again agent based methods perform better 
than the monte carlo method and the agents using 
difference rewards perform better than agents using the system 
reward to verify that the performance improvement of our 
methods is maintained when there are a different number of 
agents we perform additional experiments with agents 
the results displayed in figure show that indeed the 
relative performances of the methods are comparable when the 
number of agents is increased to figure shows scaling 
results and demonstrates that the conclusions hold over a 
wide range of number of agents agents using dest 
 
perform slightly better than agents using dest 
in all cases but 
for agents this slight advantage stems from dest 
 
providing the agents with a cleaner signal since its estimate 
uses more data points 
 penalty tradeoffs 
the system evaluation function used in the experiments is 
g z − −α d z αc z which comprises of penalties 
for both congestion and lateness this evaluation function 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure impact of number of agents on system 
performance two congestion problem with 
aircraft and α 
forces the agents to tradeoff these relative penalties 
depending on the value of α with high α the optimization focuses 
on reducing congestion while with low α the system focuses 
on reducing lateness to verify that the results obtained 
above are not specific to a particular value of α we repeat 
the experiment with agents for α figure shows 
that qualitatively the relative performance of the algorithms 
remain the same 
next we perform a series of experiments where α ranges 
from to figure shows the results which lead to 
three interesting observations 
 first there is a zero congestion penalty solution this 
solution has agents enforce large mit values to block 
all air traffic which appears viable when the system 
evaluation does not account for delays all algorithms 
find this solution though it is of little interest in 
practice due to the large delays it would cause 
 second if the two penalties were independent an 
optimal solution would be a line from the two end points 
therefore unless d is far from being optimal the two 
penalties are not independent note that for α 
the difference between d and this hypothetical line is 
as large as it is anywhere else making α a 
reasonable choice for testing the algorithms in a difficult 
setting 
 third monte carlo and g are particularly poor at 
handling multiple objectives for both algorithms the 
performance degrades significantly for mid-ranges of α 
 computational cost 
the results in the previous section show the performance 
of the different algorithms after a specific number of episodes 
those results show that d is significantly superior to the 
other algorithms one question that arises though is what 
computational overhead d puts on the system and what 
results would be obtained if the additional computational 
expense of d is made available to the other algorithms 
the computation cost of the system evaluation g 
 equation is almost entirely dependent on the computation of 
figure performance on two congestion problem 
with aircraft agents and α 
figure tradeoff between objectives on two 
congestion problem with aircraft and agents 
note that monte carlo and g are particularly bad 
at handling multiple objectives 
the airplane counts for the sectors kt s which need to be 
computed using facet except when d is used the 
values of k are computed once per episode however to 
compute the counterfactual term in d if facet is treated as 
a black box each agent would have to compute their own 
values of k for their counterfactual resulting in n 
computations of k per episode while it may be possible to 
streamline the computation of d with some knowledge of 
the internals of facet given the complexity of the facet 
simulation it is not unreasonable in this case to treat it as 
a black box 
table shows the performance of the algorithms after 
 g computations for each of the algorithms for the 
simulations presented in figure where there were agents 
 congestions and α all the algorithms except the 
fully computed d reach k computations at time step 
 d however computes k once for the system and then 
once for each agent leading to computations per time 
step it therefore reaches computations at time step 
 we also show the results of the full d computation 
at t which needs computations of k as d k 
 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
table system performance for agents 
congestions and α after g evaluations except 
for d k 
which has g evaluations at t 
reward g σ 
√ 
n time 
dest 
- 
dest 
- 
d - 
d k 
- 
g - 
mc - 
although d k 
provides the best result by a slight margin 
it is achieved at a considerable computational cost indeed 
the performance of the two d estimates is remarkable in this 
case as they were obtained with about twenty times fewer 
computations of k furthermore the two d estimates 
significantly outperform the full d computation for a given 
number of computations of k and validate the assumptions 
made in section this shows that for this domain in 
practice it is more fruitful to perform more learning steps 
and approximate d than few learning steps with full d 
computation when we treat facet as a black box 
 discussion 
the efficient safe and reliable management of air traffic 
flow is a complex problem requiring solutions that integrate 
control policies with time horizons ranging from minutes 
up to a year the main contribution of this paper is to 
present a distributed adaptive air traffic flow management 
algorithm that can be readily implemented and to test that 
algorithm using facet a simulation tool widely used by 
the faa nasa and the industry our method is based on 
agents representing fixes and having each agent determine 
the separation between aircraft approaching its fix it offers 
the significant benefit of not requiring radical changes to 
the current air flow management structure and is therefore 
readily deployable the agents use reinforcement learning to 
learn control policies and we explore different agent reward 
functions and different ways of estimating those functions 
we are currently extending this work in three directions 
first we are exploring new methods of estimating agent 
rewards to further speed up the simulations second we are 
investigating deployment strategies and looking for 
modifications that would have larger impact one such 
modification is to extend the definition of agents from fixes to 
sectors giving agents more opportunity to control the 
traffic flow and allow them to be more efficient in eliminating 
congestion finally in cooperation with domain experts 
we are investigating different system evaluation functions 
above and beyond the delay and congestion dependent g 
presented in this paper 
acknowledgments the authors thank banavar 
sridhar for his invaluable help in describing both current air 
traffic flow management and ngats and shon grabbe for 
his detailed tutorials on facet 
 references 
 a agogino and k tumer efficient evaluation 
functions for multi-rover systems in the genetic and 
evolutionary computation conference pages - 
seatle wa june 
 a agogino and k tumer multi agent reward 
analysis for learning in noisy domains in proceedings 
of the fourth international joint conference on 
autonomous agents and multi-agent systems 
utrecht netherlands july 
 a k agogino and k tumer handling communiction 
restrictions and team formation in congestion games 
journal of autonous agents and multi agent systems 
 - 
 k d bilimoria b sridhar g b chatterji k s 
shethand and s r grabbe facet future atm 
concepts evaluation tool air traffic control 
quarterly 
 karl d bilimoria a geometric optimization approach 
to aircraft conflict resolution in aiaa guidance 
navigation and control conf denver co 
 martin s eby and wallace e kelly iii free flight 
separation assurance using distributed algorithms in 
proc of aerospace conf aspen co 
 faa opsnet data jan-dec us department of 
transportation website 
 s grabbe and b sridhar central east pacific flight 
routing in aiaa guidance navigation and control 
conference and exhibit keystone co 
 jared c hill f ryan johnson james k archibald 
richard l frost and wynn c stirling a cooperative 
multi-agent approach to free flight in aamas 
proceedings of the fourth international joint conference 
on autonomous agents and multiagent systems pages 
 - new york ny usa acm press 
 p k menon g d sweriduk and b sridhar 
optimal strategies for free flight air traffic conflict 
resolution journal of guidance control and 
dynamics - 
 nasa software of the year award nomination 
facet future atm concepts evaluation tool case 
no arc- - 
 m pechoucek d sislak d pavlicek and m uller 
autonomous agents for air-traffic deconfliction in 
proc of the fifth int jt conf on autonomous agents 
and multi-agent systems hakodate japan may 
 b sridhar and s grabbe benefits of direct-to in 
national airspace system in aiaa guidance 
navigation and control conf denver co 
 b sridhar t soni k sheth and g b chatterji 
aggregate flow model for air-traffic management 
journal of guidance control and dynamics 
 - 
 r s sutton and a g barto reinforcement 
learning an introduction mit press cambridge 
ma 
 c tomlin g pappas and s sastry conflict 
resolution for air traffic management ieee tran on 
automatic control - 
 k tumer and d wolpert editors collectives and the 
design of complex systems springer new york 
 
 d h wolpert and k tumer optimal payoff 
functions for members of collectives advances in 
complex systems - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
hypotheses refinement under topological 
communication constraints ∗ 
gauvain bourgne gael hette nicolas maudet and suzanne pinson 
lamsade univ paris-dauphine france 
{bourgne hette maudet pinson} lamsade dauphine fr 
abstract 
we investigate the properties of a multiagent system where 
each distributed agent locally perceives its environment 
upon perception of an unexpected event each agent locally 
computes its favoured hypothesis and tries to propagate it 
to other agents by exchanging hypotheses and supporting 
arguments observations however we further assume that 
communication opportunities are severely constrained and 
change dynamically in this paper we mostly investigate 
the convergence of such systems towards global consistency 
we first show that for a wide class of protocols that we 
shall define the communication constraints induced by the 
topology will not prevent the convergence of the system at 
the condition that the system dynamics guarantees that no 
agent will ever be isolated forever and that agents have 
unlimited time for computation and arguments exchange as 
this assumption cannot be made in most situations though 
we then set up an experimental framework aiming at 
comparing the relative efficiency and effectiveness of different 
interaction protocols for hypotheses exchange we study a 
critical situation involving a number of agents aiming at 
escaping from a burning building the results reported here 
provide some insights regarding the design of optimal 
protocol for hypotheses refinement in this context 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence-multiagent systems 
general terms 
theory experimentation 
 introduction 
we consider a multiagent system where each distributed 
agent locally perceives its environment and we assume that 
some unexpected event occurs in that system if each agent 
computes only locally its favoured hypothesis it is only 
natural to assume that agents will seek to coordinate and 
refine their hypotheses by confronting their observations with 
other agents if in addition the communication 
opportunities are severely constrained for instance agents can 
only communicate when they are close enough to some other 
agent and dynamically changing for instance agents may 
change their locations it becomes crucial to carefully 
design protocols that will allow agents to converge to some 
desired state of global consistency in this paper we 
exhibit some sufficient conditions on the system dynamics and 
on the protocol strategy structures that allow to guarantee 
that property and we experimentally study some contexts 
where some of these assumptions are relaxed 
while problems of diagnosis are among the venerable 
classics in the ai tradition their multiagent counterparts have 
much more recently attracted some attention roos and 
colleagues in particular study a situation where a number 
of distributed entities try to come up with a satisfying global 
diagnosis of the whole system they show in particular that 
the number of messages required to establish this global 
diagnosis is bound to be prohibitive unless the communication 
is enhanced with some suitable protocol however they do 
not put any restrictions on agents communication options 
and do not assume either that the system is dynamic 
the benefits of enhancing communication with supporting 
information to make convergence to a desired global state 
of a system more efficient has often been put forward in the 
literature this is for instance one of the main idea 
underlying the argumentation-based negotiation approach where 
the desired state is a compromise between agents with 
conflicting preferences many of these works however make the 
assumption that this approach is beneficial to start with 
and study the technical facets of the problem or instead 
emphasize other advantages of using argumentation 
notable exceptions are the works of which studied in 
contexts different from ours the efficiency of argumentation 
the rest of the paper is as follows section specifies 
the basic elements of our model and section goes on to 
presenting the different protocols and strategies used by the 
agents to exchange hypotheses and observations we put 
special attention at clearly emphasizing the conditions on 
the system dynamics and protocols strategies that will be 
exploited in the rest of the paper section details one of 
 
 - - - - rps c ifaamas 
the main results of the paper namely the fact that under the 
aforementioned conditions the constraints that we put on 
the topology will not prevent the convergence of the system 
towards global consistency at the condition that no agent 
ever gets completely lost forever in the system and that 
unlimited time is allowed for computation and argument 
exchange while the conditions on protocols and strategies are 
fairly mild it is also clear that these system requirements 
look much more problematic even frankly unrealistic in 
critical situations where distributed approaches are precisely 
advocated to get a clearer picture of the situation induced 
when time is a critical factor we have set up an 
experimental framework that we introduce and discuss in section 
the critical situation involves a number of agents aiming 
at escaping from a burning building the results reported 
here show that the effectiveness of argument exchange 
crucially depends upon the nature of the building and provide 
some insights regarding the design of optimal protocol for 
hypotheses refinement in this context 
 basic notions 
we start by defining the basic elements of our system 
environment 
let o be the potentially infinite set of possible 
observations we assume the sensors of our agents to be perfect 
hence the observations to be certain let h be the set of 
hypotheses uncertain and revisable let cons h o be the 
consistency relation a binary relation between a hypothesis 
h ∈ h and a set of observations o ⊆ o in most cases cons 
will refer to classical consistency relation however we may 
overload its meaning and add some additional properties to 
that relation in which case we will mention it 
the environment may include some dynamics and change 
over the course of time we define below sequences of time 
points to deal with it 
definition sequence of time points a 
sequence of time points t t tn from t is an ordered 
set of time points t t tn such that t ≥ t and 
∀i ∈ n − ti ≥ ti 
agent 
we take a system populated by n agents a an each 
agent is defined as a tuple f oi hi where 
 f the set of facts common knowledge to all agents 
 oi ∈ o 
 the set of observations made by the agent 
so far we assume a perfect memory hence this set 
grows monotonically 
 hi ∈ h the favourite hypothesis of the agent 
a key notion governing the formation of hypotheses is that 
of consistency defined below 
definition consistency we say that 
 an agent is consistent cons ai iff cons hi oi 
 that is its hypothesis is consistent with its 
observation set 
 an agent ai consistent with a partner agent aj iff 
cons ai and cons hi oj that is this agent is 
consistent and its hypothesis can explain the observation 
set of the other agent 
 two agents ai and aj are mutually consistent 
 mcons ai aj iff cons ai aj and cons aj ai 
 a system is consistent iff ∀ i j ∈ n 
it is the case 
that mcons ai aj 
to ensure its consistency each agent is equipped with an 
abstract reasoning machinery that we shall call the 
explanation function eh this deterministic function takes a 
set of observation and returns a single prefered hypothesis 
 o 
→ h we assume h eh o to be consistent with 
o by definition of eh so using this function on its 
observation set to determine its favourite hypothesis is a sure way 
for the agent to achieve consistency note however that an 
hypothesis does not need to be generated by eh to be 
consistent with an observation set as a concrete example of such 
a function and one of the main inspiration of this work 
one can cite the theorist reasoning system -as long as 
it is coupled with a filter selecting a single prefered theory 
among the ones initially selected by theorist 
note also that hi may only be modified as a consequence 
of the application eh we refer to this as the autonomy of the 
agent no other agent can directly impose a given 
hypothesis to an agent as a consequence only a new observation 
 being it a new perception or an observation communicated 
by a fellow agent can result in a modification of its prefered 
hypothesis hi but not necessarily of course 
we finally define a property of the system that we shall 
use in the rest of the paper 
definition bounded perceptions a system 
involves a bounded perception for agents iff ∃n s t 
∀t ∪n 
i oi ≤ n that is the number of observations to 
be made by the agents in the system is not infinite 
agent cycle 
now we need to see how these agents will evolve and interact 
in their environment in our context agents evolve in a 
dynamic environment and we classicaly assume the following 
system cycle 
 environment dynamics the environment evolves 
according to the defined rules of the system dynamics 
 perception step agents get perceptions from the 
environment these perceptions are typically partial e g 
the agent can only see a portion of a map 
 reasoning step agents compare perception with 
predictions seek explanations for potential 
difference s refine their hypothesis draw new conclusions 
 communication step agents can communicate 
hypotheses and observations with other agents through 
a defined protocol any agent can only be involved in 
one communication with another agent by step 
 action step agents do some practical reasoning using 
the models obtained from the previous steps and select 
an action they can then modify the environment by 
executing it 
the communication of the agents will be further 
constrained by topological consideration at a given time an 
agent will only be able to communicate with a number of 
neighbours its connexions with these others agents may 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
evolve with its situation in the environment typically an 
agent can only communicate with agents that it can sense 
but one could imagine evolving topological constraints on 
communication based on a network of communications 
between agents where the links are not always active 
communication 
in our system agents will be able to communicate with each 
other however due to the aforementionned topological 
constraints they will not be able to communicate with any 
agents at anytime who an agent can communicate with 
will be defined dynamically for instance this can be a 
consequence of the agents being close enough to get in touch 
we will abstractly denote by c ai aj t the communication 
property in other words the fact that agents ai and aj can 
communicate at time t note that this relation is assumed 
to be symetric but of course not transitive we are now in 
a position to define two essential properties of our system 
definition temporal path there exists a 
temporal communication path at horizon tf noted ltf ai aj 
between ai and aj iff there exists a sequence of time points 
t t tn from tf and a sequence of agents k k kn 
s t i c ai ak t ii c akn aj tn iii ∀i ∈ n 
c aki aki ti 
intuitively what this property says is that it is possible to 
find a temporal path in the future that would allow to link 
agent ai and aj via a sequence of intermediary agents note 
that the time points are not necessarily successive and that 
the sequence of agents may involve the same agents several 
times 
definition temporal connexity a system is 
temporaly connex iff ∀t ∀ i j ∈ n 
lt ai aj 
in short a temporaly connex system guarantees that any 
agent will be able to communicate with any other agents 
no matter how long it might take to do so at any time to 
put it another way it is never the case that an agent will be 
isolated for ever from another agent of the system 
we will next discuss the detail of how communication 
concretely takes place in our system remember that in this 
paper we only consider the case of bilateral exchanges an 
agent can only speak to a single other agent and that we 
also assume that any agent can only engage in a single 
exchange in a given round 
 protocols and strategies 
in this section we discuss the requirements of the 
interaction protocols that govern the exchange of messages between 
agents and provide some example instantiation of such 
protocols to clarify the presentation we distinguish two 
levels the local level which is concerned with the regulation 
of bilateral exchanges and the global level which essentially 
regulates the way agents can actually engage into a 
conversation at each level we separate what is specified by the 
protocol and what is left to agents strategies 
local protocol and strategies 
we start by inspecting local protocols and strategies that 
will regulate the communication between the agents of the 
system as we limit ourselves to bilateral communication 
these protocols will simply involve two agents such protocol 
will have to meet one basic requirement to be satisfying 
 consistency cons - a local protocol has to 
guarantee the mutual consistency of agents upon termination 
 which implies termination of course 
figure a hypotheses exchange protocol 
one example such protocol is the protocol described in 
that is pictured in fig to further illustrate how such 
protocol can be used by agents we give some details on a 
possible strategy upon receiving a hypothesis h propose h or 
counterpropose h from a agent a is in state and has 
the following possible replies counterexample if the agent 
knows an example contradicting the hypothesis or not 
explained by this hypothesis challenge if the agents lacks 
evidence to accept this hypothesis counterpropose if the 
agent agrees with the hypothesis but prefers another one 
or accept if it is indeed as good as its favourite 
hypothesis this strategy guarantees among other properties the 
eventual mutual logical consistency of the involved agents 
 
global protocol 
the global protocol regulates the way bilateral exchanges 
will be initiated between agents at each turn agents will 
concurrently send one weighted request to communicate to 
other agents this weight is a value measuring the agent s 
willingness to converse with the targeted agent in practice 
this can be based on different heuristics but we shall make 
some assumptions on agents strategies see below sending 
such a request is a kind of conditional commitment for the 
agent an agent sending a weighted request commits to 
engage in conversation with the target if he does not receive 
and accept himself another request once all request have 
been received each agent replies with either an acccept or 
a reject by answering with an accept an agent makes a 
full commitment to engage in conversation with the sender 
therefore it can only send one accept in a given round as an 
agent can only participate in one conversation per time step 
when all response have been received each agent receiving 
an accept can either initiate a conversation using the local 
protocol or send a cancel if it has accepted another request 
at the end of all the bilateral exchanges the agents 
engaged in conversation are discarded from the protocol then 
each of the remaining agents resends a request and the 
process iterates until no more requests are sent 
global strategy 
we now define four requirements for the strategies used by 
agents depending on their role in the protocol two are 
concerned with the requestee role how to decide who the 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
agent wishes to communicate with the other two with the 
responder role how to decide which communication request 
to accept or not 
 willingness to solve inconsistancies solve -agents 
want to communicate with any other agents unless 
they know they are mutually consistent 
 focus on solving inconsistencies focus -agents do 
not request communication with an agent with whom 
they know they are mutually consistent 
 willingness to communicate comm -agents cannot 
refuse a weighted communication request unless they 
have just received or send a request with a greater 
weight 
 commitment to communication request 
 requ agents cannot accept a weighted communication 
request if they have themselves sent a communication 
request with a greater weight therefore they will 
not cancel their request unless they have received a 
communicational request with greater weight 
now the protocol structure together with the properties 
comm requ ensure that a request can only be rejected 
if its target agent engages in communication with another 
agent suppose indeed that agent ai wants to communicate 
with aj by sending a request with weight w comm 
guarantees that an agent receiving a weighted request will either 
accept this communication accept a communication with a 
greater weight or wait for the answer to a request with a 
greater weight this ensures that the request with 
maximal weight will be accepted and not cancelled as requ 
ensures that an agent sending a request can only cancel it if 
he accepts another request with greater weight therefore 
at least two agents will engage in conversation per round 
of the global protocol as the protocol ensures that ai can 
resend its request while aj is not engaged in a conversation 
there will be a turn in which aj must engage in a 
conversation either with ai or another agent 
these requirements concern request sending and 
acceptation but agents also need some strategy of weight 
attribution we describe below an altruist strategy used in our 
experiments being cooperative an agent may want to know 
more of the communication wishes of other agents in order to 
improve the overall allocation of exchanges to agents a 
context request step is then added to the global protocol before 
sending their chosen weighted request agents attribute a 
weight to all agents they are prepared to communicate with 
according to some internal factors in the simplest case this 
weight will be for all agent with whom the agent is not 
sure of being mutually consistent ensuring solve other 
agent being not considered for communication ensuring 
focus the agent then sends a context request to all agents 
with whom communication is considered this request also 
provides information about the sender list of considered 
communications along with their weight after reception 
of all the context requests agents will either reply with a 
deny iff they are already engaged in a conversation in which 
case the requesting agent will not consider communication 
with them anymore in this turn or an inform giving the 
requester information about the requests it has sent and 
received when all replies have been received each agent can 
calculate the weight of all requests concerning it it does so 
by substracting from the weight of its request the weight of 
all requests concerning either it or its target that is the 
final weight of the request from ai to aj is wi j wi j wj i − 
 
p 
k∈r i −{j} wi k 
p 
k∈s i −{j} wk i 
p 
k∈r j −{i} wj k 
p 
k∈s j −{i} wk j where wi j is the weight of the request of 
ai to aj r i is the set of indice of agents having received a 
request from ai and s i is the set of indice of agents 
having send a request to ai it then finally sends a weighted 
request to the agents who maximise this weight or wait for 
a request as described in the global protocol 
 conditional convergence to 
global consistency 
in this section we will show that the requirements 
regarding protocols and strategies just discussed will be sufficient 
to ensure that the system will eventually converge towards 
global consistency under some conditions we first show 
that if two agents are not mutually consistent at some time 
then there will be necessarily a time in the future such that 
an agent will learn a new observation being it because it is 
new for the system or by learning it from another agent 
lemma let s be a system populated by n agents 
a a an temporaly connex and involving bounded 
perceptions for these agents let n be the sum of 
cardinalities of the intersection of pairwise observation sets 
 n 
p 
 i j ∈ n oi ∩ oj let n be the cardinality of 
the union of all agents observations sets n ∪n 
i oi 
if ¬mcons ai aj at time t there is necessarily a time 
t t s t either n or n will increase 
proof suppose that there exist a time t 
and indices i j s t ¬mcons ai aj we will 
use mt 
p 
 k l ∈ n εcomm ak al t where 
εcomm ak al t if ak and al have 
communicated at least once since t and otherwise 
temporal connexity guarantees that there exist t tm 
and k km s t c ai ak t c akm aj tm and 
∀p ∈ m c akp akp tp clearly if mcons ai ak 
mcons akm aj and ∀p mcons akp akp we have 
mcons ai aj which contradicts our hypothesis mcons 
being transitive mcons ai ak ∧mcons ak ak implies 
that mcons ai ak and so on till mcons ai akm ∧ 
mcons akm aj which implies mcons ai aj 
at least two agents are then necessarily 
inconsistent ¬mcons ai ak or ¬mcons akm aj or ∃p t q 
¬mcons akp 
 akp let ak and al be these two 
neighbours at a time t t 
 
 the solve property ensures 
that either ak or al will send a communication request to 
the other agent at time t as shown before this in turn 
ensures that at least one of these agents will be involved in 
a communication then there are two possibilities 
 case i ak and al communicate at time t in this case 
we know that ¬mcons ak al this and the cons 
property ensures that at least one of the agents must change its 
 
strictly speaking the transitivity of mcons only ensure 
that ak and al are inconsistent at a time t ≥ t that can 
be different from the time t at which they can 
communicate but if they become consistent between t and t or 
inconsistent between t and t it means that at least one 
of them have changed its hypothesis between t and t that 
is after t we can then apply the reasoning of case iib 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
hypothesis which in turn since agents are autonomous 
implies at least one exchange of observation but then ok ∩ol 
is bound to increase n t n t 
 case ii ak communicates with ap at time t we then have 
again two possibilities 
 case iia ak and ap did not communicate since t but then 
εcomm ak ap t had value and takes value hence mt 
increases 
 case iib ak and ap did communicate at some time t 
t the cons property of the protocol ensures that 
mcons ak ap at that time now the fact that they 
communicate and focus implies that at least one of them did 
change its hypothesis in the meantime the fact that agents 
are autonomous implies in turn that a new observation 
 perceived or received from another agent necessarily provoked 
this change the latter case would ensure the existence of a 
time t t and an agent aq s t either op ∩oq or ok ∩oq 
increases of at that time implying n t n t the 
former case means that the agent gets a new perception o 
at time t if that observation was unknown in the system 
before then n t n t if some agent aq already knew 
this observation before then either op ∩ oq or ok ∩ oq 
increases of at time t which implies that n t n t 
hence ¬mcons ai aj at time t guarantees that either 
−∃t t t q n t n t or 
−∃t t t q n t n t or 
−∃t t t q mt increases of at time t 
by iterating the reasoning with t but keeping t as the 
time reference for mt we can eliminate the third case 
 mt is integer and bounded by n 
 which means that 
after a maximum of n 
iterations we necessarily will be in 
one of two other cases as a result we have proven that if 
¬mcons ai aj at time t there is necessarily a time t s t 
either n or n will increase 
theorem global consistency let s be a 
system populated by n agents a a an temporaly connex 
and involving bounded perceptions for these agents let 
cons ai aj be a transitive consistency property then any 
protocol and strategies satisfying properties cons solve 
focus comm and requ guarantees that the system will 
converge towards global consistency 
proof for the sake of contradiction let us assume 
∃i j ∈ n s t ∀t ∃t t t q ¬cons ai aj t 
using the lemma this implies that ∃t t s t either 
n t n t or n t n t but we can apply 
the same reasoning taking t t which would give us 
t t t s t ¬cons ai aj t which gives us t t 
s t either n t n t or n t n t by 
successive iterations we can then construct a sequence t t tn 
which can be divided in two sub-sequences t t tn and 
t t tn s t n t n t n tn and 
n t n t n tn one of these sub-sequences 
has to be infinite however n ti and n ti are strictly 
growing integer and bounded which implies that both are 
finite contradiction 
what the previous result essentially shows is that in a 
system where no agent will be isolated from the rest of the 
agents for ever only very mild assumptions on the protocols 
and strategies used by agents suffice to guarantee 
convergence towards system consistency in a finite amount of time 
 although it might take very long unfortunately in many 
critical situations it will not be possible to assume this 
temporal connexity as distributed approaches as the one 
advocated in this paper are precisely often presented as a 
good way to tackle problems of reliability or problems of 
dependence to a center that are of utmost importance in these 
critical applications it is certainly interesting to further 
explore how such a system would behave when we relax this 
assumption 
 experimental study 
this experiment involves agents trying to escape from a 
burning building the environment is described as a spatial 
grid with a set of walls and thankfully some exits time 
and space are considered discrete time is divided in rounds 
agents are localised by their position on the spatial grid 
these agents can move and communicate with other agents 
in a round an agent can move of one cell in any of the four 
cardinal directions provided it is not blocked by a wall in 
this application agents communicate with any other agent 
 but recall a single one given that this agent is in view 
and that they have not yet exchanged their current favoured 
hypothesis suddenly a fire erupts in these premises from 
this moment the fire propagates each round for each cases 
where there is fire the fire propagates in the four directions 
however the fire cannot propagate through a wall if the 
fire propagates in a case where an agent is positioned that 
agent burns and is considered dead it can of course no 
longer move nor communicate if an agent gets to an exit 
it is considered saved and can no longer be burned agents 
know the environment and the rules governing the 
dynamics of this environment that is they know the map as well 
as the rules of fire propagation previously described they 
also locally perceive this environment but cannot see 
further than cases away in any direction walls also block 
the line of view preventing agents from seeing behind them 
within their sight they can see other agents and whether 
or not the cases they see are on fire all these perceptions 
are memorised 
we now show how this instantiates the abstract 
framework presented the paper 
 o {fire x y t nofire x y t agent ai x y t } 
observations can then be positive o ∈ p o iff ∃h ∈ 
h s t h o or negative o ∈ n o iff ∃h ∈ h s t 
h ¬o 
 h {fireorigin x y t ∧ ∧fireorigin xl yl tl } 
hypotheses are conjunctions of fireorigins 
 cons h o consistency relation satisfies 
- coherence ∀o ∈ n o h ¬o 
- completeness ∀o ∈ p o h o 
- minimality for all h ∈ h if h is coherent and 
complete for o then h is prefered to h according 
to the preference relation h ≤p h 
 
selects first the minimal number of origins then the most 
recent least preemptive strategy then uses some 
arbitrary fixed ranking to discriminate ex-aequo the resulting 
relation is a total order hence minimality implies that there 
will be a single h s t cons o h for a given o this in 
turn means that mcons ai aj iff cons ai cons aj and 
hi hj this relation is then transitive and symmetric 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 eh takes o as argument and returns min≤p of the 
coherent and complete hypothesis for o 
 experimental evaluation 
we will classically see e g assess the effectiveness 
and efficiency of different interaction protocols 
effectiveness of a protocol 
the proportion of agents surviving the fire over the initial 
number of agents involved in the experiment will determine 
the effectiveness of a given protocol if this value is high 
the protocol has been effective to propagate the information 
and or for the agents to refine their hypotheses and 
determine the best way to the exit 
efficiency of a protocol 
typically the use of supporting information will involve a 
communication overhead we will assume here that the 
efficiency of a given protocol is characterised by the data flow 
induced by this protocol in this paper we will only discuss 
this aspect wrt local protocols the main measure that we 
shall then use here is the mean total size of messages that 
are exchanged by agents per exchange hence taking into 
account both the number of messages and the actual size of the 
messages because it could be that messages happen to be 
very big containing e g a large number of observations 
which could counter-balance a low number of messages 
 experimental settings 
the chosen experimental settings are the following 
 environmental topology- performances of 
information propagation are highly constrained by the 
environment topology the perception skills of the agents 
depend on the openness of the environment with 
a large number of walls the perceptions of agents are 
limited and also the number of possible inter-agent 
communications whereas an open environment will 
provide optimal possibilities of perception and 
information propagation thus we propose a topological 
index see below as a common basis to charaterize the 
environments maps used during experimentations 
the topological index ti is the ratio of the number of 
cells that can be perceived by agents summed up from 
all possible positions divided by the number of cells 
that would be perceived from the same positions but 
without any walls the closer to the more open the 
environment we shall also use two additional more 
classical measures the characteristic path length 
 cpl and the clustering coefficient 
 cc 
 number of agents- the propagation of information 
also depends on the initial number of agents involved 
during an experimentation for instance the more 
agents the more potential communications there is 
this means that there will be more potential for 
propagation but also that the bilateral exchange restriction 
will be more crucial 
 
the cpl is the median of the means of the shortest path 
lengths connecting each node to all other nodes 
 
characterising the isolation degree of a region of an 
environment in terms of acessibility number of roads still usable 
to reach this region 
map t i c p l c c 
 - 
 - 
 - 
 - 
 - 
 - 
 - 
 - 
 - 
table topological characteristics of the maps 
 initial positions of the agents- initial positions of the 
agents have a significant influence on the overall 
behavior of an instance of our system being close from 
an exit will in general ease the escape 
 experimental environments 
we choose to realize experiments on three very 
different topological indexes for open environments 
for mixed environments and for labyrinth-like 
environments 
figure two maps left ti right ti 
we designed three different maps for each index fig 
shows two of them containing the same maximum number 
of agents agents max with a maximum density of one 
agent per cell the same number of exits and a similar fire 
origin e g starting time and position the three differents 
maps of a given index are designed as follows the first map 
is a model of an existing building floor the second map has 
the same enclosure exits and fire origin as the first one 
but the number and location of walls are different wall 
locations are designed by an heuristic which randomly creates 
walls on the spatial grid such that no fully closed rooms are 
created and that no exit is closed the third map is 
characterised by geometrical enclosure in wich walls location 
is also designed with the aforementioned heuristic table 
summarizes the different topological measures 
characterizing these different maps it is worth pointing out that the 
values confirm the relevance of ti maps with a high ti 
have a low cpl and a high cc however the cpl and cc 
allows to further refine the difference between the maps e g 
between - and - 
 experimental results 
for each triple of maps defined as above we conduct the 
same experiments in each experiment the society differs in 
terms of its initial proportion of involved agents from 
to this initial proportion represents the percentage 
of involved agents with regards to the possible maximum 
number of agents for each map and each initial proportion 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
we select randomly different initial agents locations 
for each of those different locations we execute the system 
one time for each different interaction protocol 
effectiveness of communication and argumentation 
the first experiment that we set up aims at testing how 
effective is hypotheses exchange he and in particular how 
the topological aspects will affect this effectiveness in order 
to do so we have computed the ratio of improvement offered 
by that protocol over a situation where agents could simply 
not communicate no comm to get further insights as 
to what extent the hypotheses exchange was really crucial 
we also tested a much less elaborated protocol consisting 
of mere observation exchanges oe more precisely this 
protocol requires that each agent stores any unexpected 
observation that it perceives and agents simply exchange 
their respective lists of observations when they discuss in 
this case the local protocol is different note in 
particular that it does not guarantee mutual consistency but the 
global protocol remains the same at the only exception that 
agents motivation to communicate is to synchronise their 
list of observations not their hypothesis if this protocol is 
at best as effective as he it has the advantage of being more 
efficient this is obvious wrt the number of messages which 
will be limited to less straightforward as far as the size of 
messages is concerned but the rough observation that the 
exchange of observations can be viewed as a flat version 
of the challenge is helpful to see this the results of these 
experiments are reported in fig 
figure comparative effectiveness ratio gain of 
protocols when the proportion of agents augments 
the first observation that needs to be made is that 
communication improves the effectiveness of the process and 
this ratio increases as the number of agents grows in the 
system the second lesson that we learn here is that 
closeness relatively makes communication more effective over non 
communication maps exhibiting a t i of are 
constantly above the two others and are still slightly but 
significantly better than however these curves also 
suggest perhaps surprisingly that he outperforms oe in 
precisely those situations where the ratio gain is less 
important the only noticeable difference occurs for rather open 
maps where t i is this may be explained as follows 
when a map is open agents have many potential explanation 
candidates and argumentation becomes useful to 
discriminate between those when a map is labyrinth-like there are 
fewer possible explanations to an unexpected event 
importance of the global protocol 
the second set of experiments seeks to evaluate the 
importance of the design of the global protocol we tested our 
protocol against a local broadcast lb protocol local 
broadcast means that all the neighbours agents perceived 
by an agent will be involved in a communication with that 
agent in a given round -we alleviate the constraint of a 
single communication by agent this gives us a rough upper 
bound upon the possible ratio gain in the system for a given 
local protocol again we evaluated the ratio gain induced 
by that lb over our classical he for the three different 
classes of maps the results are reported in fig 
figure ratio gain of local broadcast over 
hypotheses exchange 
note to begin with that the ratio gain is when the 
proportion of agents is which is easily explained by the fact 
that it corresponds to situations involving only two agents 
we first observe that all classes of maps witness a ratio 
gain increasing when the proportion of agents augments the 
gain reaches to depending on the class of maps 
considered if one compares this with the improvement reported 
in the previous experiment it appears to be of the same 
magnitude this illustrates that the design of the global 
protocol cannot be ignored especially when the proportion 
of agents is high however we also note that the 
effectiveness ratio gain curves have very different shapes in both 
cases the gain induced by the accuracy of the local protocol 
increases very quickly with the proportion of agents while 
the curve is really smooth for the global one 
now let us observe more carefully the results reported 
here the curve corresponding to a ti of is above that 
corresponding to this is so because the more open 
a map the more opportunities to communicate with more 
than one agent and hence benefits from broadcast 
however we also observe that curve for is below that for 
 this is explained as follows in the case of the 
potential gain to be made in terms of surviving agents is 
much lower because our protocols already give rather 
efficient outcomes anyway quickly reaching see fig 
a simple rule of thumb could be that when the number of 
agents is small special attention should be put on the local 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
protocol whereas when that number is large one should 
carefully design the global one unless the map is so open 
that the protocol is already almost optimally efficient 
efficiency of the protocols 
the final experiment reported here is concerned with the 
analysis of the efficiency of the protocols we analysis here 
the mean size of the totality of the messages that are 
exchanged by agents mean size of exchanges for short using 
the following protocols he oe and two variant 
protocols the first one is an intermediary restricted hypotheses 
exchange protocol rhe rhe is as follows it does not 
involve any challenge nor counter-propose which means that 
agents cannot switch their role during the protocol this 
differs from re in that respect in short rhe allows an agent 
to exhaust its partner s criticism and eventually this 
partner will come to adopt the agent s hypothesis note that this 
means that the autonomy of the agent is not preserved here 
 as an agent will essentially accept any hypothesis it cannot 
undermine with the hope that the gain in efficiency will be 
significant enough to compensate a loss in effectiveness the 
second variant protocol is a complete observation exchange 
protocol coe coe uses the same principles as oe but 
includes in addition all critical negative examples nofire in 
the exchange thus giving all examples used as arguments 
by the hypotheses exchanges protocol hence improving 
effectiveness results for map - are shown on fig 
figure mean size of exchanges 
first we can observe the fact that the ordering of the 
protocols from the least efficient to the most efficient is coe 
he rhe and then oe he being more efficient than coe 
proves that the argumentation process gains efficiency by 
selecting when it is needed to provide negative example which 
have less impact that positive ones in our specific testbed 
however by communicating hypotheses before eventually 
giving observation to support it he instead of directly 
giving the most crucial observations oe the argumentation 
process doubles the size of data exchanges it is the cost for 
ensuring consistency at the end of the exchange a property 
that oe does not support also significant is the fact the 
the mean size of exchanges is slightly higher when the 
number of agents is small this is explained by the fact that in 
these cases only a very few agents have relevant informations 
in their possession and that they will need to communicate 
a lot in order to come up with a common view of the 
situation when the number of agents increases this knowledge 
is distributed over more agents which need shorter 
discussions to get to mutual consistency as a consequence the 
relative gain in efficiency of using rhe appears to be better 
when the number of agents is small when it is high they 
will hardly argue anyway finally it is worth noticing that 
the standard deviation for these experiments is rather high 
which means that the conversation do not converge to any 
stereotypic pattern 
 conclusion 
this paper has investigated the properties of a 
multiagent system where each distributed agent locally perceives 
its environment and tries to reach consistency with other 
agents despite severe communication restrictions in 
particular we have exhibited conditions allowing convergence and 
experimentally investigated a typical situation where those 
conditions cannot hold there are many possible extensions 
to this work the first being to further investigate the 
properties of different global protocols belonging to the class we 
identified and their influence on the outcome there are in 
particular many heuristics highly dependent on the context 
of the study that could intuitively yield interesting results 
 in our study selecting the recipient on the basis of what can 
be inferred from his observed actions could be such a 
heuristic one obvious candidate for longer term issues concern 
the relaxation of the assumption of perfect sensing 
 references 
 g bourgne n maudet and s pinson when agents 
communicate hypotheses in critical situations in 
proceedings of dalt- may 
 p harvey c f chang and a ghose support-based 
distributed search a new approach for multiagent 
constraint processing in proceedings of aamas 
 
 h jung and m tambe argumentation as distributed 
constraint satisfaction applications and results in 
proceedings of agents 
 n c karunatillake and n r jennings is it worth 
arguing in proceedings of argmas 
 s onta˜n´on and e plaza arguments and 
counterexamples in case-based joint deliberation in 
proceedings of argmas- may 
 d poole explanation and prediction an architecture 
for default and abductive reasoning computational 
intelligence - 
 i rahwan s d ramchurn n r jennings 
p mcburney s parsons and l sonenberg 
argumention-based negotiation the knowledge 
engineering review - 
 n roos a ten tije and c witteveen a protocol 
for multi-agent diagnosis with spatially distributed 
knowledge in proceedings of aamas 
 n roos a ten tije and c witteveen reaching 
diagnostic agreement in multiagent diagnosis in 
proceedings of aamas 
 t takahashi y kaneda and n ito preliminary 
study - using robocuprescue simulations for disasters 
prevention in proceedings of srmed 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
learning consumer preferences using semantic 
similarity ∗ 
reyhan aydo˘gan 
reyhan aydogan gmail com 
pınar yolum 
pinar yolum boun edu tr 
department of computer engineering 
bo˘gaziçi university 
bebek istanbul turkey 
abstract 
in online dynamic environments the services requested by 
consumers may not be readily served by the providers this requires 
the service consumers and providers to negotiate their service needs 
and offers multiagent negotiation approaches typically assume 
that the parties agree on service content and focus on finding a 
consensus on service price in contrast this work develops an 
approach through which the parties can negotiate the content of a 
service this calls for a negotiation approach in which the parties 
can understand the semantics of their requests and offers and learn 
each other s preferences incrementally over time accordingly we 
propose an architecture in which both consumers and producers 
use a shared ontology to negotiate a service through repetitive 
interactions the provider learns consumers needs accurately and 
can make better targeted offers to enable fast and accurate 
learning of preferences we develop an extension to version space and 
compare it with existing learning techniques we further develop 
a metric for measuring semantic similarity between services and 
compare the performance of our approach using different 
similarity metrics 
categories and subject descriptors 
i distributed artificial intelligence multiagent systems 
general terms 
algorithms experimentation 
 introduction 
current approaches to e-commerce treat service price as the 
primary construct for negotiation by assuming that the service content 
is fixed however negotiation on price presupposes that other 
properties of the service have already been agreed upon 
nevertheless many times the service provider may not be offering the exact 
requested service due to lack of resources constraints in its 
business policy and so on when this is the case the producer and 
the consumer need to negotiate the content of the requested service 
 
however most existing negotiation approaches assume that all 
features of a service are equally important and concentrate on the 
price however in reality not all features may be relevant and 
the relevance of a feature may vary from consumer to consumer 
for instance completion time of a service may be important for one 
consumer whereas the quality of the service may be more important 
for a second consumer without doubt considering the preferences 
of the consumer has a positive impact on the negotiation process 
for this purpose evaluation of the service components with 
different weights can be useful some studies take these weights as a 
priori and uses the fixed weights on the other hand mostly 
the producer does not know the consumer s preferences before the 
negotiation hence it is more appropriate for the producer to learn 
these preferences for each consumer 
preference learning as an alternative we propose an 
architecture in which the service providers learn the relevant features 
of a service for a particular customer over time we represent 
service requests as a vector of service features we use an ontology 
in order to capture the relations between services and to construct 
the features for a given service by using a common ontology we 
enable the consumers and producers to share a common 
vocabulary for negotiation the particular service we have used is a wine 
selling service the wine seller learns the wine preferences of the 
customer to sell better targeted wines the producer models the 
requests of the consumer and its counter offers to learn which 
features are more important for the consumer since no information is 
present before the interactions start the learning algorithm has to 
be incremental so that it can be trained at run time and can revise 
itself with each new interaction 
service generation even after the producer learns the important 
features for a consumer it needs a method to generate offers that 
are the most relevant for the consumer among its set of possible 
services in other words the question is how the producer uses the 
information that was learned from the dialogues to make the best 
offer to the consumer for instance assume that the producer has 
learned that the consumer wants to buy a red wine but the producer 
can only offer rose or white wine what should the producer s offer 
 
 - - - - rps c ifaamas 
contain white wine or rose wine if the producer has some domain 
knowledge about semantic similarity e g knows that the red and 
rose wines are taste-wise more similar than white wine then it can 
generate better offers however in addition to domain knowledge 
this derivation requires appropriate metrics to measure similarity 
between available services and learned preferences 
the rest of this paper is organized as follows section explains 
our proposed architecture section explains the learning 
algorithms that were studied to learn consumer preferences section 
studies the different service offering mechanisms section 
contains the similarity metrics used in the experiments the details of 
the developed system is analyzed in section section provides 
our experimental setup test cases and results finally section 
discusses and compares our work with other related work 
 architecture 
our main components are consumer and producer agents which 
communicate with each other to perform content-oriented 
negotiation figure depicts our architecture the consumer agent 
represents the customer and hence has access to the preferences of the 
customer the consumer agent generates requests in accordance 
with these preferences and negotiates with the producer based on 
these preferences similarly the producer agent has access to the 
producer s inventory and knows which wines are available or not 
a shared ontology provides the necessary vocabulary and hence 
enables a common language for agents this ontology describes 
the content of the service further since an ontology can represent 
concepts their properties and their relationships semantically the 
agents can reason the details of the service that is being negotiated 
since a service can be anything such as selling a car reserving a 
hotel room and so on the architecture is independent of the 
ontology used however to make our discussion concrete we use the 
well-known wine ontology with some modification to 
illustrate our ideas and to test our system the wine ontology describes 
different types of wine and includes features such as color body 
winery of the wine and so on with this ontology the service that 
is being negotiated between the consumer and the producer is that 
of selling wine 
the data repository in figure is used solely by the producer 
agent and holds the inventory information of the producer the 
data repository includes information on the products the producer 
owns the number of the products and ratings of those products 
ratings indicate the popularity of the products among customers 
those are used to decide which product will be offered when there 
exists more than one product having same similarity to the request 
of the consumer agent 
the negotiation takes place in a turn-taking fashion where the 
consumer agent starts the negotiation with a particular service 
request the request is composed of significant features of the 
service in the wine example these features include color winery and 
so on this is the particular wine that the customer is interested in 
purchasing if the producer has the requested wine in its inventory 
the producer offers the wine and the negotiation ends otherwise 
the producer offers an alternative wine from the inventory when 
the consumer receives a counter offer from the producer it will 
evaluate it if it is acceptable then the negotiation will end 
otherwise the customer will generate a new request or stick to the 
previous request this process will continue until some service is 
accepted by the consumer agent or all possible offers are put 
forward to the consumer by the producer 
one of the crucial challenges of the content-oriented negotiation 
is the automatic generation of counter offers by the service 
producer when the producer constructs its offer it should consider 
figure proposed negotiation architecture 
three important things the current request consumer preferences 
and the producer s available services both the consumer s current 
request and the producer s own available services are accessible by 
the producer however the consumer s preferences in most cases 
will not be available hence the producer will have to understand 
the needs of the consumer from their interactions and generate a 
counter offer that is likely to be accepted by the consumer this 
challenge can be studied in three stages 
 preference learning how can the producers learn about 
each customer s preferences based on requests and counter 
offers section 
 service offering how can the producers revise their offers 
based on the consumer s preferences that they have learned 
so far section 
 similarity estimation how can the producer agent estimate 
similarity between the request and available services 
 section 
 preference learning 
the requests of the consumer and the counter offers of the 
producer are represented as vectors where each element in the vector 
corresponds to the value of a feature the requests of the consumers 
represent individual wine products whereas their preferences are 
constraints over service features for example a consumer may 
have preference for red wine this means that the consumer is 
willing to accept any wine offered by the producers as long as the 
color is red accordingly the consumer generates a request where 
the color feature is set to red and other features are set to arbitrary 
values e g medium strong red 
at the beginning of negotiation the producer agent does not 
know the consumer s preferences but will need to learn them 
using information obtained from the dialogues between the producer 
and the consumer the preferences denote the relative importance 
of the features of the services demanded by the consumer agents 
for instance the color of the wine may be important so the 
consumer insists on buying the wine whose color is red and rejects all 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
table how dcea works 
type sample the most the most 
general set specific set 
 full strong white { } { full strong white } 
{{ -full } 
- full delicate rose { -delicate } { full strong white } 
{ -rose }} 
{{ -full } {{ full strong white } 
 medium moderate red { -delicate } { medium moderate red }} 
{ -rose }} 
the offers involving the wine whose color is white or rose on the 
contrary the winery may not be as important as the color for this 
customer so the consumer may have a tendency to accept wines 
from any winery as long as the color is red 
to tackle this problem we propose to use incremental learning 
algorithms this is necessary since no training data is 
available before the interactions start we particularly investigate two 
approaches the first one is inductive learning this technique is 
applied to learn the preferences as concepts we elaborate on 
candidate elimination algorithm cea for version space cea 
is known to perform poorly if the information to be learned is 
disjunctive interestingly most of the time consumer preferences are 
disjunctive say we are considering an agent that is buying wine 
the consumer may prefer red wine or rose wine but not white wine 
to use cea with such preferences a solid modification is 
necessary the second approach is decision trees decision trees can 
learn from examples easily and classify new instances as positive 
or negative a well-known incremental decision tree is id r 
however id r is known to suffer from high computational 
complexity for this reason we instead use the id algorithm and 
iteratively build decision trees to simulate incremental learning 
 cea 
cea is one of the inductive learning algorithms that learns 
concepts from observed examples the algorithm maintains two 
sets to model the concept to be learned the first set is the most 
general set g g contains hypotheses about all the possible values 
that the concept may obtain as the name suggests it is a 
generalization and contains all possible values unless the values have 
been identified not to represent the concept the second set is the 
most specific set s s contains only hypotheses that are known to 
identify the concept that is being learned at the beginning of the 
algorithm g is initialized to cover all possible concepts while s is 
initialized to be empty 
during the interactions each request of the consumer can be 
considered as a positive example and each counter offer generated by 
the producer and rejected by the consumer agent can be thought of 
as a negative example at each interaction between the producer 
and the consumer both g and s are modified the negative 
samples enforce the specialization of some hypotheses so that g does 
not cover any hypothesis accepting the negative samples as 
positive when a positive sample comes the most specific set s should 
be generalized in order to cover the new training instance as a 
result the most general hypotheses and the most special hypotheses 
cover all positive training samples but do not cover any negative 
ones incrementally g specializes and s generalizes until g and 
s are equal to each other when these sets are equal the algorithm 
converges by means of reaching the target concept 
 disjunctive cea 
unfortunately cea is primarily targeted for conjunctive 
concepts on the other hand we need to learn disjunctive concepts in 
the negotiation of a service since consumer may have several 
alternative wishes there are several studies on learning disjunctive 
concepts via version space some of these approaches use multiple 
version space for instance hong et al maintain several version 
spaces by split and merge operation to be able to learn 
disjunctive concepts they create new version spaces by examining the 
consistency between g and s 
we deal with the problem of not supporting disjunctive concepts 
of cea by extending our hypothesis language to include 
disjunctive hypothesis in addition to the conjunctives and negation each 
attribute of the hypothesis has two parts inclusive list which holds 
the list of valid values for that attribute and exclusive list which is 
the list of values which cannot be taken for that feature 
example assume that the most specific set is { light 
delicate red } and a positive example light delicate white comes 
the original cea will generalize this as light delicate 
meaning the color can take any value however in fact we only know 
that the color can be red or white in the dcea we generalize it as 
{ light delicate white red } only when all the values exist 
in the list they will be replaced by in other words we let the 
algorithm generalize more slowly than before 
we modify the cea algorithm to deal with this change the 
modified algorithm dcea is given as algorithm note that 
compared to the previous studies of disjunctive versions our 
approach uses only a single version space rather than multiple version 
space the initialization phase is the same as the original algorithm 
 lines if any positive sample comes we add the sample to the 
special set as before line however we do not eliminate the 
hypotheses in g that do not cover this sample since g now contains a 
disjunction of many hypotheses some of which will be conflicting 
with each other removing a specific hypothesis from g will result 
in loss of information since other hypotheses are not guaranteed 
to cover it after some time some hypotheses in s can be merged 
and can construct one hypothesis lines 
when a negative sample comes we do not change s as before 
we only modify the most general hypotheses not to cover this 
negative sample lines - different from the original cea we 
try to specialize the g minimally the algorithm removes the 
hypothesis covering the negative sample line then we generate 
new hypotheses as the number of all possible attributes by using 
the removed hypothesis 
for each attribute in the negative sample we add one of them 
at each time to the exclusive list of the removed hypothesis thus 
all possible hypotheses that do not cover the negative sample are 
generated line note that exclusive list contains the values that 
the attribute cannot take for example consider the color attribute 
if a hypothesis includes red in its exclusive list and in its inclusive 
list this means that color may take any value except red 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
algorithm disjunctive candidate elimination algorithm 
 g ←the set of maximally general hypotheses in h 
 s ←the set of maximally specific hypotheses in h 
 for each training example d 
 if d is a positive example then 
 add d to s 
 if s in s can be combined with d to make one element then 
 combine s and d into sd {sd is the rule covers s and d} 
 end if 
 end if 
 if d is a negative example then 
 for each hypothesis g in g does cover d 
 assume g x x xn and d d d dn 
 - remove g from g 
 - add hypotheses g g gn where g x -d x xn 
g x x -d xn and gn x x xn-dn 
 - remove from g any hypothesis that is less general than 
another hypothesis in g 
 end if 
example table illustrates the first three interactions and 
the workings of dcea the most general set and the most specific 
set show the contents of g and s after the sample comes in after 
the first positive sample s is generalized to also cover the instance 
the second sample is negative thus we replace by three 
disjunctive hypotheses each hypothesis being minimally 
specialized in this process at each time one attribute value of negative 
sample is applied to the hypothesis in the general set the third 
sample is positive and generalizes s even more 
note that in table we do not eliminate { -full } from 
the general set while having a positive sample such as full strong 
white this stems from the possibility of using this rule in the 
generation of other hypotheses for instance if the example continues 
with a negative sample full strong red we can specialize the 
previous rule such as { -full -red } by algorithm we 
do not miss any information 
 id 
id is an algorithm that constructs decision trees in a 
topdown fashion from the observed examples represented in a vector 
with attribute-value pairs applying this algorithm to our system 
with the intention of learning the consumer s preferences is 
appropriate since this algorithm also supports learning disjunctive 
concepts in addition to conjunctive concepts 
the id algorithm is used in the learning process with the 
purpose of classification of offers there are two classes positive and 
negative positive means that the service description will possibly 
be accepted by the consumer agent whereas the negative implies 
that it will potentially be rejected by the consumer consumer s 
requests are considered as positive training examples and all rejected 
counter-offers are thought as negative ones 
the decision tree has two types of nodes leaf node in which the 
class labels of the instances are held and non-leaf nodes in which 
test attributes are held the test attribute in a non-leaf node is one of 
the attributes making up the service description for instance body 
flavor color and so on are potential test attributes for wine service 
when we want to find whether the given service description is 
acceptable we start searching from the root node by examining the 
value of test attributes until reaching a leaf node 
the problem with this algorithm is that it is not an 
incremental algorithm which means all the training examples should exist 
before learning to overcome this problem the system keeps 
consumer s requests throughout the negotiation interaction as positive 
examples and all counter-offers rejected by the consumer as 
negative examples after each coming request the decision tree is 
rebuilt without doubt there is a drawback of reconstruction such 
as additional process load however in practice we have evaluated 
id to be fast and the reconstruction cost to be negligible 
 service offering 
after learning the consumer s preferences the producer needs to 
make a counter offer that is compatible with the consumer s 
preferences 
 service offering via cea and dcea 
to generate the best offer the producer agent uses its service 
ontology and the cea algorithm the service offering mechanism 
is the same for both the original cea and dcea but as explained 
before their methods for updating g and s are different 
when producer receives a request from the consumer the 
learning set of the producer is trained with this request as a positive 
sample the learning components the most specific set s and the 
most general set g are actively used in offering service the most 
general set g is used by the producer in order to avoid offering the 
services which will be rejected by the consumer agent in other 
words it filters the service set from the undesired services since 
g contains hypotheses that are consistent with the requests of the 
consumer the most specific set s is used in order to find best 
offer which is similar to the consumer s preferences since the most 
specific set s holds the previous requests and the current request 
estimating similarity between this set and every service in the 
service list is very convenient to find the best offer from the service 
list 
when the consumer starts the interaction with the producer agent 
producer agent loads all related services to the service list object 
this list constitutes the provider s inventory of services upon 
receiving a request if the producer can offer an exactly matching 
service then it does so for example for a wine this corresponds 
to selling a wine that matches the specified features of the 
consumer s request identically when the producer cannot offer the 
service as requested it tries to find the service that is most similar 
to the services that have been requested by the consumer during the 
negotiation to do this the producer has to compute the similarity 
between the services it can offer and the services that have been 
requested in s 
we compute the similarities in various ways as will be explained 
in section after the similarity of the available services with the 
current s is calculated there may be more than one service with 
the maximum similarity the producer agent can break the tie in a 
number of ways here we have associated a rating value with each 
service and the producer prefers the higher rated service to others 
 service offering via id 
if the producer learns the consumer s preferences with id a 
similar mechanism is applied with two differences first since id 
does not maintain g the list of unaccepted services that are 
classified as negative are removed from the service list second the 
similarities of possible services are not measured with respect to s 
but instead to all previously made requests 
 alternative service offering mechanisms 
in addition to these three service offering mechanisms service 
offering with cea service offering with dcea and service 
offering with id we include two other mechanisms 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 random service offering ro the producer generates a 
counter offer randomly from the available service list 
without considering the consumer s preferences 
 service offering considering only the current request scr 
the producer selects a counter offer according to the 
similarity of the consumer s current request but does not consider 
previous requests 
 similarity estimation 
similarity can be estimated with a similarity metric that takes 
two entries and returns how similar they are there are several 
similarity metrics used in case based reasoning system such as weighted 
sum of euclidean distance hamming distance and so on 
the similarity metric affects the performance of the system while 
deciding which service is the closest to the consumer s request we 
first analyze some existing metrics and then propose a new 
semantic similarity metric named rp similarity 
 tversky s similarity metric 
tversky s similarity metric compares two vectors in terms of 
the number of exactly matching features in equation 
common represents the number of matched attributes whereas 
different represents the number of the different attributes our 
current assumption is that α and β is equal to each other 
smpq 
α common 
α common β different 
 
here when two features are compared we assign zero for 
dissimilarity and one for similarity by omitting the semantic closeness 
among the feature values 
tversky s similarity metric is designed to compare two feature 
vectors in our system whereas the list of services that can be 
offered by the producer are each a feature vector the most specific 
set s is not a feature vector s consists of hypotheses of feature 
vectors therefore we estimate the similarity of each hypothesis 
inside the most specific set s and then take the average of the 
similarities 
example assume that s contains the following two 
hypothesis { {light moderate red white } {full strong rose}} 
take service s as light strong rose then the similarity of the 
first one is equal to and the second one is equal to in 
accordance with equation normally we take the average of it 
and obtain equally however the first 
hypothesis involves the effect of two requests and the second hypothesis 
involves only one request as a result we expect the effect of the 
first hypothesis to be greater than that of the second therefore 
we calculate the average similarity by considering the number of 
samples that hypotheses cover 
let ch denote the number of samples that hypothesis h covers 
and sm h service denote the similarity of hypothesis h with the 
given service we compute the similarity of each hypothesis with 
the given service and weight them with the number of samples they 
cover we find the similarity by dividing the weighted sum of the 
similarities of all hypotheses in s with the service by the number 
of all samples that are covered in s 
av g−sm service s 
 s 
 h ch ∗ sm h service 
 s 
 h ch 
 
figure sample taxonomy for similarity estimation 
example for the above example the similarity of light 
strong rose with the specific set is ∗ equally 
 the possible number of samples that a hypothesis covers can 
be estimated with multiplying cardinalities of each attribute for 
example the cardinality of the first attribute is two and the others 
is equal to one for the given hypothesis such as {light moderate 
 red white } when we multiply them we obtain two ∗ ∗ 
 
 lin s similarity metric 
a taxonomy can be used while estimating semantic similarity 
between two concepts estimating semantic similarity in a is-a 
taxonomy can be done by calculating the distance between the nodes 
related to the compared concepts the links among the nodes can 
be considered as distances then the length of the path between the 
nodes indicates how closely similar the concepts are an 
alternative estimation to use information content in estimation of semantic 
similarity rather than edge counting method was proposed by lin 
 the equation shows lin s similarity where c and c 
are the compared concepts and c is the most specific concept that 
subsumes both of them besides p c represents the probability 
of an arbitrary selected object belongs to concept c 
similarity c c 
 × log p c 
log p c log p c 
 
 wu palmer s similarity metric 
different from lin wu and palmer use the distance between the 
nodes in is-a taxonomy the semantic similarity is 
represented with equation here the similarity between c and 
c is estimated and c is the most specific concept subsuming these 
classes n is the number of edges between c and c n is the 
number of edges between c and c n is the number of is-a 
links of c from the root of the taxonomy 
simw u p almer c c 
 × n 
n n × n 
 
 rp semantic metric 
we propose to estimate the relative distance in a taxonomy 
between two concepts using the following intuitions we use figure 
 to illustrate these intuitions 
 parent versus grandparent parent of a node is more 
similar to the node than grandparents of that generalization of 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a concept reasonably results in going further away that 
concept the more general concepts are the less similar they 
are for example anywinecolor is parent of reddishcolor 
and reddishcolor is parent of red then we expect the 
similarity between reddishcolor and red to be higher than that 
of the similarity between anywinecolor and red 
 parent versus sibling a node would have higher similarity 
to its parent than to its sibling for instance red and rose 
are children of reddishcolor in this case we expect the 
similarity between red and reddishcolor to be higher than 
that of red and rose 
 sibling versus grandparent a node is more similar to it s 
sibling then to its grandparent to illustrate anywinecolor 
is grandparent of red and red and rose are siblings 
therefore we possibly anticipate that red and rose are more 
similar than anywinecolor and red 
as a taxonomy is represented in a tree that tree can be traversed 
from the first concept being compared through the second concept 
at starting node related to the first concept the similarity value is 
constant and equal to one this value is diminished by a constant 
at each node being visited over the path that will reach to the node 
including the second concept the shorter the path between the 
concepts the higher the similarity between nodes 
algorithm estimate-rp-similarity c c 
require the constants should be m n m 
where m n ∈ 
r 
 similarity ← 
 if c is equal to c then 
 return similarity 
 end if 
 commonparent ← findcommonparent c c 
{commonparent is the most specific concept that covers 
both c and c } 
 n ← finddistance commonparent c 
 n ← finddistance commonparent c {n n are 
the number of links between the concept and parent concept} 
 if commonparent c or commonparent c then 
 similarity ← similarity ∗ m n n 
 else 
 similarity ← similarity ∗ n ∗ m n n − 
 end if 
 return similarity 
relative distance between nodes c and c is estimated in the 
following way starting from c the tree is traversed to reach c 
at each hop the similarity decreases since the concepts are getting 
farther away from each other however based on our intuitions 
not all hops decrease the similarity equally 
let m represent the factor for hopping from a child to a parent 
and n represent the factor for hopping from a sibling to another 
sibling since hopping from a node to its grandparent counts as 
two parent hops the discount factor of moving from a node to its 
grandparent is m 
 according to the above intuitions our constants 
should be in the form m n m 
where the value of m and n 
should be between zero and one algorithm shows the distance 
calculation 
according to the algorithm firstly the similarity is initialized 
with the value of one line if the concepts are equal to each other 
then similarity will be one lines - otherwise we compute the 
common parent of the two nodes and the distance of each concept 
to the common parent without considering the sibling lines - 
if one of the concepts is equal to the common parent then there 
is no sibling relation between the concepts for each level we 
multiply the similarity by m and do not consider the sibling factor 
in the similarity estimation as a result we decrease the similarity 
at each level with the rate of m line otherwise there has to be 
a sibling relation this means that we have to consider the effect of 
n when measuring similarity recall that we have counted n n 
edges between the concepts since there is a sibling relation two of 
these edges constitute the sibling relation hence when calculating 
the effect of the parent relation we use n n − edges line 
some similarity estimations related to the taxonomy in figure 
are given in table in this example m is taken as and n is 
taken as 
table sample similarity estimation over sample taxonomy 
similarity reddishcolor rose ∗ 
similarity red rose ∗ 
similarity anyw inecolor rose ∗ 
 
similarity w hite rose ∗ ∗ 
for all semantic similarity metrics in our architecture the 
taxonomy for features is held in the shared ontology in order to evaluate 
the similarity of feature vector we firstly estimate the similarity for 
feature one by one and take the average sum of these similarities 
then the result is equal to the average semantic similarity of the 
entire feature vector 
 developed system 
we have implemented our architecture in java to ease testing 
of the system the consumer agent has a user interface that allows 
us to enter various requests the producer agent is fully automated 
and the learning and service offering operations work as explained 
before in this section we explain the implementation details of the 
developed system 
we use owl as our ontology language and jena as our 
ontology reasoner the shared ontology is the modified version of 
the wine ontology it includes the description of wine as a 
concept and different types of wine all participants of the 
negotiation use this ontology for understanding each other according 
to the ontology seven properties make up the wine concept the 
consumer agent and the producer agent obtain the possible values 
for the these properties by querying the ontology thus all 
possible values for the components of the wine concept such as color 
body sugar and so on can be reached by both agents also a 
variety of wine types are described in this ontology such as burgundy 
chardonnay cheninblanc and so on intuitively any wine type 
described in the ontology also represents a wine concept this allows 
us to consider instances of chardonnay wine as instances of wine 
class 
in addition to wine description the hierarchical information of 
some features can be inferred from the ontology for instance 
we can represent the information europe continent covers 
western country western country covers french region which covers 
some territories such as loire bordeaux and so on this 
hierarchical information is used in estimation of semantic similarity in this 
part some reasoning can be made such as if a concept x covers y 
and y covers z then concept x covers z for example europe 
continent covers bordeaux 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
for some features such as body flavor and sugar there is no 
hierarchical information but their values are semantically leveled 
when that is the case we give the reasonable similarity values for 
these features for example the body can be light medium or 
strong in this case we assume that light is similar to medium 
but only to strong 
winestock ontology is the producer s inventory and describes 
a product class as wineproduct this class is necessary for the 
producer to record the wines that it sells ontology involves the 
individuals of this class the individuals represent available services 
that the producer owns we have prepared two separate winestock 
ontologies for testing in the first ontology there are available 
wine products and in the second ontology there are products 
 performance evaluation 
we evaluate the performance of the proposed systems in respect 
to learning technique they used dcea and id by comparing 
them with the cea ro for random offering and scr offering 
based on current request only 
we apply a variety of scenarios on this dataset in order to see the 
performance differences each test scenario contains a list of 
preferences for the user and number of matches from the product list 
table shows these preferences and availability of those products 
in the inventory for first five scenarios note that these preferences 
are internal to the consumer and the producer tries to learn these 
during negotiation 
table availability of wines in different test scenarios 
id preference of consumer availability out of 
 dry wine 
 red and dry wine 
 red dry and moderate wine 
 red and strong wine 
 red or rose and strong 
 comparison of learning algorithms 
in comparison of learning algorithms we use the five scenarios 
in table here first we use tversky s similarity measure with 
these test cases we are interested in finding the number of 
iterations that are required for the producer to generate an acceptable 
offer for the consumer since the performance also depends on the 
initial request we repeat our experiments with different initial 
requests consequently for each case we run the algorithms five 
times with several variations of the initial requests in each 
experiment we count the number of iterations that were needed to reach 
an agreement we take the average of these numbers in order to 
evaluate these systems fairly as is customary we test each 
algorithm with the same initial requests 
table compares the approaches using different learning 
algorithm when the large parts of inventory is compatible with the 
customer s preferences as in the first test case the performance of 
all techniques are nearly same e g scenario as the number of 
compatible services drops ro performs poorly as expected the 
second worst method is scr since it only considers the customer s 
most recent request and does not learn from previous requests 
cea gives the best results when it can generate an answer but 
cannot handle the cases containing disjunctive preferences such as the 
one in scenario id and dcea achieve the best results their 
performance is comparable and they can handle all cases including 
scenario 
table comparison of learning algorithms in terms of average 
number of interactions 
run dcea scr ro cea id 
scenario 
scenario 
scenario 
scenario 
scenario no offer 
avg of all cases no offer 
 comparison of similarity metrics 
to compare the similarity metrics that were explained in 
section we fix the learning algorithm to dcea in addition to the 
scenarios shown in table we add following five new scenarios 
considering the hierarchical information 
 the customer wants to buy wine whose winery is located in 
california and whose grape is a type of white grape 
moreover the winery of the wine should not be expensive there 
are only four products meeting these conditions 
 the customer wants to buy wine whose color is red or rose 
and grape type is red grape in addition the location of wine 
should be in europe the sweetness degree is wished to be 
dry or off dry the flavor should be delicate or moderate 
where the body should be medium or light furthermore the 
winery of the wine should be an expensive winery there are 
two products meeting all these requirements 
 the customer wants to buy moderate rose wine which is 
located around french region the category of winery should 
be moderate winery there is only one product meeting 
these requirements 
 the customer wants to buy expensive red wine which is 
located around california region or cheap white wine which 
is located in around texas region there are five available 
products 
 the customer wants to buy delicate white wine whose 
producer in the category of expensive winery there are two 
available products 
the first seven scenarios are tested with the first dataset that 
contains a total of services and the last three scenarios are tested 
with the second dataset that contains services 
table gives the performance evaluation in terms of the number 
of interactions needed to reach a consensus tversky s metric gives 
the worst results since it does not consider the semantic similarity 
lin s performance are better than tversky but worse than others 
wu palmer s metric and rp similarity measure nearly give the same 
performance and better than others when the results are examined 
considering semantic closeness increases the performance 
 discussion 
we review the recent literature in comparison to our work tama 
et al propose a new approach based on ontology for 
negotiation according to their approach the negotiation protocols used 
in e-commerce can be modeled as ontologies thus the agents can 
perform negotiation protocol by using this shared ontology without 
the need of being hard coded of negotiation protocol details while 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
table comparison of similarity metrics in terms of number 
of interactions 
run tversky lin wu palmer rp 
scenario 
scenario 
scenario 
scenario 
scenario 
scenario 
scenario 
scenario 
scenario 
scenario 
average of all cases 
tama et al model the negotiation protocol using ontologies we 
have instead modeled the service to be negotiated further we have 
built a system with which negotiation preferences can be learned 
sadri et al study negotiation in the context of resource 
allocation agents have limited resources and need to require 
missing resources from other agents a mechanism which is based on 
dialogue sequences among agents is proposed as a solution the 
mechanism relies on observe-think-action agent cycle these 
dialogues include offering resources resource exchanges and offering 
alternative resource each agent in the system plans its actions to 
reach a goal state contrary to our approach sadri et al s study is 
not concerned with learning preferences of each other 
brzostowski and kowalczyk propose an approach to select an 
appropriate negotiation partner by investigating previous multi-attribute 
negotiations for achieving this they use case-based reasoning 
their approach is probabilistic since the behavior of the partners 
can change at each iteration in our approach we are interested in 
negotiation the content of the service after the consumer and 
producer agree on the service price-oriented negotiation mechanisms 
can be used to agree on the price 
fatima et al study the factors that affect the negotiation such as 
preferences deadline price and so on since the agent who 
develops a strategy against its opponent should consider all of them 
in their approach the goal of the seller agent is to sell the service 
for the highest possible price whereas the goal of the buyer agent 
is to buy the good with the lowest possible price time interval 
affects these agents differently compared to fatima et al our focus 
is different while they study the effect of time on negotiation our 
focus is on learning preferences for a successful negotiation 
faratin et al propose a multi-issue negotiation mechanism where 
the service variables for the negotiation such as price quality of 
the service and so on are considered traded-offs against each other 
 i e higher price for earlier delivery they generate a 
heuristic model for trade-offs including fuzzy similarity estimation and a 
hill-climbing exploration for possibly acceptable offers although 
we address a similar problem we learn the preferences of the 
customer by the help of inductive learning and generate counter-offers 
in accordance with these learned preferences faratin et al only 
use the last offer made by the consumer in calculating the 
similarity for choosing counter offer unlike them we also take into 
account the previous requests of the consumer in their experiments 
faratin et al assume that the weights for service variables are fixed 
a priori on the contrary we learn these preferences over time 
in our future work we plan to integrate ontology reasoning into 
the learning algorithm so that hierarchical information can be learned 
from subsumption hierarchy of relations further by using 
relationships among features the producer can discover new 
knowledge from the existing knowledge these are interesting directions 
that we will pursue in our future work 
 references 
 j brzostowski and r kowalczyk on possibilistic 
case-based reasoning for selecting partners for 
multi-attribute agent negotiation in proceedings of the th 
intl joint conference on autonomous agents and 
multiagent systems aamas pages - 
 l busch and i horstman a comment on issue-by-issue 
negotiations games and economic behavior - 
 
 j k debenham managing e-market negotiation in context 
with a multiagent system in proceedings st international 
conference on knowledge based systems and applied 
artificial intelligence es 
 p faratin c sierra and n r jennings using similarity 
criteria to make issue trade-offs in automated negotiations 
artificial intelligence - 
 s fatima m wooldridge and n jennings optimal agents 
for multi-issue negotiation in proceeding of the nd intl 
joint conference on autonomous agents and multiagent 
systems aamas pages - 
 c giraud-carrier a note on the utility of incremental 
learning ai communications - 
 t -p hong and s -s tseng splitting and merging version 
spaces to learn disjunctive concepts ieee transactions on 
knowledge and data engineering - 
 d lin an information-theoretic definition of similarity in 
proc th international conf on machine learning pages 
 - morgan kaufmann san francisco ca 
 p maes r h guttman and a g moukas agents that buy 
and sell communications of the acm - 
 t m mitchell machine learning mcgraw hill ny 
 owl owl web ontology language guide 
http www w org tr cr-owl-guide- 
 s k pal and s c k shiu foundations of soft case-based 
reasoning john wiley sons new jersey 
 j r quinlan induction of decision trees machine learning 
 - 
 f sadri f toni and p torroni dialogues for negotiation 
agent varieties and dialogue sequences in atal 
revised papers volume of lnai pages - 
springer-verlag 
 m p singh value-oriented electronic commerce ieee 
internet computing - 
 v tamma s phelps i dickinson and m wooldridge 
ontologies for supporting negotiation in e-commerce 
engineering applications of artificial intelligence 
 - 
 a tversky features of similarity psychological review 
 - 
 p e utgoff incremental induction of decision trees 
machine learning - 
 wine 
http www w org tr cr-owl-guide wine rdf 
 z wu and m palmer verb semantics and lexical selection 
in nd annual meeting of the association for 
computational linguistics pages - 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
letting loose a spider on a network of pomdps 
generating quality guaranteed policies 
pradeep varakantham janusz marecki yuichi yabu∗ 
 milind tambe makoto yokoo∗ 
university of southern california los angeles ca {varakant marecki tambe} usc edu 
∗ dept of intelligent systems kyushu university fukuoka - japan yokoo is kyushu-u ac jp 
abstract 
distributed partially observable markov decision problems 
 distributed pomdps are a popular approach for modeling multi-agent 
systems acting in uncertain domains given the significant 
complexity of solving distributed pomdps particularly as we scale 
up the numbers of agents one popular approach has focused on 
approximate solutions though this approach is efficient the 
algorithms within this approach do not provide any guarantees on 
solution quality a second less popular approach focuses on global 
optimality but typical results are available only for two agents 
and also at considerable computational cost this paper overcomes 
the limitations of both these approaches by providing spider a 
novel combination of three key features for policy generation in 
distributed pomdps i it exploits agent interaction structure given 
a network of agents i e allowing easier scale-up to larger number 
of agents ii it uses a combination of heuristics to speedup policy 
search and iii it allows quality guaranteed approximations 
allowing a systematic tradeoff of solution quality for time 
experimental results show orders of magnitude improvement in performance 
when compared with previous global optimal algorithms 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligencemulti-agent systems 
general terms 
algorithms theory 
 introduction 
distributed partially observable markov decision problems 
 distributed pomdps are emerging as a popular approach for 
modeling sequential decision making in teams operating under 
uncertainty the uncertainty arises on account of 
nondeterminism in the outcomes of actions and because the world state 
may only be partially or incorrectly observable unfortunately as 
shown by bernstein et al the problem of finding the optimal 
joint policy for general distributed pomdps is nexp-complete 
researchers have attempted two different types of approaches 
towards solving these models the first category consists of highly 
efficient approximate techniques that may not reach globally 
optimal solutions the key problem with these techniques 
has been their inability to provide any guarantees on the quality 
of the solution in contrast the second less popular category of 
approaches has focused on a global optimal result though 
these approaches obtain optimal solutions they typically consider 
only two agents furthermore they fail to exploit structure in the 
interactions of the agents and hence are severely hampered with 
respect to scalability when considering more than two agents 
to address these problems with the existing approaches we 
propose approximate techniques that provide guarantees on the 
quality of the solution while focussing on a network of more than two 
agents we first propose the basic spider search for policies 
in distributed environments algorithm there are two key novel 
features in spider i it is a branch and bound heuristic search 
technique that uses a mdp-based heuristic function to search for an 
optimal joint policy ii it exploits network structure of agents by 
organizing agents into a depth first search dfs pseudo tree and 
takes advantage of the independence in the different branches of the 
dfs tree we then provide three enhancements to improve the 
efficiency of the basic spider algorithm while providing guarantees 
on the quality of the solution the first enhancement uses 
abstractions for speedup but does not sacrifice solution quality in 
particular it initially performs branch and bound search on abstract 
policies and then extends to complete policies the second 
enhancement obtains speedups by sacrificing solution quality but within 
an input parameter that provides the tolerable expected value 
difference from the optimal solution the third enhancement is again 
based on bounding the search for efficiency however with a 
tolerance parameter that is provided as a percentage of optimal 
we experimented with the sensor network domain presented in 
nair et al a domain representative of an important class of 
problems with networks of agents working in uncertain 
environments in our experiments we illustrate that spider dominates 
an existing global optimal approach called goa the only 
known global optimal algorithm with demonstrated experimental 
results for more than two agents furthermore we demonstrate 
that abstraction improves the performance of spider significantly 
 while providing optimal solutions we finally demonstrate a key 
feature of spider by utilizing the approximation enhancements 
it enables principled tradeoffs in run-time versus solution quality 
 
 - - - - rps c ifaamas 
 domain distributed sensor nets 
distributed sensor networks are a large important class of 
domains that motivate our work this paper focuses on a set of target 
tracking problems that arise in certain types of sensor networks 
first introduced in figure shows a specific problem instance 
within this type consisting of three sensors here each sensor node 
can scan in one of four directions north south east or west see 
figure to track a target and obtain associated reward two 
sensors with overlapping scanning areas must coordinate by scanning 
the same area simultaneously in figure to track a target in 
loc sensor needs to scan  east and sensor needs to scan  west 
simultaneously thus sensors have to act in a coordinated fashion 
we assume that there are two independent targets and that each 
target s movement is uncertain and unaffected by the sensor agents 
based on the area it is scanning each sensor receives observations 
that can have false positives and false negatives the sensors 
observations and transitions are independent of each other s actions 
e g the observations that sensor receives are independent of 
sensor s actions each agent incurs a cost for scanning whether the 
target is present or not but no cost if it turns off given the sensors 
observational uncertainty the targets uncertain transitions and the 
distributed nature of the sensor nodes these sensor nets provide a 
useful domains for applying distributed pomdp models 
figure a -chain sensor configuration 
 background 
 model network distributed pomdp 
the nd-pomdp model was introduced in motivated by 
domains such as the sensor networks introduced in section it is 
defined as the tuple s a p ω o r b where s × ≤i≤nsi × 
su is the set of world states si refers to the set of local states of 
agent i and su is the set of unaffectable states unaffectable state 
refers to that part of the world state that cannot be affected by the 
agents actions e g environmental factors like target locations that 
no agent can control a × ≤i≤nai is the set of joint actions 
where ai is the set of action for agent i 
nd-pomdp assumes transition independence where the 
transition function is defined as p s a s pu su su · ≤i≤n 
pi si su ai si where a a an is the joint action 
performed in state s s sn su and s s sn su is 
the resulting state 
ω × ≤i≤nωi is the set of joint observations where ωi is 
the set of observations for agents i observational independence 
is assumed in nd-pomdps i e the joint observation function is 
defined as o s a ω ≤i≤n oi si su ai ωi where s 
s sn su is the world state that results from the agents 
performing a a an in the previous state and 
ω ω ωn ∈ ω is the observation received in state s this 
implies that each agent s observation depends only on the 
unaffectable state its local action and on its resulting local state 
the reward function r is defined as 
r s a l rl sl slr su al alr where each l 
could refer to any sub-group of agents and r l based on 
the reward function an interaction hypergraph is constructed a 
hyper-link l exists between a subset of agents for all rl that 
comprise r the interaction hypergraph is defined as g ag e 
where the agents ag are the vertices and e {l l ⊆ ag ∧ 
rl is a component of r} are the edges 
the initial belief state distribution over the initial state b is 
defined as b s bu su · ≤i≤n bi si where bu and bi refer 
to the distribution over initial unaffectable state and agent i s initial 
belief state respectively the goal in nd-pomdp is to compute 
the joint policy π π πn that maximizes team s expected 
reward over a finite horizon t starting from the belief state b 
an nd-pomdp is similar to an n-ary distributed constraint 
optimization problem dcop where the variable at each 
node represents the policy selected by an individual agent πi with 
the domain of the variable being the set of all local policies πi 
the reward component rl where l can be thought of as a 
local constraint while the reward component rl where l 
corresponds to a non-local constraint in the constraint graph 
 algorithm global optimal algorithm goa 
in previous work goa has been defined as a global optimal 
algorithm for nd-pomdps we will use goa in our 
experimental comparisons since goa is a state-of-the-art global optimal 
algorithm and in fact the only one with experimental results 
available for networks of more than two agents goa borrows from a 
global optimal dcop algorithm called dpop goa s message 
passing follows that of dpop the first phase is the util 
propagation where the utility messages in this case values of policies 
are passed up from the leaves to the root value for a policy at an 
agent is defined as the sum of best response values from its 
children and the joint policy reward associated with the parent policy 
thus given a policy for a parent node goa requires an agent to 
iterate through all its policies finding the best response policy and 
returning the value to the parent - while at the parent node to find 
the best policy an agent requires its children to return their best 
responses to each of its policies this util propagation process 
is repeated at each level in the tree until the root exhausts all its 
policies in the second phase of value propagation where the 
optimal policies are passed down from the root till the leaves 
goa takes advantage of the local interactions in the interaction 
graph by pruning out unnecessary joint policy evaluations 
 associated with nodes not connected directly in the tree since the 
interaction graph captures all the reward interactions among agents 
and as this algorithm iterates through all the relevant joint policy 
evaluations this algorithm yields a globally optimal solution 
 spider 
as mentioned in section an nd-pomdp can be treated as a 
dcop where the goal is to compute a joint policy that maximizes 
the overall joint reward the brute-force technique for computing 
an optimal policy would be to examine the expected values for all 
possible joint policies the key idea in spider is to avoid 
computation of expected values for the entire space of joint policies by 
utilizing upper bounds on the expected values of policies and the 
interaction structure of the agents 
akin to some of the algorithms for dcop spider has a 
pre-processing step that constructs a dfs tree corresponding to the 
given interaction structure note that these dfs trees are pseudo 
trees that allow links between ancestors and children we 
employ the maximum constrained node mcn heuristic used in the 
dcop algorithm adopt however other heuristics such as 
mlsp heuristic from can also be employed mcn heuristic 
tries to place agents with more number of constraints at the top of 
the tree this tree governs how the search for the optimal joint 
polthe sixth intl joint conf on autonomous agents and multi-agent systems aamas 
icy proceeds in spider the algorithms presented in this paper are 
easily extendable to hyper-trees however for expository purposes 
we assume binary trees 
spider is an algorithm for centralized planning and distributed 
execution in distributed pomdps in this paper we employ the 
following notation to denote policies and expected values 
ancestors i ⇒ agents from i to the root not including i 
tree i ⇒ agents in the sub-tree not including i for which i is 
the root 
πroot 
⇒ joint policy of all agents 
πi 
⇒ joint policy of all agents in tree i ∪ i 
πi− 
⇒ joint policy of agents that are in ancestors i 
πi ⇒ policy of the ith agent 
ˆv πi πi− 
 ⇒ upper bound on the expected value for πi 
given πi 
and policies of ancestor agents i e πi− 
 
ˆvj πi πi− 
 ⇒ upper bound on the expected value for πi 
from the 
jth child 
v πi πi− 
 ⇒ expected value for πi given policies of ancestor agents 
πi− 
 
v πi 
 πi− 
 ⇒ expected value for πi 
given policies of ancestor 
agents πi− 
 
vj πi 
 πi− 
 ⇒ expected value for πi 
from the jth child 
figure execution of spider an example 
 outline of spider 
spider is based on the idea of branch and bound search where 
the nodes in the search tree represent partial complete joint 
policies figure shows an example search tree for the spider 
algorithm using an example of the three agent chain before spider 
begins its search we create a dfs tree i e pseudo tree from the 
three agent chain with the middle agent as the root of this tree 
spider exploits the structure of this dfs tree while engaging in 
its search note that in our example figure each agent is assigned 
a policy with t thus each rounded rectange search tree node 
indicates a partial complete joint policy a rectangle indicates an 
agent and the ovals internal to an agent show its policy heuristic 
or actual expected value for a joint policy is indicated in the top 
right corner of the rounded rectangle if the number is italicized 
and underlined it implies that the actual expected value of the joint 
policy is provided 
spider begins with no policy assigned to any of the agents 
 shown in the level of the search tree level of the search tree 
indicates that the joint policies are sorted based on upper bounds 
computed for root agent s policies level shows one spider 
search node with a complete joint policy a policy assigned to each 
of the agents the expected value for this joint policy is used to 
prune out the nodes in level the ones with upper bounds 
when creating policies for each non-leaf agent i spider 
potentially performs two steps 
 obtaining upper bounds and sorting in this step agent i 
computes upper bounds on the expected values ˆv πi πi− 
 of the 
joint policies πi 
corresponding to each of its policy πi and fixed 
ancestor policies an mdp based heuristic is used to compute these 
upper bounds on the expected values detailed description about 
this mdp heuristic is provided in section all policies of agent 
i πi are then sorted based on these upper bounds also referred to 
as heuristic values henceforth in descending order exploration of 
these policies in step below are performed in this descending 
order as indicated in the level of the search tree of figure all 
the joint policies are sorted based on the heuristic values indicated 
in the top right corner of each joint policy the intuition behind 
sorting and then exploring policies in descending order of upper 
bounds is that the policies with higher upper bounds could yield 
joint policies with higher expected values 
 exploration and pruning exploration implies computing 
the best response joint policy πi ∗ 
corresponding to fixed 
ancestor policies of agent i πi− 
 this is performed by iterating through 
all policies of agent i i e πi and summing two quantities for each 
policy i the best response for all of i s children obtained by 
performing steps and at each of the child nodes ii the expected 
value obtained by i for fixed policies of ancestors thus 
exploration of a policy πi yields actual expected value of a joint policy 
πi 
represented as v πi 
 πi− 
 the policy with the highest 
expected value is the best response policy 
pruning refers to avoiding exploring all policies or computing 
expected values at agent i by using the current best expected value 
vmax 
 πi 
 πi− 
 henceforth this vmax 
 πi 
 πi− 
 will be referred 
to as threshold a policy πi need not be explored if the upper 
bound for that policy ˆv πi πi− 
 is less than the threshold this is 
because the expected value for the best joint policy attainable for 
that policy will be less than the threshold 
on the other hand when considering a leaf agent spider 
computes the best response policy and consequently its expected value 
corresponding to fixed policies of its ancestors πi− 
 this is 
accomplished by computing expected values for each of the policies 
 corresponding to fixed policies of ancestors and selecting the highest 
expected value policy in figure spider assigns best response 
policies to leaf agents at level the policy for the left leaf agent is 
to perform action east at each time step in the policy while the 
policy for the right leaf agent is to perform off at each time step 
these best response policies from the leaf agents yield an actual 
expected value of for the complete joint policy 
algorithm provides the pseudo code for spider this 
algorithm outputs the best joint policy πi ∗ 
 with an expected value 
greater than threshold for the agents in tree i lines - 
compute the best response policy of a leaf agent i while lines - 
computes the best response joint policy for agents in tree i this 
best response computation for a non-leaf agent i includes a 
sorting of policies in descending order based on heuristic values on 
line b computing best response policies at each of the 
children for fixed policies of agent i in lines - and c maintaining 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
algorithm spider i πi− 
 threshold 
 πi ∗ ← null 
 πi ← get-all-policies horizon ai ωi 
 if is-leaf i then 
 for all πi ∈ πi do 
 v πi πi− ← joint-reward πi πi− 
 if v πi πi− threshold then 
 πi ∗ ← πi 
 threshold ← v πi πi− 
 else 
 children ← children i 
 ˆπi ← upper-bound-sort i πi πi− 
 for all πi ∈ ˆπi do 
 ˜πi ← πi 
 if ˆv πi πi− threshold then 
 go to line 
 for all j ∈ children do 
 jthres ← threshold − v πi πi− − 
σk∈children k j ˆvk πi πi− 
 πj ∗ ← spider j πi πi− jthres 
 ˜πi ← ˜πi πj ∗ 
 ˆvj πi πi− ← v πj ∗ πi πi− 
 if v ˜πi πi− threshold then 
 threshold ← v ˜πi πi− 
 πi ∗ ← ˜πi 
 return πi ∗ 
algorithm upper-bound-sort i πi πi− 
 
 children ← children i 
 ˆπi ← null stores the sorted list 
 for all πi ∈ πi do 
 ˆv πi πi− ← joint-reward πi πi− 
 for all j ∈ children do 
 ˆvj πi πi− ← upper-bound i j πi πi− 
 ˆv πi πi− 
 
← ˆvj πi πi− 
 ˆπi ← insert-into-sorted πi ˆπi 
 return ˆπi 
best expected value joint policy in lines - 
algorithm provides the pseudo code for sorting policies based 
on the upper bounds on the expected values of joint policies 
expected value for an agent i consists of two parts value obtained 
from ancestors and value obtained from its children line 
computes the expected value obtained from ancestors of the agent 
 using joint-reward function while lines - compute the 
heuristic value from the children the sum of these two parts yields an 
upper bound on the expected value for agent i and line of the 
algorithm sorts the policies based on these upper bounds 
 mdp based heuristic function 
the heuristic function quickly provides an upper bound on the 
expected value obtainable from the agents in tree i the 
subtree of agents is a distributed pomdp in itself and the idea here 
is to construct a centralized mdp corresponding to the sub-tree 
distributed pomdp and obtain the expected value of the optimal 
policy for this centralized mdp to reiterate this in terms of the 
agents in dfs tree interaction structure we assume full 
observability for the agents in tree i and for fixed policies of the agents in 
{ancestors i ∪ i} we compute the joint value ˆv πi 
 πi− 
 
we use the following notation for presenting the equations for 
computing upper bounds heuristic values for agents i and k 
let ei− 
denote the set of links between agents in {ancestors i ∪ 
i} and tree i ei 
denote the set of links between agents in 
tree i also if l ∈ ei− 
 then l is the agent in {ancestors i ∪ 
i} and l is the agent in tree i that l connects together we first 
compact the standard notation 
ot 
k ok st 
k st 
u πk ωt 
k ωt 
k 
pt 
k pk st 
k st 
u πk ωt 
k st 
k · ot 
k 
pt 
u p st 
u st 
u 
st 
l st 
l 
 st 
l 
 st 
u ωt 
l ωt 
l 
 ωt 
l 
rt 
l rl st 
l πl 
 ωt 
l 
 πl 
 ωt 
l 
 
vt 
l v t 
πl 
 st 
l st 
u ωt 
l 
 ωt 
l 
 
depending on the location of agent k in the agent tree we have the following 
cases 
if k ∈ {ancestors i ∪ i} ˆpt 
k pt 
k 
if k ∈ tree i ˆpt 
k pk st 
k st 
u πk ωt 
k st 
k 
if l ∈ ei− 
 ˆrt 
l max 
{al 
} 
rl st 
l πl 
 ωt 
l 
 al 
 
if l ∈ ei 
 ˆrt 
l max 
{al 
 al 
} 
rl st 
l al 
 al 
 
the value function for an agent i executing the joint policy πi 
at 
time η − is provided by the equation 
v η− 
πi sη− 
 ωη− 
 l∈ei− vη− 
l l∈ei vη− 
l 
where vη− 
l rη− 
l ω 
η 
l 
 sη pη− 
l 
pη− 
l 
pη− 
u vη 
l 
algorithm upper-bound i j πj− 
 
 val ← 
 for all l ∈ ej− ∪ ej do 
 if l ∈ ej− then πl 
← φ 
 for all s 
l do 
 val 
 
← startbel s 
l · upper-bound-time 
 i s 
l j πl 
 
 return val 
algorithm upper-bound-time i st 
l j πl ωt 
l 
 
 maxv al ← −∞ 
 for all al 
 al 
do 
 if l ∈ ei− and l ∈ ej− then al 
← πl 
 ωt 
l 
 
 val ← get-reward st 
l al 
 al 
 
 if t πi horizon − then 
 for all st 
l ωt 
l 
do 
 futv al←pt 
u ˆpt 
l 
ˆpt 
l 
 futv al 
∗ 
← upper-bound-time st 
l j πl 
 ωt 
l 
ωt 
l 
 
 val 
 
← futv al 
 if val maxv al then maxv al ← val 
 return maxv al 
upper bound on the expected value for a link is computed by 
modifying the equation to reflect the full observability 
assumption this involves removing the observational probability term 
for agents in tree i and maximizing the future value ˆvη 
l over the 
actions of those agents in tree i thus the equation for the 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
computation of the upper bound on a link l is as follows 
if l ∈ ei− 
 ˆvη− 
l ˆrη− 
l max 
al 
ω 
η 
l 
 s 
η 
l 
ˆpη− 
l 
ˆpη− 
l 
pη− 
u ˆvη 
l 
if l ∈ ei 
 ˆvη− 
l ˆrη− 
l max 
al 
 al 
s 
η 
l 
ˆpη− 
l 
ˆpη− 
l 
pη− 
u ˆvη 
l 
algorithm and algorithm provide the algorithm for computing 
upper bound for child j of agent i using the equations descirbed 
above while algorithm computes the upper bound on a link 
given the starting state algorithm sums the upper bound values 
computed over each of the links in ei− 
∪ ei 
 
 abstraction 
algorithm spider-abs i πi− 
 threshold 
 πi ∗ ← null 
 πi ← get-policies 
 if is-leaf i then 
 for all πi ∈ πi do 
 absheuristic ← get-abs-heuristic πi πi− 
 absheuristic 
∗ 
← timehorizon − πi horizon 
 if πi horizon timehorizon and πi absnodes then 
 v πi πi− ← joint-reward πi πi− 
 if v πi πi− threshold then 
 πi ∗ ← πi threshold ← v πi πi− 
 else if v πi πi− absheuristic threshold then 
 ˆπi ← extend-policy πi πi absnodes 
 πi 
 
← insert-sorted-policies ˆπi 
 remove πi 
 else 
 children ← children i 
 πi ← upper-bound-sort i πi πi− 
 for all πi ∈ πi do 
 ˜πi ← πi 
 absheuristic ← get-abs-heuristic πi πi− 
 absheuristic 
∗ 
← timehorizon − πi horizon 
 if πi horizon timehorizon and πi absnodes then 
 if ˆv πi πi− threshold and πi absnodes then 
 go to line 
 for all j ∈ children do 
 jthres ← threshold − v πi πi− − 
σk∈children k j ˆvk πi πi− 
 πj ∗ ← spider j πi πi− jthres 
 ˜πi ← ˜πi πj ∗ ˆvj πi πi− ← v πj ∗ πi πi− 
 if v ˜πi πi− threshold then 
 threshold ← v ˜πi πi− πi ∗ ← ˜πi 
 else if ˆv πi πi− absheuristic threshold then 
 ˆπi ← extend-policy πi πi absnodes 
 πi 
 
← insert-sorted-policies ˆπi 
 remove πi 
 return πi ∗ 
in spider the exploration pruning phase can only begin after 
the heuristic or upper bound computation and sorting for the 
policies has ended we provide an approach to possibly circumvent the 
exploration of a group of policies based on heuristic computation 
for one abstract policy thus leading to an improvement in runtime 
performance without loss in solution quality the important steps 
in this technique are defining the abstract policy and how heuristic 
values are computated for the abstract policies in this paper we 
propose two types of abstraction 
 horizon based abstraction hba here the abstract policy is 
defined as a shorter horizon policy it represents a group of longer 
horizon policies that have the same actions as the abstract policy 
for times less than or equal to the horizon of the abstract policy 
in figure a a t abstract policy that performs east action 
represents a group of t policies that perform east in the first 
time step 
for hba there are two parts to heuristic computation 
 a computing the upper bound for the horizon of the abstract 
policy this is same as the heuristic computation defined by the 
getheuristic algorithm for spider however with a shorter time 
horizon horizon of the abstract policy 
 b computing the maximum possible reward that can be 
accumulated in one time step using get-abs-heuristic and 
multiplying it by the number of time steps to time horizon this 
maximum possible reward for one time step is obtained by iterating 
through all the actions of all the agents in tree i and computing 
the maximum joint reward for any joint action 
sum of a and b is the heuristic value for a hba abstract policy 
 node based abstraction nba here an abstract policy is 
obtained by not associating actions to certain nodes of the policy tree 
unlike in hba this implies multiple levels of abstraction this is 
illustrated in figure b where there are t policies that do not 
have an action for observation  tp these incomplete t 
policies are abstractions for t complete policies increased levels of 
abstraction leads to faster computation of a complete joint policy 
πroot 
and also to shorter heuristic computation and exploration 
pruning phases for nba the heuristic computation is similar to 
that of a normal policy except in cases where there is no action 
associated with policy nodes in such cases the immediate reward 
is taken as rmax maximum reward for any action 
we combine both the abstraction techniques mentioned above 
into one technique spider-abs algorithm provides the 
algorithm for this abstraction technique for computing optimal joint 
policy with spider-abs a non-leaf agent i initially examines all 
abstract t policies line and sorts them based on abstract 
policy heuristic computations line the abstraction horizon is 
gradually increased and these abstract policies are then explored 
in descending order of heuristic values and ones that have heuristic 
values less than the threshold are pruned lines - exploration 
in spider-abs has the same definition as in spider if the policy 
being explored has a horizon of policy computation which is equal 
to the actual time horizon and if all the nodes of the policy have an 
action associated with them lines - however if those 
conditions are not met then it is substituted by a group of policies that it 
represents using extend-policy function lines - 
extend-policy function is also responsible for 
initializing the horizon and absnodes of a policy absnodes 
represents the number of nodes at the last level in the policy tree 
that do not have an action assigned to them if πi absnodes 
 ωi πi horizon− 
 i e total number of policy nodes possible at 
πi horizon then πi absnodes is set to zero and πi horizon is 
increased by otherwise πi absnodes is increased by thus 
this function combines both hba and nba by using the policy 
variables horizon and absnodes before substituting the abstract 
policy with a group of policies those policies are sorted based on 
heuristic values line similar type of abstraction based best 
response computation is adopted at leaf agents lines - 
 value approximation vax 
in this section we present an approximate enhancement to 
spider called vax the input to this technique is an approximation 
parameter which determines the difference from the optimal 
solution quality this approximation parameter is used at each agent 
for pruning out joint policies the pruning mechanism in spider 
and spider-abs dictates that a joint policy be pruned only if the 
threshold is exactly greater than the heuristic value however the 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure example of abstraction for a hba horizon based abstraction and b nba node based abstraction 
idea in this technique is to prune out joint a policy if the following 
condition is satisfied threshold ˆv πi 
 πi− 
 apart from the 
pruning condition vax is the same as spider spider-abs 
in the example of figure if the heuristic value for the second 
joint policy or second search tree node in level were instead 
of then that policy could not be be pruned using spider or 
spider-abs however in vax with an approximation parameter 
of the joint policy in consideration would also be pruned this is 
because the threshold at that juncture plus the approximation 
parameter i e would have been greater than the heuristic 
value for that joint policy it can be noted from the example 
 just discussed that this kind of pruning can lead to fewer 
explorations and hence lead to an improvement in the overall run-time 
performance however this can entail a sacrifice in the quality of 
the solution because this technique can prune out a candidate 
optimal solution a bound on the error introduced by this approximate 
algorithm as a function of is provided by proposition 
 percentage approximation pax 
in this section we present the second approximation 
enhancement over spider called pax input to this technique is a 
parameter δ that represents the minimum percentage of the optimal 
solution quality that is desired output of this technique is a policy 
with an expected value that is at least δ of the optimal solution 
quality a policy is pruned if the following condition is satisfied 
threshold δ 
 
ˆv πi 
 πi− 
 like in vax the only difference 
between pax and spider spider-abs is this pruning condition 
again in figure if the heuristic value for the second search 
tree node in level were instead of then pax with an 
input parameter of would be able to prune that search tree node 
 since 
 
∗ this type of pruning leads to fewer 
explorations and hence an improvement in run-time performance while 
potentially leading to a loss in quality of the solution proposition 
provides the bound on quality loss 
 theoretical results 
proposition heuristic provided using the centralized mdp 
heuristic is admissible 
proof for the value provided by the heuristic to be admissible 
it should be an over estimate of the expected value for a joint policy 
thus we need to show that for l ∈ ei 
∪ ei− 
 ˆvt 
l ≥ vt 
l refer to 
notation in section 
we use mathematical induction on t to prove this 
base case t t − irrespective of whether l ∈ ei− 
or l ∈ 
ei 
 ˆrt 
l is computed by maximizing over all actions of the agents 
in tree i while rt 
l is computed for fixed policies of the same 
agents hence ˆrt 
l ≥ rt 
l and also ˆvt 
l ≥ vt 
l 
assumption proposition holds for t η where ≤ η t − 
we now have to prove that the proposition holds for t η − 
we show the proof for l ∈ ei− 
and similar reasoning can be 
adopted to prove for l ∈ ei 
 the heuristic value function for 
l ∈ ei− 
is provided by the following equation 
ˆvη− 
l ˆrη− 
l max 
al 
ω 
η 
l 
 s 
η 
l 
ˆpη− 
l 
ˆpη− 
l 
pη− 
u ˆvη 
l 
rewriting the rhs and using eqn in section 
 ˆrη− 
l max 
al 
ω 
η 
l 
 s 
η 
l 
pη− 
u pη− 
l 
ˆpη− 
l 
ˆvη 
l 
 ˆrη− 
l 
ω 
η 
l 
 s 
η 
l 
pη− 
u pη− 
l 
max 
al 
ˆpη− 
l 
ˆvη 
l 
since maxal 
ˆpη− 
l 
ˆvη 
l ≥ ωl 
oη− 
l 
ˆpη− 
l 
ˆvη 
l and pη− 
l 
 oη− 
l 
ˆpη− 
l 
≥ˆrη− 
l 
ω 
η 
l 
 s 
η 
l 
pη− 
u pη− 
l 
ωl 
pη− 
l 
ˆvη 
l 
since ˆvη 
l ≥ vη 
l from the assumption 
≥ˆrη− 
l 
ω 
η 
l 
 s 
η 
l 
pη− 
u pη− 
l 
ωl 
pη− 
l 
vη 
l 
since ˆrη− 
l ≥ rη− 
l by definition 
≥rη− 
l 
ω 
η 
l 
 s 
η 
l 
pη− 
u pη− 
l 
ωl 
pη− 
l 
vη 
l 
 rη− 
l 
 ω 
η 
l 
 s 
η 
l 
 
pη− 
u pη− 
l 
pη− 
l 
vη 
l vη− 
l 
thus proved 
proposition spider provides an optimal solution 
proof spider examines all possible joint policies given the 
interaction structure of the agents the only exception being when 
a joint policy is pruned based on the heuristic value thus as long 
as a candidate optimal policy is not pruned spider will return an 
optimal policy as proved in proposition the expected value for 
a joint policy is always an upper bound hence when a joint policy 
is pruned it cannot be an optimal solution 
proposition error bound on the solution quality for vax 
 implemented over spider-abs with an approximation 
parameter of is ρ where ρ is the number of leaf nodes in the dfs tree 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
proof we prove this proposition using mathematical induction 
on the depth of the dfs tree 
base case depth i e one node best response is 
computed by iterating through all policies πk a policy πk is pruned 
if ˆv πk πk− 
 threshold thus the best response policy 
computed by vax would be at most away from the optimal best 
response hence the proposition holds for the base case 
assumption proposition holds for d where ≤ depth ≤ d 
we now have to prove that the proposition holds for d 
without loss of generality lets assume that the root node of this 
tree has k children each of this children is of depth ≤ d and hence 
from the assumption the error introduced in kth child is ρk where 
ρk is the number of leaf nodes in kth child of the root therefore 
ρ k ρk where ρ is the number of leaf nodes in the tree 
in spider-abs threshold at the root agent thresspider 
k v πk 
 πk− 
 however with vax the threshold at the root 
agent will be in the worst case threshvax k v πk 
 πk− 
 − 
k ρk hence with vax a joint policy is pruned at the root 
agent if ˆv πroot πroot− 
 threshvax ⇒ ˆv πroot πroot− 
 
threshspider − k ρk − ≤ threshspider − k ρk ≤ 
threshspider − ρ hence proved 
proposition for pax implemented over spider-abs with 
an input parameter of δ the solution quality is at least δ 
 
v πroot ∗ 
 
where v πroot ∗ 
 denotes the optimal solution quality 
proof we prove this proposition using mathematical induction 
on the depth of the dfs tree 
base case depth i e one node best response is 
computed by iterating through all policies πk a policy πk is pruned 
if δ 
 
ˆv πk πk− 
 threshold thus the best response policy 
computed by pax would be at least δ 
 
times the optimal best 
response hence the proposition holds for the base case 
assumption proposition holds for d where ≤ depth ≤ d 
we now have to prove that the proposition holds for d 
without loss of generality lets assume that the root node of 
this tree has k children each of this children is of depth ≤ d 
and hence from the assumption the solution quality in the kth 
child is at least δ 
 
v πk ∗ 
 πk− 
 for pax with spider-abs 
a joint policy is pruned at the root agent if ˆv πroot πroot− 
 
k v πk ∗ 
 πk− 
 however with pax a joint policy is pruned if 
δ 
 
ˆv πroot πroot− 
 k 
δ 
 
v πk ∗ 
 πk− 
 ⇒ ˆv πroot πroot− 
 
k v πk ∗ 
 πk− 
 since the pruning condition at the root agent in 
pax is the same as the one in spider-abs there is no error 
introduced at the root agent and all the error is introduced in the 
children thus overall solution quality is at least δ 
 
of the optimal 
solution hence proved 
 experimental results 
all our experiments were conducted on the sensor network 
domain from section the five network configurations employed 
are shown in figure algorithms that we experimented with 
are goa spider spider-abs pax and vax we compare 
against goa because it is the only global optimal algorithm that 
considers more than two agents we performed two sets of 
experiments i firstly we compared the run-time performance of the 
above algorithms and ii secondly we experimented with pax and 
vax to study the tradeoff between run-time and solution quality 
experiments were terminated after seconds 
 
figure a provides run-time comparisons between the optimal 
algorithms goa spider spider-abs and the approximate 
algorithms pax of and vax δ of x-axis denotes the 
 
machine specs for all experiments intel xeon ghz processor 
 gb ram 
sensor network configuration used while y-axis indicates the 
runtime on a log-scale the time horizon of policy computation was 
 for each configuration -chain -chain -star and -star there 
are five bars indicating the time taken by goa spider 
spiderabs pax and vax goa did not terminate within the time limit 
for -star and -star configurations spider-abs dominated the 
spider and goa for all the configurations for instance in the 
 chain configuration spider-abs provides -fold speedup over 
goa and -fold speedup over spider and for the -chain 
configuration it provides -fold speedup over goa and -fold speedup 
over spider the two approximation approaches vax and pax 
provided further improvement in performance over spider-abs 
for instance in the -star configuration vax provides a -fold 
speedup and pax provides a -fold speedup over spider-abs 
figures b provides a comparison of the solution quality 
obtained using the different algorithms for the problems tested in 
figure a x-axis denotes the sensor network configuration while 
y-axis indicates the solution quality since goa spider and 
spider-abs are all global optimal algorithms the solution 
quality is the same for all those algorithms for -p configuration 
the global optimal algorithms did not terminate within the limit of 
 seconds so the bar for optimal quality indicates an upper 
bound on the optimal solution quality with both the 
approximations we obtained a solution quality that was close to the optimal 
solution quality in -chain and -star configurations it is 
remarkable that both pax and vax obtained almost the same actual 
quality as the global optimal algorithms despite the approximation 
parameter and δ for other configurations as well the loss in quality 
was less than of the optimal solution quality 
figure c provides the time to solution with pax for 
varying epsilons x-axis denotes the approximation parameter δ 
 percentage to optimal used while y-axis denotes the time taken to 
compute the solution on a log-scale the time horizon for all 
the configurations was as δ was decreased from to the 
time to solution decreased drastically for instance in the -chain 
case there was a total speedup of -fold when the δ was changed 
from to interestingly even with a low δ of the actual 
solution quality remained equal to the one obtained at 
figure d provides the time to solution for all the 
configurations with vax for varying epsilons x-axis denotes the 
approximation parameter used while y-axis denotes the time taken to 
compute the solution on a log-scale the time horizon for all the 
configurations was as was increased the time to solution 
decreased drastically for instance in the -star case there was a total 
speedup of -fold when the was changed from to again 
the actual solution quality did not change with varying epsilon 
figure sensor network configurations 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure comparison of goa spider spider-abs and vax for t on a runtime and b solution quality c time to solution for pax 
with varying percentage to optimal for t d time to solution for vax with varying epsilon for t 
 summary and related work 
this paper presents four algorithms spider spider-abs pax 
and vax that provide a novel combination of features for 
policy search in distributed pomdps i exploiting agent interaction 
structure given a network of agents i e easier scale-up to larger 
number of agents ii using branch and bound search with an mdp 
based heuristic function iii utilizing abstraction to improve 
runtime performance without sacrificing solution quality iv 
providing a priori percentage bounds on quality of solutions using pax 
and v providing expected value bounds on the quality of solutions 
using vax these features allow for systematic tradeoff of solution 
quality for run-time in networks of agents operating under 
uncertainty experimental results show orders of magnitude 
improvement in performance over previous global optimal algorithms 
researchers have typically employed two types of techniques 
for solving distributed pomdps the first set of techniques 
compute global optimal solutions hansen et al present an 
algorithm based on dynamic programming and iterated elimination of 
dominant policies that provides optimal solutions for distributed 
pomdps szer et al provide an optimal heuristic search 
method for solving decentralized pomdps this algorithm is based 
on the combination of a classical heuristic search algorithm a∗ 
and 
decentralized control theory the key differences between spider 
and maa are a enhancements to spider vax and pax 
provide for quality guaranteed approximations while maa is a 
global optimal algorithm and hence involves significant 
computational complexity b due to maa s inability to exploit 
interaction structure it was illustrated only with two agents however 
spider has been illustrated for networks of agents and c 
spider explores the joint policy one agent at a time while maa 
expands it one time step at a time simultaneously for all the agents 
the second set of techniques seek approximate policies 
emerymontemerlo et al approximate posgs as a series of one-step 
bayesian games using heuristics to approximate future value 
trading off limited lookahead for computational efficiency resulting in 
locally optimal policies with respect to the selected heuristic nair 
et al s jesp algorithm uses dynamic programming to reach a 
local optimum solution for finite horizon decentralized pomdps 
peshkin et al and bernstein et al are examples of policy 
search techniques that search for locally optimal policies though 
all the above techniques improve the efficiency of policy 
computation considerably they are unable to provide error bounds on the 
quality of the solution this aspect of quality bounds differentiates 
spider from all the above techniques 
acknowledgements this material is based upon work 
supported by the defense advanced research projects agency darpa 
through the department of the interior nbc acquisition services 
division under contract no nbchd the views and 
conclusions contained in this document are those of the authors and 
should not be interpreted as representing the official policies either 
expressed or implied of the defense advanced research projects 
agency or the u s government 
 references 
 r becker s zilberstein v lesser and c v goldman 
solving transition independent decentralized markov 
decision processes jair - 
 d s bernstein e a hansen and s zilberstein bounded 
policy iteration for decentralized pomdps in ijcai 
 d s bernstein s zilberstein and n immerman the 
complexity of decentralized control of mdps in uai 
 r emery-montemerlo g gordon j schneider and 
s thrun approximate solutions for partially observable 
stochastic games with common payoffs in aamas 
 e hansen d bernstein and s zilberstein dynamic 
programming for partially observable stochastic games in 
aaai 
 v lesser c ortiz and m tambe distributed sensor nets 
a multiagent perspective kluwer 
 r maheswaran m tambe e bowring j pearce and 
p varakantham taking dcop to the real world efficient 
complete solutions for distributed event scheduling in 
aamas 
 p j modi w shen m tambe and m yokoo an 
asynchronous complete method for distributed constraint 
optimization in aamas 
 r nair d pynadath m yokoo m tambe and s marsella 
taming decentralized pomdps towards efficient policy 
computation for multiagent settings in ijcai 
 r nair p varakantham m tambe and m yokoo 
networked distributed pomdps a synthesis of distributed 
constraint optimization and pomdps in aaai 
 l peshkin n meuleau k -e kim and l kaelbling 
learning to cooperate via policy search in uai 
 a petcu and b faltings a scalable method for multiagent 
constraint optimization in ijcai 
 d szer f charpillet and s zilberstein maa a heuristic 
search algorithm for solving decentralized pomdps in 
ijcai 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
operational semantics of multiagent interactions 
juan m serrano 
university rey juan carlos 
c tulipan s n 
madrid spain 
juanmanuel serrano urjc es 
sergio saugar 
university rey juan carlos 
c tulipan s n 
madrid spain 
sergio saugar urjc es 
abstract 
the social stance advocated by institutional frameworks and 
most multi-agent system methodologies has resulted in a 
wide spectrum of organizational and communicative 
abstractions which have found currency in several programming 
frameworks and software platforms still these tools and 
frameworks are designed to support a limited range of 
interaction capabilities that constrain developers to a fixed set 
of particular pre-defined abstractions the main hypothesis 
motivating this paper is that the variety of multi-agent 
interaction mechanisms - both organizational and 
communicative share a common semantic core in the realm of software 
architectures the paper proposes a connector-based model 
of multi-agent interactions which attempts to identify the 
essential structure underlying multi-agent interactions 
furthermore the paper also provides this model with a formal 
execution semantics which describes the dynamics of social 
interactions the proposed model is intended as the 
abstract machine of an organizational programming language 
which allows programmers to accommodate an open set of 
interaction mechanisms 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence-multi-agent systems 
general terms 
languages theory design 
 introduction 
the suitability of agent-based computing to manage the 
complex patterns of interactions naturally occurring in the 
development of large scale open systems has become one 
of its major assets over the last few years 
particularly the organizational or social stance advocated by 
institutional frameworks and most multi-agent system 
 mas methodologies provides an excellent basis to 
deal with the complexity and dynamism of the interactions 
among system components this approach has resulted in 
a wide spectrum of organizational and communicative 
abstractions such as institutions normative positions power 
relationships organizations groups scenes dialogue games 
communicative actions cas etc to effectively model the 
interaction space of mas this wealth of computational 
abstractions has found currency in several programming 
frameworks and software platforms ameli madkit 
ingenias toolkit etc which leverage multi-agent 
middlewares built upon raw acl-based interaction mechanism 
 and minimize the gap between organizational 
metamodels and target implementation languages 
still these tools and frameworks are designed to support 
a limited range of interaction capabilities that constrain 
developers to a fixed set of particular pre-defined abstractions 
the main hypothesis motivating this paper is that the 
variety of multi-agent interaction mechanisms - both 
organizational and communicative share a common semantic core 
this paper thus focuses on the fundamental building blocks 
of multi-agent interactions those which may be composed 
extended or refined in order to define more complex 
organizational or communicative types of interactions 
its first goal is to carry out a principled analysis of 
multiagent interactions departing from general features commonly 
ascribed to agent-based computing autonomy situatedness 
and sociality to approach this issue we draw on the 
notion of connector put forward within the field of software 
architectures the outcome of this analysis will be a 
connector-based model of multi-agent interactions between 
autonomous social and situated components i e agents 
attempting to identify their essential structure furthermore 
the paper also provides this model with a formal 
execution semantics which describes the dynamics of multi-agent 
 or social interactions structural operational semantics 
 sos a common technique to specify the operational 
semantics of programming languages is used for this 
purpose 
the paper is structured as follows first the major entities 
and relationships which constitute the structure of social 
interactions are introduced next the dynamics of social 
interactions will show how these entities and relationships 
evolve last relevant work in the literature is discussed 
 
 - - - - rps c ifaamas 
with respect to the proposal limitations are addressed and 
current and future work is described 
 social interaction structure 
from an architectural point of view interactions between 
software components are embodied in software connectors 
first-class entities defined on the basis of the different roles 
played by software components and the protocols that 
regulate their behaviour the roles of a connector represent 
its participants such as the caller and callee roles of an 
rpc connector or the sender and receiver roles in a 
message passing connector the attachment operation binds a 
component to the role of a given connector 
the analysis of social interactions introduced in this 
section gives rise to a new kind of social connector it refines the 
generic model in several respects attending to the features 
commonly ascribed to agent-based computing 
 according to the autonomy feature we may 
distinguish a first kind of participant i e role in a 
social interaction so-called agents basically agents are 
those software components which will be regarded as 
autonomous within the scope of the interaction 
 
 a second group of participants so-called 
environmental resources may be identified from the situatedness 
feature unlike agents resources represent those 
nonautonomous components whose state may be 
externally controlled by other components agents or 
resources within the interaction moreover the 
participation of resources in an interaction is not 
mandatory 
 last according to the sociality of agents the 
specification of social connector protocols - the glue linking 
agents among themselves and with resources will rely 
on normative concepts such as permissions obligations 
and empowerments 
besides agents resources and social protocols two other 
kinds of entities are of major relevance in our analysis of 
social interactions actions which represent the way in which 
agents alter the environmental and social state of the 
interaction and events which represent the changes in the 
interaction resulting from the performance of actions or the 
activity of environmental resources 
in the following we describe the basic entities involved in 
social interactions each kind of entity t will be specified as 
a record type t l t ln tn possibly followed by 
a number of invariants definitions and the actions affecting 
their state instances or values v of a record type t will be 
represented as v v vn t the type sett 
represents a collection of values drawn from type t the type 
queuet represents a queue of values v t waiting to be 
processed the value v in the expression v queue t 
represents the head of the queue the type enum {v vn} 
 
note that we think of the autonomy feature in a relative 
rather than absolute perspective basically this means that 
software components counting as agents in a social 
interaction may behave non-autonomously in other contexts e g 
in their interactions through human-user interfaces this 
conceptualization of agenthood resembles the way in which 
objects are understood in corba as any kind of software 
component c prolog cobol etc attached to an orb 
represents an enumeration type whose values are v 
vn given some value v t the term vl 
refers to the value 
of the field l of a record type t given some labels l l 
 the expression vl l 
is syntactic sugar for vl 
 l 
 
the special term nil will be used to represent the absence 
of proper value for an optional field so that vl 
 nil will be 
true in those cases and false otherwise the formal model 
will be illustrated with several examples drawn from the 
design of a virtual organization to aid in the management of 
university courses 
 social interactions 
social interactions shall be considered as composite 
connectors structured in terms of a tree of nested 
subinteractions let s consider an interaction representing a 
university course e g on data structures on the one 
hand this interaction is actually a complex one made up 
of lower-level interactions for instance within the scope 
of the course agents will participate in programming 
assignment groups lectures tutoring meetings examinations and 
so on assignment groups in turn may hold a number of 
assignment submissions and test requests interactions a 
test request may also be regarded as a complex interaction 
ultimately decomposed in the atomic or bottom-level 
interactions represented by communicative actions e g 
request agree refuse on the other hand courses are 
run within the scope of a particular degree e g computer 
science a higher-level interaction traversing upwards from 
a degree to its ancestors we find its faculty the university 
and finally the multi-agent community or agent society 
the community is thus the top-level interaction which 
subsumes any other kind of multi-agent interaction 
 
the organizational and communicative interaction types 
identified above clearly differ in many ways however we 
may identify four major components in all of them the 
participating agents the resources that agents manipulate 
the protocol regulating the agent activities and the 
subinteraction space accordingly we may specify the type i 
of social interactions ranged over by the meta-variable i as 
follows 
i state si ini a mem set a env set r 
sub set i prot p ch ch 
def icontext i ⇔ i ∈ isub 
 
inv iini nil ⇔ icontext nil 
act setup join create destroy 
where the member and environment fields represent the 
agents a and local resources r participating in the 
interaction the sub-interaction field its set of inner interactions 
and the protocol field the rules that govern the interaction 
 p the event channel to be described in the next 
section allows the dispatching of local events to external 
interactions the context of some interaction is defined as its 
super-interaction def so that the context of the 
toplevel interaction is nil 
the type si enum {open closing closed} represents 
the possible execution states of the interaction any 
interaction but the top-level one is set up within the context of 
another interaction by an initiator agent the initiator is 
 
in the context of this application a one-to-one mapping 
between human users and software components attached to 
the community as agents would be a right choice 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
thus a mandatory feature for any interaction different to the 
community inv the life-cycle of the interaction begins 
in the open state its sets of agent and resource participants 
initially empty vary as agents join and leave the interaction 
and as they create and destroy resources from its local 
environment eventually the interaction may come to an end 
 according to the protocol s rules or be explicitly closed by 
some agent thus prematurely disabling the activity of its 
participants the transient closing state will be described 
in the next section 
 agents 
components attach themselves as agents in social 
interactions with the purpose of achieving something the purpose 
declared by some agent when it joins an interaction shall be 
regarded as the institutional goal that it purports to satisfy 
within that context 
 the types of agents participating in 
a given interaction are primarily identified from their 
purposes for instance students are those agents participating 
in a course who purport to obtain a certificate in the course s 
subject other members of the course include lecturers and 
teaching assistants 
the type a of agents ranged over by meta-variable a is 
defined as follows 
a state sa player a purp f att queue act 
ev queue e obl set o 
def acontext i ⇔ a ∈ imem 
 a ∈ aroles ⇔ aplayer 
 a 
 i ∈ apartin ⇔ a ∈ imem ∧ a ∈ aroles 
act see 
where the purpose is represented as a well-formed boolean 
formula of a generic type f which evaluates to true if the 
purpose is satisfied and false otherwise the context of some 
agent is defined as the interaction in which it participates 
 def 
the type sa enum {playing leaving succ unsuc} 
represents the execution state of the agent its life-cycle 
begins in the playing state when its player agent joins the 
interaction or some software component is attached as an agent 
to the multi-agent system in this latter case the player 
value is nil the derived roles and partin features 
represent the roles played by the agent and the contexts in which 
these roles are played def 
 an agent may play roles 
at interactions within or outside the scope of its context for 
instance students of a course are played by student agents 
belonging to the undergraduate degree whereas lecturers 
may be played by teachers of a given department and the 
assistant role may be played by students of a ph d degree 
 both the department and the ph d degrees are modelled 
as sub-interactions of the faculty 
components will normally attempt to perform different 
actions e g to set up sub-interactions in order to satisfy 
their purposes within some interaction moreover 
components need to be aware of the current state of the interaction 
so that they will also be capable of observing certain events 
from the interaction both the visibility of the interaction 
 
thus it may or may not correspond to actual internal 
goals or intentions of the component 
 
free variables in the antecedents consequents of 
implications shall be understood as universally existentially 
quantified 
and the attempts of members are subject to the rules 
governing the interaction the attempts and events fields of 
the agent structure represent the queues of attempts to 
execute some actions act and the events e received by 
the agent which have not been observed yet an agent may 
update its event queue by seeing the state of some entity 
of the community the last field of the structure represents 
the obligations o of agents to be described later 
eventually the participation of some agent in the 
interaction will be over this may either happen when certain 
conditions are met specified by the protocol rules or when 
the agent takes the explicit decision of leaving the 
interaction in either case the final state of the agent will be 
successful if its purpose was satisfied unsuccessful 
otherwise the transient leaving state will be described in the 
next section 
 resources 
resources are software components which may represent 
different types of non-autonomous informational or 
computational entities for instance objectives topics 
assignments grades and exams are different kinds of informational 
resources created by lecturers and assistants in the context 
of the course interaction students may also create programs 
to satisfy the requirements of some assignment other types 
of computational resources put at the disposal of students 
by teachers include compilers and interpreters 
the type r of resources ranged over by meta-variable r 
can be specified by the following record type 
r cr a owners set a op set op 
def rcontext i ⇔ r ∈ ienv 
act take share give invoke 
essentially resources can be regarded as objects deployed 
in a social setting this means that resources are created 
accessed and manipulated by agents in a social interaction 
context def according to the rules specified by its 
protocol the mandatory feature creator represents the agent 
who created this resource moreover resources may have 
owners the ownership relationship between members and 
resources is considered as a normative device aimed at the 
simplification of the protocol s rules that govern the 
interaction of agents and the environment members may gain 
ownership of some resource by taking it and grant 
ownership to other agents by giving or sharing their own 
properties for instance the ownership of programs may be shared 
by several students if the assignment can be performed by 
groups of two or more students 
the last operations feature represents the interface of the 
resource consisting of a set of operations a resource is 
structured around several public operations that 
participants may invoke in accordance to the rules specified by the 
interaction s protocol the set of operations of a resource 
makes up its interface 
 protocols 
the protocol of any interaction is made up of the rules 
which govern its overall state and dynamics the present 
specification abstracts away the particular formalism used to 
specify these rules and focuses instead on several 
requirements concerning the structure and interface of protocols 
accordingly the type p of protocols ranged over by 
metathe sixth intl joint conf on autonomous agents and multi-agent systems aamas 
variable p is defined as follows 
 
p emp a × act → boolean 
perm a × act → boolean 
obl → set a × set o × set e 
monitor e → set a 
finish → boolean 
over a → boolean 
def pcontext i ⇔ p iprot 
inv pfinish ∧ s ∈ pcontext sub ⇒ sprot finish 
 pfinish ∧ a ∈ pcontext mem ⇒ pover a 
 pover a ∧ ai ∈ aroles ⇒ acontext prot over 
i ai 
 αadd ∪ {a} ⊆ pmonitor a α 
act close leave 
we demand from protocols four major kinds of functions 
firstly protocols shall include rules to identify the 
empowerments and permissions of any agent attempting to alter 
the state of the interaction e g its members the 
environment etc through the execution of some action e g join 
create etc empowerments shall be regarded as the 
institutional capabilities which some agent possesses in order 
to satisfy its purpose corresponding rules encapsulated 
by the empowered function field shall allow to determine 
whether some agent is capable to perform a given action 
over the interaction empowerments may only be exercised 
under certain circumstances - that permissions specify 
permission rules shall allow to determine whether the attempt 
of an empowered agent to perform some particular action 
is satisfied or not cf permitted field for instance the 
course s protocol specifies that the agents empowered to 
join the interaction as students are those students of the 
degree who have payed the fee established for the course s 
subject and own the certificates corresponding to its 
prerequisite subjects permission rules in turn specify that those 
students may only join the course in the admission stage 
hence even if some student has paid the fee the attempt 
to join the course will fail if the course has not entered the 
corresponding stage 
 
secondly protocols shall allow to determine the 
obligations of agents towards the interaction obligations 
represent a normative device of social enforcement fully 
compatible with the autonomy of agents used to bias their 
behaviour in a certain direction these kinds of rules shall 
allow to determine whether some agent must perform an 
action of a given type as well as if some obligation was fulfilled 
violated or needs to be revoked the function obligations of 
the protocol structure thus identifies the agents whose 
obligation set must be updated moreover it returns for each 
agent a collection of events representing the changes in the 
obligation set for instance the course s protocol 
establishes that members of departments must join the course as 
teachers whenever they are assigned to the course s subject 
thirdly the protocol shall allow to specify monitoring 
rules for the different events originating within the 
interaction corresponding rules shall establish the set of agents 
that must be awared of some event for instance this 
func 
the formalization assumes that protocol s functions 
implicitly recieve as input the interaction being regulated 
 
the haspaidfee relationship between degree students 
and subject resources is represented by an additional 
application-dependent field of the agent structure for this 
kind of roles similarly the admission stage is an additional 
boolean field of the structure for school interactions the 
generic types i a r and p are thus extendable 
tionality is exploited by teachers in order to monitor the 
enrollment of students to the course 
last the protocol shall allow to control the state of the 
interaction as well as the states of its members corresponding 
rules identify the conditions under which some interaction 
will be automatically finished and whether the participation 
of some member agent will be automatically over thus the 
function field finish returns true if the regulated interaction 
must finish its execution if so happens a well-defined set of 
protocols must ensure that its sub-interactions and members 
are finished as well inv similarly the function over 
returns true if the participation of the specified member must 
be over well-formed protocols must ensure the consistency 
between these functions across playing roles inv 
 for 
instance the course s protocol establishes that the 
participation of students is over when they gain ownership of the 
course s certificate or the chances to get it are exhausted 
it also establishes that the course must be finished when 
the admission stage has passed and all the students finished 
their participation 
 social interaction dynamics 
the dynamics of the multi-agent community is influenced 
by the external actions executed by software components 
and the protocols governing their interactions this section 
focuses on the dynamics resulting from a particular kind of 
external action the attempt of some component attached 
to the community as an agent to execute a given internal 
action the description of other external actions concerning 
agents e g observe the events from its event queue enter 
or exit from the community and resources e g a timer 
resource may signal the pass of time will be skipped 
the processing of some attempt may give rise to changes 
in the scope of the target interaction such as the 
instantiation of new participants agents or resources or the 
setting up of new sub-interactions these resulting events 
may cause further changes in the state of other interactions 
 the target one included namely in its execution state as 
well as in the execution state obligations and visibility of 
their members this section will also describe the way in 
which these events are processed the resulting dynamics 
described bellow allows for actions and events corresponding 
to different agents and interactions to be processed 
simultaneously due to lack of space we only include some of the 
operational rules that formalise the execution semantics 
 attempt processing 
an attempt is defined by the structure at t perf 
a act act where the performer represents the agent 
in charge of executing the specified action this action is 
intended to alter the state of some target interaction 
 possibly the performer s context itself and notify a collection 
of addressees of the changes resulting from a successful 
execution accordingly the type act of actions ranged over 
by meta-variable α is specified as follows 
act state sact target i add set a 
def αperf a ⇔ α ∈ aatt 
 
the close and leave actions update the finish and over 
function fields as explained in the next section additional 
actions such as permit forbid empower etc to update other 
protocol s fields are yet to be identified in future work 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
where the performer is formally defined as the agent who 
stores the action in its queue of attempts and the state field 
represents the current phase of processing this process 
goes through four major phases as specified by the 
enumeration type sact enum {emp perm exec} 
empowerment checking permission checking and action execution 
described in the sequel 
 empowerment checking 
the post-condition of an attempt consists of inserting the 
action in the queue of attempts of the specified performer 
as rule specifies 
 this will only be possible if the 
performer is empowered to execute that action according to the 
rules that govern the state of the target interaction if this 
condition is not met the attempt will simply be ignored 
moreover the performer agent must be in the playing state 
 this pre-condition is also required for any rule concerning 
the processing of attempts if these pre-conditions are 
satisfied the rule is fired and the processing of the action 
continues in the permission checking stage for instance when 
the software component attached as a student in a degree 
attempts to join as a student the course in which some subject 
is teached the empowerment rules of the course interaction 
are checked if the degree student has passed the course s 
prerequisite subjects the join action will be inserted in its 
queue of attempts and considered for execution 
αtarget prot emp a α 
a playing qact 
a α at t 
−→ playing qact 
 
w here α state 
 perm 
 qact insert α qact 
 permissions checking 
the processing of the action resumes when the possible 
preceding actions in the performer s queue of attempts are 
fully processed and removed from the queue moreover 
there should be no pending events to be processed in the 
interaction for these events may cause the member or the 
interaction to be finished as will be shortly explained in the 
next sub-section if these conditions are met the 
permissions to execute the given action and notify the specified 
addressees are checked e g it will be checked whether the 
student paid the fee for the course s subject if the protocol 
of the target interaction grants permission the processing 
of the attempt moves to the action execution stage rule 
otherwise the action is discharged and removed from the 
queue unlike unempowered attempts a forbidden one will 
cause an event to be generated and transfered to the event 
channel for further processing 
αstate perm ∧ acontext ch in ev ∅ ∧ αtarget prot perm a α 
a playing α −→ playing α 
 
w here α state 
 exec 
 
labels of record instances are omitted to allow for more 
compact specifications moreover note that record updates 
in where clauses only affect the specified fields 
 action execution 
the transitions fired in this stage are classified 
according to the different types of actions to be executed the 
intended effects of some actions may directly be achieved 
in a single step while others will required an indirect 
approach and possibly several execution steps actions of the 
first kind are constructive ones such as set up and join 
the second group of actions include those such as close and 
leave whose effects are indirectly achieved by updating the 
interaction protocol 
as an example of constructive action let s consider the 
execution of a set up action whose type is defined as 
follows 
 
setup act · new i 
inv αnew mem αnew res αnew sub ∅ 
 αnew state open 
where the new field represents the new interaction to be 
initiated its sets of participants agents and resources and 
sub-interactions must be empty inv and its state must 
be open inv the setting up of the new interaction may 
thus affect its protocol and possible application-dependent 
fields e g the subject of a course interaction according 
to rule the outcome of the execution is threefold firstly 
the performer s attempt queue is updated so that the 
executing action is removed secondly the new interaction is 
added to the target s set of sub-interactions moreover its 
initiator field is set to the performer agent last the event 
representing this change which includes a description of the 
change the agent that caused it and the action performed 
is inserted in the output port of the target s event channel 
αstate exec ∧ α setup ∧ αnew i 
a playing α qact −→ playing qact 
αtarget open si c −→ open si ∪ i c 
 
w here i ini 
 a 
 c out ev 
 insert a α sub αtarget 
 i cout ev 
 
let s consider now the case of a close action this action 
represents an attempt by the performer to force some 
interaction to finish thus bypassing its current protocol rules 
 those concerning the finish function the way to achieve 
this effect is to cause an update on the protocol so that the 
finish function returns true afterwards 
 accordingly we 
may specify this type of action as follows 
close act · upd → bool → → bool 
inv αtarget state open 
 αtarget context nil 
 αupd αtarget prot finish 
where the inherited target field represents the interaction 
to be closed which must be open and different to the 
topinteraction according to invariants and and the new 
 
the resulting type consists of the fields of the act record 
extended with an additional new field 
 
this strategy is also followed in the definition of leave and 
may also be used in the definition of other types of actions 
such as fire permit forbid etc 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
update field represents a proper higher-order function to 
update the target s protocol inv the transition which 
models the execution of this action specified by rule 
defines two effects in the target interaction its protocol is 
updated and the event representing this change is inserted 
in its output port this event will actually trigger the 
closing process of the interaction as described in the next 
subsection 
αstate exec ∧ α close 
a playing α qact −→ playing qact 
αtarget open p c −→ open p c 
 
w here p finish 
 αupd 
 pfinish 
 
 c out ev 
 insert a α finish αtarget 
 cout ev 
 
 event processing 
the processing of events is encapsulated in the event 
channels of interactions channels ranged over by meta-variable 
c are defined by two input and output ports according to 
the following definition 
ch out outp in inp 
inv ccontext ∈ cout disp finish ccontext 
 ccontext ∈ cout disp over a 
 ccontext sub ⊆ cout disp closing ccontext 
 apartsin ⊆ cout disp leaving a 
 ccontext ∈ cout disp closed i 
 {ccontext aplayer context} ⊆ cout disp left a 
outp ev queue e disp e → set i int set i ag set a 
inp ev queue e stage enum {int mem obl} ag set a 
the output port stores and processes the events originated 
within the scope of the channel s interaction its first 
purpose is to dispatch the local events to the agents 
identified by the protocol s monitoring function moreover since 
these events may influence the results of the finishing over 
and obligation functions of certain protocols they will also 
be dispatched to the input ports of the interactions 
identified through a dispatching function - whose invariants will 
be explained later on thus input ports serve as a 
coordination mechanism which activate the re-evaluation of the 
above functios whenever some event is received 
 
accordingly the processing of some event goes through four major 
stages event dispatching interaction state update member 
state update and obligations update the first one takes place 
in the output port of the interaction in which the event 
originated whereas the other ones execute in separate control 
threads associated to the input ports of the interactions to 
which the event was dispatched 
 event dispatching 
the processing of some event stored in the output port is 
triggered when all its preceding events have been dispatched 
as a first step the auxiliary int and ag fields are initialised 
 
alternatively we may have assumed that interactions are 
fully aware of any change in the multi-agent community in 
this scenario interactions would trigger themselves without 
requiring any explicit notification on the contrary we 
adhere to the more realistic assumption of limited awareness 
with the returned values of the dispatching and protocol s 
monitoring functions respectively rule then additional 
rules simply iterate over these collections until all agents and 
interactions have been notified i e both sets are empty 
last the event is removed from the queue and the auxiliary 
fields are re-set to nil 
the dispatching function shall identify the set of 
interactions possibly empty that may be affected by the event 
 which may include the channel s interaction itself 
 for 
instance according to the finishing rule of university courses 
mentioned in the last section the event representing the 
end of the admission stage originated within the scope of 
the school interaction will be dispatched to every course of 
the school s degrees concerning the monitoring function 
according to invariant of protocols if the event is 
generated as the result of an action performance the agents 
to be notified will include the performer and addressees of 
that action thus according to the monitoring rule of 
university courses if a student of some degree joins a certain 
course and specifies a colleague as addressee of that action 
the course s teachers and itself will also be notified of the 
successful execution 
ccontext state 
s open ∧ ccontext prot monitor 
s mon 
cs e d nil nil −→ e d e mon e 
 
 interaction state update 
input port activity is triggered when a new event is 
received irrespective of the kind of incoming event the first 
processing action is to check whether the channel s 
interaction must be finished thus the dispatching of the finish 
event resulting from a close action inv serves as a 
trigger of the closing procedure if the interaction has not to 
be finished the input port stage field is set to the member 
state update stage and the auxiliary ag field is initialised to 
the interaction members otherwise we can consider two 
possible scenarios in the first one the interaction has no 
members and no sub-interactions in this case the 
interaction can be inmediately closed down as rule shows 
the interaction is closed removed from the context s set of 
sub-interactions and a closed event is inserted in its output 
channel according to invariant this event will be later 
inserted to its input channel to allow for further treatment 
cin ev 
 ∅ ∧ cin stage 
 int ∧ pfinish 
 {i} ∪ si c −→ si c 
i ∅ ∅ p c −→ closed 
 
w here c out ev 
 insert closed i cout ev 
 
in the second scenario the interaction has some member 
or sub-interaction in this case clean-up is required prior to 
the disposal of the interaction e g if the admission period 
ends and no student has matriculated for the course 
teachers has to be finished before finishing the course itself as 
rule shows the interaction is moved to the transient 
closing state and a corresponding event is inserted in the output 
port according to invariant the closing event will be 
dispatched to every sub-interaction in order to activate its 
closing procedure guaranteed by invariant moreover 
 
this is essentially determined by the protocol rules of these 
interactions the way in which the dispatching function is 
initialised and updated is out of the scope of this paper 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the stage and ag fields are properly initialised so that the 
process goes on in the next member state update stage this 
stage will further initiate the leaving process of the members 
 according to invariant 
cin ev ∅ ∧ cin stage int ∧ pfinish ∧ sa ∅ ∨ si ∅ 
i open sa si p c −→ closing sa si p c 
 
w here c out ev 
 insert closing i cout ev 
 
 c in stage 
 mem 
 c in ag 
 sa 
eventually every member will leave the interaction and 
every sub-interaction will be closed corresponding events 
will be received by the interaction according to invariants 
 and so that the conditions of the first scenario will 
hold 
 member state update 
this stage simply iterates over the members of the 
interaction to check whether they must be finished according to 
the protocol s over function when all members have been 
checked the stage field will be set to the next obligation 
update stage and the auxiliary ag field will be initalised 
with the agents identified by the protocol s obligation 
update function 
if some member has to end its participation in the 
interaction and it is not playing any role it will be inmediately 
abandoned successfully or unsuccessfully according to the 
satisfaction of its purpose the corresponding event will 
be forwarded to its interaction and to the interaction of its 
player agent to account for further changes inv 
otherwise the member enters the transient leaving state thus 
preventing any action performance then it waits for the 
completion of the leaving procedures of its played roles 
triggered by proper dispatching of the leaving event inv 
 obligations update 
in this stage the obligations of agents not necessaryly 
members of the interaction towards the interaction are 
updated accordingly when all the identified agents have been 
updated the event is removed from the input queue and 
the stage field is set back to the interaction state update 
for instance when a course interaction receives an event 
representing the assignment of some department member to 
its subject an obligation to join the course as a teacher is 
created for that member moreover the event representing 
this change is added to the output channel of the department 
interaction 
 discussion 
this paper has attempted to expose a possible 
semantic core underlying the wide spectrum of interaction types 
between autonomous social and situated software 
components in the realm of software architectures this core has 
been formalised as an operational model of social 
connectors intended to describe both the basic structure and 
dynamics of multi-agent interactions from the largest the 
agent society itself down to the smallest ones 
 communicative actions thus top-level interactions may represent the 
kind of agent-web pursued by large-scale initiatives such as 
the agentcities opennet one large-scale interactions 
modelling complex aggregates of agent interactions such as 
those represented by e-institutions or virtual organizations 
 are also amenable to be conceptualised as 
particular kinds of first-level social interactions the last 
levels of the interaction tree may represent small-scale 
multiagent interactions such as those represented by interaction 
protocols dialogue games or scenes finally 
bottom-level interactions may represent communicative 
actions from this perspective the member types of a ca 
include the speaker and possibly many listeners the 
purpose of the speaker coincides with the illocutionary purpose 
of the ca whereas the purpose of any listener is to 
declare that it actually the software component successfully 
processed the meaning of the ca 
the analysis of social interactions put forward in this 
paper draws upon current proposals of the literature in 
several general respects such as the institutional and 
organizational character of multi-agent systems and the 
normative perspective on multi-agent protocols 
these proposals as well as others focusing in relevant 
abstractions such as power relationships contracts trust and 
reputation mechanisms in organizational settings etc could 
be further exploited in order to characterize more accurately 
the organizational character of some multi-agent 
interactions similarly the conceptualization of communicative 
actions as atomic interactions may similarly benefit from 
public semantics of communicative actions such as the one 
introduced in last the abstract model of protocols may 
be refined taking into account existing operational models 
of norms these analyses shall result in new 
organizational and communicative abstractions obtained through 
a refinement and or extension of the general model of 
social interactions thus the proposed model is not intended 
to capture every organizational or communicative feature of 
multi-agent interactions but to reveal their roots in basic 
interaction mechanisms in turn this would allow for the 
exploitation of common formalisms particularly concerning 
protocols 
unlike the development of individual agents which has 
greatly benefited from the design of several agent 
programming languages societal features of multi-agent systems 
are mostly implemented in terms of visual modelling 
and a fixed set of interaction abstractions we argue that 
the current field of multi-agent system programming may 
greatly benefit from multi-agent programming languages that 
allow programmers to accommodate an open set of 
interaction mechanisms the model of social interactions put 
forward in this paper is intended as the abstract machine 
of a language of this type this abstract machine would 
be independent of particular agent architectures and 
languages i e software components may be programmed in a 
bdi language such as jason or in a non-agent oriented 
language 
on top of the presented execution semantics current and 
future work aims at the specification of the type system 
which allows to program the abstract machine the 
specification of the corresponding surface syntaxes both textual 
and visual and the design and implementation of a virtual 
machine over existing middleware technologies such as fipa 
platforms or web services we also plan to study particular 
refinements and limitations to the proposed model 
particularly with respect to the dispatching of events semantics 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
of obligations dynamic updates of protocols and rule 
formalisms in this latter aspect we plan to investigate the 
use of answer set programming to specify the rules of 
protocols attending to the role that incompleteness rules may 
only specify either necessary or sufficient conditions for 
instance explicit negation e g prohibitions and defaults 
play in this domain 
 acknowledgments 
the authors thank anonymous reviewers for their 
comments and suggestions research sponsored by the spanish 
ministry of science and education mec project 
tin -c - 
 references 
 r allen and d garlan a formal basis for 
architectural connection acm transactions on 
software engineering and methodology - 
june 
 j l arcos m esteva p noriega j a rodr´ıguez 
and c sierra engineering open environments with 
electronic institutions journal on engineering 
applications of artificial intelligence - 
 
 g boella r damiano j hulstijn and l w n 
van der torre role-based semantics for agent 
communication embedding of the mental attitudes 
and social commitments semantics in aamas 
pages - 
 r h bordini l braubach m dastani a e f 
seghrouchni j j g sanz j leite g o hare 
a pokahr and a ricci a survey of programming 
languages and platforms for multi-agent systems 
informatica - 
 r h bordini j f h¨ubner and r vieira jason and 
the golden fleece of agent-oriented programming in 
r h bordini d m j dix and 
a el fallah seghrouchni editors multi-agent 
programming languages platforms and applications 
chapter springer-verlag 
 o cliffe m d vos and j a padget specifying and 
analysing agent-based social institutions using answer 
set programming in eumas pages - 
 v dignum j v´azquez-salceda and f dignum 
omni introducing social structure norms and 
ontologies into agent organizations in r bordini 
m dastani j dix and a seghrouchni editors 
programming multi-agent systems second 
international workshop promas volume 
of lnai pages - springer 
 m esteva d de la cruz and c sierra islander 
an electronic institutions editor in m gini t ishida 
c castelfranchi and w l johnson editors 
proceedings of the first international joint 
conference on autonomous agents and multiagent 
systems aamas pages - acm press 
july 
 m esteva b rosell j a rodr´ıguez-aguilar and 
j l arcos ameli an agent-based middleware for 
electronic institutions in proceedings of the third 
international joint conference on autonomous agents 
and multiagent systems volume pages - 
 
 j ferber o gutknecht and f michel from agents 
to organizations an organizational view of 
multi-agent systems in aose pages - 
 foundation for intelligent physical agents fipa 
interaction protocol library specification 
http www fipa org repository ips html 
 a garc´ıa-camino j a rodr´ıguez-aguilar c sierra 
and w vasconcelos norm-oriented programming of 
electronic institutions in aamas pages - 
 
 o gutknecht and j ferber the madkit agent 
platform architecture lecture notes in computer 
science - 
 jade the jade project home page 
http jade cselt it 
 m luck p mcburney o shehory and s willmott 
agent technology computing as interaction - a 
roadmap for agent-based computing agentlink iii 
 
 p mcburney and s parsons a formal framework for 
inter-agent dialogues in j p m¨uller e andre 
s sen and c frasson editors proceedings of the 
fifth international conference on autonomous 
agents pages - montreal canada may 
acm press 
 n r mehta n medvidovic and s phadke towards 
a taxonomy of software connectors in proceedings of 
the nd international conference on software 
engineering pages - acm press june 
 j pav´on and j g´omez-sanz agent oriented software 
engineering with ingenias in v marik j muller and 
m pechoucek editors proceedings of the rd 
international central and eastern european 
conference on multi-agent systems springer verlag 
 
 b c pierce types and programming languages the 
mit press cambridge ma 
 j pitt l kamara m sergot and a artikis voting 
in multi-agent systems feb 
 g plotkin a structural approach to operational 
semantics technical report daimi fn- aarhus 
university sept 
 j searle speech acts cambridge university press 
 
 m sergot a computational theory of normative 
positions acm transactions on computational logic 
 - oct 
 m p singh agent-based abstractions for software 
development in f bergenti m -p gleizes and 
f zambonelli editors methodologies and software 
engineering for agent systems chapter pages - 
kluwer 
 s willmot and al agentcities opennet testbed 
http x-opennet net 
 f zambonelli n r jennings and m wooldridge 
developing multiagent systems the gaia 
methodology acm transactions on software 
engineering and methodology - july 
 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a randomized method for the shapley value 
for the voting game 
shaheen s fatima 
department of 
computer science 
university of liverpool 
liverpool l bx uk 
shaheen csc liv ac uk 
michael wooldridge 
department of 
computer science 
university of liverpool 
liverpool l bx uk 
mjw csc liv ac uk 
nicholas r jennings 
school of electronics and 
computer science 
university of southampton 
southampton so bj uk 
nrj ecs soton ac uk 
abstract 
the shapley value is one of the key solution concepts for 
coalition games its main advantage is that it provides a unique and fair 
solution but its main problem is that for many coalition games 
the shapley value cannot be determined in polynomial time in 
particular the problem of finding this value for the voting game is 
known to be p-complete in the general case however in this 
paper we show that there are some specific voting games for which 
the problem is computationally tractable for other general voting 
games we overcome the problem of computational complexity by 
presenting a new randomized method for determining the 
approximate shapley value the time complexity of this method is linear 
in the number of players we also show through empirical studies 
that the percentage error for the proposed method is always less 
than and in most cases less than 
categories and subject descriptors 
i distributed artificial intelligence multiagent systems 
general terms 
algorithms design theory 
 introduction 
coalition formation a key form of interaction in multi-agent 
systems is the process of joining together two or more agents so as 
to achieve goals that individuals on their own cannot or to achieve 
them more efficiently often in such situations 
there is more than one possible coalition and a player s payoff 
depends on which one it joins given this a key problem is to ensure 
that none of the parties in a coalition has any incentive to break 
away from it and join another coalition i e the coalitions should 
be stable however in many cases there may be more than one 
solution i e a stable coalition in such cases it becomes difficult 
to select a single solution from among the possible ones especially 
if the parties are self-interested i e they have different preferences 
over stable coalitions 
in this context cooperative game theory deals with the 
problem of coalition formation and offers a number of solution 
concepts that possess desirable properties like stability fair division 
of joint gains and uniqueness cooperative game theory 
differs from its non-cooperative counterpart in that for the former 
the players are allowed to form binding agreements and so there is 
a strong incentive to work together to receive the largest total 
payoff also unlike non-cooperative game theory cooperative game 
theory does not specify a game through a description of the 
strategic environment including the order of players moves and the set 
of actions at each move and the resulting payoffs but instead it 
reduces this collection of data to the coalitional form where each 
coalition is represented by a single real number there are no 
actions moves or individual payoffs the chief advantage of this 
approach at least in multiple-player environments is its practical 
usefulness thus many more real-life situations fit more easily into 
a coalitional form game whose structure is more tractable than that 
of a non-cooperative game whether that be in normal or extensive 
form and it is for this reason that we focus on such forms in this 
paper 
given these observations a number of multiagent systems 
researchers have used and extended cooperative game-theoretic 
solutions to facilitate automated coalition formation 
moreover in this work one of the most extensively studied solution 
concepts is the shapley value a player s shapley value gives an 
indication of its prospects of playing the game - the higher the 
shapley value the better its prospects the main advantage of the 
shapley value is that it provides a solution that is both unique and 
fair see section for a discussion of the property of fairness 
however while these are both desirable properties the shapley 
value has one major drawback for many coalition games it 
cannot be determined in polynomial time for instance finding this 
value for the weighted voting game is in general p-complete 
a problem is p-hard if solving it is as hard as counting 
satisfying assignments of propositional logic formulae p since 
 p-completeness thus subsumes np-completeness this implies that 
computing the shapley value for the weighted voting game will be 
intractable in general in other words it is practically infeasible to 
try to compute the exact shapley value however the voting game 
has practical relevance to multi-agent systems as it is an important 
means of reaching consensus between multiple agents hence our 
objective is to overcome the computational complexity of finding 
the shapley value for this game specifically we first show that 
there are some specific voting games for which the exact value can 
 
 - - - - rps c ifaamas 
be computed in polynomial time by identifying such games we 
show for the first tme when it is feasible to find the exact value and 
when it is not for the computationally complex voting games we 
present a new randomised method along the lines of monte-carlo 
simulation for computing the approximate shapley value 
the computational complexity of such games has typically been 
tackled using two main approaches the first is to use 
generating functions this method trades time complexity for 
storage space the second uses an approximation technique based on 
monte carlo simulation however the method we propose is 
more general than either of these see section for details 
moreover no work has previously analysed the approximation error the 
approximation error relates to how close the approximate is to the 
true shapley value specifically it is the difference between the true 
and the approximate shapley value it is important to determine 
this error because the performance of an approximation method is 
evaluated in terms of two criteria its time complexity and its 
approximation error thus our contribution lies in also in providing 
for the first time an analysis of the percentage error in the 
approximate shapley value this analysis is carried out empirically 
our experiments show that the error is always less than 
and in most cases it is under finally our method has time 
complexity linear in the number of players and it does not require 
any arrays i e it is economical in terms of both computing time 
and storage space given this and the fact that software agents 
have limited computational resources and therefore cannot 
compute the true shapley value our results are especially relevant to 
such resource bounded agents 
the rest of the paper is organised as follows section defines 
the shapley value and describes the weighted voting game in 
section we describe voting games whose shapley value can be found 
in polynomial time in section we present a randomized method 
for finding the approximate shapley value and analyse its 
performance in section section discusses related literature finally 
section concludes 
 background 
we begin by introducing coalition games and the shapley value and 
then define the weighted voting game a coalition game is a game 
where groups of players coalitions may enforce cooperative 
behaviour between their members hence the game is a competition 
between coalitions of players rather than between individual 
players 
depending on how the players measure utility coalition game 
theory is split into two parts if the players measure utility or the 
payoff in the same units and there is a means of exchange of utility 
such as side payments we say the game has transferable utility 
otherwise it has non-transferable utility more formally a coalition 
game with transferable utility n v consists of 
 a finite set n { n} of players and 
 a function v that associates with every non-empty subset s 
of n i e a coalition a real number v s the worth of s 
for each coalition s the number v s is the total payoff that is 
available for division among the members of s i e the set of joint 
actions that coalition s can take consists of all possible divisions 
of v s among the members of s coalition games with 
nontransferable payoffs differ from ones with transferable payoffs in 
the following way for the former each coalition is associated with 
a set of payoff vectors that is not necessarily the set of all possible 
divisions of some fixed amount the focus of this paper is on the 
weighted voting game described in section which is a game 
with transferable payoffs 
thus in either case the players will only join a coalition if they 
expect to gain from it here the players are allowed to form 
binding agreements and so there is strong incentive to work together to 
receive the largest total payoff the problem then is how to split the 
total payoff between or among the players in this context shapley 
 constructed a solution using an axiomatic approach shapley 
defined a value for games to be a function that assigns to a game 
 n v a number ϕi n v for each i in n this function satisfies 
three axioms 
 symmetry this axiom requires that the names of players 
play no role in determining the value 
 carrier this axiom requires that the sum of ϕi n v for all 
players i in any carrier c equal v c a carrier c is a subset 
of n such that v s v s ∩ c for any subset of players 
s ⊂ n 
 additivity this axiom specifies how the values of different 
games must be related to one another it requires that for 
any games ϕi n v and ϕi n v ϕi n v ϕi n v 
ϕi n v v for all i in n 
shapley showed that there is a unique function that satisfies these 
three axioms 
shapley viewed this value as an index for measuring the power 
of players in a game like a price index or other market indices the 
value uses averages or weighted averages in some of its 
generalizations to aggregate the power of players in their various cooperation 
opportunities alternatively one can think of the shapley value as 
a measure of the utility of risk neutral players in a game 
we first introduce some notation and then define the shapley 
value let s denote the set n − {i} and fi s → n−{i} 
be 
a random variable that takes its values in the set of all subsets of 
n − {i} and has the probability distribution function g defined 
as 
g fi s s 
 s n − s − 
n 
the random variable fi is interpreted as the random choice of a 
coalition that player i joins then a player s shapley value is 
defined in terms of its marginal contribution thus the marginal 
contribution of player i to coalition s with i ∈ s is a function δiv that 
is defined as follows 
δiv s v s ∪ {i} − v s 
thus a player s marginal contribution to a coalition s is the 
increase in the value of s as a result of i joining it 
definition the shapley value ϕi of the game n v for 
player i is the expectation e of its marginal contribution to a 
coalition that is chosen randomly 
ϕi n v e δiv ◦ fi 
the shapley value is interpreted as follows suppose that all 
the players are arranged in some order all orderings being equally 
likely then ϕi n v is the expected marginal contribution over 
all orderings of player i to the set of players who precede him 
the method for finding a player s shapley value depends on the 
definition of the value function v this function is different for 
different games but here we focus specifically on the weighted 
voting game for the reasons outlined in section 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 the weighted voting game 
we adopt the definition of the voting game given in thus there 
is a set of n players that may for example represent shareholders 
in a company or members in a parliament the weighted voting 
game is then a game g n v in which 
v s 
j 
 if w s ≥ q 
 otherwise 
for some q ∈ ir and wi ∈ irn 
 where 
w s 
x 
i∈s 
wi 
for any coalition s thus wi is the number of votes that player i 
has and q is the number of votes needed to win the game i e the 
quota 
note that for this game denoted q w wn a player s 
marginal contribution is either zero or one this is because the 
value of any coalition is either zero or one a coalition with value 
zero is called a losing coalition and with value one a winning 
coalition if a player s entry to a coalition changes it from losing to 
winning then the player s marginal contribution for that coalition 
is one otherwise it is zero 
the main advantage of the shapley value is that it gives a 
solution that is both unique and fair the property of uniqueness 
is desirable because it leaves no ambiguity the property of 
fairness relates to how the gains from cooperation are split between 
the members of a coalition in this case a player s shapley value 
is proportional to the contribution it makes as a member of a 
coalition the more contribution it makes the higher its value thus 
from a player s perspective both uniqueness and fairness are 
desirable properties 
 voting games with polynomial 
time solutions 
here we describe those voting games for which the shapley value 
can be determined in polynomial time this is achieved using the 
direct enumeration approach i e listing all possible coalitions and 
finding a player s marginal contribution to each of them we 
characterise such games in terms of the number of players and their 
weights 
 all players have equal weight 
consider the game q j j with m parties each party has j 
votes if q ≤ j then there would be no need for the players to form 
a coalition on the other hand if q mj m n is the number 
of players only the grand coalition is possible the interesting 
games are those for which the quota q satisfies the constraint 
 j ≤ q ≤ j m − for these games the value of a coalition 
is one if the weight of the coalition is greater than or equal to q 
otherwise it is zero 
let ϕ denote the shapley value for a player consider any one 
player this player can join a coalition as the ith member where 
 ≤ i ≤ m however the marginal contribution of the player is 
only if it joins a coalition as the q j th member in all other cases 
its marginal contribution is zero thus the shapley value for each 
player ϕ m since ϕ requires one division operation it can 
be found in constant time i e o 
 a single large party 
consider a game in which there are two types of players large 
 with weight wl ws and small with weight ws there is one 
large player and m small ones the quota for this game is q i e we 
have a game of the form q wl ws ws ws the total number 
of players is m the value of a coalition is one if the weight 
of the coalition is greater than or equal to q otherwise it is zero 
let ϕl denote the shapley value for the large player and ϕs that for 
each small player 
we first consider ws and then ws the smallest 
possible value for q is wl this is because if q ≤ wl then the 
large party can win the election on its own without the need for 
a coalition thus the quota for the game satisfies the constraint 
wl ≤ q ≤ m wl − also the lower and upper limits for 
wl are and q − respectively the lower limit is because 
the weight of the large party has to be greater than each small one 
furthermore the weight of the large party cannot be greater than 
q since in that case there would be no need for the large party 
to form a coalition recall that for our voting game a player s 
marginal contribution to a coalition can only be zero or one 
consider the large party this party can join a coalition as the 
ith member where ≤ i ≤ m however the marginal 
contribution of the large party is one if it joins a coalition as the 
ith member where q − wl ≤ i q in all the remaining cases 
its marginal contribution is zero thus out of the total m 
possible cases its marginal contribution is one in wl cases hence 
the shapley value of the large party is ϕl wl m in the 
same way we obtain the shapley value of the large party for the 
general case where ws as 
ϕl wl ws m 
now consider a small player we know that the sum of the 
shapley values of all the m players is one also since the small 
parties have equal weights their shapley values are the same hence 
we get 
ϕs 
 − ϕl 
m 
thus both ϕl and ϕs can be computed in constant time this 
is because both require a constant number of basic operations 
 addition subtraction multiplication and division in the same way 
the shapley value for a voting game with a single large party and 
multiple small parties can be determined in constant time 
 multiple large and small parties 
we now consider a voting game that has two player types large 
and small as in section but now there are multiple large and 
multiple small parties the set of parties consists of ml large 
parties and ms small parties the weight of each large party is wl and 
that of each small one is ws where ws wl we show the 
computational tractability for this game by considering the following four 
possible scenarios 
s q ≤ mlwl and q ≤ msws 
s q ≤ mlwl and q ≥ msws 
s q ≥ mlwl and q ≥ msws 
s q ≥ mlwl and q ≤ msws 
for the first scenario consider a large player in order to determine 
the shapley value for this player we need to consider the number 
of all possible coalitions that give it a marginal contribution of one 
it is possible for the marginal contribution of this player to be one if 
it joins a coalition in which the number of large players is between 
zero and q − wl in other words there are q − wl such 
cases and we now consider each of them 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
consider a coalition such that when the large player joins in 
there are i large players and q − iwl − ws small players 
already in it and the remaining players join after the large player 
such a coalition gives the large player unit marginal contribution 
let c 
l i q denote the number of all such coalitions to begin 
consider the case i 
c 
l q c 
„ 
ms 
q − 
ws 
 
× factorial 
„ 
q − 
ws 
 
× 
factorial 
„ 
ml ms − 
q − 
ws 
− 
 
where c y x denotes the number of possible combinations of x 
items from a set of y items for i we get 
c 
l q c ml × c 
„ 
ms 
q − wl − 
ws 
 
× 
factorial 
„ 
q − wl − 
ws 
 
× 
factorial 
„ 
ml ms − 
q − wl − 
ws 
− 
 
in general for i we get 
c 
l i q c ml i × c 
„ 
ms 
q − iwl − 
ws 
 
× 
factorial 
„ 
q − iwl − 
ws 
 
× 
factorial 
„ 
ml ms − 
q − wl − 
ws 
− 
 
thus the large player s shapley value is 
ϕl 
q− 
wlx 
i 
c 
l i q factorial ml ms 
for a given i the time to find c 
l i q is o t where 
t mlms q − iwl − ml ms ws 
hence the time to find the shapley value is o t q wl 
in the same way a small player s shapley value is 
ϕs 
q− 
wsx 
i 
c 
s i q factorial ml ms 
and can be found in time o t q ws likewise the remaining three 
scenarios s to s can be shown to have the same time 
complexity 
 three player types 
we now consider a voting game that has three player types 
and the set of parties consists of m players of type each 
with weight w m players of type each with weight w and 
m players of type each with weight w 
for this voting game consider a player of type it is possible 
for the marginal contribution of this player to be one if it joins a 
coalition in which the number of type players is between zero 
and q − w in other words there are q − w such 
cases and we now consider each of them 
consider a coalition such that when the type player joins in 
there are i type players already in it the remaining players join 
after the type player let c 
l i q denote the number of all such 
coalitions that give a marginal contribution of one to the type 
player where 
c 
 i q 
q− 
w x 
i 
q−iw − 
w x 
j 
c 
 j q − iw 
therefore the shapley value of the type player is 
ϕ 
q− 
w x 
i 
c 
 i q factorial m m m 
the time complexity of finding this value is o t q 
 w w where 
t 
 y 
i 
mi q − iwl − 
 x 
i 
mi w w 
likewise for the other two player types and 
thus we have identified games for which the exact shapley 
value can be easily determined however the computational 
complexity of the above direct enumeration method increases with the 
number of player types for a voting game with more than three 
player types the time complexity of the above method is a 
polynomial of degree four or more to deal with such situations therefore 
the following section presents a faster randomised method for 
finding the approximate shapley value 
 finding the approximate shapley 
value 
we first give a brief introduction to randomized algorithms and 
then present our randomized method for finding the approximate 
shapley value randomized algorithms are the most commonly 
used approach for finding approximate solutions to 
computationally hard problems a randomized algorithm is an algorithm that 
during some of its steps performs random choices the 
random steps performed by the algorithm imply that by executing the 
algorithm several times with the same input we are not guaranteed 
to find the same solution now since such algorithms generate 
approximate solutions their performance is evaluated in terms of two 
criteria their time complexity and their error of approximation 
the approximation error refers to the difference between the 
exact solution and its approximation against this background we 
present a randomized method for finding the approximate shapley 
value and empirically evaluate its error 
we first describe the general voting game and then present our 
randomized algorithm in its general form a voting game has more 
than two types of players let wi denote the weight of player 
i thus for m players and for quota q the game is of the form 
q w w wm the weights are specified in terms of a 
probability distribution function for such a game we want to find the 
approximate shapley value 
we let p denote a population of players the players weights 
in this population are defined by a probability distribution function 
irrespective of the actual probability distribution function let μ be 
the mean weight for the population of players and ν the variance in 
the players weights from this population of players we randomly 
draw samples and find the sum of the players weights in the sample 
using the following rule from sampling theory see p 
if w w wn is a random sample of size n drawn 
from any distribution with mean μ and variance ν then 
the sample sum has an approximate normal 
distribution with mean nμ and variance ν 
n 
 the larger the n the 
better the approximation 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
r-shapleyvalue p μ ν q wi 
p population of players 
μ mean weight of the population p 
ν variance in the weights for poulation p 
q quota for the voting game 
wi player i s weight 
 ti ← a ← q − wi b ← q − 
 for x from to m repeatedly do the following 
 select a random sample sx of size x from the 
population p 
 evaluate expected marginal contribution δx 
i 
of player i to sx as 
δx 
i ← √ 
 πν x 
r b 
a 
e−x 
 x−xμ 
 ν dx 
 ti ← ti δx 
i 
 evaluate shapley value of player i as 
ϕi ← ti m 
table randomized algorithm to find the shapley value for 
player i 
we know from definition that the shapley value for a player is 
the expectation e of its marginal contribution to a coalition that is 
chosen randomly we use this rule to determine the shapley value 
as follows 
for player i with weight wi let ϕi denote the shapley value let 
x denote the size of a random sample drawn from a population 
in which the individual player weights have any distribution the 
marginal contribution of player i to this random sample is one if the 
total weight of the x players in the sample is greater than or equal 
to a q −wi but less than b q − where is an inifinitesimally 
small quantity otherwise its marginal contribution is zero thus 
the expected marginal contribution of player i denoted δx 
i to the 
sample coalition is the area under the curve defined by n xμ ν 
x 
 
in the interval a b this area is shown as the region b in figure 
 the dotted line in the figure is xμ hence we get 
δx 
i 
 
p 
 πν x 
z b 
a 
e−x 
 x−xμ 
 ν dx 
and the shapley value is 
ϕi 
 
m 
mx 
x 
δx 
i 
the above steps are described in table in more detail step 
 does the initialization in step we vary x between and m 
and repeatedly do the following in step we randomly select a 
sample sx of size x from the population p player i s marginal 
contribution to the random coalition sx is found in step the 
average marginal contribution is found in step - and this is the 
shapley value for player i 
theorem the time complexity of the proposed randomized 
method is linear in the number of players 
proof as per equation δx 
i must be computed m times 
this is done in the for loop of step in table hence the time 
complexity of computing a player s shapley value is o m 
the following section analyses the approximation error for the 
proposed method 
 performance of the randomized 
method 
we first derive the formula for measuring the error in the 
approximate shapley value and then conduct experiments for evaluating 
this error in a wide range of settings however before doing so we 
introduce the idea of error 
the concept of error relates to a measurement made of a 
quantity which has an accepted value obviously it cannot be 
determined exactly how far off a measurement is from the accepted 
value if this could be done it would be possible to just give a more 
accurate corrected value thus error has to do with uncertainty in 
measurements that nothing can be done about if a measurement is 
repeated the values obtained will differ and none of the results can 
be preferred over the others however although it is not possible 
to do anything about such error it can be characterized 
as described in section we make measurements on samples 
that are drawn randomly from a given population p of players 
now there are statistical errors associated with sampling which 
are unavoidable and must be lived with hence if the result of a 
measurement is to have meaning it cannot consist of the measured 
value alone an indication of how accurate the result is must be 
included also thus the result of any physical measurement has 
two essential components 
 a numerical value giving the best estimate possible of the 
quantity measured and 
 the degree of uncertainty associated with this estimated value 
for example if the estimate of a quantity is x and the uncertainty 
is e x the quantity would lie in x ± e x 
for sampling experiments the standard error is by far the most 
common way of characterising uncertainty given this the 
following section defines this error and uses it to evaluate the 
performance of the proposed randomized method 
 approximation error 
the accuracy of the above randomized method depends on its 
sampling error which is defined as follows 
definition the sampling error or standard error is 
defined as the standard deviation for a set of measurements divided 
by the square root of the number of measurements 
to this end let e σx 
 be the sampling error in the sum of the 
weights for a sample of size x drawn from the distribution n xμ ν 
x 
 
where 
e σx 
 
p 
 ν x 
p 
 x 
 
p 
 ν x 
let e δx 
i denote the error in the marginal contribution for player 
i given in equation this error is obtained by propagating the 
error in equation to equation in equation a and b are the 
lower and upper limits for the sum of the players weights for a 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
b 
c 
b 
a − e σx 
a 
a 
 x σeb sum of weights 
z z 
figure a normal distribution for the sum of players weights 
in a coalition of size x 
 
 
 
 
 
 
 
 
 
 
 
 
 
quotaweight 
percentageerrorintheshapleyvalue 
figure performance of the randomized method for m 
players 
coalition of size x since the error in this sum is e σx 
 the actual 
values of a and b lie in the interval a ± e σx 
 and b ± e σx 
 
respectively hence the error in equation is either the probability 
that the sum lies between the limits a − e σx 
 and a i e the area 
under the curve defined by n xμ ν 
x 
 between a − e σx 
 and a 
which is the shaded region a in figure or the probability that the 
sum of weights lies between the limits b and b e σx 
 i e the area 
under the curve defined by n xμ ν 
x 
 between b and b e σx 
 
which is the shaded region c in figure more specifically the 
error is the maximum of these two probabilities 
e δx 
i 
 
p 
 πν x 
× max 
„z a 
a−e σx 
e−x 
 x−xμ 
 ν dx 
z b e σx 
 
b 
e−x 
 x−xμ 
 ν dx 
 
on the basis of the above error we find the error in the shapley 
value by using the following standard error propagation rules 
r if x and y are two random variables with errors e x and e y 
respectively then the error in the random variable z x y 
is given by 
e z e x e y 
r if x is a random variable with error e x and z kx where 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
quotaweight 
percentageerrorintheshapleyvalue 
figure performance of the randomized method for m 
players 
the constant k has no error then the error in z is 
e z k e x 
using the above rules the error in the shapley value given in 
equation is obtained by propagating the error in equation to 
all coalitions between the sizes x and x m let e ϕi 
denote this error where 
e ϕi 
 
m 
mx 
x 
e δx 
i 
we analyze the performance of our method in terms of the 
percentage error pe in the approximate shapley value which is defined 
as follows 
pe × e ϕi ϕi 
 experimental results 
we now compute the percentage error in the shapley value using 
the above equation for pe since this error depends on the 
parameters of the voting game we evaluate it in a range of settings by 
systematically varying the parameters of the voting game 
in particular we conduct experiments in the following setting 
for a player with weight w the percentage error in a player s 
shapley value depends on the following five parameters see equation 
 the number of parties m 
 the mean weight μ 
 the variance in the player s weights ν 
 the quota for the voting game q 
 the given player s weight w 
we fix μ and ν this is because for the normal 
distribution μ ensures that for almost all the players the 
weight is positive and ν is used most commonly in statistical 
experiments ν can be higher or lower but pe is increasing in 
νsee equations and we then vary m q and w as follows we 
vary m between and since beyond we found that the 
error is close to zero for each m we vary q between μ and mμ 
 we impose these limits because they ensure that the size of the 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
quotaweight 
percentageerrorintheshapleyvalue 
figure performance of the randomized method for m 
players 
winning coalition is more than one and less than m - see section 
for details and for each q we vary w between and q− because 
a winning coalition must contain at least two players the results 
of these experiments are shown in figures and as seen in 
the figures the maximum pe is around and in most cases it is 
below 
we now analyse the effect of the three parameters w q and m 
on the percentage error in more detail 
- effect of w the pe depends on e σx 
 because in 
equation the limits of integration depend on e σx 
 the 
interval over which the first integration in equation is done is 
a − a e σx 
 e σx 
 and the interval over which the 
second one is done is b e σx 
 − b e σx 
 thus the 
interval is the same for both integrations and it is independent 
of wi note that each of the two functions that are integrated 
in equation are the same as the function that is integrated 
in equation only the limits of the integration are different 
also the interval over which the integration for the marginal 
contribution of equation is done is b − a wi − see 
figure the error in the marginal contribution is either the 
area of the shaded region a between a − e σx 
 and a in 
figure or the shaded area c between b and b e σx 
 
as per equation it is the maximum of these two areas 
since e σx 
 is independent of wi as wi increases e σx 
 
remains unchanged however the area of the unshaded 
region b increases hence as wi increases the error in the 
marginal contribution decreases and pe also decreases 
- effect of q for a given q the shapley value for player i is 
as given in equation we know that for a sample of size 
x the sum of the players weights is distributed normally 
with mean xμ and variance ν x since of a normal 
distribution lies within two standard deviations of its mean 
 player i s marginal contribution to a sample of size x is 
almost zero if 
a xμ 
p 
ν x or b xμ − 
p 
ν x 
this is because the three regions a b and c in figure 
lie either to the right of z or to the left of z however 
player i s marginal contribution is greater than zero for those 
x for which the following constraint is satisfied 
xμ − 
p 
ν x a b xμ 
p 
ν x 
for this constraint the three regions a b and c lie 
somewhere between z and z since a q −wi and b q − 
equation can also be written as 
xμ − 
p 
ν x q − wi q − xμ 
p 
ν x 
the smallest x that satisfies the constraint in equation 
strictly increases with q as x increases the error in sum 
of weights in a sample i e e σx 
 
p 
 ν x decreases 
consequently the error in a player s marginal contribution 
 see equation also decreases this implies that as q 
increases the error in the marginal contribution and 
consequently the error in the shapley value decreases 
- effect of m it is clear from equation that the error e σx 
 
is highest for x and it decreases with x hence for 
small m e σ 
 has a significant effect on pe but as m 
increases the effect of e σ 
 on pe decreases and as a result 
pe decreases 
 related work 
in order to overcome the computational complexity of finding the 
shapley value two main approaches have been proposed in the 
literature one approach is to use generating functions this 
method is an exact procedure that overcomes the problem of time 
complexity but its storage requirements are substantial - it requires 
huge arrays it also has the limitation not shared by other 
approaches that it can only be applied to games with integer weights 
and quotas 
the other method uses an approximation technique based on 
monte carlo simulation in for instance the shapley value is 
computed by considering a random sample from a large population 
of players the method we propose differs from this in that they 
define the shapley value by treating a player s number of swings if a 
player can change a losing coalition to a winning one then for the 
player the coalition is counted as a swing as a random variable 
while we treat the players weights as random variables in 
however the question remains how to get the number of swings 
from the definition of a voting game and what is the time 
complexity of doing this since the voting game is defined in terms of the 
players weights and the number of swings are obtained from these 
weights our method corresponds more closely to the definition of 
the voting game our method also differs from in that while 
presents a method for the case where all the players weights are 
distributed normally our method applies to any type of distribution 
for these weights thus as stated in section our method is more 
general than also unlike all the above mentioned work 
we provide an analysis of the performance of our method in terms 
of the percentage error in the approximate shapley value 
a method for finding the shapley value was also proposed in 
 this method gives the exact shapley value but its time 
complexity is exponential furthermore the method can be used only 
if the game is represented in a specific form viz the multi-issue 
representation not otherwise finally present a 
polynomial time method for finding the shapley value this method can 
be used if the coalition game is represented as a marginal 
contribution net furthermore they assume that the shapley value of 
a component of a given coalition game is given by an oracle and 
on the basis of this assumption aggregate these values to find the 
value for the overall game in contrast our method is independent 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
of the representation and gives an approximate shapley value in 
linear time without the need for an oracle 
 conclusions and future work 
coalition formation is an important form of interaction in 
multiagent systems an important issue in such work is for the agents to 
decide how to split the gains from cooperation between the 
members of a coalition in this context cooperative game theory offers 
a solution concept called the shapley value the main advantage of 
the shapley value is that it provides a solution that is both unique 
and fair however its main problem is that for many coalition 
games the shapley value cannot be determined in polynomial time 
in particular the problem of finding this value for the voting game 
is p-complete although this problem is in general p-complete 
we show that there are some specific voting games for which the 
shapley value can be determined in polynomial time and 
characterise such games by doing so we have shown when it is 
computationally feasible to find the exact shapley value for other complex 
voting games we presented a new randomized method for 
determining the approximate shapley value the time complexity of the 
proposed method is linear in the number of players we analysed 
the performance of this method in terms of the percentage error in 
the approximate shapley value 
our experiments show that the percentage error in the shapley 
value is at most furthermore in most cases the error is less 
than finally we analyse the effect of the different parameters 
of the voting game on this error our study shows that the error 
decreases as 
 a player s weight increases 
 the quota increases and 
 the number of players increases 
given the fact that software agents have limited computational 
resources and therefore cannot compute the true shapley value our 
results are especially relevant to such resource bounded agents in 
future we will explore the problem of determining the shapley 
value for other commonly occurring coalition games like the 
production economy and the market economy 
 references 
 r aumann acceptable points in general cooperative 
n-person games in contributions to thetheory of games 
volume iv princeton university press 
 g ausiello p crescenzi g gambosi v kann 
a marchetti-spaccamela and m protasi complexity and 
approximation combinatorial optimization problems and 
their approximability properties springer 
 j m bilbao j r fernandez a j losada and j j lopez 
generating functions for computing power indices 
efficiently top - 
 p bork h grote d notz and m regler data analysis 
techniques in high energy physics experiments cambridge 
university press 
 v conitzer and t sandholm computing shapley values 
manipulating value division schemes and checking core 
membership in multi-issue domains in proceedings of the 
national conference on artificial intelligence pages 
 - san jose california 
 x deng and c h papadimitriou on the complexity of 
cooperative solution concepts mathematics of operations 
research - 
 s s fatima m wooldridge and n r jennings an 
analysis of the shapley value and its uncertainty for the 
voting game in proc th int workshop on agent mediated 
electronic commerce pages - 
 a francis advanced level statistics stanley thornes 
publishers 
 s ieong and y shoham marginal contribution nets a 
compact representation scheme for coalitional games in 
proceedings of the sixth acm conference on electronic 
commerce pages - vancouver canada 
 s ieong and y shoham multi-attribute coalition games in 
proceedings of the seventh acm conference on electronic 
commerce pages - ann arbor michigan 
 j p kahan and a rapoport theories of coalition 
formation lawrence erlbaum associates publishers 
 i mann and l s shapley values for large games iv 
evaluating the electoral college exactly technical report 
the rand corporation santa monica 
 a mascolell m whinston and j r green 
microeconomic theory oxford university press 
 m j osborne and a rubinstein a course in game theory 
the mit press 
 c h papadimitriou computational complexity addison 
wesley longman 
 a rapoport n-person game theory concepts and 
applications dover publications mineola ny 
 a e roth introduction to the shapley value in a e roth 
editor the shapley value pages - university of 
cambridge press cambridge 
 t sandholm and v lesser coalitions among 
computationally bounded agents artificial intelligence 
journal - 
 l s shapley a value for n person games in a e roth 
editor the shapley value pages - university of 
cambridge press cambridge 
 o shehory and s kraus a kernel-oriented model for 
coalition-formation in general environments implemetation 
and results in in proceedings of the national conference on 
artificial intelligence aaai- pages - 
 o shehory and s kraus methods for task allocation via 
agent coalition formation artificial intelligence journal 
 - 
 j r taylor an introduction to error analysis the study of 
uncertainties in physical measurements university science 
books 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
an efficient heuristic approach for security against 
multiple adversaries 
praveen paruchuri jonathan p pearce 
milind tambe fernando ordonez 
university of southern california 
los angeles ca 
{paruchur jppearce tambe fordon} usc edu 
sarit kraus 
bar-ilan university 
ramat-gan israel 
sarit cs biu ac il 
abstract 
in adversarial multiagent domains security commonly defined as 
the ability to deal with intentional threats from other agents is a 
critical issue this paper focuses on domains where these threats 
come from unknown adversaries these domains can be modeled 
as bayesian games much work has been done on finding equilibria 
for such games however it is often the case in multiagent security 
domains that one agent can commit to a mixed strategy which its 
adversaries observe before choosing their own strategies in this 
case the agent can maximize reward by finding an optimal 
strategy without requiring equilibrium previous work has shown this 
problem of optimal strategy selection to be np-hard therefore 
we present a heuristic called asap with three key advantages to 
address the problem first asap searches for the highest-reward 
strategy rather than a bayes-nash equilibrium allowing it to find 
feasible strategies that exploit the natural first-mover advantage of 
the game second it provides strategies which are simple to 
understand represent and implement third it operates directly on the 
compact bayesian game representation without requiring 
conversion to normal form we provide an efficient mixed integer linear 
program milp implementation for asap along with 
experimental results illustrating significant speedups and higher rewards over 
other approaches 
categories and subject descriptors 
i computing methodologies artificial intelligence 
distributed artificial intelligence - intelligent agents 
general terms 
security design theory 
 introduction 
in many multiagent domains agents must act in order to 
provide security against attacks by adversaries a common issue that 
agents face in such security domains is uncertainty about the 
adversaries they may be facing for example a security robot may 
need to make a choice about which areas to patrol and how often 
 however it will not know in advance exactly where a robber 
will choose to strike a team of unmanned aerial vehicles uavs 
 monitoring a region undergoing a humanitarian crisis may also 
need to choose a patrolling policy they must make this decision 
without knowing in advance whether terrorists or other adversaries 
may be waiting to disrupt the mission at a given location it may 
indeed be possible to model the motivations of types of adversaries 
the agent or agent team is likely to face in order to target these 
adversaries more closely however in both cases the security robot 
or uav team will not know exactly which kinds of adversaries may 
be active on any given day 
a common approach for choosing a policy for agents in such 
scenarios is to model the scenarios as bayesian games a bayesian 
game is a game in which agents may belong to one or more types 
the type of an agent determines its possible actions and payoffs 
the distribution of adversary types that an agent will face may 
be known or inferred from historical data usually these games 
are analyzed according to the solution concept of a bayes-nash 
equilibrium an extension of the nash equilibrium for bayesian 
games however in many settings a nash or bayes-nash 
equilibrium is not an appropriate solution concept since it assumes that 
the agents strategies are chosen simultaneously 
in some settings one player can or must commit to a strategy 
before the other players choose their strategies these scenarios are 
known as stackelberg games in a stackelberg game a leader 
commits to a strategy first and then a follower or group of 
followers selfishly optimize their own rewards considering the action 
chosen by the leader for example the security agent leader must 
first commit to a strategy for patrolling various areas this strategy 
could be a mixed strategy in order to be unpredictable to the 
robbers followers the robbers after observing the pattern of patrols 
over time can then choose their strategy which location to rob 
often the leader in a stackelberg game can attain a higher 
reward than if the strategies were chosen simultaneously to see the 
advantage of being the leader in a stackelberg game consider a 
simple game with the payoff table as shown in table the leader 
is the row player and the follower is the column player here the 
leader s payoff is listed first 
 
 
 
table payoff table for example normal form game 
the only nash equilibrium for this game is when the leader plays 
 and the follower plays which gives the leader a payoff of 
 
 - - - - rps c ifaamas 
however if the leader commits to a uniform mixed strategy of 
playing and with equal probability the follower s best 
response is to play to get an expected payoff of and with 
equal probability the leader s payoff would then be and 
with equal probability in this case the leader now has an 
incentive to deviate and choose a pure strategy of to get a payoff of 
 however this would cause the follower to deviate to strategy 
 as well resulting in the nash equilibrium thus by committing 
to a strategy that is observed by the follower and by avoiding the 
temptation to deviate the leader manages to obtain a reward higher 
than that of the best nash equilibrium 
the problem of choosing an optimal strategy for the leader to 
commit to in a stackelberg game is analyzed in and found to 
be np-hard in the case of a bayesian game with multiple types of 
followers thus efficient heuristic techniques for choosing 
highreward strategies in these games is an important open issue 
methods for finding optimal leader strategies for non-bayesian games 
 can be applied to this problem by converting the bayesian game 
into a normal-form game by the harsanyi transformation if on 
the other hand we wish to compute the highest-reward nash 
equilibrium new methods using mixed-integer linear programs milps 
 may be used since the highest-reward bayes-nash 
equilibrium is equivalent to the corresponding nash equilibrium in the 
transformed game however by transforming the game the 
compact structure of the bayesian game is lost in addition since the 
nash equilibrium assumes a simultaneous choice of strategies the 
advantages of being the leader are not considered 
this paper introduces an efficient heuristic method for 
approximating the optimal leader strategy for security domains known as 
asap agent security via approximate policies this method has 
three key advantages first it directly searches for an optimal 
strategy rather than a nash or bayes-nash equilibrium thus allowing 
it to find high-reward non-equilibrium strategies like the one in the 
above example second it generates policies with a support which 
can be expressed as a uniform distribution over a multiset of fixed 
size as proposed in this allows for policies that are simple 
to understand and represent as well as a tunable parameter 
 the size of the multiset that controls the simplicity of the policy 
third the method allows for a bayes-nash game to be expressed 
compactly without conversion to a normal-form game allowing for 
large speedups over existing nash methods such as and 
the rest of the paper is organized as follows in section we 
fully describe the patrolling domain and its properties section 
introduces the bayesian game the harsanyi transformation and 
existing methods for finding an optimal leader s strategy in a 
stackelberg game then in section the asap algorithm is presented 
for normal-form games and in section we show how it can be 
adapted to the structure of bayesian games with uncertain 
adversaries experimental results showing higher reward and faster 
policy computation over existing nash methods are shown in section 
 and we conclude with a discussion of related work in section 
 the patrolling domain 
in most security patrolling domains the security agents like 
uavs or security robots cannot feasibly patrol all areas all 
the time instead they must choose a policy by which they patrol 
various routes at different times taking into account factors such as 
the likelihood of crime in different areas possible targets for crime 
and the security agents own resources number of security agents 
amount of available time fuel etc it is usually beneficial for 
this policy to be nondeterministic so that robbers cannot safely rob 
certain locations knowing that they will be safe from the security 
agents to demonstrate the utility of our algorithm we use a 
simplified version of such a domain expressed as a game 
the most basic version of our game consists of two players the 
security agent the leader and the robber the follower in a world 
consisting of m houses m the security agent s set of pure 
strategies consists of possible routes of d houses to patrol in an 
order the security agent can choose a mixed strategy so that the 
robber will be unsure of exactly where the security agent may 
patrol but the robber will know the mixed strategy the security agent 
has chosen for example the robber can observe over time how 
often the security agent patrols each area with this knowledge the 
robber must choose a single house to rob we assume that the 
robber generally takes a long time to rob a house if the house chosen 
by the robber is not on the security agent s route then the robber 
successfully robs it otherwise if it is on the security agent s route 
then the earlier the house is on the route the easier it is for the 
security agent to catch the robber before he finishes robbing it 
we model the payoffs for this game with the following variables 
 vl x value of the goods in house l to the security agent 
 vl q value of the goods in house l to the robber 
 cx reward to the security agent of catching the robber 
 cq cost to the robber of getting caught 
 pl probability that the security agent can catch the robber at 
the lth house in the patrol pl pl ⇐⇒ l l 
the security agent s set of possible pure strategies patrol routes 
is denoted by x and includes all d-tuples i w w wd 
with w wd m where no two elements are equal the 
agent is not allowed to return to the same house the robber s 
set of possible pure strategies houses to rob is denoted by q and 
includes all integers j m the payoffs security agent 
robber for pure strategies i j are 
 −vl x vl q for j l ∈ i 
 plcx −pl −vl x −plcq −pl vl q for j l ∈ i 
with this structure it is possible to model many different types 
of robbers who have differing motivations for example one robber 
may have a lower cost of getting caught than another or may value 
the goods in the various houses differently if the distribution of 
different robber types is known or inferred from historical data 
then the game can be modeled as a bayesian game 
 bayesian games 
a bayesian game contains a set of n agents and each agent n 
must be one of a given set of types θn for our patrolling domain 
we have two agents the security agent and the robber θ is the set 
of security agent types and θ is the set of robber types since there 
is only one type of security agent θ contains only one element 
during the game the robber knows its type but the security agent 
does not know the robber s type for each agent the security agent 
or the robber n there is a set of strategies σn and a utility function 
un θ × θ × σ × σ → 
a bayesian game can be transformed into a normal-form game 
using the harsanyi transformation once this is done new 
linear-program lp -based methods for finding high-reward 
strategies for normal-form games can be used to find a strategy in the 
transformed game this strategy can then be used for the bayesian 
game while methods exist for finding bayes-nash equilibria 
directly without the harsanyi transformation they find only a 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
single equilibrium in the general case which may not be of high 
reward recent work has led to efficient mixed-integer linear 
program techniques to find the best nash equilibrium for a given 
agent however these techniques do require a normal-form game 
and so to compare the policies given by asap against the optimal 
policy as well as against the highest-reward nash equilibrium we 
must apply these techniques to the harsanyi-transformed matrix 
the next two subsections elaborate on how this is done 
 harsanyi transformation 
the first step in solving bayesian games is to apply the harsanyi 
transformation that converts the bayesian game into a normal 
form game given that the harsanyi transformation is a standard 
concept in game theory we explain it briefly through a simple 
example in our patrolling domain without introducing the 
mathematical formulations let us assume there are two robber types a and 
b in the bayesian game robber a will be active with probability 
α and robber b will be active with probability − α the rules 
described in section allow us to construct simple payoff tables 
assume that there are two houses in the world and and 
hence there are two patrol routes pure strategies for the agent 
{ } and { } the robber can rob either house or house 
and hence he has two strategies denoted as l l for robber type 
l since there are two types assumed denoted as a and b we 
construct two payoff tables shown in table corresponding to 
the security agent playing a separate game with each of the two 
robber types with probabilities α and − α first consider robber 
type a borrowing the notation from the domain section we assign 
the following values to the variables v x v q v x 
v q cx cq p p using these 
values we construct a base payoff table as the payoff for the game 
against robber type a for example if the security agent chooses 
route { } when robber a is active and robber a chooses house 
the robber receives a reward of - for being caught and the agent 
receives a reward of for catching the robber the payoffs for the 
game against robber type b are constructed using different values 
security agent { } { } 
robber a 
 a - - 
 a - - - 
robber b 
 b - - 
 b - - - 
table payoff tables security agent vs robbers a and b 
using the harsanyi technique involves introducing a chance node 
that determines the robber s type thus transforming the security 
agent s incomplete information regarding the robber into imperfect 
information the bayesian equilibrium of the game is then 
precisely the nash equilibrium of the imperfect information game the 
transformed normal-form game is shown in table in the 
transformed game the security agent is the column player and the set 
of all robber types together is the row player suppose that robber 
type a robs house and robber type b robs house while the 
security agent chooses patrol { } then the security agent and the 
robber receive an expected payoff corresponding to their payoffs 
from the agent encountering robber a at house with probability α 
and robber b at house with probability − α 
 finding an optimal strategy 
although a nash equilibrium is the standard solution concept for 
games in which agents choose strategies simultaneously in our 
security domain the security agent the leader can gain an advantage 
by committing to a mixed strategy in advance since the followers 
 the robbers will know the leader s strategy the optimal response 
for the followers will be a pure strategy given the common 
assumption taken in in the case where followers are indifferent 
they will choose the strategy that benefits the leader there must 
exist a guaranteed optimal strategy for the leader 
from the bayesian game in table we constructed the harsanyi 
transformed bimatrix in table the strategies for each player 
 security agent or robber in the transformed game correspond to all 
combinations of possible strategies taken by each of that player s 
types therefore we denote x σθ 
 σ and q σθ 
 as the 
index sets of the security agent and robbers pure strategies 
respectively with r and c as the corresponding payoff matrices rij is 
the reward of the security agent and cij is the reward of the 
robbers when the security agent takes pure strategy i and the robbers 
take pure strategy j a mixed strategy for the security agent is a 
probability distribution over its set of pure strategies and will be 
represented by a vector x px px px x where pxi ≥ 
and 
p 
pxi here pxi is the probability that the security agent 
will choose its ith pure strategy 
the optimal mixed strategy for the security agent can be found 
in time polynomial in the number of rows in the normal form game 
using the following linear program formulation from 
for every possible pure strategy j by the follower the set of all 
robber types 
max 
p 
i∈x pxirij 
s t ∀j ∈ q 
p 
i∈σ 
pxicij ≥ 
p 
i∈σ 
pxicij 
p 
i∈x pxi 
∀i∈x pxi 
 
then for all feasible follower strategies j choose the one that 
maximizes 
p 
i∈x pxirij the reward for the security agent leader 
the pxi variables give the optimal strategy for the security agent 
note that while this method is polynomial in the number of rows 
in the transformed normal-form game the number of rows 
increases exponentially with the number of robber types using this 
method for a bayesian game thus requires running σ θ 
 
separate linear programs this is no surprise since finding the leader s 
optimal strategy in a bayesian stackelberg game is np-hard 
 heuristic approaches 
given that finding the optimal strategy for the leader is np-hard 
we provide a heuristic approach in this heuristic we limit the 
possible mixed strategies of the leader to select actions with 
probabilities that are integer multiples of k for a predetermined integer 
k previous work has shown that strategies with high entropy 
are beneficial for security applications when opponents utilities 
are completely unknown in our domain if utilities are not 
considered this method will result in uniform-distribution strategies 
one advantage of such strategies is that they are compact to 
represent as fractions and simple to understand therefore they can 
be efficiently implemented by real organizations we aim to 
maintain the advantage provided by simple strategies for our security 
application problem incorporating the effect of the robbers 
rewards on the security agent s rewards thus the asap heuristic 
will produce strategies which are k-uniform a mixed strategy is 
denoted k-uniform if it is a uniform distribution on a multiset s of 
pure strategies with s k a multiset is a set whose elements 
may be repeated multiple times thus for example the mixed 
strategy corresponding to the multiset { } would take strategy 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
{ } { } 
{ a b} − α − − α α − α − α − − α α − α 
{ a b} − α − − α α − − α − α − − α α − α 
{ a b} − α − − α − α − α − α − − α α − α 
{ a b} − α − − α − α − − α − α − − α α − α 
table harsanyi transformed payoff table 
with probability and strategy with probability asap 
allows the size of the multiset to be chosen in order to balance the 
complexity of the strategy reached with the goal that the identified 
strategy will yield a high reward 
another advantage of the asap heuristic is that it operates 
directly on the compact bayesian representation without requiring 
the harsanyi transformation this is because the different follower 
 robber types are independent of each other hence evaluating 
the leader strategy against a harsanyi-transformed game matrix 
is equivalent to evaluating against each of the game matrices for 
the individual follower types this independence property is 
exploited in asap to yield a decomposition scheme note that the lp 
method introduced by to compute optimal stackelberg policies 
is unlikely to be decomposable into a small number of games as it 
was shown to be np-hard for bayes-nash problems finally note 
that asap requires the solution of only one optimization problem 
rather than solving a series of problems as in the lp method of 
for a single follower type the algorithm works the following 
way given a particular k for each possible mixed strategy x for the 
leader that corresponds to a multiset of size k evaluate the leader s 
payoff from x when the follower plays a reward-maximizing pure 
strategy we then take the mixed strategy with the highest payoff 
we need only to consider the reward-maximizing pure 
strategies of the followers robbers since for a given fixed strategy x 
of the security agent each robber type faces a problem with fixed 
linear rewards if a mixed strategy is optimal for the robber then 
so are all the pure strategies in the support of that mixed strategy 
note also that because we limit the leader s strategies to take on 
discrete values the assumption from section that the followers 
will break ties in the leader s favor is not significant since ties will 
be unlikely to arise this is because in domains where rewards are 
drawn from any random distribution the probability of a follower 
having more than one pure optimal response to a given leader 
strategy approaches zero and the leader will have only a finite number 
of possible mixed strategies 
our approach to characterize the optimal strategy for the security 
agent makes use of properties of linear programming we briefly 
outline these results here for completeness for detailed discussion 
and proofs see one of many references on the topic such as 
every linear programming problem such as 
max ct 
x ax b x ≥ 
has an associated dual linear program in this case 
min bt 
y at 
y ≥ c 
these primal dual pairs of problems satisfy weak duality for any x 
and y primal and dual feasible solutions respectively ct 
x ≤ bt 
y 
thus a pair of feasible solutions is optimal if ct 
x bt 
y and 
the problems are said to satisfy strong duality in fact if a linear 
program is feasible and has a bounded optimal solution then the 
dual is also feasible and there is a pair x∗ 
 y∗ 
that satisfies ct 
x∗ 
 
bt 
y∗ 
 these optimal solutions are characterized with the following 
optimality conditions as defined in 
 primal feasibility ax b x ≥ 
 dual feasibility at 
y ≥ c 
 complementary slackness xi at 
y − c i for all i 
note that this last condition implies that 
ct 
x xt 
at 
y bt 
y 
which proves optimality for primal dual feasible solutions x and y 
in the following subsections we first define the problem in its 
most intuititive form as a mixed-integer quadratic program miqp 
and then show how this problem can be converted into a 
mixedinteger linear program milp 
 mixed-integer quadratic program 
we begin with the case of a single type of follower let the 
leader be the row player and the follower the column player we 
denote by x the vector of strategies of the leader and q the vector 
of strategies of the follower we also denote x and q the index 
sets of the leader and follower s pure strategies respectively the 
payoff matrices r and c correspond to rij is the reward of the 
leader and cij is the reward of the follower when the leader takes 
pure strategy i and the follower takes pure strategy j let k be the 
size of the multiset 
we first fix the policy of the leader to some k-uniform policy 
x the value xi is the number of times pure strategy i is used in 
the k-uniform policy which is selected with probability xi k we 
formulate the optimization problem the follower solves to find its 
optimal response to x as the following linear program 
max 
x 
j∈q 
x 
i∈x 
 
k 
cijxi qj 
s t 
p 
j∈q qj 
q ≥ 
 
the objective function maximizes the follower s expected reward 
given x while the constraints make feasible any mixed strategy q 
for the follower the dual to this linear programming problem is 
the following 
min a 
s t a ≥ 
x 
i∈x 
 
k 
cijxi j ∈ q 
from strong duality and complementary slackness we obtain that 
the follower s maximum reward value a is the value of every pure 
strategy with qj that is in the support of the optimal mixed 
strategy therefore each of these pure strategies is optimal 
optimal solutions to the follower s problem are characterized by linear 
programming optimality conditions primal feasibility constraints 
in dual feasibility constraints in and complementary 
slackness 
qj a − 
x 
i∈x 
 
k 
cijxi 
 
 j ∈ q 
these conditions must be included in the problem solved by the 
leader in order to consider only best responses by the follower to 
the k-uniform policy x 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the leader seeks the k-uniform solution x that maximizes its 
own payoff given that the follower uses an optimal response q x 
therefore the leader solves the following integer problem 
max 
x 
i∈x 
x 
j∈q 
 
k 
rijq x j xi 
s t 
p 
i∈x xi k 
xi ∈ { k} 
 
problem maximizes the leader s reward with the follower s best 
response qj for fixed leader s policy x and hence denoted q x j 
by selecting a uniform policy from a multiset of constant size k we 
complete this problem by including the characterization of q x 
through linear programming optimality conditions to simplify 
writing the complementary slackness conditions we will constrain 
q x to be only optimal pure strategies by just considering integer 
solutions of q x the leader s problem becomes 
maxx q 
x 
i∈x 
x 
j∈q 
 
k 
rijxiqj 
s t 
p 
i xi kp 
j∈q qj 
 ≤ a − 
p 
i∈x 
 
k 
cijxi ≤ − qj m 
xi ∈ { k} 
qj ∈ { } 
 
here the constant m is some large number the first and fourth 
constraints enforce a k-uniform policy for the leader and the 
second and fifth constraints enforce a feasible pure strategy for the 
follower the third constraint enforces dual feasibility of the 
follower s problem leftmost inequality and the complementary 
slackness constraint for an optimal pure strategy q for the follower 
 rightmost inequality in fact since only one pure strategy can be 
selected by the follower say qh this last constraint enforces that 
a 
p 
i∈x 
 
k 
cihxi imposing no additional constraint for all other 
pure strategies which have qj 
we conclude this subsection noting that problem is an 
integer program with a non-convex quadratic objective in general 
as the matrix r need not be positive-semi-definite efficient 
solution methods for non-linear non-convex integer problems remains 
a challenging research question in the next section we show a 
reformulation of this problem as a linear integer programming 
problem for which a number of efficient commercial solvers exist 
 mixed-integer linear program 
we can linearize the quadratic program of problem through the 
change of variables zij xiqj obtaining the following problem 
maxq z 
p 
i∈x 
p 
j∈q 
 
k 
rijzij 
s t 
p 
i∈x 
p 
j∈q zij k 
p 
j∈q zij ≤ k 
kqj ≤ 
p 
i∈x zij ≤ k 
p 
j∈q qj 
 ≤ a − 
p 
i∈x 
 
k 
cij 
p 
h∈q zih ≤ − qj m 
zij ∈ { k} 
qj ∈ { } 
 
proposition problems and are equivalent 
proof consider x q a feasible solution of we will show 
that q zij xiqj is a feasible solution of of same objective 
function value the equivalence of the objective functions and 
constraints and of are satisfied by construction the fact 
that 
p 
j∈q zij xi as 
p 
j∈q qj explains constraints and 
 of constraint of is satisfied because 
p 
i∈x zij kqj 
let us now consider q z feasible for we will show that q and 
xi 
p 
j∈q zij are feasible for with the same objective value 
in fact all constraints of are readily satisfied by construction to 
see that the objectives match notice that if qh then the third 
constraint in implies that 
p 
i∈x zih k which means that 
zij for all i ∈ x and all j h therefore 
xiqj 
x 
l∈q 
zilqj zihqj zij 
this last equality is because both are when j h this shows 
that the transformation preserves the objective function value 
completing the proof 
given this transformation to a mixed-integer linear program milp 
we now show how we can apply our decomposition technique on 
the milp to obtain significant speedups for bayesian games with 
multiple follower types 
 decomposition for multiple 
adversaries 
the milp developed in the previous section handles only one 
follower since our security scenario contains multiple follower 
 robber types we change the response function for the follower 
from a pure strategy into a weighted combination over various pure 
follower strategies where the weights are probabilities of 
occurrence of each of the follower types 
 decomposed miqp 
to admit multiple adversaries in our framework we modify the 
notation defined in the previous section to reason about multiple 
follower types we denote by x the vector of strategies of the leader 
and ql 
the vector of strategies of follower l with l denoting the 
index set of follower types we also denote by x and q the index 
sets of leader and follower l s pure strategies respectively we also 
index the payoff matrices on each follower l considering the 
matrices rl 
and cl 
 
using this modified notation we characterize the optimal 
solution of follower l s problem given the leaders k-uniform policy x 
with the following optimality conditions 
x 
j∈q 
ql 
j 
al 
− 
x 
i∈x 
 
k 
cl 
ijxi ≥ 
ql 
j al 
− 
x 
i∈x 
 
k 
cl 
ijxi 
ql 
j ≥ 
again considering only optimal pure strategies for follower l s 
problem we can linearize the complementarity constraint above 
we incorporate these constraints on the leader s problem that 
selects the optimal k-uniform policy therefore given a priori 
probabilities pl 
 with l ∈ l of facing each follower the leader solves 
the following problem 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
maxx q 
x 
i∈x 
x 
l∈l 
x 
j∈q 
pl 
k 
rl 
ijxiql 
j 
s t 
p 
i xi kp 
j∈q ql 
j 
 ≤ al 
− 
p 
i∈x 
 
k 
cl 
ijxi ≤ − ql 
j m 
xi ∈ { k} 
ql 
j ∈ { } 
 
problem for a bayesian game with multiple follower types 
is indeed equivalent to problem on the payoff matrix obtained 
from the harsanyi transformation of the game in fact every pure 
strategy j in problem corresponds to a sequence of pure 
strategies jl one for each follower l ∈ l this means that qj if 
and only if ql 
jl 
 for all l ∈ l in addition given the a 
priori probabilities pl 
of facing player l the reward in the harsanyi 
transformation payoff table is rij 
p 
l∈l pl 
rl 
ijl 
 the same 
relation holds between c and cl 
 these relations between a pure 
strategy in the equivalent normal form game and pure strategies in 
the individual games with each followers are key in showing these 
problems are equivalent 
 decomposed milp 
we can linearize the quadratic programming problem through 
the change of variables zl 
ij xiql 
j obtaining the following 
problem 
maxq z 
p 
i∈x 
p 
l∈l 
p 
j∈q 
pl 
k 
rl 
ijzl 
ij 
s t 
p 
i∈x 
p 
j∈q zl 
ij k 
p 
j∈q zl 
ij ≤ k 
kql 
j ≤ 
p 
i∈x zl 
ij ≤ k 
p 
j∈q ql 
j 
 ≤ al 
− 
p 
i∈x 
 
k 
cl 
ij 
p 
h∈q zl 
ih ≤ − ql 
j m 
p 
j∈q zl 
ij 
p 
j∈q z 
ij 
zl 
ij ∈ { k} 
ql 
j ∈ { } 
 
proposition problems and are equivalent 
proof consider x ql 
 al 
with l ∈ l a feasible solution of 
we will show that ql 
 al 
 zl 
ij xiql 
j is a feasible solution of 
of same objective function value the equivalence of the objective 
functions and constraints and of are satisfied by 
construction the fact that 
p 
j∈q zl 
ij xi as 
p 
j∈q ql 
j explains 
constraints and of constraint of is satisfied 
because 
p 
i∈x zl 
ij kql 
j 
lets now consider ql 
 zl 
 al 
feasible for we will show that 
ql 
 al 
and xi 
p 
j∈q z 
ij are feasible for with the same 
objective value in fact all constraints of are readily satisfied by 
construction to see that the objectives match notice for each l 
one ql 
j must equal and the rest equal let us say that ql 
jl 
 
then the third constraint in implies that 
p 
i∈x zl 
ijl 
 k which 
means that zl 
ij for all i ∈ x and all j jl in particular this 
implies that 
xi 
x 
j∈q 
z 
ij z 
ij 
 zl 
ijl 
 
the last equality from constraint of therefore xiql 
j zl 
ijl 
ql 
j 
zl 
ij this last equality is because both are when j jl 
effectively constraint ensures that all the adversaries are calculating 
their best responses against a particular fixed policy of the agent 
this shows that the transformation preserves the objective function 
value completing the proof 
we can therefore solve this equivalent linear integer program 
with efficient integer programming packages which can handle 
problems with thousands of integer variables we implemented the 
decomposed milp and the results are shown in the following section 
 experimental results 
the patrolling domain and the payoffs for the associated game 
are detailed in sections and we performed experiments for this 
game in worlds of three and four houses with patrols consisting of 
two houses the description given in section is used to generate 
a base case for both the security agent and robber payoff functions 
the payoff tables for additional robber types are constructed and 
added to the game by adding a random distribution of varying size 
to the payoffs in the base case all games are normalized so that 
for each robber type the minimum and maximum payoffs to the 
security agent and robber are and respectively 
using the data generated we performed the experiments using 
four methods for generating the security agent s strategy 
 uniform randomization 
 asap 
 the multiple linear programs method from to find the true 
optimal strategy 
 the highest reward bayes-nash equilibrium found using the 
mip-nash algorithm 
the last three methods were applied using cplex because 
the last two methods are designed for normal-form games rather 
than bayesian games the games were first converted using the 
harsanyi transformation the uniform randomization method is 
simply choosing a uniform random policy over all possible patrol 
routes we use this method as a simple baseline to measure the 
performance of our heuristics we anticipated that the uniform policy 
would perform reasonably well since maximum-entropy policies 
have been shown to be effective in multiagent security domains 
 the highest-reward bayes-nash equilibria were used in order 
to demonstrate the higher reward gained by looking for an optimal 
policy rather than an equilibria in stackelberg games such as our 
security domain 
based on our experiments we present three sets of graphs to 
demonstrate the runtime of asap compared to other common 
methods for finding a strategy the reward guaranteed by asap 
compared to other methods and the effect of varying the 
parameter k the size of the multiset on the performance of asap 
in the first two sets of graphs asap is run using a multiset of 
 elements in the third set this number is varied the first set of 
graphs shown in figure shows the runtime graphs for three-house 
 left column and four-house right column domains each of the 
three rows of graphs corresponds to a different randomly-generated 
scenario the x-axis shows the number of robber types the 
security agent faces and the y-axis of the graph shows the runtime in 
seconds all experiments that were not concluded in minutes 
 seconds were cut off the runtime for the uniform policy 
is always negligible irrespective of the number of adversaries and 
hence is not shown 
the asap algorithm clearly outperforms the optimal 
multiplelp method as well as the mip-nash algorithm for finding the 
highestreward bayes-nash equilibrium with respect to runtime for a 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure runtimes for various algorithms on problems of 
and houses 
domain of three houses the optimal method cannot reach a 
solution for more than seven robber types and for four houses it 
cannot solve for more than six types within the cutoff time in any of 
the three scenarios mip-nash solves for even fewer robber types 
within the cutoff time on the other hand asap runs much faster 
and is able to solve for at least adversaries for the three-house 
scenarios and for at least adversaries in the four-house 
scenarios within the cutoff time the runtime of asap does not increase 
strictly with the number of robber types for each scenario but in 
general the addition of more types increases the runtime required 
the second set of graphs figure shows the reward to the patrol 
agent given by each method for three scenarios in the three-house 
 left column and four-house right column domains this reward 
is the utility received by the security agent in the patrolling game 
and not as a percentage of the optimal reward since it was not 
possible to obtain the optimal reward as the number of robber types 
increased the uniform policy consistently provides the lowest 
reward in both domains while the optimal method of course 
produces the optimal reward the asap method remains consistently 
close to the optimal even as the number of robber types increases 
the highest-reward bayes-nash equilibria provided by the 
mipnash method produced rewards higher than the uniform method 
but lower than asap this difference clearly illustrates the gains in 
the patrolling domain from committing to a strategy as the leader 
in a stackelberg game rather than playing a standard bayes-nash 
strategy 
the third set of graphs shown in figure shows the effect of the 
multiset size on runtime in seconds left column and reward right 
column again expressed as the reward received by the security 
agent in the patrolling game and not a percentage of the optimal 
figure reward for various algorithms on problems of and 
 houses 
reward results here are for the three-house domain the trend is 
that as as the multiset size is increased the runtime and reward level 
both increase not surprisingly the reward increases monotonically 
as the multiset size increases but what is interesting is that there is 
relatively little benefit to using a large multiset in this domain in 
all cases the reward given by a multiset of elements was within 
at least of the reward given by an -element multiset the 
runtime does not always increase strictly with the multiset size 
indeed in one example scenario with robber types using a 
multiset of elements took seconds while using elements 
only took seconds in general runtime should increase since a 
larger multiset means a larger domain for the variables in the milp 
and thus a larger search space however an increase in the number 
of variables can sometimes allow for a policy to be constructed 
more quickly due to more flexibility in the problem 
 summary and related work 
this paper focuses on security for agents patrolling in hostile 
environments in these environments intentional threats are caused 
by adversaries about whom the security patrolling agents have 
incomplete information specifically we deal with situations where 
the adversaries actions and payoffs are known but the exact 
adversary type is unknown to the security agent agents acting in the 
real world quite frequently have such incomplete information about 
other agents bayesian games have been a popular choice to model 
such incomplete information games the gala toolkit is one 
method for defining such games without requiring the game to 
be represented in normal form via the harsanyi transformation 
gala s guarantees are focused on fully competitive games much 
work has been done on finding optimal bayes-nash equilbria for 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure reward for asap using multisets of and 
elements 
subclasses of bayesian games finding single bayes-nash 
equilibria for general bayesian games or approximate bayes-nash 
equilibria less attention has been paid to finding the optimal 
strategy to commit to in a bayesian game the stackelberg scenario 
 however the complexity of this problem was shown to be 
np-hard in the general case which also provides algorithms for 
this problem in the non-bayesian case 
therefore we present a heuristic called asap with three key 
advantages towards addressing this problem first asap searches 
for the highest reward strategy rather than a bayes-nash 
equilibrium allowing it to find feasible strategies that exploit the 
natural first-mover advantage of the game second it provides 
strategies which are simple to understand represent and implement 
third it operates directly on the compact bayesian game 
representation without requiring conversion to normal form we provide 
an efficient mixed integer linear program milp implementation 
for asap along with experimental results illustrating significant 
speedups and higher rewards over other approaches 
our k-uniform strategies are similar to the k-uniform strategies 
of while that work provides epsilon error-bounds based on 
the k-uniform strategies their solution concept is still that of a 
nash equilibrium and they do not provide efficient algorithms for 
obtaining such k-uniform strategies this contrasts with asap 
where our emphasis is on a highly efficient heuristic approach that 
is not focused on equilibrium solutions 
finally the patrolling problem which motivated our work has 
recently received growing attention from the multiagent community 
due to its wide range of applications however most of this 
work is focused on either limiting energy consumption involved in 
patrolling or optimizing on criteria like the length of the path 
traveled without reasoning about any explicit model of an 
adversary 
acknowledgments this research is supported by the united states 
department of homeland security through center for risk and economic 
analysis of terrorism events create it is also supported by the 
defense advanced research projects agency darpa through the 
department of the interior nbc acquisition services division under contract 
no nbchd sarit kraus is also affiliated with umiacs 
 references 
 r w beard and t mclain multiple uav cooperative 
search under collision avoidance and limited range 
communication constraints in ieee cdc 
 d bertsimas and j tsitsiklis introduction to linear 
optimization athena scientific 
 j brynielsson and s arnborg bayesian games for threat 
prediction and situation analysis in fusion 
 y chevaleyre theoretical analysis of multi-agent patrolling 
problem in aamas 
 v conitzer and t sandholm choosing the best strategy to 
commit to in acm conference on electronic commerce 
 
 d fudenberg and j tirole game theory mit press 
 c gui and p mohapatra virtual patrol a new power 
conservation design for surveillance using sensor networks 
in ipsn 
 j c harsanyi and r selten a generalized nash solution for 
two-person bargaining games with incomplete information 
management science - 
 d koller and a pfeffer generating and solving imperfect 
information games in ijcai pages - 
 d koller and a pfeffer representations and solutions for 
game-theoretic problems artificial intelligence 
 - 
 c lemke and j howson equilibrium points of bimatrix 
games journal of the society for industrial and applied 
mathematics - 
 r j lipton e markakis and a mehta playing large 
games using simple strategies in acm conference on 
electronic commerce 
 a machado g ramalho j d zucker and a drougoul 
multi-agent patrolling an empirical analysis on alternative 
architectures in mabs 
 p paruchuri m tambe f ordonez and s kraus security 
in multiagent systems by policy randomization in aamas 
 
 t roughgarden stackelberg scheduling strategies in acm 
symposium on toc 
 s ruan c meirina f yu k r pattipati and r l popp 
patrolling in a stochastic environment in th intl 
command and control research symp 
 t sandholm a gilpin and v conitzer mixed-integer 
programming methods for finding nash equilibria in aaai 
 
 s singh v soni and m wellman computing approximate 
bayes-nash equilibria with tree-games of incomplete 
information in acm conference on electronic commerce 
 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
searching for joint gains in automated negotiations 
based on multi-criteria decision making theory 
quoc bao vo 
school of computer science and it 
rmit university australia 
vqbao cs rmit edu au 
lin padgham 
school of computer science and it 
rmit university australia 
linpa cs rmit edu au 
abstract 
it is well established by conflict theorists and others that successful 
negotiation should incorporate creating value as well as 
claiming value joint improvements that bring benefits to all parties 
can be realised by i identifying attributes that are not of direct 
conflict between the parties ii tradeoffs on attributes that are 
valued differently by different parties and iii searching for values 
within attributes that could bring more gains to one party while not 
incurring too much loss on the other party in this paper we 
propose an approach for maximising joint gains in automated 
negotiations by formulating the negotiation problem as a multi-criteria 
decision making problem and taking advantage of several 
optimisation techniques introduced by operations researchers and conflict 
theorists we use a mediator to protect the negotiating parties from 
unnecessary disclosure of information to their opponent while also 
allowing an objective calculation of maximum joint gains we 
separate out attributes that take a finite set of values simple attributes 
from those with continuous values and we show that for simple 
attributes the mediator can determine the pareto-optimal values 
in addition we show that if none of the simple attributes strongly 
dominates the other simple attributes then truth telling is an 
equilibrium strategy for negotiators during the optimisation of simple 
attributes we also describe an approach for improving joint gains 
on non-simple attributes by moving the parties in a series of steps 
towards the pareto-optimal frontier 
categories and subject descriptors 
i distributed artificial intelligence multiagent systems 
k computers and society electronic commerce 
general terms 
algorithms design 
 introduction 
given that negotiation is perhaps one of the oldest activities in 
the history of human communication it s perhaps surprising that 
conducted experiments on negotiations have shown that negotiators 
more often than not reach inefficient compromises raiffa 
 and sebenius provide analyses on the negotiators failure 
to achieve efficient agreements in practice and their unwillingness 
to disclose private information due to strategic reasons according 
to conflict theorists lax and sebenius most negotiation 
actually involves both integrative and distributive bargaining which 
they refer to as creating value and claiming value they argue 
that negotiation necessarily includes both cooperative and 
competitive elements and that these elements exist in tension negotiators 
face a dilemma in deciding whether to pursue a cooperative or a 
competitive strategy at a particular time during a negotiation they 
refer to this problem as the negotiator s dilemma 
we argue that the negotiator s dilemma is essentially 
informationbased due to the private information held by the agents such 
private information contains both the information that implies the 
agent s bottom lines or her walk-away positions and the 
information that enforces her bargaining strength for instance when 
bargaining to sell a house to a potential buyer the seller would 
try to hide her actual reserve price as much as possible for she 
hopes to reach an agreement at a much higher price than her 
reserve price on the other hand the outside options available to her 
 e g other buyers who have expressed genuine interest with fairly 
good offers consist in the information that improves her 
bargaining strength about which she would like to convey to her opponent 
but at the same time her opponent is well aware of the fact that it 
is her incentive to boost her bargaining strength and thus will not 
accept every information she sends out unless it is substantiated by 
evidence 
coming back to the negotiator s dilemma it s not always 
possible to separate the integrative bargaining process from the 
distributive bargaining process in fact more often than not the two 
processes interplay with each other making information manipulation 
become part of the integrative bargaining process this is because a 
negotiator could use the information about his opponent s interests 
against her during the distributive negotiation process that is a 
negotiator may refuse to concede on an important conflicting issue 
by claiming that he has made a major concession on another 
issue to meet his opponent s interests even though the concession he 
made could be insignificant to him for instance few buyers would 
start a bargaining with a dealer over a deal for a notebook computer 
by declaring that he is most interested in an extended warranty for 
the item and therefore prepared to pay a high price to get such an 
extended warranty 
negotiation support systems nsss and negotiating software 
 
 - - - - rps c ifaamas 
agents nsas have been introduced either to assist humans in 
making decisions or to enable automated negotiation to allow 
computer processes to engage in meaningful negotiation to reach 
agreements see for instance however because of 
the negotiator s dilemma and given even bargaining power and 
incomplete information the following two undesirable situations 
often arise i negotiators reach inefficient compromises or ii 
negotiators engage in a deadlock situation in which both 
negotiators refuse to act upon with incomplete information and at the same 
time do not want to disclose more information 
in this paper we argue for the role of a mediator to resolve the 
above two issues the mediator thus plays two roles in a 
negotiation i to encourage cooperative behaviour among the negotiators 
and ii to absorb the information disclosure by the negotiators to 
prevent negotiators from using uncertainty and private information 
as a strategic device to take advantage of existing results in 
negotiation analysis and operations research or literatures we 
employ multi-criteria decision making mcdm theory to allow 
the negotiation problem to be represented and analysed section 
provides background on mcdm theory and the negotiation 
framework section formulates the problem in section we discuss 
our approach to integrative negotiation section discusses the 
future work with some concluding remarks 
 background 
 multi-criteria decision making theory 
let a denote the set of feasible alternatives available to a 
decision maker m as an act or decision a in a may involve 
multiple aspects we usually describe the alternatives a with a set of 
attributes j j m attributes are also referred to as 
issues or decision variables a typical decision maker also has 
several objectives x xk we assume that xi i k 
maps the alternatives to real numbers thus a tuple x xk 
 x a xk a denotes the consequence of the act a to the 
decision maker m by definition objectives are statements that 
delineate the desires of a decision maker thus m wishes to 
maximise his objectives however as discussed thoroughly by keeney 
and raiffa it is quite likely that a decision maker s objectives 
will conflict with each other in that the improved achievement with 
one objective can only be accomplished at the expense of another 
for instance most businesses and public services have objectives 
like minimise cost and maximise the quality of services since 
better services can often only be attained for a price these 
objectives conflict 
due to the conflicting nature of a decision maker s objectives m 
usually has to settle at a compromise solution that is he may have 
to choose an act a ∈ a that does not optimise every objective this 
is the topic of the multi-criteria decision making theory part of the 
solution to this problem is that m has to try to identify the pareto 
frontier in the consequence space { x a xk a }a∈a 
definition dominant 
let x x xk and x x xk be two 
consequences x dominates x iff xi xi for all i and the inequality is 
strict for at least one i 
the pareto frontier in a consequence space then consists of all 
consequences that are not dominated by any other consequence 
this is illustrated in fig in which an alternative consists of two 
attributes d and d and the decision maker tries to maximise the 
two objectives x and x a decision a ∈ a whose consequence 
does not lie on the pareto frontier is inefficient while the pareto 
 x 
d 
a x a x a 
d 
 
x 
 
alternative spacea 
pareto frontier 
consequence space 
optimal consequenc 
figure the pareto frontier 
frontier allows m to avoid taking inefficient decisions m still has 
to decide which of the efficient consequences on the pareto frontier 
is most preferred by him 
mcdm theorists introduce a mechanism to allow the objective 
components of consequences to be normalised to the payoff 
valuations for the objectives consequences can then be ordered if the 
gains in satisfaction brought about by c in comparison to c 
equals to the losses in satisfaction brought about by c in 
comparison to c then the two consequences c and c are considered 
indifferent m can now construct the set of indifference curves 
in 
the consequence space the dashed curves in fig the most 
preferred indifference curve that intersects with the pareto frontier is 
in focus its intersection with the pareto frontier is the sought after 
consequence i e the optimal consequence in fig 
 a negotiation framework 
a multi-agent negotiation framework consists of 
 a set of two negotiating agents n { } 
 a set of attributes att {α αm} characterising the 
issues the agents are negotiating over each attribute α can take a 
value from the set v alα 
 a set of alternative outcomes o an outcome o ∈ o is 
represented by an assignment of values to the corresponding attributes 
in att 
 agents utility based on the theory of multiple-criteria decision 
making we define the agents utility as follows 
 objectives agent i has a set of ni objectives or interests 
denoted by j j ni to measure how much an 
outcome o fulfills an objective j to an agent i we use objective 
functions for each agent i we define i s interests using the 
objective vector function fi fij o → rni 
 
 value functions instead of directly evaluating an outcome o 
agent i looks at how much his objectives are fulfilled and will 
make a valuation based on these more basic criteria thus 
for each agent i there is a value function σi rni 
→ r 
in particular raiffa shows how to systematically 
construct an additive value function to each party involved in a 
negotiation 
 utility now given an outcome o ∈ o an agent i is able 
to determine its value i e σi fi o however a 
negotiation infrastructure is usually required to facilitate negotiation 
this might involve other mechanisms and factors parties e g 
a mediator a legal institution participation fees etc the 
standard way to implement such a thing is to allow money 
 
in fact given the k-dimensional space these should be called 
indifference surfaces however we will not bog down to that level of 
details 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
and side-payments in this paper we ignore those side-effects 
and assume that agent i s utility function ui is normalised so 
that ui o → 
example there are two agents a and b agent a has 
a task t that needs to be done and also units of a resource 
r agent b has the capacity to perform task t and would like to 
obtain at least and at most units of the resource r agent b is 
indifferent on any amount between and units of the resource 
r the objective functions for both agents a and b are cost and 
revenue and they both aim at minimising costs while maximising 
revenues having t done generates for a a revenue ra t while 
doing t incurs a cost cb t to b agent b obtains a revenue rb r 
for each unit of the resource r while providing each unit of the 
resource r costs agent a ca r 
assuming that money transfer between agents is possible the set 
att then contains three attributes 
 t taking values from the set { } indicates whether the 
task t is assigned to agent b 
 r taking values from the set of non-negative integer 
indicates the amount of resource r being allocated to agent b 
and 
 mt taking values from r indicates the payment p to be 
transferred from a to b 
consider the outcome o t r k mt p i e the 
task t is assigned to b and a allocates to b with k units of the 
resource r and a transfers p dollars to b then costa o 
k ca r p and reva o ra t and costb o cb t and 
reva o 
j 
k rb r p if ≤ k ≤ 
p otherwise 
and σi costi o revi o revi o − costi o i a b 
 problem formalisation 
consider example assume that ra t and cb t 
 and rb r and ca r that is the revenues 
generated for a exceeds the costs incurred to b to do task t and b 
values resource r more highly than the cost for a to provide it 
the optimal solution to this problem scenario is to assign task t to 
agent b and to allocate units of resource r i e the maximal 
amount of resource r required by agent b from agent a to agent 
b this outcome regarding the resource and task allocation 
problems leaves payoffs of to agent a and to agent b 
any 
other outcome would leave at least one of the agents worse off in 
other words the presented outcome is pareto-efficient and should 
be part of the solution outcome for this problem scenario 
however as the agents still have to bargain over the amount of 
money transfer p neither agent would be willing to disclose their 
respective costs and revenues regarding the task t and the resource 
r as a consequence agents often do not achieve the optimal 
outcome presented above in practice to address this issue we 
introduce a mediator to help the agents discover better agreements than 
the ones they might try to settle on note that this problem is 
essentially the problem of searching for joint gains in a multilateral 
negotiation in which the involved parties hold strategic information 
i e the integrative part in a negotiation in order to help facilitate 
this process we introduce the role of a neutral mediator before 
formalising the decision problems faced by the mediator and the 
 
certainly without money transfer to compensate agent a this 
outcome is not a fair one 
negotiating agents we discuss the properties of the solution 
outcomes to be achieved by the mediator in a negotiation setting the 
two typical design goals would be 
 efficiency avoid the agents from settling on an outcome that 
is not pareto-optimal and 
 fairness avoid agreements that give the most of the gains 
to a subset of agents while leaving the rest with too little 
the above goals are axiomatised in nash s seminal work on 
cooperative negotiation games essentially nash advocates for the 
following properties to be satisfied by solution to the bilateral 
negotiation problem i it produces only pareto-optimal outcomes ii 
it is invariant to affine transformation to the consequence space 
 iii it is symmetric and iv it is independent from irrelevant 
alternatives a solution satisfying nash s axioms is called a nash 
bargaining solution 
it then turns out that by taking the negotiators utilities as its 
objectives the mediator itself faces a multi-criteria decision making 
problem the issues faced by the mediator are i the mediator 
requires access to the negotiators utility functions and ii 
making fair tradeoffs between different agents utilities our methods 
allow the agents to repeatedly interact with the mediator so that a 
nash solution outcome could be found by the parties 
informally the problem faced by both the mediator and the 
negotiators is construction of the indifference curves why are the 
indifference curves so important 
 to the negotiators knowing the options available along 
indifference curves opens up opportunities to reach more 
efficient outcomes for instance consider an agent a who is 
presenting his opponent with an offer θa which she refuses 
to accept rather than having to concede a could look at 
his indifference curve going through θa and choose another 
proposal θa to him θa and θa are indifferent but θa could 
give some gains to b and thus will be more acceptable to b 
in other words the outcome θa is more efficient than θa to 
these two negotiators 
 to the mediator constructing indifference curves requires a 
measure of fairness between the negotiators the mediator 
needs to determine how much utility it needs to take away 
from the other negotiators to give a particular negotiator a 
specific gain g in utility 
in order to search for integrative solutions within the outcome 
space o we characterise the relationship between the agents over 
the set of attributes att as the agents hold different objectives and 
have different capacities it may be the case that changing between 
two values of a specific attribute implies different shifts in utility 
of the agents however the problem of finding the exact 
paretooptimal set 
is np-hard 
our approach is thus to solve this optimisation problem in two 
steps in the first steps the more manageable attributes will be 
solved these are attributes that take a finite set of values the 
result of this step would be a subset of outcomes that contains the 
pareto-optimal set in the second step we employ an iterative 
procedure that allows the mediator to interact with the negotiators to 
find joint improvements that move towards a pareto-optimal 
outcome this approach will not work unless the attributes from att 
 
the pareto-optimal set is the set of outcomes whose consequences 
 in the consequence space correspond to the pareto frontier 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
are independent most works on multi-attribute or multi-issue 
negotiation e g assume that the attributes or the issues are 
independent resulting in an additive value function for each agent 
assumption let i ∈ n and s ⊆ att denote by ¯s the 
set att \ s assume that vs and vs are two assignments of values 
to the attributes of s and v 
¯s v 
¯s are two arbitrary value 
assignments to the attributes of ¯s then ui vs v 
¯s − ui vs v 
¯s 
 ui vs v 
¯s −ui vs v 
¯s that is the utility function of agent i 
will be defined on the attributes from s independently of any value 
assignment to other attributes 
 mediator-based bilateral 
negotiations 
as discussed by lax and sebenius under incomplete 
information the tension between creating and claiming values is the 
primary cause of inefficient outcomes this can be seen most 
easily in negotiations involving two negotiators during the 
distributive phase of the negotiation the two negotiators s objectives are 
directly opposing each other we will now formally characterise 
this relationship between negotiators by defining the opposition 
between two negotiating parties the following exposition will be 
mainly reproduced from 
assuming for the moment that all attributes from att take values 
from the set of real numbers r i e v alj ⊆ r for all j ∈ att we 
further assume that the set o ×j∈attv alj of feasible outcomes 
is defined by constraints that all parties must obey and o is convex 
now an outcome o ∈ o is just a point in the m-dimensional space 
of real numbers then the questions are i from the point of view 
of an agent i is o already the best outcome for i ii if o is not 
the best outcome for i then is there another outcome o such that o 
gives i a better utility than o and o does not cause a utility loss to 
the other agent j in comparison to o 
the above questions can be answered by looking at the directions 
of improvement of the negotiating parties at o i e the directions 
in the outcome space o into which their utilities increase at point 
o under the assumption that the parties utility functions ui are 
differentiable concave the set of all directions of improvement for 
a party at a point o can be defined in terms of his most preferred 
or gradient direction at that point when the gradient direction 
∇ui o of agent i at point o is outright opposing to the gradient 
direction ∇uj o of agent j at point o then the two parties strongly 
disagree at o and no joint improvements can be achieved for i and 
j in the locality surrounding o 
since opposition between the two parties can vary considerably 
over the outcome space with one pair of outcomes considered 
highly antagonistic and another pair being highly cooperative we 
need to describe the local properties of the relationship we begin 
with the opposition at any point of the outcome space rm 
 the 
following definition is reproduced from 
definition the parties are in local strict opposition 
at a point x ∈ rm 
iff for all points x ∈ rm 
that are 
sufficiently close to x i e for some such that 
∀x x −x an increase of one utility can be achieved 
only at the expense of a decrease of the other utility 
 the parties are in local non-strict opposition at a point x ∈ 
rm 
iff they are not in local strict opposition at x i e iff it is 
possible for both parties to raise their utilities by moving an 
infinitesimal distance from x 
 
klein et al explore several implications of complex contracts 
in which attributes are possibly inter-dependent 
 the parties are in local weak opposition at a point x ∈ rm 
iff ∇u x ∇u x ≥ i e iff the gradients at x of the two 
utility functions form an acute or right angle 
 the parties are in local strong opposition at a point x ∈ rm 
iff ∇u x ∇u x i e iff the gradients at x form an 
obtuse angle 
 the parties are in global strict nonstrict weak strong 
opposition iff for every x ∈ rm 
they are in local strict 
 nonstrict weak strong opposition 
global strict and nonstrict oppositions are complementary cases 
essentially under global strict opposition the whole outcome space 
o becomes the pareto-optimal set as at no point in o can the 
negotiating parties make a joint improvement i e every point in o 
is a pareto-efficient outcome in other words under global strict 
opposition the outcome space o can be flattened out into a single 
line such that for each pair of outcomes x y ∈ o u x u y 
iff u x u y i e at every point in o the gradient of the two 
utility functions point to two different ends of the line 
intuitively global strict opposition implies that there is no way to 
obtain joint improvements for both agents as a consequence the 
negotiation degenerates to a distributive negotiation i e the 
negotiating parties should try to claim as much shares from the 
negotiation issues as possible while the mediator should aim for the 
fairness of the division on the other hand global nonstrict opposition 
allows room for joint improvements and all parties might be better 
off trying to realise the potential gains by reaching pareto-efficient 
agreements weak and strong oppositions indicate different levels 
of opposition the weaker the opposition the more potential gains 
can be realised making cooperation the better strategy to employ 
during negotiation on the other hand stronger opposition 
suggests that the negotiating parties tend to behave strategically 
leading to misrepresentation of their respective objectives and utility 
functions and making joint gains more difficult to realise 
we have been temporarily making the assumption that the 
outcome space o is the subset of rm 
 in many real-world 
negotiations this assumption would be too restrictive we will continue 
our exposition by lifting this restriction and allowing discrete 
attributes however as most negotiations involve only discrete 
issues with a bounded number of options we will assume that each 
attribute takes values either from a finite set or from the set of real 
numbers r in the rest of the paper we will refer to attributes whose 
values are from finite sets as simple attributes and attributes whose 
values are from r as continuous attributes the notions of local 
oppositions i e strict nonstrict weak and strong are not 
applicable to outcome spaces that contain simple attributes and nor are the 
notions of global weak and strong oppositions however the 
notions of global strict and nonstrict oppositions can be generalised 
for outcome spaces that contain simple attributes 
definition given an outcome space o the parties are in 
global strict opposition iff ∀x y ∈ o u x u y iff u x 
u y 
the parties are in global nonstrict opposition if they are not in 
global strict opposition 
 optimisation on simple attributes 
in order to extract the optimal values for a subset of attributes 
in the first step of this optimisation process the mediator requests 
the negotiators to submit their respective utility functions over the 
set of simple attributes let simp ⊆ att denote the set of all 
simple attributes from att note that due to assumption agent i s 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
utility function can be characterised as follows 
ui vsimp vsimp wi 
 ∗ ui vsimp wi 
 ∗ ui vsimp 
where simp att \ simp and ui and ui are the utility 
components of ui over the sets of attributes simp and simp respectively 
and wi 
 wi 
 and wi 
 wi 
 
as attributes are independent of each other regarding the agents 
utility functions the optimisation problem over the attributes from 
simp can be carried out by fixing ui vsimp to a constant c 
and then search for the optimal values within the set of attributes 
simp now how does the mediator determine the optimal values 
for the attributes in simp several well-known optimisation 
strategies could be applicable here 
 the utilitarian solution the sum of the agents utilities are 
maximised thus the optimal values are the solution of the 
following optimisation problem 
arg max 
v∈v alsimp 
x 
i∈n 
ui v 
 the nash solution the product of the agents utilities are 
maximised thus the optimal values are the solution of the 
following optimisation problem 
arg max 
v∈v alsimp 
y 
i∈n 
ui v 
 the egalitarian solution aka the maximin solution the 
utility of the agent with minimum utility is maximised thus 
the optimal values are the solution of the following 
optimisation problem 
arg max 
v∈v alsimp 
min 
i∈n 
ui v 
the question now is of course whether a negotiator has the 
incentive to misrepresent his utility function first of all recall that the 
agents utility functions are bounded i e ∀o ∈ o ≤ ui o ≤ 
thus the agents have no incentive to overstate their utility 
regarding an outcome o if o is the most preferred outcome to an agent 
i then he already assigns the maximal utility to o on the other 
hand if o is not the most preferred outcome to i then by 
overstating the utility he assigns to o the agent i runs the risk of having 
to settle on an agreement which would give him less payoffs than 
he is supposed to receive however agents do have an incentive 
to understate their utility if the final settlement will be based on the 
above solutions alone essentially the mechanism to avoid an agent 
to understate his utility regarding particular outcomes is to 
guarantee a certain measure of fairness for the final settlement that is 
the agents lose the incentive to be dishonest to obtain gains from 
taking advantage of the known solutions to determine the 
settlement outcome for they would be offset by the fairness maintenance 
mechanism firsts we state an easy lemma 
lemma when simp contains one single attributes the agents 
have the incentive to understate their utility functions regarding 
outcomes that are not attractive to them 
by way of illustration consider the set simp containing only one 
attribute that could take values from the finite set {a b c d} 
assume that negotiator assigns utilities of and 
to a b c and d respectively assume also that negotiator 
assigns utilities of and to a b c and d 
respectively if agent misrepresents his utility function to the mediator 
by reporting utility for all values a b and c and utility for 
value d then the agent who plays honestly in his report to the 
mediator will obtain the worst outcome d given any of the above 
solutions note that agent doesn t need to know agent s utility 
function nor does he need to know the strategy employed by agent 
 as long as he knows that the mediator is going to employ one of 
the above three solutions then the above misrepresentation is the 
dominant strategy for this game 
however when the set simp contains more than one attribute 
and none of the attributes strongly dominate the other attributes 
then the above problem disminishes by itself thanks to the 
integrative solution we of course have to define clearly what it means 
for an attribute to strongly dominate other attributes intuitively if 
most of an agent s utility concentrates on one of the attributes then 
this attribute strongly dominates other attributes we again appeal 
to the assumption on additivity of utility functions to achieve a 
measure of fairness within this negotiation setting due to 
assumption we can characterise agent i s utility component over the set 
of attributes simp by the following equation 
ui vsimp 
x 
j∈simp 
wi 
j ∗ ui j vj 
where 
p 
j∈simp wj 
then an attribute ∈ simp strongly dominates the rest of the 
attributes in simp for agent i iff wi 
 
p 
j∈ simp− wi 
j attribute 
is said to be strongly dominant for agent i wrt the set of simple 
attributes simp 
the following theorem shows that if the set of attributes simp 
does not contain a strongly dominant attribute then the negotiators 
have no incentive to be dishonest 
theorem given a negotiation framework if for every agent 
the set of simple attributes doesn t contain a strongly dominant 
attribute then truth-telling is an equilibrium strategy for the 
negotiators during the optimisation of simple attributes 
so far we have been concentrating on the efficiency issue while 
leaving the fairness issue aside a fair framework does not only 
support a more satisfactory distribution of utility among the agents 
but also often a good measure to prevent misrepresentation of 
private information by the agents of the three solutions presented 
above the utilitarian solution does not support fairness on the 
other hand nash proves that the nash solution satisfies the 
above four axioms for the cooperative bargaining games and is 
considered a fair solution the egalitarian solution is another 
mechanism to achieve fairness by essentially helping the worst off the 
problem with these solutions as discussed earlier is that they are 
vulnerable to strategic behaviours when one of the attributes strongly 
dominates the rest of attributes 
however there is yet another solution that aims to guarantee 
fairness the minimax solution that is the utility of the agent with 
maximum utility is minimised it s obvious that the minimax 
solution produces inefficient outcomes however to get around this 
problem given that the pareto-optimal set can be tractably 
computed we can apply this solution over the pareto-optimal set only 
let poset ⊆ v alsimp be the pareto-optimal subset of the simple 
outcomes the minimax solution is defined to be the solution of the 
following optimisation problem 
arg min 
v∈p oset 
max 
i∈n 
ui v 
while overall efficiency often suffers under a minimax solution 
i e the sum of all agents utilities are often lower than under other 
solutions it can be shown that the minimax solution is less 
vulnerable to manipulation 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
theorem given a negotiation framework under the 
minimax solution if the negotiators are uncertain about their 
opponents preferences then truth-telling is an equilibrium strategy for 
the negotiators during the optimisation of simple attributes 
that is even when there is only one single simple attribute if an 
agent is uncertain whether the other agent s most preferred 
resolution is also his own most preferred resolution then he should opt for 
truth-telling as the optimal strategy 
 optimisation on continuous attributes 
when the attributes take values from infinite sets we assume that 
they are continuous this is similar to the common practice in 
operations research in which linear programming solutions techniques 
are applied to integer programming problems 
we denote the number of continuous attributes by k i e att 
simp ∪ simp and simp k then the outcome space o can be 
represented as follows o 
q 
j∈simp v alj × 
q 
l∈simp v all 
where 
q 
l∈simp v all ⊆ rk 
is the continuous component of o let 
oc 
denote the set 
q 
l∈simp v all we ll refer to oc 
as the feasible 
set and assume that oc 
is closed and convex after carrying out 
the optimisation over the set of simple attributes we are able to 
assign the optimal values to the simple attributes from simp thus 
we reduce the original problem to the problem of searching for 
optimal and fair outcomes within the feasible set oc 
 recall that 
by assumption we can characterise agent i s utility function as 
follows 
ui v∗ 
simp vsimp c wi 
 ∗ ui vsimp 
where c is the constant wi 
 ∗ ui v∗ 
simp and v∗ 
simp denotes the 
optimal values of the simple attributes in simp hence without loss 
of generality albeit with a blatant abuse of notation we can take 
the agent i s utility function as ui rk 
→ accordingly we 
will also take the set of outcomes under consideration by the agents 
to be the feasible set oc 
 we now state another assumption to be 
used in this section 
assumption the negotiators utility functions can be 
described by continuously differentiable and concave functions ui 
rk 
→ i 
it should be emphasised that we do not assume that agents 
explicitly know their utility functions for the method to be described 
in the following to work we only assume that the agents know the 
relevant information e g at certain point within the feasible set oc 
 
the gradient direction of their own utility functions and some 
section of their respective indifference curves assume that a tentative 
agreement which is a point x ∈ rk 
 is currently on the table the 
process for the agents to jointly improve this agreement in order to 
reach a pareto-optimal agreement can be described as follows the 
mediator asks the negotiators to discretely submit their respective 
gradient directions at x i e ∇u x and ∇u x 
note that the goal of the process to be described here is to search 
for agreements that are more efficient than the tentative agreement 
currently on the table that is we are searching for points x within 
the feasible set oc 
such that moving to x from the current tentative 
agreement x brings more gains to at least one of the agents while 
not hurting any of the agents due to the assumption made above 
i e the feasible set oc 
is bounded the conditions for an alternative 
x ∈ oc 
to be efficient vary depending on the position of x the 
following results are proved in 
let b x denote the equation of the boundary of oc 
 
defining x ∈ oc 
iff b x ≥ an alternative x∗ 
∈ oc 
is efficient iff 
either 
a x∗ 
is in the interior of oc 
and the parties are in local strict 
opposition at x∗ 
 i e 
∇u x∗ 
 −γ∇u x∗ 
 
where γ or 
b x∗ 
is on the boundary of oc 
 and for some α β ≥ 
α∇u x∗ 
 β∇u x∗ 
 ∇b x∗ 
 
we are now interested in answering the following questions 
 i what is the initial tentative agreement x 
 ii how to find the more efficient agreement xh given the 
current tentative agreement xh 
 determining a fair initial tentative agreement 
it should be emphasised that the choice of the initial tentative 
agreement affects the fairness of the final agreement to be reached 
by the presented method for instance if the initial tentative 
agreement x is chosen to be the most preferred alternative to one of 
the agents then it is also a pareto-optimal outcome making it 
impossible to find any joint improvement from x however if x 
will then be chosen to be the final settlement and if x turns out 
to be the worst alternative to the other agent then this outcome is a 
very unfair one thus it s important that the choice of the initial 
tentative agreement be sensibly made 
ehtamo et al present several methods to choose the initial 
tentative agreement called reference point in their paper however 
their goal is to approximate the pareto-optimal set by 
systematically choosing a set of reference points once an approximate 
pareto-optimal set is generated it is left to the negotiators to decide 
which of the generated pareto-optimal outcomes to be chosen as 
the final settlement that is distributive negotiation will then be 
required to settle the issue 
we on the other hand are interested in a fair initial tentative 
agreement which is not necessarily efficient improving a given 
tentative agreement to yield a pareto-optimal agreement is 
considered in the next section for each attribute j ∈ simp an agent i will 
be asked to discretely submit three values from the set v alj the 
most preferred value denoted by pvi j the least preferred value 
denoted by wvi j and a value that gives i an approximately 
average payoff denoted by avi j note that this is possible 
because the set v alj is bounded if pv j and pv j are sufficiently 
close i e pv j − pv j δ for some pre-defined δ 
then pv j and pv j are chosen to be the two core values 
denoted by cv and cv otherwise between the two values pv j 
and av j we eliminate the one that is closer to wv j the 
remaining value is denoted by cv similarly we obtain cv from the 
two values pv j and av j if cv cv then cv is selected as 
the initial value for the attribute j as part of the initial tentative 
agreement otherwise without loss of generality we assume that 
cv cv the mediator selects randomly p values mv mvp 
from the open interval cv cv where p ≥ the mediator then 
asks the agents to submit their valuations over the set of values 
{cv cv mv mvp} the value whose the two valuations of 
two agents are closest is selected as the initial value for the attribute 
j as part of the initial tentative agreement 
the above procedure guarantees that the agents do not gain by 
behaving strategically by performing the above procedure on 
every attribute j ∈ simp we are able to identify the initial tentative 
agreement x such that x ∈ oc 
 the next step is to compute 
a new tentative agreement from an existing tentative agreement so 
that the new one would be more efficient than the existing one 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 computing new tentative agreement 
our procedure is a combination of the method of jointly 
improving direction introduced by ehtamo et al and a method we 
propose in the coming section basically the idea is to see how strong 
the opposition the parties are in if the two parties are in local 
weak opposition at the current tentative agreement xh i e their 
improving directions at xh are close to each other then the 
compromise direction proposed by ehtamo et al is likely to point to 
a better agreement for both agents however if the two parties are 
in local strong opposition at the current point xh then it s unclear 
whether the compromise direction would really not hurt one of the 
agents whilst bringing some benefit to the other 
we will first review the method proposed by ehtamo et al 
to compute the compromise direction for a group of negotiators at 
a given point x ∈ oc 
 ehtamo et al define a a function t x 
that describes the mediator s choice for a compromise direction at 
x for the case of two-party negotiations the following bisecting 
function denoted by t bs 
 can be defined over the interior set of oc 
 
note that the closed set oc 
contains two disjoint subsets oc 
 
oc 
 ∪oc 
b where oc 
 denotes the set of interior points of oc 
and oc 
b 
denotes the boundary of oc 
 the bisecting compromise is defined 
by a function t bs 
 oc 
 → r 
 
t bs 
 x 
∇u x 
∇u x 
 
∇u x 
∇u x 
 x ∈ oc 
 
given the current tentative agreement xh h ≥ the mediator 
has to choose a point xh along d t xh so that all parties 
gain ehtamo et al then define a mechanism to generate a sequence 
of points and prove that when the generated sequence is bounded 
and when all generated points from the sequence belong to the 
interior set oc 
 then the sequence converges to a weakly 
paretooptimal agreement pp - 
as the above mechanism does not work at the boundary points 
of oc 
 we will introduce a procedure that works everywhere in an 
alternative space oc 
 let x ∈ oc 
and let θ x denote the angle 
between the gradients ∇u x and ∇u x at x that is 
θ x arccos 
∇u x ∇u x 
∇u x ∇u x 
 
from definition it is obvious that the two parties are in local 
strict opposition at x iff θ x π and they are in local strong 
opposition iff π ≥ θ x π and they are in local weak 
opposition iff π ≥ θ x ≥ note also that the two vectors ∇u x 
and ∇u x define a hyperplane denoted by h∇ x in the 
kdimensional space rk 
 furthermore there are two indifference 
curves of agents and going through point x denoted by ic x 
and ic x respectively let ht x and ht x denote the 
tangent hyperplanes to the indifference curves ic x and ic x 
respectively at point x the planes ht x and ht x intersect 
h∇ x in the lines is x and is x respectively note that 
given a line l x going through the point x there are two unit 
vectors from x along l x pointing to two opposite directions 
denoted by l 
 x and l− 
 x 
we can now informally explain our solution to the problem of 
searching for joint gains when it isn t possible to obtain a 
compromise direction for joint improvements at a point x ∈ oc 
either 
because the compromise vector points to the space outside of the 
feasible set oc 
or because the two parties are in local strong 
opposition at x we will consider to move along the indifference curve of 
one party while trying to improve the utility of the other party as 
 
let s be the set of alternatives x∗ 
is weakly pareto optimal if 
there is no x ∈ s such that ui x ui x∗ 
 for all agents i 
the mediator does not know the indifference curves of the parties 
he has to use the tangent hyperplanes to the indifference curves of 
the parties at point x note that the tangent hyperplane to a curve 
is a useful approximation of the curve in the immediate vicinity of 
the point of tangency x 
we are now describing an iteration step to reach the next tentative 
agreement xh from the current tentative agreement xh ∈ oc 
 a 
vector v whose tail is xh is said to be bounded in oc 
if ∃λ 
such that xh λv ∈ oc 
 to start the mediator asks the negotiators 
for their gradients ∇u xh and ∇u xh respectively at xh 
 if xh is a pareto-optimal outcome according to equation or 
equation then the process is terminated 
 if ≥ ∇u xh ∇u xh and the vector t bs 
 xh is 
bounded in oc 
then the mediator chooses the compromise 
improving direction d t bs 
 xh and apply the method 
described by ehtamo et al to generate the next tentative 
agreement xh 
 otherwise among the four vectors isσ 
i xh i and 
σ − the mediator chooses the vector that i is bounded 
in oc 
 and ii is closest to the gradient of the other agent 
∇uj xh j i denote this vector by t g xh that is 
we will be searching for a point on the indifference curve of 
agent i ici xh while trying to improve the utility of agent 
j note that when xh is an interior point of oc 
then the 
situation is symmetric for the two agents and and the mediator 
has the choice of either finding a point on ic xh to 
improve the utility of agent or finding a point on ic xh to 
improve the utility of agent to decide on which choice to 
make the mediator has to compute the distribution of gains 
throughout the whole process to avoid giving more gains to 
one agent than to the other now the point xh to be 
generated lies somewhere on the intersection of ici xh and the 
hyperplane defined by ∇ui xh and t g xh this 
intersection is approximated by t g xh thus the sought 
after point xh can be generated by first finding a point yh 
along the direction of t g xh and then move from yh to the 
same direction of ∇ui xh until we intersect with ici xh 
mathematically let ζ and ξ denote the vectors t g xh and 
∇ui xh respectively xh is the solution to the following 
optimisation problem 
max 
λ λ ∈l 
uj xh λ ζ λ ξ 
s t xh λ ζ λ ξ ∈ oc 
 and ui xh λ ζ λ ξ ui xh 
where l is a suitable interval of positive real numbers e g 
l {λ λ } or l {λ a λ ≤ b} ≤ a b 
given an initial tentative agreement x the method described 
above allows a sequence of tentative agreements x x to be 
iteratively generated the iteration stops whenever a weakly pareto 
optimal agreement is reached 
theorem if the sequence of agreements generated by the 
above method is bounded then the method converges to a point 
x∗ 
∈ oc 
that is weakly pareto optimal 
 conclusion and future work 
in this paper we have established a framework for negotiation 
that is based on mcdm theory for representing the agents 
objectives and utilities the focus of the paper is on integrative 
negotiation in which agents aim to maximise joint gains or create value 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
we have introduced a mediator into the negotiation in order to 
allow negotiators to disclose information about their utilities 
without providing this information to their opponents furthermore the 
mediator also works toward the goal of achieving fairness of the 
negotiation outcome 
that is the approach that we describe aims for both efficiency in 
the sense that it produces pareto optimal outcomes i e no aspect 
can be improved for one of the parties without worsening the 
outcome for another party and also for fairness which chooses 
optimal solutions which distribute gains amongst the agents in some 
appropriate manner we have developed a two step process for 
addressing the np-hard problem of finding a solution for a set of 
integrative attributes which is within the pareto-optimal set for those 
attributes for simple attributes i e those which have a finite set 
of values we use known optimisation techniques to find a 
paretooptimal solution in order to discourage agents from 
misrepresenting their utilities to gain an advantage we look for solutions that 
are least vulnerable to manipulation we have shown that as long 
as one of the simple attributes does not strongly dominate the 
others then truth telling is an equilibrium strategy for the negotiators 
during the stage of optimising simple attributes for non-simple 
attributes we propose a mechanism that provides stepwise 
improvements to move the proposed solution in the direction of a 
paretooptimal solution 
the approach presented in this paper is similar to the ideas 
behind negotiation analysis ehtamo et al presents an 
approach to searching for joint gains in multi-party negotiations the 
relation of their approach to our approach is discussed in the 
preceding section lai et al provide an alternative approach to 
integrative negotiation while their approach was clearly described 
for the case of two-issue negotiations the generalisation to 
negotiations with more than two issues is not entirely clear 
zhang et at discuss the use of integrative negotiation in 
agent organisations they assume that agents are honest their 
main result is an experiment showing that in some situations agents 
cooperativeness may not bring the most benefits to the organisation 
as a whole while giving no explanation jonker et al consider 
an approach to multi-attribute negotiation without the use of a 
mediator thus their approach can be considered a complement of 
ours their experimental results show that agents can reach 
paretooptimal outcomes using their approach 
the details of the approach have currently been shown only for 
bilateral negotiation and while we believe they are generalisable to 
multiple negotiators this work remains to be done there is also 
future work to be done in more fully characterising the outcomes 
of the determination of values for the non-simple attributes in 
order to provide a complete framework we are also working on the 
distributive phase using the mediator 
acknowledgement 
the authors acknowledge financial support by arc dicovery grant 
 - grant dp and dest iap grant - 
grant cg the authors would like to thank lawrence 
cavedon and the rmit agents research group for their helpful 
comments and suggestions 
 references 
 f alemi p fos and w lacorte a demonstration of 
methods for studying negotiations between physicians and 
health care managers decision science - 
 m ehrgott multicriteria optimization springer-verlag 
berlin 
 h ehtamo r p hamalainen p heiskanen j teich 
m verkama and s zionts generating pareto solutions in a 
two-party setting constraint proposal methods 
management science - 
 h ehtamo e kettunen and r p hmlinen searching for 
joint gains in multi-party negotiations european journal of 
operational research - 
 p faratin automated service negotiation between 
autonomous computational agents phd thesis university 
of london 
 a foroughi minimizing negotiation process losses with 
computerized negotiation support systems the journal of 
applied business research - 
 c m jonker v robu and j treur an agent architecture 
for multi-attribute negotiation using incomplete preference 
information j autonomous agents and multi-agent 
systems to appear 
 r l keeney and h raiffa decisions with multiple 
objectives preferences and value trade-offs john wiley 
and sons inc new york 
 g kersten and s noronha rational agents contract curves 
and non-efficient compromises ieee systems man and 
cybernetics - 
 m klein p faratin h sayama and y bar-yam protocols 
for negotiating complex contracts ieee intelligent systems 
 - 
 s kraus j wilkenfeld and g zlotkin multiagent 
negotiation under time constraints artificial intelligence 
journal - 
 g lai c li and k sycara efficient multi-attribute 
negotiation with incomplete information group decision 
and negotiation - 
 d lax and j sebenius the manager as negotiator the 
negotiator s dilemma creating and claiming value nd ed 
in s goldberg f sander n rogers editors dispute 
resolution nd ed pages - little brown co 
 m lomuscio and n jennings a classification scheme for 
negotiation in electronic commerce in agent-mediated 
electronic commerce a european agentlink perspective 
springer-verlag 
 r maes and a moukas agents that buy and sell 
communications of the acm - 
 j nash two-person cooperative games econometrica 
 - april 
 h raiffa the art and science of negotiation harvard 
university press cambridge usa 
 h raiffa j richardson and d metcalfe negotiation 
analysis the science and art of collaborative decision 
making belknap press cambridge ma 
 t sandholm agents in electronic commerce component 
technologies for automated negotiation and coalition 
formation jaamas - 
 j sebenius negotiation analysis a characterization and 
review management science - 
 l weingart e hyder and m pietrula knowledge matters 
the effect of tactical descriptions on negotiation behavior 
and outcome tech report cmu 
 x zhang v r lesser and t wagner integrative 
negotiation among agents situated in organizations ieee 
trans on systems man and cybernetics part c 
 - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
on the relevance of utterances in formal inter-agent 
dialogues 
simon parsons 
peter mcburney 
 
department of computer information science 
brooklyn college city university of new york 
brooklyn ny usa 
{parsons sklar} sci brooklyn cuny edu 
elizabeth sklar 
michael wooldridge 
 
department of computer science 
university of liverpool 
liverpool l zf uk 
{p j mcburney m j wooldridge} csc liv ac uk 
abstract 
work on argumentation-based dialogue has defined 
frameworks within which dialogues can be carried out established 
protocols that govern dialogues and studied different 
properties of dialogues this work has established the space in 
which agents are permitted to interact through dialogues 
recently there has been increasing interest in the 
mechanisms agents might use to choose how to act - the 
rhetorical manoeuvring that they use to navigate through the space 
defined by the rules of the dialogue key in such 
considerations is the idea of relevance since a usual requirement is 
that agents stay focussed on the subject of the dialogue and 
only make relevant remarks here we study several notions 
of relevance showing how they can be related to both the 
rules for carrying out dialogues and to rhetorical 
manoeuvring 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence coherence co-ordination languages 
structures multiagent systems 
general terms 
design languages theory 
 introduction 
finding ways for agents to reach agreements in 
multiagent systems is an area of active research one mechanism 
for achieving agreement is through the use of argumentation 
- where one agent tries to convince another agent of 
something during the course of some dialogue early examples of 
argumentation-based approaches to multiagent agreement 
include the work of dignum et al kraus 
parsons and jennings reed schroeder et al and 
sycara 
the work of walton and krabbe popularised in the 
multiagent systems community by reed has been 
particularly influential in the field of argumentation-based 
dialogue this work influenced the field in a number of ways 
perhaps most deeply in framing multi-agent interactions as 
dialogue games in the tradition of hamblin viewing 
dialogues in this way as in provides a powerful 
framework for analysing the formal properties of dialogues and 
for identifying suitable protocols under which dialogues can 
be conducted the dialogue game view overlaps with 
work on conversation policies see for example but 
differs in considering the entire dialogue rather than dialogue 
segments 
in this paper we extend the work of by considering 
the role of relevance - the relationship between utterances 
in a dialogue relevance is a topic of increasing interest 
in argumentation-based dialogue because it relates to the 
scope that an agent has for applying strategic manoeuvering 
to obtain the outcomes that it requires our 
work identifes the limits on such rhetorical manoeuvering 
showing when it can and cannot have an effect 
 background 
we begin by introducing the formal system of 
argumentation that underpins our approach as well as the 
corresponding terminology and notation all taken from 
a dialogue is a sequence of messages passed between two 
or more members of a set of agents a an agent α maintains 
a knowledge base σα containing formulas of a propositional 
language l and having no deductive closure agent α also 
maintains the set of its past utterances called the 
commitment store csα we refer to this as an agent s public 
knowledge since it contains information that is shared with 
other agents in contrast the contents of σα are private 
to α 
note that in the description that follows we assume that 
is the classical inference relation that ≡ stands for logical 
equivalence and we use δ to denote all the information 
available to an agent thus in a dialogue between two agents 
α and β δα σα ∪ csα ∪ csβ so the commitment store 
csα can be loosely thought of as a subset of δα consisting of 
the assertions that have been made public in some dialogue 
games such as those in anything in csα is either in σα 
or can be derived from it in other dialogue games such as 
 
 - - - - rps c ifaamas 
those in csα may contain things that cannot be derived 
from σα 
definition an argument a is a pair s p where p 
is a formula of l and s a subset of δ such that i s is 
consistent ii s p and iii s is minimal so no proper 
subset of s satisfying both and exists 
s is called the support of a written s support a and p 
is the conclusion of a written p conclusion a thus we 
talk of p being supported by the argument s p 
in general since δ may be inconsistent arguments in 
a δ the set of all arguments which can be made from δ 
may conflict and we make this idea precise with the notion 
of undercutting 
definition let a and a be arguments in a δ 
a undercuts a iff ∃¬p ∈ support a such that p ≡ 
conclusion a 
in other words an argument is undercut if and only if there 
is another argument which has as its conclusion the negation 
of an element of the support for the first argument 
to capture the fact that some beliefs are more strongly 
held than others we assume that any set of beliefs has a 
preference order over it we consider all information 
available to an agent δ to be stratified into non-overlapping 
subsets δ δn such that beliefs in δi are all equally 
preferred and are preferred over elements in δj where i j 
the preference level of a nonempty subset s ⊂ δ where 
different elements s ∈ s may belong to different layers δi 
is valued at the highest numbered layer which has a member 
in s and is referred to as level s in other words s is only 
as strong as its weakest member note that the strength of 
a belief as used in this context is a separate concept from 
the notion of support discussed earlier 
definition let a and a be arguments in a δ 
a is preferred to a according to pref a 
pref 
a iff 
level support a level support a if a is 
preferred to a we say that a is stronger than a 
we can now define the argumentation system we will use 
definition an argumentation system is a triple 
a δ undercut pref 
such that 
 a δ is a set of the arguments built from δ 
 undercut is a binary relation representing the defeat 
relationship between arguments undercut ⊆ a δ × 
a δ and 
 pref is a pre-ordering on a δ × a δ 
the preference order makes it possible to distinguish 
different types of relations between arguments 
definition let a a be two arguments of a δ 
 if a undercuts a then a defends itself against a 
iff a 
pref 
a otherwise a does not defend itself 
 a set of arguments a defends a iff for every a that 
undercuts a where a does not defend itself against 
a then there is some a ∈ a such that a undercuts 
a and a does not defend itself against a 
we write aundercut pref to denote the set of all non-undercut 
arguments and arguments defending themselves against all 
their undercutting arguments the set a δ of acceptable 
arguments of the argumentation system 
a δ undercut pref 
is the least fixpoint of a function f 
a ⊆ a δ 
f a { s p ∈ a δ s p is defended by a} 
definition the set of acceptable arguments for an 
argumentation system a δ undercut pref is recursively 
defined as 
a δ 
 
fi≥ ∅ 
 aundercut pref ∪ 
h 
fi≥ aundercut pref 
i 
an argument is acceptable if it is a member of the acceptable 
set and a proposition is acceptable if it is the conclusion of 
an acceptable argument 
an acceptable argument is one which is in some sense 
proven since all the arguments which might undermine it 
are themselves undermined 
definition if there is an acceptable argument for a 
proposition p then the status of p is accepted while if there 
is not an acceptable argument for p the status of p is not 
accepted 
argument a is said to affect the status of another argument 
a if changing the status of a will change the status of a 
 dialogues 
systems like those described in lay down sets of 
locutions that agents can make to put forward propositions 
and the arguments that support them and protocols that 
define precisely which locutions can be made at which points 
in the dialogue we are not concerned with such a level 
of detail here instead we are interested in the interplay 
between arguments that agents put forth as a result we 
will consider only that agents are allowed to put forward 
arguments we do not discuss the detail of the mechanism 
that is used to put these arguments forward - we just 
assume that arguments of the form s p are inserted into 
an agent s commitment store where they are then visible to 
other agents 
we then have a typical definition of a dialogue 
definition a dialogue d is a sequence of moves 
m m mn 
a given move mi is a pair α ai where ai is an argument 
that α places into its commitment store csα 
moves in an argumentation-based dialogue typically attack 
moves that have been made previously while in general 
a dialogue can include moves that undercut several 
arguments in the remainder of this paper we will only consider 
dialogues that put forward moves that undercut at most 
one argument for now we place no additional constraints 
on the moves that make up a dialogue later we will see 
how different restrictions on moves lead to different kinds of 
dialogue 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the sequence of arguments put forward in the dialogue 
is determined by the agents who are taking part in the 
dialogue but they are usually not completely free to choose 
what arguments they make as indicated earlier their choice 
is typically limited by a protocol if we write the sequence 
of n moves m m mn as mn and denote the empty 
sequence as m then we can define a profocol in the following 
way 
definition a protocol p is a function on a sequence 
of moves mi in a dialogue d that for all i ≥ identifies 
a set of possible moves mi from which the mi th move 
may be drawn 
p mi → mi 
in other words for our purposes here at every point in 
a dialogue a protocol determines a set of possible moves 
that agents may make as part of the dialogue if a dialogue 
d always picks its moves m from the set m identified by 
protocol p then d is said to conform to p 
even if a dialogue conforms to a protocol it is typically 
the case that the agent engaging in the dialogue has to make 
a choice of move - it has to choose which of the moves in m 
to make this excercise of choice is what we refer to as an 
agent s use of rhetoric in its oratorical sense of influencing 
the thought and conduct of an audience some of our 
results will give a sense of how much scope an agent has to 
exercise rhetoric under different protocols 
as arguments are placed into commitment stores and 
hence become public agents can determine the relationships 
between them in general after several moves in a 
dialogue some arguments will undercut others we will denote 
the set of arguments {a a aj} asserted after moves 
m m mj of a dialogue to be aj - the relationship of 
the arguments in aj can be described as an argumentation 
graph similar to those described in for example 
definition an argumentation graph ag over a set 
of arguments a is a directed graph v e such that every 
vertex v v ∈ v denotes one argument a ∈ a every 
argument a is denoted by one vertex v and every directed edge 
e ∈ e from v to v denotes that v undercuts v 
we will use the term argument graph as a synonym for 
argumentation graph 
note that we do not require that the argumentation graph 
is connected in other words the notion of an argumentation 
graph allows for the representation of arguments that do 
not relate by undercutting or being undercut to any other 
arguments we will come back to this point very shortly 
we adapt some standard graph theoretic notions in order 
to describe various aspects of the argumentation graph if 
there is an edge e from vertex v to vertex v then v is said 
to be the parent of v and v is said to be the child of v 
in a reversal of the usual notion we define a root of an 
argumentation graph 
as follows 
definition a root of an argumentation graph ag 
 v e is a node v ∈ v that has no children 
thus a root of a graph is a node to which directed edges 
may be connected but from which no directed edges 
connect to other nodes thus a root is a node representing an 
 
note that we talk of a root rather than the root - as defined 
an argumentation graph need not be a tree 
v v 
figure an example argument graph 
argument that is undercut but which itself does no 
undercutting similarly 
definition a leaf of an argumentation graph ag 
 v e is a node v ∈ v that has no parents 
thus a leaf in an argumentation graph represents an 
argument that undercuts another argument but does no 
undercutting thus in figure v is a root and v is a leaf the 
reason for the reversal of the usual notions of root and leaf 
is that as we shall see we will consider dialogues to 
construct argumentation graphs from the roots in our sense 
to the leaves the reversal of the terminology means that it 
matches the natural process of tree construction 
since as described above argumentation graphs are 
allowed to be not connected in the usual graph theory sense 
it is helpful to distinguish nodes that are connected to other 
nodes in particular to the root of the tree we say that node 
v is connected to node v if and only if there is a path from 
v to v since edges represent undercut relations the notion 
of connectedness between nodes captures the influence that 
one argument may have on another 
proposition given an argumentation graph ag if 
there is any argument a denoted by node v that affects the 
status of another argument a denoted by v then v is 
connected to v the converse does not hold 
proof given definitions and the only ways in 
which a can affect the status of a is if a either undercuts 
a or if a undercuts some argument a that undercuts a 
or if a undercuts some a that undercuts some a that 
undercuts a and so on in all such cases a sequence of 
undercut relations relates the two arguments and if they are 
both in an argumentation graph this means that they are 
connected 
since the notion of path ignores the direction of the 
directed arcs nodes v and v are connected whether the edge 
between them runs from v to v or vice versa since a only 
undercuts a if the edge runs from v to v we cannot infer 
that a will affect the status of a from information about 
whether or not they are connected 
the reason that we need the concept of the argumentation 
graph is that the properties of the argumentation graph tell 
us something about the set of arguments a the graph 
represents when that set of arguments is constructed through a 
dialogue there is a relationship between the structure of the 
argumentation graph and the protocol that governs the 
dialogue it is the extent of the relationship between structure 
and protocol that is the main subject of this paper to study 
this relationship we need to establish a correspondence 
between a dialogue and an argumentation graph given the 
definitions we have so far this is simple 
definition a dialogue d consisting of a sequence 
of moves mn and an argument graph ag v e 
correspond to one another iff ∀m ∈ mn the argument ai that 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
is advanced at move mi is represented by exactly one node 
v ∈ v and ∀v ∈ v v represents exactly one argument ai 
that has been advanced by a move m ∈ mn 
thus a dialogue corresponds to an argumentation graph if 
and only if every argument made in the dialogue corresponds 
to a node in the graph and every node in the graph 
corresponds to an argument made in the dialogue this 
one-toone correspondence allows us to consider each node v in the 
graph to have an index i which is the index of the move in 
the dialogue that put forward the argument which that node 
represents thus we can for example refer to the third 
node in the argumentation graph meaning the node that 
represents the argument put forward in the third move of 
the dialogue 
 relevance 
most work on dialogues is concerned with what we might 
call coherent dialogues that is dialogues in which the 
participants are as in the work of walton and krabbe 
focused on resolving some question through the dialogue 
to capture this coherence it seems we need a notion of 
relevance to constrain the statements made by agents here 
we study three notions of relevance 
definition consider a dialogue d consisting of a 
sequence of moves mi with a corresponding argument graph 
ag the move mi i is said to be relevant if one or 
more of the following hold 
r making mi will change the status of the argument 
denoted by the first node of ag 
r making mi will add a node vi that is connected to 
the first node of ag 
r making mi will add a node vi that is connected to 
the last node to be added to ag 
r -relevance is the form of relevance defined by in their 
study of strategic and tactical reasoning 
 r -relevance was 
suggested by the notion used in and though it differs 
somewhat from that suggested there we believe it captures 
the essence of its predecessor 
note that we only define relevance for the second move 
of the dialogue onwards because the first move is taken to 
identify the subject of the dialogue that is the central 
question that the dialogue is intended to answer and hence it 
must be relevant to the dialogue no matter what it is in 
assuming this we focus our attention on the same kind of 
dialogues as 
we can think of relevance as enforcing a form of 
parsimony on a dialogue - it prevents agents from making 
statements that do not bear on the current state of the dialogue 
this promotes efficiency in the sense of limiting the 
number of moves in the dialogue and as in prevents agents 
revealing information that they might better keep hidden 
another form of parsimony is to insist that agents are not 
allowed to put forward arguments that will be undercut by 
arguments that have already been made during the dialogue 
we therefore distinguish such arguments 
 
see for examples of dialogues where this is not the case 
 
we consider such reasoning sub-types of rhetoric 
definition consider a dialogue d consisting of a 
sequence of moves mi with a corresponding argument graph 
ag the move mi and the argument it puts forward 
ai are both said to be pre-empted if ai is undercut by 
some a ∈ ai 
we use the term pre-empted because if such an argument 
is put forward it can seem as though another agent 
anticipated the argument being made and already made an 
argument that would render it useless in the rest of this 
paper we will only deal with protocols that permit moves 
that are relevant in any of the senses introduced above and 
are not allowed to be pre-empted we call such protocols 
basic protocols and dialogues carried out under such protocols 
basic dialogues 
the argument graph of a basic dialogue is somewhat 
restricted 
proposition consider a basic dialogue d the 
argumentation graph ag that corresponds to d is a tree with 
a single root 
proof recall that definition requires only that ag 
be a directed graph to show that it is a tree we have to 
show that it is acyclic and connected 
that the graph is connected follows from the construction 
of the graph under a protocol that enforces relevance if the 
notion of relevance is r each move adds a node that is 
connected to the previous node if the notion of relevance is 
r then every move adds a node that is connected to the 
root and thus is connected to some node in the graph if the 
notion of relevance is r then every move has to change the 
status of the argument denoted by the root proposition 
tells us that to affect the status of an argument a the node 
v representing the argument a that is effecting the change 
has to be connected to v the node representing a and so 
it follows that every new node added as a result of an 
r relevant move will be connected to the argumentation graph 
thus ag is connected 
since a basic dialogue does not allow moves that are 
preempted every edge that is added during construction is 
directed from the node that is added to one already in the graph 
 thus denoting that the argument a denoted by the added 
node v undercuts the argument a denoted by the node to 
which the connection is made v rather than the other way 
around since every edge that is added is directed from the 
new node to the rest of the graph there can be no cycles 
thus ag is a tree 
to show that ag has a single root consider its 
construction from the initial node after m the graph has one node 
v that is both a root and a leaf after m the graph is two 
nodes connected by an edge and v is now a root and not a 
leaf v is a leaf and not a root however the third node is 
added the argument earlier in this proof demonstrates that 
there will be a directed edge from it to some other node 
making it a leaf thus v will always be the only root the ruling 
out of pre-empted moves means that v will never cease to 
be a root and so the argumentation graph will always have 
one root 
since every argumentation graph constructed by a basic 
dialogue is a tree with a single root this means that the first 
node of every argumentation graph is the root 
although these results are straightforward to obtain they 
allow us to show how the notions of relevance are related 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
proposition consider a basic dialogue d 
consisting of a sequence of moves mi with a corresponding 
argument graph ag 
 every move mi that is r -relevant is r -relevant 
the converse does not hold 
 every move mi that is r -relevant is r -relevant 
the converse does not hold 
 not every move mi that is r -relevant is r -relevant 
and not every move mi that is r -relevant is 
r relevant 
proof for consider how move mi can satisfy r 
proposition tells us that if ai can change the status 
of the argument denoted by the root v which as observed 
above is the first node of ag then vi must be connected 
to the root this is precisely what is required to satisfy r 
and the relatiosnhip is proved to hold 
to see that the converse does not hold we have to consider 
what it takes to change the status of r since proposition 
tells us that connectedness is not enough to ensure a change 
of status - if it did r and r relevance would coincide 
for mi to change the status of the root it will have to 
make the argument a represented by r either unacceptable 
if it were acceptable before the move or acceptable if it 
were unacceptable before the move given the definition of 
acceptability it can achieve either by directly 
undercutting the argument represented by r in which case vi will 
be directly connected to r by some edge or by undercutting 
some argument a that is part of the set of non-undercut 
arguments defending a in the latter case vi will be 
directly connected to the node representing a and by 
proposition to r to achieve vi will have to undercut 
an argument a that is either currently undercutting a or 
is undercutting an argument that would otherwise defend a 
now further consider that mi puts forward an argument 
ai that undercuts the argument denoted by some node v 
but this latter argument defends itself against ai in such 
a case the set of acceptable arguments will not change and 
so the status of ar will not change thus a move that is 
r -relevant need not be r -relevant 
for consider that mi can satisfy r simply by adding 
a node that is connected to vi the last node to be added 
to ag by proposition it is connected to r and so is 
r -relevant 
to see that the converse does not hold consider that an 
r -relevant move can connect to any node in ag 
the first part of follows by a similar argument to that we 
just used - an r -relevant move does not have to connect to 
vi just to some v that is part of the graph - and the second 
part follows since a move that is r -relevant may introduce 
an argument ai that undercuts the argument ai put 
forward by the previous move and so vi is connected to vi 
but finds that ai defends itself against ai preventing a 
change of status at the root 
what is most interesting is not so much the results but 
why they hold since this reveals some aspects of the 
interplay between relevance and the structure of argument 
graphs for example to restate a case from the proof of 
proposition a move that is r -relevant by definition has 
to add a node to the argument graph that is connected to the 
last node that was added since a move that is r -relevant 
can add a node that connects anywhere on an argument 
graph any move that is r -relevant will be r -relevant 
but the converse does not hold 
it turns out that we can exploit the interplay between 
structure and relevance that propositions and have 
started to illuminate to establish relationships between the 
protocols that govern dialogues and the argument graphs 
constructed during such dialogues to do this we need to 
define protocols in such a way that they refer to the structure 
of the graph we have 
definition a protocol is single-path if all dialogues 
that conform to it construct argument graphs that have only 
one branch 
proposition a basic protocol p is single-path if for 
all i the set of permitted moves mi at move i are all 
r relevant the converse does not hold 
proof r -relevance requires that every node added to 
the argument graph be connected to the previous node 
starting from the first node this recursively constructs a tree with 
just one branch and the relationship holds the converse 
does not hold because even if one or more moves in the 
protocol are r - or r -relevant it may be the case that because 
of an agent s rhetorical choice or because of its knowledge 
every argument that is chosen to be put forward will 
undercut the previous argument and so the argument graph is a 
one-branch tree 
looking for more complex kinds of protocol that construct 
more complex kinds of argument graph it is an obvious 
move to turn to 
definition a basic protocol is multi-path if all 
dialogues that conform to it can construct argument graphs that 
are trees 
but on reflection since any graph with only one branch is 
also a tree 
proposition any single-path protocol is an instance 
of a multi-path protocol 
and furthermore 
proposition any basic protocol p is multi-path 
proof immediate from proposition 
so the notion of a multi-path protocol does not have much 
traction as a result we distinguish multi-path protocols 
that permit dialogues that can construct trees that have 
more than one branch as bushy protocols we then have 
proposition a basic protocol p is bushy if for some 
i the set of permitted moves mi at move i are all r - or 
r -relevant 
proof from proposition we know that if all moves 
are r -relevant then we ll get a tree with one branch and 
from proposition we know that all basic protocols will 
build an argument graph that is a tree so providing we 
exclude r -relevant moves we will get protocols that can build 
multi-branch trees 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
of course since by proposition any move that is 
r relevant is r -relevant and can quite possibly be r -relevant 
 all that proposition tells us is that there is no 
guarantee that it will be all that proposition tells us is that 
dialogues that conform to bushy protocols may have more 
than one branch all we can do is to identify a bound on 
the number of branches 
proposition consider a basic dialogue d that 
includes m moves that are not r -relevant and has a 
corresponding argumentation graph ag the number of branches 
in ag is less than or equal to m 
proof since it must connect a node to the last node 
added to ag an r -relevant move can only extend an 
existing branch since they do not have the same restriction 
r and r -relevant moves may create a new branch by 
connecting to a node that is not the last node added every 
such move could create a new branch and if they do we 
will have m branches if there were r -relevant moves 
before any of these new-branch-creating moves then these m 
branches are in addition to the initial branch created by the 
r -relevant moves and we have a maximum of m 
possible branches 
we distinguish bushy protocols from multi-path protocols 
and hence r - and r -relevance from r -relevance because 
of the kinds of dialogue that r -relevance enforces in a 
dialogue in which all moves must be r -relevant the 
argumentation graph has a single branch - the dialogue consists of 
a sequence of arguments each of which undercuts the 
previous one and the last move to be made is the one that settles 
the dialogue this as we will see next means that such a 
dialogue only allows a subset of all the moves that would 
otherwise be possible 
 completeness 
the above discussion of the difference between dialogues 
carried out under single-path and bushy protocols brings us 
to the consideration of what called predeterminism 
but we now prefer to describe using the term 
completeness the idea of predeterminism as described in 
captures the notion that under some circumstances the 
result of a dialogue can be established without actually having 
the dialogue - the agents have sufficiently little room for 
rhetorical manoeuver that were one able to see the contents 
of all the σi of all the αi ∈ a one would be able to 
identify the outcome of any dialogue on a given subject 
 we 
develop this idea by considering how the argument graphs 
constructed by dialogues under different protocols compare 
to benchmark complete dialogues we start by developing 
ideas of what complete might mean one reasonable 
definition is that 
definition a basic dialogue d between the set of 
agents a with a corresponding argumentation graph ag is 
topic-complete if no agent can construct an argument a that 
undercuts any argument a represented by a node in ag 
the argumentation graph constructed by a topic-complete 
dialogue is called a topic-complete argumentation graph and 
is denoted ag d t 
 
assuming that the σi do not change during the dialogue which is 
the usual assumption in this kind of dialogue 
a dialogue is topic-complete when no agent can add 
anything that is directly connected to the subject of the 
dialogue some protocols will prevent agents from making 
moves even though the dialogue is not topic-complete to 
distinguish such cases we have 
definition a basic dialogue d between the set of 
agents a with a corresponding argumentation graph ag is 
protocol-complete under a protocol p if no agent can make 
a move that adds a node to the argumentation graph that is 
permitted by p 
the argumentation graph constructed by a protocol-complete 
dialogue is called a protocol-complete argumentation graph 
and is denoted ag d p clearly 
proposition any dialogue d under a basic protocol 
p is protocol-complete if it is topic-complete the converse 
does not hold in general 
proof if d is topic-complete no agent can make a move 
that will extend the argumentation graph this means that 
no agent can make a move that is permitted by a basic 
protocol and so d is also protocol complete 
the converse does not hold since some basic dialogues 
 under a protocol that only permits r -relevant moves for 
example will not permit certain moves like the addition of 
a node that connects to the root of the argumentation graph 
after more than two moves that would be allowed in a 
topiccomplete dialogue 
corollary for a basic dialogue d ag d p is a 
sub-graph of ag d t 
obviously from the definition of a sub-graph the converse 
of corollary does not hold in general 
the important distinction between topic- and 
protocolcompleteness is that the former is determined purely by the 
state of the dialogue - as captured by the argumentation 
graph - and is thus independent of the protocol while the 
latter is determined entirely by the protocol any time that 
a dialogue ends in a state of protocol-completeness rather 
than topic completeness it is ending when agents still have 
things to say but can t because the protocol won t allow 
them to 
with these definitions of completeness our task is to 
relate topic-completeness - the property that ensures that 
agents can say everything that they have to say in a dialogue 
that is in some sense important - to the notions of 
relevance we have developed - which determine what agents 
are allowed to say when we need very specific conditions to 
make protocol-complete dialogues topic-complete it means 
that agents have lots of room for rhetorical maneouver when 
those conditions are not in force that is there are many 
ways they can bring dialogues to a close before everything 
that can be said has been said where few conditions are 
required or conditions are absent then dialogues between 
agents with the same knowledge will always play out the 
same way and rhetoric has no place we have 
proposition a protocol-complete basic dialogue d 
under a protocol which only allows r -relevant moves will 
be topic-complete only when ag d t has a single branch 
in which the nodes are labelled in increasing order from the 
root 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
proof given what we know about r -relevance the 
condition on ag d p having a single branch is obvious this 
is not a sufficient condition on its own because certain 
protocols may prevent - through additional restrictions like 
strict turn-taking in a multi-party dialogue - all the nodes 
in ag d t which is not subject to such restrictions being 
added to the graph only when ag d t includes the nodes 
in the exact order that the corresponding arguments are put 
forward is it necessary that a topic-complete argumentation 
graph be constructed 
given proposition these are the conditions under which 
dialogues conducted under the notion of r -relevance will 
always be predetermined and given how restrictive the 
conditions are such dialogues seem to have plenty of room for 
rhetoric to play a part 
to find similar conditions for dialogues composed of 
r and r -relevant moves we first need to distinguish between 
them we can do this in terms of the structure of the 
argumentation graph 
proposition consider a basic dialogue d with 
argumentation graph ag which has root r denoting an 
argument a if argument a denoted by node v is an an 
r relevant move m m is not r -relevant if and only if 
 there are two nodes v and v on the path between v 
and r and the argument denoted by v defends itself 
against the argument denoted by v or 
 there is an argument a denoted by node v that 
affects the status of a and the path from v to r has one 
or more nodes in common with the path from v to r 
proof for the first condition consider that since ag is 
a tree v is connected to r thus there is a series of undercut 
relations between a and a and this corrresponds to a path 
through ag if this path is the only branch in the tree then 
a will affect the status of a unless the chain of affect 
is broken by an undercut that can t change the status of the 
undercut argument because the latter defends itself 
for the second condition as for the first the only way 
that a cannot affect the status of a is if something is 
blocking its influence if this is not due to defending against 
it must be because there is some node u on the path that 
represents an argument whose status is fixed somehow and 
that must mean that there is another chain of undercut 
relations another branch of the tree that is incident at u since 
this second branch denotes another chain of arguments and 
these affect the status of the argument denoted by u they 
must also affect the status of a any of these are the a in 
the condition 
so an r -relevant move m is not r -relevant if either its 
effect is blocked because an argument upstream is not strong 
enough or because there is another line of argument that 
is currently determining the status of the argument at the 
root this in turn means that if the effect is not due to 
defending against then there is an alternative move that 
is r -relevant - a move that undercuts a in the second 
condition above 
 we can now show 
 
though whether the agent in question can make such a move is 
another question 
proposition a protocol-complete basic dialogue d 
will always be topic-complete under a protocol which only 
includes r -relevant moves and allows every r -relevant move 
to be made 
the restriction on r -relevant rules is exactly that for 
topiccompleteness so a dialogue that has only r -relevant moves 
will continue until every argument that any agent can make 
has been put forward given this and what we revealed 
about r -relevance in proposition we can see that 
proposition a protocol-complete basic dialogue d 
under a protocol which only includes r -relevant moves will 
be topic-complete if ag d t 
 includes no path with adjacent nodes v denoting a 
and v denoting a such that a undercuts a and a 
is stronger that a and 
 is such that the nodes in every branch have consecutive 
indices and no node with degree greater than two is an 
odd number of arcs from a leaf node 
proof the first condition rules out the first condition 
in proposition and the second deals with the situation 
that leads to the second condition in proposition the 
second condition ensures that each branch is constructed in 
full before any new branch is added and when a new branch 
is added the argument that is undercut as part of the 
addition will be acceptable and so the addition will change the 
status of the argument denoted by that node and hence the 
root with these conditions every move required to 
construct ag d t will be permitted and so the dialogue will be 
topic-complete when every move has been completed 
the second part of this result only identifies one possible 
way to ensure that the second condition in proposition 
is met so the converse of this result does not hold 
however what we have is sufficient to answer the 
question about predetermination that we started with for 
dialogues to be predetermined every move that is r -relevant 
must be made in such cases every dialogue is topic 
complete if we do not require that all r -relevant moves are 
made then there is some room for rhetoric - the way in 
which alternative lines of argument are presented becomes 
an issue if moves are forced to be r -relevant then there 
is considerable room for rhetorical play 
 summary 
this paper has studied the different ideas of relevance in 
argumentation-based dialogue identifying the relationship 
between these ideas and showing how they can impact the 
extent to which the way that agents choose moves in a 
dialogue - what some authors have called the strategy and 
tactics of a dialogue this extends existing work on 
relvance such as by showing how different notions of 
relevance can have an effect on the outcome of a dialogue 
in particular when they render the outcome predetermined 
this connection extends the work of which considered 
dialogue outcome but stopped short of identifying the 
conditions under which it is predetermined 
there are two ways we are currently trying to extend this 
work both of which will generalise the results and extend its 
applicability first we want to relax the restrictions that 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
we have imposed the exclusion of moves that attack 
several arguments without which the argument graph can be 
mulitply-connected and the exclusion of pre-empted moves 
without which the argument graph can have cycles 
second we want to extend the ideas of relevance to cope with 
moves that do not only add undercutting arguments but 
also supporting arguments thus taking account of bipolar 
argumentation frameworks 
acknowledgments 
the authors are grateful for financial support received from 
the ec through project ist-fp - and from the nsf 
under grants rec- - and nsf iis- they are 
also grateful to peter stone for a question now several years 
old which this paper has finally answered 
 references 
 l amgoud and c cayrol on the acceptability of 
arguments in preference-based argumentation 
framework in proceedings of the th conference on 
uncertainty in artificial intelligence pages - 
 l amgoud s parsons and n maudet arguments 
dialogue and negotiation in w horn editor 
proceedings of the fourteenth european conference on 
artificial intelligence pages - berlin 
germany ios press 
 j bentahar m mbarki and b moulin strategic and 
tactic reasoning for communicating agents in 
n maudet i rahwan and s parsons editors 
proceedings of the third workshop on argumentation 
in muliagent systems hakodate japan 
 p besnard and a hunter a logic-based theory of 
deductive arguments artificial intelligence 
 - 
 c cayrol c devred and m -c lagasquie-schiex 
handling controversial arguments in bipolar 
argumentation frameworks in p e dunne and 
t j m bench-capon editors computational models 
of argument proceedings of comma pages 
 - ios press 
 b chaib-draa and f dignum trends in agent 
communication language computational intelligence 
 - 
 f dignum b dunin-k¸eplicz and r verbrugge 
agent theory for team formation by dialogue in 
c castelfranchi and y lesp´erance editors seventh 
workshop on agent theories architectures and 
languages pages - boston usa 
 p m dung on the acceptability of arguments and its 
fundamental role in nonmonotonic reasoning logic 
programming and n-person games artificial 
intelligence - 
 p m dung r a kowalski and f toni dialectic 
proof procedures for assumption-based admissable 
argumentation artificial intelligence - 
 
 r a flores and r c kremer to commit or not to 
commit computational intelligence - 
 
 d m gabbay and j woods more on 
non-cooperation in dialogue logic logic journal of 
the igpl - 
 d m gabbay and j woods non-cooperation in 
dialogue logic synthese - - 
 c l hamblin mathematical models of dialogue 
theoria - 
 s kraus k sycara and a evenchik reaching 
agreements through argumentation a logical model 
and implementation artificial intelligence 
 - - 
 n oren t j norman and a preece loose lips sink 
ships a heuristic for argumentation in n maudet 
i rahwan and s parsons editors proceedings of the 
third workshop on argumentation in muliagent 
systems hakodate japan 
 s parsons and n r jennings negotiation through 
argumentation - a preliminary report in proceedings 
of second international conference on multi-agent 
systems pages - 
 s parsons m wooldridge and l amgoud an 
analysis of formal inter-agent dialogues in st 
international conference on autonomous agents and 
multi-agent systems acm press 
 s parsons m wooldridge and l amgoud on the 
outcomes of formal inter-agent dialogues in nd 
international conference on autonomous agents and 
multi-agent systems acm press 
 h prakken on dialogue systems with speech acts 
arguments and counterarguments in proceedings of 
the seventh european workshop on logic in artificial 
intelligence berlin germany springer verlag 
 h prakken relating protocols for dynamic dispute 
with logics for defeasible argumentation synthese 
 - 
 h prakken and g sartor modelling reasoning with 
precedents in a formal dialogue game artificial 
intelligence and law - 
 i rahwan p mcburney and e sonenberg towards 
a theory of negotiation strategy in i rahwan 
p moraitis and c reed editors proceedings of the 
 st international workshop on argumentation in 
multiagent systems new york ny 
 c reed dialogue frames in agent communications in 
y demazeau editor proceedings of the third 
international conference on multi-agent systems 
pages - ieee press 
 m rovatsos i rahwan f fisher and g weiss 
adaptive strategies for practical argument-based 
negotiation in i rahwan p moraitis and c reed 
editors proceedings of the st international workshop 
on argumentation in multiagent systems new york 
ny 
 m schroeder d a plewe and a raab ultima 
ratio should hamlet kill claudius in proceedings of 
the nd international conference on autonomous 
agents pages - 
 k sycara argumentation planning other agents 
plans in proceedings of the eleventh joint conference 
on artificial intelligence pages - 
 d n walton and e c w krabbe commitment in 
dialogue basic concepts of interpersonal reasoning 
state university of new york press albany ny 
usa 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
on the benefits of cheating by self-interested agents in 
vehicular networks∗ 
raz lin and sarit kraus 
computer science department 
bar-ilan university 
ramat-gan israel 
{linraz sarit} cs biu ac il 
yuval shavitt 
school of electrical engineering 
tel-aviv university israel 
shavitt eng tau ac il 
abstract 
as more and more cars are equipped with gps and wi-fi 
transmitters it becomes easier to design systems that will 
allow cars to interact autonomously with each other e g 
regarding traffic on the roads indeed car manufacturers 
are already equipping their cars with such devices though 
currently these systems are a proprietary we envision a 
natural evolution where agent applications will be developed 
for vehicular systems e g to improve car routing in dense 
urban areas nonetheless this new technology and agent 
applications may lead to the emergence of self-interested car 
owners who will care more about their own welfare than the 
social welfare of their peers these car owners will try to 
manipulate their agents such that they transmit false data 
to their peers using a simulation environment which 
models a real transportation network in a large city we 
demonstrate the benefits achieved by self-interested agents if no 
counter-measures are implemented 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence-intelligent agents 
general terms 
experimentation 
 introduction 
as technology advances more and more cars are being 
equipped with devices which enable them to act as 
autonomous agents an important advancement in this 
respect is the introduction of ad-hoc communication networks 
 such as wi-fi which enable the exchange of information 
between cars e g for locating road congestions and 
optimal routes or improving traffic safety 
vehicle-to-vehicle v v communication is already 
onboard by some car manufactures enabling the 
collaboration between different cars on the road for example gm s 
proprietary algorithm called the threat assessment 
algorithm constantly calculates in real time other vehicles 
positions and speeds and enables messaging other cars when 
a collision is imminent also honda has began testing its 
system in which vehicles talk with each other and with the 
highway system itself 
in this paper we investigate the attraction of being a 
selfish agent in vehicular networks that is we investigate the 
benefits achieved by car owners who tamper with on-board 
devices and incorporate their own self-interested agents in 
them which act for their benefit we build on the notion 
of gossip networks introduced by shavitt and shay in 
which the agents can obtain road congestion information by 
gossiping with peer agents using ad-hoc communication 
we recognize two typical behaviors that the self-interested 
agents could embark upon in the context of vehicular 
networks in the first behavior described in section the 
objective of the self-interested agents is to maximize their 
own utility expressed by their average journey duration on 
the road this situation can be modeled in real life by car 
owners whose aim is to reach their destination as fast as 
possible and would like to have their way free of other cars to 
this end they will let their agents cheat the other agents by 
injecting false information into the network this is achieved 
by reporting heavy traffic values for the roads on their route 
to other agents in the network in the hope of making the 
other agents believe that the route is jammed and causing 
them to choose a different route 
the second type of behavior described in section is 
modeled by the self-interested agents objective to cause 
disorder in the network more than they are interested in 
maximizing their own utility this kind of behavior could 
be generated for example by vandalism or terrorists who 
aim to cause as much mayhem in the network as possible 
we note that the introduction of self-interested agents to 
the network would most probably motivate other agents to 
try and detect these agents in order to minimize their effect 
this is similar though in a different context to the problem 
introduced by lamport et al as the byzantine generals 
problem however the introduction of mechanisms to deal 
with self-interested agents is costly and time consuming in 
this paper we focus mainly on the attractiveness of selfish 
behavior by these agents while we also provide some insights 
 
 - - - - rps c ifaamas 
into the possibility of detecting self-interested agents and 
minimizing their effect 
to demonstrate the benefits achieved by self-interested 
agents we have used a simulation environment which 
models the transportation network in a central part of a large 
real city the simulation environment is further described in 
section our simulations provide insights to the benefits 
of self-interested agents cheating our findings can motivate 
future research in this field in order to minimize the effect 
of selfish-agents 
the rest of this paper is organized as follows in section 
we review related work in the field of self-interested agents 
and v v communications we continue and formally 
describe our environment and simulation settings in section 
sections and describe the different behaviors of the 
selfinterested agents and our findings finally we conclude the 
paper with open questions and future research directions 
 related work 
in their seminal paper lamport et al describe the 
byzantine generals problem in which processors need to 
handle malfunctioning components that give conflicting 
information to different parts of the system they also present 
a model in which not all agents are connected and thus an 
agent cannot send a message to all the other agents dolev et 
al has built on this problem and has analyzed the number 
of faulty agents that can be tolerated in order to eventually 
reach the right conclusion about true data similar work 
is presented by minsky et al who discuss techniques 
for constructing gossip protocols that are resilient to up to 
t malicious host failures as opposed to the above works 
our work focuses on vehicular networks in which the agents 
are constantly roaming the network and exchanging data 
also the domain of transportation networks introduces 
dynamic data as the load of the roads is subject to change in 
addition the system in transportation networks has a 
feedback mechanism since the load in the roads depends on the 
reports and the movement of the agents themselves 
malkhi et al present a gossip algorithm for 
propagating information in a network of processors in the presence 
of malicious parties their algorithm prevents the spreading 
of spurious gossip and diffuses genuine data this is done 
in time which is logarithmic in the number of processes 
and linear in the number of corrupt parties nevertheless 
their work assumes that the network is static and also that 
the agents are static they discuss a network of processors 
this is not true for transportation networks for example 
in our model agents might gossip about heavy traffic load 
of a specific road which is currently jammed yet this 
information might be false several minutes later leaving the 
agents to speculate whether the spreading agents are indeed 
malicious or not in addition as the agents are constantly 
moving each agent cannot choose with whom he interacts 
and exchanges data 
in the context of analyzing the data and deciding whether 
the data is true or not researchers have focused on 
distributed reputation systems or decision mechanisms to decide 
whether or not to share data 
yu and singh build a social network of agents 
reputations every agent keeps a list of its neighbors which can 
be changed over time and computes the trustworthiness of 
other agents by updating the current values of testimonies 
obtained from reliable referral chains after a bad 
experience with another agent every agent decreases the rating of 
the bad agent and propagates this bad experience 
throughout the network so that other agents can update their 
ratings accordingly this approach might be implemented in 
our domain to allow gossip agents to identify self-interested 
agents and thus minimize their effect however the 
implementation of such a mechanism is an expensive addition 
to the infrastructure of autonomous agents in 
transportation networks this is mainly due to the dynamic nature of 
the list of neighbors in transportation networks thus not 
only does it require maintaining the neighbors list since the 
neighbors change frequently but it is also harder to build a 
good reputation system 
leckie et al focus on the issue of when to share 
information between the agents in the network their domain 
involves monitoring distributed sensors each agent 
monitors a subset of the sensors and evaluates a hypothesis based 
on the local measurements of its sensors if the agent 
believes that a hypothesis is sufficient likely he exchanges this 
information with the other agents in their domain the 
goal of all the agents is to reach a global consensus about 
the likelihood of the hypothesis in our domain however as 
the agents constantly move they have many samples which 
they exchange with each other also the data might also 
vary e g a road might be reported as jammed but a few 
minutes later it could be free thus making it harder to 
decide whether to trust the agent who sent the data 
moreover the agent might lie only about a subset of its samples 
thus making it even harder to detect his cheating 
some work has been done in the context of gossip networks 
or transportation networks regarding the spreading of data 
and its dissemination 
datta et al focus on information dissemination in 
mobile ad-hoc networks manet they propose an 
autonomous gossiping algorithm for an infrastructure-less 
mobile ad-hoc networking environment their autonomous 
gossiping algorithm uses a greedy mechanism to spread data 
items in the network the data items are spread to 
immediate neighbors that are interested in the information and 
avoid ones that are not interested the decision which node 
is interested in the information is made by the data item 
itself using heuristics however their work concentrates on 
the movement of the data itself and not on the agents who 
propagate the data this is different from our scenario in 
which each agent maintains the data it has gathered while 
the agent itself roams the road and is responsible and has 
the capabilities for spreading the data to other agents in 
the network 
das et al propose a cooperative strategy for content 
delivery in vehicular networks in their domain peers 
download a file from a mesh and exchange pieces of the file among 
themselves we on the other hand are interested in 
vehicular networks in which there is no rule forcing the agents to 
cooperate among themselves 
shibata et al propose a method for cars to 
cooperatively and autonomously collect traffic jam statistics to 
estimate arrival time to destinations for each car the 
communication is based on ieee without using a fixed 
infrastructure on the ground while we use the same 
domain we focus on a different problem shibata et al 
mainly focus on efficiently broadcasting the data between 
agents e g avoid duplicates and communication overhead 
as we focus on the case where agents are not cooperative in 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
nature and on how selfish agents affect other agents and the 
network load 
wang et al also assert in the context of wireless 
networks that individual agents are likely to do what is most 
beneficial for their owners and will act selfishly they design 
a protocol for communication in networks in which all agents 
are selfish their protocol motivates every agent to 
maximize its profit only when it behaves truthfully a mechanism 
of incentive compatibility however the domain of wireless 
networks is quite different from the domain of 
transportation networks in the wireless network the wireless terminal 
is required to contribute its local resources to transmit data 
thus wang et al use a payment mechanism which 
attaches costs to terminals when transmitting data and thus 
enables them to maximize their utility when transmitting 
data instead of acting selfishly unlike this in the context 
of transportation networks constructing such a mechanism 
is not quite a straightforward task as self-interested agents 
and regular gossip agents might incur the same cost when 
transmitting data the difference between the two types of 
agents only exists regarding the credibility of the data they 
exchange 
in the next section we will describe our transportation 
network model and gossiping between the agents we will 
also describe the different agents in our system 
 model and simulations 
we first describe the formal transportation network model 
and then we describe the simulations designs 
 formal model 
following shavitt and shay and parshani the 
transportation network is represented by a directed graph 
g v e where v is the set of vertices representing 
junctions and e is the set of edges representing roads an edge 
e ∈ e is associated with a weight w which specifies 
the time it takes to traverse the road associated with that 
edge the roads weights vary in time according to the 
network traffic load each car which is associated with an 
autonomous agent is given a pair of origin and destination 
points vertices a journey is defined as the not 
necessarily simple path taken by an agent between the origin vertex 
and the destination vertex we assume that there is always 
a path between a source and a destination a journey length 
is defined as the sum of all weights of the edges constituting 
this path every agent has to travel between its origin and 
destination points and aims to minimize its journey length 
initially agents are ignorant about the state of the roads 
regular agents are only capable of gathering information 
about the roads as they traverse them however we assume 
that some agents have means of inter-vehicle 
communication e g ieee with a given communication range 
which enables them to communicate with other agents with 
the same device those agents are referred to as gossip 
agents since the communication range is limited the 
exchange of information using gossiping is done in one of two 
ways a between gossip agents passing one another or b 
between gossip agents located at the same junction we 
assume that each agent stores the most recent information it 
has received or gathered around the edges in the network 
a subset of the gossip agents are those agents who are 
selfinterested and manipulate the devices for their own benefit 
we will refer to these agents as self-interested agents a 
detailed description of their behavior is given in sections 
and 
 simulation design 
building on the network in our simulations replicates 
a central part of a large city and consists of junctions 
and roads which are approximately the number of main 
streets in the city each simulation consists of iterations 
the basic time unit of the iteration is a step which 
equivalents to about seconds each iteration simulates six hours 
of movements the average number of cars passing through 
the network during the iteration is about and the 
average number of cars in the network at a specific time unit 
is about cars in each iteration the same agents are 
used with the same origin and destination points whereas 
the data collected in earlier iterations is preserved in the 
future iterations referred to as the history of the agent 
this allows us to simulate somewhat a daily routine in the 
transportation network e g a working week 
each of the experiments that we describe below is run 
with different traffic scenarios each such traffic scenario 
differs from one another by the initial load of the roads and 
the designated routes of the agents cars in the network 
for each such scenario simulations are run creating a total 
of simulations for each experiment 
it has been shown by parshani et al that the 
information propagation in the network is very efficient when 
the percentage of gossiping agents is or more yet due 
to congestion caused by too many cars rushing to what is 
reported as the less congested part of the network - 
of gossiping agents leads to the most efficient routing results 
in their experiments thus in our simulation we focus only 
on simulations in which the percentage of gossip agents is 
 
the simulations were done with different percentages of 
self-interested agents to gain statistical significance we ran 
each simulation with changes in the set of the gossip agents 
and the set of the self-interested agents 
in order to gain a similar ordinal scale the results were 
normalized the normalized values were calculated by 
comparing each agent s result to his results when the same 
scenario was run with no self-interested agents this was done 
for all of the iterations using the normalized values enabled 
us to see how worse or better each agent would perform 
compared to the basic setting for example if an average 
journey length of a certain agent in iteration with no 
selfinterested agent was and the length was in the same 
scenario and iteration in which self-interested agents were 
involved then the normalized value for that agent would be 
 
more details regarding the simulations are described in 
sections and 
 spreading lies maximizing 
utility 
in the first set of experiments we investigated the benefits 
achieved by the self-interested agents whose aim was to 
minimize their own journey length the self-interested agents 
adopted a cheating approach in which they sent false data 
to their peers 
in this section we first describe the simulations with the 
self-interested agents then we model the scenario as a 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
game with two types of agents and prove that the 
equilibrium result can only be achieved when there is no efficient 
exchange of gossiping information in the network 
 modeling the self-interested agents 
behavior 
while the gossip agents gather data and send it to other 
agents the self-interested agents behavior is modeled as 
follows 
 calculate the shortest path from origin to destination 
 communicate the following data to other agents 
 a if the road is not in the agent s route - send the 
true data about it e g data about roads it has 
received from other agents 
 b for all roads in the agent s route which the agent 
has not yet traversed send a random high weight 
basically the self-interested agent acts the same as the 
gossip agent it collects data regarding the weight of the roads 
 either by traversing the road or by getting the data from 
other agents and sends the data it has collected to other 
agents however the self-interested agent acts differently 
when the road is in its route since the agent s goal is to 
reach its destination as fast as possible the agent will falsely 
report that all the roads in its route are heavily congested 
this is in order to free the path for itself by making other 
agents recalculate their paths this time without including 
roads on the self-interested agent s route to this end for 
all the roads in its route which the agent has not yet passed 
the agent generates a random weight which is above the 
average weight of the roads in the network it then associates 
these new weights with the roads in its route and sends them 
to the other agents 
while an agent can also divert cars from its route by 
falsely reporting congested roads in parallel to its route as 
free this behavior is not very likely since other agents 
attempting to use the roads will find the mistake within a 
short time and spread the true congestion on the road on 
the other hand if an agent manages to persuade other agents 
not to use a road it will be harder for them to detect that 
the said roads are not congested 
in addition to avoid being influenced by its own lies and 
other lies spreading in the network all self-interested agents 
will ignore data received about roads with heavy traffic note 
that data about roads that are not heavily traffic will not 
be ignored 
 
in the next subsection we describe the simulation results 
involving the self-interested agents 
 simulation results 
to test the benefits of cheating by the self-interested agents 
we ran several experiments in the first set of experiments 
we created a scenario in which a small group of self-interested 
agents spread lies on the same route and tested its 
effect on the journey length of all the agents in the network 
 
in other simulations we have run in which there had been 
several real congestions in the network we indeed saw that 
even when the roads are jammed the self-interested agents 
were less affected if they ignored all reported heavy traffic 
since by such they also discarded all lies roaming the network 
table normalized journey length values 
selfinterested agents with the same route 
iteration self-interested gossip - gossip - regular 
number agents sr others agents 
 
 
 
 
 
 
thus several cars which had the same origin and 
destination points were designated as self-interested agents in this 
simulation we selected only agents to be part of the group 
of the self-interested agents as we wanted to investigate the 
effect achieved by only a small number of agents 
in each simulation in this experiment different agents 
were randomly chosen to be part of the group of self-interested 
agents as described above in addition one road on the 
route of these agents was randomly selected to be partially 
blocked letting only one car go through that road at each 
time step about agents were randomly selected as 
regular gossip agents and the other agents were 
designated as regular agents 
we analyzed the average journey length of the self-interested 
agents as opposed to the average journey length of other 
regular gossip agents traveling along the same route table 
 summarizes the normalized results for the self-interested 
agents the gossip agents those having the same origin and 
destination points as the self-interested agents denoted 
gossip - sr and all other gossip agents denoted gossip - 
others and the regular agents as a function of the iteration 
number 
we can see from the results that the first time the 
selfinterested agents traveled the route while spreading the false 
data about the roads did not help them using the paired 
t-test we show that those agents had significantly lower 
journey lengths in the scenario in which they did not spread any 
lies with p this is mainly due to the fact that 
the lies do not bypass the self-interested agent and reach 
other cars that are ahead of the self-interested car on the 
same route thus spreading the lies in the first iteration 
does not help the self-interested agent to free the route he 
is about to travel in the first iteration 
only when the self-interested agents had repeated their 
journey in the next iteration iteration did it help them 
significantly p the reason for this is that other 
gossip agents received this data and used it to recalculate their 
shortest path thus avoiding entrance to the roads for which 
the self-interested agents had spread false information about 
congestion it is also interesting to note the large value 
attained by the self-interested agents in the first iteration 
this is mainly due to several self-interested agents who 
entered the jammed road this situation occurred since the 
self-interested agents ignored all heavy traffic data and thus 
ignored the fact that the road was jammed as they started 
spreading lies about this road more cars shifted from this 
route thus making the road free for the future iterations 
however we also recall that the self-interested agents 
ignore all information about the heavy traffic roads thus 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
table normalized journey length values 
spreading lies for a beneficiary agent 
iteration beneficiary gossip - gossip - regular 
number agent sr others agents 
 
 
 
 
 
 
when the network becomes congested more self-interested 
cars are affected since they might enter jammed roads 
which they would otherwise not have entered this can be 
seen for example in iterations - in which the normalized 
value of the self-interested agents increased above 
using the paired t-test to compare these values with the values 
achieved by these agents when no lies are used we see that 
there is no significant difference between the two scenarios 
as opposed to the gossip agents we can see how little 
effect the self-interested agents have on the regular agents 
as compared to the gossip agents on the same route that 
have traveled as much as more when self-interested 
agents are introduced the average journey length for the 
regular agents has only increased by about this result 
is even lower than the effect on other gossip agents in the 
entire network 
since we noticed that cheating by the self-interested agents 
does not benefit them in the first iteration we devised 
another set of experiments in the second set of experiments 
the self-interested agents have the objective to help another 
agent who is supposed to enter the network some time 
after the self-interested agent entered we refer to the latter 
agent as the beneficiary agent just like a self-interested 
agent the beneficiary agent also ignores all data regarding 
heavy traffic in real-life this can be modeled for 
example by a husband who would like to help his wife find a 
faster route to her destination table summarizes the 
normalized values for the different agents as in the first set 
of experiments simulations were run for each scenario 
with a total of simulations in each of these simulation 
one agent was randomly selected as a self-interested agent 
and then another agent with the same origin as the 
selfinterested agent was randomly selected as the beneficiary 
agent the other and agents were designated 
as regular gossip agents and regular agents respectively 
we can see that as the number of iterations advances the 
lower the normalized value for the beneficiary agent in this 
scenario just like the previous one in the first iterations 
not only does the beneficiary agent not avoid the jammed 
roads since he ignores all heavy traffic he also does not 
benefit from the lies spread by the self-interested agent this 
is due to the fact that the lies are not yet incorporated by 
other gossip agents thus if we compare the average journey 
length in the first iteration when lies are spread and when 
there are no lies the average is significantly lower when there 
are no lies p on the other hand if we compare 
the average journey length in all of the iterations there is 
no significant difference between the two settings still in 
most of the iterations the average journey length of the 
beneficiary agent is longer than in the case when no lies are 
spread 
we can also see the impact on the other agents in the 
system while the gossip agents which are not on the 
route of the beneficiary agent virtually are not affected by 
the self-interested agent those on the route and the 
regular agents are affected and have higher normalized values 
that is even with just one self-interested car we can see 
that both the gossip agents that follow the same route as 
the lies spread by the self-interested agents and other 
regular agents increase their journey length by more than 
in our third set of experiments we examined a setting in 
which there was an increasing number of agents and the 
agents did not necessarily have the same origin and 
destination points to model this we randomly selected 
selfinterested agents whose objective was to minimize their 
average journey length assuming the cars were repeating 
their journeys that is more than one iteration was made 
as opposed to the first set of experiments in this set the 
self-interested agents were selected randomly and we did 
not enforce the constraint that they will all have the same 
origin and destination points 
as in the previous sets of experiments we ran 
different simulations per scenario in each simulation runs 
were made each run with different numbers of self-interested 
agents no self-interested agents and each 
agent adopted the behavior modeled in section figure 
 shows the normalized value achieved by the self-interested 
agents as a function of their number the figure shows these 
values for iterations - the first iteration is not shown 
intentionally as we assume repeated journeys also we have 
seen in the previous set of experiments and we have 
provided explanations as to why the self-interested agents do 
not gain much from their behavior in the first iteration 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
self-interested agents number 
normalizedvalue 
iteration iteration iteration 
iteration iteration 
figure self-interested agents normalized values as 
a function of the number of self-interested agents 
using these simulations we examined what the threshold 
could be for the number of randomly selected self-interested 
agents in order to allow themselves to benefit from their 
selfish behavior we can see that up to self-interested 
agents the average normalized value is below that is 
they benefit from their malicious behavior in the case of 
one self-interested agent there is a significant difference 
between the average journey length of when the agent spread 
lies and when no lies are spread p while when 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
there are and self-interested agents there is no 
significance difference yet as the number of self-interested 
agents increases the normalized value also increases in 
such cases the normalized value is larger than and the 
self-interested agents journey length becomes significantly 
higher than their journey length in cases where there are 
no self-interested agents in the system 
in the next subsection we analyze the scenario as a game 
and show that when in equilibrium the exchange of gossiping 
between the agents becomes inefficient 
 when gossiping is inefficient 
we continued and modeled our scenario as a game in 
order to find the equilibrium there are two possible types for 
the agents a regular gossip agents and b self-interested 
agents each of these agents is a representative of its group 
and thus all agents in the same group have similar behavior 
we note that the advantage of using gossiping in 
transportation networks is to allow the agents to detect anomalies 
in the network e g traffic jams and to quickly adapt to 
them by recalculating their routes we also assume that 
the objective of the self-interested agents is to minimize their 
own journey length thus they spread lies on their routes as 
described in section we also assume that sophisticated 
methods for identifying the self-interested agents or 
managing reputation are not used this is mainly due to the 
complexity of incorporating and maintaining such mechanisms 
as well as due to the dynamics of the network in which 
interactions between different agents are frequent agents may 
leave the network and data about the road might change as 
time progresses e g a road might be reported by a regular 
gossip agent as free at a given time yet it may currently be 
jammed due to heavy traffic on the road 
let tavg be the average time it takes to traverse an edge 
in the transportation network that is the average load of 
an edge let tmax be the maximum time it takes to 
traverse an edge we will investigate the game in which the 
self-interested and the regular gossip agents can choose the 
following actions the self-interested agents can choose how 
much to lie that is they can choose to spread how long not 
necessarily the true duration it takes to traverse certain 
roads since the objective of the self-interested agents is to 
spread messages as though some roads are jammed the 
traversal time they report is obviously larger than the average 
time we denote the time the self-interested agents spread 
as ts such that tavg ≤ ts ≤ tmax motivated by the 
results of the simulations we have described above we saw that 
the agents are less affected if they discard the heavy traffic 
values thus the regular gossip cars attempting to 
mitigate the effect of the liars can choose a strategy to ignore 
abnormal congestion values above a certain threshold tg 
obviously tavg ≤ tg ≤ tmax in order to prevent the 
gossip agents from detecting the lies and just discarding those 
values the self-interested agents send lies in a given range 
 ts tmax with an inverse geometric distribution that is 
the higher the t value the higher its frequency 
now we construct the utility functions for each type of 
agents which is defined by the values of ts and tg if the 
self-interested agents spread traversal times higher than or 
equal to the regular gossip cars threshold they will not 
benefit from those lies thus the utility value of the 
selfinterested agents in this case is on the other hand if the 
self-interested agents spread traversal time which is lower 
than the threshold they will gain a positive utility value 
from the regular gossip agents point-of-view if they accept 
messages from the self-interested agents then they 
incorporate the lies in their calculation thus they will lose utility 
points on the other hand if they discard the false values 
the self-interested agents send that is they do not 
incorporate the lies they will gain utility values formally we use 
us 
to denote the utility of the self-interested agents and ug 
to denote the utility of the regular gossip agents we also 
denote the strategy profile in the game as {ts tg} the 
utility functions are defined as 
us 
 
 if ts ≥ tg 
ts − tavg if ts tg 
 
ug 
 
tg − tavg if ts ≥ tg 
ts − tg if ts tg 
 
we are interested in finding the nash equilibrium we 
recall from that the nash equilibrium is a strategy 
profile where no player has anything to gain by deviating from 
his strategy given that the other agent follows his strategy 
profile formally let s u denote the game where s is 
the set of strategy profiles and u is the set of utility 
functions when each agent i ∈ {regular gossip self-interested} 
chooses a strategy ti resulting in a strategy profile t 
 ts tg then agent i obtains a utility of ui 
 t a strategy 
profile t∗ 
∈ s is a nash equilibrium if no deviation in the 
strategy by any single agent is profitable that is if for all i 
ui 
 t∗ 
 ≥ ui 
 ti t∗ 
−i that is ts tg is a nash equilibrium 
if the self-interested agents have no other value ts such that 
us 
 ts tg us 
 ts tg and similarly for the gossip agents 
we now have the following theorem 
theorem tavg tavg is the only nash equilibrium 
proof first we will show that tavg tavg is a nash 
equilibrium assume by contradiction that the gossip agents 
choose another value tg tavg thus ug 
 tavg tg 
tavg − tg on the other hand ug 
 tavg tavg 
thus the regular gossip agents have no incentive to 
deviate from this strategy the self-interested agents also have 
no incentive to deviate from this strategy by 
contradiction again assume that the self-interested agents choose 
another value ts tavg thus us 
 ts tavg while 
us 
 tavg tavg 
we will now show that the above solution is unique we 
will show that any other tuple ts tg such that tavg 
tg ≤ tmax and tavg ts ≤ tmax is not a nash equilibrium 
we have three cases in the first tavg tg ts ≤ tmax 
thus us 
 ts tg and ug 
 ts tg tg − tavg in this 
case the regular gossip agents have an incentive to deviate 
and choose another strategy tg since by doing so they 
increase their own utility ug 
 ts tg tg − tavg 
in the second case we have tavg ts tg ≤ tmax thus 
ug 
 ts tg ts − tg also the regular gossip agents 
have an incentive to deviate and choose another strategy 
tg − in which their utility value is higher ug 
 ts tg − 
ts − tg 
in the last case we have tavg ts tg ≤ tmax thus 
us 
 ts tg ts − tg in this case the self-interested 
agents have an incentive to deviate and choose another 
strategy tg − in which their utility value is higher us 
 tg − 
 tg tg − − tavg tg − tavg 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
table normalized journey length values for the 
first iteration 
self-interested self-interested gossip regular 
agents number agents agents agents 
 
 
 
 
 
 
 
 
 
 
table normalized journey length values for all 
iterations 
self-interested self-interested gossip regular 
agents number agents agents agents 
 
 
 
 
 
 
 
 
 
 
the above theorem proves that the equilibrium point is 
reached only when the self-interested agents send the time 
to traverse certain edges equals the average time and on 
the other hand the regular gossip agents discard all data 
regarding roads that are associated with an average time or 
higher thus for this equilibrium point the exchange of 
gossiping information between agents is inefficient as the gossip 
agents are unable to detect any anomalies in the network 
in the next section we describe another scenario for the 
self-interested agents in which they are not concerned with 
their own utility but rather interested in maximizing the 
average journey length of other gossip agents 
 spreading lies causing chaos 
another possible behavior that can be adopted by 
selfinterested agents is characterized by their goal to cause 
disorder in the network this can be achieved for example by 
maximizing the average journey length of all agents even at 
the cost of maximizing their own journey length 
to understand the vulnerability of the gossip based 
transportation support system we ran different simulations for 
each scenario in each simulation different agents were 
randomly chosen using a uniform distribution to act as 
gossip agents among them self-interested agents were chosen 
each self-interested agent behaved in the same manner as 
described in section 
every simulation consisted of runs with each run 
comprising different numbers of self-interested agents no 
selfinterested agents and 
also in each run the number of self-interested agents was 
increased incrementally for example the run with 
selfinterested agents consisted of all the self-interested agents 
that were used in the run with self-interested agents but 
with an additional self-interested agents 
tables and summarize the normalized journey length 
for the self-interested agents the regular gossip agents and 
the regular non-gossip agents table summarizes the 
data for the first iteration and table summarizes the data 
for the average of all iterations figure demonstrates 
the changes in the normalized values for the regular gossip 
agents and the regular agents as a function of the iteration 
number similar to the results in our first set of experiments 
described in section we can see that randomly selected 
self-interested agents who follow different randomly selected 
routes do not benefit from their malicious behavior that is 
their average journey length does not decrease however 
when only one self-interested agent is involved it does 
benefit from the malicious behavior even in the first iteration 
the results also indicate that the regular gossip agents are 
more sensitive to malicious behavior than regular 
agentsthe average journey length for the gossip agents increases 
significantly e g with self-interested agents the average 
journey length for the gossip agents was higher than 
in the setting with no self-interested agents at all as 
opposed to an increase of only for the regular agents in 
contrast these results also indicate that the self-interested 
agents do not succeed in causing a significant load in the 
network by their malicious behavior 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
iteration number 
normalizedvalue 
 self-interested agents gossip agents normalized value 
 self-interested agents gossip agents normalized value 
 self-interested agents regular agents normalized value 
 self-interested agents regular agents normalized value 
figure gossip and regular agents normalized 
values as a function of the iteration 
since the goal of the self-interested agents in this case is 
to cause disorder in the network rather than use the lies for 
their own benefits the question arises as to why would the 
behavior of the self-interested agents be to send lies about 
their routes only furthermore we hypothesize that if they 
all send lies about the same major roads the damage they 
might inflict on the entire network would be larger that had 
each of them sent lies about its own route to examine this 
hypothesis we designed another set of experiments in this 
set of experiments all the self-interested agents spread lies 
about the same main roads in the network however the 
results show quite a smaller impact on other gossip and 
reguthe sixth intl joint conf on autonomous agents and multi-agent systems aamas 
table normalized journey length values for all 
iterations network with congestions 
self-interested self-interested gossip regular 
agents number agents agents agents 
 
 
 
 
 
 
 
 
 
 
lar agents in the network the average normalized value for 
the gossip agents in these simulations was only about 
as opposed to in the original scenario when analyzing 
the results we saw that although the false data was spread 
it did not cause other gossip cars to change their route the 
main reason was that the lies were spread on roads that were 
not on the route of the self-interested agents thus it took 
the data longer to reach agents on the main roads and when 
the agents reached the relevant roads this data was too old 
to be incorporated in the other agents calculations 
we also examined the impact of sending lies in order to 
cause chaos when there are already congestions in the 
network to this end we simulated a network in which main 
roads are jammed the behavior of the self-interested agents 
is as described in section and the self-interested agents 
spread lies about their own route the simulation results 
detailed in table show that there is a greater incentive 
for the self-interested agents to cheat when the network is 
already congested as their cheating causes more damage 
to the other agents in the network for example whereas 
the average journey length of the regular agents increased 
only by about in the original scenario in which the 
network was not congested in this scenario the average journey 
length of the agents had increased by about 
 conclusions 
in this paper we investigated the benefits achieved by 
self-interested agents in vehicular networks using 
simulations we investigated two behaviors that might be taken 
by self-interested agents a trying to minimize their 
journey length and b trying to cause chaos in the network 
our simulations indicate that in both behaviors the 
selfinterested agents have only limited success achieving their 
goal even if no counter-measures are taken this is in 
contrast to the greater impact inflicted by self-interested agents 
in other domains e g e-commerce some reasons for this 
are the special characteristics of vehicular networks and their 
dynamic nature while the self-interested agents spread lies 
they cannot choose which agents with whom they will 
interact also by the time their lies reach other agents they 
might become irrelevant as more recent data has reached 
the same agents 
motivated by the simulation results future research in 
this field will focus on modeling different behaviors of the 
self-interested agents which might cause more damage to 
the network another research direction would be to find 
ways of minimizing the effect of selfish-agents by using 
distributed reputation or other measures 
 references 
 a bejan and r lawrence peer-to-peer cooperative 
driving in proceedings of iscis pages - 
orlando usa october 
 i chisalita and n shahmehri a novel architecture for 
supporting vehicular communication in proceedings of 
vtc pages - canada september 
 s das a nandan and g pau spawn a swarming 
protocol for vehicular ad-hoc wireless networks in 
proceedings of vanet pages - 
 a datta s quarteroni and k aberer autonomous 
gossiping a self-organizing epidemic algorithm for 
selective information dissemination in mobile ad-hoc 
networks in proceedings of ic-snw pages - 
maison des polytechniciens paris france june 
 d dolev r reischuk and h r strong early 
stopping in byzantine agreement jacm 
 - 
 gm threat assessment algorithm 
http www nhtsa dot gov people injury research pub 
acas acas-fieldtest 
 honda 
http world honda com news c html 
 lamport shostak and pease the byzantine generals 
problem in advances in ultra-dependable distributed 
systems n suri c j walter and m m hugue 
 eds ieee computer society press 
 c leckie and r kotagiri policies for sharing 
distributed probabilistic beliefs in proceedings of 
acsc pages - adelaide australia 
 d malkhi e pavlov and y sella gossip with 
malicious parties technical report - school of 
computer science and engineering - the hebrew 
university of jerusalem israel march 
 y m minsky and f b schneider tolerating 
malicious gossip distributed computing - 
february 
 m j osborne and a rubinstein a course in game 
theory mit press cambridge ma 
 r parshani routing in gossip networks master s 
thesis department of computer science bar-ilan 
university ramat-gan israel october 
 r parshani s kraus and y shavitt a study of 
gossiping in transportation networks submitted for 
publication 
 y shavitt and a shay optimal routing in gossip 
networks ieee transactions on vehicular 
technology - july 
 n shibata t terauchi t kitani k yasumoto 
m ito and t higashino a method for sharing traffic 
jam information using inter-vehicle communication in 
proceedings of v vcom usa 
 w wang x -y li and y wang truthful multicast 
routing in selfish wireless networks in proceedings of 
mobicom pages - usa 
 b yu and m p singh a social mechanism of 
reputation management in electronic communities in 
proceedings of cia 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a multilateral multi-issue negotiation protocol 
miniar hemaissia 
thales research 
technology france 
rd 
f- palaiseau cedex 
france 
miniar hemaissia lip fr 
amal el fallah 
seghrouchni 
lip university of paris 
 rue du capitaine scott 
f- paris france 
amal elfallah lip fr 
christophe labreuche 
and juliette mattioli 
thales research 
technology france 
rd 
f- palaiseau cedex 
france 
abstract 
in this paper we present a new protocol to address 
multilateral multi-issue negotiation in a cooperative context we 
consider complex dependencies between multiple issues by 
modelling the preferences of the agents with a multi-criteria 
decision aid tool also enabling us to extract relevant 
information on a proposal assessment this information is 
used in the protocol to help in accelerating the search for 
a consensus between the cooperative agents in addition 
the negotiation procedure is defined in a crisis management 
context where the common objective of our agents is also 
considered in the preferences of a mediator agent 
categories and subject descriptors 
i distributed artificial intelligence intelligent 
agents multiagent systems 
general terms 
theory design experimentation 
 introduction 
multi-issue negotiation protocols represent an important 
field of study since negotiation problems in the real world 
are often complex ones involving multiple issues to date 
most of previous work in this area dealt 
almost exclusively with simple negotiations involving 
independent issues however real-world negotiation problems 
involve complex dependencies between multiple issues when 
one wants to buy a car for example the value of a given 
car is highly dependent on its price consumption comfort 
and so on the addition of such interdependencies greatly 
complicates the agents utility functions and classical 
utility functions such as the weighted sum are not sufficient 
to model this kind of preferences in the 
authors consider inter-dependencies between issues most 
often defined with boolean values except for while we can 
deal with continuous and discrete dependent issues thanks 
to the modelling power of the choquet integral in 
the authors deal with bilateral negotiation while we are 
interested in a multilateral negotiation setting klein et al 
 present an approach similar to ours using a mediator 
too and information about the strength of the approval or 
rejection that an agent makes during the negotiation in 
our protocol we use more precise information to improve 
the proposals thanks to the multi-criteria methodology and 
tools used to model the preferences of our agents lin in 
 also presents a mediation service but using an 
evolutionary algorithm to reach optimal solutions and as explained 
in players in the evolutionary models need to repeatedly 
interact with each other until the stable state is reached 
as the population size increases the time it takes for the 
population to stabilize also increases resulting in excessive 
computation communication and time overheads that can 
become prohibitive and for one-to-many and many-to-many 
negotiations the overheads become higher as the number of 
players increases in the authors consider a non-linear 
utility function by using constraints on the domain of the 
issues and a mediation service to find a combination of bids 
maximizing the social welfare our preference model a 
nonlinear utility function too is more complex than one since 
the choquet integral takes into account the interactions and 
the importance of each decision criteria issue not only the 
dependencies between the values of the issues to determine 
the utility we also use an iterative protocol enabling us to 
find a solution even when no bid combination is possible 
in this paper we propose a negotiation protocol suited for 
multiple agents with complex preferences and taking into 
account at the same time multiple interdependent issues and 
recommendations made by the agents to improve a proposal 
moreover the preferences of our agents are modelled using 
a multi-criteria methodology and tools enabling us to take 
into account information about the improvements that can 
be made to a proposal in order to help in accelerating the 
search for a consensus between the agents therefore we 
propose a negotiation protocol consisting of solving our 
decision problem using a mas with a multi-criteria decision 
aiding modelling at the agent level and a cooperation-based 
multilateral multi-issue negotiation protocol this protocol 
is studied under a non-cooperative approach and it is shown 
 
 - - - - rps c ifaamas 
that it has subgame perfect equilibria provided that agents 
behave rationally in the sense of von neumann and 
morgenstern the approach proposed in this paper has been first 
introduced and presented in in this paper we present 
our first experiments with some noteworthy results and a 
more complex multi-agent system with representatives to 
enable us to have a more robust system 
in section we present our application a crisis 
management problem section deals with the general aspect of the 
proposed approach the preference modelling is described 
in sect whereas the motivations of our protocol are 
considered in sect and the agent multiagent modelling in 
sect section presents the formal modelling and 
properties of our protocol before presenting our first experiments 
in sect finally in section we conclude and present 
the future work 
 case study 
this protocol is applied to a crisis management problem 
crisis management is a relatively new field of management 
and is composed of three types of activities crisis 
prevention operational preparedness and management of declared 
crisis the crisis prevention aims to bring the risk of crisis 
to an acceptable level and when possible avoid that the 
crisis actually happens the operational preparedness includes 
strategic advanced planning training and simulation to 
ensure availability rapid mobilisation and deployment of 
resources to deal with possible emergencies the management 
of declared crisis is the response to - including the 
evacuation search and rescue - and the recovery from the crisis by 
minimising the effects of the crises limiting the impact on 
the community and environment and on a longer term by 
bringing the community s systems back to normal in this 
paper we focus on the response part of the management of 
declared crisis activity and particularly on the evacuation 
of the injured people in disaster situations when a crisis 
is declared the plans defined during the operational 
preparedness activity are executed for disasters master plans 
are executed these plans are elaborated by the authorities 
with the collaboration of civil protection agencies police 
health services non-governmental organizations etc 
when a victim is found several actions follow first a 
rescue party is assigned to the victim who is examined and is 
given first aid on the spot then the victims can be placed 
in an emergency centre on the ground called the medical 
advanced post for all victims a sorter physician - 
generally a hospital physician - examines the seriousness of their 
injuries and classifies the victims by pathology the 
evacuation by emergency health transport if necessary can take 
place after these clinical examinations and classifications 
nowadays to evacuate the injured people the physicians 
contact the emergency call centre to pass on the medical 
assessments of the most urgent cases the emergency call 
centre then searches for available and appropriate spaces in 
the hospitals to care for these victims the physicians are 
informed of the allocations so they can proceed to the 
evacuations choosing the emergency health transports according 
to the pathologies and the transport modes provided in 
this context we can observe that the evacuation is based 
on three important elements the examination and 
classification of the victims the search for an allocation and the 
transport in the case of the march madrid attacks 
for instance some injured people did not receive the 
appropriate health care because during the search for space the 
emergency call centre did not consider the transport 
constraints and in particular the traffic therefore for a large 
scale crisis management problem there is a need to support 
the emergency call centre and the physicians in the 
dispatching to take into account the hospitals and the transport 
constraints and availabilities 
 proposed approach 
to accept a proposal an agent has to consider several 
issues such as in the case of the crisis management problem 
the availabilities in terms of number of beds by unit 
medical and surgical staffs theatres and so on therefore each 
agent has its own preferences in correlation with its resource 
constraints and other decision criteria such as for the case 
study the level of congestion of a hospital all the agents 
also make decisions by taking into account the dependencies 
between these decision criteria 
the first hypothesis of our approach is that there are 
several parties involved in and impacted by the decision and 
so they have to decide together according to their own 
constraints and decision criteria negotiation is the process by 
which a group facing a conflict communicates with one 
another to try and come to a mutually acceptable agreement 
or decision and so the agents have to negotiate the 
conflict we have to resolve is finding an acceptable solution for 
all the parties by using a particular protocol in our 
context multilateral negotiation is a negotiation protocol type 
that is the best suited for this type of problem this type 
of protocol enables the hospitals and the physicians to 
negotiate together the negotiation also deals with multiple 
issues moreover an other hypothesis is that we are in a 
cooperative context where all the parties have a common 
objective which is to provide the best possible solution for 
everyone this implies the use of a negotiation protocol 
encouraging the parties involved to cooperate as satisfying its 
preferences 
taking into account these aspects a multi-agent system 
 mas seems to be a reliable method in the case of a 
distributed decision making process indeed a mas is a suitable 
answer when the solution has to combine at least 
distribution features and reasoning capabilities another motivation 
for using mas lies in the fact that mas is well known for 
facilitating automated negotiation at the operative decision 
making level in various applications 
therefore our approach consists of solving a multiparty 
decision problem using a mas with 
 the preferences of the agents are modelled using a 
multi-criteria decision aid tool myriad also 
enabling us to consider multi-issue problems by evaluating 
proposals on several criteria 
 a cooperation-based multilateral and multi-issue 
negotiation protocol 
 the preference model 
we consider a problem where an agent has several decision 
criteria a set nk { nk} of criteria for each agent k 
involved in the negotiation protocol these decision criteria 
enable the agents to evaluate the set of issues that are 
negotiated the issues correspond directly or not to the decision 
criteria however for the example of the crisis management 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
problem the issues are the set of victims to dispatch 
between the hospitals these issues are translated to decision 
criteria enabling the hospital to evaluate its congestion and 
so to an updated number of available beds medical teams 
and so on in order to take into account the complexity that 
exists between the criteria issues we use a multi-criteria 
decision aiding mcda tool named myriad developed 
at thales for mcda applications based on a two-additive 
choquet integral which is a good compromise between 
versatility and ease to understand and model the interactions 
between decision criteria 
the set of the attributes of nk is denoted by xk 
 xk 
nk 
 
all the attributes are made commensurate thanks to the 
introduction of partial utility functions uk 
i xk 
i → the 
 scale depicts the satisfaction of the agent k regarding 
the values of the attributes an option x is identified to an 
element of xk 
 xk 
 × · · · × xk 
nk 
 with x x xnk 
then the overall assessment of x is given by 
uk x hk uk 
 x uh 
nk 
 xnk 
where hk nk → is the aggregation function the 
overall preference relation over xk 
is then 
x y ⇐⇒ uk x ≥ uk y 
the two-additive choquet integral is defined for 
 z znk ∈ nk by 
hk z znk 
x 
i∈nk 
 
 vk 
i − 
 
 
x 
j i 
 ik 
i j 
 
a zi 
 
x 
ik 
i j 
ik 
i j zi ∧ zj 
x 
ii j 
 ii j zi ∨ zj 
where vk 
i is the relative importance of criterion i for agent 
k and ik 
i j is the interaction between criteria i and j ∧ and 
∨ denote the min and max functions respectively assume 
that zi zj a positive interaction between criteria i and 
j depicts complementarity between these criteria positive 
synergy hence the lower score of z on criterion i 
conceals the positive effect of the better score on criterion j to 
a larger extent on the overall evaluation than the impact of 
the relative importance of the criteria taken independently 
of the other ones in other words the score of z on criterion 
j is penalized by the lower score on criterion i conversely a 
negative interaction between criteria i and j depicts 
substitutability between these criteria negative synergy the 
score of z on criterion i is then saved by a better score on 
criterion j 
in myriad we can also obtain some recommendations 
corresponding to an indicator ωc h x measuring the worth 
to improve option x w r t hk on some criteria c ⊆ nk as 
follows 
ωc hk x 
z 
 
hk 
` 
 − τ xc τ xnk\c 
´ 
− hk x 
ec τ x 
dτ 
where −τ xc τ xnk\c is the compound act that equals 
 − τ xi τ if i ∈ c and equals xi if i ∈ nk \ c moreover 
ec τ x is the effort to go from the profile x to the profile 
 − τ xc τ xnk\c function ωc hk x depicts the 
average improvement of hk when the criteria of coalition a 
range from xc to c divided by the average effort needed 
for this improvement we generally assume that ec is of 
order that is ec τ x τ 
p 
i∈c − xi the expression 
of ωc hk x when hk is a choquet integral is given in 
the agent is then recommended to improve of coalition c 
for which ωc hk x is maximum this recommendation is 
very useful in a negotiation protocol since it helps the agents 
to know what to do if they want an offer to be accepted while 
not revealing their own preference model 
 protocol motivations 
for multi-issue problems there are two approaches a 
complete package approach where the issues are 
negotiated simultaneously in opposition to the sequential approach 
where the issues are negotiated one by one when the issues 
are dependant then it is the best choice to bargain 
simultaneously over all issues thus the complete package is 
the adopted approach so that an offer will be on the 
overall set of injured people while taking into account the other 
decision criteria 
we have to consider that all the parties of the 
negotiation process have to agree on the decision since they are all 
involved in and impacted by this decision and so an 
unanimous agreement is required in the protocol in addition no 
party can leave the process until an agreement is reached 
i e a consensus achieved this makes sense since a proposal 
concerns all the parties moreover we have to guarantee the 
availability of the resources needed by the parties to ensure 
that a proposal is realistic to this end the information 
about these availabilities are used to determine admissible 
proposals such that an offer cannot be made if one of the 
parties has not enough resources to execute achieve it at the 
beginning of the negotiation each party provides its 
maximum availabilities this defining the constraints that have 
to be satisfied for each offer submitted 
the negotiation has also to converge quickly on an 
unanimous agreement we decided to introduce in the negotiation 
protocol an incentive to cooperate taking into account the 
passed negotiation time this incentive is defined on the 
basis of a time dependent penalty the discounting factor as 
in or a time-dependent threshold this penalty has to 
be used in the accept reject stage of our consensus 
procedure in fact in the case of a discounting factor each party 
will accept or reject an offer by evaluating the proposal 
using its utility function deducted from the discounting factor 
in the case of a time-dependent threshold if the evaluation 
is greater or equal to this threshold the offer is accepted 
otherwise in the next period its threshold is reduced 
the use of a penalty is not enough alone since it does 
not help in finding a solution some information about the 
assessments of the parties involved in the negotiation are 
needed in particular it would be helpful to know why an 
offer has been rejected and or what can be done to make 
a proposal that would be accepted myriad provides an 
analysis that determines the flaws an option here a 
proposal in particular it gives this type of information which 
criteria of a proposal should be improved so as to reach the 
highest possible overall evaluation as we use this tool 
to model the parties involved in the negotiation the 
information about the criteria to improve can be used by the 
mediator to elaborate the proposals we also consider that 
the dual function can be used to take into account another 
type of information on which criteria of a proposal no 
improvement is necessary so that the overall evaluation of a 
proposal is still acceptable do not decrease thus all 
information is a constraint to be satisfied as much as possible by 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure an illustration of some system 
the parties to make a new proposal 
we are in a cooperative context and revealing one s 
opinion on what can be improved is not prohibited on the 
contrary it is useful and recommended here seeing that it helps 
in converging on an agreement therefore when one of the 
parties refuses an offer some information will be 
communicated in order to facilitate and speed up the negotiation 
we introduce a mediator this specific entity is in charge 
of making the proposals to the other parties in the system 
by taking into account their public constraints e g their 
availabilities and the recommendations they make this 
mediator can also be considered as the representative of 
the general interest we can have in some applications such 
as in the crisis management problem the physician will be 
the mediator and will also have some more information to 
consider when making an offer e g traffic state transport 
mode and time each party in a negotiation n a 
negotiator can also be a mediator of another negotiation n this 
party becoming the representative of n in the negotiation 
n as illustrated by fig what can also help in reducing 
the communication time 
 agentification 
how the problem is transposed in a mas problem is a 
very important aspect when designing such a system the 
agentification has an influence upon the systems efficiency in 
solving the problem therefore in this section we describe 
the elements and constraints taken into account during the 
modelling phase and for the model itself however for this 
negotiation application the modelling is quite natural when 
one observes the negotiation protocol motivations and main 
properties 
first of all it seems obvious that there should be one agent 
for each player of our multilateral multi-issue negotiation 
protocol the agents have the involved parties information 
and preferences these agents are 
 autonomous they decide for themselves what when 
and under what conditions actions should be 
performed 
 rational they have a means-ends competence to fit 
its decisions according to its knowledge preferences 
and goal 
 self-interested they have their own interests which 
may conflict with the interests of other agents 
moreover their preferences are modelled and a proposal 
evaluated and analysed using myriad each agent has 
private information and can access public information as 
knowledge 
in fact there are two types of agents the mediator type 
for the agents corresponding to the mediator of our 
negotiation protocol the delegated physician in our application 
and the negotiator type for the agents corresponding to 
the other parties the hospitals the main behaviours that 
an agent of type mediator needs to negotiate in our protocol 
are the following 
 convert improvements converts the information 
given by the other agents involved in the negotiation 
about the improvements to be done into constraints 
on the next proposal to be made 
 convert no decrease converts the information given 
by the other agents involved in the negotiation about 
the points that should not be changed into constraints 
on the next proposal to be made 
 construct proposal constructs a new proposal 
according to the constraints obtained with 
convert improvements convert no decrease and the agent 
preferences 
the main behaviours that an agent of type negotiator 
needs to negotiate in our protocol are the following 
 convert proposal converts a proposal to a myriad 
option of the agent according to its preferences model 
and its private data 
 convert improvements wc converts the agent 
recommendations for the improvements of a myriad 
option into general information on the proposal 
 convert no decrease wc converts the agent 
recommendations about the criteria that should not be 
changed in the myriad option into general information 
on the proposal 
in addition to these behaviours there are for the two types 
of agents access behaviours to myriad functionalities 
such as the evaluation and improvement functions 
 evaluate option evaluates the myriad option 
obtained using the agent behaviour convert proposal 
 improvements gets the agent recommendations to 
improve a proposal from the myriad option 
 no decrease gets the agent recommendations to not 
change some criteria from the myriad option 
of course before running the system with such agents we 
must have defined each party preferences model in 
myriad this model has to be part of the agent so that it could 
be used to make the assessments and to retrieve the 
improvements in addition to these behaviours the communication 
acts between the agents is as follows 
 mediator agent communication acts 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
m m 
 
 
m 
inform m 
mediator negotiator 
accept−proposal 
l 
 
accept−proposal 
m−l 
reject−proposal 
propose 
propose 
figure the protocol diagram in auml and 
where m is the number of negotiator agents and l 
is the number of agents refusing current proposal 
 a propose sends a message containing a proposal 
to all negotiator agents 
 b inform sends a message to all negotiator agents 
to inform them that an agreement has been 
reached and containing the consensus outcome 
 negotiator agent communication acts 
 a accept-proposal sends a message to the 
mediator agent containing the agent recommendations 
to improve the proposal and obtained with 
convert improvements wc 
 b reject-proposal sends a message to the 
mediator agent containing the agent recommendations 
about the criteria that should not be changed and 
obtained with convert no decrease wc 
such agents are interchangeable in a case of failure since 
they all have the same properties and represent a user with 
his preference model not depending on the agent but on the 
model defined in myriad when the issues and the 
decision criteria are different from each other the information 
about the criteria improvement have to be pre-processed to 
give some instructions on the directions to take and about 
the negotiated issues it is the same for the evaluation of a 
proposal each agent has to convert the information about 
the issues to update its private information and to obtain 
the values of each attribute of the decision criteria 
 our protocol 
formally we consider negotiations where a set of 
players a { m} and a player a are negotiating over 
a set q of size q the player a is the protocol 
mediator the mediator agent of the agentification the 
utility preference function of a player k ∈ a ∪ {a} is uk 
defined using myriad as presented in section with a set 
nk of criteria xk 
an option and so on an offer is a 
vector p p p · · · pm a partition of q in which pk is 
player k s share of q we have p ∈ p where p is the set of 
admissible proposals a finite set note that p is determined 
using all players general constraints on the proposals and q 
moreover let ˜p denote a particular proposal defined as a s 
preferred proposal 
we also have the following notation δk is the 
threshold decrease factor of player k φk pk → xk 
is player 
k s function to convert a proposal to an option and ψk is 
the function indicating which points p has to be improved 
with ψk its dual function - on which points no 
improvement is necessary ψk is obtained using the dual function of 
ωc hk x 
eωc hk x 
z 
 
hk x − hk 
` 
τ xc xnk\c 
´ 
eec τ x 
dτ 
where eec τ x is the cost effort to go from τxc xnk\c 
to x 
in period t of our consensus procedure player a proposes 
an agreement p all players k ∈ a respond to a by 
accepting or rejecting p the responses are made simultaneously 
if all players k ∈ a accept the offer the game ends if any 
player k rejects p then the next period t begins player a 
makes another proposal p by taking into account 
information provided by the players and the ones that have rejected 
p apply a penalty therefore our negotiation protocol can 
be as follows 
protocol p 
 at the beginning we set period t 
 a makes a proposal p ∈ p that has not been 
proposed before 
 wait that all players of a give their opinion 
yes or no to the player a if all players 
agree on p this later is chosen otherwise 
t is incremented and we go back to previous 
point 
 if there is no more offer left from p the 
default offer ˜p will be chosen 
 the utility of players regarding a given 
offer decreases over time more precisely the 
utility of player k ∈ a at period t regarding 
offer p is uk φk pk t ft uk φk pk 
where one can take for instance ft x 
x δk t 
or ft x x − δk t as penalty 
function 
lemma protocol p has at least one subgame perfect 
equilibrium 
 
proof protocol p is first transformed in a game in 
extensive form to this end one shall specify the order in which 
the responders a react to the offer p of a however the 
order in which the players answer has no influence on the 
course of the game and in particular on their personal 
utility hence protocol p is strictly equivalent to a game in 
 
a subgame perfect equilibrium is an equilibrium such that 
players strategies constitute a nash equilibrium in every 
subgame of the original game a nash equilibrium 
is a set of strategies one for each player such that no player 
has incentive to unilaterally change his her action 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
extensive form considering any order of the players a this 
game is clearly finite since p is finite and each offer can 
only be proposed once finally p corresponds to a game 
with perfect information we end the proof by using a 
classical result stating that any finite game in extensive form 
with perfect information has at least one subgame perfect 
equilibrium see e g 
rational players in the sense of von neumann and 
morgenstern involved in protocol p will necessarily come up 
with a subgame perfect equilibrium 
example consider an example with a { } and 
p {p 
 p 
 p 
} where the default offer is p 
 assume 
that ft x x − t consider the following table giving 
the utilities at t 
p 
p 
p 
a 
 
 
it is easy to see that there is one single subgame perfect 
equilibrium for protocol p corresponding to these values this 
equilibrium consists of the following choices first a proposes 
p 
 player rejects this offer a proposes then p 
and both 
players and accepts otherwise they are threatened to 
receive the worse offer p 
for them finally offer p 
is 
chosen option p 
is the best one for a but the two other players 
vetoed it it is interesting to point out that even though a 
prefers p 
to p 
 offer p 
is first proposed and this make 
p 
being accepted if a proposes p 
first then the subgame 
perfect equilibrium in this situation is p 
 to sum up the 
worse preferred options have to be proposed first in order to 
get finally the best one but this entails a waste of time 
analysing the previous example one sees that the game 
outcome at the equilibrium is p 
that is not very attractive 
for player option p 
seems more balanced since no player 
judges it badly it could be seen as a better solution as a 
consensus among the agents 
in order to introduce this notion of balanceness in the 
protocol we introduce a condition under which a player will 
be obliged to accept the proposal reducing the autonomy 
of the agents but for increasing rationality and cooperation 
more precisely if the utility of a player is larger than a given 
threshold then acceptance is required the threshold 
decreases over time so that players have to make more and 
more concession therefore the protocol becomes as 
follows 
protocol p 
 at the beginning we set period t 
 a makes a proposal p ∈ p that has not been 
proposed before 
 wait that all players of a give their opinion 
yes or no to the player a a player k must 
accept the offer if uk φk pk ≥ ρk t where 
ρk t tends to zero when t grows 
moreover there exists t such that for all t ≥ t 
ρk t if all players agree on p this 
later is chosen otherwise t is incremented 
and we go back to previous point 
 if there is no more offer left from p the 
default offer ˜p will be chosen 
one can show exactly as in lemma that protocol p 
has at least one subgame perfect equilibrium we expect 
that protocol p provides a solution not to far from p 
so it favours fairness among the players therefore our 
cooperation-based multilateral multi-issue protocol is the 
following 
protocol p 
 at the beginning we set period t 
 a makes a proposal p ∈ p that has not 
been proposed before considering ψk pt 
 
and ψk pt 
 for all players k ∈ a 
 wait that all players of a give their opinion 
 yes ψk pt 
 or no ψk pt 
 to the 
player a a player k must accept the offer 
if uk φk pk ≥ ρk t where ρk t tends to 
zero when t grows moreover there exists 
t such that for all t ≥ t ρk t if 
all players agree on p this later is chosen 
otherwise t is incremented and we go back 
to previous point 
 if there is no more offer left from p the 
default offer ˜p will be chosen 
 experiments 
we developed a mas using the widely used jade agent 
platform this mas is designed to be as general as 
possible e g a general framework to specialise according to the 
application and enable us to make some preliminary 
experiments the experiments aim at verifying that our approach 
gives solutions as close as possible to the maximin solution 
and in a small number of rounds and hopefully in a short 
time since our context is highly cooperative we defined 
the two types of agents and their behaviours as introduced 
in section the agents and their behaviours correspond 
to the main classes of our prototype negotiatoragent 
and negotiatorbehaviour for the negotiator agents and 
mediatoragent and mediatorbehaviour for the mediator 
agent these classes extend jade classes and integrate 
myriad into the agents reducing the amount of 
communications in the system some functionalities depending on 
the application have to be implemented according to the 
application by extending these classes in particular all 
conversion parts of the agents have to be specified according to 
the application since to convert a proposal into decision 
criteria we need to know first this model and the correlations 
between the proposals and this model 
first to illustrate our protocol we present a simple 
example of our dispatch problem in this example we have 
three hospitals h h and h each hospital can receive 
victims having a particular pathology in such a way that 
h can receive patients with the pathology burn surgery 
or orthopedic h can receive patients with the pathology 
surgery orthopedic or cardiology and h can receive 
patients with the pathology burn or cardiology all the 
hospitals have similar decision criteria reflecting their preferences 
on the level of congestion they can face for the overall 
hospital and the different services available as briefly explained 
for hospital h hereafter 
for hospital h the preference model fig is composed 
of five criteria these criteria correspond to the preferences 
on the pathologies the hospital can treat in the case of 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure the h preference model in myriad 
the pathology burn the corresponding criterion also named 
burn as shown in fig represents the preferences of h 
according to the value of cburn which is the current capacity 
of burn therefore the utility function of this criterion 
represents a preference such that the more there are patients of 
this pathology in the hospital the less the hospital may 
satisfy them and this with an initial capacity in addition to 
reflecting this kind of viewpoint the aggregation function as 
defined in myriad introduces a veto on the criteria burn 
surgery orthopedic and ereceipt where ereceipt is the 
criterion for the preferences about the capacity to receive a 
number of patients at the same time 
in this simplified example the physician have no 
particular preferences on the dispatch and the mediator agent 
chooses a proposal randomly in a subset of the set of 
admissibility this subset have to satisfy as much as possible the 
recommendations made by the hospitals to solve this 
problem for this example we decided to solve a linear problem 
with the availability constraints and the recommendations 
as linear constraints on the dispatch values the set of 
admissibility is then obtained by solving this linear problem 
by the use of prolog moreover only the recommendations 
on how to improve a proposal are taken into account the 
problem to solve is then to dispatch to hospital h h and 
h the set of victims composed of victims with the 
pathology burn with surgery with orthopedic and with 
cardiology the availabilities of the hospitals are as 
presented in the following table 
available overall burn surg orthop cardio 
h 
 h - 
h - - 
we obtain a multiagent system with the mediator agent 
and three agents of type negotiator for the three hospital 
in the problem the hospitals threshold are fixed 
approximatively to the level where an evaluation is considered as 
good to start the negotiator agents send their 
availabilities the mediator agent makes a proposal chosen randomly 
in admissible set obtained with these availabilities as 
linear constraints this proposal is the vector p h burn 
 h surgery h orthopaedic h surgery 
 h orthopaedic h cardiology h burn h 
cardiology and the mediator sends propose p to h 
h and h for approval each negotiator agent evaluates 
this proposal and answers back by accepting or rejecting p 
 agent h rejects this offer since its evaluation is very 
far from the threshold a bad score and gives 
a recommendation to improve burn and surgery by 
sending the message 
reject proposal burn surgery 
 agent h accepts this offer by sending the message 
accept proposal the proposal evaluation being 
good 
 agent h accepts p by sending the message accept 
proposal the proposal evaluation being good 
just with the recommendations provided by agent h the 
mediator is able to make a new proposal by restricting the 
value of burn and surgery the new proposal obtained is 
then p h burn h surgery h orthopaedic 
 h surgery h orthopaedic h cardiology 
 h burn h cardiology the mediator sends 
propose p the negotiator agents h h and h answer 
back by sending the message accept proposal p being 
evaluated with a high enough score to be acceptable and 
also considered as a good proposal when using the 
explanation function of myriad an agreement is reached with 
p note that the evaluation of p by h has decreased in 
comparison with p but not enough to be rejected and that 
this solution is the pareto one p∗ 
 
other examples have been tested with the same settings 
issues in in three negotiator agents and the same mediator 
agent with no preference model but selecting randomly the 
proposal we obtained solutions either equal or close to the 
maximin solution the distance from the standard deviation 
being less than the evaluations not far from the ones 
obtained with p∗ 
and with less than seven proposals made 
this shows us that we are able to solve this multi-issue 
multilateral negotiation problem in a simple and efficient way 
with solutions close to the pareto solution 
 conclusion and future work 
this paper presents a new protocol to address 
multilateral multi-issue negotiation in a cooperative context the 
first main contribution is that we take into account complex 
inter-dependencies between multiple issues with the use of 
a complex preference modelling this contribution is 
reinforced by the use of multi-issue negotiation in a multilateral 
context our second contribution is the use of sharp 
recommendations in the protocol to help in accelerating the search 
of a consensus between the cooperative agents and in finding 
an optimal solution we have also shown that the protocol 
has subgame perfect equilibria and these equilibria converge 
to the usual maximum solution moreover we tested this 
protocol in a crisis management context where the 
negotiation aim is where to evacuate a whole set of injured people 
to predefined hospitals 
we have already developed a first mas in particular 
integrating myriad to test this protocol in order to know 
more about its efficiency in terms of solution quality and 
quickness in finding a consensus this prototype enabled 
us to solve some examples with our approach and the 
results we obtained are encouraging since we obtained quickly 
good agreements close to the pareto solution in the light 
of the initial constraints of the problem the availabilities 
we still have to improve our mas by taking into account 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the two types of recommendations and by adding a 
preference model to the mediator of our system moreover a 
comparative study has to be done in order to evaluate the 
performance of our framework against the existing ones and 
against some variations on the protocol 
 acknowledgement 
this work is partly funded by the icis research project 
under the dutch bsik program bsik 
 references 
 jade http jade tilab com 
 p faratin c sierra and n r jennings using 
similarity criteria to make issue trade-offs in 
automated negotiations artificial intelligence 
 - 
 s s fatima m wooldridge and n r jennings 
optimal negotiation of multiple issues in incomplete 
information settings in rd international joint 
conference on autonomous agents and multiagent 
systems aamas pages - new york 
usa 
 s s fatima m wooldridge and n r jennings a 
comparative study of game theoretic and evolutionary 
models of bargaining for software agents artificial 
intelligence review - 
 s s fatima m wooldridge and n r jennings on 
efficient procedures for multi-issue negotiation in th 
international workshop on agent-mediated electronic 
commerce amec pages - hakodate japan 
 
 m grabisch the application of fuzzy integrals in 
multicriteria decision making european j of 
operational research - 
 m grabisch t murofushi and m sugeno fuzzy 
measures and integrals theory and applications 
 edited volume studies in fuzziness physica verlag 
 
 m hemaissia a el fallah-seghrouchni 
c labreuche and j mattioli cooperation-based 
multilateral multi-issue negotiation for crisis 
management in th international workshop on 
rational robust and secure negotiation rrs 
pages - hakodate japan may 
 t ito m klein and h hattori a negotiation 
protocol for agents with nonlinear utility functions in 
aaai 
 m klein p faratin h sayama and y bar-yam 
negotiating complex contracts group decision and 
negotiation - march 
 c labreuche determination of the criteria to be 
improved first in order to improve as much as possible 
the overall evaluation in ipmu pages - 
perugia italy 
 c labreuche and f le hu´ed´e myriad a tool suite 
for mcda in eusflat pages - 
barcelona spain 
 r y k lau towards genetically optimised 
multi-agent multi-issue negotiations in proceedings of 
the th annual hawaii international conference on 
system sciences hicss big island hawaii 
 r j lin bilateral multi-issue contract negotiation for 
task redistribution using a mediation service in agent 
mediated electronic commerce vi amec new 
york usa 
 j f nash non cooperative games annals of 
mathematics - 
 g owen game theory academic press new york 
 
 v robu d j a somefun and j a l poutr´e 
modeling complex multi-issue negotiations using 
utility graphs in th international joint conference 
on autonomous agents and multiagent systems 
 aamas pages - 
 a rubinstein perfect equilibrium in a bargaining 
model econometrica - jan 
 l -k soh and x li adaptive confidence-based 
multiagent negotiation strategy in rd international 
joint conference on autonomous agents and 
multiagent systems aamas pages - 
los alamitos ca usa 
 h -w tung and r j lin automated contract 
negotiation using a mediation service in th ieee 
international conference on e-commerce technology 
 cec pages - munich germany 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
on opportunistic techniques for solving decentralized 
markov decision processes with temporal constraints 
janusz marecki and milind tambe 
computer science department 
university of southern california 
 w th place los angeles ca 
{marecki tambe} usc edu 
abstract 
decentralized markov decision processes dec-mdps are a 
popular model of agent-coordination problems in domains with 
uncertainty and time constraints but very difficult to solve in this 
paper we improve a state-of-the-art heuristic solution method for 
dec-mdps called oc-dec-mdp that has recently been shown 
to scale up to larger dec-mdps our heuristic solution method 
called value function propagation vfp combines two 
orthogonal improvements of oc-dec-mdp first it speeds up 
oc-decmdp by an order of magnitude by maintaining and manipulating 
a value function for each state as a function of time rather than a 
separate value for each pair of sate and time interval furthermore 
it achieves better solution qualities than oc-dec-mdp because 
as our analytical results show it does not overestimate the expected 
total reward like oc-dec- mdp we test both improvements 
independently in a crisis-management domain as well as for other 
types of domains our experimental results demonstrate a 
significant speedup of vfp over oc-dec-mdp as well as higher solution 
qualities in a variety of situations 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligencemulti-agent systems 
general terms 
algorithms theory 
 introduction 
the development of algorithms for effective coordination of 
multiple agents acting as a team in uncertain and time critical domains 
has recently become a very active research field with potential 
applications ranging from coordination of agents during a hostage 
rescue mission to the coordination of autonomous mars 
exploration rovers because of the uncertain and dynamic 
characteristics of such domains decision-theoretic models have received 
a lot of attention in recent years mainly thanks to their 
expressiveness and the ability to reason about the utility of actions over 
time 
key decision-theoretic models that have become popular in the 
literature include decentralized markov decision processes 
 decmdps and decentralized partially observable markov decision 
processes dec-pomdps unfortunately solving these models 
optimally has been proven to be nexp-complete hence more 
tractable subclasses of these models have been the subject of 
intensive research in particular network distributed pomdp 
which assume that not all the agents interact with each other 
transition independent dec-mdp which assume that transition 
function is decomposable into local transition functions or dec-mdp 
with event driven interactions which assume that interactions 
between agents happen at fixed time points constitute good 
examples of such subclasses although globally optimal algorithms for 
these subclasses have demonstrated promising results domains on 
which these algorithms run are still small and time horizons are 
limited to only a few time ticks 
to remedy that locally optimal algorithms have been proposed 
 in particular opportunity cost dec-mdp 
referred to as oc-dec-mdp is particularly notable as it has been 
shown to scale up to domains with hundreds of tasks and double 
digit time horizons additionally oc-dec-mdp is unique in its 
ability to address both temporal constraints and uncertain method 
execution durations which is an important factor for real-world 
domains oc-dec-mdp is able to scale up to such domains mainly 
because instead of searching for the globally optimal solution it 
carries out a series of policy iterations in each iteration it performs 
a value iteration that reuses the data computed during the previous 
policy iteration however oc-dec-mdp is still slow especially 
as the time horizon and the number of methods approach large 
values the reason for high runtimes of oc-dec-mdp for such 
domains is a consequence of its huge state space i e oc-dec-mdp 
introduces a separate state for each possible pair of method and 
method execution interval furthermore oc-dec-mdp 
overestimates the reward that a method expects to receive for enabling 
the execution of future methods this reward also referred to as 
the opportunity cost plays a crucial role in agent decision making 
and as we show later its overestimation leads to highly suboptimal 
policies 
in this context we present vfp value function p ropagation 
an efficient solution technique for the dec-mdp model with 
temporal constraints and uncertain method execution durations that 
builds on the success of oc-dec-mdp vfp introduces our two 
orthogonal ideas first similarly to and we maintain 
 
 - - - - rps c ifaamas 
and manipulate a value function over time for each method rather 
than a separate value for each pair of method and time interval 
such representation allows us to group the time points for which 
the value function changes at the same rate its slope is 
constant which results in fast functional propagation of value 
functions second we prove both theoretically and empirically that 
oc-dec- mdp overestimates the opportunity cost and to remedy 
that we introduce a set of heuristics that correct the opportunity 
cost overestimation problem 
this paper is organized as follows in section we motivate this 
research by introducing a civilian rescue domain where a team of 
fire- brigades must coordinate in order to rescue civilians trapped in 
a burning building in section we provide a detailed description of 
our dec-mdp model with temporal constraints and in section 
we discuss how one could solve the problems encoded in our model 
using globally optimal and locally optimal solvers sections and 
 discuss the two orthogonal improvements to the state-of-the-art 
oc-dec-mdp algorithm that our vfp algorithm implements 
finally in section we demonstrate empirically the impact of our two 
orthogonal improvements i e we show that i the new 
heuristics correct the opportunity cost overestimation problem leading to 
higher quality policies and ii by allowing for a systematic 
tradeoff of solution quality for time the vfp algorithm runs much faster 
than the oc-dec-mdp algorithm 
 motivating example 
we are interested in domains where multiple agents must 
coordinate their plans over time despite uncertainty in plan execution 
duration and outcome one example domain is large-scale disaster 
like a fire in a skyscraper because there can be hundreds of 
civilians scattered across numerous floors multiple rescue teams have 
to be dispatched and radio communication channels can quickly 
get saturated and useless in particular small teams of fire-brigades 
must be sent on separate missions to rescue the civilians trapped in 
dozens of different locations 
picture a small mission plan from figure where three 
firebrigades have been assigned a task to rescue the civilians trapped 
at site b accessed from site a e g an office accessed from the 
floor 
 general fire fighting procedures involve both i putting 
out the flames and ii ventilating the site to let the toxic high 
temperature gases escape with the restriction that ventilation should 
not be performed too fast in order to prevent the fire from spreading 
the team estimates that the civilians have minutes before the fire 
at site b becomes unbearable and that the fire at site a has to be 
put out in order to open the access to site b as has happened in 
the past in large scale disasters communication often breaks down 
and hence we assume in this domain that there is no 
communication between the fire-brigades and denoted as fb fb and 
fb consequently fb does not know if it is already safe to 
ventilate site a fb does not know if it is already safe to enter site a 
and start fighting fire at site b etc we assign the reward for 
evacuating the civilians from site b and a smaller reward for 
the successful ventilation of site a since the civilians themselves 
might succeed in breaking out from site b 
one can clearly see the dilemma that fb faces it can only 
estimate the durations of the fight fire at site a methods to be 
executed by fb and fb and at the same time fb knows that time 
is running out for civilians if fb ventilates site a too early the 
fire will spread out of control whereas if fb waits with the 
ventilation method for too long fire at site b will become unbearable for 
the civilians in general agents have to perform a sequence of such 
 
we explain the est and let notation in section 
figure civilian rescue domain and a mission plan dotted 
arrows represent implicit precedence constraints within an agent 
difficult decisions in particular decision process of fb involves 
first choosing when to start ventilating site a and then 
 depending on the time it took to ventilate site a choosing when to start 
evacuating the civilians from site b such sequence of decisions 
constitutes the policy of an agent and it must be found fast because 
time is running out 
 model description 
we encode our decision problems in a model which we refer to as 
decentralized mdp with temporal constraints 
 each instance of 
our decision problems can be described as a tuple m a c p r 
where m {mi} 
 m 
i is the set of methods and a {ak} 
 a 
k 
is the set of agents agents cannot communicate during mission 
execution each agent ak is assigned to a set mk of methods 
such that 
s a 
k mk m and ∀i j i jmi ∩ mj ø also each 
method of agent ak can be executed only once and agent ak can 
execute only one method at a time method execution times are 
uncertain and p {pi} 
 m 
i is the set of distributions of method 
execution durations in particular pi t is the probability that the 
execution of method mi consumes time t c is a set of 
temporal constraints in the system methods are partially ordered and 
each method has fixed time windows inside which it can be 
executed i e c c≺ ∪ c where c≺ is the set of predecessor 
constraints and c is the set of time window constraints for 
c ∈ c≺ c mi mj means that method mi precedes method 
mj i e execution of mj cannot start before mi terminates in 
particular for an agent ak all its methods form a chain linked by 
predecessor constraints we assume that the graph g m c≺ 
is acyclic does not have disconnected nodes the problem cannot 
be decomposed into independent subproblems and its source and 
sink vertices identify the source and sink methods of the system 
for c ∈ c c mi est let means that execution of mi 
can only start after the earliest starting time est and must 
finish before the latest end time let we allow methods to have 
multiple disjoint time window constraints although distributions 
pi can extend to infinite time horizons given the time window 
constraints the planning horizon δ max m τ τ ∈c τ is 
considered as the mission deadline finally r {ri} 
 m 
i is the set of 
non-negative rewards i e ri is obtained upon successful 
execution of mi 
since there is no communication allowed an agent can only 
estimate the probabilities that its methods have already been enabled 
 
one could also use the oc-dec-mdp framework which models 
both time and resource constraints 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
by other agents consequently if mj ∈ mk is the next method 
to be executed by the agent ak and the current time is t ∈ δ 
the agent has to make a decision whether to execute the method 
mj denoted as e or to wait denoted as w in case agent ak 
decides to wait it remains idle for an arbitrary small time and 
resumes operation at the same place about to execute method mj 
at time t in case agent ak decides to execute the next method 
two outcomes are possible 
success the agent ak receives reward rj and moves on to its 
next method if such method exists so long as the following 
conditions hold i all the methods {mi mi mj ∈ c≺} that 
directly enable method mj have already been completed ii 
execution of method mj started in some time window of method mj i e 
∃ mj τ τ ∈c 
such that t ∈ τ τ and iii execution of method 
mj finished inside the same time window i e agent ak completed 
method mj in time less than or equal to τ − t 
failure if any of the above-mentioned conditions does not hold 
agent ak stops its execution other agents may continue their 
execution but methods mk ∈ {m mj m ∈ c≺} will never become 
enabled 
the policy πk of an agent ak is a function πk mk × δ → 
{w e} and πk m t a means that if ak is at method m 
at time t it will choose to perform the action a a joint policy 
π πk 
 a 
k is considered to be optimal denoted as π∗ 
 if it 
maximizes the sum of expected rewards for all the agents 
 solution techniques 
 optimal algorithms 
optimal joint policy π∗ 
is usually found by using the bellman 
update principle i e in order to determine the optimal policy for 
method mj optimal policies for methods mk ∈ {m mj m ∈ 
c≺} are used unfortunately for our model the optimal 
policy for method mj also depends on policies for methods mi ∈ 
{m m mj ∈ c≺} this double dependency results from the 
fact that the expected reward for starting the execution of method 
mj at time t also depends on the probability that method mj will be 
enabled by time t consequently if time is discretized one needs to 
consider δ m 
candidate policies in order to find π∗ 
 thus 
globally optimal algorithms used for solving real-world problems are 
unlikely to terminate in reasonable time the complexity of 
our model could be reduced if we considered its more restricted 
version in particular if each method mj was allowed to be 
enabled at time points t ∈ tj ⊂ δ the coverage set algorithm 
 csa could be used however csa complexity is double 
exponential in the size of ti and for our domains tj can store all 
values ranging from to δ 
 locally optimal algorithms 
following the limited applicability of globally optimal algorithms 
for dec-mdps with temporal constraints locally optimal 
algorithms appear more promising specially the oc-dec-mdp 
algorithm is particularly significant as it has shown to easily scale 
up to domains with hundreds of methods the idea of the 
oc-decmdp algorithm is to start with the earliest starting time policy π 
 according to which an agent will start executing the method m as 
soon as m has a non-zero chance of being already enabled and 
then improve it iteratively until no further improvement is 
possible at each iteration the algorithm starts with some policy π 
which uniquely determines the probabilities pi τ τ that method 
mi will be performed in the time interval τ τ it then performs 
two steps 
step it propagates from sink methods to source methods the 
values vi τ τ that represent the expected utility for executing 
method mi in the time interval τ τ this propagation uses the 
probabilities pi τ τ from previous algorithm iteration we call 
this step a value propagation phase 
step given the values vi τ τ from step the algorithm chooses 
the most profitable method execution intervals which are stored in 
a new policy π it then propagates the new probabilities pi τ τ 
from source methods to sink methods we call this step a 
probability propagation phase if policy π does not improve π the 
algorithm terminates 
there are two shortcomings of the oc-dec-mdp algorithm that 
we address in this paper first each of oc-dec-mdp states is a 
pair mj τ τ where τ τ is a time interval in which method 
mj can be executed while such state representation is beneficial 
in that the problem can be solved with a standard value iteration 
algorithm it blurs the intuitive mapping from time t to the expected 
total reward for starting the execution of mj at time t 
consequently if some method mi enables method mj and the values 
vj τ τ ∀τ τ ∈ δ are known the operation that calculates the 
values vi τ τ ∀τ τ ∈ δ during the value propagation phase 
runs in time o i 
 where i is the number of time intervals 
 since 
the runtime of the whole algorithm is proportional to the runtime of 
this operation especially for big time horizons δ the oc- 
decmdp algorithm runs slow 
second while oc-dec-mdp emphasizes on precise calculation 
of values vj τ τ it fails to address a critical issue that determines 
how the values vj τ τ are split given that the method mj has 
multiple enabling methods as we show later oc-dec-mdp splits 
vj τ τ into parts that may overestimate vj τ τ when summed up 
again as a result methods that precede the method mj 
overestimate the value for enabling mj which as we show later can have 
disastrous consequences in the next two sections we address both 
of these shortcomings 
 value function propagation vfp 
the general scheme of the vfp algorithm is identical to the 
ocdec-mdp algorithm in that it performs a series of policy 
improvement iterations each one involving a value and probability 
propagation phase however instead of propagating separate 
values vfp maintains and propagates the whole functions we 
therefore refer to these phases as the value function propagation phase 
and the probability function propagation phase to this end for 
each method mi ∈ m we define three new functions 
value function denoted as vi t that maps time t ∈ δ to the 
expected total reward for starting the execution of method mi at 
time t 
opportunity cost function denoted as vi t that maps time 
t ∈ δ to the expected total reward for starting the execution 
of method mi at time t assuming that mi is enabled 
probability function denoted as pi t that maps time t ∈ δ 
to the probability that method mi will be completed before time 
t 
such functional representation allows us to easily read the current 
policy i e if an agent ak is at method mi at time t then it will 
wait as long as value function vi t will be greater in the future 
formally 
πk mi t 
j 
w if ∃t t such that vi t vi t 
e otherwise 
we now develop an analytical technique for performing the value 
function and probability function propagation phases 
 
similarly for the probability propagation phase 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 value function propagation phase 
suppose that we are performing a value function propagation phase 
during which the value functions are propagated from the sink 
methods to the source methods at any time during this phase we 
encounter a situation shown in figure where opportunity cost 
functions vjn n 
n of methods mjn n 
n are known and the 
opportunity cost vi of method mi is to be derived let pi be the 
probability distribution function of method mi execution 
duration and ri be the immediate reward for starting and 
completing the execution of method mi inside a time interval τ τ such 
that mi τ τ ∈ c the function vi is then derived from ri 
and opportunity costs vjn i t n n from future methods 
formally 
vi t 
 
 
 
r τ −t 
 
pi t ri 
pn 
n vjn i t t dt 
if ∃ mi 
τ τ ∈c 
such that t ∈ τ τ 
 otherwise 
 
note that for t ∈ τ τ if h t ri 
pn 
n vjn i τ −t then 
vi is a convolution of p and h vi t pi ∗h τ −t 
assume for now that vjn i represents a full opportunity cost 
postponing the discussion on different techniques for splitting the 
opportunity cost vj into vj ik k 
k until section we now show 
how to derive vj i derivation of vjn i for n follows the 
same scheme 
figure fragment of an mdp of agent ak probability 
functions propagate forward left to right whereas value functions 
propagate backward right to left 
let v j i t be the opportunity cost of starting the execution of 
method mj at time t given that method mi has been completed 
it is derived by multiplying vi by the probability functions of all 
methods other than mi that enable mj formally 
v j i t vj t · 
ky 
k 
pik t 
where similarly to and we ignored the dependency of plk k 
k 
observe that v j i does not have to be monotonically 
decreasing i e delaying the execution of the method mi can sometimes 
be profitable therefore the opportunity cost vj i t of enabling 
method mi at time t must be greater than or equal to v j i 
furthermore vj i should be non-increasing formally 
vj i min 
f∈f 
f 
where f {f f ≥ v j i and f t ≥ f t ∀t t } 
knowing the opportunity cost vi we can then easily derive the 
value function vi let ak be an agent assigned to the method mi 
if ak is about to start the execution of mi it means that ak must 
have completed its part of the mission plan up to the method mi 
since ak does not know if other agents have completed methods 
 mlk k k 
k in order to derive vi it has to multiply vi by the 
probability functions of all methods of other agents that enable mi 
formally 
vi t vi t · 
ky 
k 
plk t 
where the dependency of plk k 
k is also ignored 
we have consequently shown a general scheme how to propagate 
the value functions knowing vjn n 
n and vjn n 
n of methods 
 mjn n 
n we can derive vi and vi of method mi in general the 
value function propagation scheme starts with sink nodes it then 
visits at each time a method m such that all the methods that m 
enables have already been marked as visited the value function 
propagation phase terminates when all the source methods have 
been marked as visited 
 reading the policy 
in order to determine the policy of agent ak for the method mj 
we must identify the set zj of intervals z z ⊂ δ such 
that 
∀t∈ z z πk mj t w 
one can easily identify the intervals of zj by looking at the time 
intervals in which the value function vj does not decrease 
monotonically 
 probability function propagation phase 
assume now that value functions and opportunity cost values have 
all been propagated from sink methods to source nodes and the sets 
zj for all methods mj ∈ m have been identified since value 
function propagation phase was using probabilities pi t for 
methods mi ∈ m and times t ∈ δ found at previous algorithm 
iteration we now have to find new values pi t in order to prepare 
the algorithm for its next iteration we now show how in the general 
case figure propagate the probability functions forward through 
one method i e we assume that the probability functions pik k 
k 
of methods mik k 
k are known and the probability function pj 
of method mj must be derived let pj be the probability 
distribution function of method mj execution duration and zj be the 
set of intervals of inactivity for method mj found during the last 
value function propagation phase if we ignore the dependency of 
 pik k 
k then the probability pj t that the execution of method 
mj starts before time t is given by 
pj t 
 qk 
k pik τ if ∃ τ τ ∈ zj s t t ∈ τ τ 
qk 
k pik t otherwise 
given pj t the probability pj t that method mj will be 
completed by time t is derived by 
pj t 
z t 
 
z t 
 
 
∂pj 
∂t 
 t · pj t − t dt dt 
which can be written compactly as 
∂pj 
∂t 
 pj ∗ 
∂p j 
∂t 
 
we have consequently shown how to propagate the probability 
functions pik k 
k of methods mik k 
k to obtain the probability 
function pj of method mj the general the probability function 
propagation phase starts with source methods msi for which we 
know that psi since they are enabled by default we then 
visit at each time a method m such that all the methods that enable 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
m have already been marked as visited the probability function 
propagation phase terminates when all the sink methods have been 
marked as visited 
 the algorithm 
similarly to the oc-dec-mdp algorithm vfp starts the policy 
improvement iterations with the earliest starting time policy π 
 
then at each iteration it i propagates the value functions vi 
 m 
i 
using the old probability functions pi 
 m 
i from previous algorithm 
iteration and establishes the new sets zi 
 m 
i of method inactivity 
intervals and ii propagates the new probability functions pi 
 m 
i 
using the newly established sets zi 
 m 
i these new functions 
 pi 
 m 
i are then used in the next iteration of the algorithm 
similarly to oc-dec-mdp vfp terminates if a new policy does not 
improve the policy from the previous algorithm iteration 
 implementation of function operations 
so far we have derived the functional operations for value function 
and probability function propagation without choosing any 
function representation in general our functional operations can 
handle continuous time and one has freedom to choose a desired 
function approximation technique such as piecewise linear or 
piecewise constant approximation however since one of our goals 
is to compare vfp with the existing oc-dec- mdp algorithm that 
works only for discrete time we also discretize time and choose to 
approximate value functions and probability functions with 
piecewise linear pwl functions 
when the vfp algorithm propagates the value functions and 
probability functions it constantly carries out operations represented by 
equations and and we have already shown that these 
operations are convolutions of some functions p t and h t if time is 
discretized functions p t and h t are discrete however h t can 
be nicely approximated with a pwl function bh t which is exactly 
what vfp does as a result instead of performing o δ 
 
multiplications to compute f t vfp only needs to perform o k · δ 
multiplications to compute f t where k is the number of linear 
segments of bh t note that since h t is monotonic bh t is 
usually close to h t with k δ since pi values are in range 
 and vi values are in range 
p 
mi∈m ri we suggest to 
approximate vi t with bvi t within error v and pi t with bpi t 
within error p we now prove that the overall approximation error 
accumulated during the value function propagation phase can be 
expressed in terms of p and v 
theorem let c≺ be a set of precedence constraints of a 
dec-mdp with temporal constraints and p and v be the 
probability function and value function approximation errors 
respectively the overall error π maxv supt∈ δ v t − bv t of 
value function propagation phase is then bounded by 
 c≺ 
 
v p c≺ 
− 
p 
mi∈m ri 
 
 
proof in order to establish the bound for π we first prove 
by induction on the size of c≺ that the overall error of 
probability function propagation phase π p maxp supt∈ δ p t − 
bp t is bounded by p c≺ 
− 
induction base if n only two methods are present and we 
will perform the operation identified by equation only once 
introducing the error π p p p c≺ 
− 
induction step suppose that π p for c≺ n is bounded by 
 p n 
− and we want to prove that this statement holds for 
 c≺ n let g m c≺ be a graph with at most n 
edges and g m c≺ be a subgraph of g such that c≺ 
c≺ − { mi mj } where mj ∈ m is a sink node in g from the 
induction assumption we have that c≺ introduces the probability 
propagation phase error bounded by p n 
− we now add 
back the link { mi mj } to c≺ which affects the error of only 
one probability function namely pj by a factor of p since 
probability propagation phase error in c≺ was bounded by 
p n 
− in c≺ c≺ ∪ { mi mj } it can be at most 
p n 
− p p n 
− thus if opportunity cost 
functions are not overestimated they are bounded by 
p 
mi∈m ri 
and the error of a single value function propagation operation will 
be at most 
z δ 
 
p t v p 
 c≺ 
− 
x 
mi∈m 
ri dt v p 
 c≺ 
− 
x 
mi∈m 
ri 
since the number of value function propagation operations is c≺ 
the total error π of the value function propagation phase is bounded 
by c≺ 
 
v p c≺ 
− 
p 
mi∈m ri 
 
 
 splitting the opportunity cost 
functions 
in section we left out the discussion about how the 
opportunity cost function vj of method mj is split into opportunity cost 
functions vj ik k 
k sent back to methods mik k 
k that 
directly enable method mj so far we have taken the same 
approach as in and in that the opportunity cost function vj ik 
that the method mik sends back to the method mj is a 
minimal non-increasing function that dominates function v j ik t 
 vj · 
q 
k ∈{ k} 
k k 
pik 
 t we refer to this approach as 
heuristic h before we prove that this heuristic overestimates the 
opportunity cost we discuss three problems that might occur when 
splitting the opportunity cost functions i overestimation ii 
underestimation and iii starvation consider the situation in figure 
figure splitting the value function of method mj among 
methods mik k 
k 
 when value function propagation for methods mik k 
k is 
performed for each k k equation derives the 
opportunity cost function vik from immediate reward rk and 
opportunity cost function vj ik if m is the only methods that precedes 
method mk then v ik vik is propagated to method m and 
consequently the opportunity cost for completing the method m at 
time t is equal to 
pk 
k vik t if this cost is overestimated then 
an agent a at method m will have too much incentive to finish 
the execution of m at time t consequently although the 
probability p t that m will be enabled by other agents by time t is low 
agent a might still find the expected utility of starting the 
execution of m at time t higher than the expected utility of doing it later 
as a result it will choose at time t to start executing method m 
instead of waiting which can have disastrous consequences 
similarly if 
pk 
k vik t is underestimated agent a might loose 
interest in enabling the future methods mik k 
k and just focus on 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
maximizing the chance of obtaining its immediate reward r since 
this chance is increased when agent a waits 
 it will consider at 
time t to be more profitable to wait instead of starting the 
execution of m which can have similarly disastrous consequences 
finally if vj is split in a way that for some k vj ik it is the 
method mik that underestimates the opportunity cost of enabling 
method mj and the similar reasoning applies we call such 
problem a starvation of method mk that short discussion shows the 
importance of splitting the opportunity cost function vj in such a 
way that overestimation underestimation and starvation problem 
is avoided we now prove that 
theorem heuristic h can overestimate the 
opportunity cost 
proof we prove the theorem by showing a case where the 
overestimation occurs for the mission plan from figure let 
h split vj into v j ik vj · 
q 
k ∈{ k} 
k k 
pik 
 k 
k sent to 
methods mik k 
k respectively also assume that methods mik k 
k 
provide no local reward and have the same time windows i e 
rik estik letik δ for k k to prove the 
overestimation of opportunity cost we must identify t ∈ δ 
such that the opportunity cost 
pk 
k vik t for methods mik k 
k 
at time t ∈ δ is greater than the opportunity cost vj t 
from equation we have 
vik 
 t 
z δ−t 
 
pik 
 t vj ik 
 t t dt 
summing over all methods mik k 
k we obtain 
kx 
k 
vik 
 t 
kx 
k 
z δ−t 
 
pik 
 t vj ik 
 t t dt 
≥ 
kx 
k 
z δ−t 
 
pik 
 t v j ik 
 t t dt 
 
kx 
k 
z δ−t 
 
pik 
 t vj t t 
y 
k ∈{ k} 
k k 
pik 
 t t dt 
let c ∈ be a constant and t ∈ δ be such that ∀t t 
and ∀k k we have 
q 
k ∈{ k} 
k k 
pik 
 t c then 
kx 
k 
vik 
 t 
kx 
k 
z δ−t 
 
pik 
 t vj t t · c dt 
because pjk 
is non-decreasing now suppose there exists t ∈ 
 t δ such that 
pk 
k 
r t −t 
 
pik t dt 
vj 
 t 
c·vj 
 t 
 since 
decreasing the upper limit of the integral over positive function also 
decreases the integral we have 
kx 
k 
vik 
 t c 
kx 
k 
z t 
t 
pik 
 t − t vj t dt 
and since vj t is non-increasing we have 
kx 
k 
vik 
 t c · vj t 
kx 
k 
z t 
t 
pik 
 t − t dt 
 c · vj t 
kx 
k 
z t −t 
 
pik 
 t dt 
 c · vj t 
vj t 
c · vj t 
 vj t 
 
assuming let t 
consequently the opportunity cost 
pk 
k vik t of starting the 
execution of methods mik k 
k at time t ∈ δ is greater 
than the opportunity cost vj t which proves the theorem figure 
 shows that the overestimation of opportunity cost is easily 
observable in practice 
to remedy the problem of opportunity cost overestimation we 
propose three alternative heuristics that split the opportunity cost 
functions 
 heuristic h only one method mik gets the full 
expected reward for enabling method mj i e v j ik 
 t 
for k ∈ { k}\{k} and v j ik t vj · 
q 
k ∈{ k} 
k k 
pik 
 t 
 heuristic h each method mik k 
k gets the full 
opportunity cost for enabling method mj divided by the 
number k of methods enabling the method mj i e v j ik t 
 
k 
 vj · 
q 
k ∈{ k} 
k k 
pik 
 t for k ∈ { k} 
 heuristic bh this is a normalized version of the h 
heuristic in that each method mik k 
k initially gets the full 
opportunity cost for enabling the method mj to avoid 
opportunity cost overestimation we normalize the split 
functions when their sum exceeds the opportunity cost function 
to be split formally 
v j ik t 
 
 
 
v 
h 
j ik 
 t if 
pk 
k v 
h 
j ik 
 t vj t 
vj t 
v 
h 
j ik 
 t 
pk 
k 
v 
h 
j ik 
 t 
otherwise 
where v 
h 
j ik 
 t vj · 
q 
k ∈{ k} 
k k 
pjk 
 t 
for the new heuristics we now prove that 
theorem heuristics h h and bh do not 
overestimate the opportunity cost 
proof when heuristic h is used to split the opportunity 
cost function vj only one method e g mik gets the opportunity 
cost for enabling method mj thus 
kx 
k 
vik 
 t 
z δ−t 
 
pik 
 t vj ik 
 t t dt 
and since vj is non-increasing 
≤ 
z δ−t 
 
pik 
 t vj t t · 
y 
k ∈{ k} 
k k 
pjk 
 t t dt 
≤ 
z δ−t 
 
pik 
 t vj t t dt ≤ vj t 
the last inequality is also a consequence of the fact that vj is 
non-increasing 
for heuristic h we similarly have 
kx 
k 
vik 
 t ≤ 
kx 
k 
z δ−t 
 
pik 
 t 
 
k 
vj t t 
y 
k ∈{ k} 
k k 
pjk 
 t t dt 
≤ 
 
k 
kx 
k 
z δ−t 
 
pik 
 t vj t t dt 
≤ 
 
k 
· k · vj t vj t 
for heuristic bh the opportunity cost function vj is by 
definition split in such manner that 
pk 
k vik t ≤ vj t 
consequently we have proved that our new heuristics h h 
and bh avoid the overestimation of the opportunity cost 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the reason why we have introduced all three new heuristics is the 
following since h overestimates the opportunity cost one 
has to choose which method mik will receive the reward from 
enabling the method mj which is exactly what the heuristic h 
does however heuristic h leaves k − methods that 
precede the method mj without any reward which leads to starvation 
starvation can be avoided if opportunity cost functions are split 
using heuristic h that provides reward to all enabling 
methods however the sum of split opportunity cost functions for the 
h heuristic can be smaller than the non-zero split 
opportunity cost function for the h heuristic which is clearly 
undesirable such situation figure heuristic h occurs because 
the mean f g 
 
of two functions f g is not smaller than f nor g 
only if f g this is why we have proposed the bh heuristic 
which by definition avoids the overestimation underestimation and 
starvation problems 
 experimental evaluation 
since the vfp algorithm that we introduced provides two 
orthogonal improvements over the oc-dec-mdp algorithm the 
experimental evaluation we performed consisted of two parts in part 
we tested empirically the quality of solutions that an locally optimal 
solver either oc-dec-mdp or vfp finds given it uses different 
opportunity cost function splitting heuristic and in part we 
compared the runtimes of the vfp and oc-dec- mdp algorithms for 
a variety of mission plan configurations 
part we first ran the vfp algorithm on a generic mission plan 
configuration from figure where only methods mj mi mi 
and m were present time windows of all methods were set to 
 duration pj of method mj was uniform i e pj t 
 
and durations pi pi of methods mi mi were normal 
distributions i e pi n μ σ and pi n μ 
 σ we assumed that only method mj provided 
reward i e rj was the reward for finishing the execution of 
method mj before time t we show our results in figure 
 where the x-axis of each of the graphs represents time whereas 
the y-axis represents the opportunity cost the first graph confirms 
that when the opportunity cost function vj was split into 
opportunity cost functions vi and vi using the h heuristic the 
function vi vi was not always below the vj function in particular 
vi vi exceeded vj by when 
heuristics h h and bh were used graphs and 
the function vi vi was always below vj 
we then shifted our attention to the civilian rescue domain 
introduced in figure for which we sampled all action execution 
durations from the normal distribution n μ σ to 
obtain the baseline for the heuristic performance we implemented 
a globally optimal solver that found a true expected total reward 
for this domain figure a we then compared this reward with 
a expected total reward found by a locally optimal solver guided 
by each of the discussed heuristics figure a which plots on 
the y-axis the expected total reward of a policy complements our 
previous results h heuristic overestimated the expected total 
reward by whereas the other heuristics were able to guide the 
locally optimal solver close to a true expected total reward 
part we then chose h to split the opportunity cost 
functions and conducted a series of experiments aimed at testing the 
scalability of vfp for various mission plan configurations using 
the performance of the oc-dec-mdp algorithm as a benchmark 
we began the vfp scalability tests with a configuration from figure 
 a associated with the civilian rescue domain for which method 
execution durations were extended to normal distributions n μ 
figure mission plan configurations a civilian rescue 
domain b chain of n methods c tree of n methods with 
branching factor and d square mesh of n methods 
figure vfp performance in the civilian rescue domain 
 σ and the deadline was extended to δ 
we decided to test the runtime of the vfp algorithm running with 
three different levels of accuracy i e different approximation 
parameters p and v were chosen such that the cumulative error 
of the solution found by vfp stayed within and of 
the solution found by the oc- dec-mdp algorithm we then run 
both algorithms for a total of policy improvement iterations 
figure b shows the performance of the vfp algorithm in the 
civilian rescue domain y-axis shows the runtime in milliseconds 
as we see for this small domain vfp runs faster than 
ocdec-mdp when computing the policy with an error of less than 
 for comparison the globally optimal solved did not terminate 
within the first three hours of its runtime which shows the strength 
of the opportunistic solvers like oc-dec-mdp 
we next decided to test how vfp performs in a more difficult 
domain i e with methods forming a long chain figure b we 
tested chains of and methods increasing at the same 
time method time windows to and to ensure that 
later methods can be reached we show the results in figure a 
where we vary on the x-axis the number of methods and plot on 
the y-axis the algorithm runtime notice the logarithmic scale as 
we observe scaling up the domain reveals the high performance of 
vfp within error it runs up to times faster than 
oc-decmdp 
we then tested how vfp scales up given that the methods are 
arranged into a tree figure c in particular we considered trees 
with branching factor of and depth of and increasing at 
the same time the time horizon from to and then to 
we show the results in figure b although the speedups are 
smaller than in case of a chain the vfp algorithm still runs up to 
times faster than oc-dec-mdp when computing the policy with 
an error of less than 
we finally tested how vfp handles the domains with methods 
arranged into a n × n mesh i e c≺ { mi j mk j } for i 
 n k n j n − in particular we consider 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure visualization of heuristics for opportunity costs splitting 
figure scalability experiments for oc-dec-mdp and vfp for different network configurations 
meshes of × × and × methods for such configurations 
we have to greatly increase the time horizon since the 
probabilities of enabling the final methods by a particular time decrease 
exponentially we therefore vary the time horizons from to 
 and then to we show the results in figure c where 
especially for larger meshes the vfp algorithm runs up to one 
order of magnitude faster than oc-dec-mdp while finding a policy 
that is within less than from the policy found by oc- 
decmdp 
 conclusions 
decentralized markov decision process dec-mdp has been very 
popular for modeling of agent-coordination problems it is very 
difficult to solve especially for the real-world domains in this 
paper we improved a state-of-the-art heuristic solution method for 
dec-mdps called oc-dec-mdp that has recently been shown 
to scale up to large dec-mdps our heuristic solution method 
called value function propagation vfp provided two 
orthogonal improvements of oc-dec-mdp i it speeded up 
oc-decmdp by an order of magnitude by maintaining and manipulating a 
value function for each method rather than a separate value for each 
pair of method and time interval and ii it achieved better solution 
qualities than oc-dec-mdp because it corrected the 
overestimation of the opportunity cost of oc-dec-mdp 
in terms of related work we have extensively discussed the 
ocdec-mdp algorithm furthermore as discussed in section 
there are globally optimal algorithms for solving dec-mdps with 
temporal constraints unfortunately they fail to scale up to 
large-scale domains at present time beyond oc-dec-mdp there 
are other locally optimal algorithms for dec-mdps and 
decpomdps yet they have traditionally not dealt with 
uncertain execution times and temporal constraints finally value 
function techniques have been studied in context of single agent 
mdps however similarly to they fail to address the 
lack of global state knowledge which is a fundamental issue in 
decentralized planning 
acknowledgments 
this material is based upon work supported by the darpa ipto 
coordinators program and the air force research 
laboratory under contract no fa c the authors also want 
to thank sven koenig and anonymous reviewers for their valuable 
comments 
 references 
 r becker v lesser and s zilberstein decentralized mdps with 
event-driven interactions in aamas pages - 
 r becker s zilberstein v lesser and c v goldman 
transition-independent decentralized markov decision processes in 
aamas pages - 
 d s bernstein s zilberstein and n immerman the complexity of 
decentralized control of markov decision processes in uai pages 
 - 
 a beynier and a mouaddib a polynomial algorithm for 
decentralized markov decision processes with temporal constraints 
in aamas pages - 
 a beynier and a mouaddib an iterative algorithm for solving 
constrained decentralized markov decision processes in aaai pages 
 - 
 c boutilier sequential optimality and coordination in multiagent 
systems in ijcai pages - 
 j boyan and m littman exact solutions to time-dependent mdps 
in nips pages - 
 c goldman and s zilberstein optimizing information exchange in 
cooperative multi-agent systems 
 l li and m littman lazy approximation for solving continuous 
finite-horizon mdps in aaai pages - 
 y liu and s koenig risk-sensitive planning with one-switch utility 
functions value iteration in aaai pages - 
 d musliner e durfee j wu d dolgov r goldman and 
m boddy coordinated plan management using multiagent mdps in 
aaai spring symposium 
 r nair m tambe m yokoo d pynadath and s marsella taming 
decentralized pomdps towards efficient policy computation for 
multiagent settings in ijcai pages - 
 r nair p varakantham m tambe and m yokoo networked 
distributed pomdps a synergy of distributed constraint 
optimization and pomdps in ijcai pages - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
agents beliefs and plausible behavior in a temporal 
setting 
nils bulling and wojciech jamroga 
department of informatics clausthal university 
of technology germany 
{bulling wjamroga} in tu-clausthal de 
abstract 
logics of knowledge and belief are often too static and 
inflexible to be used on real-world problems in particular they 
usually offer no concept for expressing that some course of 
events is more likely to happen than another we address 
this problem and extend ctlk computation tree logic 
with knowledge with a notion of plausibility which allows 
for practical and counterfactual reasoning the new logic 
ctlkp ctlk with plausibility includes also a 
particular notion of belief a plausibility update operator is added 
to this logic in order to change plausibility assumptions 
dynamically furthermore we examine some important 
properties of these concepts in particular we show that for a 
natural class of models belief is a kd modality we also 
show that model checking ctlkp is ptime-complete 
and can be done in time linear with respect to the size of 
models and formulae 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence-multiagent systems i artificial intelligence 
knowledge representation formalisms and methods-modal 
logic 
general terms 
theory 
 introduction 
notions like time knowledge and beliefs are very 
important for analyzing the behavior of agents and multi-agent 
systems in this paper we extend modal logics of time and 
knowledge with a concept of plausible behavior this notion 
is added to the language of ctlk which is a 
straightforward combination of the branching-time temporal logic 
ctl and standard epistemic logic 
in our approach plausibility can be seen as a temporal 
property of behaviors that is some behaviors of the 
system can be assumed plausible and others implausible with 
the underlying idea that the latter should perhaps be 
ignored in practical reasoning about possible future courses 
of action moreover behaviors can be formally understood 
as temporal paths in the kripke structure modeling a 
multiagent system as a consequence we obtain a language to 
reason about what can or must plausibly happen we 
propose a particular notion of beliefs inspired by 
defined in terms of epistemic relations and plausibility the 
main intuition is that beliefs are facts that an agent would 
know if he assumed that only plausible things could happen 
we believe that humans use such a concept of plausibility 
and practical beliefs quite often in their everyday 
reasoning restricting one s reasoning to plausible possibilities is 
essential to make the reasoning feasible as the space of all 
possibilities is exceedingly large in real life we investigate 
some important properties of plausibility knowledge and 
belief in this new framework in particular we show that 
knowledge is an s modality and that beliefs satisfy 
axioms k in general and kd for the class of plausibly 
serial models finally we show that the relationship 
between knowledge and belief for plausibly serial models is 
natural and reflects the initial intuition well we also show 
how plausibility assumptions can be specified in the object 
language via a plausibility update operator and we study 
properties of such updates finally we show that model 
checking of the new logic is no more complex than model 
checking ctl and ctlk 
our ultimate goal is to come up with a logic that 
allows the study of strategies time knowledge and 
plausible rational behavior under both perfect and imperfect 
information as combining all these dimensions is highly 
nontrivial cf it seems reasonable to split this task 
while this paper deals with knowledge plausibility and 
belief the companion paper proposes a general framework 
for multi-agent systems that regard game-theoretical 
rationality criteria like nash equilibrium pareto optimality etc 
the latter approach is based on the more powerful logic 
atl 
the paper is structured as follows firstly we briefly 
present branching-time logic with knowledge ctlk in 
section we present our approach to plausibility and 
formally define ctlk with plausibility we also show how 
 
 - - - - rps c ifaamas 
temporal formulae can be used to describe plausible paths 
and we compare our logic with existing related work in 
section properties of knowledge belief and plausibility are 
explored finally we present verification complexity results 
for ctlkp in section 
 branching time and knowledge 
in this paper we develop a framework for agents beliefs 
about how the world can or must evolve thus we need a 
notion of time and change plus a notion of what the agents 
are supposed to know in particular situations ctlk 
is a straightforward combination of the computation tree 
logic ctl and standard epistemic logic 
ctl includes operators for temporal properties of 
systems i e path quantifier e there is a path together 
with temporal operators f in the next state 
 always from now on and u until 
every occurrence of 
a temporal operator is preceded by exactly one path 
quantifier in ctl this variant of the language is sometimes called 
vanilla ctl epistemic logic uses operators for 
representing agents knowledge kaϕ is read as agent a knows 
that ϕ 
let π be a set of atomic propositions with a typical 
element p and agt { k} be a set of agents with a typical 
element a the language of ctlk consists of formulae ϕ 
given as follows 
ϕ p ¬ϕ ϕ ∧ ϕ eγ kaϕ 
γ fϕ ϕ ϕu ϕ 
we will sometimes refer to formulae ϕ as vanilla state 
formulae and to formulae γ as vanilla path formulae 
the semantics of ctlk is based on kripke models m 
q r ∼ ∼k π which include a nonempty set of states 
q a state transition relation r ⊆ q × q epistemic 
indistinguishability relations ∼a⊆ q × q one per agent and a 
valuation of propositions π π → p q we assume that 
relation r is serial and that all ∼a are equivalence relations 
a path λ in m refers to a possible behavior or 
computation of system m and can be represented as an infinite 
sequence of states that follow relation r that is a sequence 
q q q such that qirqi for every i we 
denote the ith state in λ by λ i the set of all paths in m 
is denoted by λm if the model is clear from context m 
will be omitted a q-path is a path that starts from q 
i e λ q a q-subpath is a sequence of states starting 
from q which is a subpath of some path in the model i e 
a sequence q q such that q q and there are q 
 qi 
such that q 
 qi 
q q ∈ λm 
the semantics of ctlk is 
defined as follows 
m q p iff q ∈ π p 
m q ¬ϕ iff m q ϕ 
m q ϕ ∧ ψ iff m q ϕ and m q ψ 
m q e fϕ iff there is a q-path λ such that m λ ϕ 
m q e ϕ iff there is a q-path λ such that m λ i ϕ 
for every i ≥ 
 
additional operators a for every path and ♦ 
 sometime in the future are defined in the usual way 
 
for ctlk models λ is a q-subpath iff it is a q-path it 
will not always be so when plausible paths are introduced 
m q eϕu ψ iff there is a q-path λ and i ≥ such that 
m λ i ψ and m λ j ϕ for every ≤ j i 
m q kaϕ iff m q ϕ for every q such that q ∼a q 
 extending time and knowledge 
with plausibility and beliefs 
in this section we discuss the central concept of this 
paper i e the concept of plausibility first we outline the 
idea informally then we extend ctlk with the notion 
of plausibility by adding plausible path operators pl a and 
physical path operator ph to the logic formula pl aϕ has 
the intended meaning according to agent a it is plausible 
that ϕ holds formula ph ϕ reads as ϕ holds in all 
physically possible scenarios i e even in implausible ones the 
plausible path operator restricts statements only to those 
paths which are defined to be sensible whereas the 
physical path operator generates statements about all paths that 
may theoretically occur furthermore we define beliefs on 
top of plausibility and knowledge as the facts that an agent 
would know if he assumed that only plausible things could 
happen finally we discuss related work 
and compare it with our approach 
 the concept of plausibility 
it is well known how knowledge or beliefs can be 
modeled with kripke structures however it is not so obvious 
how we can capture knowledge and beliefs in a sensible way 
in one framework clearly there should be a connection 
between these two notions our approach is to use the 
notion of plausibility for this purpose plausibility can serve 
as a primitive concept that helps to define the semantics 
of beliefs in a similar way as indistinguishability of states 
 represented by relation ∼a is the semantic concept that 
underlies knowledge in this sense our work follows 
essentially beliefs are what an agent would know if he took 
only plausible options into account in our approach 
however plausibility is explicitly seen as a temporal property 
that is we do not consider states or possible worlds to be 
more plausible than others but rather define some behaviors 
to be plausible and others implausible moreover 
behaviors can be formally understood as temporal paths in the 
kripke structure modeling a multi-agent system 
an actual notion of plausibility that is a particular set of 
plausible paths can emerge in many different ways it may 
result from observations and learning an agent can learn 
from its observations and see specific patterns of events as 
plausible a lot of people wear black shoes if they wear a 
suit knowledge exchange is another possibility e g an 
agent a can tell agent b that player c always bluffs when he 
is smiling game theory with its rationality criteria 
 undominated strategies maxmin nash equilibrium etc is 
another viable source of plausibility assumptions last but not 
least folk knowledge can be used to establish 
plausibilityrelated classifications of behavior players normally want 
to win a game people want to live 
in any case restricting the reasoning to plausible 
possibilities can be essential if we want to make the reasoning 
feasible as the space of all possibilities we call them physical 
possibilities in the rest of the paper is exceedingly large in 
real life of course this does not exclude a more extensive 
analysis in special cases e g when our plausibility 
assumptions do not seem accurate any more or when the cost of 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
inaccurate assumptions can be too high as in the case of 
high-budget business decisions but even in these cases we 
usually do not get rid of plausibility assumptions completely 
- we only revise them to make them more cautious 
to formalize this idea we extend models of ctlk with 
sets of plausible paths and add plausibility operators pl a 
physical paths operator ph and belief operators ba to the 
language of ctlk now it is possible to make statements 
that refer to plausible paths only as well as statements that 
regard all paths that may occur in the system 
 ctlk with plausibility 
in this section we extend the logic of ctlk with 
plausibility we call the resulting logic ctlkp formally the 
language of ctlkp is defined as 
ϕ p ¬ϕ ϕ ∧ ϕ eγ pl aϕ ph ϕ kaϕ baϕ 
γ fϕ ϕ ϕu ϕ 
for instance we may claim it is plausible to assume that 
a shop is closed after the opening hours though the manager 
may be physically able to open it at any time pl aa late → 
¬open ∧ ph e♦ late ∧ open 
the semantics of ctlkp extends that of ctlk as 
follows firstly we augment the models with sets of plausible 
paths a model with plausibility is given as 
m q r ∼ ∼k υ υk π 
where q r ∼ ∼k π is a ctlk model and υa ⊆ λm 
is the set of paths in m that are plausible according to agent 
a if we want to make it clear that υa is taken from model 
m we will write υm 
a it seems worth emphasizing that this 
notion of plausibility is subjective and holistic it is 
subjective because υa represents agent a s subjective view on what 
is plausible - and indeed different agents may have 
different ideas on plausibility i e υa may differ from υb it is 
holistic because υa represents agent a s idea of the 
plausible behavior of the whole system including the behavior of 
other agents 
remark in our models plausibility is also global i e 
plausibility sets do not depend on the state of the system 
investigating systems in which plausibility is relativized with 
respect to states like in might be an interesting avenue 
of future work however such an approach - while obviously 
more flexible - allows for potentially counterintuitive system 
descriptions for example it might be the case that path λ 
is plausible in q λ but the set of plausible paths in 
q λ is empty that is by following plausible path λ we 
are bound to get to an implausible situation but then does 
it make sense to consider λ as plausible 
secondly we use a non-standard satisfaction relation p 
which we call plausible satisfaction let m be a ctlkp 
 
that is when planning to open an industrial plant in the 
uk we will probably consider the possibility of our main 
contractor taking her life but we will still not take into 
account the possibilities of an invasion of ufo england being 
destroyed by a meteorite fidel castro becoming the british 
prime minister etc note that this is fundamentally different 
from using a probabilistic model in which all these unlikely 
scenarios are assigned very low probabilities in that case 
they also have a very small influence on our final decision 
but we must process the whole space of physical possibilities 
to evaluate the options 
model and p ⊆ λm be an arbitrary subset of paths in m 
 not necessarily any υm 
a p restricts the evaluation of 
temporal formulae to the paths given in p only the 
absolute satisfaction relation is defined as λm 
let on p be the set of all states that lie on at least one 
path in p i e on p {q ∈ q ∃λ ∈ p∃i λ i q } now 
the semantics of ctlkp can be given through the 
following clauses 
m q p p iff q ∈ π p 
m q p ¬ϕ iff m q p ϕ 
m q p ϕ ∧ ψ iff m q p ϕ and m q p ψ 
m q p e fϕ iff there is a q-subpath λ ∈ p such that 
m λ p ϕ 
m q p e ϕ iff there is a q-subpath λ ∈ p such that 
m λ i p ϕ for every i ≥ 
m q p eϕu ψ iff there is a q-subpath λ ∈ p and i ≥ 
such that m λ i p ψ and m λ j p ϕ for every 
 ≤ j i 
m q p pl aϕ iff m q υa 
ϕ 
m q p ph ϕ iff m q ϕ 
m q p kaϕ iff m q ϕ for every q such that q ∼a q 
m q p baϕ iff for all q ∈ on υa with q ∼a q we have 
that m q υa 
ϕ 
one of the main reasons for using the concept of 
plausibility is that we want to define agents beliefs out of more 
primitive concepts - in our case these are plausibility and 
indistinguishability - in a way analogous to if an 
agent knows that ϕ he must be sure about it however 
beliefs of an agent are not necessarily about reliable facts 
still they should make sense to the agent if he believes that 
ϕ then the formula should at least hold in all futures that 
he envisages as plausible thus beliefs of an agent may be 
seen as things known to him if he disregards all non-plausible 
possibilities 
we say that ϕ is m-true m ϕ if m q ϕ for all 
q ∈ qm ϕ is valid ϕ if m ϕ for all models m ϕ 
is m-strongly true m ≡ ϕ if m q p ϕ for all q ∈ qm 
and all p ⊆ λm ϕ is strongly valid ≡ ϕ if m ≡ ϕ for all 
models m 
proposition strong truth and strong validity imply 
truth and validity respectively the reverse does not hold 
ultimately we are going to be interested in normal not 
strong validity as parameterizing the satisfaction relation 
with a set p is just a technical device for propagating sets 
of plausible paths υa into the semantics of nested formulae 
the importance of strong validity however lies in the fact 
that ≡ ϕ ↔ ψ makes ϕ and ψ completely interchangeable 
while the same is not true for normal validity 
proposition let φ ϕ ψ denote formula φ in which 
every occurrence of ψ was replaced by ϕ also let ≡ ϕ ↔ ψ 
then for all m q p m q p φ iff m q p φ ϕ ψ in 
particular m q φ iff m q φ ϕ ψ 
note that ϕ ↔ ψ does not even imply that m q φ iff 
m q φ ϕ ψ 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure guessing robots game 
example guessing robots consider a simple 
game with two agents a and b shown in figure first 
a chooses a real number r ∈ without revealing the 
number to b then b chooses a real number r ∈ 
the agents win the game and collect eur if 
both chose otherwise they lose formally we model the 
game with a ctlkp model m in which the set of states 
q includes qs for the initial situation states qr r ∈ 
for the situations after a has chosen number r and final 
states qw ql for the winning and the losing situation 
respectively the transition relation is as follows qsrqr and 
qrrql for all r ∈ q rqw qwrqw and qlrql moreover 
π one {q } and π win {qw} player a has perfect 
information in the game i e q ∼a q iff q q but player 
b does not distinguish between states qr i e qr ∼b qr for 
all r r ∈ obviously the only sensible thing to do 
for both agents is to choose using game-theoretical 
vocabulary these strategies are strongly dominant for the 
respective players thus there is only one plausible course of 
events if we assume that our players are rational and hence 
υa υb {qsq qwqw } 
note that in principle the outcome of the game is 
uncertain m qs ¬a♦ win∧¬a ¬win however assuming 
rationality of the players makes it only plausible that the game 
must end up with a win m qs pla a♦ win ∧ plb a♦ win 
and the agents believe that this will be the case m qs 
baa♦ win ∧ bba♦ win note also that in any of the states 
qr agent b believes that a being rational has played 
m qr bbone for all r ∈ 
 defining plausible paths with formulae 
so far we have assumed that sets of plausible paths are 
somehow given in models in this section we present a 
dynamic approach where an actual notion of plausibility can 
be specified in the object language note that we want to 
specify usually infinite sets of infinite paths and we need a 
finite representation of these structures one logical solution 
is given by using path formulae γ these formulae describe 
properties of paths therefore a specific formula can be used 
to characterize a set of paths for instance think about a 
country in africa where it has never snowed then 
plausible paths might be defined as ones in which it never snows 
i e all paths that satisfy ¬snows formally let γ be a 
ctlk path formula we define γ m to be the set of paths 
that satisfy γ in model m 
 fϕ m {λ m λ ϕ} 
 ϕ m {λ ∀i m λ i ϕ } 
 ϕ u ϕ m {λ ∃i 
` 
m λ i ϕ ∧ 
∀j ≤ j i ⇒ m λ j ϕ 
´ 
} 
moreover we define the plausible paths model update as 
follows let m q r ∼ ∼k υ υk π be a 
ctlkp model and let p ⊆ λm be a set of paths then 
ma p 
 q r ∼ ∼k υ υa− p υa υk π 
denotes model m with a s set of plausible paths reset to p 
now we can extend the language of ctlkp with 
formulae set-pla γ ϕ with the intuitive reading suppose that 
γ exactly characterizes the set of plausible paths then ϕ 
holds and formal semantics given below 
m q p set-pla γ ϕ iff ma γ m q p ϕ 
we observe that this update scheme is similar to the one 
proposed in 
 comparison to related work 
several modal notions of plausibility were already 
discussed in the existing literature in these 
papers like in ours plausibility is used as a primitive 
semantic concept that helps to define beliefs on top of agents 
knowledge a similar idea was introduced by moses and 
shoham in their work preceded both and 
 and although moses and shoham do not explicitly mention 
the term plausibility it seems appropriate to summarize 
their idea first 
moses and shoham beliefs as conditional knowledge 
in beliefs are relativized with respect to a formula α 
 which can be seen as a plausibility assumption expressed 
in the object language more precisely worlds that satisfy α 
can be considered as plausible this concept is expressed via 
symbols bα 
i ϕ the index i ∈ { } is used to distinguish 
between three different implementations of beliefs the first 
version is given by bα 
 ϕ ≡ k α → ϕ 
a drawback of 
this version is that if α is false then everything will be 
believed with respect to α the second version overcomes 
this problem bα 
 ϕ ≡ k α → ϕ ∧ k¬α → kϕ now ϕ is 
only believed if it is known that ϕ follows from assumption 
α and ϕ must be known if assumption α is known to be false 
finally bα 
 ϕ ≡ k α → ϕ ∧ ¬k¬α if the assumption α is 
known to be false nothing should be believed with respect to 
α the strength of these different notions is given as follows 
bα 
 ϕ implies bα 
 ϕ and bα 
 ϕ implies bα 
 ϕ in this approach 
belief is strongly connected to knowledge in the sense that 
belief is knowledge with respect to a given assumption 
friedman and halpern plausibility spaces 
the work of friedman and halpern extends the concepts 
of knowledge and belief with an explicit notion of 
plausibility i e some worlds are more plausible for an agent 
than others to implement this idea kripke models are 
extended with function p which assigns a plausibility space 
p q a ω q a q a to every state or more generally 
every possible world q and agent a the plausibility space 
 
unlike in most approaches k is interpreted over all worlds 
and not only over the indistinguishable worlds 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
is just a partially ordered subset of states worlds that is 
ω q a ⊆ q and q a ⊆ q ×q is a reflexive and transitive 
relation let s t ⊆ ω q a be finite subsets of states now 
t is defined to be plausible given s with respect to p q a 
denoted by s →p q a t iff all minimal points states in 
s with respect to q a are also in t 
friedman and 
halpern s view to modal plausibility is closely related to 
probability and more generally plausibility measures 
logics of plausibility can be seen as a qualitative description of 
agents preferences knowledge logics of probability 
on the other hand offer a quantitative description 
the logic from is defined by the following grammar 
ϕ p ϕ∧ϕ ¬ϕ kaϕ ϕ →a ϕ where the semantics of 
all operators except →a is given as usual and formulae ϕ →a 
ψ have the meaning that ψ is true in the most plausible 
worlds in which ϕ holds formally the semantics for →a 
is given as m q ϕ →a ψ iff sϕ 
p q a →p q a 
sψ 
p q a 
where sϕ 
 q a {q ∈ ω q a m q ϕ} are the states in 
ω q a that satisfy ϕ the idea of defining beliefs is given 
by the assumption that an agent believes in something if he 
knows that it is true in the most plausible worlds of ω q a 
formally this can be stated as baϕ ≡ ka →a ϕ 
friedman and halpern have shown that the kd 
axioms are valid for operator ba if plausibility spaces satisfy 
consistency for all states q ∈ q it holds that ω q a ⊆ { q ∈ 
q q ∼a q } and normality for all states q ∈ q it holds 
that ω q a ∅ 
a temporal extension of the language 
 mentioned briefly in and discussed in more detail in 
uses the interpreted systems approach a system r 
is given by runs where a run r n → q is a function from 
time moments modeled by n to global states and a time 
point r i is given by a time point i ∈ n and a run r a 
global state is a combination of local states one per agent 
an interpreted system m r π is given by a system r 
and a valuation of propositions π epistemic relations are 
defined over time points i e r m ∼a r m iff agent 
a s local states ra m and ra m of r m and r m are 
equal formulae are interpreted in a straightforward way 
with respect to interpreted systems e g m r m kaϕ iff 
m r m ϕ for all r m ∼a r m now these are time 
points that play the role of possible worlds consequently 
plausibility spaces p r m a are assigned to each point r m 
and agent a 
su et al kbc logic 
su et al have developed a multi-modal 
computationally grounded logic with modalities k b and c knowledge 
belief and certainty the computational model consists of 
 global states q qvis 
 qinv 
 qper 
 qpls 
 where the 
environment is divided into a visible qvis 
 and an invisible part 
 qinv 
 and qper 
captures the agent s perception of the visible 
part of the environment external sources may provide the 
agent with information about the invisible part of a state 
which results in a set of states qpls 
that are plausible for 
the agent 
given a global state q we additionally define v is q 
qvis 
 inv q qinv 
 per q qper 
 and pls q qpls 
 the 
 
when there are infinite chains q q a q the 
definition is much more sophisticated an interested reader 
is referred to for more details 
 
note that this normality is essentially seriality of states 
wrt plausibility spaces 
semantics is given by an extension of interpreted systems 
 here it is called interpreted kbc systems kbc 
formulae are defined as ϕ p ¬ϕ ϕ ∧ ϕ kϕ bϕ cϕ 
the epistemic relation ∼vis is captured in the following way 
 r i ∼vis r i iff v is r i v is r i the semantic 
clauses for belief and certainty are given below 
m r i bϕ iff m r i ϕ for all r i with v is r i 
per r i and inv r i ∈ pls r i 
m r i cϕ iff m r i ϕ for all r i with v is r i 
per r i 
thus an agent believes ϕ if and only if ϕ is true in all 
states which look like what he sees now and seem plausible 
in the current state certainty is stronger if an agent is 
certain about ϕ the formula must hold in all states with 
a visible part equal to the current perception regardless of 
whether the invisible part is plausible or not 
the logic does not include temporal formulae although 
it might be extended with temporal operators as time is 
already present in kbc models 
what are the differences to our logic 
in our approach plausibility is explicitly seen as a temporal 
property i e it is a property of temporal paths rather than 
states in the object language this is reflected by the fact 
that plausibility assumptions are specified through path 
formulae in contrast the approach of and is static 
not only the logics do not include operators for talking about 
time and or change but these are states that are assumed 
plausible or not in their semantics 
the differences to are more subtle firstly the 
framework of friedman and halpern is static in the sense 
that plausibility is taken as a property of abstract 
possible worlds this formulation is flexible enough to allow for 
incorporating time still in our approach time is inherent 
to plausibility rather than incidental 
secondly our framework is more computationally oriented 
the implementation of temporal plausibility in is based 
on the interpreted systems approach with time points r m 
being subject to plausibility as runs are included in time 
points they can also be defined plausible or implausible 
however it also means that time points serve the role of 
possible worlds in the basic formulation which yields kripke 
structures with uncountable possible world spaces in all but 
the most trivial cases 
thirdly build on linear time a run more precisely 
a time moment r m is fixed when a formula is interpreted 
in contrast we use branching time with explicit 
quantification over temporal paths 
we believe that branching time 
is more suitable for non-deterministic domains cf e g 
of which multi-agent systems are a prime example note 
that branching time makes our notion of belief different from 
friedman and halpern s most notably property kϕ → bϕ 
is valid in their approach but not in ours an agent may 
 
friedman and halpern even briefly mention how 
plausibility of runs can be embedded in their framework 
 
to be more precise time in does implicitly branch at 
epistemic states this is because r m ∼a r m iff a s 
local state corresponding to both time points is the same 
 ra m ra m in consequence the semantics of kaϕ 
can be read as for every run and every moment on this 
run that yields the same local state as now ϕ holds 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
know that some course of events is in principle possible 
without believing that it can really become the case see 
section as proposition suggests such a subtle 
distinction between knowledge and beliefs is possible in our 
approach because branching time logics allow for existential 
quantification over runs 
fourthly while friedman and halpern s models are very 
flexible they also enable system descriptions that may seem 
counterintuitive suppose that r m is plausible in itself 
 formally r m is minimal wrt r m a but r m is 
not plausible in r m this means that following the 
plausible path makes it implausible cf remark which 
is even stranger in the case of linear time combining the 
argument with computational aspects we suggest that our 
approach can be more natural and straightforward for many 
applications 
last but not least our logic provides a mechanism for 
specifying and updating sets of plausible paths in the 
object language thus plausibility sets can be specified in a 
succinct way which is another feature that makes our 
framework computation-friendly the model checking results from 
section are especially encouraging in this light 
 plausibility knowledge and 
beliefs in ctlkp 
in this section we study some relevant properties of 
plausibility knowledge and beliefs in particular axioms kdt 
are examined but first we identify two important 
subclasses of models with plausibility 
a ctlkp model is plausibly serial or p-serial for agent 
a if every state of the system is part of a plausible path 
according to a i e on υa q as we will see further a 
weaker requirement is sometimes sufficient we call a model 
weakly p-serial if every state has at least one 
indistinguishable counterpart which lies on a plausible path i e for each 
q ∈ q there is a q ∈ q such that q ∼a q and q ∈ on υa 
obviously p-seriality implies weak p-seriality we get the 
following characterization of both model classes 
proposition m is plausibly serial for agent a iff 
formula pl ae f is valid in m m is weakly p-serial for agent 
a iff ¬kapl aa f⊥ is valid in m 
 axiomatic properties 
theorem axioms k d and for knowledge are 
strongly valid and axiom t is valid that is modalities ka 
form system s in the sense of normal validity and kd 
in the sense of strong validity 
we do not include proofs here due to lack of space the 
interested reader is referred to where detailed proofs are 
given 
proposition axioms k and for beliefs are 
strongly valid that is we have 
 ≡ baϕ ∧ ba ϕ → ψ → baψ ≡ baϕ → babaϕ and 
 ≡ ¬baϕ → ba¬baϕ 
the next proposition concerns the consistency axiom 
d baϕ → ¬ba¬ϕ it is easy to see that the axiom is not 
valid in general as we have no restrictions on plausibility 
sets υa it may be as well that υa ∅ in that case we have 
baϕ ∧ ba¬ϕ for all formulae ϕ because the set of states to 
be considered becomes empty however it turns out that d 
is valid for a very natural class of models 
proposition axiom d for beliefs is not valid in the 
class of all ctlkp models however it is strongly valid in 
the class of weak p-serial models and therefore also in the 
class of p-serial models 
moreover as one may expect beliefs do not have to be 
always true 
proposition axiom t for beliefs is not valid i e 
 baϕ → ϕ the axiom is not even valid in the class of 
p-serial models 
theorem belief modalities ba form system k in 
the class of all models and kd in the class of weakly 
plausibly serial models in the sense of both normal and 
strong validity axiom t is not even valid for p-serial 
models 
 plausibility knowledge and beliefs 
first we investigate the relationship between knowledge 
and plausibility physicality operators then we look at the 
interaction between knowledge and beliefs 
proposition let ϕ be a ctlkp formula and m 
be a ctlkp model we have the following strong validities 
 i ≡ pl akaϕ ↔ kaϕ 
 ii ≡ ph kaϕ ↔ kaph ϕ and ≡ kaph ϕ ↔ kaϕ 
we now want to examine the relationship between 
knowledge and belief for instance if agent a believes in 
something he knows that he believes it or if he knows a fact 
he also believes that he knows it on the other hand for 
instance an agent does not necessarily believe in all the 
things he knows for example we may know that an 
invasion from another galaxy is in principle possible kae♦ 
invasion but if we do not take this possibility as plausible 
 ¬pl ae♦ invasion then we reject the corresponding belief 
in consequence ¬bae♦ invasion note that this property 
reflects the strong connection between belief and plausibility 
in our framework 
proposition the following formulae are strongly 
valid 
 i baϕ → kabaϕ ii kabaϕ → baϕ 
 iii kaϕ → bakaϕ 
the following formulae are not valid 
 iv baϕ → bakaϕ v kaϕ → baϕ 
the last invalidity is especially important it is not the 
case that knowing something implies believing in it this 
emphasizes that we study a specific concept of beliefs here 
note that its specific is not due to the plausibility-based 
definition of beliefs the reason lies rather in the fact that we 
investigate knowledge beliefs and plausibility in a temporal 
framework as proposition shows 
proposition let ϕ be a ctlkp formula that does 
not include any temporal operators then kaϕ → baϕ is 
strongly valid and in the class of p-serial models we have 
even that ≡ kaϕ ↔ baϕ 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
moreover it is important that we use branching time with 
explicit quantification over paths this observation is 
formalized in proposition 
definition we define the universal sublanguage of 
ctlk in a way similar to 
ϕu p ¬p ϕu ∧ ϕu ϕu ∨ ϕu aγu kaϕu 
γu fϕu ϕu ϕuu ϕu 
we call such ϕu universal formulae and γu universal path 
formulae 
proposition let ϕu be a universal ctlk formula 
then ≡ kaϕu → baϕu 
the following two theorems characterize the relationship 
between knowledge and beliefs first for the class of p-serial 
models and then finally for all models 
theorem the following formulae are strongly valid 
in the class of plausibly serial ctlkp models 
 i baϕ ↔ kapl aϕ ii kaϕ ↔ baph ϕ 
theorem formula baϕ ↔ kapl a e f → ϕ is 
strongly valid 
note that this characterization has a strong commonsense 
reading believing in ϕ is knowing that ϕ plausibly holds in 
all plausibly imaginable situations 
 properties of the update 
the first notable property of plausibility update is that it 
influences only formulae in which plausibility plays a role 
i e ones in which belief or plausibility modalities occur 
proposition let ϕ be a ctlkp formula that does 
not include operators pl a and ba and γ be a ctlkp path 
formula then we have ≡ ϕ ↔ set-pla γ ϕ 
what can be said about the result of an update at first 
sight formula set-pla γ pl aaγ seems a natural 
characterization however it is not valid this is because by leaving 
the other implausible paths out of scope we may leave out 
of γ some paths that were needed to satisfy γ see the 
example in section we propose two alternative ways out 
the first one restricts the language of the update similarly 
to the other refers to physical possibilities in a way 
analogous to 
proposition the ctlkp formula set-pla γ pl aaγ 
is not valid however we have the following validities 
 i ≡ set-pla γu pl aaγu where γu is a universal ctlk 
path formula from definition 
 ii if ϕ ϕ ϕ are arbitrary ctlk formulae then 
 ≡ set-pla 
fϕ pl aa f ph ϕ 
 ≡ set-pla ϕ pl aa ph ϕ and 
 ≡ set-pla ϕ u ϕ pl aa ph ϕ u ph ϕ 
 verification of plausibility 
time and beliefs 
in this section we report preliminary results on model 
checking ctlkp formulae clearly verifying ctlkp 
properties directly against models with plausibility does not 
make much sense since these models are inherently infinite 
what we need is a finite representation of plausibility sets 
one such representation has been discussed in section 
plausibility sets can be defined by path formulae and the 
update operator set-pla γ 
we follow this idea here studying the complexity of model 
checking ctlkp formulae against ctlk models which 
can be seen as a compact representation of ctlkp 
models in which all the paths are assumed plausible with the 
underlying idea that plausibility sets when needed must be 
defined explicitly in the object language below we sketch 
an algorithm that model-checks ctlkp formulae in time 
linear wrt the size of the model and the length of the 
formula this means that we have extended ctlk to a more 
expressive language with no computational price to pay 
first of all we get rid of the belief operators due to 
theorem replacing every occurrence of baϕ with kapl a e f 
→ ϕ now let −→γ γ γk be a vector of vanilla 
path formulae one per agent with the initial vector −→γ 
 and −→γ γ a denoting vector −→γ in which −→γ a 
is replaced with γ additionally we define −→γ we 
translate the resulting ctlkp formulae to ones without 
plausibility via function tr ϕ tr−→γ ϕ defined as 
follows 
tr−→γ i p p 
tr−→γ i ϕ ∧ ϕ tr−→γ i ϕ ∧ tr−→γ i ϕ 
tr−→γ i ¬ϕ ¬tr−→γ i ϕ 
tr−→γ i kaϕ ka tr−→γ ϕ 
tr−→γ i pla ϕ tr−→γ a ϕ 
tr−→γ i set-pla γ ϕ tr−→γ γ a i ϕ 
tr−→γ i ph ϕ tr−→γ ϕ 
tr−→γ i fϕ ftr−→γ i ϕ 
tr−→γ i ϕ tr−→γ i ϕ 
tr−→γ i ϕ u ϕ tr−→γ i ϕ u tr−→γ i ϕ 
tr−→γ i eγ e −→γ i ∧ tr−→γ i γ 
note that the resulting sentences belong to the logic of 
ctlk that is ctl where each path quantifier can be 
followed by a boolean combination of vanilla path 
formulae 
with epistemic modalities the following proposition 
justifies the translation 
proposition for any ctlkp formula ϕ without 
ba we have that m q ctlkp ϕ iff m q ctlk tr ϕ 
in general model checking ctl and also ctlk 
is δp 
 -complete however in our case the boolean 
combinations of path subformulae are always conjunctions of at 
most two non-negated elements which allows us to propose 
the following model checking algorithm first subformulae 
are evaluated recursively for every subformula ψ of ϕ the 
set of states in m that satisfy ψ is computed and labeled 
with a new proposition pψ now it is enough to define 
checking m q ϕ for ϕ in which all state subformulae 
are propositions with the following cases 
case m q e p ∧ γ if m q p then return no 
otherwise remove from m all the states that do not satisfy 
p yielding a sparser model m and check the ctl 
formula eγ in m q with any ctl model-checker 
case m q e fp ∧ γ create m by adding a copy q of 
state q in which only the transitions to states 
satisfying p are kept i e m q r iff m q r and q rq 
iff qrq and m q p then check eγ in m q 
 
for the semantics of ctl and discussion of model 
checking complexity cf 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
case m q e p u p ∧ p u p note that this is 
equivalent to checking e p ∧ p u p ∧ ep u p ∨ e p ∧ 
p u p ∧ ep u p which is a ctl formula 
other cases the above cases cover all possible formulas 
that begin with a path quantifier for other cases 
standard ctlk model checking can be used 
theorem model checking ctlkp against ctlk 
models is ptime-complete and can be done in time o ml 
where m is the number of transitions in the model and l is 
the length of the formula to be checked that is the 
complexity is no worse than for ctlk itself 
 conclusions 
in this paper a notion of plausible behavior is considered 
with the underlying idea that implausible options should be 
usually ignored in practical reasoning about possible future 
courses of action we add the new notion of plausibility to 
the logic of ctlk and obtain a language which 
enables reasoning about what can or must plausibly happen 
as a technical device to define the semantics of the resulting 
logic we use a non-standard satisfaction relation p that 
allows to propagate the current set of plausible paths into 
subformulae furthermore we propose a non-standard 
notion of beliefs defined in terms of indistinguishability and 
plausibility we also propose how plausibility assumptions 
can be specified in the object language via a plausibility 
update operator in a way similar to 
we use this new framework to investigate some important 
properties of plausibility knowledge beliefs and updates 
in particular we show that knowledge is an s modality 
and that beliefs satisfy axioms k in general and kd 
for the class of plausibly serial models we also prove that 
believing in ϕ is knowing that ϕ plausibly holds in all 
plausibly possible situations that is the relationship between 
knowledge and beliefs is very natural and reflects the 
initial intuition precisely moreover the model checking 
results from section show that verification for ctlkp is 
no more complex than for ctl and ctlk 
we would like to stress that we do not see this contribution 
as a mere technical exercise in formal logic human agents 
use a similar concept of plausibility and practical beliefs 
in their everyday reasoning in order to reduce the search 
space and make the reasoning feasible as a consequence we 
suggest that the framework we propose may prove suitable 
for modeling design and analysis resource-bounded agents 
in general 
we would like to thank juergen dix for fruitful 
discussions useful comments and improvements 
 references 
 r alur t a henzinger and o kupferman 
alternating-time temporal logic journal of the 
acm - 
 n bulling and w jamroga agents beliefs and 
plausible behavior in a temporal setting technical 
report ifi- - clausthal univ of technology 
 e a emerson temporal and modal logic in j van 
leeuwen editor handbook of theoretical computer 
science volume b pages - elsevier 
 e a emerson and j y halpern sometimes and 
not never revisited on branching versus linear time 
temporal logic journal of the acm - 
 
 r fagin j y halpern y moses and m y vardi 
reasoning about knowledge mit press cambridge 
ma 
 r fagin and j y halpern reasoning about 
knowledge and probability journal of acm 
 - 
 n friedman and j y halpern a knowledge-based 
framework for belief change part i foundations in 
proceedings of tark pages - 
 n friedman and j y halpern a knowledge-based 
framework for belief change part ii revision and 
update in proceedings of kr 
 j y halpern reasoning about knowledge a survey 
in handbook of logic in artificial intelligence and 
logic programming vol epistemic and temporal 
reasoning pages - oxford university press 
oxford 
 j y halpern and r fagin modelling knowledge and 
action in distributed systems distributed computing 
 - 
 w jamroga and n bulling a general framework for 
reasoning about rational agents in proceedings of 
aamas short paper 
 w jamroga and w van der hoek agents that know 
how to play fundamenta informaticae 
 - - 
 w jamroga w van der hoek and m wooldridge 
intentions and strategies in game-like scenarios in 
progress in artificial intelligence proceedings of 
epia volume of lnai pages - 
springer verlag 
 w jamroga and thomas ˚agotnes constructive 
knowledge what agents can achieve under incomplete 
information technical report ifi- - clausthal 
university of technology 
 b p kooi probabilistic dynamic epistemic logic 
journal of logic language and information 
 - 
 p lamarre and y shoham knowledge certainty 
belief and conditionalisation abbreviated version in 
proceedings of kr pages - 
 f laroussinie n markey and ph schnoebelen 
model checking ctl and fctl is hard in 
proceedings of fossacs volume of lncs 
pages - springer 
 y moses and y shoham belief as defeasible 
knowledge artificial intelligence - 
 w penczek and a lomuscio verifying epistemic 
properties of multi-agent systems via bounded model 
checking in proceedings of aamas pages 
 - new york ny usa acm press 
 k su a sattar g governatori and q chen a 
computationally grounded logic of knowledge belief 
and certainty in proceedings of aamas pages 
 - acm press 
 w van der hoek m roberts and m wooldridge 
social laws in alternating time effectiveness 
feasibility and synthesis synthese 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
graphical models for online solutions to interactive 
pomdps 
prashant doshi 
dept of computer science 
university of georgia 
athens ga usa 
pdoshi cs uga edu 
yifeng zeng 
dept of computer science 
aalborg university 
dk- aalborg denmark 
yfzeng cs aau edu 
qiongyu chen 
dept of computer science 
national univ of singapore 
 singapore 
chenqy comp nus edu sg 
abstract 
we develop a new graphical representation for interactive partially 
observable markov decision processes i-pomdps that is 
significantly more transparent and semantically clear than the previous 
representation these graphical models called interactive dynamic 
influence diagrams i-dids seek to explicitly model the structure 
that is often present in real-world problems by decomposing the 
situation into chance and decision variables and the dependencies 
between the variables i-dids generalize dids which may be viewed 
as graphical representations of pomdps to multiagent settings in 
the same way that i-pomdps generalize pomdps i-dids may be 
used to compute the policy of an agent online as the agent acts and 
observes in a setting that is populated by other interacting agents 
using several examples we show how i-dids may be applied and 
demonstrate their usefulness 
categories and subject descriptors 
i distributed artificial intelligence multiagent systems 
general terms 
theory 
 introduction 
interactive partially observable markov decision processes 
 ipomdps provide a framework for sequential decision-making 
in partially observable multiagent environments they generalize 
pomdps to multiagent settings by including the other agents 
computable models in the state space along with the states of the 
physical environment the models encompass all information 
influencing the agents behaviors including their preferences 
capabilities and beliefs and are thus analogous to types in bayesian 
games i-pomdps adopt a subjective approach to 
understanding strategic behavior rooted in a decision-theoretic framework that 
takes a decision-maker s perspective in the interaction 
in polich and gmytrasiewicz introduced interactive 
dynamic influence diagrams i-dids as the computational 
representations of i-pomdps i-dids generalize dids which may 
be viewed as computational counterparts of pomdps to 
multiagents settings in the same way that i-pomdps generalize pomdps 
i-dids contribute to a growing line of work that includes 
multi-agent influence diagrams maids and more recently 
networks of influence diagrams nids these formalisms seek 
to explicitly model the structure that is often present in real-world 
problems by decomposing the situation into chance and decision 
variables and the dependencies between the variables maids 
provide an alternative to normal and extensive game forms using 
a graphical formalism to represent games of imperfect information 
with a decision node for each agent s actions and chance nodes 
capturing the agent s private information maids objectively 
analyze the game efficiently computing the nash equilibrium profile 
by exploiting the independence structure nids extend maids to 
include agents uncertainty over the game being played and over 
models of the other agents each model is a maid and the network 
of maids is collapsed bottom up into a single maid for 
computing the equilibrium of the game keeping in mind the different 
models of each agent graphical formalisms such as maids and nids 
open up a promising area of research that aims to represent 
multiagent interactions more transparently however maids provide an 
analysis of the game from an external viewpoint and the 
applicability of both is limited to static single play games matters are more 
complex when we consider interactions that are extended over time 
where predictions about others future actions must be made using 
models that change as the agents act and observe i-dids address 
this gap by allowing the representation of other agents models as 
the values of a special model node both other agents models and 
the original agent s beliefs over these models are updated over time 
using special-purpose implementations 
in this paper we improve on the previous preliminary 
representation of the i-did shown in by using the insight that the static 
i-id is a type of nid thus we may utilize nid-specific language 
constructs such as multiplexers to represent the model node and 
subsequently the i-id more transparently furthermore we clarify 
the semantics of the special purpose policy link introduced in the 
representation of i-did by and show that it could be replaced 
by traditional dependency links in the previous representation of 
the i-did the update of the agent s belief over the models of others 
as the agents act and receive observations was denoted using a 
special link called the model update link that connected the model 
nodes over time we explicate the semantics of this link by 
showing how it can be implemented using the traditional dependency 
links between the chance nodes that constitute the model nodes 
the net result is a representation of i-did that is significantly more 
transparent semantically clear and capable of being implemented 
using the standard algorithms for solving dids we show how 
idids may be used to model an agent s uncertainty over others 
models that may themselves be i-dids solution to the i-did is 
a policy that prescribes what the agent should do over time given 
its beliefs over the physical state and others models analogous to 
dids i-dids may be used to compute the policy of an agent online 
as the agent acts and observes in a setting that is populated by other 
interacting agents 
 background finitely nested 
ipomdps 
interactive pomdps generalize pomdps to multiagent settings 
by including other agents models as part of the state space 
since other agents may also reason about others the interactive 
state space is strategically nested it contains beliefs about other 
agents models and their beliefs about others for simplicity of 
presentation we consider an agent i that is interacting with one 
other agent j 
a finitely nested i-pomdp of agent i with a strategy level l is 
defined as the tuple 
i-pomdpi l isi l a ti ωi oi ri 
where isi l denotes a set of interactive states defined as isi l 
s × mj l− where mj l− {θj l− ∪ smj} for l ≥ and 
isi s where s is the set of states of the physical 
environment θj l− is the set of computable intentional models of agent 
j θj l− bj l− ˆθj where the frame ˆθj a ωj tj oj rj 
ocj here j is bayes rational and ocj is j s optimality criterion 
smj is the set of subintentional models of j simple examples of 
subintentional models include a no-information model and a 
fictitious play model both of which are history independent 
we give a recursive bottom-up construction of the interactive state 
space below 
isi s θj { bj ˆθj bj ∈ δ isj } 
isi s × {θj ∪ smj} θj { bj ˆθj bj ∈ δ isj } 
 
 
 
 
 
 
isi l s × {θj l− ∪ smj} θj l { bj l ˆθj bj l ∈ δ isj l } 
similar formulations of nested spaces have appeared in 
 a ai × aj is the set of joint actions of all agents in the 
environment ti s ×a×s → describes the effect of the 
joint actions on the physical states of the environment ωi is the 
set of observations of agent i oi s × a × ωi → gives 
the likelihood of the observations given the physical state and joint 
action ri isi × a → r describes agent i s preferences over 
its interactive states usually only the physical states will matter 
agent i s policy is the mapping ω∗ 
i → δ ai where ω∗ 
i is 
the set of all observation histories of agent i since belief over the 
interactive states forms a sufficient statistic the policy can also 
be represented as a mapping from the set of all beliefs of agent i to 
a distribution over its actions δ isi → δ ai 
 belief update 
analogous to pomdps an agent within the i-pomdp 
framework updates its belief as it acts and observes however there are 
two differences that complicate the belief update in multiagent 
settings when compared to single agent ones first since the state of 
the physical environment depends on the actions of both agents i s 
prediction of how the physical state changes has to be made based 
on its prediction of j s actions second changes in j s models have 
to be included in i s belief update specifically if j is intentional 
then an update of j s beliefs due to its action and observation has 
to be included in other words i has to update its belief based on 
its prediction of what j would observe and how j would update 
its belief if j s model is subintentional then j s probable 
observations are appended to the observation history contained in the 
model formally we have 
pr ist 
 at− 
i bt− 
i l β ist− mt− 
j θt 
j 
bt− 
i l ist− 
 
× at− 
j 
pr at− 
j θt− 
j l− oi st 
 at− 
i at− 
j ot 
i 
×ti st− 
 at− 
i at− 
j st 
 ot 
j 
oj st 
 at− 
i at− 
j ot 
j 
×τ seθt 
j 
 bt− 
j l− at− 
j ot 
j − bt 
j l− 
 
where β is the normalizing constant τ is if its argument is 
otherwise it is pr at− 
j θt− 
j l− is the probability that at− 
j is 
bayes rational for the agent described by model θt− 
j l− and se · 
is an abbreviation for the belief update for a version of the belief 
update when j s model is subintentional see 
if agent j is also modeled as an i-pomdp then i s belief update 
invokes j s belief update via the term seθt 
j 
 bt− 
j l− at− 
j ot 
j 
which in turn could invoke i s belief update and so on this 
recursion in belief nesting bottoms out at the th 
level at this level the 
belief update of the agent reduces to a pomdp belief update 
for 
illustrations of the belief update additional details on i-pomdps 
and how they compare with other multiagent frameworks see 
 value iteration 
each belief state in a finitely nested i-pomdp has an associated 
value reflecting the maximum payoff the agent can expect in this 
belief state 
un 
 bi l θi max 
ai∈ai is∈isi l 
eri is ai bi l is 
γ 
oi∈ωi 
pr oi ai bi l un− 
 seθi 
 bi l ai oi θi 
 
where eri is ai aj 
ri is ai aj pr aj mj l− since 
is s mj l− eq is a basis for value iteration in i-pomdps 
agent i s optimal action a∗ 
i for the case of finite horizon with 
discounting is an element of the set of optimal actions for the belief 
state opt θi defined as 
opt bi l θi argmax 
ai∈ai is∈isi l 
eri is ai bi l is 
 γ 
oi∈ωi 
pr oi ai bi l un 
 seθi 
 bi l ai oi θi 
 
 interactiveinfluencediagrams 
a naive extension of influence diagrams ids to settings 
populated by multiple agents is possible by treating other agents as 
automatons represented using chance nodes however this approach 
assumes that the agents actions are controlled using a probability 
distribution that does not change over time interactive influence 
diagrams i-ids adopt a more sophisticated approach by 
generalizing ids to make them applicable to settings shared with other 
agents who may act and observe and update their beliefs 
 syntax 
in addition to the usual chance decision and utility nodes 
iids include a new type of node called the model node we show a 
general level l i-id in fig a where the model node mj l− is 
denoted using a hexagon we note that the probability distribution 
over the chance node s and the model node together represents 
agent i s belief over its interactive states in addition to the model 
 
the th 
level model is a pomdp other agent s actions are treated 
as exogenous events and folded into the t o and r functions 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure a a generic level l i-id for agent i situated with one other agent j the hexagon is the model node mj l− whose structure we show in 
 b members of the model node are i-ids themselves m 
j l− m 
j l− diagrams not shown here for simplicity whose decision nodes are mapped to 
the corresponding chance nodes a 
j a 
j depending on the value of the node mod mj the distribution of each of the chance nodes is assigned to 
the node aj c the transformed i-id with the model node replaced by the chance nodes and the relationships between them 
node i-ids differ from ids by having a dashed link called the 
policy link in between the model node and a chance node 
aj that represents the distribution over the other agent s actions 
given its model in the absence of other agents the model node and 
the chance node aj vanish and i-ids collapse into traditional ids 
the model node contains the alternative computational models 
ascribed by i to the other agent from the set θj l− ∪ smj where 
θj l− and smj were defined previously in section thus a 
model in the model node may itself be an i-id or id and the 
recursion terminates when a model is an id or subintentional because 
the model node contains the alternative models of the other agent 
as its values its representation is not trivial in particular some of 
the models within the node are i-ids that when solved generate the 
agent s optimal policy in their decision nodes each decision node 
is mapped to the corresponding chance node say a 
j in the 
following way if opt is the set of optimal actions obtained by solving 
the i-id or id then pr aj ∈ a 
j 
 op t 
if aj ∈ opt 
otherwise 
borrowing insights from previous work we observe that the 
model node and the dashed policy link that connects it to the 
chance node aj could be represented as shown in fig b the 
decision node of each level l − i-id is transformed into a chance 
node as we mentioned previously so that the actions with the 
largest value in the decision node are assigned uniform 
probabilities in the chance node while the rest are assigned zero probability 
the different chance nodes a 
j a 
j one for each model and 
additionally the chance node labeled mod mj form the parents of the 
chance node aj thus there are as many action nodes a 
j a 
j 
in mj l− as the number of models in the support of agent i s 
beliefs the conditional probability table of the chance node aj 
is a multiplexer that assumes the distribution of each of the action 
nodes a 
j a 
j depending on the value of mod mj the values of 
mod mj denote the different models of j in other words when 
mod mj has the value m 
j l− the chance node aj assumes the 
distribution of the node a 
j and aj assumes the distribution of a 
j 
when mod mj has the value m 
j l− the distribution over the 
node mod mj is the agent i s belief over the models of j given a 
physical state for more agents we will have as many model nodes 
as there are agents notice that fig b clarifies the semantics of 
the policy link and shows how it can be represented using the 
traditional dependency links 
in fig c we show the transformed i-id when the model node 
is replaced by the chance nodes and relationships between them in 
contrast to the representation in there are no special-purpose 
policy links rather the i-id is composed of only those types of 
nodes that are found in traditional ids and dependency 
relationships between the nodes this allows i-ids to be represented and 
implemented using conventional application tools that target ids 
note that we may view the level l i-id as a nid specifically each 
of the level l − models within the model node are blocks in the 
nid see fig if the level l each block is a traditional id 
otherwise if l each block within the nid may itself be a nid 
note that within the i-ids or ids at each level there is only a 
single decision node thus our nid does not contain any maids 
figure a level l i-id represented as a nid the probabilities 
assigned to the blocks of the nid are i s beliefs over j s models 
conditioned on a physical state 
 solution 
the solution of an i-id proceeds in a bottom-up manner and is 
implemented recursively we start by solving the level models 
which if intentional are traditional ids their solutions provide 
probability distributions over the other agents actions which are 
entered in the corresponding chance nodes found in the model node 
of the level i-id the mapping from the level models decision 
nodes to the chance nodes is carried out so that actions with the 
largest value in the decision node are assigned uniform 
probabilities in the chance node while the rest are assigned zero probability 
given the distributions over the actions within the different chance 
nodes one for each model of the other agent the level i-id is 
transformed as shown in fig c during the transformation the 
conditional probability table cpt of the node aj is populated 
such that the node assumes the distribution of each of the chance 
nodes depending on the value of the node mod mj as we 
mentioned previously the values of the node mod mj denote the 
different models of the other agent and its distribution is the agent i s 
belief over the models of j conditioned on the physical state the 
transformed level i-id is a traditional id that may be solved 
us the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 a b 
figure a a generic two time-slice level l i-did for agent i in a setting with one other agent j notice the dotted model update link that denotes 
the update of the models of j and the distribution over the models over time b the semantics of the model update link 
ing the standard expected utility maximization method this 
procedure is carried out up to the level l i-id whose solution gives 
the non-empty set of optimal actions that the agent should perform 
given its belief notice that analogous to ids i-ids are suitable for 
online decision-making when the agent s current belief is known 
 interactive dynamic influence 
diagrams 
interactive dynamic influence diagrams i-dids extend i-ids 
 and nids to allow sequential decision-making over several time 
steps just as dids are structured graphical representations of pomdps 
i-dids are the graphical online analogs for finitely nested i-pomdps 
i-dids may be used to optimize over a finite look-ahead given 
initial beliefs while interacting with other possibly similar agents 
 syntax 
we depict a general two time-slice i-did in fig a in 
addition to the model nodes and the dashed policy link what 
differentiates an i-did from a did is the model update link shown as a 
dotted arrow in fig a we explained the semantics of the model 
node and the policy link in the previous section we describe the 
model updates next 
the update of the model node over time involves two steps first 
given the models at time t we identify the updated set of models 
that reside in the model node at time t recall from section 
that an agent s intentional model includes its belief because the 
agents act and receive observations their models are updated to 
reflect their changed beliefs since the set of optimal actions for 
a model could include all the actions and the agent may receive 
any one of ωj possible observations the updated set at time step 
t will have at most mt 
j l− aj ωj models here mt 
j l− 
is the number of models at time step t aj and ωj are the largest 
spaces of actions and observations respectively among all the 
models second we compute the new distribution over the updated 
models given the original distribution and the probability of the 
agent performing the action and receiving the observation that led 
to the updated model these steps are a part of agent i s belief 
update formalized using eq 
in fig b we show how the dotted model update link is 
implemented in the i-did if each of the two level l − models 
ascribed to j at time step t results in one action and j could make 
one of two possible observations then the model node at time step 
t contains four updated models mt 
j l− mt 
j l− mt 
j l− and 
mt 
j l− these models differ in their initial beliefs each of which 
is the result of j updating its beliefs due to its action and a possible 
observation the decision nodes in each of the i-dids or dids that 
represent the lower level models are mapped to the corresponding 
figure transformed i-did with the model nodes and model update 
link replaced with the chance nodes and the relationships in bold 
chance nodes as mentioned previously next we describe how the 
distribution over the updated set of models the distribution over the 
chance node mod mt 
j in mt 
j l− is computed the probability 
that j s updated model is say mt 
j l− depends on the probability 
of j performing the action and receiving the observation that led to 
this model and the prior distribution over the models at time step 
t because the chance node at 
j assumes the distribution of each 
of the action nodes based on the value of mod mt 
j the 
probability of the action is given by this chance node in order to obtain 
the probability of j s possible observation we introduce the chance 
node oj which depending on the value of mod mt 
j assumes the 
distribution of the observation node in the lower level model 
denoted by mod mt 
j because the probability of j s observations 
depends on the physical state and the joint actions of both agents 
the node oj is linked with st 
 at 
j and at 
i 
analogous to at 
j 
the conditional probability table of oj is also a multiplexer 
modulated by mod mt 
j finally the distribution over the prior models 
at time t is obtained from the chance node mod mt 
j in mt 
j l− 
consequently the chance nodes mod mt 
j at 
j and oj form the 
parents of mod mt 
j in mt 
j l− notice that the model update 
link may be replaced by the dependency links between the chance 
nodes that constitute the model nodes in the two time slices in 
fig we show the two time-slice i-did with the model nodes 
replaced by the chance nodes and the relationships between them 
chance nodes and dependency links that not in bold are standard 
usually found in dids 
expansion of the i-did over more time steps requires the 
repetition of the two steps of updating the set of models that form the 
 
note that oj represents j s observation at time t 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
values of the model node and adding the relationships between the 
chance nodes as many times as there are model update links we 
note that the possible set of models of the other agent j grows 
exponentially with the number of time steps for example after t steps 
there may be at most mt 
j l− aj ωj t − 
candidate models 
residing in the model node 
 solution 
analogous to i-ids the solution to a level l i-did for agent i 
expanded over t time steps may be carried out recursively for the 
purpose of illustration let l and t the solution method uses 
the standard look-ahead technique projecting the agent s action 
and observation sequences forward from the current belief state 
and finding the possible beliefs that i could have in the next time 
step because agent i has a belief over j s models as well the 
lookahead includes finding out the possible models that j could have in 
the future consequently each of j s subintentional or level 
models represented using a standard did in the first time step must be 
solved to obtain its optimal set of actions these actions are 
combined with the set of possible observations that j could make in that 
model resulting in an updated set of candidate models that include 
the updated beliefs that could describe the behavior of j beliefs 
over this updated set of candidate models are calculated using the 
standard inference methods using the dependency relationships 
between the model nodes as shown in fig b we note the recursive 
nature of this solution in solving agent i s level i-did j s level 
dids must be solved if the nesting of models is deeper all models 
at all levels starting from are solved in a bottom-up manner 
we briefly outline the recursive algorithm for solving agent i s 
algorithm for solving i-did 
input level l ≥ i-id or level id t 
expansion phase 
 for t from to t − do 
 if l ≥ then 
populate mt 
j l− 
 for each mt 
j in range mt 
j l− do 
 recursively call algorithm with the l − i-id or id 
that represents mt 
j and the horizon t − t 
 map the decision node of the solved i-id or id 
opt mt 
j to a chance node aj 
 for each aj in opt mt 
j do 
 for each oj in oj part of mt 
j do 
 update j s belief bt 
j ← se bt 
j aj oj 
 mt 
j ← new i-id or id with bt 
j as the 
initial belief 
 range mt 
j l− 
∪ 
← {mt 
j } 
 add the model node mt 
j l− and the dependency links 
between mt 
j l− and mt 
j l− shown in fig b 
 add the chance decision and utility nodes for t time 
slice and the dependency links between them 
 establish the cpts for each chance node and utility node 
look-ahead phase 
 apply the standard look-ahead and backup method to solve 
the expanded i-did 
figure algorithm for solving a level l ≥ i-did 
level l i-did expanded over t time steps with one other agent j in 
fig we adopt a two-phase approach given an i-id of level l 
 described previously in section with all lower level models also 
represented as i-ids or ids if level the first step is to expand 
the level l i-id over t time steps adding the dependency links and 
the conditional probability tables for each node we particularly 
focus on establishing and populating the model nodes lines - 
note that range · returns the values lower level models of the 
random variable given as input model node in the second phase 
we use a standard look-ahead technique projecting the action and 
observation sequences over t time steps in the future and backing 
up the utility values of the reachable beliefs similar to i-ids the 
i-dids reduce to dids in the absence of other agents 
as we mentioned previously the -th level models are the 
traditional dids their solutions provide probability distributions over 
actions of the agent modeled at that level to i-dids at level given 
probability distributions over other agent s actions the level 
idids can themselves be solved as dids and provide probability 
distributions to yet higher level models assume that the number 
of models considered at each level is bound by a number m 
solving an i-did of level l in then equivalent to solving o ml 
 dids 
 example applications 
to illustrate the usefulness of i-dids we apply them to three 
problem domains we describe in particular the formulation of 
the i-did and the optimal prescriptions obtained on solving it 
 followership-leadership in the multiagent 
tiger problem 
we begin our illustrations of using i-ids and i-dids with a slightly 
modified version of the multiagent tiger problem discussed in 
the problem has two agents each of which can open the right door 
 or the left door ol or listen l in addition to hearing growls 
 from the left gl or from the right gr when they listen the 
agents also hear creaks from the left cl from the right cr or 
no creaks s which noisily indicate the other agent s opening one 
of the doors when any door is opened the tiger persists in its 
original location with a probability of agent i hears growls with 
a reliability of and creaks with a reliability of agent j 
on the other hand hears growls with a reliability of thus 
the setting is such that agent i hears agent j opening doors more 
reliably than the tiger s growls this suggests that i could use j s 
actions as an indication of the location of the tiger as we discuss 
below each agent s preferences are as in the single agent game 
discussed in the transition observation and reward 
functions are shown in 
a good indicator of the usefulness of normative methods for 
decision-making like i-dids is the emergence of realistic social 
behaviors in their prescriptions in settings of the persistent 
multiagent tiger problem that reflect real world situations we demonstrate 
followership between the agents and as shown in deception 
among agents who believe that they are in a follower-leader type 
of relationship in particular we analyze the situational and 
epistemological conditions sufficient for their emergence the 
followership behavior for example results from the agent knowing its own 
weaknesses assessing the strengths preferences and possible 
behaviors of the other and realizing that its best for it to follow the 
other s actions in order to maximize its payoffs 
let us consider a particular setting of the tiger problem in which 
agent i believes that j s preferences are aligned with its own - both 
of them just want to get the gold - and j s hearing is more reliable 
in comparison to itself as an example suppose that j on listening 
can discern the tiger s location of the times compared to i s 
 accuracy additionally agent i does not have any initial 
information about the tiger s location in other words i s single-level 
nested belief bi assigns to each of the two locations of the 
tiger in addition i considers two models of j which differ in j s 
flat level initial beliefs this is represented in the level i-id 
shown in fig a according to one model j assigns a 
probability of that the tiger is behind the left door while the other 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure a level i-id of agent i b two level ids of agent j 
whose decision nodes are mapped to the chance nodes a 
j a 
j in a 
model assigns to that location see fig b agent i is 
undecided on these two models of j if we vary i s hearing ability 
and solve the corresponding level i-id expanded over three time 
steps we obtain the normative behavioral policies shown in fig 
that exhibit followership behavior if i s probability of correctly 
hearing the growls is then as shown in the policy in fig a 
i begins to conditionally follow j s actions i opens the same door 
that j opened previously iff i s own assessment of the tiger s 
location confirms j s pick if i loses the ability to correctly interpret 
the growls completely it blindly follows j and opens the same door 
that j opened previously fig b 
figure emergence of a conditional followership and b blind 
followership in the tiger problem behaviors of interest are in bold is 
a wildcard and denotes any one of the observations 
we observed that a single level of belief nesting - beliefs about 
the other s models - was sufficient for followership to emerge in the 
tiger problem however the epistemological requirements for the 
emergence of leadership are more complex for an agent say j to 
emerge as a leader followership must first emerge in the other agent 
i as we mentioned previously if i is certain that its preferences 
are identical to those of j and believes that j has a better sense 
of hearing i will follow j s actions over time agent j emerges 
as a leader if it believes that i will follow it which implies that 
j s belief must be nested two levels deep to enable it to recognize 
its leadership role realizing that i will follow presents j with an 
opportunity to influence i s actions in the benefit of the collective 
good or its self-interest alone for example in the tiger problem 
let us consider a setting in which if both i and j open the correct 
door then each gets a payoff of that is double the original if 
j alone selects the correct door it gets the payoff of on the 
other hand if both agents pick the wrong door their penalties are 
cut in half in this setting it is in both j s best interest as well as the 
collective betterment for j to use its expertise in selecting the 
correct door and thus be a good leader however consider a slightly 
different problem in which j gains from i s loss and is penalized 
if i gains specifically let i s payoff be subtracted from j s 
indicating that j is antagonistic toward i - if j picks the correct door 
and i the wrong one then i s loss of becomes j s gain agent 
j believes that i incorrectly thinks that j s preferences are those 
that promote the collective good and that it starts off by believing 
with confidence where the tiger is because i believes that its 
preferences are similar to those of j and that j starts by believing 
almost surely that one of the two is the correct location two level 
 models of j i will start by following j s actions we show i s 
normative policy on solving its singly-nested i-did over three time 
steps in fig a the policy demonstrates that i will blindly 
follow j s actions since the tiger persists in its original location with 
a probability of i will select the same door again if j begins 
the game with a probability that the tiger is on the right 
solving j s i-did nested two levels deep results in the policy shown in 
fig b even though j is almost certain that ol is the correct 
action it will start by selecting or followed by ol agent j s 
intention is to deceive i who it believes will follow j s actions so 
as to gain in the second time step which is more than what j 
would gain if it were to be honest 
figure emergence of deception between agents in the tiger 
problem behaviors of interest are in bold denotes as before a agent 
i s policy demonstrating that it will blindly follow j s actions b even 
though j is almost certain that the tiger is on the right it will start by 
selecting or followed by ol in order to deceive i 
 altruism and reciprocity in the public 
good problem 
the public good pg problem consists of a group of m 
agents each of whom must either contribute some resource to a 
public pot or keep it for themselves since resources contributed to 
the public pot are shared among all the agents they are less 
valuable to the agent when in the public pot however if all agents 
choose to contribute their resources then the payoff to each agent 
is more than if no one contributes since an agent gets its share of 
the public pot irrespective of whether it has contributed or not the 
dominating action is for each agent to not contribute and instead 
free ride on others contributions however behaviors of human 
players in empirical simulations of the pg problem differ from the 
normative predictions the experiments reveal that many players 
initially contribute a large amount to the public pot and continue 
to contribute when the pg problem is played repeatedly though 
in decreasing amounts many of these experiments report 
that a small core group of players persistently contributes to the 
public pot even when all others are defecting these experiments 
also reveal that players who persistently contribute have altruistic 
or reciprocal preferences matching expected cooperation of others 
for simplicity we assume that the game is played between m 
 agents i and j let each agent be initially endowed with xt 
amount of resources while the classical pg game formulation 
permits each agent to contribute any quantity of resources ≤ xt to 
the public pot we simplify the action space by allowing two 
possible actions each agent may choose to either contribute c a fixed 
amount of the resources or not contribute the latter action is 
dethe sixth intl joint conf on autonomous agents and multi-agent systems aamas 
noted as defect d we assume that the actions are not observable 
to others the value of resources in the public pot is discounted 
by ci for each agent i where ci is the marginal private return we 
assume that ci so that the agent does not benefit enough that 
it contributes to the public pot for private gain simultaneously 
cim making collective contribution pareto optimal 
i j c d 
c cixt cjxt cixt − cp xt cjxt − p 
d xt cixt − p cjxt − cp xt xt 
table the one-shot pg game with punishment 
in order to encourage contributions the contributing agents 
punish free riders but incur a small cost for administering the 
punishment let p be the punishment meted out to the defecting agent 
and cp the non-zero cost of punishing for the contributing agent 
for simplicity we assume that the cost of punishing is same for 
both the agents the one-shot pg game with punishment is shown 
in table let ci cj cp and if p xt − cixt then 
defection is no longer a dominating action if p xt − cixt then 
defection is the dominating action for both if p xt − cixt 
then the game is not dominance-solvable 
figure a level i-id of agent i b level ids of agent j with 
decision nodes mapped to the chance nodes a 
j and a 
j in a 
we formulate a sequential version of the pg problem with 
punishment from the perspective of agent i though in the repeated pg 
game the quantity in the public pot is revealed to all the agents after 
each round of actions we assume in our formulation that it is 
hidden from the agents each agent may contribute a fixed amount xc 
or defect an agent on performing an action receives an observation 
of plenty py or meager mr symbolizing the state of the 
public pot notice that the observations are also indirectly indicative of 
agent j s actions because the state of the public pot is influenced by 
them the amount of resources in agent i s private pot is perfectly 
observable to i the payoffs are analogous to table 
borrowing from the empirical investigations of the pg problem we 
construct level ids for j that model altruistic and non-altruistic 
types fig b specifically our altruistic agent has a high 
marginal private return cj is close to and does not punish others 
who defect let xc and the level agent be punished half the 
times it defects with one action remaining both types of agents 
choose to contribute to avoid being punished with two actions 
to go the altruistic type chooses to contribute while the other 
defects this is because cj for the altruistic type is close to thus the 
expected punishment p − cj which the altruistic type 
avoids because cj for the non-altruistic type is less it prefers not 
to contribute with three steps to go the altruistic agent contributes 
to avoid punishment p − cj and the non-altruistic 
type defects for greater than three steps while the altruistic agent 
continues to contribute to the public pot depending on how close 
its marginal private return is to the non-altruistic type prescribes 
defection 
we analyzed the decisions of an altruistic agent i modeled using 
a level i-did expanded over time steps i ascribes the two level 
 models mentioned previously to j see fig if i believes with 
a probability that j is altruistic i chooses to contribute for each of 
the three steps this behavior persists when i is unaware of whether 
j is altruistic fig a and when i assigns a high probability to 
j being the non-altruistic type however when i believes with a 
probability that j is non-altruistic and will thus surely defect i 
chooses to defect to avoid being punished and because its marginal 
private return is less than these results demonstrate that the 
behavior of our altruistic type resembles that found experimentally 
the non-altruistic level agent chooses to defect regardless of how 
likely it believes the other agent to be altruistic we analyzed the 
behavior of a reciprocal agent type that matches expected 
cooperation or defection the reciprocal type s marginal private return 
is similar to that of the non-altruistic type however it obtains a 
greater payoff when its action is similar to that of the other we 
consider the case when the reciprocal agent i is unsure of whether 
j is altruistic and believes that the public pot is likely to be half 
full for this prior belief i chooses to defect on receiving an 
observation of plenty i decides to contribute while an observation of 
meager makes it defect fig b this is because an 
observation of plenty signals that the pot is likely to be greater than half 
full which results from j s action to contribute thus among the 
two models ascribed to j its type is likely to be altruistic making 
it likely that j will contribute again in the next time step agent i 
therefore chooses to contribute to reciprocate j s action an 
analogous reasoning leads i to defect when it observes a meager pot 
with one action to go i believing that j contributes will choose to 
contribute too to avoid punishment regardless of its observations 
figure a an altruistic level agent always contributes b a 
reciprocal agent i starts off by defecting followed by choosing to 
contribute or defect based on its observation of plenty indicating that j is 
likely altruistic or meager j is non-altruistic 
 strategies in two-player poker 
poker is a popular zero sum card game that has received much 
attention among the ai research community as a testbed poker is 
played among m ≥ players in which each player receives a hand 
of cards from a deck while several flavors of poker with 
varying complexity exist we consider a simple version in which each 
player has three plys during which the player may either exchange 
a card e keep the existing hand k fold f and withdraw from 
the game or call c requiring all players to show their hands to 
keep matters simple let m and each player receive a hand 
consisting of a single card drawn from the same suit thus during 
a showdown the player who has the numerically larger card is 
the lowest ace is the highest wins the pot during an exchange of 
cards the discarded card is placed either in the l pile indicating to 
the other agent that it was a low numbered card less than or in the 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
h pile indicating that the card had a rank greater than or equal to 
 notice that for example if a lower numbered card is discarded 
the probability of receiving a low card in exchange is now reduced 
we show the level i-id for the simplified two-player poker in 
fig we considered two models personality types of agent j 
the conservative type believes that it is likely that its opponent has 
a high numbered card in its hand on the other hand the 
aggressive agent j believes with a high probability that its opponent has 
a lower numbered card thus the two types differ in their beliefs 
over their opponent s hand in both these level models the 
opponent is assumed to perform its actions following a fixed uniform 
distribution with three actions to go regardless of its hand 
 unless it is an ace the aggressive agent chooses to exchange its card 
with the intent of improving on its current hand this is because it 
believes the other to have a low card which improves its chances 
of getting a high card during the exchange the conservative agent 
chooses to keep its card no matter its hand because its chances of 
getting a high card are slim as it believes that its opponent has one 
figure a level i-id of agent i the observation reveals 
information about j s hand of the previous time step b level ids of agent 
j whose decision nodes are mapped to the chance nodes a 
j a 
j in a 
the policy of a level agent i who believes that each card 
except its own has an equal likelihood of being in j s hand neutral 
personality type and j could be either an aggressive or 
conservative type is shown in fig i s own hand contains the card 
numbered the agent starts by keeping its card on seeing that 
j did not exchange a card n i believes with probability that j 
is conservative and hence will keep its cards i responds by either 
keeping its card or exchanging it because j is equally likely to have 
a lower or higher card if i observes that j discarded its card into 
the l or h pile i believes that j is aggressive on observing l 
i realizes that j had a low card and is likely to have a high card 
after its exchange because the probability of receiving a low card 
is high now i chooses to keep its card on observing h 
believing that the probability of receiving a high numbered card is high 
i chooses to exchange its card in the final step i chooses to call 
regardless of its observation history because its belief that j has a 
higher card is not sufficiently high to conclude that its better to fold 
and relinquish the payoff this is partly due to the fact that an 
observation of say l resets the agent i s previous time step beliefs 
over j s hand to the low numbered cards only 
 discussion 
we showed how dids may be extended to i-dids that enable 
online sequential decision-making in uncertain multiagent settings 
our graphical representation of i-dids improves on the previous 
figure a level agent i s three step policy in the poker problem 
i starts by believing that j is equally likely to be aggressive or 
conservative and could have any card in its hand with equal probability 
work significantly by being more transparent semantically clear 
and capable of being solved using standard algorithms that target 
dids i-dids extend nids to allow sequential decision-making 
over multiple time steps in the presence of other interacting agents 
i-dids may be seen as concise graphical representations for 
ipomdps providing a way to exploit problem structure and carry 
out online decision-making as the agent acts and observes given its 
prior beliefs we are currently investigating ways to solve i-dids 
approximately with provable bounds on the solution quality 
acknowledgment we thank piotr gmytrasiewicz for some 
useful discussions related to this work the first author would like 
to acknowledge the support of a ugarf grant 
 references 
 r j aumann interactive epistemology i knowledge international 
journal of game theory - 
 d billings a davidson j schaeffer and d szafron the challenge 
of poker aij 
 a brandenburger and e dekel hierarchies of beliefs and common 
knowledge journal of economic theory - 
 c camerer behavioral game theory experiments in strategic 
interaction princeton university press 
 e fehr and s gachter cooperation and punishment in public goods 
experiments american economic review - 
 d fudenberg and d k levine the theory of learning in games 
mit press 
 d fudenberg and j tirole game theory mit press 
 y gal and a pfeffer a language for modeling agent s 
decision-making processes in games in aamas 
 p gmytrasiewicz and p doshi a framework for sequential planning 
in multiagent settings jair - 
 p gmytrasiewicz and e durfee rational coordination in 
multi-agent environments jaamas - 
 j c harsanyi games with incomplete information played by 
bayesian players management science - 
 r a howard and j e matheson influence diagrams in r a 
howard and j e matheson editors the principles and applications 
of decision analysis strategic decisions group menlo park ca 
 
 l kaelbling m littman and a cassandra planning and acting in 
partially observable stochastic domains artificial intelligence 
journal 
 d koller and b milch multi-agent influence diagrams for 
representing and solving games in ijcai pages - 
 k polich and p gmytrasiewicz interactive dynamic influence 
diagrams in gtdt workshop aamas 
 b rathnas p doshi and p j gmytrasiewicz exact solutions to 
interactive pomdps using behavioral equivalence in autonomous 
agents and multi-agent systems conference aamas 
 s russell and p norvig artificial intelligence a modern approach 
 second edition prentice hall 
 r d shachter evaluating influence diagrams operations research 
 - 
 d suryadi and p gmytrasiewicz learning models of other agents 
using influence diagrams in um 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a formal model for situated semantic alignment 
manuel atencia marco schorlemmer 
iiia artificial intelligence research institute 
csic spanish national research council 
bellaterra barcelona catalonia spain 
{manu marco} iiia csic es 
abstract 
ontology matching is currently a key technology to achieve 
the semantic alignment of ontological entities used by 
knowledge-based applications and therefore to enable their 
interoperability in distributed environments such as 
multiagent systems most ontology matching mechanisms 
however assume matching prior integration and rely on 
semantics that has been coded a priori in concept hierarchies or 
external sources in this paper we present a formal model for 
a semantic alignment procedure that incrementally aligns 
differing conceptualisations of two or more agents relative 
to their respective perception of the environment or domain 
they are acting in it hence makes the situation in which 
the alignment occurs explicit in the model we resort to 
channel theory to carry out the formalisation 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence-coherence and coordination multiagent systems 
d software engineering interoperability-data 
mapping i artificial intelligence knowledge 
representation formalisms and methods-semantic networks 
relation systems 
general terms 
theory 
 introduction 
an ontology is commonly defined as a specification of the 
conceptualisation of a particular domain it fixes the 
vocabulary used by knowledge engineers to denote concepts and 
their relations and it constrains the interpretation of this 
vocabulary to the meaning originally intended by knowledge 
engineers as such ontologies have been widely adopted as 
a key technology that may favour knowledge sharing in 
distributed environments such as multi-agent systems 
federated databases or the semantic web but the proliferation 
of many diverse ontologies caused by different 
conceptualisations of even the same domain -and their subsequent 
specification using varying terminology- has highlighted 
the need of ontology matching techniques that are 
capable of computing semantic relationships between entities of 
separately engineered ontologies 
until recently most ontology matching mechanisms 
developed so far have taken a classical functional approach 
to the semantic heterogeneity problem in which ontology 
matching is seen as a process taking two or more 
ontologies as input and producing a semantic alignment of 
ontological entities as output furthermore matching 
often has been carried out at design-time before 
integrating knowledge-based systems or making them interoperate 
this might have been successful for clearly delimited and 
stable domains and for closed distributed systems but it is 
untenable and even undesirable for the kind of applications 
that are currently deployed in open systems multi-agent 
communication peer-to-peer information sharing and 
webservice composition are all of a decentralised dynamic and 
open-ended nature and they require ontology matching to 
be locally performed during run-time in addition in many 
situations peer ontologies are not even open for inspection 
 e g when they are based on commercially confidential 
information 
certainly there exist efforts to efficiently match 
ontological entities at run-time taking only those ontology 
fragment that are necessary for the task at hand 
nevertheless the techniques used by these systems to 
establish the semantic relationships between ontological entities 
-even though applied at run-time- still exploit a priori 
defined concept taxonomies as they are represented in the 
graph-based structures of the ontologies to be matched use 
previously existing external sources such as thesauri e g 
wordnet and upper-level ontologies e g cyc or sumo 
or resort to additional background knowledge repositories or 
shared instances 
we claim that semantic alignment of ontological 
terminology is ultimately relative to the particular situation in which 
the alignment is carried out and that this situation should 
be made explicit and brought into the alignment 
mechanism even two agents with identical conceptualisation 
capabilities and using exactly the same vocabulary to specify 
their respective conceptualisations may fail to interoperate 
 
 - - - - rps c ifaamas 
in a concrete situation because of their differing perception 
of the domain imagine a situation in which two agents 
are facing each other in front of a checker board agent 
a may conceptualise a figure on the board as situated on 
the left margin of the board while agent a may 
conceptualise the same figure as situated on the right although 
the conceptualisation of  left and  right is done in exactly 
the same manner by both agents and even if both use the 
terms left and right in their communication they still will 
need to align their respective vocabularies if they want to 
successfully communicate to each other actions that change 
the position of figures on the checker board their semantic 
alignment however will only be valid in the scope of their 
interaction within this particular situation or environment 
the same agents situated differently may produce a different 
alignment 
this scenario is reminiscent to those in which a group of 
distributed agents adapt to form an ontology and a shared 
lexicon in an emergent bottom-up manner with only local 
interactions and no central control authority this sort 
of self-organised emergence of shared meaning is namely 
ultimately grounded on the physical interaction of agents with 
the environment in this paper however we address the 
case in which agents are already endowed with a top-down 
engineered ontology it can even be the same one which 
they do not adapt or refine but for which they want to 
find the semantic relationships with separate ontologies of 
other agents on the grounds of their communication within a 
specific situation in particular we provide a formal model 
that formalises situated semantic alignment as a sequence of 
information-channel refinements in the sense of barwise and 
seligman s theory of information flow this theory is 
particularly useful for our endeavour because it models the flow 
of information occurring in distributed systems due to the 
particular situations -or tokens- that carry information 
analogously the semantic alignment that will allow 
information to flow ultimately will be carried by the particular 
situation agents are acting in 
we shall therefore consider a scenario with two or more 
agents situated in an environment each agent will have its 
own viewpoint of the environment so that if the 
environment is in a concrete state both agents may have different 
perceptions of this state because of these differences there 
may be a mismatch in the meaning of the syntactic 
entities by which agents describe their perceptions and which 
constitute the agents respective ontologies we state that 
these syntactic entities can be related according to the 
intrinsic semantics provided by the existing relationship 
between the agents viewpoint of the environment the 
existence of this relationship is precisely justified by the fact that 
the agents are situated and observe the same environment 
in section we describe our formal model for situated 
semantic alignment ssa first in section we associate 
a channel to the scenario under consideration and show how 
the distributed logic generated by this channel provides the 
logical relationships between the agents viewpoints of the 
environment second in section we present a method by 
which agents obtain approximations of this distributed logic 
these approximations gradually become more reliable as the 
method is applied in section we report on an application 
of our method conclusions and further work are analyzed 
in section finally an appendix summarizes the terms and 
theorems of channel theory used along the paper we do not 
assume any knowledge of channel theory we restate basic 
definitions and theorems in the appendix but any detailed 
exposition of the theory is outside the scope of this paper 
 a formal model for ssa 
 the logic of ssa 
consider a scenario with two agents a and a situated 
in an environment e the generalization to any numerable 
set of agents is straightforward we associate a numerable 
set s of states to e and at any given instant we suppose 
e to be in one of these states we further assume that 
each agent is able to observe the environment and has its 
own perception of it this ability is faithfully captured by 
a surjective function seei s → pi where i ∈ { } and 
typically see and see are different 
according to channel theory information is only viable 
where there is a systematic way of classifying some range 
of things as being this way or that in other words where 
there is a classification see appendix a so in order to be 
within the framework of channel theory we must associate 
classifications to the components of our system 
for each i ∈ { } we consider a classification ai that 
models ai s viewpoint of e first tok ai is composed of 
ai s perceptions of e states that is tok ai pi second 
typ ai contains the syntactic entities by which ai describes 
its perceptions the ones constituting the ontology of ai 
finally ai synthesizes how ai relates its perceptions with 
these syntactic entities 
now with the aim of associating environment e with a 
classification e we choose the power classification of s as e 
which is the classification whose set of types is equal to s 
 
whose tokens are the elements of s and for which a token 
e is of type ε if e ∈ ε the reason for taking the power 
classification is because there are no syntactic entities that 
may play the role of types for e since in general there is no 
global conceptualisation of the environment however the 
set of types of the power classification includes all possible 
token configurations potentially described by types thus 
tok e s typ e s 
and e e ε if and only if e ∈ ε 
the notion of channel see appendix a is fundamental in 
barwise and seligman s theory the information flow among 
the components of a distributed system is modelled in terms 
of a channel and the relationships among these components 
are expressed via infomorphisms see appendix a which 
provide a way of moving information between them 
the information flow of the scenario under consideration 
is accurately described by channel e {fi ai → e}i∈{ } 
defined as follows 
 ˆfi α {e ∈ tok e seei e ai α} for each α ∈ 
typ ai 
 ˇfi e seei e for each e ∈ tok e 
where i ∈ { } definition of ˇfi seems natural while ˆfi is 
defined in such a way that the fundamental property of the 
infomorphisms is fulfilled 
ˇfi e ai α iff seei e ai α by definition of ˇfi 
iff e ∈ ˆfi α by definition of ˆfi 
iff e e 
ˆfi α by definition of e 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
consequently e is the core of channel e and a state 
e ∈ tok e connects agents perceptions ˇf e and ˇf e see 
figure 
typ e 
typ a 
ˆf 
 ttttttttt 
typ a 
ˆf 
eejjjjjjjjj 
tok e 
 e 
 
 
 
 
 
 
 
ˇf yyttttttttt 
ˇf jjjjjjjjj 
tok a 
 a 
 
 
 
 
 
 
 
tok a 
 a 
 
 
 
 
 
 
 
figure channel e 
e explains the information flow of our scenario by virtue 
of agents a and a being situated and perceiving the same 
environment e we want to obtain meaningful relations 
among agents syntactic entities that is agents types we 
state that meaningfulness must be in accord with e 
the sum operation see appendix a gives us a way of 
putting the two agents classifications of channel e together 
into a single classification namely a a and also the two 
infomorphisms together into a single infomorphism f f 
a a → e 
a a assembles agents classifications in a very coarse 
way tok a a is the cartesian product of tok a and 
tok a that is tok a a { p p pi ∈ pi} so a 
token of a a is a pair of agents perceptions with no 
restrictions typ a a is the disjoint union of typ a 
and typ a and p p is of type i α if pi is of type 
α we attach importance to take the disjoint union because 
a and a could use identical types with the purpose of 
describing their respective perceptions of e 
classification a a seems to be the natural place in 
which to search for relations among agents types now 
channel theory provides a way to make all these relations 
explicit in a logical fashion by means of theories and local 
logics see appendix a the theory generated by the sum 
classification th a a and hence its logic generated 
log a a involve all those constraints among agents 
types valid according to a a notice however that these 
constraints are obvious as we stated above meaningfulness 
must be in accord with channel e 
classifications a a and e are connected via the sum 
infomorphism f f f where 
 ˆf i α ˆfi α {e ∈ tok e seei e ai α} for 
each i α ∈ typ a a 
 ˇf e ˇf e ˇf e see e see e for each e ∈ 
tok e 
meaningful constraints among agents types are in accord 
with channel e because they are computed making use of f 
as we expound below 
as important as the notion of channel is the concept of 
distributed logic see appendix a given a channel c and 
a logic l on its core dlogc l represents the reasoning 
about relations among the components of c justified by l 
if l log c the distributed logic we denoted by log c 
captures in a logical fashion the information flow inherent 
in the channel 
in our case log e explains the relationship between the 
agents viewpoints of the environment in a logical fashion 
on the one hand constraints of th log e are defined by 
γ log e δ if ˆf γ log e 
ˆf δ 
where γ δ ⊆ typ a a on the other hand the set of 
normal tokens nlog e is equal to the range of function ˇf 
nlog e ˇf tok e 
 { see e see e e ∈ tok e } 
therefore a normal token is a pair of agents perceptions 
that are restricted by coming from the same environment 
state unlike a a tokens 
all constraints of th log e are satisfied by all normal 
tokens because of being a logic in this particular case this 
condition is also sufficient the proof is straightforward as 
alternative to we have 
γ log e δ iff for all e ∈ tok e 
if ∀ i γ ∈ γ seei e ai γ 
then ∃ j δ ∈ δ seej e aj δ 
where γ δ ⊆ typ a a 
log e is the logic of ssa th log e comprises the 
most meaningful constraints among agents types in accord 
with channel e in other words the logic of ssa contains 
and also justifies the most meaningful relations among those 
syntactic entities that agents use in order to describe their 
own environment perceptions 
log e is complete since log e is complete but it is not 
necessarily sound because although log e is sound ˇf is 
not surjective in general see appendix b if log e is also 
sound then log e log a a see appendix b that 
means there is no significant relation between agents points 
of view of the environment according to e it is just the fact 
that log e is unsound what allows a significant relation 
between the agents viewpoints this relation is expressed 
at the type level in terms of constraints by th log e and 
at the token level by nlog e 
 approaching the logic of ssa 
through communication 
we have dubbed log e the logic of ssa th log e 
comprehends the most meaningful constraints among agents 
types according to e the problem is that neither agent 
can make use of this theory because they do not know e 
completely in this section we present a method by which 
agents obtain approximations to th log e we also prove 
these approximations gradually become more reliable as the 
method is applied 
agents can obtain approximations to th log e through 
communication a and a communicate by exchanging 
information about their perceptions of environment states 
this information is expressed in terms of their own 
classification relations specifically if e is in a concrete state e 
we assume that agents can convey to each other which types 
are satisfied by their respective perceptions of e and which 
are not this exchange generates a channel c {fi ai → 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
c}i∈{ } and th log c contains the constraints among 
agents types justified by the fact that agents have observed 
e now if e turns to another state e and agents proceed 
as before another channel c {fi ai → c }i∈{ } gives 
account of the new situation considering also the previous 
information th log c comprises the constraints among 
agents types justified by the fact that agents have observed 
e and e the significant point is that c is a refinement of 
c see appendix a theorem below ensures that the 
refined channel involves more reliable information 
the communication supposedly ends when agents have 
observed all the environment states again this situation can 
be modeled by a channel call it c∗ 
 {f∗ 
i ai → c∗ 
}i∈{ } 
theorem states that th log c∗ 
 th log e 
theorem and theorem assure that applying the 
method agents can obtain approximations to th log e 
gradually more reliable 
theorem let c {fi ai → c}i∈{ } and c 
{fi ai → c }i∈{ } be two channels if c is a refinement 
of c then 
 th log c ⊆ th log c 
 nlog c ⊇ nlog c 
proof since c is a refinement of c then there exists a 
refinement infomorphism r from c to c so fi r ◦ fi let 
a def a a f def f f and f def f f 
 let γ and δ be subsets of typ a and assume that 
γ log c δ which means ˆf γ c 
ˆf δ we have 
to prove γ log c δ or equivalently ˆf γ c 
ˆf δ 
we proceed by reductio ad absurdum suppose c ∈ 
tok c does not satisfy the sequent ˆf γ ˆf δ then 
c c 
ˆf γ for all γ ∈ γ and c c 
ˆf δ for all δ ∈ δ 
let us choose an arbitrary γ ∈ γ we have that 
γ i α for some α ∈ typ ai and i ∈ { } thus 
ˆf γ ˆf i α ˆfi α ˆr ◦ ˆfi α ˆr ˆfi α 
therefore 
c c 
ˆf γ iff c c ˆr ˆfi α 
iff ˇr c c 
ˆfi α 
iff ˇr c c 
ˆf i α 
iff ˇr c c 
ˆf γ 
consequently ˇr c c 
ˆf γ for all γ ∈ γ since 
ˆf γ c 
ˆf δ then there exists δ∗ 
∈ δ such that 
ˇr c c 
ˆf δ∗ 
 a sequence of equivalences similar to 
the above one justifies c c 
ˆf δ∗ 
 contradicting that c 
is a counterexample to ˆf γ ˆf δ hence γ log c δ 
as we wanted to prove 
 let a a ∈ tok a and assume a a ∈ nlog c 
therefore there exists c token in c such that a a 
ˇf c then we have ai ˇfi c ˇfi ◦ ˇr c ˇfi ˇr c 
for i ∈ { } hence a a ˇf ˇr c and a a ∈ 
nlog c consequently nlog c ⊇ nlog c which 
concludes the proof 
remark theorem asserts that the more refined 
channel gives more reliable information even though its 
theory has less constraints it has more normal tokens to 
which they apply 
in the remainder of the section we explicitly describe the 
process of communication and we conclude with the proof 
of theorem 
let us assume that typ ai is finite for i ∈ { } and s 
is infinite numerable though the finite case can be treated 
in a similar form we also choose an infinite numerable set 
of symbols {cn 
 n ∈ n} 
 
we omit informorphisms superscripts when no confusion 
arises types are usually denoted by greek letters and tokens 
by latin letters so if f is an infomorphism f α ≡ ˆf α and 
f a ≡ ˇf a 
agents communication starts from the observation of e 
let us suppose that e is in state e 
∈ s tok e a s 
perception of e 
is f e 
 and a s perception of e 
is f e 
 
we take for granted that a can communicate a those 
types that are and are not satisfied by f e 
 according to 
its classification a so can a do since both typ a and 
typ a are finite this process eventually finishes after 
this communication a channel c 
 {f 
i ai → c 
}i 
arises see figure 
c 
a 
f 
 
 
a 
f 
 
aacccccccc 
figure the first communication stage 
on the one hand c 
is defined by 
 tok c 
 {c 
} 
 typ c 
 typ a a 
 c 
 c i α if fi e 
 ai α 
 for every i α ∈ typ a a 
on the other hand f 
i with i ∈ { } is defined by 
 f 
i α i α 
 for every α ∈ typ ai 
 f 
i c 
 fi e 
 
log c 
 represents the reasoning about the first stage of 
communication it is easy to prove that th log c 
 
th c 
 the significant point is that both agents know c 
as the result of the communication hence they can compute 
separately theory th c 
 typ c 
 c which contains 
the constraints among agents types justified by the fact that 
agents have observed e 
 
now let us assume that e turns to a new state e 
 agents 
can proceed as before exchanging this time information 
about their perceptions of e 
 another channel c 
 {f 
i 
ai → c 
}i∈{ } comes up we define c 
so as to take also 
into account the information provided by the previous stage 
of communication 
on the one hand c 
is defined by 
 tok c 
 {c 
 c 
} 
 
we write these symbols with superindices because we limit 
the use of subindices for what concerns to agents note this 
set is chosen with the same cardinality of s 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 typ c 
 typ a a 
 ck 
 c i α if fi ek 
 ai α 
 for every k ∈ { } and i α ∈ typ a a 
on the other hand f 
i with i ∈ { } is defined by 
 f 
i α i α 
 for every α ∈ typ ai 
 f 
i ck 
 fi ek 
 
 for every k ∈ { } 
log c 
 represents the reasoning about the former and 
the later communication stages th log c 
 is equal to 
th c 
 typ c 
 c then it contains the constraints 
among agents types justified by the fact that agents have 
observed e 
and e 
 a and a knows c 
so they can use 
these constraints the key point is that channel c 
is a 
refinement of c 
 it is easy to check that f 
defined as 
the identity function on types and the inclusion function on 
tokens is a refinement infomorphism see at the bottom of 
figure by theorem c 
constraints are more reliable 
than c 
constraints 
in the general situation once the states e 
 e 
 en− 
 n ≥ have been observed and a new state en 
appears 
channel cn 
 {fn 
i ai → cn 
}i∈{ } informs about agents 
communication up to that moment cn 
definition is 
similar to the previous ones and analogous remarks can be 
made see at the top of figure theory th log cn 
 
th cn 
 typ cn 
 cn contains the constraints among 
agents types justified by the fact that agents have observed 
e 
 e 
 en 
 
cn 
fn− 
 
a 
fn− 
 
 ppppppppppppp 
fn 
 
uunnnnnnnnnnnnn 
f 
 
 
f 
 
 a 
fn 
 
ggppppppppppppp 
fn− 
 
wwnnnnnnnnnnnnn 
f 
 
õõ 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
f 
 
øø 
cn− 
 
 
 
 
 
c 
f 
 
c 
figure agents communication 
remember we have assumed that s is infinite numerable 
it is therefore unpractical to let communication finish when 
all environment states have been observed by a and a 
at that point the family of channels {cn 
}n∈n would inform 
of all the communication stages it is therefore up to the 
agents to decide when to stop communicating should a good 
enough approximation have been reached for the purposes of 
their respective tasks but the study of possible termination 
criteria is outside the scope of this paper and left for future 
work from a theoretical point of view however we can 
consider the channel c∗ 
 {f∗ 
i ai → c∗ 
}i∈{ } which 
informs of the end of the communication after observing all 
environment states 
on the one hand c∗ 
is defined by 
 tok c∗ 
 {cn 
 n ∈ n} 
 typ c∗ 
 typ a a 
 cn 
 c∗ i α if fi en 
 ai α 
 for n ∈ n and i α ∈ typ a a 
on the other hand f∗ 
i with i ∈ { } is defined by 
 f∗ 
i α i α 
 for α ∈ typ ai 
 f∗ 
i cn 
 fi en 
 
 for n ∈ n 
theorem below constitutes the cornerstone of the model 
exposed in this paper it ensures together with theorem 
 that at each communication stage agents obtain a theory 
that approximates more closely to the theory generated by 
the logic of ssa 
theorem the following statements hold 
 for all n ∈ n c∗ 
is a refinement of cn 
 
 th log e th c∗ 
 th log c∗ 
 
proof 
 it is easy to prove that for each n ∈ n gn 
defined as the 
identity function on types and the inclusion function 
on tokens is a refinement infomorphism from c∗ 
to cn 
 
 the second equality is straightforward the first one 
follows directly from 
cn 
 c∗ i α iff ˇfi en 
 ai α 
 by definition of c∗ 
iff en 
 e 
ˆfi α 
 because fi is infomorphim 
iff en 
 e 
ˆf i α 
 by definition of ˆf 
e 
c∗ 
gn 
 
a 
fn 
 
 ooooooooooooo 
f∗ 
 
uuooooooooooooo 
f 
cc 
a 
f∗ 
 
ggooooooooooooo 
fn 
 
wwooooooooooooo 
f 
 
cn 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 an example 
in the previous section we have described in great detail 
our formal model for ssa however we have not tackled 
the practical aspect of the model yet in this section we 
give a brushstroke of the pragmatic view of our approach 
we study a very simple example and explain how agents 
can use those approximations of the logic of ssa they can 
obtain through communication 
let us reflect on a system consisting of robots located in 
a two-dimensional grid looking for packages with the aim of 
moving them to a certain destination figure robots 
can carry only one package at a time and they can not move 
through a package 
figure the scenario 
robots have a partial view of the domain and there exist 
two kinds of robots according to the visual field they have 
some robots are capable of observing the eight adjoining 
squares but others just observe the three squares they have 
in front see figure we call them urdl shortened 
form of up-right-down-left and lcr abbreviation for 
left-center-right robots respectively 
describing the environment states as well as the robots 
perception functions is rather tedious and even unnecessary 
we assume the reader has all those descriptions in mind 
all robots in the system must be able to solve package 
distribution problems cooperatively by communicating their 
intentions to each other in order to communicate agents 
send messages using some ontology in our scenario there 
coexist two ontologies the udrl and lcr ontologies both 
of them are very simple and are just confined to describe 
what robots observe 
figure robots field of vision 
when a robot carrying a package finds another package 
obstructing its way it can either go around it or if there is 
another robot in its visual field ask it for assistance let 
us suppose two urdl robots are in a situation like the one 
depicted in figure robot the one carrying a package 
decides to ask robot for assistance and sends a request 
this request is written below as a kqml message and it 
should be interpreted intuitively as robot pick up the 
package located in my up square knowing that you are 
located in my up-right square 
` 
request 
 sender robot 
 receiver robot 
 language packages distribution-language 
 ontology urdl-ontology 
 content pick up u package because ur robot 
´ 
figure robot assistance 
robot understands the content of the request and it can 
use a rule represented by the following constraint 
 ur robot ul robot u package 
 u package 
the above constraint should be interpreted intuitively as 
if robot is situated in robot s up-right square robot 
is situated in robot s up-left square and a package is 
located in robot s up square then a package is located 
in robot s up square 
now problems arise when a lcr robot and a urdl 
robot try to interoperate see figure robot sends a 
request of the form 
` 
request 
 sender robot 
 receiver robot 
 language packages distribution-language 
 ontology lcr-ontology 
 content pick up r robot because c package 
´ 
robot does not understand the content of the request but 
they decide to begin a process of alignment -corresponding 
with a channel c 
 once finished robot searches in th c 
 
for constraints similar to the expected one that is those of 
the form 
 r robot ul robot c package 
c λ package 
where λ ∈ {u r d l ur dr dl ul} from these only 
the following constraints are plausible according to c 
 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure ontology mismatch 
 r robot ul robot c package 
c u package 
 r robot ul robot c package 
c l package 
 r robot ul robot c package 
c dr package 
if subsequently both robots adopting the same roles take 
part in a situation like the one depicted in figure a new 
process of alignment -corresponding with a channel c 
- takes 
place c 
also considers the previous information and hence 
refines c 
 the only constraint from the above ones that 
remains plausible according to c 
is 
 r robot ul robot c package 
c u package 
notice that this constraint is an element of the theory of the 
distributed logic agents communicate in order to cooperate 
successfully and success is guaranteed using constrains of the 
distributed logic 
figure refinement 
 conclusions and further work 
in this paper we have exposed a formal model of semantic 
alignment as a sequence of information-channel refinements 
that are relative to the particular states of the environment 
in which two agents communicate and align their respective 
conceptualisations of these states before us kent and 
kalfoglou and schorlemmer have applied channel 
theory to formalise semantic alignment using also barwise 
and seligman s insight to focus on tokens as the enablers 
of information flow their approach to semantic alignment 
however like most ontology matching mechanisms 
developed to date regardless of whether they follow a functional 
design-time-based approach or an interaction-based 
runtime-based approach still defines semantic alignment in 
terms of a priori design decisions such as the concept 
taxonomy of the ontologies or the external sources brought into 
the alignment process instead the model we have presented 
in this paper makes explicit the particular states of the 
environment in which agents are situated and are attempting 
to gradually align their ontological entities 
in the future our effort will focus on the practical side of 
the situated semantic alignment problem we plan to 
further refine the model presented here e g to include 
pragmatic issues such as termination criteria for the alignment 
process and to devise concrete ontology negotiation 
protocols based on this model that agents may be able to enact 
the formal model exposed in this paper will constitute a 
solid base of future practical results 
acknowledgements 
this work is supported under the upic project sponsored 
by spain s ministry of education and science under grant 
number tin - -c - and also under the 
openknowledge specific targeted research project strep 
sponsored by the european commission under contract 
number fp - marco schorlemmer is supported by a 
ram´on y cajal research fellowship from spain s ministry 
of education and science partially funded by the european 
social fund 
 references 
 j barwise and j seligman information flow the 
logic of distributed systems cambridge university 
press 
 c ghidini and f giunchiglia local models 
semantics or contextual reasoning locality 
compatibility artificial intelligence - 
 
 f giunchiglia and p shvaiko semantic matching 
the knowledge engineering review - 
 
 y kalfoglou and m schorlemmer if-map an 
ontology-mapping method based on information-flow 
theory in journal on data semantics i lncs 
 
 y kalfoglou and m schorlemmer ontology mapping 
the sate of the art the knowledge engineering 
review - 
 r e kent semantic integration in the information 
flow framework in semantic interoperability and 
integration dagstuhl seminar proceedings 
 
 d lenat cyc a large-scale investment in knowledge 
infrastructure communications of the acm 
 
 v l´opez m sabou and e motta powermap 
mapping the real semantic web on the fly 
proceedings of the iswc 
 f mcneill dynamic ontology refinement phd 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
thesis school of informatics the university of 
edinburgh 
 m schorlemmer and y kalfoglou progressive 
ontology alignment for meaning coordination an 
information-theoretic foundation in th int joint 
conf on autonomous agents and multiagent systems 
 
 p shvaiko and j euzenat a survey of schema-based 
matching approaches in journal on data semantics 
iv lncs 
 l steels the origins of ontologies and 
communication conventions in multi-agent systems 
in journal of autonomous agents and multi-agent 
systems - 
 j van diggelen et al anemone an effective 
minimal ontology negotiation environment in th 
int joint conf on autonomous agents and 
multiagent systems 
appendix 
a channel theory terms 
classification is a tuple a tok a typ a a where 
tok a is a set of tokens typ a is a set of types and 
 a is a binary relation between tok a and typ a if 
a a α then a is said to be of type α 
infomorphism f a → b from classifications a to b is 
a contravariant pair of functions f ˆf ˇf where ˆf 
typ a → typ b and ˇf tok b → tok a satisfying 
the following fundamental property 
ˇf b a α iff b b 
ˆf α 
for each token b ∈ tok b and each type α ∈ typ a 
channel consists of two infomorphisms c {fi ai → 
c}i∈{ } with a common codomain c called the core 
of c c tokens are called connections and a connection 
c is said to connect tokens ˇf c and ˇf c 
sum given classifications a and b the sum of a and b 
denoted by a b is the classification with tok a 
b tok a × tok b { a b a ∈ tok a and b ∈ 
tok b } typ a b typ a typ b { i γ 
i and γ ∈ typ a or i and γ ∈ typ b } and 
relation a b defined by 
a b a b α if a a α 
a b a b β if b b β 
given infomorphisms f a → c and g b → c 
the sum f g a b → c is defined on types by 
ˆ f g α ˆf α and ˆ f g β ˆg β and 
on tokens by ˇ f g c ˇf c ˇg c 
theory given a set σ a sequent of σ is a pair γ δ of 
subsets of σ a binary relation between subsets of 
σ is called a consequence relation on σ a theory is a 
pair t σ where is a consequence relation on 
σ a sequent γ δ of σ for which γ δ is called a 
constraint of the theory t t is regular if it satisfies 
 identity α α 
 weakening if γ δ then γ γ δ δ 
 
in fact this is the definition of a binary channel a channel 
can be defined with an arbitrary index set 
 global cut if γ π δ π for each partition 
π π of π i e π ∪ π π and π ∩ π ∅ 
then γ δ 
for all α ∈ σ and all γ γ δ δ π ⊆ σ 
theory generated by a classification let a be a 
classification a token a ∈ tok a satisfies a sequent γ δ 
of typ a provided that if a is of every type in γ then 
it is of some type in δ the theory generated by a 
denoted by th a is the theory typ a a where 
γ a δ if every token in a satisfies γ δ 
local logic is a tuple l tok l typ l l l nl 
where 
 tok l typ l l is a classification denoted by 
cla l 
 typ l l is a regular theory denoted by th l 
 nl is a subset of tok l called the normal tokens 
of l which satisfy all constraints of th l 
a local logic l is sound if every token in cla l is 
normal that is nl tok l l is complete if every 
sequent of typ l satisfied by every normal token is a 
constraint of th l 
local logic generated by a classification given a 
classification a the local logic generated by a written 
log a is the local logic on a i e cla log a 
a with th log a th a and such that all its 
tokens are normal i e nlog a tok a 
inverse image given an infomorphism f a → b and 
a local logic l on b the inverse image of l under 
f denoted f− 
 l is the local logic on a such that 
γ f− l δ if ˆf γ l 
ˆf δ and nf− l ˇf nl 
{a ∈ tok a a ˇf b for some b ∈ nl } 
distributed logic let c {fi ai → c}i∈{ } be a 
channel and l a local logic on its core c the distributed 
logic of c generated by l written dlogc l is the 
inverse image of l under the sum f f 
refinement let c {fi ai → c}i∈{ } and c {fi 
ai → c }i∈{ } be two channels with the same 
component classifications a and a a refinement 
infomorphism from c to c is an infomorphism r c → c 
such that for each i ∈ { } fi r ◦fi i e ˆfi ˆr ◦ ˆfi 
and ˇfi ˇfi ◦ˇr channel c is a refinement of c if there 
exists a refinement infomorphism r from c to c 
b channel theory theorems 
theorem b the logic generated by a classification is 
sound and complete furthermore given a classification a 
and a logic l on a l is sound and complete if and only if 
l log a 
theorem b let l be a logic on a classification b and 
f a → b an infomorphism 
 if l is complete then f− 
 l is complete 
 if l is sound and ˇf is surjective then f− 
 l is sound 
 
all theories considered in this paper are regular 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
combinatorial resource scheduling for multiagent mdps 
dmitri a dolgov michael r james and michael e samples 
ai and robotics group 
technical research toyota technical center usa 
{ddolgov michael r james michael samples} gmail com 
abstract 
optimal resource scheduling in multiagent systems is a 
computationally challenging task particularly when the values 
of resources are not additive we consider the combinatorial 
problem of scheduling the usage of multiple resources among 
agents that operate in stochastic environments modeled as 
markov decision processes mdps in recent years 
efficient resource-allocation algorithms have been developed for 
agents with resource values induced by mdps however this 
prior work has focused on static resource-allocation 
problems where resources are distributed once and then utilized 
in infinite-horizon mdps we extend those existing models 
to the problem of combinatorial resource scheduling where 
agents persist only for finite periods between their 
 predefined arrival and departure times requiring resources only 
for those time periods we provide a computationally 
efficient procedure for computing globally optimal resource 
assignments to agents over time we illustrate and 
empirically analyze the method in the context of a stochastic 
jobscheduling domain 
categories and subject descriptors 
i artificial intelligence problem solving control 
methods and search i artificial intelligence 
distributed artificial intelligence-multiagent systems 
general terms 
algorithms performance design 
 introduction 
the tasks of optimal resource allocation and scheduling 
are ubiquitous in multiagent systems but solving such 
optimization problems can be computationally difficult due to 
a number of factors in particular when the value of a set of 
resources to an agent is not additive as is often the case with 
resources that are substitutes or complements the utility 
function might have to be defined on an exponentially large 
space of resource bundles which very quickly becomes 
computationally intractable further even when each agent has 
a utility function that is nonzero only on a small subset of 
the possible resource bundles obtaining optimal allocation 
is still computationally prohibitive as the problem becomes 
np-complete 
such computational issues have recently spawned several 
threads of work in using compact models of agents 
preferences one idea is to use any structure present in utility 
functions to represent them compactly via for example 
logical formulas an alternative is to directly model 
the mechanisms that define the agents utility functions and 
perform resource allocation directly with these models a 
way of accomplishing this is to model the processes by which 
an agent might utilize the resources and define the utility 
function as the payoff of these processes in particular if 
an agent uses resources to act in a stochastic environment 
its utility function can be naturally modeled with a markov 
decision process whose action set is parameterized by the 
available resources this representation can then be used to 
construct very efficient resource-allocation algorithms that 
lead to an exponential speedup over a straightforward 
optimization problem with flat representations of combinatorial 
preferences 
however this existing work on resource allocation with 
preferences induced by resource-parameterized mdps makes 
an assumption that the resources are only allocated once and 
are then utilized by the agents independently within their 
infinite-horizon mdps this assumption that no reallocation 
of resources is possible can be limiting in domains where 
agents arrive and depart dynamically 
in this paper we extend the work on resource allocation 
under mdp-induced preferences to discrete-time scheduling 
problems where agents are present in the system for finite 
time intervals and can only use resources within these 
intervals in particular agents arrive and depart at arbitrary 
 predefined times and within these intervals use resources 
to execute tasks in finite-horizon mdps we address the 
problem of globally optimal resource scheduling where the 
objective is to find an allocation of resources to the agents 
across time that maximizes the sum of the expected rewards 
that they obtain 
in this context our main contribution is a 
mixed-integerprogramming formulation of the scheduling problem that 
chooses globally optimal resource assignments starting times 
and execution horizons for all agents within their 
arrival 
 - - - - rps c ifaamas 
departure intervals we analyze and empirically compare 
two flavors of the scheduling problem one where agents 
have static resource assignments within their finite-horizon 
mdps and another where resources can be dynamically 
reallocated between agents at every time step 
in the rest of the paper we first lay down the necessary 
groundwork in section and then introduce our model and 
formal problem statement in section in section we 
describe our main result the optimization program for 
globally optimal resource scheduling following the discussion of 
our experimental results on a job-scheduling problem in 
section we conclude in section with a discussion of possible 
extensions and generalizations of our method 
 background 
similarly to the model used in previous work on 
resourceallocation with mdp-induced preferences we define 
the value of a set of resources to an agent as the value of the 
best mdp policy that is realizable given those resources 
however since the focus of our work is on scheduling 
problems and a large part of the optimization problem is to 
decide how resources are allocated in time among agents 
with finite arrival and departure times we model the agents 
planning problems as finite-horizon mdps in contrast to 
previous work that used infinite-horizon discounted mdps 
in the rest of this section we first introduce some 
necessary background on finite-horizon mdps and present a 
linear-programming formulation that serves as the basis for 
our solution algorithm developed in section we also 
outline the standard methods for combinatorial resource 
scheduling with flat resource values which serve as a comparison 
benchmark for the new model developed here 
 markov decision processes 
a stationary finite-domain discrete-time mdp see for 
example for a thorough and detailed development can 
be described as s a p r where s is a finite set of 
system states a is a finite set of actions that are available to 
the agent p is a stationary stochastic transition function 
where p σ s a is the probability of transitioning to state σ 
upon executing action a in state s r is a stationary reward 
function where r s a specifies the reward obtained upon 
executing action a in state s 
given such an mdp a decision problem under a finite 
horizon t is to choose an optimal action at every time step 
to maximize the expected value of the total reward accrued 
during the agent s finite lifetime the agent s optimal 
policy is then a function of current state s and the time until 
the horizon an optimal policy for such a problem is to act 
greedily with respect to the optimal value function defined 
recursively by the following system of finite-time bellman 
equations 
v s t max 
a 
r s a 
x 
σ 
p σ s a v σ t 
∀s ∈ s t ∈ t − 
v s t ∀s ∈ s 
where v s t is the optimal value of being in state s at time 
t ∈ t 
this optimal value function can be easily computed using 
dynamic programming leading to the following optimal 
policy π where π s a t is the probability of executing action 
a in state s at time t 
π s a t 
 
 a argmaxa r s a 
p 
σ p σ s a v σ t 
 otherwise 
the above is the most common way of computing the 
optimal value function and therefore an optimal policy for 
a finite-horizon mdp however we can also formulate the 
problem as the following linear program similarly to the 
dual lp for infinite-horizon discounted mdps 
max 
x 
s 
x 
a 
r s a 
x 
t 
x s a t 
subject to 
x 
a 
x σ a t 
x 
s a 
p σ s a x s a t ∀σ t ∈ t − 
x 
a 
x s a α s ∀s ∈ s 
 
where α s is the initial distribution over the state space and 
x is the non-stationary occupation measure x s a t ∈ 
 is the total expected number of times action a is 
executed in state s at time t an optimal non-stationary 
policy is obtained from the occupation measure as follows 
π s a t x s a t 
x 
a 
x s a t ∀s ∈ s t ∈ t 
note that the standard unconstrained finite-horizon mdp 
as described above always has a uniformly-optimal 
solution optimal for any initial distribution α s therefore 
an optimal policy can be obtained by using an arbitrary 
constant α s in particular α s will result in 
x s a t π s a t 
however for mdps with resource constraints as defined 
below in section uniformly-optimal policies do not in 
general exist in such cases α becomes a part of the 
problem input and a resulting policy is only optimal for that 
particular α this result is well known for infinite-horizon 
mdps with various types of constraints and it also 
holds for our finite-horizon model which can be easily 
established via a line of reasoning completely analogous to the 
arguments in 
 combinatorial resource scheduling 
a straightforward approach to resource scheduling for a 
set of agents m whose values for the resources are induced 
by stochastic planning problems in our case finite-horizon 
mdps would be to have each agent enumerate all possible 
resource assignments over time and for each one compute 
its value by solving the corresponding mdp then each 
agent would provide valuations for each possible resource 
bundle over time to a centralized coordinator who would 
compute the optimal resource assignments across time based 
on these valuations 
when resources can be allocated at different times to 
different agents each agent must submit valuations for 
every combination of possible time horizons let each agent 
m ∈ m execute its mdp within the arrival-departure time 
interval τ ∈ τa 
m τd 
m hence agent m will execute an mdp 
with time horizon no greater than tm τd 
m−τa 
m let bτ be 
the global time horizon for the problem before which all of 
the agents mdps must finish we assume τd 
m bτ ∀m ∈ m 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
for the scheduling problem where agents have static 
resource requirements within their finite-horizon mdps the 
agents provide a valuation for each resource bundle for each 
possible time horizon from tm that they may use let 
ω be the set of resources to be allocated among the agents 
an agent will get at most one resource bundle for one of the 
time horizons let the variable ψ ∈ ψm enumerate all 
possible pairs of resource bundles and time horizons for agent 
m so there are ω 
× tm values for ψ the space of bundles 
is exponential in the number of resource types ω 
the agent m must provide a value vψ 
m for each ψ and 
the coordinator will allocate at most one ψ resource time 
horizon pair to each agent this allocation is expressed as 
an indicator variable zψ 
m ∈ { } that shows whether ψ is 
assigned to agent m for time τ and resource ω the function 
nm ψ τ ω ∈ { } indicates whether the bundle in ψ uses 
resource ω at time τ we make the assumption that agents 
have binary resource requirements this allocation problem 
is np-complete even when considering only a single time 
step and its difficulty increases significantly with multiple 
time steps because of the increasing number of values of ψ 
the problem of finding an optimal allocation that satisfies 
the global constraint that the amount of each resource ω 
allocated to all agents does not exceed the available amount 
bϕ ω can be expressed as the following integer program 
max 
x 
m∈m 
x 
ψ∈ψm 
zψ 
mvψ 
m 
subject to 
x 
ψ∈ψm 
zψ 
m ≤ ∀m ∈ m 
x 
m∈m 
x 
ψ∈ψm 
zψ 
mnm ψ τ ω ≤ bϕ ω ∀τ ∈ bτ ∀ω ∈ ω 
 
the first constraint in equation says that no agent can 
receive more than one bundle and the second constraint 
ensures that the total assignment of resource ω does not at 
any time exceed the resource bound 
for the scheduling problem where the agents are able to 
dynamically reallocate resources each agent must specify 
a value for every combination of bundles and time steps 
within its time horizon let the variable ψ ∈ ψm in this case 
enumerate all possible resource bundles for which at most 
one bundle may be assigned to agent m at each time step 
therefore in this case there are 
p 
t∈ tm ω 
 t 
∼ ω tm 
possibilities of resource bundles assigned to different time 
slots for the tm different time horizons 
the same set of equations can be used to solve this 
dynamic scheduling problem but the integer program is 
different because of the difference in how ψ is defined in this 
case the number of ψ values is exponential in each agent s 
planning horizon tm resulting in a much larger program 
this straightforward approach to solving both of these 
scheduling problems requires an enumeration and solution 
of either ω 
tm static allocation or 
p 
t∈ tm ω t 
 
 dynamic reallocation mdps for each agent which very quickly 
becomes intractable with the growth of the number of 
resources ω or the time horizon tm 
 model and problem statement 
we now formally introduce our model of the 
resourcescheduling problem the problem input consists of the 
following components 
 m ω bϕ τa 
m τd 
m bτ are as defined above in section 
 {θm} {s a pm rm αm} are the mdps of all agents 
m ∈ m without loss of generality we assume that state 
and action spaces of all agents are the same but each has 
its own transition function pm reward function rm and 
initial conditions αm 
 ϕm a×ω → { } is the mapping of actions to resources 
for agent m ϕm a ω indicates whether action a of agent 
m needs resource ω an agent m that receives a set of 
resources that does not include resource ω cannot execute 
in its mdp policy any action a for which ϕm a ω we 
assume all resource requirements are binary as discussed 
below in section this assumption is not limiting 
given the above input the optimization problem we 
consider is to find the globally optimal-maximizing the sum 
of expected rewards-mapping of resources to agents for all 
time steps δ τ × m × ω → { } a solution is feasible 
if the corresponding assignment of resources to the agents 
does not violate the global resource constraint 
x 
m 
δm τ ω ≤ bϕ ω ∀ω ∈ ω τ ∈ bτ 
we consider two flavors of the resource-scheduling 
problem the first formulation restricts resource assignments to 
the space where the allocation of resources to each agent is 
static during the agent s lifetime the second formulation 
allows reassignment of resources between agents at every time 
step within their lifetimes 
figure depicts a resource-scheduling problem with three 
agents m {m m m } three resources ω {ω ω ω } 
and a global problem horizon of bτ the agents arrival 
and departure times are shown as gray boxes and are { } 
{ } and { } respectively a solution to this problem 
is shown via horizontal bars within each agents box where 
the bars correspond to the allocation of the three resource 
types figure a shows a solution to a static scheduling 
problem according to the shown solution agent m begins the 
execution of its mdp at time τ and has a lock on all 
three resources until it finishes execution at time τ note 
that agent m relinquishes its hold on the resources before 
its announced departure time of τd 
m 
 ostensibly because 
other agents can utilize the resources more effectively thus 
at time τ resources ω and ω are allocated to agent 
m who then uses them to execute its mdp using only 
actions supported by resources ω and ω until time τ 
agent m holds resource ω during the interval τ ∈ 
figure b shows a possible solution to the dynamic version 
of the same problem there resources can be reallocated 
between agents at every time step for example agent m 
gives up its use of resource ω at time τ although it 
continues the execution of its mdp until time τ notice 
that an agent is not allowed to stop and restart its mdp so 
agent m is only able to continue executing in the interval 
τ ∈ if it has actions that do not require any resources 
 ϕm a ω 
clearly the model and problem statement described above 
make a number of assumptions about the problem and the 
desired solution properties we discuss some of those 
assumptions and their implications in section 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 a b 
figure illustration of a solution to a resource-scheduling problem with three agents and three resources a static 
resource assignments resource assignments are constant within agents lifetimes b dynamic assignment resource 
assignments are allowed to change at every time step 
 resource scheduling 
our resource-scheduling algorithm proceeds in two stages 
first we perform a preprocessing step that augments the 
agent mdps this process is described in section 
second using these augmented mdps we construct a global 
optimization problem which is described in section 
 augmenting agents mdps 
in the model described in the previous section we assume 
that if an agent does not possess the necessary resources to 
perform actions in its mdp its execution is halted and the 
agent leaves the system in other words the mdps cannot 
be paused and resumed for example in the problem 
shown in figure a agent m releases all resources after time 
τ at which point the execution of its mdp is halted 
similarly agents m and m only execute their mdps in the 
intervals τ ∈ and τ ∈ respectively therefore an 
important part of the global decision-making problem is to 
decide the window of time during which each of the agents 
is active i e executing its mdp 
to accomplish this we augment each agent s mdp with 
two new states start and finish states sb 
 sf 
 
respectively and a new start stop action a∗ 
 as illustrated in 
figure the idea is that an agent stays in the start state 
sb 
until it is ready to execute its mdp at which point it 
performs the start stop action a∗ 
and transitions into the 
state space of the original mdp with the transition 
probability that corresponds to the original initial distribution 
α s for example in figure a for agent m this would 
happen at time τ once the agent gets to the end of its 
activity window time τ for agent m in figure a it 
performs the start stop action which takes it into the sink 
finish state sf 
at time τ 
more precisely given an mdp s a pm rm αm we 
define an augmented mdp s a pm rm αm as follows 
s s ∪ sb 
∪ sf 
 a a ∪ a∗ 
 
p s sb 
 a∗ 
 α s ∀s ∈ s p sb 
 sb 
 a ∀a ∈ a 
p sf 
 s a∗ 
 ∀s ∈ s 
p σ s a p σ s a ∀s σ ∈ s a ∈ a 
r sb 
 a r sf 
 a ∀a ∈ a 
r s a r s a ∀s ∈ s a ∈ a 
α sb 
 α s ∀s ∈ s 
where all non-specified transition probabilities are assumed 
to be zero further in order to account for the new starting 
state we begin the mdp one time-step earlier setting τa 
m ← 
τa 
m − this will not affect the resource allocation due to 
the resource constraints only being enforced for the original 
mdp states as will be discussed in the next section for 
example the augmented mdps shown in figure b which 
starts in state sb 
at time τ would be constructed from 
an mdp with original arrival time τ figure b also 
shows a sample trajectory through the state space the agent 
starts in state sb 
 transitions into the state space s of the 
original mdp and finally exists into the sink state sf 
 
note that if we wanted to model a problem where agents 
could pause their mdps at arbitrary time steps which might 
be useful for domains where dynamic reallocation is 
possible we could easily accomplish this by including an extra 
action that transitions from each state to itself with zero 
reward 
 milp for resource scheduling 
given a set of augmented mdps as defined above the 
goal of this section is to formulate a global optimization 
program that solves the resource-scheduling problem in this 
section and below all mdps are assumed to be the 
augmented mdps as defined in section 
our approach is similar to the idea used in we 
begin with the linear-program formulation of agents mdps 
 and augment it with constraints that ensure that the 
corresponding resource allocation across agents and time is 
valid the resulting optimization problem then 
simultaneously solves the agents mdps and resource-scheduling 
problems in the rest of this section we incrementally develop a 
mixed integer program milp that achieves this 
in the absence of resource constraints the agents 
finitehorizon mdps are completely independent and the globally 
optimal solution can be trivially obtained via the following 
lp which is simply an aggregation of single-agent 
finitehorizon lps 
max 
x 
m 
x 
s 
x 
a 
rm s a 
x 
t 
xm s a t 
subject to 
x 
a 
xm σ a t 
x 
s a 
pm σ s a xm s a t 
∀m ∈ m σ ∈ s t ∈ tm − 
x 
a 
xm s a αm s ∀m ∈ m s ∈ s 
 
where xm s a t is the occupation measure of agent m and 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 a b 
figure illustration of augmenting an mdp to allow for variable starting and stopping times a left the original 
two-state mdp with a single action right the augmented mdp with new states sb and sf and the new action a∗ 
 note that the origianl transitions are not changed in the augmentation process b the augmented mdp displayed as 
a trajectory through time grey lines indicate all transitions while black lines indicate a given trajectory 
objective function 
 sum of expected rewards over all agents 
max 
x 
m 
x 
s 
x 
a 
rm s a 
x 
t 
xm s a t 
meaning implication linear constraints 
tie x to θ agent is 
only active when 
occupation measure is nonzero 
in original mdp states 
θm τ ⇒ xm s a τ −τa 
m 
∀s ∈ {sb 
 sf 
} a ∈ a 
x 
s ∈{sb sf } 
x 
a 
xm s a t ≤ θm τa 
m t − 
∀m ∈ m ∀t ∈ tm 
 
agent can only be active 
in τ ∈ τa 
m τd 
m θm τ ∀m ∈ m τ ∈ τa 
m τd 
m 
cannot use resources 
when not active 
θm τ ⇒ δm τ ω 
∀τ ∈ bτ ω ∈ ω δm τ ω ≤ θm τ ∀m ∈ m τ ∈ bτ ω ∈ ω 
tie x to δ nonzero x 
forces corresponding δ 
to be nonzero 
δm τ ω ϕm a ω ⇒ 
xm s a τ − τa 
m 
∀s ∈ {sb 
 sf 
} 
 a 
x 
a 
ϕm a ω 
x 
s ∈{sb sf } 
xm s a t ≤ δm t τa 
m − ω 
∀m ∈ m ω ∈ ω t ∈ tm 
 
resource bounds 
x 
m 
δm τ ω ≤ bϕ ω ∀ω ∈ ω τ ∈ bτ 
agent cannot change 
resources while 
active only enabled for 
scheduling with static 
assignments 
θm τ and θm τ ⇒ 
δm τ ω δm τ ω 
δm τ ω − z − θm τ ≤ 
δm τ ω z − θm τ 
δm τ ω z − θm τ ≥ 
δm τ ω − z − θm τ 
∀m ∈ m ω ∈ ω τ ∈ bτ 
 
table milp for globally optimal resource scheduling 
tm τd 
m − τa 
m is the time horizon for the agent s mdp 
using this lp as a basis we augment it with constraints 
that ensure that the resource usage implied by the agents 
occupation measures {xm} does not violate the global 
resource requirements bϕ at any time step τ ∈ bτ to 
formulate these resource constraints we use the following binary 
variables 
 δm τ ω { } ∀m ∈ m τ ∈ bτ ω ∈ ω which 
serve as indicator variables that define whether agent m 
possesses resource ω at time τ these are analogous to 
the static indicator variables used in the one-shot static 
resource-allocation problem in 
 θm { } ∀m ∈ m τ ∈ bτ are indicator variables 
that specify whether agent m is active i e executing 
its mdp at time τ 
the meaning of resource-usage variables δ is illustrated in 
figure δm τ ω only if resource ω is allocated to 
agent m at time τ the meaning of the activity 
indicators θ is illustrated in figure b when agent m is in either 
the start state sb 
or the finish state sf 
 the corresponding 
θm but once the agent becomes active and enters one 
of the other states we set θm this meaning of θ can be 
enforced with a linear constraint that synchronizes the 
values of the agents occupation measures xm and the activity 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
indicators θ as shown in in table 
another constraint we have to add-because the activity 
indicators θ are defined on the global timeline τ-is to 
enforce the fact that the agent is inactive outside of its 
arrivaldeparture window this is accomplished by constraint in 
table 
furthermore agents should not be using resources while 
they are inactive this constraint can also be enforced via a 
linear inequality on θ and δ as shown in 
constraint sets the value of θ to match the policy 
defined by the occupation measure xm in a similar fashion 
we have to make sure that the resource-usage variables δ are 
also synchronized with the occupation measure xm this is 
done via constraint in table which is nearly identical 
to the analogous constraint from 
after implementing the above constraint which enforces 
the meaning of δ we add a constraint that ensures that the 
agents resource usage never exceeds the amounts of 
available resources this condition is also trivially expressed as 
a linear inequality in table 
finally for the problem formulation where resource 
assignments are static during a lifetime of an agent we add a 
constraint that ensures that the resource-usage variables δ 
do not change their value while the agent is active θ 
this is accomplished via the linear constraint where 
z ≥ is a constant that is used to turn off the constraints 
when θm τ or θm τ this constraint is not 
used for the dynamic problem formulation where resources 
can be reallocated between agents at every time step 
to summarize table together with the 
conservationof-flow constraints from defines the milp that 
simultaneously computes an optimal resource assignment for all 
agents across time as well as optimal finite-horizon mdp 
policies that are valid under that resource assignment 
as a rough measure of the complexity of this milp let 
us consider the number of optimization variables and 
constraints let tm 
p 
tm 
p 
m τa 
m − τd 
m be the sum 
of the lengths of the arrival-departure windows across all 
agents then the number of optimization variables is 
tm bτ m ω bτ m 
tm of which are continuous xm and bτ m ω bτ m are 
binary δ and θ however notice that all but tm m of 
the θ are set to zero by constraint which also 
immediately forces all but tm m ω of the δ to be zero via the 
constraints the number of constraints not including 
the degenerate constraints in in the milp is 
tm tm ω bτ ω bτ m ω 
despite the fact that the complexity of the milp is in the 
worst case exponential 
in the number of binary variables 
the complexity of this milp is significantly exponentially 
lower than that of the milp with flat utility functions 
described in section this result echos the efficiency gains 
reported in for single-shot resource-allocation problems 
but is much more pronounced because of the explosion of 
the flat utility representation due to the temporal aspect of 
the problem recall the prohibitive complexity of the 
combinatorial optimization in section we empirically analyze 
the performance of this method in section 
 
strictly speaking solving milps to optimality is 
npcomplete in the number of integer variables 
 experimental results 
although the complexity of solving milps is in the worst 
case exponential in the number of integer variables there 
are many efficient methods for solving milps that allow 
our algorithm to scale well for parameters common to 
resource allocation and scheduling problems in particular 
this section introduces a problem domain-the repairshop 
problem-used to empirically evaluate our algorithm s 
scalability in terms of the number of agents m the number of 
shared resources ω and the varied lengths of global time 
bτ during which agents may enter and exit the system 
the repairshop problem is a simple parameterized mdp 
adopting the metaphor of a vehicular repair shop agents 
in the repair shop are mechanics with a number of 
independent tasks that yield reward only when completed in our 
mdp model of this system actions taken to advance through 
the state space are only allowed if the agent holds certain 
resources that are publicly available to the shop these 
resources are in finite supply and optimal policies for the shop 
will determine when each agent may hold the limited 
resources to take actions and earn individual rewards each 
task to be completed is associated with a single action 
although the agent is required to repeat the action numerous 
times before completing the task and earning a reward 
this model was parameterized in terms of the number 
of agents in the system the number of different types of 
resources that could be linked to necessary actions a global 
time during which agents are allowed to arrive and depart 
and a maximum length for the number of time steps an agent 
may remain in the system 
all datapoints in our experiments were obtained with 
evaluations using cplex to solve the milps on a 
pentium computer with gb of ram trials were conducted on 
both the static and the dynamic version of the 
resourcescheduling problem as defined earlier 
figure shows the runtime and policy value for 
independent modifications to the parameter set the top row 
shows how the solution time for the milp scales as we 
increase the number of agents m the global time horizon bτ 
and the number of resources ω increasing the number of 
agents leads to exponential complexity scaling which is to 
be expected for an np-complete problem however 
increasing the global time limit bτ or the total number of resource 
types ω -while holding the number of agents 
constantdoes not lead to decreased performance this occurs because 
the problems get easier as they become under-constrained 
which is also a common phenomenon for np-complete 
problems we also observe that the solution to the dynamic 
version of the problem can often be computed much faster than 
the static version 
the bottom row of figure shows the joint policy value 
of the policies that correspond to the computed optimal 
resource-allocation schedules we can observe that the 
dynamic version yields higher reward as expected since the 
reward for the dynamic version is always no less than the 
reward of the static version we should point out that these 
graphs should not be viewed as a measure of performance of 
two different algorithms both algorithms produce optimal 
solutions but to different problems but rather as 
observations about how the quality of optimal solutions change as 
more flexibility is allowed in the reallocation of resources 
figure shows runtime and policy value for trials in which 
common input variables are scaled together this allows 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
− 
 
− 
 
− 
 
 
 
 
 
 
 
 
 
 
number of agents m 
cputime sec 
 ω τ 
static 
dynamic 
 
 
− 
 
− 
 
 
 
 
 
 
 
 
global time boundary τ 
cputime sec 
 m ω 
static 
dynamic 
 
 
− 
 
− 
 
 
 
 
 
 
number of resources ω 
cputime sec 
 m τ 
static 
dynamic 
 
 
 
 
 
 
 
 
 
number of agents m 
value 
 ω τ 
static 
dynamic 
 
 
 
 
 
 
 
 
 
 
 
 
global time boundary τ 
value 
 m ω 
static 
dynamic 
 
 
 
 
 
 
 
 
 
 
 
number of resources ω 
value 
 m τ 
static 
dynamic 
figure evaluation of our milp for variable numbers of agents column lengths of global-time window column 
 and numbers of resource types column top row shows cpu time and bottom row shows the joint reward of 
agents mdp policies error bars show the st and rd quartiles and 
 
 
− 
 
− 
 
− 
 
 
 
 
 
 
 
 
number of agents m 
cputime sec 
τ m 
static 
dynamic 
 
 
− 
 
− 
 
− 
 
 
 
 
 
 
 
 
 
 
number of agents m 
cputime sec 
 ω m 
static 
dynamic 
 
 
− 
 
− 
 
− 
 
 
 
 
 
 
 
 
 
 
number of agents m 
cputime sec 
 ω m 
static 
dynamic 
 
 
 
 
 
 
 
 
 
 
 
 
number of agents m 
value 
τ m 
static 
dynamic 
 
 
 
 
 
 
 
 
 
 
 
number of agents m 
value 
 ω m 
static 
dynamic 
 
 
 
 
 
 
 
number of agents m 
value 
 ω m 
static 
dynamic 
figure evaluation of our milp using correlated input variables the left column tracks the performance and cpu 
time as the number of agents and global-time window increase together bτ m the middle and the right column 
track the performance and cpu time as the number of resources and the number of agents increase together as 
 ω m and ω m respectively error bars show the st and rd quartiles and 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
us to explore domains where the total number of agents 
scales proportionally to the total number of resource types 
or the global time horizon while keeping constant the 
average agent density per unit of global time or the average 
number of resources per agent which commonly occurs in 
real-life applications 
overall we believe that these experimental results 
indicate that our milp formulation can be used to effectively 
solve resource-scheduling problems of nontrivial size 
 discussion and conclusions 
throughout the paper we have made a number of 
assumptions in our model and solution algorithm we discuss 
their implications below 
 continual execution we assume that once an agent 
stops executing its mdp transitions into state sf 
 it 
exits the system and cannot return it is easy to relax 
this assumption for domains where agents mdps can be 
paused and restarted all that is required is to include an 
additional pause action which transitions from a given 
state back to itself and has zero reward 
 indifference to start time we used a reward model 
where agents rewards depend only on the time horizon 
of their mdps and not the global start time this is a 
consequence of our mdp-augmentation procedure from 
section it is easy to extend the model so that the 
agents incur an explicit penalty for idling by assigning a 
non-zero negative reward to the start state sb 
 
 binary resource requirements for simplicity we have 
assumed that resource costs are binary ϕm a ω { } 
but our results generalize in a straightforward manner to 
non-binary resource mappings analogously to the 
procedure used in 
 cooperative agents the optimization procedure 
discussed in this paper was developed in the context of 
cooperative agents but it can also be used to design a 
mechanism for scheduling resources among self-interested agents 
this optimization procedure can be embedded in a 
vickreyclarke-groves auction completely analogously to the way 
it was done in in fact all the results of about the 
properties of the auction and information privacy directly 
carry over to the scheduling domain discussed in this 
paper requiring only slight modifications to deal with 
finitehorizon mdps 
 known deterministic arrival and departure times 
finally we have assumed that agents arrival and 
departure times τa 
m and τd 
m are deterministic and known a 
priori this assumption is fundamental to our solution 
method while there are many domains where this 
assumption is valid in many cases agents arrive and 
depart dynamically and their arrival and departure times 
can only be predicted probabilistically leading to online 
resource-allocation problems in particular in the case of 
self-interested agents this becomes an interesting version 
of an online-mechanism-design problem 
in summary we have presented an milp formulation for 
the combinatorial resource-scheduling problem where agents 
values for possible resource assignments are defined by 
finitehorizon mdps this result extends previous work 
on static one-shot resource allocation under mdp-induced 
preferences to resource-scheduling problems with a temporal 
aspect as such this work takes a step in the direction of 
designing an online mechanism for agents with combinatorial 
resource preferences induced by stochastic planning 
problems relaxing the assumption about deterministic arrival 
and departure times of the agents is a focus of our future 
work 
we would like to thank the anonymous reviewers for their 
insightful comments and suggestions 
 references 
 e altman and a shwartz adaptive control of 
constrained markov chains criteria and policies 
annals of operations research special issue on 
markov decision processes - 
 r bellman dynamic programming princeton 
university press 
 c boutilier solving concisely expressed combinatorial 
auction problems in proc of aaai- pages 
 - 
 c boutilier and h h hoos bidding languages for 
combinatorial auctions in proc of ijcai- pages 
 - 
 d dolgov integrated resource allocation and 
planning in stochastic multiagent environments phd 
thesis computer science department university of 
michigan february 
 d a dolgov and e h durfee optimal resource 
allocation and policy formulation in loosely-coupled 
markov decision processes in proc of icaps- 
pages - june 
 d a dolgov and e h durfee computationally 
efficient combinatorial auctions for resource allocation 
in weakly-coupled mdps in proc of aamas- 
new york ny usa acm press 
 d a dolgov and e h durfee resource allocation 
among agents with preferences induced by factored 
mdps in proc of aamas- 
 k larson and t sandholm mechanism design and 
deliberative agents in proc of aamas- pages 
 - new york ny usa acm press 
 n nisan bidding and allocation in combinatorial 
auctions in electronic commerce 
 d c parkes and s singh an mdp-based approach 
to online mechanism design in proc of the 
seventeenths annual conference on neural 
information processing systems nips- 
 d c parkes s singh and d yanovsky 
approximately efficient online mechanism design in 
proc of the eighteenths annual conference on neural 
information processing systems nips- 
 m l puterman markov decision processes john 
wiley sons new york 
 m h rothkopf a pekec and r m harstad 
computationally manageable combinational auctions 
management science - 
 t sandholm an algorithm for optimal winner 
determination in combinatorial auctions in proc of 
ijcai- pages - san francisco ca usa 
 morgan kaufmann publishers inc 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the logic negotiation model 
carles sierra 
institut d investigacio en intel ligencia artificial 
spanish scientific research council uab 
 bellaterra catalonia spain 
sierra iiia csic es 
john debenham 
faculty of information technology 
university of technology sydney 
nsw australia 
debenham it uts edu au 
abstract 
successful negotiators prepare by determining their position 
along five dimensions legitimacy options goals 
independence and commitment logic we introduce a 
negotiation model based on these dimensions and on two primitive 
concepts intimacy degree of closeness and balance degree 
of fairness the intimacy is a pair of matrices that 
evaluate both an agent s contribution to the relationship and 
its opponent s contribution each from an information view 
and from a utilitarian view across the five logic 
dimensions the balance is the difference between these 
matrices a relationship strategy maintains a target intimacy for 
each relationship that an agent would like the relationship to 
move towards in future the negotiation strategy maintains 
a set of options that are in-line with the current intimacy 
level and then tactics wrap the options in argumentation 
with the aim of attaining a successful deal and 
manipulating the successive negotiation balances towards the target 
intimacy 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence-multiagent systems 
general terms 
theory 
 introduction 
in this paper we propose a new negotiation model to deal 
with long term relationships that are founded on successive 
negotiation encounters the model is grounded on results 
from business and psychological studies and 
acknowledges that negotiation is an information exchange 
process as well as a utility exchange process we 
believe that if agents are to succeed in real application domains 
they have to reconcile both views informational and 
gametheoretical our aim is to model trading scenarios where 
agents represent their human principals and thus we want 
their behaviour to be comprehensible by humans and to 
respect usual human negotiation procedures whilst being 
consistent with and somehow extending game theoretical and 
information theoretical results in this sense agents are not 
just utility maximisers but aim at building long lasting 
relationships with progressing levels of intimacy that determine 
what balance in information and resource sharing is 
acceptable to them these two concepts intimacy and balance are 
key in the model and enable us to understand competitive 
and co-operative game theory as two particular theories of 
agent relationships i e at different intimacy levels these 
two theories are too specific and distinct to describe how 
a business relationship might grow because interactions 
have some aspects of these two extremes on a continuum in 
which for example agents reveal increasing amounts of 
private information as their intimacy grows we don t follow 
the co-opetition aproach where co-operation and 
competition depend on the issue under negotiation but instead 
we belief that the willingness to co-operate compete affect 
all aspects in the negotiation process negotiation strategies 
can naturally be seen as procedures that select tactics used 
to attain a successful deal and to reach a target intimacy 
level it is common in human settings to use tactics that 
compensate for unbalances in one dimension of a 
negotiation with unbalances in another dimension in this sense 
humans aim at a general sense of fairness in an interaction 
in section we outline the aspects of human negotiation 
modelling that we cover in this work then in section 
we introduce the negotiation language section explains 
in outline the architecture and the concepts of intimacy and 
balance and how they influence the negotiation section 
contains a description of the different metrics used in the 
agent model including intimacy finally section outlines 
how strategies and tactics use the logic framework 
intimacy and balance 
 human negotiation 
before a negotiation starts human negotiators prepare the 
dialogic exchanges that can be made along the five logic 
dimensions 
 legitimacy what information is relevant to the 
negotiation process what are the persuasive arguments 
about the fairness of the options 
 
 - - - - rps c ifaamas 
 options what are the possible agreements we can 
accept 
 goals what are the underlying things we need or care 
about what are our goals 
 independence what will we do if the negotiation fails 
what alternatives have we got 
 commitment what outstanding commitments do we 
have 
negotiation dialogues in this context exchange 
dialogical moves i e messages with the intention of getting 
information about the opponent or giving away information 
about us along these five dimensions request for 
information propose options inform about interests issue promises 
appeal to standards a key part of any negotiation process 
is to build a model of our opponent s along these 
dimensions all utterances agents make during a negotiation give 
away information about their current logic model that 
is about their legitimacy options goals independence and 
commitments also several utterances can have a 
utilitarian interpretation in the sense that an agent can associate 
a preferential gain to them for instance an offer may 
inform our negotiation opponent about our willingness to sign 
a contract in the terms expressed in the offer and at the 
same time the opponent can compute what is its associated 
expected utilitarian gain these two views 
informationbased and utility-based are central in the model proposed 
in this paper 
 intimacy and balance in relationships 
there is evidence from psychological studies that humans 
seek a balance in their negotiation relationships the 
classical view is that people perceive resource allocations as 
being distributively fair i e well balanced if they are 
proportional to inputs or contributions i e equitable 
however more recent studies show that humans follow 
a richer set of norms of distributive justice depending on 
their intimacy level equity equality and need equity 
being the allocation proportional to the effort e g the profit 
of a company goes to the stock holders proportional to their 
investment equality being the allocation in equal amounts 
 e g two friends eat the same amount of a cake cooked by 
one of them and need being the allocation proportional to 
the need for the resource e g in case of food scarcity a 
mother gives all food to her baby for instance if we are in 
a purely economic setting low intimacy we might request 
equity for the options dimension but could accept equality 
in the goals dimension 
the perception of a relation being in balance i e fair 
depends strongly on the nature of the social relationships 
between individuals i e the intimacy level in purely 
economical relationships e g business equity is perceived as 
more fair in relations where joint action or fostering of social 
relationships are the goal e g friends equality is perceived 
as more fair and in situations where personal development 
or personal welfare are the goal e g family allocations are 
usually based on need 
we believe that the perception of balance in dialogues in 
negotiation or otherwise is grounded on social relationships 
and that every dimension of an interaction between humans 
can be correlated to the social closeness or intimacy 
between the parties involved according to the previous 
studies the more intimacy across the five logic dimensions the 
more the need norm is used and the less intimacy the more 
the equity norm is used this might be part of our social 
evolution there is ample evidence that when human 
societies evolved from a hunter-gatherer structure 
to a 
shelterbased one 
the probability of survival increased when food 
was scarce 
in this context we can clearly see that for instance 
families exchange not only goods but also information and 
knowledge based on need and that few families would consider 
their relationships as being unbalanced and thus unfair 
when there is a strong asymmetry in the exchanges a mother 
explaining everything to her children or buying toys does 
not expect reciprocity in the case of partners there is some 
evidence that the allocations of goods and burdens i e 
positive and negative utilities are perceived as fair or in 
balance based on equity for burdens and equality for goods 
see table for some examples of desired balances along the 
logic dimensions 
the perceived balance in a negotiation dialogue allows 
negotiators to infer information about their opponent about 
its logic stance and to compare their relationships with 
all negotiators for instance if we perceive that every time 
we request information it is provided and that no significant 
questions are returned or no complaints about not 
receiving information are given then that probably means that 
our opponent perceives our social relationship to be very 
close alternatively we can detect what issues are causing 
a burden to our opponent by observing an imbalance in the 
information or utilitarian senses on that issue 
 communication model 
 ontology 
in order to define a language to structure agent dialogues we 
need an ontology that includes a minimum repertoire of 
elements a set of concepts e g quantity quality material 
organised in a is-a hierarchy e g platypus is a mammal 
australian-dollar is a currency and a set of relations over 
these concepts e g price beer aud 
we model 
ontologies following an algebraic approach as 
an ontology is a tuple o c r ≤ σ where 
 c is a finite set of concept symbols including basic 
data types 
 r is a finite set of relation symbols 
 ≤ is a reflexive transitive and anti-symmetric relation 
on c a partial order 
 σ r → c 
is the function assigning to each relation 
symbol its arity 
 
in its purest form individuals in these societies collect food 
and consume it when and where it is found this is a pure 
equity sharing of the resources the gain is proportional to 
the effort 
 
in these societies there are family units around a shelter 
that represent the basic food sharing structure usually 
food is accumulated at the shelter for future use then the 
food intake depends more on the need of the members 
 
usually a set of axioms defined over the concepts and 
relations is also required we will omit this here 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
element a new trading partner my butcher my boss my partner my children 
legitimacy equity equity equity equality need 
options equity equity equity mixeda 
need 
goals equity need equity need need 
independence equity equity equality need need 
commitment equity equity equity mixed need 
a 
equity on burden equality on good 
table some desired balances sense of fairness examples depending on the relationship 
where ≤ is the traditional is-a hierarchy to simplify 
computations in the computing of probability distributions we 
assume that there is a number of disjoint is-a trees covering 
different ontological spaces e g a tree for types of fabric 
a tree for shapes of clothing and so on r contains 
relations between the concepts in the hierarchy this is needed 
to define  objects e g deals that are defined as a tuple of 
issues 
the semantic distance between concepts within an 
ontology depends on how far away they are in the structure 
defined by the ≤ relation semantic distance plays a 
fundamental role in strategies for information-based agency how 
signed contracts commit · about objects in a particular 
semantic region and their execution done · affect our 
decision making process about signing future contracts in 
nearby semantic regions is crucial to modelling the common 
sense that human beings apply in managing trading 
relationships a measure bases the semantic similarity 
between two concepts on the path length induced by ≤ more 
distance in the ≤ graph means less semantic similarity and 
the depth of the subsumer concept common ancestor in the 
shortest path between the two concepts the deeper in the 
hierarchy the closer the meaning of the concepts semantic 
similarity is then defined as 
sim c c e−κ l 
· 
eκ h 
− e−κ h 
eκ h e−κ h 
where l is the length i e number of hops of the 
shortest path between the concepts h is the depth of the deepest 
concept subsuming both concepts and κ and κ are 
parameters scaling the contributions of the shortest path length 
and the depth respectively 
 language 
the shape of the language that α uses to represent the 
information received and the content of its dialogues depends on 
two fundamental notions first when agents interact within 
an overarching institution they explicitly or implicitly accept 
the norms that will constrain their behaviour and accept 
the established sanctions and penalties whenever norms are 
violated second the dialogues in which α engages are built 
around two fundamental actions i passing information 
and ii exchanging proposals and contracts a contract 
δ a b between agents α and β is a pair where a and b 
represent the actions that agents α and β are responsible 
for respectively contracts signed by agents and 
information passed by agents are similar to norms in the sense that 
they oblige agents to behave in a particular way so as to 
satisfy the conditions of the contract or to make the world 
consistent with the information passed contracts and 
information can thus be thought of as normative statements 
that restrict an agent s behaviour 
norms contracts and information have an obvious 
temporal dimension thus an agent has to abide by a norm 
while it is inside an institution a contract has a validity 
period and a piece of information is true only during an 
interval in time the set of norms affecting the behaviour of 
an agent defines the context that the agent has to take into 
account 
α s communication language has two fundamental 
primitives commit α β ϕ to represent in ϕ the world that α 
aims at bringing about and that β has the right to verify 
complain about or claim compensation for any deviations 
from and done μ to represent the event that a certain 
action μ 
has taken place in this way norms contracts 
and information chunks will be represented as instances of 
commit · where α and β can be individual agents or 
institutions c is 
μ illoc α β ϕ t μ μ 
let context in μ end 
ϕ term done μ commit α β ϕ ϕ ∧ ϕ 
ϕ ∨ ϕ ¬ϕ ∀v ϕv ∃v ϕv 
context ϕ id ϕ prolog clause context context 
where ϕv is a formula with free variable v illoc is any 
appropriate set of illocutionary particles   means sequencing 
and context represents either previous agreements previous 
illocutions the ontological working context that is a 
projection of the ontological trees that represent the focus of 
the conversation or code that aligns the ontological 
differences between the speakers needed to interpret an action 
a representing an ontology as a set predicates in prolog 
is simple the set term contains instances of the ontology 
concepts and relations 
for example we can represent the following offer if you 
spend a total of more than e in my shop during 
october then i will give you a discount on all goods in 
november as 
offer α β spent β α october x ∧ x ≥ e → 
∀ y done inform ξ α pay β α y november → 
commit α β discount y 
ξ is an institution agent that reports the payment 
 
without loss of generality we will assume that all actions 
are dialogical 
 
we assume the convention that c c means that c is an 
instance of concept c and r c cn implicitly determines 
that ci is an instance of the concept in the i-th position of 
the relation r 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure the logic agent architecture 
 agent architecture 
a multiagent system {α β βn ξ θ θt} contains 
an agent α that interacts with other argumentation agents 
βi information providing agents θj and an institutional 
agent ξ that represents the institution where we assume 
the interactions happen the institutional agent reports 
promptly and honestly on what actually occurs after an 
agent signs a contract or makes some other form of 
commitment in section this enables us to measure the 
difference between an utterance and a subsequent observation 
the communication language c introduced in section 
enables us both to structure the dialogues and to structure the 
processing of the information gathered by agents agents 
have a probabilistic first-order internal language l used to 
represent a world model mt 
 a generic information-based 
architecture is described in detail in 
the logic agent architecture is shown in figure agent 
α acts in response to a need that is expressed in terms of the 
ontology a need may be exogenous such as a need to trade 
profitably and may be triggered by another agent offering to 
trade or endogenous such as α deciding that it owns more 
wine than it requires needs trigger α s goal plan 
proactive reasoning while other messages are dealt with by α s 
reactive reasoning 
each plan prepares for the negotiation 
by assembling the contents of a  logic briefcase that the 
agent  carries into the negotiation 
 the relationship 
strategy determines which agent to negotiate with for a given 
need it uses risk management analysis to preserve a 
strategic set of trading relationships for each mission-critical need 
- this is not detailed here for each trading relationship 
this strategy generates a relationship target that is expressed 
in the logic framework as a desired level of intimacy to 
be achieved in the long term 
each negotiation consists of a dialogue ψt 
 between two 
agents with agent α contributing utterance μ and the 
part 
each of α s plans and reactions contain constructors for an 
initial world model mt 
 mt 
is then maintained from 
percepts received using update functions that transform 
percepts into constraints on mt 
- for details see 
 
empirical evidence shows that in human negotiation 
better outcomes are achieved by skewing the opening options 
in favour of the proposer we are unaware of any 
empirical investigation of this hypothesis for autonomous agents 
in real trading scenarios 
ner β contributing μ using the language described in 
section each dialogue ψt 
 is evaluated using the logic 
framework in terms of the value of ψt 
to both α and β - see 
section the negotiation strategy then determines the 
current set of options {δi} and then the tactics guided by 
the negotiation target decide which if any of these options 
to put forward and wraps them in argumentation dialogue 
- see section we now describe two of the distributions 
in mt 
that support offer exchange 
pt 
 acc α β χ δ estimates the probability that α should 
accept proposal δ in satisfaction of her need χ where δ 
 a b is a pair of commitments a for α and b for β α will 
accept δ if pt 
 acc α β χ δ c for level of certainty c 
this estimate is compounded from subjective and objective 
views of acceptability the subjective estimate takes account 
of the extent to which the enactment of δ will satisfy α s 
need χ how much δ is  worth to α and the extent to which 
α believes that she will be in a position to execute her 
commitment a sα β a is a random variable denoting 
α s estimate of β s subjective valuation of a over some finite 
numerical evaluation space the objective estimate captures 
whether δ is acceptable on the open market and variable 
uα b denotes α s open-market valuation of the enactment 
of commitment b again taken over some finite numerical 
valuation space we also consider needs the variable tα β a 
denotes α s estimate of the strength of β s motivating need 
for the enactment of commitment a over a valuation space 
then for δ a b pt 
 acc α β χ δ 
pt 
„ 
tα β a 
tα α b 
 h 
× 
„ 
sα α b 
sα β a 
 g 
× 
uα b 
uα a 
≥ s 
 
 
where g ∈ is α s greed h ∈ is α s degree of 
altruism and s ≈ is derived from the stance 
described in 
section the parameters g and h are independent we can 
imagine a relationship that begins with g and h 
then as the agents share increasing amounts of their 
information about their open market valuations g gradually 
reduces to and then as they share increasing amounts of 
information about their needs h increases to the basis 
for the acceptance criterion has thus developed from equity 
to equality and then to need 
pt 
 acc β α δ estimates the probability that β would 
accept δ by observing β s responses for example if β 
sends the message offer δ then α derives the constraint 
{pt 
 acc β α δ } on the distribution pt 
 β α δ and 
if this is a counter offer to a former offer of α s δ then 
{pt 
 acc β α δ } in the not-atypical special case of 
multi-issue bargaining where the agents preferences over the 
individual issues only are known and are complementary to 
each other s maximum entropy reasoning can be applied 
to estimate the probability that any multi-issue δ will be 
acceptable to β by enumerating the possible worlds that 
represent β s limit of acceptability 
 updating the world model mt 
α s world model consists of probability distributions that 
represent its uncertainty in the world state α is interested 
 
if α chooses to inflate her opening options then this is 
achieved in section by increasing the value of s if s 
then a deal may not be possible this illustrates the 
wellknown inefficiency of bilateral bargaining established 
analytically by myerson and satterthwaite in 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
in the degree to which an utterance accurately describes 
what will subsequently be observed all observations about 
the world are received as utterances from an all-truthful 
institution agent ξ for example if β communicates the goal 
i am hungry and the subsequent negotiation terminates 
with β purchasing a book from α by ξ advising α that a 
certain amount of money has been credited to α s account 
then α may conclude that the goal that β chose to satisfy 
was something other than hunger so α s world model 
contains probability distributions that represent its uncertain 
expectations of what will be observed on the basis of 
utterances received 
we represent the relationship between utterance ϕ and 
subsequent observation ϕ by pt 
 ϕ ϕ ∈ mt 
 where ϕ and 
ϕ may be ontological categories in the interest of 
computational feasibility for example if ϕ is i will deliver a bucket 
of fish to you tomorrow then the distribution p ϕ ϕ need 
not be over all possible things that β might do but could 
be over ontological categories that summarise β s possible 
actions 
in the absence of in-coming utterances the conditional 
probabilities pt 
 ϕ ϕ should tend to ignorance as 
represented by a decay limit distribution d ϕ ϕ α may have 
background knowledge concerning d ϕ ϕ as t → ∞ 
otherwise α may assume that it has maximum entropy whilst 
being consistent with the data in general given a 
distribution pt 
 xi and a decay limit distribution d xi pt 
 xi 
decays by 
pt 
 xi δi d xi pt 
 xi 
where δi is the decay function for the xi satisfying the 
property that limt→∞ pt 
 xi d xi for example δi 
could be linear pt 
 xi − νi × d xi νi × pt 
 xi 
where νi is the decay rate for the i th distribution 
either the decay function or the decay limit distribution 
could also be a function of time δt 
i and dt 
 xi 
suppose that α receives an utterance μ illoc α β ϕ t 
from agent β at time t suppose that α attaches an 
epistemic belief rt 
 α β μ to μ - this probability takes account 
of α s level of personal caution we model the update of 
pt 
 ϕ ϕ in two cases one for observations given ϕ second 
for observations given φ in the semantic neighbourhood of 
ϕ 
 update of pt 
 ϕ ϕ given ϕ 
first if ϕk is observed then α may set pt 
 ϕk ϕ to some 
value d where {ϕ ϕ ϕm} is the set of all possible 
observations we estimate the complete posterior 
distribution pt 
 ϕ ϕ by applying the principle of minimum 
relative entropy 
as follows let p μ be the distribution 
 
given a probability distribution q the minimum relative 
entropy distribution p p pi subject to a set of j 
linear constraints g {gj p aj · p − cj } j j 
 that must include the constraint 
p 
i pi − is p 
arg minr 
p 
j rj log 
rj 
qj 
 this may be calculated by 
introducing lagrange multipliers λ l p λ 
p 
j pj log 
pj 
qj 
 λ · g 
minimising l { ∂l 
∂λj 
 gj p } j j is the set of 
given constraints g and a solution to ∂l 
∂pi 
 i i 
leads eventually to p entropy-based inference is a form of 
bayesian inference that is convenient when the data is sparse 
 and encapsulates common-sense reasoning 
arg minx 
p 
j xj log 
xj 
pt ϕ ϕ j 
that satisfies the constraint p μ k 
 d then let q μ be the distribution 
q μ rt 
 α β μ × p μ − rt 
 α β μ × pt 
 ϕ ϕ 
and then let 
r μ 
 
q μ if q μ is more interesting than pt 
 ϕ ϕ 
pt 
 ϕ ϕ otherwise 
a general measure of whether q μ is more interesting than 
pt 
 ϕ ϕ is k q μ d ϕ ϕ k pt 
 ϕ ϕ d ϕ ϕ where 
k x y 
p 
j xj ln 
xj 
yj 
is the kullback-leibler distance 
between two probability distributions x and y 
finally incorporating eqn we obtain the method for 
updating a distribution pt 
 ϕ ϕ on receipt of a message μ 
pt 
 ϕ ϕ δi d ϕ ϕ r μ 
this procedure deals with integrity decay and with two 
probabilities first the probability z in the utterance μ and 
second the belief rt 
 α β μ that α attached to μ 
 update of pt 
 φ φ given ϕ 
the sim method given as above μ illoc α β ϕ t and 
the observation ϕk we define the vector t by 
ti pt 
 φi φ − sim ϕk ϕ − sim φi φ · sim ϕk φ 
with {φ φ φp} the set of all possible observations in 
the context of φ and i p t is not a probability 
distribution the multiplying factor sim ϕ φ limits the 
variation of probability to those formulae whose 
ontological context is not too far away from the observation the 
posterior pt 
 φ φ is obtained with equation with r μ 
defined to be the normalisation of t 
the valuation method for a given φk wexp 
 φk pm 
j pt 
 φj φk · w φj is α s expectation of the value of 
what will be observed given that β has stated that φk will 
be observed for some measure w now suppose that as 
before α observes ϕk after agent β has stated ϕ α revises 
the prior estimate of the expected valuation wexp 
 φk in the 
light of the observation ϕk to 
 wrev 
 φk ϕk ϕ 
g wexp 
 φk sim φk ϕ w φk w ϕ wi ϕk 
for some function g - the idea being for example that if the 
execution ϕk of the commitment ϕ to supply cheese was 
devalued then α s expectation of the value of a commitment 
φ to supply wine should decrease we estimate the posterior 
by applying the principle of minimum relative entropy as for 
equation where the distribution p μ p φ φ satisfies the 
constraint 
p 
x 
j 
p ϕ ϕ j · wi φj 
g wexp 
 φk sim φk ϕ w φk w ϕ wi ϕk 
 summary measures 
a dialogue ψt 
 between agents α and β is a sequence of 
inter-related utterances in context a relationship ψ∗t 
 is a 
sequence of dialogues we first measure the confidence that 
an agent has for another by observing for each utterance 
the difference between what is said the utterance and what 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
subsequently occurs the observation second we evaluate 
each dialogue as it progresses in terms of the logic 
framework - this evaluation employs the confidence measures 
finally we define the intimacy of a relationship as an 
aggregation of the value of its component dialogues 
 confidence 
confidence measures generalise what are commonly called 
trust reliability and reputation measures into a single 
computational framework that spans the logic categories in 
section confidence measures are applied to valuing 
fulfilment of promises in the legitimacy category - we formerly 
called this honour to the execution of commitments 
- we formerly called this trust and to valuing 
dialogues in the goals category - we formerly called this 
reliability 
ideal observations consider a distribution of 
observations that represent α s ideal in the sense that it is the 
best that α could reasonably expect to observe this 
distribution will be a function of α s context with β denoted 
by e and is pt 
i ϕ ϕ e here we measure the relative 
entropy between this ideal distribution pt 
i ϕ ϕ e and the 
distribution of expected observations pt 
 ϕ ϕ that is 
c α β ϕ − 
x 
ϕ 
pt 
i ϕ ϕ e log 
pt 
i ϕ ϕ e 
pt ϕ ϕ 
 
where the is an arbitrarily chosen constant being the 
maximum value that this measure may have this equation 
measures confidence for a single statement ϕ it makes sense 
to aggregate these values over a class of statements say over 
those ϕ that are in the ontological context o that is ϕ ≤ o 
c α β o − 
p 
ϕ ϕ≤o pt 
β ϕ − c α β ϕ 
p 
ϕ ϕ≤o pt 
β ϕ 
where pt 
β ϕ is a probability distribution over the space of 
statements that the next statement β will make to α is ϕ 
similarly for an overall estimate of β s confidence in α 
c α β − 
x 
ϕ 
pt 
β ϕ − c α β ϕ 
preferred observations the previous measure requires 
that an ideal distribution pt 
i ϕ ϕ e has to be specified for 
each ϕ here we measure the extent to which the 
observation ϕ is preferable to the original statement ϕ given a 
predicate prefer c c e meaning that α prefers c to c in 
environment e then if ϕ ≤ o 
c α β ϕ 
x 
ϕ 
pt 
 prefer ϕ ϕ o pt 
 ϕ ϕ 
and 
c α β o 
p 
ϕ ϕ≤o pt 
β ϕ c α β ϕ 
p 
ϕ ϕ≤o pt 
β ϕ 
certainty in observation here we measure the 
consistency in expected acceptable observations or the lack of 
expected uncertainty in those possible observations that are 
better than the original statement if ϕ ≤ o let φ ϕ o κ ˘ 
ϕ pt 
 prefer ϕ ϕ o κ 
¯ 
for some constant κ and 
c α β ϕ 
 
b∗ 
· 
x 
ϕ ∈φ ϕ o κ 
pt 
 ϕ ϕ log pt 
 ϕ ϕ 
where pt 
 ϕ ϕ is the normalisation of pt 
 ϕ ϕ for ϕ ∈ 
φ ϕ o κ 
b∗ 
 
 
 if φ ϕ o κ 
log φ ϕ o κ otherwise 
as above we aggregate this measure for observations in a 
particular context o and measure confidence as before 
computational note the various measures given above 
involve extensive calculations for example eqn containsp 
ϕ that sums over all possible observations ϕ we obtain 
a more computationally friendly measure by appealing to 
the structure of the ontology described in section and 
the right-hand side of eqn may be approximated to 
 − 
x 
ϕ sim ϕ ϕ ≥η 
pt 
η i ϕ ϕ e log 
pt 
η i ϕ ϕ e 
pt 
η ϕ ϕ 
where pt 
η i ϕ ϕ e is the normalisation of pt 
i ϕ ϕ e for 
sim ϕ ϕ ≥ η and similarly for pt 
η ϕ ϕ the extent 
of this calculation is controlled by the parameter η an 
even tighter restriction may be obtained with sim ϕ ϕ ≥ 
η and ϕ ≤ ψ for some ψ 
 valuing negotiation dialogues 
suppose that a negotiation commences at time s and by 
time t a string of utterances φt 
 μ μn has been 
exchanged between agent α and agent β this 
negotiation dialogue is evaluated by α in the context of α s world 
model at time s ms 
 and the environment e that includes 
utterances that may have been received from other agents 
in the system including the information sources {θi} let 
ψt 
 φt 
 ms 
 e then α estimates the value of this dialogue 
to itself in the context of ms 
and e as a × array vα ψt 
 
where 
vx ψt 
 
„ 
il 
x ψt 
 io 
x ψt 
 ig 
x ψt 
 ii 
x ψt 
 ic 
x ψt 
 
ul 
x ψt 
 uo 
x ψt 
 ug 
x ψt 
 ui 
x ψt 
 uc 
x ψt 
 
 
where the i · and u · functions are information-based and 
utility-based measures respectively as we now describe α 
estimates the value of this dialogue to β as vβ ψt 
 by 
assuming that β s reasoning apparatus mirrors its own 
in general terms the information-based valuations 
measure the reduction in uncertainty or information gain that 
the dialogue gives to each agent they are expressed in terms 
of decrease in entropy that can always be calculated the 
utility-based valuations measure utility gain are expressed in 
terms of some suitable utility evaluation function u · that 
can be difficult to define this is one reason why the 
utilitarian approach has no natural extension to the management 
of argumentation that is achieved here by our 
informationbased approach for example if α receives the utterance 
today is tuesday then this may be translated into a 
constraint on a single distribution and the resulting decrease 
in entropy is the information gain attaching a utilitarian 
measure to this utterance may not be so simple 
we use the term × array loosely to describe vα in 
that the elements of the array are lists of measures that will 
be determined by the agent s requirements table shows 
a sample measure for each of the ten categories in it the 
dialogue commences at time s and terminates at time t 
in that table u · is a suitable utility evaluation function 
needs β χ means agent β needs the need χ cho β χ γ 
means agent β satisfies need χ by choosing to negotiate 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
with agent γ n is the set of needs chosen from the 
ontology at some suitable level of abstraction tt 
is the set 
of offers on the table at time t com β γ b means agent 
β has an outstanding commitment with agent γ to execute 
the commitment b where b is defined in the ontology at 
some suitable level of abstraction b is the number of such 
commitments and there are n agents in the system 
 intimacy and balance 
the balance in a negotiation dialogue ψt 
 is defined as 
bαβ ψt 
 vα ψt 
 vβ ψt 
 for an element-by-element 
difference operator that respects the structure of v ψt 
 
the intimacy between agents α and β i∗t 
αβ is the pattern 
of the two × arrays v ∗t 
α and v ∗t 
β that are computed by 
an update function as each negotiation round terminates 
i∗t 
αβ 
` 
v ∗t 
α v ∗t 
β 
´ 
 if ψt 
terminates at time t 
v ∗t 
x ν × vx ψt 
 − ν × v ∗t 
x 
where ν is the learning rate and x α β additionally 
v ∗t 
x continually decays by v ∗t 
x τ × v ∗t 
x − τ × 
dx where x α β τ is the decay rate and dx is a × 
 array being the decay limit distribution for the value to 
agent x of the intimacy of the relationship in the absence 
of any interaction dx is the reputation of agent x the 
relationship balance between agents α and β is b∗t 
αβ v ∗t 
α 
v ∗t 
β in particular the intimacy determines values for the 
parameters g and h in equation as a simple example if 
both io 
α ψ∗t 
 and io 
β ψ∗t 
 increase then g decreases and as 
the remaining eight information-based logic components 
increase h increases 
the notion of balance may be applied to pairs of 
utterances by treating them as degenerate dialogues in simple 
multi-issue bargaining the equitable information revelation 
strategy generalises the tit-for-tat strategy in single-issue 
bargaining and extends to a tit-for-tat argumentation 
strategy by applying the same principle across the logic 
framework 
 strategies and tactics 
each negotiation has to achieve two goals first it may 
be intended to achieve some contractual outcome second 
it will aim to contribute to the growth or decline of the 
relationship intimacy 
we now describe in greater detail the contents of the 
negotiation box in figure the negotiation literature 
consistently advises that an agent s behaviour should not be 
predictable even in close intimate relationships the 
required variation of behaviour is normally described as 
varying the negotiation stance that informally varies from 
friendly guy to tough guy the stance is shown in figure 
it injects bounded random noise into the process where the 
bound tightens as intimacy increases the stance st 
αβ is a 
 × matrix of randomly chosen multipliers each ≈ that 
perturbs α s actions the value in the x y position in the 
matrix where x i u and y l o g i c is chosen at 
random from 
l i∗t 
αβ 
 x y 
 l i∗t 
αβ x y where l i∗t 
αβ x y is the 
bound and i∗t 
αβ is the intimacy 
the negotiation strategy is concerned with maintaining a 
working set of options if the set of options is empty then 
α will quit the negotiation α perturbs the acceptance 
machinery see section by deriving s from the st 
αβ matrix 
such as the value at the i o position in line with the 
comment in footnote in the early stages of the 
negotiation α may decide to inflate her opening options this is 
achieved by increasing the value of s in equation the 
following strategy uses the machinery described in section 
fix h g s and c set the options to the empty set let 
dt 
s {δ pt 
 acc α β χ δ c} then 
 repeat the following as many times as desired add 
δ arg maxx{pt 
 acc β α x x ∈ dt 
s} to options 
remove {y ∈ dt 
s sim y δ k} for some k from dt 
s 
by using pt 
 acc β α δ this strategy reacts to β s history 
of propose and reject utterances 
negotiation tactics are concerned with selecting some 
options and wrapping them in argumentation prior 
interactions with agent β will have produced an intimacy pattern 
expressed in the form of 
` 
v ∗t 
α v ∗t 
β 
´ 
 suppose that the 
relationship target is t∗t 
α t∗t 
β following from equation α 
will want to achieve a negotiation target nβ ψt 
 such that 
ν · nβ ψt 
 − ν · v ∗t 
β is a bit on the t∗t 
β side of v ∗t 
β 
nβ ψt 
 
ν − κ 
ν 
v ∗t 
β ⊕ 
κ 
ν 
t∗t 
β 
for small κ ∈ ν that represents α s desired rate of 
development for her relationship with β nβ ψt 
 is a × 
matrix containing variations in the logic dimensions that 
α would like to reveal to β during ψt 
 e g i ll pass a bit 
more information on options than usual i ll be stronger 
in concessions on options etc it is reasonable to 
expect β to progress towards her target at the same rate and 
nα ψt 
 is calculated by replacing β by α in equation 
nα ψt 
 is what α hopes to receive from β during ψt 
 this 
gives a negotiation balance target of nα ψt 
 nβ ψt 
 that 
can be used as the foundation for reactive tactics by 
striving to maintain this balance across the logic dimensions 
a cautious tactic could use the balance to bound the 
response μ to each utterance μ from β by the constraint 
vα μ vβ μ ≈ st 
αβ ⊗ nα ψt 
 nβ ψt 
 where ⊗ is 
element-by-element matrix multiplication and st 
αβ is the 
stance a less neurotic tactic could attempt to achieve the 
target negotiation balance over the anticipated complete 
dialogue if a balance bound requires negative information 
revelation in one logic category then α will contribute 
nothing to it and will leave this to the natural decay to the 
reputation d as described above 
 discussion 
in this paper we have introduced a novel approach to 
negotiation that uses information and game-theoretical 
measures grounded on business and psychological studies it 
introduces the concepts of intimacy and balance as key 
elements in understanding what is a negotiation strategy and 
tactic negotiation is understood as a dialogue that affect 
five basic dimensions legitimacy options goals 
independence and commitment each dialogical move produces a 
change in a × matrix that evaluates the dialogue along five 
information-based measures and five utility-based measures 
the current balance and intimacy levels and the desired or 
target levels are used by the tactics to determine what to 
say next we are currently exploring the use of this model as 
an extension of a currently widespread eprocurement 
software commercialised by isoco a spin-off company of the 
laboratory of one of the authors 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
il 
α ψt 
 
x 
ϕ∈ψt 
ct 
 α β ϕ − cs 
 α β ϕ ul 
α ψt 
 
x 
ϕ∈ψt 
x 
ϕ 
pt 
β ϕ ϕ × uα ϕ 
io 
α ψt 
 
p 
δ∈t t hs 
 acc β α δ − 
p 
δ∈t t ht 
 acc β α δ 
 tt 
uo 
α ψt 
 
x 
δ∈t t 
pt 
 acc β α δ × 
x 
δ 
pt 
 δ δ uα δ 
ig 
α ψt 
 
p 
χ∈n hs 
 needs β χ − ht 
 needs β χ 
 n 
ug 
α ψt 
 
x 
χ∈n 
pt 
 needs β χ × et 
 uα needs β χ 
ii 
α ψt 
 
po 
i 
p 
χ∈n hs 
 cho β χ βi − ht 
 cho β χ βi 
n × n 
ui 
α ψt 
 
ox 
i 
x 
χ∈n 
ut 
 cho β χ βi − us 
 cho β χ βi 
ic 
α ψt 
 
po 
i 
p 
δ∈b hs 
 com β βi b − ht 
 com β βi b 
n × b 
uc 
α ψt 
 
ox 
i 
x 
δ∈b 
ut 
 com β βi b − us 
 com β βi b 
table sample measures for each category in vα ψt 
 similarly for vβ ψt 
 
acknowledgements carles sierra is partially supported 
by the openknowledge european strep project and by 
the spanish iea project 
 references 
 adams j s inequity in social exchange in advances 
in experimental social psychology l berkowitz ed 
vol new york academic press 
 arcos j l esteva m noriega p 
rodr´ıguez j a and sierra c environment 
engineering for multiagent systems journal on 
engineering applications of artificial intelligence 
 
 bazerman m h loewenstein g f and 
white s b reversal of preference in allocation 
decisions judging an alternative versus choosing 
among alternatives administration science quarterly 
 - 
 brandenburger a and nalebuff b 
co-opetition a revolution mindset that combines 
competition and cooperation doubleday new york 
 
 cheeseman p and stutz j bayesian inference 
and maximum entropy methods in science and 
engineering american institute of physics melville 
ny usa ch on the relationship between 
bayesian and maximum entropy inference pp 
 
 debenham j bargaining with information in 
proceedings third international conference on 
autonomous agents and multi agent systems 
aamas- july n jennings c sierra 
l sonenberg and m tambe eds acm press new 
york pp - 
 fischer r ury w and patton b getting to 
yes negotiating agreements without giving in 
penguin books 
 kalfoglou y and schorlemmer m if-map 
an ontology-mapping method based on 
information-flow theory in journal on data 
semantics i s spaccapietra s march and 
k aberer eds vol of lecture notes in 
computer science springer-verlag heidelberg 
germany pp - 
 lewicki r j saunders d m and minton 
j w essentials of negotiation mcgraw hill 
 li y bandar z a and mclean d an 
approach for measuring semantic similarity between 
words using multiple information sources ieee 
transactions on knowledge and data engineering 
 july august - 
 mackay d information theory inference and 
learning algorithms cambridge university press 
 
 paris j common sense and maximum entropy 
synthese - 
 sierra c and debenham j an 
information-based model for trust in proceedings 
fourth international conference on autonomous 
agents and multi agent systems aamas- 
 utrecht the netherlands july f dignum 
v dignum s koenig s kraus m singh and 
m wooldridge eds acm press new york pp 
- 
 sierra c and debenham j trust and honour in 
information-based agency in proceedings fifth 
international conference on autonomous agents and 
multi agent systems aamas- hakodate japan 
may p stone and g weiss eds acm press 
new york pp - 
 sierra c and debenham j information-based 
agency in proceedings of twentieth international 
joint conference on artificial intelligence ijcai- 
 hyderabad india january pp - 
 sondak h neale m a and pinkley r the 
negotiated allocations of benefits and burdens the 
impact of outcome valence contribution and 
relationship organizational behaviour and human 
decision processes december - 
 valley k l neale m a and mannix e a 
friends lovers colleagues strangers the effects of 
relationships on the process and outcome of 
negotiations in research in negotiation in 
organizations r bies r lewicki and b sheppard 
eds vol jai press pp - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
unifying distributed constraint algorithms in a bdi 
negotiation framework 
bao chau le dinh and kiam tian seow 
school of computer engineering 
nanyang technological university 
republic of singapore 
{ledi asktseow} ntu edu sg 
abstract 
this paper presents a novel unified distributed constraint 
satisfaction framework based on automated negotiation the 
distributed constraint satisfaction problem dcsp is one that 
entails several agents to search for an agreement which is a 
consistent combination of actions that satisfies their mutual constraints 
in a shared environment by anchoring the dcsp search on 
automated negotiation we show that several well-known dcsp 
algorithms are actually mechanisms that can reach agreements 
through a common belief-desire-intention bdi protocol but 
using different strategies a major motivation for this bdi 
framework is that it not only provides a conceptually clearer 
understanding of existing dcsp algorithms from an agent model 
perspective but also opens up the opportunities to extend and 
develop new strategies for dcsp to this end a new strategy called 
unsolicited mutual advice uma is proposed performance 
evaluation shows that the uma strategy can outperform some 
existing mechanisms in terms of computational cycles 
categories and subject descriptors 
i distributed artificial intelligence intelligent 
agents multiagent systems 
general terms 
algorithms design experimentation 
 introduction 
at the core of many emerging distributed applications is the 
distributed constraint satisfaction problem dcsp - one which 
involves finding a consistent combination of actions abstracted as 
domain values to satisfy the constraints among multiple agents 
in a shared environment important application examples include 
distributed resource allocation and distributed scheduling 
many important algorithms such as distributed breakout dbo 
 asynchronous backtracking abt asynchronous partial 
overlay apo and asynchronous weak-commitment awc 
 have been developed to address the dcsp and provide the 
agent solution basis for its applications broadly speaking these 
algorithms are based on two different approaches either 
extending from classical backtracking algorithms or introducing 
mediation among the agents 
while there has been no lack of efforts in this promising 
research field especially in dealing with outstanding issues such as 
resource restrictions e g limits on time and communication 
and privacy requirements there is unfortunately no 
conceptually clear treatment to prise open the model-theoretic workings of 
the various agent algorithms that have been developed as a 
result for instance a deeper intellectual understanding on why one 
algorithm is better than the other beyond computational issues 
is not possible 
in this paper we present a novel unified distributed constraint 
satisfaction framework based on automated negotiation 
negotiation is viewed as a process of several agents searching for a 
solution called an agreement the search can be realized via a 
negotiation mechanism or algorithm by which the agents follow 
a high level protocol prescribing the rules of interactions using 
a set of strategies devised to select their own preferences at each 
negotiation step 
anchoring the dcsp search on automated negotiation we 
show in this paper that several well-known dcsp algorithms 
 are actually mechanisms that share the same 
belief-desireintention bdi interaction protocol to reach agreements but 
use different action or value selection strategies the proposed 
framework provides not only a clearer understanding of existing 
dcsp algorithms from a unified bdi agent perspective but also 
opens up the opportunities to extend and develop new strategies 
for dcsp to this end a new strategy called unsolicited mutual 
advice uma is proposed our performance evaluation shows 
that uma can outperform abt and awc in terms of the average 
number of computational cycles for both the sparse and critical 
coloring problems 
the rest of this paper is organized as follows in section 
we provide a formal overview of dcsp section presents a bdi 
negotiation model by which a dcsp agent reasons section 
presents the existing algorithms abt awc and dbo as 
different strategies formalized on a common protocol a new strategy 
called unsolicited mutual advice is proposed in section our 
empirical results and discussion attempt to highlight the merits 
of the new strategy over existing ones section concludes the 
paper and points to some future work 
 dcsp problem formalization 
the dcsp considers the following environment 
 there are n agents with k variables x x · · · xk− n ≤ 
k which have values in domains d d · · · dk 
respectively we define a partial function b over the 
productrange { n− }×{ k − } such that that 
variable xj belongs to agent i is denoted by b i j the 
exclamation mark   means  is defined 
 there are m constraints c c · · · cm− to be conjunctively 
satisfied in a similar fashion as defined for b i j we use 
e l j ≤ l m ≤ j k to denote that xj is 
relevant to the constraint cl 
the dcsp may be formally stated as follows 
problem statement ∀i j ≤ i n ≤ j k where 
b i j find the assignment xj dj ∈ dj such that ∀l ≤ l 
m where e l j cl is satisfied 
a constraint may consist of different variables belonging to 
different agents an agent cannot change or modify the 
assignment values of other agents variables therefore in 
cooperatively searching for a dcsp solution the agents would need to 
communicate with one another and adjust and re-adjust their 
own variable assignments in the process 
 dcsp agent model 
in general all dcsp agents must cooperatively interact and 
essentially perform the assignment and reassignment of domain 
values to variables to resolve all constraint violations if the 
agents succeed in their resolution a solution is found 
in order to engage in cooperative behavior a dcsp agent needs 
five fundamental parameters namely i a variable or a 
variable set ii domains iii priority iv a neighbor list and 
 v a constraint list 
each variable assumes a range of values called a domain a 
domain value which usually abstracts an action is a possible 
option that an agent may take each agent has an assigned priority 
these priority values help decide the order in which they revise 
or modify their variable assignments an agent s priority may be 
fixed static or changing dynamic when searching for a 
solution if an agent has more than one variable each variable can 
be assigned a different priority to help determine which variable 
assignment the agent should modify first 
an agent which shares the same constraint with another agent 
is called the latter s neighbor each agent needs to refer to its list 
of neighbors during the search process this list may also be kept 
unchanged or updated accordingly in runtime similarly each 
agent maintains a constraint list the agent needs to ensure that 
there is no violation of the constraints in this list constraints can 
be added or removed from an agent s constraint list in runtime 
as with an agent a constraint can also be associated with a 
priority value constraints with a high priority are said to be 
more important than constraints with a lower priority to 
distinguish it from the priority of an agent the priority of a constraint 
is called its weight 
 the bdi negotiation model 
the bdi model originates with the work of m bratman 
according to ch the bdi architecture is based on a 
philosophical model of human practical reasoning and draws out the 
process of reasoning by which an agent decides which actions to 
perform at consecutive moments when pursuing certain goals 
grounding the scope to the dcsp framework the common goal 
of all agents is finding a combination of domain values to satisfy a 
set of predefined constraints in automated negotiation such 
a solution is called an agreement among the agents within this 
scope we found that we were able to unearth the generic behavior 
of a dcsp agent and formulate it in a negotiation protocol 
prescribed using the powerful concepts of bdi thus our proposed 
negotiation model can be said to combine the bdi concepts with 
automated negotiation in a multiagent framework allowing us 
to conceptually separate dcsp mechanisms into a common bdi 
interaction protocol and the adopted strategies 
 the generic protocol 
figure shows the basic reasoning steps in an arbitrary round 
of negotiation that constitute the new protocol the solid line 
indicates the common component or transition which always 
exists regardless of the strategy used the dotted line indicates the 
percept 
belief 
desire 
intention 
mediation 
execution 
p 
b 
d 
i 
i 
i 
info message 
info message 
negotiation message 
negotiation message 
negotiation message 
negotiation message 
negotiation message 
negotiation message 
negotiation message 
figure the bdi interaction protocol 
component or transition which may or may not appear depending 
on the adopted strategy 
two types of messages are exchanged through this protocol 
namely the info message and the negotiation message 
an info message perceived is a message sent by another agent 
the message will contain the current selected values and priorities 
of the variables of that sending agent the main purpose of this 
message is to update the agent about the current environment 
info message is sent out at the end of one negotiation round also 
called a negotiation cycle and received at the beginning of next 
round 
a negotiation message is a message which may be sent within 
a round this message is for mediation purposes the agent may 
put different contents into this type of message as long as it is 
agreed among the group the format of the negotiation message 
and when it is to be sent out are subject to the strategy a 
negotiation message can be sent out at the end of one reasoning 
step and received at the beginning of the next step 
mediation is a step of the protocol that depends on whether the 
agent s interaction with others is synchronous or asynchronous 
in synchronous mechanism mediation is required in every 
negotiation round in an asynchronous one mediation is needed only in 
a negotiation round when the agent receives a negotiation 
message a more in-depth view of this mediation step is provided 
later in this section 
the bdi protocol prescribes the skeletal structure for dcsp 
negotiation we will show in section that several well-known 
dcsp mechanisms all inherit this generic model 
the details of the six main reasoning steps for the protocol 
 see figure are described as follows for a dcsp agent for a 
conceptually clearer description we assume that there is only one 
variable per agent 
 percept in this step the agent receives info messages 
from its neighbors in the environment and using its percept 
function returns an image p this image contains the 
current values assigned to the variables of all agents in its 
neighbor list the image p will drive the agent s actions 
in subsequent steps the agent also updates its constraint 
list c using some criteria of the adopted strategy 
 belief using the image p and constraint list c the agent 
will check if there is any violated constraint if there is 
no violation the agent will believe it is choosing a correct 
option and therefore will take no action the agent will 
do nothing if it is in a local stable state - a snapshot of 
the variables assignments of the agent and all its neighbors 
by which they satisfy their shared constraints when all 
agents are in their local stable states the whole 
environment is said to be in a global stable state and an 
agreethe sixth intl joint conf on autonomous agents and multi-agent systems aamas 
ment is found in case the agent finds its value in conflict 
with some of its neighbors i e the combination of values 
assigned to the variables leads to a constraint violation 
the agent will first try to reassign its own variable using a 
specific strategy if it finds a suitable option which meets 
some criteria of the adopted strategy the agent will believe 
it should change to the new option however it does not 
always happen that an agent can successfully find such an 
option if no option can be found the agent will believe it 
has no option and therefore will request its neighbors to 
reconsider their variable assignments 
to summarize there are three types of beliefs that a dcsp 
agent can form i it can change its variable assignment to 
improve the current situation ii it cannot change its 
variable assignment and some constraints violations cannot be 
resolved and iii it need not change its variable assignment 
as all the constraints are satisfied 
once the beliefs are formed the agent will determine its 
desires which are the options that attempt to resolve the 
current constraint violations 
 desire if the agent takes belief i it will generate a list of 
its own suitable domain values as its desire set if the agent 
takes belief ii it cannot ascertain its desire set but will 
generate a sublist of agents from its neighbor list whom it 
will ask to reconsider their variable assignments how this 
sublist is created depends on the strategy devised for the 
agent in this situation the agent will use a virtual desire 
set that it determines based on its adopted strategy if the 
agent takes belief iii it will have no desire to revise its 
domain value and hence no intention 
 intention the agent will select a value from its desire 
set as its intention an intention is the best desired 
option that the agent assigns to its variable the criteria for 
selecting a desire as the agent s intention depend on the 
strategy used once the intention is formed the agent may 
either proceed to the execution step or undergo mediation 
again the decision to do so is determined by some criteria 
of the adopted strategy 
 mediation this is an important function of the agent 
since if the agent executes its intention without 
performing intention mediation with its neighbors the constraint 
violation between the agents may not be resolved take 
for example suppose two agents have variables x and x 
associated with the same domain { } and their shared 
constraint is x x then if both the variables are 
initialized with value they will both concurrently switch 
between the values and in the absence of mediation 
between them 
there are two types of mediation local mediation and 
group mediation in the former the agents exchange their 
intentions when an agent receives another s intention 
which conflicts with its own the agent must mediate 
between the intentions by either changing its own intention 
or informing the other agent to change its intention in the 
latter there is an agent which acts as a group mediator 
this mediator will collect the intentions from the group - a 
union of the agent and its neighbors - and determine which 
intention is to be executed the result of this mediation is 
passed back to the agents in the group following 
mediation the agent may proceed to the next reasoning step to 
execute its intention or begin a new negotiation round 
 execution this is the last step of a negotiation round 
the agent will execute by updating its variable assignment 
if the intention obtained at this step is its own following 
execution the agent will inform its neighbors about its new 
variable assignment and updated priority to do so the 
agent will send out an info message 
 the strategy 
a strategy plays an important role in the negotiation process 
within the protocol it will often determine the efficiency of the 
percept 
belief 
desire 
intention 
mediation 
execution 
p 
b 
d 
i 
info message 
info message 
negotiation message 
negotiation message 
negotiation message 
figure bdi protocol with asynchronous 
backtracking strategy 
search process in terms of computational cycles and message 
communication costs 
the design space when devising a strategy is influenced by the 
following dimensions i asynchronous or synchronous ii 
dynamic or static priority iii dynamic or static constraint weight 
 iv number of negotiation messages to be communicated v the 
negotiation message format and vi the completeness property 
in other words these dimensions provide technical considerations 
for a strategy design 
 dcsp algorithms bdi protocol 
 strategies 
in this section we apply the proposed bdi negotiation model 
presented in section to expose the bdi protocol and the 
different strategies used for three well-known algorithms abt awc 
and dbo all these algorithms assume that there is only one 
variable per agent under our framework we call the strategies 
applied the abt awc and dbo strategies respectively 
to describe each strategy formally the following mathematical 
notations are used 
 n is the number of agents m is the number of constraints 
 xi denotes the variable held by agent i ≤ i n 
 di denotes the domain of variable xi fi denotes the 
neighbor list of agent i ci denotes its constraint list 
 pi denotes the priority of agent i and pi { xj vj pj 
k agent j ∈ fi vj ∈ dj is the current value assigned 
to xj and the priority value k is a positive integer } is the 
perception of agent i 
 wl denotes the weight of constraint l ≤ l m 
 si v is the total weight of the violated constraints in ci 
when its variable has the value v ∈ di 
 asynchronous backtracking 
figure presents the bdi negotiation model incorporating the 
asynchronous backtracking abt strategy as mentioned in 
section for an asynchronous mechanism that abt is the 
mediation step is needed only in a negotiation round when an agent 
receives a negotiation message 
for agent i beginning initially with wl ≤ l m 
pi i ≤ i n and fi contains all the agents who share the 
constraints with agent i its bdi-driven abt strategy is described 
as follows 
step - percept update pi upon receiving the info 
messages from the neighbors in fi update ci to be the list of 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
constraints which only consists of agents in fi that have equal or 
higher priority than this agent 
step - belief the belief function gb pi ci will return a 
value bi ∈ { } decided as follows 
 bi when agent i can find an optimal option i e if 
 si vi or vi is in bad values list and ∃a ∈ di si a 
 and a is not in a list of domain values called bad values 
list initially this list is empty and it will be cleared when a 
neighbor of higher priority changes its variable assignment 
 bi when it cannot find an optimal option i e if ∀a ∈ 
di si a or a is in bad values list 
 bi when its current variable assignment is an optimal 
option i e if si vi and vi is not in bad value list 
step - desire the desire function gd bi will return a 
desire set denoted by ds decided as follows 
 if bi then ds {a a vi si a and a is not 
in the bad value list } 
 if bi then ds ∅ the agent also finds agent k which 
is determined by {k pk min pj with agent j ∈ fi and 
pk pi } 
 if bi then ds ∅ 
step - intention the intention function gi ds will 
return an intention decided as follows 
 if ds ∅ then select an arbitrary value say vi from ds 
as the intention 
 if ds ∅ then assign nil as the intention to denote its 
lack thereof 
step - execution 
 if agent i has a domain value as its intention the agent will 
update its variable assignment with this value 
 if bi agent i will send a negotiation message to agent 
k then remove k from fi and begin its next negotiation 
round the negotiation message will contain the list of 
variable assignments of those agents in its neighbor list fi 
that have a higher priority than agent i in the current image 
pi 
mediation when agent i receives a negotiation message 
several sub-steps are carried out as follows 
 if the list of agents associated with the negotiation message 
contains agents which are not in fi it will add these agents 
to fi and request these agents to add itself to their 
neighbor lists the request is considered as a type of negotiation 
message 
 agent i will first check if the sender agent is updated with 
its current value vi the agent will add vi to its bad values 
list if it is so or otherwise send its current value to the 
sender agent 
following this step agent i proceeds to the next negotiation 
round 
 asynchronous weak commitment search 
figure presents the bdi negotiation model incorporating the 
asynchronous weak commitment awc strategy the model is 
similar to that of incorporating the abt strategy see figure 
this is not surprising awc and abt are found to be 
strategically similar differing only in the details of some reasoning steps 
the distinguishing point of awc is that when the agent cannot 
find a suitable variable assignment it will change its priority to 
the highest among its group members {i} ∪ fi 
for agent i beginning initially with wl ≤ l m 
pi i ≤ i n and fi contains all the agents who share 
the constraints with agent i its bdi-driven awc strategy is 
described as follows 
step - percept this step is identical to the percept step 
of abt 
step - belief the belief function gb pi ci will return a 
value bi ∈ { } decided as follows 
percept 
belief 
desire 
intention 
mediation 
execution 
p 
b 
d 
i 
info message 
info message 
negotiation message 
negotiation message 
negotiation message 
figure bdi protocol with asynchronous 
weakcommitment strategy 
 bi when the agent can find an optimal option i e if 
 si vi or the assignment xi vi and the current 
variables assignments of the neighbors in fi who have higher 
priority form a nogood stored in a list called nogood list 
and ∃a ∈ di si a initially the list is empty 
 bi when the agent cannot find any optimal option i e 
if ∀a ∈ di si a 
 bi when the current assignment is an optimal option 
i e if si vi and the current state is not a nogood in 
nogood list 
step - desire the desire function gd bi will return a 
desire set ds decided as follows 
 if bi then ds {a a vi si a and the 
number of constraint violations with lower priority agents 
is minimized } 
 if bi then ds {a a ∈ di and the number of 
violations of all relevant constraints is minimized } 
 if bi then ds ∅ 
following if bi agent i will find a list ki of higher priority 
neighbors defined by ki {k agent k ∈ fi and pk pi} 
step - intention this step is similar to the intention step 
of abt however for this strategy the negotiation message will 
contain the variable assignments of the current image pi for 
all the agents in ki this list of assignment is considered as 
a nogood if the same negotiation message had been sent out 
before agent i will have nil intention otherwise the agent will 
send the message and save the nogood in the nogood list 
step - execution 
 if agent i has a domain value as its intention the agent will 
update its variable assignment with this value 
 if bi it will send the negotiation message to its 
neighbors in ki and set pi max{pj} with agent j ∈ fi 
mediation this step is identical to the mediation step of 
abt except that agent i will now add the nogood contained in 
the negotiation message received to its own nogood list 
 distributed breakout 
figure presents the bdi negotiation model incorporating the 
distributed breakout dbo strategy essentially by this 
synchronous strategy each agent will search iteratively for 
improvement by reducing the total weight of the violated constraints 
the iteration will continue until no agent can improve further 
at which time if some constraints remain violated the weights of 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
percept 
belief 
desire 
intention 
mediation 
execution 
p 
b 
d 
i 
i 
a 
info message 
info message 
negotiation message 
negotiation message 
figure bdi protocol with distributed breakout 
strategy 
these constraints will be increased by to help  breakout from a 
local minimum 
for agent i beginning initially with wl ≤ l m 
pi i ≤ i n and fi contains all the agents who share the 
constraints with agent i its bdi-driven dbo strategy is described 
as follows 
step - percept update pi upon receiving the info 
messages from the neighbors in fi update ci to be the list of its 
relevant constraints 
step - belief the belief function gb pi ci will return a 
value bi ∈ { } decided as follows 
 bi when agent i can find an option to reduce the number 
violations of the constraints in ci i e if ∃a ∈ di si a 
si vi 
 bi when it cannot find any option to improve situation 
i e if ∀a ∈ di a vi si a ≥ si vi 
 bi when its current assignment is an optimal option 
i e if si vi 
step - desire the desire function gd bi will return a 
desire set ds decided as follows 
 if bi then ds {a a vi si a si vi and 
 si vi −si a is maximized } max{ si vi −si a } will 
be referenced by hmax 
i in subsequent steps and it defines 
the maximal reduction in constraint violations 
 otherwise ds ∅ 
step - intention the intention function gi ds will 
return an intention decided as follows 
 if ds ∅ then select an arbitrary value say vi from ds 
as the intention 
 if ds ∅ then assign nil as the intention 
following agent i will send its intention to all its neighbors 
in return it will receive intentions from these agents before 
proceeding to mediation step 
mediation agent i receives all the intentions from its 
neighbors if it finds that the intention received from a neighbor agent 
j is associated with hmax 
j hmax 
i the agent will automatically 
cancel its current intention 
step - execution 
 if agent i did not cancel its intention it will update its 
variable assignment with the intended value 
percept 
belief 
desire 
intention 
mediation 
execution 
p 
b 
d 
i 
i 
a 
info message 
info message 
negotiation message 
negotiation message 
negotiation message 
negotiation message 
figure bdi protocol with unsolicited mutual 
advice strategy 
 if all intentions received and its own one are nil intention 
the agent will increase the weight of each currently violated 
constraint by 
 the uma strategy 
figure presents the bdi negotiation model incorporating the 
unsolicited mutual advice uma strategy 
unlike when using the strategies of the previous section a 
dcsp agent using uma will not only send out a negotiation 
message when concluding its intention step but also when 
concluding its desire step the negotiation message that it sends out 
to conclude the desire step constitutes an unsolicited advice for 
all its neighbors in turn the agent will wait to receive unsolicited 
advices from all its neighbors before proceeding on to determine 
its intention 
for agent i beginning initially with wl ≤ l m 
pi i ≤ i n and fi contains all the agents who share 
the constraints with agent i its bdi-driven uma strategy is 
described as follows 
step - percept update pi upon receiving the info 
messages from the neighbors in fi update ci to be the list of 
constraints relevant to agent i 
step - belief the belief function gb pi ci will return a 
value bi ∈ { } decided as follows 
 bi when agent i can find an option to reduce the number 
violations of the constraints in ci i e if ∃a ∈ di si a 
si vi and the assignment xi a and the current variable 
assignments of its neighbors do not form a local state stored 
in a list called bad states list initially this list is empty 
 bi when it cannot find a value a such as a ∈ di si a 
si vi and the assignment xi a and the current variable 
assignments of its neighbors do not form a local state stored 
in the bad states list 
 bi when its current assignment is an optimal option 
i e if si vi 
step - desire the desire function gd bi will return a 
desire set ds decided as follows 
 if bi then ds {a a vi si a si vi and 
 si vi − si a is maximized } and the assignment xi a 
and the current variable assignments of agent i s neighbors 
do not form a state in the bad states list in this case ds is 
called a set of voluntary desires max{ si vi −si a } will 
be referenced by hmax 
i in subsequent steps and it defines 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the maximal reduction in constraint violations it is also 
referred to as an improvement 
 if bi then ds {a a vi si a is minimized } and 
the assignment xi a and the current variable assignments 
of agent i s neighbors do not form a state in the bad states 
list in this case ds is called a set of reluctant desires 
 if bi then ds ∅ 
following if bi agent i will send a negotiation message 
containing hmax 
i to all its neighbors this message is called a 
voluntary advice if bi agent i will send a negotiation message 
called change advice to the neighbors in fi who share the violated 
constraints with agent i 
agent i receives advices from all its neighbors and stores them 
in a list called a before proceeding to the next step 
step - intention the intention function gi ds a will 
return an intention decided as follows 
 if there is a voluntary advice from an agent j which is 
associated with hmax 
j hmax 
i assign nil as the intention 
 if ds ∅ ds is a set of voluntary desires and hmax 
i is 
the biggest improvement among those associated with the 
voluntary advices received select an arbitrary value say 
vi from ds as the intention this intention is called a 
voluntary intention 
 if ds ∅ ds is a set of reluctant desires and agent i 
receives some change advices select an arbitrary value say 
vi from ds as the intention this intention is called 
reluctant intention 
 if ds ∅ then assign nil as the intention 
following if the improvement hmax 
i is the biggest improvement 
and equal to some improvements associated with the received 
voluntary advices agent i will send its computed intention to all 
its neighbors if agent i has a reluctant intention it will also 
send this intention to all its neighbors in both cases agent i 
will attach the number of received change advices in the current 
negotiation round with its intention in return agent i will receive 
the intentions from its neighbors before proceeding to mediation 
step 
mediation if agent i does not send out its intention before 
this step i e the agent has either a nil intention or a voluntary 
intention with biggest improvement it will proceed to next step 
otherwise agent i will select the best intention among all the 
intentions received including its own if any the criteria to 
select the best intention are listed applied in descending order of 
importance as follows 
 a voluntary intention is preferred over a reluctant intention 
 a voluntary intention if any with biggest improvement is 
selected 
 if there is no voluntary intention the reluctant intention 
with the lowest number of constraint violations is selected 
 the intention from an agent who has received a higher 
number of change advices in the current negotiation round is 
selected 
 intention from an agent with highest priority is selected 
if the selected intention is not agent i s intention it will cancel 
its intention 
step - execution if agent i does not cancel its intention 
it will update its variable assignment with the intended value 
termination condition since each agent does not have 
full information about the global state it may not know when it 
has reached a solution i e when all the agents are in a global 
stable state hence an observer is needed that will keep track 
of the negotiation messages communicated in the environment 
following a certain period of time when there is no more message 
communication and this happens when all the agents have no 
more intention to update their variable assignments the observer 
will inform the agents in the environment that a solution has been 
found 
 
 
 
 
 
 
 
 
 
figure example problem 
 an example 
to illustrate how uma works consider a -color graph problem 
 as shown in figure in this example each agent has a color 
variable representing a node there are color variables sharing 
the same domain {black white} 
the following records the outcome of each step in every 
negotiation round executed 
round 
step - percept each agent obtains the current color 
assignments of those nodes agents adjacent to it i e its 
neighbors 
step - belief agents which have positive improvements are 
agent this agent believes it should change its color to 
white agent this believes should change its color to 
white agent this agent believes it should change its 
color to black and agent this agent believes it should 
change its value to black in this negotiation round the 
improvements achieved by these agents are agents which 
do not have any improvements are agents and agents 
 and need not change as all their relevant constraints 
are satisfied 
step - desire agents and have the voluntary desire 
 white color for agents and black color for agents 
 these agents will send the voluntary advices to all 
their neighbors meanwhile agents and have the 
reluctant desires white color for agent and black color 
for agents agent will send a change advice to 
agent as agent is sharing the violated constraint with 
it similarly agents and will send change advices to 
agents and respectively agents and do not have 
any desire to update their color assignments 
step - intention agents and receive the change 
advices from agents and respectively they form their 
voluntary intentions agents and receive the 
voluntary advices from agents and hence they will not 
have any intention agents and do not have any 
intention following the intention from the agents will be 
sent to all their neighbors 
mediation agent finds that the intention from agent is 
better than its intention this is because although both 
agents have voluntary intentions with improvement of 
agent has received one change advice from agent while 
agent has not received any hence agent cancels its 
intention agent will keep its intention 
agents and keep their intentions since none of their 
neighbors has an intention 
the rest of the agents do nothing in this step as they do 
not have any intention 
step - execution agent changes its color to white agents 
 and change their colors to black 
the new state after round is shown in figure 
round 
step - percept the agents obtain the current color 
assignments of their neighbors 
step - belief agent is the only agent who has a positive 
improvement which is it believes it should change its 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
 
 
 
 
 
 
 
figure the graph after round 
color to black agent does not have any positive 
improvement the rest of the agents need not make any change as 
all their relevant constraints are satisfied they will have 
no desire and hence no intention 
step - desire agent desires to change its color to black 
voluntarily hence it sends out a voluntary advice to its 
neighbor i e agent agent does not have any value for 
its reluctant desire set as the only option black color will 
bring agent and its neighbors to the previous state which 
is known to be a bad state since agent is sharing the 
constraint violation with agent it sends a change advice 
to agent 
step - intention agent will have a voluntary intention 
while agent will not have any intention as it receives the 
voluntary advice from agent 
mediation agent will keep its intention as its only neighbor 
agent does not have any intention 
step - execution agent changes its color to black 
the new state after round is shown in figure 
round in this round every agent finds that it has no 
desire and hence no intention to revise its variable assignment 
following with no more negotiation message communication in 
the environment the observer will inform all the agents that a 
solution has been found 
 
 
 
 
 
 
 
 
figure the solution obtained 
 performance evaluation 
to facilitate credible comparisons with existing strategies we 
measured the execution time in terms of computational cycles 
as defined in and built a simulator that could reproduce the 
published results for abt and awc the definition of a 
computational cycle is as follows 
 in one cycle each agent receives all the incoming messages 
performs local computation and sends out a reply 
 a message which is sent at time t will be received at time 
t the network delay is neglected 
 each agent has it own clock the initial clock s value is 
 agents attach their clock value as a time-stamp in the 
outgoing message and use the time-stamp in the incoming 
message to update their own clock s value 
four benchmark problems were considered namely n-queens 
and node coloring for sparse dense and critical graphs for each 
problem a finite number of test cases were generated for 
various problem sizes n the maximum execution time was set to 
 
 
 
 
 
 
 
number of queens 
cycles 
asynchronous 
backtracking 
asynchronous weak 
commitment 
unsolicited mutual 
advice 
figure relationship between execution time and 
problem size 
 cycles for node coloring for critical graphs and cycles 
for other problems the simulator program was terminated after 
this period and the algorithm was considered to fail a test case if 
it did not find a solution by then in such a case the execution 
time for the test was counted as cycles 
 evaluation with n-queens problem 
the n-queens problem is a traditional problem of constraint 
satisfaction test cases were generated for each problem size 
n ∈ { and } 
figure shows the execution time for different problem sizes 
when abt awc and uma were run 
 evaluation with graph coloring problem 
the graph coloring problem can be characterized by three 
parameters i the number of colors k the number of nodes agents 
n and the number of links m based on the ratio m n the 
problem can be classified into three types i sparse with 
m n ii critical with m n or and iii dense 
 with m n n − for this problem we did not include 
abt in our empirical results as its failure rate was found to be 
very high this poor performance of abt was expected since 
the graph coloring problem is more difficult than the n-queens 
problem on which abt already did not perform well see figure 
 
the sparse and dense coloring problem types are relatively 
easy while the critical type is difficult to solve in the 
experiments we fix k test cases were created using the method 
described in for each value of n ∈ { } for each 
problem type 
the simulation results for each type of problem are shown in 
figures - 
 
 
 
 
 
 
 
number of nodes 
cycles 
asynchronous 
weak 
commitment 
unsolicited 
mutual advice 
figure comparison between awc and uma 
 sparse graph coloring 
 discussion 
 comparison with abt and awc 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
 
 
 
 
 
 
number of nodes 
cycles 
asynchronous 
weak 
commitment 
unsolicited 
mutual advice 
figure comparison between awc and uma 
 critical graph coloring 
 
 
 
 
 
 
 
number of nodes 
cycles 
asynchronous 
weak 
commitment 
unsolicited 
mutual advice 
figure comparison between awc and uma 
 dense graph coloring 
figure shows that the average performance of uma is slightly 
better than awc for the sparse problem uma outperforms 
awc in solving the critical problem as shown in figure it 
was observed that the latter strategy failed in some test cases 
however as seen in figure both the strategies are very 
efficient when solving the dense problem with awc showing slightly 
better performance 
the performance of uma in the worst time complexity case 
is similar to that of all evaluated strategies the worst case 
occurs when all the possible global states of the search are reached 
since only a few agents have the right to change their variable 
assignments in a negotiation round the number of redundant 
computational cycles and info messages is reduced as we observe 
from the backtracking in abt and awc the difference in the 
ordering of incoming messages can result in a different number of 
computational cycles to be executed by the agents 
 comparison with dbo 
the computational performance of uma is arguably better 
than dbo for the following reasons 
 uma can guarantee that there will be a variable 
reassignment following every negotiation round whereas dbo 
cannot 
 uma introduces one more communication round trip that 
of sending a message and awaiting a reply than dbo 
which occurs due to the need to communicate unsolicited 
advices although this increases the communication cost 
per negotiation round we observed from our simulations 
that the overall communication cost incurred by uma is 
lower due to the significantly lower number of negotiation 
rounds 
 using uma in the worst case an agent will only take or 
communication round trips per negotiation round following 
which the agent or its neighbor will do a variable 
assignment update using dbo this number of round trips is 
uncertain as each agent might have to increase the weights 
of the violated constraints until an agent has a positive 
improvement this could result in a infinite loop 
 conclusion 
applying automated negotiation to dcsp this paper has 
proposed a protocol that prescribes the generic reasoning of a dcsp 
agent in a bdi architecture our work shows that several 
wellknown dcsp algorithms namely abt awc and dbo can be 
described as mechanisms sharing the same proposed protocol and 
only differ in the strategies employed for the reasoning steps per 
negotiation round as governed by the protocol importantly this 
means that it might furnish a unified framework for dcsp that 
not only provides a clearer bdi agent-theoretic view of existing 
dcsp approaches but also opens up the opportunities to enhance 
or develop new strategies towards the latter we have proposed 
and formulated a new strategy - the uma strategy empirical 
results and our discussion suggest that uma is superior to abt 
awc and dbo in some specific aspects 
it was observed from our simulations that uma possesses the 
completeness property future work will attempt to formally 
establish this property as well as formalize other existing dscp 
algorithms as bdi negotiation mechanisms including the recent 
endeavor that employs a group mediator the idea of dcsp 
agents using different strategies in the same environment will also 
be investigated 
 references 
 p j modi h jung m tambe w -m shen and 
s kulkarni dynamic distributed resource allocation a 
distributed constraint satisfaction approach in lecture 
notes in computer science p 
 h schlenker and u geske simulating large railway 
networks using distributed constraint satisfaction in nd 
ieee international conference on industrial informatics 
 indin- pp - 
 m yokoo distributed constraint satisfaction 
foundations of cooperation in multi-agent systems 
springer verlag springer series on agent technology 
 m yokoo e h durfee t ishida and k kuwabara the 
distributed constraint satisfaction problem formalization 
and algorithms ieee transactions on knowledge and 
data engineering vol no pp - 
september october 
 r mailler and v lesser using cooperative mediation to 
solve distributed constraint satisfaction problems in 
proceedings of the third international joint conference on 
autonomous agents and multiagent systems 
 aamas- pp - 
 e tsang foundation of constraint satisfaction 
academic press 
 r mailler r vincent v lesser t middlekoop and 
j shen soft real-time cooperative negotiation for 
distributed resource allocation aaai fall symposium 
on negotiation methods for autonomous cooperative 
systems november 
 m yokoo k suzuki and k hirayama secure 
distributed constraint satisfaction reaching agreement 
without revealing private information artificial 
intelligence vol no - pp - 
 j s rosenschein and g zlotkin rules of encounter 
the mit press 
 m yokoo and k hirayama distributed constraint 
satisfaction algorithm for complex local problems in 
proceedings of the third international conference on 
multiagent systems icmas- pp - 
 m e bratman intentions plans and practical reason 
harvard university press cambridge m a 
 g weiss ed multiagent system a modern approach to 
distributed artificial intelligence the mit press 
london u k 
 s minton m d johnson a b philips and p laird 
minimizing conflicts a heuristic repair method for 
constraint satisfaction and scheduling problems artificial 
intelligence vol e no - pp - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
bidding algorithms for a distributed combinatorial auction 
benito mendoza 
∗ 
and jos´e m vidal 
computer science and engineering 
university of south carolina 
columbia sc 
mendoza  engr sc edu vidal sc edu 
abstract 
distributed allocation and multiagent coordination 
problems can be solved through combinatorial auctions 
however most of the existing winner determination algorithms 
for combinatorial auctions are centralized the pause 
auction is one of a few efforts to release the auctioneer from 
having to do all the work it might even be possible to get 
rid of the auctioneer it is an increasing price 
combinatorial auction that naturally distributes the problem of 
winner determination amongst the bidders in such a way that 
they have an incentive to perform the calculation it can 
be used when we wish to distribute the computational load 
among the bidders or when the bidders do not wish to reveal 
their true valuations unless necessary pause establishes 
the rules the bidders must obey however it does not tell 
us how the bidders should calculate their bids we have 
developed a couple of bidding algorithms for the bidders in 
a pause auction our algorithms always return the set of 
bids that maximizes the bidder s utility since the problem 
is np-hard run time remains exponential on the number 
of items but it is remarkably better than an exhaustive 
search in this paper we present our bidding algorithms 
discuss their virtues and drawbacks and compare the 
solutions obtained by them to the revenue-maximizing solution 
found by a centralized winner determination algorithm 
categories and subject descriptors 
i computing methodologies distributed 
artificial intelligence-intelligent agents multiagent systems 
general terms 
algorithms performance 
 introduction 
both the research and practice of combinatorial auctions 
have grown rapidly in the past ten years in a 
combinatorial auction bidders can place bids on combinations of 
items called packages or bidsets rather than just 
individual items once the bidders place their bids it is necessary 
to find the allocation of items to bidders that maximizes 
the auctioneer s revenue this problem known as the 
winner determination problem is a combinatorial optimization 
problem and is np-hard nevertheless several 
algorithms that have a satisfactory performance for problem 
sizes and structures occurring in practice have been 
developed the practical applications of combinatorial auctions 
include allocation of airport takeoff and landing time slots 
procurement of freight transportation services procurement 
of public transport services and industrial procurement 
because of their wide applicability one cannot hope for a 
general-purpose winner determination algorithm that can 
efficiently solve every instance of the problem thus 
several approaches and algorithms have been proposed to 
address the winner determination problem however most of 
the existing winner determination algorithms for 
combinatorial auctions are centralized meaning that they require 
all agents to send their bids to a centralized auctioneer who 
then determines the winners examples of these algorithms 
are cass bidtree and cabob we believe that 
distributed solutions to the winner determination problem 
should be studied as they offer a better fit for some 
applications as when for example agents do not want to reveal 
their valuations to the auctioneer 
the pause progressive adaptive user selection 
environment auction is one of a few efforts to distribute 
the problem of winner determination amongst the bidders 
pause establishes the rules the participants have to adhere 
to so that the work is distributed amongst them however 
it is not concerned with how the bidders determine what 
they should bid 
in this paper we present two algorithms pausebid and 
cachedpausebid which enable agents in a pause 
auction to find the bidset that maximizes their utility our 
algorithms implement a myopic utility maximizing strategy 
and are guaranteed to find the bidset that maximizes the 
agent s utility given the outstanding best bids at a given 
time pausebid performs a branch and bound search 
completely from scratch every time that it is called 
cachedpausebid is a caching-based algorithm which explores fewer 
nodes since it caches some solutions 
 
 - - - - rps c ifaamas 
 the pause auction 
a pause auction for m items has m stages stage 
consists of having simultaneous ascending price open-cry 
auctions and during this stage the bidders can only place bids on 
individual items at the end of this state we will know what 
the highest bid for each individual item is and who placed 
that bid each successive stage k m consists of 
an ascending price auction where the bidders must submit 
bidsets that cover all items but each one of the bids must be 
for k items or less the bidders are allowed to use bids that 
other agents have placed in previous rounds when building 
their bidsets thus allowing them to find better solutions 
also any new bidset has to have a sum of bid prices which 
is bigger than that of the currently winning bidset at the 
end of each stage k all agents know the best bid for every 
subset of size k or less also at any point in time after stage 
 has ended there is a standing bidset whose value increases 
monotonically as new bidsets are submitted since in the 
final round all agents consider all possible bidsets we know 
that the final winning bidset will be one such that no agent 
can propose a better bidset note however that this 
bidset is not guaranteed to be the one that maximizes revenue 
since we are using an ascending price auction so the 
winning bid for each set will be only slightly bigger than the 
second highest bid for the particular set of items that is 
the final prices will not be the same as the prices in a 
traditional combinatorial auction where all the bidders bid their 
true valuation however there remains the open question 
of whether the final distribution of items to bidders found 
in a pause auction is the same as the revenue maximizing 
solution our test results provide an answer to this question 
the pause auction makes the job of the auctioneer very 
easy all it has to do is to make sure that each new 
bidset has a revenue bigger than the current winning bidset as 
well as make sure that every bid in an agent s bidset that 
is not his does indeed correspond to some other agents 
previous bid the computational problem shifts from one of 
winner determination to one of bid generation each agent 
must search over the space of all bidsets which contain at 
least one of its bids the search is made easier by the fact 
that the agent needs to consider only the current best bids 
and only wants bidsets where its own utility is higher than 
in the current winning bidset each agent also has a clear 
incentive for performing this computation namely its 
utility only increases with each bidset it proposes of course it 
might decrease with the bidsets that others propose 
finally the pause auction has been shown to be envy-free in 
that at the conclusion of the auction no bidder would prefer 
to exchange his allocation with that of any other bidder 
we can even envision completely eliminating the 
auctioneer and instead have every agent perform the task of the 
auctioneer that is all bids are broadcast and when an 
agent receives a bid from another agent it updates the set 
of best bids and determines if the new bid is indeed better 
than the current winning bid the agents would have an 
incentive to perform their computation as it will increase their 
expected utility also any lies about other agents bids are 
easily found out by keeping track of the bids sent out by 
every agent the set of best bids namely the only one that 
can increase an agent s bid value is the agent itself 
anyone claiming a higher value for some other agent is lying 
the only thing missing is an algorithm that calculates the 
utility-maximizing bidset for each agent 
 problem formulation 
a bid b is composed of three elements bitems 
 the set of 
items the bid is over bagent 
 the agent that placed the bid 
and bvalue 
 the value or price of the bid the agents 
maintain a set b of the current best bids one for each set of items 
of size ≤ k where k is the current stage at any point in the 
auction after the first round there will also be a set w ⊆ b 
of currently winning bids this is the set of bids that covers 
all the items and currently maximizes the revenue where 
the revenue of w is given by 
r w 
b∈w 
bvalue 
 
agent i s value function is given by vi s ∈ where s is a 
set of items given an agent s value function and the current 
winning bidset w we can calculate the agent s utility from 
w as 
ui w 
b∈w bagent i 
vi bitems 
 − bvalue 
 
that is the agent s utility for a bidset w is the value it 
receives for the items it wins in w minus the price it must 
pay for those items if the agent is not winning any items 
then its utility is zero 
the goal of the bidding agents in the pause auction is to 
maximize their utility subject to the constraint that their 
next set of bids must have a total revenue that is at least 
bigger than the current revenue where is the smallest 
increment allowed in the auction formally given that w is 
the current winning bidset agent i must find a g∗ 
i such that 
r g∗ 
i ≥ r w and 
g∗ 
i arg max 
g⊆ b 
ui g 
where each g is a set of bids that covers all items and 
∀b∈g b ∈ b or bagent 
 i and bvalue 
 b bitems 
 and 
size bitems 
 ≤ k and where b items is the value of the 
bid in b for the set items if there is no bid for those items 
it returns zero that is each bid b in g must satisfy at least 
one of the two following conditions b is already in b 
b is a bid of size ≤ k in which the agent i bids higher than 
the price for the same items in b 
 bidding algorithms 
according to the pause auction during the first stage we 
have only several english auctions with the bidders 
submitting bids on individual items in this case an agent s 
dominant strategy is to bid higher than the current winning bid 
until it reaches its valuation for that particular item our 
algorithms focus on the subsequent stages k when 
k agents have to find g∗ 
i this can be done by 
performing a complete search on b however this approach is 
computationally expensive since it produces a large search 
tree our algorithms represent alternative approaches to 
overcome this expensive search 
 the pausebid algorithm 
in the pausebid algorithm shown in figure we 
implement some heuristics to prune the search tree given 
that bidders want to maximize their utility and that at any 
given point there are likely only a few bids within b which 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
pausebid i k 
 my-bids ← ∅ 
 their-bids ← ∅ 
 for b ∈ b 
 do if bagent 
 i or vi bitems 
 bvalue 
 then my-bids ← my-bids 
 new bid bitems 
 i vi bitems 
 
 else their-bids ← their-bids b 
 for s ∈ subsets of k or fewer items such that 
vi s and ¬∃b∈bbitems 
 s 
 do my-bids ← my-bids new bid s i vi s 
 bids ← my-bids their-bids 
 g∗ 
← ∅ £ global variable 
 u∗ 
← ui w £ global variable 
 pbsearch bids ∅ 
 surplus ← b∈g∗ bagent i bvalue 
− b bitems 
 
 if surplus 
 then return g∗ 
 my-payment ← vi g∗ 
 − u∗ 
 for b ∈ g∗ 
 bagent 
 i 
 do if my-payment ≤ 
 then bvalue 
← b bitems 
 
 else bvalue 
← b bitems 
 
 my-payment ·bvalue 
−b bitems 
 
surplus 
 return g∗ 
figure the pausebid algorithm which implements 
a branch and bound search i is the agent and k is 
the current stage of the auction for k ≥ 
the agent can dominate we start by defining my-bids to be 
the list of bids for which the agent s valuation is higher than 
the current best bid as given in b we set the value of 
these bids to be the agent s true valuation but we won t 
necessarily be bidding true valuation as we explain later 
similarly we set their-bids to be the rest of the bids from b 
finally the agent s search list is simply the concatenation 
of my-bids and their-bids note that the agent s own bids 
are placed first on the search list as this will enable us to do 
more pruning pausebid lines to the agent can now 
perform a branch and bound search on the branch-on-bids 
tree produced by these bids this branch and bound search 
is implemented by pbsearch figure our algorithm not 
only implements the standard bound but it also implements 
other pruning techniques in order to further reduce the size 
of the search tree 
the bound we use is the maximum utility that the agent 
can expect to receive from a given set of bids we call it u∗ 
 
initially u∗ 
is set to ui w pausebid line since that 
is the utility the agent currently receives and any solution 
he proposes should give him more utility if pbsearch ever 
comes across a partial solution where the maximum utility 
the agent can expect to receive is less than u∗ 
then that 
subtree is pruned pbsearch line note that we can 
determine the maximum utility only after the algorithm has 
searched over all of the agent s own bids which are first on 
the list because after that we know that the solution will 
not include any more bids where the agent is the winner 
thus the agent s utility will no longer increase for example 
pbsearch bids g 
 if bids ∅ then return 
 b ← first bids 
 bids ← bids −b 
 g ← g b 
 ¯ig ← items not in g 
 if g does not contain a bid from i 
 then return 
 if g includes all items 
 then min-payment ← max r w − r g − ri g 
b∈g bagent i b bitems 
 
 max-utility ← vi g − min-payment 
 if r g r w and max-utility ≥ u∗ 
 then g∗ 
← g 
 u∗ 
← max-utility 
 pbsearch bids g − b £ b is out 
 else max-revenue ← r g max h ¯ig hi ¯ig 
 if max-revenue ≤ r w 
 then pbsearch bids g − b £ b is out 
 elseif bagent 
 i 
 then min-payment ← r w 
− r g − ri g − h ¯ig 
 max-utility ← vi g − min-payment 
 if max-utility u∗ 
 then pbsearch {x ∈ bids 
xitems 
∩ bitems 
 ∅} g £ b is in 
 pbsearch bids g − b £ b is out 
 else 
 pbsearch {x ∈ bids 
xitems 
∩ bitems 
 ∅} g £ b is in 
 pbsearch bids g − b £ b is out 
 return 
figure the pbsearch recursive procedure where 
bids is the set of available bids and g is the current 
partial solution 
if an agent has only one bid in my-bids then the maximum 
utility he can expect is equal to his value for the items in 
that bid minus the minimum possible payment we can make 
for those items and still come up with a set of bids that has 
revenue greater than r w the calculation of the minimum 
payment is shown in line for the partial solution case and 
line for the case where we have a complete solution in 
pbsearch note that in order to calculate the min-payment 
for the partial solution case we need an upper bound on the 
payments that we must make for each item this upper 
bound is provided by 
h s 
s∈s 
max 
b∈b s∈bitems 
bvalue 
size bitems 
 
this function produces a bound identical to the one used by 
the bidtree algorithm-it merely assigns to each individual 
item in s a value equal to the maximum bid in b divided 
by the number of items in that bid 
to prune the branches that cannot lead to a solution with 
revenue greater than the current w the algorithm considers 
both the values of the bids in b and the valuations of the 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
agent similarly to we define 
hi s k 
s∈s 
max 
s size s ≤k and s∈s and vi s 
vi s 
size s 
 
which assigns to each individual item s in s the maximum 
value produced by the valuation of s divided by the size 
of s where s is a set for which the agent has a valuation 
greater than zero contains s and its size is less or equal 
than k the algorithm uses the heuristics h and hi lines 
and of pbsearch to prune the just mentioned branches 
in the same way an a∗ 
algorithm uses its heuristic a final 
pruning technique implemented by the algorithm is ignoring 
any branches where the agent has no bids in the current 
answer g and no more of the agent s bids are in the list 
 pbsearch lines and 
the resulting g∗ 
found by pbsearch is thus the set of bids 
that has revenue bigger than r w and maximizes agent i s 
utility however agent i s bids in g∗ 
are still set to his own 
valuation and not to the lowest possible price lines to 
in pausebid are responsible for setting the agent s payments 
so that it can achieve its maximum utility u∗ 
 if the agent 
has only one bid in g∗ 
then it is simply a matter of reducing 
the payment of that bid by u∗ 
from the current maximum of 
the agent s true valuation however if the agent has more 
than one bid then we face the problem of how to distribute 
the agent s payments among these bids there are many 
ways of distributing the payments and there does not appear 
to be a dominant strategy for performing this distribution 
we have chosen to distribute the payments in proportion to 
the agent s true valuation for each set of items 
pausebid assumes that the set of best bids b and the 
current best winning bidset w remains constant during its 
execution and it returns the agent s myopic utility-maximizing 
bidset if there is one using a branch and bound search 
however it repeats the whole search at every stage we 
can minimize this problem by caching the result of previous 
searches 
 the cachedpausebid algorithm 
the cachedpausebid algorithm shown in figure is 
our second approach to solve the bidding problem in the 
pause auction it is based in a cache table called c-table 
where we store some solutions to avoid doing a complete 
search every time the problem is the same the agent i has 
to find g∗ 
i we note that g∗ 
i is a bidset that contains at least 
one bid of the agent i let s be a set of items for which the 
agent i has a valuation such that vi s ≥ b s let gs 
i 
be a bidset over s such that r gs 
i ≥ r w and 
gs 
i arg max 
g⊆ b 
ui g 
where each g is a set of bids that covers all items and 
∀b∈g b ∈ b or bagent 
 i and bvalue 
 b bitems 
 and 
 ∃b∈gbitems 
 s and bagent 
 i that is gs 
i is i s best 
bidset for all items which includes a bid from i for all s items 
in the pause auction we cannot bid for sets of items with 
size greater than k so if we have for each set of items s for 
which vi s and size s ≤ k its corresponding gs 
i then 
g∗ 
i is the gs 
i that maximizes the agent s utility that is 
g∗ 
i arg max 
{s vi s ∧size s ≤k} 
ui gs 
i 
each agent i implements a hash table c-table such that 
c-table s gs 
for all s which vi s ≥ b s we can 
cachedpausebid i k k-changed 
 for each s in c-table 
 do if vi s b s 
 then remove s from c-table 
 else if k-changed and size s k 
 then b ← b new bid i s vi s 
 g∗ 
← ∅ 
 u∗ 
← ui w 
 for each s with size s ≤ k in c-table 
 do ¯s ← items − s 
 gs 
← c-table s £ global variable 
 min-payment ← max r w b∈gs b bitems 
 
 us 
← r gs 
 − min-payment £ global variable 
 if k-changed and size s k 
or ∃b∈b bitems 
⊆ ¯s and bagent 
 i 
 then b ← {b ∈ b bitems 
⊆ ¯s} 
 bids ← b 
 {b ∈ b bitems 
⊆ ¯s and b ∈ b } 
 for b ∈ bids 
 do if vi bitems 
 bvalue 
 then bagent 
← i 
 bvalue 
← vi bitems 
 
 if k-changed and size s k 
 then n ← size bids 
 us 
← 
 else n ← size b 
 g ← ∅ new bid s i vi s 
 cpbsearch bids g n 
 c-table s ← gs 
 if us 
 u∗ 
and r gs 
 ≥ r w 
 then surplus ← 
b∈gs bagent i bvalue 
− b bitems 
 
 if surplus 
 then my-payment ← vi gs 
 − ui gs 
 
 for b ∈ gs 
 bagent 
 i 
 do if my-payment ≤ 
 then bvalue 
← b bitems 
 
 else bvalue 
← b bitems 
 
my-payment ·bvalue 
−b bitems 
 
surplus 
 u∗ 
← ui gs 
 
 g∗ 
← gs 
 else if us 
≤ and vi s b s 
 then remove s from c-table 
 return g∗ 
figure the cachedpausebid algorithm that 
implements a caching based search to find a bidset that 
maximizes the utility for the agent i k is the 
current stage of the auction for k ≥ and k-changed is 
a boolean that is true right after the auction moved 
to the next stage 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
cpbsearch bids g n 
 if bids ∅ or n ≤ then return 
 b ← first bids 
 bids ← bids −b 
 g ← g b 
 ¯ig ← items not in g 
 if g includes all items 
 then min-payment ← max r w − r g − ri g 
b∈g bagent i b bitems 
 
 max-utility ← vi g − min-payment 
 if r g r w and max-utility ≥ us 
 then gs 
← g 
 us 
← max-utility 
 cpbsearch bids g − b n − £ b is out 
 else max-revenue ← r g max h ¯ig hi ¯ig 
 if max-revenue ≤ r w 
 then cpbsearch bids g − b n − £ b is out 
 elseif bagent 
 i 
 then min-payment ← r w 
− r g − ri g − h ¯ig 
 max-utility ← vi g − min-payment 
 if max-utility us 
 then cpbsearch {x ∈ bids 
xitems 
∩ bitems 
 ∅} g n £ b is in 
 cpbsearch bids g − b n − £ b is out 
 else 
 cpbsearch {x ∈ bids 
xitems 
∩ bitems 
 ∅} g n £ b is in 
 cpbsearch bids g − b n − £ b is out 
 return 
figure the cpbsearch recursive procedure where 
bids is the set of available bids g is the current 
partial solution and n is a value that indicates how deep 
in the list bids the algorithm has to search 
then find g∗ 
by searching for the gs 
 stored in c-table s 
that maximizes the agent s utility considering only the set 
of items s with size s ≤ k the problem remains in 
maintaining the c-table updated and avoiding to search every 
gs 
every time cachedpausebid deals with this and other 
details 
let b be the set of bids that contains the new best bids 
that is b contains the bids recently added to b and the bids 
that have changed price always higher bidder or both and 
were already in b let ¯s items − s be the complement 
of s the set of items not included in s cachedpausebid 
takes three parameters i the agent k the current stage of 
the auction and k-changed a boolean that is true right after 
the auction moved to the next stage initially c-table has 
one row or entry for each set s for which vi s we 
start by eliminating the entries corresponding to each set s 
for which vi s b s from c-table line then in the 
case that k-changed is true for each set s with size s k 
we add to b a bid for that set with value equal to vi s 
and bidder agent i line this a bid that the agent is now 
allowed to consider we then search for g∗ 
amongst the gs 
stored in c-table for this we only need to consider the sets 
with size s ≤ k line but how do we know that the gs 
in c-table s is still the best solution for s there are only 
two cases when we are not sure about that and we need 
to do a search to update c-table s these cases are i 
when k-changed is true and size s ≤ k since there was 
no gs 
stored in c-table for this s ii when there exists at 
least one bid in b for the set of items ¯s or a subset of it 
submitted by an agent different than i since it is probable 
that this new bid can produce a solution better than the one 
stored in c-table s 
we handle the two cases mentioned above in lines to 
of cachedpausebid in both of these cases since gs 
must 
contain a bid for s we need to find a bidset that cover the 
missing items that is ¯s thus our search space consists 
of all the bids on b for the set of items ¯s or for a subset 
of it we build the list bids that contains only those bids 
however we put the bids from b at the beginning of bids 
 line since they are the ones that have changed then 
we replace the bids in bids that have a price lower than the 
valuation the agent i has for those same items with a bid 
from agent i for those items and value equal to the agent s 
valuation lines - 
the recursive procedure cpbsearch called in line of 
cachedpausebid and shown in figure is the one that 
finds the new gs 
 cpbsearch is a slightly modified version 
of our branch and bound search implemented in pbsearch 
the first modification is that it has a third parameter n that 
indicates how deep on the list bids we want to search since 
it stops searching when n less or equal to zero and not only 
when the list bids is empty line each time that there is 
a recursive call of cpbsearch n is decreased by one when a 
bid from bids is discarded or out lines and 
and n remains the same otherwise lines and we set 
the value of n before calling cpbsearch to be the size of the 
list bids cachedpausebid line in case i since we want 
cpbsearch to search over all bids and we set n to be the 
number of bids from b included in bids cachedpausebid 
line in case ii since we know that only the those first n 
bids in bids changed and can affect our current gs 
 
another difference with pbsearch is that the bound in 
cpbsearch is us 
which we set to be cachedpausebid line 
 when in case i and r gs 
 −min-payment cachedpausebid 
line when in case ii we call cpbsearch with g already 
containing a bid for s after cpbsearch is executed we 
are sure that we have the right gs 
 so we store it in the 
corresponding c-table s cachedpausebid line 
when we reach line in cachedpausebid we are sure 
that we have the right gs 
 however agent i s bids in gs 
are 
still set to his own valuation and not to the lowest possible 
price if us 
is greater than the current u∗ 
 lines to 
in cachedpausebid are responsible for setting the agent s 
payments so that it can achieve its maximum utility us 
 
as in pausebid we have chosen to distribute the payments 
in proportion to the agent s true valuation for each set of 
items in the case that us 
less than or equal to zero and 
the valuation that the agent i has for the set of items s is 
lower than the current value of the bid in b for the same 
set of items we remove the corresponding c-table s since 
we know that is not worthwhile to keep it in the cache table 
 cachedpausebid line 
the cachedpausebid function is called when k and 
returns the agent s myopic utility-maximizing bidset if there 
is one it assumes that w and b remains constant during 
its execution 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
generatevalues i items 
 for x ∈ items 
 do vi x expd 
 for n ← num-bids − items 
 do s s ←two random sets of items with values 
 vi s ∪ s vi s vi s expd 
figure algorithm for the generation of random 
value functions expd x returns a random number 
taken from an exponential distribution with mean 
 x 
 
 
 
 
 
 
 
number of items 
cachedpausebid 
 
 
 
 
pausebid 
 
 
 
 
figure average percentage of convergence 
 y-axis which is the percentage of times that our 
algorithms converge to the revenue-maximizing 
solution as function of the number of items in the 
auction 
 test and comparison 
we have implemented both algorithms and performed a 
series of experiments in order to determine how their 
solution compares to the revenue-maximizing solution and how 
their times compare with each other in order to do our 
tests we had to generate value functions for the agents 
 
the algorithm we used is shown in figure the type of 
valuations it generates correspond to domains where a set 
of agents must perform a set of tasks but there are cost 
savings for particular agents if they can bundle together certain 
subsets of tasks for example imagine a set of robots which 
must pick up and deliver items to different locations since 
each robot is at a different location and has different 
abilities each one will have different preferences over how to 
bundle their costs for the item bundles are subadditive 
which means that their preferences are superadditive the 
first experiment we performed simply ensured the proper 
 
note that we could not use cats because it generates 
sets of bids for an indeterminate number of agents it is as 
if you were told the set of bids placed in a combinatorial 
auction but not who placed each bid or even how many 
people placed bids and then asked to determine the value 
function of every participant in the auction 
 
 
 
 
 
 
 
number of items 
cachedpausebid 
 
 
 
 
 
pausebid 
 
 
 
 
 
 
figure average percentage of revenue from our 
algorithms relative to maximum revenue y-axis as 
function of the number of items in the auction 
functioning of our algorithms we then compared the 
solutions found by both of them to the revenue-maximizing 
solution as found by cass when given a set of bids that 
corresponds to the agents true valuation that is for each 
agent i and each set of items s for which vi s we 
generated a bid this set of bids was fed to cass which 
implements a centralized winner determination algorithm to find 
the solution which maximizes revenue note however that 
the revenue from the pause auction on all the auctions is 
always smaller than the revenue of the revenue-maximizing 
solution when the agents bid their true valuations since 
pause uses english auctions the final prices roughly 
represent the second-highest valuation plus for that set of 
items 
we fixed the number of agents to be and we 
experimented with different number of items namely from to 
 we ran both algorithms times for each 
combination when we compared the solutions of our algorithms 
to the revenue-maximizing solution we realized that they 
do not always find the same distribution of items as the 
revenue-maximizing solution as shown in figure the 
cases where our algorithms failed to arrive at the 
distribution of the revenue-maximizing solution are those where 
there was a large gap between the first and second 
valuation for a set or sets of items if the revenue-maximizing 
solution contains the bid or bids using these higher 
valuation then it is impossible for the pause auction to find this 
solution because that bid those bids is never placed for 
example if agent i has vi and the second highest 
valuation for is only then i only needs to place a bid 
of in order to win that item if the revenue-maximizing 
solution requires that be sold for then that solution 
will never be found because that bid will never be placed 
we also found that average percentage of times that our 
algorithms converges to the revenue-maximizing solution 
decreases as the number of items increases for items is 
almost but decreases a little bit less than percent as 
the items increase so that this average percentage of 
convergence is around for items in a few instances our 
algorithms find different solutions this is due to the different 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
 
 
 
 
number of items 
cachedpausebid 
 
 
 
 
 
 
 
 
 
pausebid 
 
 
 
 
 
 
 
 
 
 
figure average number of expanded nodes 
 y-axis as function of items in the auction 
ordering of the bids in the bids list which makes them search 
in different order 
we know that the revenue generated by the pause 
auction is generally lower than the revenue of the 
revenuemaximizing solution but how much lower to answer this 
question we calculated percentage representing the 
proportion of the revenue given by our algorithms relative to the 
revenue given by cass we found that the percentage of 
revenue of our algorithms increases in average as the 
number of items increases as shown in figure however 
we found that cachedpausebid generates a higher revenue 
than pausebid higher in average except for auctions 
with items where both have about the same percentage 
again this difference is produced by the order of the search 
in the case of items both algorithms produce in average 
a revenue proportion of while in the other extreme 
 items cachedpausebid produced in average a revenue 
proportion of while pausebid produced in average a 
revenue proportion of 
the scalability of our algorithms can be determined by 
counting the number of nodes expanded in the search tree 
for this we count the number of times that pbsearch gets 
invoked for each time that pausebid is called and the 
number of times that fastpausebidsearch gets invoked for each 
time that cachedpausebid respectively for each of our 
algorithms as expected since this is an np-hard problem 
the number of expanded nodes does grow exponentially with 
the number of items as shown in figure however we 
found that cachedpausebid outperforms pausebid since 
it expands in average less than half the number of nodes 
for example the average number of nodes expanded when 
 items is zero for cachedpausebid while for pausebid is 
 and in the other extreme items cachedpausebid 
expands in average only nodes while pausebid expands in 
average nodes a difference of more than nodes 
although the number of nodes expanded by our algorithms 
increases as function of the number of items the actual 
number of nodes is a much smaller than the worst-case scenario 
of nn 
where n is the number of items for example for 
items we expand slightly more than 
nodes for the case of 
pausebid and less than that for the case of 
cachedpause 
 
 
 
 
 
number of items 
cachedpausebid 
 
 
 
 
 
 
 
 
 
 
pausebid 
 
 
 
 
 
 
 
 
 
 
figure average time in seconds that takes to 
finish an auction y-axis as function of the number of 
items in the auction 
bid which are much smaller numbers than 
 notice also 
that our value generation algorithm figure generates a 
number of bids that is exponential on the number of items 
as might be expected in many situations as such these 
results do not support the conclusion that time grows 
exponentially with the number of items when the number of 
bids is independent of the number of items we expect that 
both algorithms will grow exponentially as a function the 
number of bids but stay roughly constant as the number of 
items grows 
we wanted to make sure that less expanded nodes does 
indeed correspond to faster execution especially since our 
algorithms execute different operations we thus ran the 
same experiment with all the agents in the same machine 
an intel centrino ghz laptop pc with gb of ram and 
a rmp gb hard drive and calculated the average 
time that takes to finish an auction for each algorithm as 
shown in figure cachedpausebid is faster than 
pausebid the difference in execution speed is even more clear as 
the number of items increases 
 related work 
a lot of research has been done on various aspects of 
combinatorial auctions we recommend for a good review 
however the study of distributed winner determination 
algorithms for combinatorial auctions is still relatively new 
one approach is given by the algorithms for distributing 
the winner determination problem in combinatorial auctions 
presented in but these algorithms assume the 
computational entities are the items being sold and thus end up 
with a different type of distribution the vsa algorithm 
 is another way of performing distributed winner 
determination in combinatorial auction but it assumes the bids 
themselves perform the computation this algorithm also 
fails to converge to a solution for most cases in the 
authors present a distributed mechanism for calculating vcg 
payments in a mechanism design problem their 
mechanism roughly amounts to having each agent calculate the 
payments for two other agents and give these to a secure 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
central server which then checks to make sure results from 
all pairs agree otherwise a re-calculation is ordered this 
general idea which they call the redundancy principle could 
also be applied to our problem but it requires the existence 
of a secure center agent that everyone trusts another 
interesting approach is given in where the bidding agents 
prioritize their bids thus reducing the set of bids that the 
centralized winner determination algorithm must consider 
making that problem easier finally in the computation 
procuring clock auction the agents are given an 
everincreasing percentage of the surplus achieved by their 
proposed solution over the current best as such it assumes 
the agents are impartial computational entities not the set 
of possible buyers as assumed by the pause auction 
 conclusions 
we believe that distributed solutions to the winner 
determination problem should be studied as they offer a better fit 
for some applications as when for example agents do not 
want to reveal their valuations to the auctioneer or when 
we wish to distribute the computational load among the 
bidders the pause auction is one of a few approaches 
to decentralize the winner determination problem in 
combinatorial auctions with this auction we can even envision 
completely eliminating the auctioneer and instead have 
every agent performe the task of the auctioneer however 
while pause establishes the rules the bidders must obey it 
does not tell us how the bidders should calculate their bids 
we have presented two algorithms pausebid and 
cachedpausebid that bidder agents can use to engage in a pause 
auction both algorithms implement a myopic utility 
maximizing strategy that is guaranteed to find the bidset that 
maximizes the agent s utility given the set of outstanding 
best bids at any given time without considering possible 
future bids both algorithms find most of the time the 
same distribution of items as the revenue-maximizing 
solution the cases where our algorithms failed to arrive at that 
distribution are those where there was a large gap between 
the first and second valuation for a set or sets of items 
as it is an np-hard problem the running time of our 
algorithms remains exponential but it is significantly better than 
a full search pausebid performs a branch and bound search 
completely from scratch each time it is invoked 
cachedpausebid caches partial solutions and performs a branch 
and bound search only on the few portions affected by the 
changes on the bids between consecutive times 
cachedpausebid has a better performance since it explores fewer 
nodes less than half and it is faster as expected the 
revenue generated by a pause auction is lower than the 
revenue of a revenue-maximizing solution found by a 
centralized winner determination algorithm however we found 
that cachedpausebid generates in average higher 
revenue than pausebid we also found that the revenue 
generated by our algorithms increases as function of the number 
of items in the auction 
our algorithms have shown that it is feasible to implement 
the complex coordination constraints supported by 
combinatorial auctions without having to resort to a centralized 
winner determination algorithm moreover because of the 
design of the pause auction the agents in the auction also 
have an incentive to perform the required computation our 
bidding algorithms can be used by any multiagent system 
that would use combinatorial auctions for coordination but 
would rather not implement a centralized auctioneer 
 references 
 p j brewer decentralized computation procurement 
and computational robustness in a smart market 
economic theory - january 
 p cramton y shoham and r steinberg editors 
combinatorial auctions mit press 
 y fujishima k leyton-brown and y shoham 
taming the computational complexity of 
combinatorial auctions optimal and approximate 
approaches in proceedings of the sixteenth 
international joint conference on artificial 
intelligence pages - morgan kaufmann 
publishers inc 
 f kelly and r stenberg a combinatorial auction 
with multiple winners for universal service 
management science - 
 a land s powell and r steinberg pause a 
computationally tractable combinatorial auction in 
cramton et al chapter pages - 
 k leyton-brown m pearson and y shoham 
towards a universal test suite for combinatorial 
auction algorithms in proceedings of the nd acm 
conference on electronic commerce pages - 
acm press http cats stanford edu 
 m v narumanchi and j m vidal algorithms for 
distributed winner determination in combinatorial 
auctions in lnai volume of amec tada springer 
 
 s park and m h rothkopf auctions with 
endogenously determined allowable combinations 
technical report rutgets center for operations 
research january rrr - 
 d c parkes and j shneidman distributed 
implementations of vickrey-clarke-groves auctions in 
proceedings of the third international joint 
conference on autonomous agents and multiagent 
systems pages - acm 
 m h rothkopf a pekec and r m harstad 
computationally manageable combinational auctions 
management science - 
 t sandholm an algorithm for winner determination 
in combinatorial auctions artificial intelligence 
 - - february 
 t sandholm s suri a gilpin and d levine 
cabob a fast optimal algorithm for winner 
determination in combinatorial auctions management 
science - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a complete distributed constraint optimization method 
for non-traditional pseudotree arrangements∗ 
james atlas 
computer and information sciences 
university of delaware 
newark de 
atlas cis udel edu 
keith decker 
computer and information sciences 
university of delaware 
newark de 
decker cis udel edu 
abstract 
distributed constraint optimization dcop is a general 
framework that can model complex problems in multi-agent systems 
several current algorithms that solve general dcop instances 
including adopt and dpop arrange agents into a traditional 
pseudotree structure we introduce an extension to the dpop algorithm 
that handles an extended set of pseudotree arrangements our 
algorithm correctly solves dcop instances for pseudotrees that 
include edges between nodes in separate branches the algorithm 
also solves instances with traditional pseudotree arrangements 
using the same procedure as dpop 
we compare our algorithm with dpop using several metrics 
including the induced width of the pseudotrees the maximum 
dimensionality of messages and computation and the maximum 
sequential path cost through the algorithm we prove that for some 
problem instances it is not possible to generate a traditional pseudotree 
using edge-traversal heuristics that will outperform a cross-edged 
pseudotree we use multiple heuristics to generate pseudotrees and 
choose the best pseudotree in linear space-time complexity for 
some problem instances we observe significant improvements in 
message and computation sizes compared to dpop 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligence-multiagent systems 
general terms 
algorithms 
 introduction 
many historical problems in the ai community can be 
transformed into constraint satisfaction problems csp with the 
advent of distributed ai multi-agent systems became a popular way 
to model the complex interactions and coordination required to 
solve distributed problems csps were originally extended to 
distributed agent environments in early domains for 
distributed constraint satisfaction problems discsp included job 
shop scheduling and resource allocation many domains 
for agent systems especially teamwork coordination distributed 
scheduling and sensor networks involve overly constrained 
problems that are difficult or impossible to satisfy for every constraint 
recent approaches to solving problems in these domains rely 
on optimization techniques that map constraints into multi-valued 
utility functions instead of finding an assignment that satisfies all 
constraints these approaches find an assignment that produces a 
high level of global utility this extension to the original discsp 
approach has become popular in multi-agent systems and has been 
labeled the distributed constraint optimization problem dcop 
 
current algorithms that solve complete dcops use two main 
approaches search and dynamic programming search based 
algorithms that originated from discsp typically use some form of 
backtracking or bounds propagation as in adopt 
dynamic programming based algorithms include dpop and its 
extensions to date both categories of algorithms arrange 
agents into a traditional pseudotree to solve the problem 
it has been shown in that any constraint graph can be mapped 
into a traditional pseudotree however it was also shown that 
finding the optimal pseudotree was np-hard we began to 
investigate the performance of traditional pseudotrees generated by 
current edge-traversal heuristics we found that these heuristics 
often produced little parallelism as the pseudotrees tended to have 
high depth and low branching factors we suspected that there 
could be other ways to arrange the pseudotrees that would 
provide increased parallelism and smaller message sizes after 
exploring these other arrangements we found that cross-edged 
pseudotrees provide shorter depths and higher branching factors than 
the traditional pseudotrees our hypothesis was that these 
crossedged pseudotrees would outperform traditional pseudotrees for 
some problem types 
in this paper we introduce an extension to the dpop algorithm 
that handles an extended set of pseudotree arrangements which 
include cross-edged pseudotrees we begin with a definition of 
 
 - - - - rps c ifaamas 
dcop traditional pseudotrees and cross-edged pseudotrees we 
then provide a summary of the original dpop algorithm and 
introduce our dcpop algorithm we discuss the complexity of our 
algorithm as well as the impact of pseudotree generation 
heuristics we then show that our distributed cross-edged pseudotree 
optimization procedure dcpop performs significantly better in 
practice than the original dpop algorithm for some problem 
instances we conclude with a selection of ideas for future work and 
extensions for dcpop 
 problem definition 
dcop has been formalized in slightly different ways in recent 
literature so we will adopt the definition as presented in a 
distributed constraint optimization problem with n nodes and m 
constraints consists of the tuple x d u where 
 x {x xn} is a set of variables each one assigned to a 
unique agent 
 d {d dn} is a set of finite domains for each variable 
 u {u um} is a set of utility functions such that each 
function involves a subset of variables in x and defines a 
utility for each combination of values among these variables 
an optimal solution to a dcop instance consists of an assignment 
of values in d to x such that the sum of utilities in u is maximal 
problem domains that require minimum cost instead of maximum 
utility can map costs into negative utilities the utility functions 
represent soft constraints but can also represent hard constraints 
by using arbitrarily large negative values for this paper we only 
consider binary utility functions involving two variables higher 
order utility functions can be modeled with minor changes to the 
algorithm but they also substantially increase the complexity 
 traditional pseudotrees 
pseudotrees are a common structure used in search procedures 
to allow parallel processing of independent branches as defined in 
 a pseudotree is an arrangement of a graph g into a rooted tree 
t such that vertices in g that share an edge are in the same branch 
in t a back-edge is an edge between a node x and any node which 
lies on the path from x to the root excluding x s parent figure 
shows a pseudotree with four nodes three edges a-b b-c 
bd and one back-edge a-c also defined in are four types of 
relationships between nodes exist in a pseudotree 
 p x - the parent of a node x the single node higher in the 
pseudotree that is connected to x directly through a tree edge 
 c x - the children of a node x the set of nodes lower in 
the pseudotree that are connected to x directly through tree 
edges 
 pp x - the pseudo-parents of a node x the set of nodes 
higher in the pseudotree that are connected to x directly 
through back-edges in figure a pp c 
 pc x - the pseudo-children of a node x the set of nodes 
lower in the pseudotree that are connected to x directly 
through back-edges in figure c pc a 
figure a traditional pseudotree solid line edges 
represent parent-child relationships and the dashed line represents 
a pseudo-parent-pseudo-child relationship 
figure a cross-edged pseudotree solid line edges represent 
parent-child relationships the dashed line represents a 
pseudoparent-pseudo-child relationship and the dotted line 
represents a branch-parent-branch-child relationship the bolded 
node b is the merge point for node e 
 cross-edged pseudotrees 
we define a cross-edge as an edge from node x to a node y that is 
above x but not in the path from x to the root a cross-edged 
pseudotree is a traditional pseudotree with the addition of cross-edges 
figure shows a cross-edged pseudotree with a cross-edge d-e 
in a cross-edged pseudotree we designate certain edges as primary 
the set of primary edges defines a spanning tree of the nodes the 
parent child pseudo-parent and pseudo-child relationships from 
the traditional pseudotree are now defined in the context of this 
primary edge spanning tree this definition also yields two additional 
types of relationships that may exist between nodes 
 bp x - the branch-parents of a node x the set of nodes 
higher in the pseudotree that are connected to x but are not 
in the primary path from x to the root in figure d 
bp e 
 bc x - the branch-children of a node x the set of nodes 
lower in the pseudotree that are connected to x but are not in 
any primary path from x to any leaf node in figure e 
bc d 
 pseudotree generation 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
current algorithms usually have a pre-execution phase to 
generate a traditional pseudotree from a general dcop instance our 
dcpop algorithm generates a cross-edged pseudotree in the same 
fashion first the dcop instance x d u translates directly 
into a graph with x as the set of vertices and an edge for each pair 
of variables represented in u next various heuristics are used to 
arrange this graph into a pseudotree one common heuristic is to 
perform a guided depth-first search dfs as the resulting traversal 
is a pseudotree and a dfs can easily be performed in a distributed 
fashion we define an edge-traversal based method as any method 
that produces a pseudotree in which all parent child pairs share an 
edge in the original graph this includes dfs breadth-first search 
and best-first search based traversals our heuristics that generate 
cross-edged pseudotrees use a distributed best-first search traversal 
 dpop algorithm 
the original dpop algorithm operates in three main phases the 
first phase generates a traditional pseudotree from the dcop 
instance using a distributed algorithm the second phase joins utility 
hypercubes from children and the local node and propagates them 
towards the root the third phase chooses an assignment for each 
domain in a top down fashion beginning with the agent at the root 
node 
the complexity of dpop depends on the size of the largest 
computation and utility message during phase two it has been shown 
that this size directly corresponds to the induced width of the 
pseudotree generated in phase one dpop uses polynomial time 
heuristics to generate the pseudotree since finding the minimum 
induced width pseudotree is np-hard several distributed 
edgetraversal heuristics have been developed to find low width 
pseudotrees at the end of the first phase each agent knows its 
parent children pseudo-parents and pseudo-children 
 utility propagation 
agents located at leaf nodes in the pseudotree begin the process 
by calculating a local utility hypercube this hypercube at node 
x contains summed utilities for each combination of values in the 
domains for p x and pp x this hypercube has dimensional size 
equal to the number of pseudo-parents plus one a message 
containing this hypercube is sent to p x agents located at non-leaf 
nodes wait for all messages from children to arrive once the agent 
at node y has all utility messages it calculates its local utility 
hypercube which includes domains for p y pp y and y the local 
utility hypercube is then joined with all of the hypercubes from 
the child messages at this point all utilities involving node y are 
known and the domain for y may be safely eliminated from the 
joined hypercube this elimination process chooses the best utility 
over the domain of y for each combination of the remaining 
domains a message containing this hypercube is now sent to p y 
the dimensional size of this hypercube depends on the number of 
overlapping domains in received messages and the local utility 
hypercube this dynamic programming based propagation phase 
continues until the agent at the root node of the pseudotree has received 
all messages from its children 
 value propagation 
value propagation begins when the agent at the root node z has 
received all messages from its children since z has no parents 
or pseudo-parents it simply combines the utility hypercubes 
received from its children the combined hypercube contains only 
values for the domain for z at this point the agent at node z 
simply chooses the assignment for its domain that has the best utility 
a value propagation message with this assignment is sent to each 
node in c z each other node then receives a value propagation 
message from its parent and chooses the assignment for its domain 
that has the best utility given the assignments received in the 
message the node adds its domain assignment to the assignments it 
received and passes the set of assignments to its children the 
algorithm is complete when all nodes have chosen an assignment for 
their domain 
 dcpop algorithm 
our extension to the original dpop algorithm shown in 
algorithm shares the same three phases the first phase generates the 
cross-edged pseudotree for the dcop instance the second phase 
merges branches and propagates the utility hypercubes the third 
phase chooses assignments for domains at branch merge points and 
in a top down fashion beginning with the agent at the root node 
for the first phase we generate a pseudotree using several 
distributed heuristics and select the one with lowest overall 
complexity the complexity of the computation and utility message size 
in dcpop does not directly correspond to the induced width of 
the cross-edged pseudotree instead we use a polynomial time 
method for calculating the maximum computation and utility 
message size for a given cross-edged pseudotree a description of 
this method and the pseudotree selection process appears in 
section at the end of the first phase each agent knows its 
parent children pseudo-parents pseudo-children branch-parents and 
branch-children 
 merging branches and utility 
propagation 
in the original dpop algorithm a node x only had utility 
functions involving its parent and its pseudo-parents in dcpop a node 
x is allowed to have a utility function involving a branch-parent 
the concept of a branch can be seen in figure with node e 
representing our node x the two distinct paths from node e to node 
b are called branches of e the single node where all branches of 
e meet is node b which is called the merge point of e 
agents with nodes that have branch-parents begin by sending 
a utility propagation message to each branch-parent this 
message includes a two dimensional utility hypercube with domains for 
the node x and the branch-parent bp x it also includes a branch 
information structure which contains the origination node of the 
branch x the total number of branches originating from x and the 
number of branches originating from x that are merged into a 
single representation by this branch information structure this 
number starts at intuitively when the number of merged branches 
equals the total number of originating branches the algorithm has 
reached the merge point for x in figure node e sends a utility 
propagation message to its branch-parent node d this message 
has dimensions for the domains of e and d and includes branch 
information with an origin of e total branches and merged 
branch 
as in the original dpop utility propagation phase an agent at 
leaf node x sends a utility propagation message to its parent in 
dcpop this message contains dimensions for the domains of p x 
and pp x if node x also has branch-parents then the utility 
propagation message also contains a dimension for the domain of x 
and will include a branch information structure in figure node 
e sends a utility propagation message to its parent node c this 
message has dimensions for the domains of e and c and includes 
branch information with an origin of e total branches and 
merged branch 
when a node y receives utility propagation messages from all of 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
its children and branch-children it merges any branches with the 
same origination node x the merged branch information structure 
accumulates the number of merged branches for x if the 
cumulative total number of merged branches equals the total number of 
branches then y is the merge point for x this means that the 
utility hypercubes present at y contain all information about the 
valuations for utility functions involving node x in addition to the 
typical elimination of the domain of y from the utility hypercubes 
we can now safely eliminate the domain of x from the utility 
hypercubes to illustrate this process we will examine what happens 
in the second phase for node b in figure 
in the second phase node b receives two utility propagation 
messages the first comes from node c and includes dimensions 
for domains e b and a it also has a branch information structure 
with origin of e total branches and merged branch the second 
comes from node d and includes dimensions for domains e and b 
it also has a branch information structure with origin of e total 
branches and merged branch node b then merges the branch 
information structures from both messages because they have the 
same origination node e since the number of merged branches 
originating from e is now and the total branches originating from 
e is node b now eliminates the dimensions for domain e node 
b also eliminates the dimension for its own domain leaving only 
information about domain a node b then sends a utility 
propagation message to node a containing only one dimension for the 
domain of a 
although not possible in dpop this method of utility 
propagation and dimension elimination may produce hypercubes at node y 
that do not share any domains in dcpop we do not join domain 
independent hypercubes but instead may send multiple hypercubes 
in the utility propagation message sent to the parent of y this lazy 
approach to joins helps to reduce message sizes 
 value propagation 
as in dpop value propagation begins when the agent at the root 
node z has received all messages from its children at this point 
the agent at node z chooses the assignment for its domain that has 
the best utility if z is the merge point for the branches of some 
node x z will also choose the assignment for the domain of x 
thus any node that is a merge point will choose assignments for 
a domain other than its own these assignments are then passed 
down the primary edge hierarchy if node x in the hierarchy has 
branch-parents then the value assignment message from p x will 
contain an assignment for the domain of x every node in the 
hierarchy adds any assignments it has chosen to the ones it received 
and passes the set of assignments to its children the algorithm is 
complete when all nodes have chosen or received an assignment for 
their domain 
 proof of correctness 
we will prove the correctness of dcpop by first noting that 
dcpop fully extends dpop and then examining the two cases for 
value assignment in dcpop given a traditional pseudotree as 
input the dcpop algorithm execution is identical to dpop using a 
traditional pseudotree arrangement no nodes have branch-parents 
or branch-children since all edges are either back-edges or tree 
edges thus the dcpop algorithm using a traditional pseudotree 
sends only utility propagation messages that contain domains 
belonging to the parent or pseudo-parents of a node since no node 
has any branch-parents no branches exist and thus no node serves 
as a merge point for any other node thus all value propagation 
assignments are chosen at the node of the assignment domain 
for dcpop execution with cross-edged pseudotrees some 
nodes serve as merge points we note that any node x that is not a 
merge point assigns its value exactly as in dpop the local utility 
hypercube at x contains domains for x p x pp x and bc x 
as in dpop the value assignment message received at x includes 
the values assigned to p x and pp x also since x is not a merge 
point all assignments to bc x must have been calculated at merge 
points higher in the tree and are in the value assignment message 
from p x thus after eliminating domains for which assignments 
are known only the domain of x is left the agent at node x can 
now correctly choose the assignment with maximum utility for its 
own domain 
if node x is a merge point for some branch-child y we know 
that x must be a node along the path from y to the root and from 
p y and all bp y to the root from the algorithm we know that 
y necessarily has all information from c y pc y and bc y 
since it waits for their messages node x has information about all 
nodes below it in the tree which would include y p y bp y 
and those pp y that are below x in the tree for any pp y above 
x in the tree x receives the assignment for the domain of pp y 
in the value assignment message from p x thus x has utility 
information about all of the utility functions of which y is a part 
by eliminating domains included in the value assignment message 
node x is left with a local utility hypercube with domains for x and 
y the agent at node x can now correctly choose the assignments 
with maximum utility for the domains of x and y 
 complexity analysis 
the first phase of dcpop sends one message to each p x 
pp x and bp x the second phase sends one value assignment 
message to each c x thus dcpop produces a linear number of 
messages with respect to the number of edges utility functions in 
the cross-edged pseudotree and the original dcop instance the 
actual complexity of dcpop depends on two additional 
measurements message size and computation size 
message size and computation size in dcpop depend on the 
number of overlapping branches as well as the number of 
overlapping back-edges it was shown in that the number of 
overlapping back-edges is equal to the induced width of the pseudotree in 
a poorly constructed cross-edged pseudotree the number of 
overlapping branches at node x can be as large as the total number 
of descendants of x thus the total message size in dcpop in a 
poorly constructed instance can be space-exponential in the total 
number of nodes in the graph however in practice a well 
constructed cross-edged pseudotree can achieve much better results 
later we address the issue of choosing well constructed 
crossedged pseudotrees from a set 
we introduce an additional measurement of the maximum 
sequential path cost through the algorithm this measurement 
directly relates to the maximum amount of parallelism achievable by 
the algorithm to take this measurement we first store the total 
computation size for each node during phase two and three this 
computation size represents the number of individual accesses to a 
value in a hypercube at each node for example a join between two 
domains of size costs ∗ two directed acyclic graphs 
 dag can then be drawn one with the utility propagation 
messages as edges and the phase two costs at nodes and the other with 
value assignment messages and the phase three costs at nodes the 
maximum sequential path cost is equal to the sum of the longest 
path on each dag from the root to any leaf node 
 heuristics 
in our assessment of complexity in dcpop we focused on the 
worst case possibly produced by the algorithm we acknowledge 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
algorithm dcpop algorithm 
 dcpop x d u 
each agent xi executes 
phase pseudotree creation 
 elect leader from all xj ∈ x 
 elected leader initiates pseudotree creation 
 afterwards xi knows p xi pp xi bp xi c xi bc xi 
and pc xi 
phase util message propagation 
 if bp xi then 
 branchxi ← bp xi 
 for all xk ∈bp xi do 
 utilxi xk ←compute utils xi xk 
 send message xk utilxi xk branchxi 
 if c xi i e xi is a leaf node then 
 utilxi p xi ← compute utils p xi pp xi 
for all pp xi 
 send message p xi 
utilxi p xi branchxi 
 send message pp xi empty util 
empty branch to all pp xi 
 activate util message handler 
phase value message propagation 
 activate value message handler 
end algorithm 
util message handler xk utilxk xi 
branchxk 
 store utilxk xi branchxk xi 
 if util messages from all children and branch children arrived 
then 
 for all bj ∈branch xi do 
 if bj is merged then 
 join all hypercubes where bj ∈util xi 
 eliminate bj from the joined hypercube 
 if p xi null that means xi is the root then 
 v ∗ i ← choose optimal null 
 send value xi v ∗ i to all c xi 
 else 
 utilxi p xi ← compute utils p xi 
pp xi 
 send message p xi utilxi p xi 
branchxi p xi 
value message handler valuexi p xi 
 add all xk ← v ∗ k ∈valuexi p xi to agent view 
 xi ← v ∗ i choose optimal agent view 
 send valuexl xi to all xl ∈c xi 
that in real world problems the generation of the pseudotree has 
a significant impact on the actual performance the problem of 
finding the best pseudotree for a given dcop instance is np-hard 
thus a heuristic is used for generation and the performance of the 
algorithm depends on the pseudotree found by the heuristic some 
previous research focused on finding heuristics to generate good 
pseudotrees while we have developed some heuristics that 
generate good cross-edged pseudotrees for use with dcpop our 
focus has been to use multiple heuristics and then select the best 
pseudotree from the generated pseudotrees 
we consider only heuristics that run in polynomial time with 
respect to the number of nodes in the original dcop instance the 
actual dcpop algorithm has worst case exponential complexity 
but we can calculate the maximum message size computation size 
and sequential path cost for a given cross-edged pseudotree in 
linear space-time complexity to do this we simply run the algorithm 
without attempting to calculate any of the local utility hypercubes 
or optimal value assignments instead messages include 
dimensional and branch information but no utility hypercubes 
after each heuristic completes its generation of a pseudotree we 
execute the measurement procedure and propagate the 
measurement information up to the chosen root in that pseudotree the 
root then broadcasts the total complexity for that heuristic to all 
nodes after all heuristics have had a chance to complete every 
node knows which heuristic produced the best pseudotree each 
node then proceeds to begin the dcpop algorithm using its 
knowledge of the pseudotree generated by the best heuristic 
the heuristics used to generate traditional pseudotrees perform 
a distributed dfs traversal the general distributed algorithm uses 
a token passing mechanism and a linear number of messages 
improved dfs based heuristics use a special procedure to choose the 
root node and also provide an ordering function over the neighbors 
of a node to determine the order of path recursion the dfs based 
heuristics used in our experiments come from the work done in 
 
 the best-first cross-edged pseudotree 
heuristic 
the heuristics used to generate cross-edged pseudotrees 
perform a best-first traversal a general distributed best-first 
algorithm for node expansion is presented in algorithm an 
evaluation function at each node provides the values that are used to 
determine the next best node to expand note that in this 
algorithm each node only exchanges its best value with its neighbors 
in our experiments we used several evaluation functions that took 
as arguments an ordered list of ancestors and a node which 
contains a list of neighbors with each neighbor s placement depth in 
the tree if it was placed from these we can calculate 
branchparents branch-children and unknown relationships for a potential 
node placement the best overall function calculated the value as 
ancestors− branchparents branchchildren with the 
number of unknown relationships being a tiebreak after completion 
each node has knowledge of its parent and ancestors so it can 
easily determine which connected nodes are pseudo-parents 
branchparents pseudo-children and branch-children 
the complexity of the best-first traversal depends on the 
complexity of the evaluation function assuming a complexity of o v 
for the evaluation function which is the case for our best 
overall function the best-first traversal is o v · e which is at worst 
o n 
 for each v ∈ v we perform a place operation and find the 
next node to place using the getbestneighbor operation the place 
operation is at most o v because of the sent messages 
finding the next node uses recursion and traverses only already placed 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
algorithm distributed best-first search algorithm 
root ← electedleader 
next root ∅ 
place node parent 
node parent ← parent 
node ancestors ← parent ancestors ∪ parent 
send placement message node node ancestors to all 
neighbors of node 
next current previous 
if current is not placed then 
place current previous 
next current ∅ 
else 
best ← getbestneighbor current previous 
if best ∅ then 
if previous ∅ then 
terminate all nodes are placed 
next previous ∅ 
else 
next best current 
getbestneighbor current previous 
best ← ∅ score ← 
for all n ∈ current neighbors do 
if n previous then 
if n is placed then 
nscore ← getbestneighbor n current 
else 
nscore ← evaluate current n 
if nscore score then 
score ← nscore 
best ← n 
return best score 
nodes so it has o v recursions each recursion performs a 
recursive getbestneighbor operation that traverses all placed nodes 
and their neighbors this operation is o v · e but results can 
be cached using only o v space at each node thus we have 
o v · v v v ·e o v 
·e if we are smart about evaluating 
local changes when each node receives placement messages from 
its neighbors and cache the results the getbestneighbor operation 
is only o e this increases the complexity of the place operation 
but for all placements the total complexity is only o v · e thus 
we have an overall complexity of o v ·e v · v e o v ·e 
 comparison of complexity in 
dpop and dcpop 
we have already shown that given the same input dcpop 
performs the same as dpop we also have shown that we can 
accurately predict performance of a given pseudotree in linear 
spacetime complexity if we use a constant number of heuristics to 
generate the set of pseudotrees we can choose the best pseudotree in 
linear space-time complexity we will now show that there exists 
a dcop instance for which a cross-edged pseudotree outperforms 
all possible traditional pseudotrees based on edge-traversal 
heuristics 
in figure a we have a dcop instance with six nodes this 
is a bipartite graph with each partition fully connected to the other 
 a b c 
figure a the dcop instance b a traditional pseudotree 
arrangement for the dcop instance c a cross-edged 
pseudotree arrangement for the dcop instance 
partition in figure b we see a traditional pseudotree 
arrangement for this dcop instance it is easy to see that any 
edgetraversal based heuristic cannot expand two nodes from the same 
partition in succession we also see that no node can have more 
than one child because any such arrangement would be an invalid 
pseudotree thus any traditional pseudotree arrangement for this 
dcop instance must take the form of figure b we can see that 
the back-edges f-b and f-a overlap node c node c also has a 
parent e and a back-edge with d using the original dpop 
algorithm or dcpop since they are identical in this case we find that 
the computation at node c involves five domains a b c d and 
e 
in contrast the cross-edged pseudotree arrangement in 
figure c requires only a maximum of four domains in any 
computation during dcpop since node a is the merge point for branches 
from both b and c we can see that each of the nodes d e and f 
have two overlapping branches in addition each of these nodes has 
node a as its parent using the dcpop algorithm we find that the 
computation at node d or e or f involves four domains a b c 
and d or e or f 
since no better traditional pseudotree arrangement can be 
created using an edge-traversal heuristic we have shown that dcpop 
can outperform dpop even if we use the optimal pseudotree found 
through edge-traversal we acknowledge that pseudotree 
arrangements that allow parent-child relationships without an actual 
constraint can solve the problem in figure a with maximum 
computation size of four domains however current heuristics used 
with dpop do not produce such pseudotrees and such a heuristic 
would be difficult to distribute since each node would require 
information about nodes with which it has no constraint also while we 
do not prove it here cross-edged pseudotrees can produce smaller 
message sizes than such pseudotrees even if the computation size 
is similar in practice since finding the best pseudotree 
arrangement is np-hard we find that heuristics that produce cross-edged 
pseudotrees often produce significantly smaller computation and 
message sizes 
 experimental results 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
existing performance metrics for dcop algorithms include the 
total number of messages synchronous clock cycles and message 
size we have already shown that the total number of messages is 
linear with respect to the number of constraints in the dcop 
instance we also introduced the maximum sequential path cost pc 
as a measurement of the maximum amount of parallelism 
achievable by the algorithm the maximum sequential path cost is equal 
to the sum of the computations performed on the longest path from 
the root to any leaf node we also include as metrics the 
maximum computation size in number of dimensions cd and 
maximum message size in number of dimensions md to analyze the 
relative complexity of a given dcop instance we find the 
minimum induced width iw of any traditional pseudotree produced 
by a heuristic for the original dpop 
 generic dcop instances 
for our initial tests we randomly generated two sets of problems 
with cases in each each problem was generated by 
assigning a random number picked from a range of constraints to each 
variable the generator then created binary constraints until each 
variable reached its maximum number of constraints the first set 
uses variables and the best dpop iw ranges from to with 
an average of the second set uses variables and the best 
dpop iw ranged from to with an average of since most 
of the problems in the second set were too complex to actually 
compute the solution we took measurements of the metrics using the 
techniques described earlier in section without actually solving 
the problem results are shown for the first set in table and for 
the second set in table 
for the two problem sets we split the cases into low density and 
high density categories low density cases consist of those 
problems that have a best dpop iw less than or equal to half of the 
total number of nodes e g iw ≤ for the node problems 
and iw ≤ for the node problems high density problems 
consist of the remainder of the problem sets 
in both table and table we have listed performance 
metrics for the original dpop algorithm the dcpop algorithm using 
only cross-edged pseudotrees dcpop-ce and the dcpop 
algorithm using traditional and cross-edged pseudotrees dcpop-all 
the pseudotrees used for dpop were generated using 
heuristics dfs dfs mcn dfs clique mcn dfs mcn dstb 
and dfs mcn bec these are all versions of the guided dfs 
traversal discussed in section the cross-edged pseudotrees used 
for dcpop-ce were generated using heuristics mcn lcn 
mcn a-b lcn a-b and lcsg a-b these are all versions of 
the best-first traversal discussed in section 
for both dpop and dcpop-ce we chose the best pseudotree 
produced by their respective heuristics for each problem in the 
set for dcpop-all we chose the best pseudotree produced by all 
 heuristics for each problem in the set for the cd and md 
metrics the value shown is the average number of dimensions for the 
pc metric the value shown is the natural logarithm of the 
maximum sequential path cost since the actual value grows 
exponentially with the complexity of the problem 
the final row in both tables is a measurement of improvement 
of dcpop-all over dpop for the cd and md metrics the value 
shown is a reduction in number of dimensions for the pc metric 
the value shown is a percentage reduction in the maximum 
sequential path cost dp op −dcp op 
dcp op 
∗ notice that 
dcpopall outperforms dpop on all metrics this logically follows from 
our earlier assertion that given the same input dcpop performs 
exactly the same as dpop thus given the choice between the 
pseudotrees produced by all heuristics dcpop-all will always 
outlow density high density 
algorithm cd md pc cd md pc 
dpop 
dcpop-ce 
dcpop-all 
improvement 
table node problems 
low density high density 
algorithm cd md pc cd md pc 
dpop 
dcpop-ce 
dcpop-all 
improvement 
table node problems 
figure computation dimension size 
figure message dimension size 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure path cost 
dcpop improvement 
ag mtg vars const iw cd md pc 
 - - 
 
 
 
 
 
table meeting scheduling problems 
perform dpop another trend we notice is that the improvement is 
greater for high density problems than low density problems we 
show this trend in greater detail in figures and notice 
how the improvement increases as the complexity of the problem 
increases 
 meeting scheduling problem 
in addition to our initial generic dcop tests we ran a series 
of tests on the meeting scheduling problem msp as described 
in the problem setup includes a number of people that are 
grouped into departments each person must attend a specified 
number of meetings meetings can be held within departments or 
among departments and can be assigned to one of eight time slots 
the msp maps to a dcop instance where each variable represents 
the time slot that a specific person will attend a specific meeting 
all variables that belong to the same person have mutual exclusion 
constraints placed so that the person cannot attend more than one 
meeting during the same time slot all variables that belong to the 
same meeting have equality constraints so that all of the 
participants choose the same time slot unary constraints are placed on 
each variable to account for a person s valuation of each meeting 
and time slot 
for our tests we generated sample problems for each 
combination of agents and meetings results are shown in table the 
values in the first five columns represent in left to right order the 
total number of agents the total number of meetings the total 
number of variables the average total number of constraints and the 
average minimum iw produced by a traditional pseudotree the 
last three columns show the same metrics we used for the generic 
dcop instances except this time we only show the improvements 
of dcpop-all over dpop performance is better on average for 
all msp instances but again we see larger improvements for more 
complex problem instances 
 conclusions and future work 
we presented a complete distributed algorithm that solves 
general dcop instances using cross-edged pseudotree arrangements 
our algorithm extends the dpop algorithm by adding additional 
utility propagation messages and introducing the concept of branch 
merging during the utility propagation phase our algorithm also 
allows value assignments to occur at higher level merge points 
for lower level nodes we have shown that dcpop fully extends 
dpop by performing the same operations given the same input 
we have also shown through some examples and experimental data 
that dcpop can achieve greater performance for some problem 
instances by extending the allowable input set to include cross-edged 
pseudotrees 
we placed particular emphasis on the role that edge-traversal 
heuristics play in the generation of pseudotrees we have shown 
that the performance penalty is minimal to generate multiple 
heuristics and that we can choose the best generated pseudotree 
in linear space-time complexity given the importance of a good 
pseudotree for performance future work will include new 
heuristics to find better pseudotrees future work will also include 
adapting existing dpop extensions that support different problem 
domains for use with dcpop 
 references 
 j liu and k p sycara exploiting problem structure for 
distributed constraint optimization in v lesser editor 
proceedings of the first international conference on 
multi-agent systems pages - san francisco ca 
 mit press 
 p j modi h jung m tambe w -m shen and s kulkarni 
a dynamic distributed constraint satisfaction approach to 
resource allocation lecture notes in computer science 
 - 
 p j modi w shen m tambe and m yokoo an 
asynchronous complete method for distributed constraint 
optimization in aamas 
 a petcu frodo a framework for open distributed 
constraint optimization technical report no 
 swiss federal institute of technology epfl 
lausanne switzerland http liawww epfl ch frodo 
 a petcu and b faltings a-dpop approximations in 
distributed optimization in poster in cp pages 
 - sitges spain october 
 a petcu and b faltings dpop a scalable method for 
multiagent constraint optimization in ijcai pages 
 - edinburgh scotland aug 
 a petcu b faltings and d parkes m-dpop faithful 
distributed implementation of efficient social choice 
problems in aamas pages - hakodate 
japan may 
 g ushakov solving meeting scheduling problems using 
distributed pseudotree-optimization procedure master s 
thesis ´ecole polytechnique f´ed´erale de lausanne 
 m yokoo e h durfee t ishida and k kuwabara 
distributed constraint satisfaction for formalizing distributed 
problem solving in international conference on distributed 
computing systems pages - 
 m yokoo e h durfee t ishida and k kuwabara the 
distributed constraint satisfaction problem formalization 
and algorithms knowledge and data engineering 
 - 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
normative system games 
thomas 
◦ 
agotnes 
dept of computer engineering 
bergen university college 
pb n- bergen 
norway 
tag hib no 
wiebe van der hoek 
dept of computer science 
university of liverpool 
liverpool l zf 
uk 
wiebe csc liv ac uk 
michael wooldridge 
dept of computer science 
university of liverpool 
liverpool l zf 
uk 
mjw csc liv ac uk 
abstract 
we develop a model of normative systems in which agents are 
assumed to have multiple goals of increasing priority and 
investigate the computational complexity and game theoretic properties of 
this model in the underlying model of normative systems we use 
kripke structures to represent the possible transitions of a 
multiagent system a normative system is then simply a subset of the 
kripke structure which contains the arcs that are forbidden by the 
normative system we specify an agent s goals as a hierarchy of 
formulae of computation tree logic ctl a widely used logic 
for representing the properties of kripke structures the intuition is 
that goals further up the hierarchy are preferred by the agent over 
those that appear further down the hierarchy using this scheme 
we define a model of ordinal utility which in turn allows us to 
interpret our kripke-based normative systems as games in which 
agents must determine whether to comply with the normative 
system or not we then characterise the computational complexity of 
a number of decision problems associated with these kripke-based 
normative system games for example we show that the 
complexity of checking whether there exists a normative system which has 
the property of being a nash implementation is np-complete 
categories and subject descriptors 
i distributed artificial intelligence multiagent systems 
i knowledge representation formalisms and methods 
general terms 
theory 
 introduction 
normative systems or social laws have proved to be an attractive 
approach to coordination in multi-agent systems 
although the various approaches to normative systems proposed in 
the literature differ on technical details they all share the same 
basic intuition that a normative system is a set of constraints on the 
behaviour of agents in the system by imposing these constraints 
it is hoped that some desirable objective will emerge the idea of 
using social laws to coordinate multi-agent systems was proposed 
by shoham and tennenholtz their approach was extended 
by van der hoek et al to include the idea of specifying a desirable 
global objective for a social law as a logical formula with the idea 
being that the normative system would be regarded as successful 
if after implementing it i e after eliminating all forbidden 
actions the objective formula was guaranteed to be satisfied in the 
system however this model did not take into account the 
preferences of individual agents and hence neglected to account 
for possible strategic behaviour by agents when deciding whether 
to comply with the normative system or not this model of 
normative systems was further extended by attributing to each agent 
a single goal in however this model was still too 
impoverished to capture the kinds of decision making that take place when 
an agent decides whether or not to comply with a social law in 
reality strategic considerations come into play an agent takes into 
account not just whether the normative system would be beneficial 
for itself but also whether other agents will rationally choose to 
participate 
in this paper we develop a model of normative systems in which 
agents are assumed to have multiple goals of increasing priority 
we specify an agent s goals as a hierarchy of formulae of 
computation tree logic ctl a widely used logic for representing the 
properties of kripke structures the intuition is that goals further 
up the hierarchy are preferred by the agent over those that appear 
further down the hierarchy using this scheme we define a model 
of ordinal utility which in turn allows us to interpret our 
kripkebased normative systems as games in which agents must determine 
whether to comply with the normative system or not we thus 
provide a very natural bridge between logical structures and languages 
and the techniques and concepts of game theory which have proved 
to be very powerful for analysing social contract-style scenarios 
such as normative systems we then characterise the 
computational complexity of a number of decision problems associated 
with these kripke-based normative system games for example we 
show that the complexity of checking whether there exists a 
normative system which has the property of being a nash implementation 
is np-complete 
 kripke structures and ctl 
we use kripke structures as our basic semantic model for 
multiagent systems a kripke structure is essentially a directed 
graph with the vertex set s corresponding to possible states of the 
system being modelled and the relation r ⊆ s × s capturing the 
 
 - - - - rps c ifaamas 
possible transitions of the system intuitively these transitions are 
caused by agents in the system performing actions although we do 
not include such actions in our semantic model see e g 
 for related models which include actions as first class citizens 
we let s 
denote the set of possible initial states of the system 
our model is intended to correspond to the well-known interleaved 
concurrency model from the reactive systems literature thus an 
arc corresponds to the execution of an atomic action by one of the 
processes in the system which we call agents 
it is important to note that in contrast to such models as 
we are therefore here not modelling synchronous action this 
assumption is not in fact essential for our analysis but it greatly 
simplifies the presentation however we find it convenient to include 
within our model the agents that cause transitions we therefore 
assume a set a of agents and we label each transition in r with 
the agent that causes the transition via a function α r → a 
finally we use a vocabulary φ {p q } of boolean variables 
to express the properties of individual states s we use a function 
v s → φ 
to label each state with the boolean variables true or 
satisfied in that state 
collecting these components together an agent-labelled kripke 
structure over φ is a -tuple 
k s s 
 r a α v where 
 s is a finite non-empty set of states 
 s 
⊆ s s 
 ∅ is the set of initial states 
 r ⊆ s × s is a total binary relation on s which we refer to 
as the transition relation 
 
 a { n} is a set of agents 
 α r → a labels each transition in r with an agent and 
 v s → φ 
labels each state with the set of propositional 
variables true in that state 
in the interests of brevity we shall hereafter refer to an 
agentlabelled kripke structure simply as a kripke structure a path 
over a transition relation r is an infinite sequence of states π 
s s which must satisfy the property that ∀u ∈ n su su ∈ 
r if u ∈ n then we denote by π u the component indexed by 
u in π thus π denotes the first element π the second and so 
on a path π such that π s is an s-path let πr s denote 
the set of s-paths over r since it will usually be clear from 
context we often omit reference to r and simply write π s we will 
sometimes refer to and think of an s-path as a possible 
computation or system evolution from s 
example our running example is of a system with a single 
non-sharable resource which is desired by two agents consider 
the kripke structure depicted in figure we have two states s and 
t and two corresponding boolean variables p and p which are 
 
in the branching time temporal logic literature a relation r ⊆ 
s × s is said to be total iff ∀s ∃s s s ∈ r note that 
the term total relation is sometimes used to refer to relations 
r ⊆ s × s such that for every pair of elements s s ∈ s we 
have either s s ∈ r or s s ∈ r we are not using the term 
in this way here it is also worth noting that for some domains 
other constraints may be more appropriate than simple totality for 
example one might consider the agent totality requirement that in 
every state every agent has at least one possible transition 
available ∀s∀i ∈ a∃s s s ∈ r and α s s i 
 p 
t 
p 
 
 
 
s 
 
 
figure the resource control running example 
mutually exclusive think of pi as meaning agent i has currently 
control over the resource each agent has two possible actions 
when in possession of the resource either give it away or keep it 
obviously there are infinitely many different s-paths and t-paths 
let us say that our set of initial states s 
equals {s t} i e we 
don t make any assumptions about who initially has control over 
the resource 
 ctl 
we now define computation tree logic ctl a branching time 
temporal logic intended for representing the properties of kripke 
structures note that since ctl is well known and widely 
documented in the literature our presentation though complete will be 
somewhat terse we will use ctl to express agents goals 
the syntax of ctl is defined by the following grammar 
ϕ p ¬ϕ ϕ ∨ ϕ e fϕ e ϕ u ϕ a fϕ a ϕ u ϕ 
where p ∈ φ we denote the set of ctl formula over φ by lφ 
since φ is understood we usually omit reference to it 
the semantics of ctl are given with respect to the satisfaction 
relation which holds between pairs of the form k s where 
k is a kripke structure and s is a state in k and formulae of the 
language the satisfaction relation is defined as follows 
k s 
k s p iff p ∈ v s where p ∈ φ 
k s ¬ϕ iff not k s ϕ 
k s ϕ ∨ ψ iff k s ϕ or k s ψ 
k s a fϕ iff ∀π ∈ π s k π ϕ 
k s e fϕ iff ∃π ∈ π s k π ϕ 
k s a ϕ u ψ iff ∀π ∈ π s ∃u ∈ n s t k π u ψ 
and ∀v ≤ v u k π v ϕ 
k s e ϕ u ψ iff ∃π ∈ π s ∃u ∈ n s t k π u ψ 
and ∀v ≤ v u k π v ϕ 
the remaining classical logic connectives ∧ → ↔ are 
assumed to be defined as abbreviations in terms of ¬ ∨ in the 
conventional manner the remaining ctl temporal operators are 
defined 
a♦ϕ ≡ a u ϕ e♦ϕ ≡ e u ϕ 
a ϕ ≡ ¬e♦¬ϕ e ϕ ≡ ¬a♦¬ϕ 
we say ϕ is satisfiable if k s ϕ for some kripke structure k 
and state s in k ϕ is valid if k s ϕ for all kripke structures 
k and states s in k the problem of checking whether k s ϕ 
for given k s ϕ model checking can be done in deterministic 
polynomial time while checking whether a given ϕ is satisfiable or 
whether ϕ is valid is exptime-complete we write k ϕ if 
k s ϕ for all s ∈ s 
 and ϕ if k ϕ for all k 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 normative systems 
for our purposes a normative system is simply a set of constraints 
on the behaviour of agents in a system more precisely a 
normative system defines for every possible system transition whether 
or not that transition is considered to be legal or not different 
normative systems may differ on whether or not a transition is 
legal formally a normative system η w r t a kripke structure 
k s s 
 r a α v is simply a subset of r such that r \ η 
is a total relation the requirement that r\η is total is a 
reasonableness constraint it prevents normative systems which lead to states 
with no successor let n r {η η ⊆ r r \ η is total } 
be the set of normative systems over r the intended 
interpretation of a normative system η is that s s ∈ η means transition 
 s s is forbidden in the context of η hence r \ η denotes the 
legal transitions of η since it is assumed η is reasonable we are 
guaranteed that a legal outward transition exists for every state we 
denote the empty normative system by η∅ so η∅ ∅ note that 
the empty normative system η∅ is reasonable with respect to any 
transition relation r 
the effect of implementing a normative system on a kripke 
structure is to eliminate from it all transitions that are forbidden 
according to this normative system see if k is a kripke 
structure and η is a normative system over k then k † η denotes the 
kripke structure obtained from k by deleting transitions forbidden 
in η formally if k s s 
 r a α v and η ∈ n r then 
let k†η k be the kripke structure k s s 
 r a α v 
where 
 s s s 
 s 
 a a and v v 
 r r \ η and 
 α is the restriction of α to r 
α s s 
j 
α s s if s s ∈ r 
undefined otherwise 
notice that for all k we have k † η∅ k 
example continued when thinking in terms of fairness it 
seems natural to consider normative systems η that contain s s 
or t t a normative system with s t would not be fair in the 
sense that a♦a ¬p ∨ a♦a ¬p holds in all paths from 
some moment on one agent will have control forever let us for 
later reference fix η { s s } η { t t } and η { s s 
 t t } 
later we will address the issue of whether or not agents should 
rationally choose to comply with a particular normative system in 
this context it is useful to define operators on normative systems 
which correspond to groups of agents defecting from the 
normative system formally let k s s 
 r a α v be a kripke 
structure let c ⊆ a be a set of agents over k and let η be a 
normative system over k then 
 η c denotes the normative system that is the same as η 
except that it only contains the arcs of η that correspond to 
the actions of agents in c we call η c the restriction of η 
to c and it is defined as 
η c { s s s s ∈ η α s s ∈ c} 
thus k † η c is the kripke structure that results if only 
the agents in c choose to comply with the normative system 
 η c denotes the normative system that is the same as η 
except that it only contains the arcs of η that do not correspond 
to actions of agents in c we call η c the exclusion of c 
from η and it is defined as 
η c { s s s s ∈ η α s s ∈ c} 
thus k † η c is the kripke structure that results if only 
the agents in c choose not to comply with the normative 
system i e the only ones who comply are those in a \ c 
note that we have η c η a\c and η c η a\c 
example continued we have η { } η { s s } 
while η { } η∅ η { } similarly we have η { } 
{ s s } and η { } { t t } 
 goals and utilities 
next we want to be able to capture the goals that agents have as 
these will drive an agent s strategic considerations - particularly as 
we will see considerations about whether or not to comply with a 
normative system we will model an agent s goals as a prioritised 
list of ctl formulae representing increasingly desired properties 
that the agent wishes to hold the intended interpretation of such a 
goal hierarchy γi for agent i ∈ a is that the further up the 
hierarchy a goal is the more it is desired by i note that we assume 
that if an agent can achieve a goal at a particular level in its goal 
hierarchy then it is unconcerned about goals lower down the 
hierarchy formally a goal hierarchy γ over a kripke structure k 
is a finite non-empty sequence of ctl formulae 
γ ϕ ϕ ϕk 
in which by convention ϕ we use a natural number 
indexing notation to extract the elements of a goal hierarchy so if 
γ ϕ ϕ ϕk then γ ϕ γ ϕ and so on we 
denote the largest index of any element in γ by γ 
a particular kripke structure k is said to satisfy a goal at 
index x in goal hierarchy γ if k γ x i e if γ x is satisfied in all 
initial states s 
of k an obvious potential property of goal 
hierarchies is monotonicity where goals at higher levels in the hierarchy 
logically imply those at lower levels in the hierarchy formally a 
goal hierarchy γ is monotonic if for all x ∈ { γ } ⊆ n we 
have γ x → γ x − the simplest type of monotonic goal 
hierarchy is where γ x γ x ∧ ψx for some ψx so at 
each successive level of the hierarchy we add new constraints to 
the goal of the previous level although this is a natural property 
of many goal hierarchies it is not a property we demand of all goal 
hierarchies 
example continued suppose the agents have similar but 
opposing goals each agent i wants to keep the source as often and 
long as possible for himself define each agent s goal hierarchy as 
γi ϕi 
 ϕi 
 e♦pi 
ϕi 
 e e♦pi ϕi 
 e♦e pi 
ϕi 
 a e♦pi ϕi 
 e♦a pi 
ϕi 
 a a♦pi ϕi 
 a a♦pi ∧ e pi 
ϕi 
 a pi 
the most desired goal of agent i is to in every computation 
always have the resource pi this is expressed in ϕi 
 thanks to our 
reasonableness constraint this goal implies ϕi 
 which says that no 
matter how the computation paths evolve it will always be that all 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
continuations will hit a point in which pi and moreover there is a 
continuation in which pi always holds goal ϕi 
 is a fairness 
constraint implied by it note that a♦pi says that every computation 
eventually reaches a pi state this may mean that after pi has 
happened it will never happen again ϕi 
 circumvents this it says that 
no matter where you are there should be a future pi state the goal 
ϕi 
 is like the strong goal ϕi 
 but it accepts that this is only achieved 
in some computation eventually ϕi 
 requires that in every path 
there is always a continuation that eventually gives pi goal ϕi 
 
says that pi should be true on some branch from some moment on 
it implies ϕi 
 which expresses that there is a computation such that 
everywhere during it it is possible to choose a continuation that 
eventually satisfies pi this implies ϕi 
 which says that pi should 
at least not be impossible if we even drop that demand we have 
the trivial goal ϕi 
 
we remark that it may seem more natural to express a fairness 
constraint ϕi 
 as a ♦pi however this is not a proper ctl 
formula it is in fact a formula in ctl 
∗ 
 and in this logic the two 
expressions would be equivalent however our basic complexity 
results in the next sections would not hold for the richer language 
ctl 
∗ 
 and the price to pay for this is that we have to formulate 
our desired goals in a somewhat more cumbersome manner than 
we might ideally like of course our basic framework does not 
demand that goals are expressed in ctl they could equally well 
be expressed in ctl 
∗ 
or indeed atl as in we 
comment on the implications of alternative goal representations at the 
conclusion of the next section 
a multi-agent system collects together a kripke structure 
 representing the basic properties of a system under consideration its 
state space and the possible state transitions that may occur in it 
together with a goal hierarchy one for each agent representing the 
aspirations of the agents in the system formally a multi-agent 
system m is an n -tuple 
m k γ γn 
where k is a kripke structure and for each agent i in k γi is a 
goal hierarchy over k 
 the utility of normative systems 
we can now define the utility of a kripke structure for an agent 
the idea is that the utility of a kripke structure is the highest index 
of any goal that is guaranteed for that agent in the kripke structure 
we make this precise in the function ui · 
ui k max{j ≤ j ≤ γi k γi j } 
note that using these definitions of goals and utility it never 
makes sense to have a goal ϕ at index n if there is a logically 
weaker goal ψ at index n k in the hierarchy by definition of 
utility it could never be n for any structure k 
example continued let m k γ γ be the 
multiagent system of figure with γ and γ as defined earlier in this 
example recall that we have defined s 
as {s t} then u k 
u k goal ϕ is true in s 
 but ϕ is not to see that 
ϕ 
 a e♦p is true in s for instance note that on ever path it 
is always the case that there is a transition to t in which p is true 
notice that since for any goal hierarchy γi we have γ 
then for all kripke structures ui k is well defined with ui k ≥ 
 
ctl 
∗ 
model checking is pspace-complete and hence much 
worse under standard complexity theoretic assumptions than 
model checking ctl 
η δ k η δ k η 
η∅ 
η 
η 
η 
c d 
c 
d 
figure benefits of implementing a normative system η left 
and pay-offs for the game σm 
 note that this is an ordinal utility measure it tells us for any 
given agent the relative utility of different kripke structures but 
utility values are not on some standard system-wide scale the fact 
that ui k ui k certainly means that i strictly prefers k 
over k but the fact that ui k uj k does not mean that i 
values k more highly than j thus it does not make sense to 
compare utility values between agents and so for example some system 
wide measures of utility notably those measures that aggregate 
individual utilities such as social welfare do not make sense when 
applied in this setting however as we shall see shortly other 
measures - such as pareto efficiency - can be usefully applied 
there are other representations for goals which would allow us 
to define cardinal utilities the simplest would be to specify goals γ 
for an agent as a finite non-empty one-to-one relation γ ⊆ l×r 
we assume that the x values in pairs ϕ x ∈ γ are specified so 
that x for agent i means the same as x for agent j and so we have 
cardinal utility we then define the utility for i of a kripke structure 
k asui k max{x ϕ x ∈ γi k ϕ} the results of 
this paper in fact hold irrespective of which of these representations 
we actually choose we fix upon the goal hierarchy approach in the 
interests of simplicity 
our next step is to show how in much the same way we can lift 
the utility function from kripke structures to normative systems 
suppose we are given a multi-agent system m k γ γn 
and an associated normative system η over k let for agent i 
δi k k be the difference in his utility when moving from k to 
k δi k k ui k − ui k then the utility of η to agent i 
wrt k is δi k k † η we will sometimes abuse notation and just 
write δi k η for this and refer to it as the benefit for agent i of 
implementing η in k note that this benefit can be negative 
summarising the utility of a normative system to an agent is the 
difference between the utility of the kripke structure in which the 
normative system was implemented and the original kripke 
structure if this value is greater than then the agent would be better 
off if the normative system were imposed while if it is less than 
 then the agent would be worse off if η were imposed than in the 
original system we say η is individually rational for i wrt k if 
δi k η and individually rational simpliciter if η is 
individually rational for every agent 
a social system now is a pair 
σ m η 
where m is a multi-agent system and η is a normative system over 
m 
example the table at the left hand in figure displays the 
utilities δi k η of implementing η in the kripke structure of our 
running example for the normative systems η η∅ η η and η 
introduced before recall that u k u k 
 universal and existential goals 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
keeping in mind that a norm η restricts the possible transitions 
of the model under consideration we make the following 
observation borrowing from some classes of goals are monotonic 
or anti-monotonic with respect to adding additional constraints to 
a system let us therefore define two fragments of the language 
of ctl the universal language lu 
with typical element μ and the 
existential fragment le 
with typical element ε 
μ p ¬p μ ∨ μ a fμ a μ a μ u μ 
ε p ¬p ε ∨ ε e fε e♦ε e ε u ε 
let us say for two kripke structures k s s 
 r a α v 
and k s s 
 r a α v that k is a subsystem of k and 
k is a supersystem of k written k k iff r ⊆ r note 
that typically k † η k then we have cf 
theorem suppose k k and s ∈ s then 
∀ε ∈ le 
 k s ε ⇒ k s ε 
∀μ ∈ lu 
 k s μ ⇒ k s μ 
this has the following effect on imposing a new norm 
corollary let k be a structure and η a normative 
system let γi denote a goal hierarchy for agent i 
 suppose agent i s utility ui k is n and γi n ∈ lu 
 i e 
γi n is a universal formula then for any normative system 
η δi k η ≥ 
 suppose agent i s utility ui k † η is n and γi n is an 
existential formula ε then δi k † η k ≥ 
corollary s first item says that an agent whose current 
maximal goal in a system is a universal formula need never fear the 
imposition of a new norm η the reason is that his current goal will 
at least remain true in fact a goal higher up in the hierarchy may 
become true it follows from this that an agent with only universal 
goals can only gain from the imposition of normative systems η 
the opposite is true for existential goals according to the second 
item of the corollary it can never be bad for an agent to undo a 
norm η hence an agent with only existential goals might well fear 
any norm η 
however these observations implicitly assume that all agents in 
the system will comply with the norm whether they will in fact do 
so of course is a strategic decision it partly depends on what the 
agent thinks that other agents will do this motivates us to consider 
normative system games 
 normative system games 
we now have a principled way of talking about the utility of 
normative systems for agents and so we can start to apply the technical 
apparatus of game theory to analyse them 
suppose we have a multi-agent system m k γ γn 
and a normative system η over k it is proposed to the agents 
in m that η should be imposed on k typically to achieve some 
coordination objective our agent - let s say agent i - is then faced 
with a choice should it comply with the strictures of the normative 
system or not note that this reasoning takes place before the agent 
is in the system - it is a design time consideration 
we can understand the reasoning here as a game as follows a 
game in strategic normal form cf p is a structure 
g ag s sn u un where 
 ag { n} is a set of agents - the players of the game 
 si is the set of strategies for each agent i ∈ ag a strategy 
for an agent i is nothing else than a choice between 
alternative actions and 
 ui s × · · · × sn → r is the utility function for agent 
i ∈ ag which assigns a utility to every combination of 
strategy choices for the agents 
now suppose we are given a social system σ m η where 
m k γ γn then we can associate a game - the 
normative system game - gς with σ as follows the agents ag in gς 
are as in σ each agent i has just two strategies available to it 
 c - comply cooperate with the normative system and 
 d - do not comply with defect from the normative system 
if s is a tuple of strategies one for each agent and x ∈ {c d} 
then we denote by agx 
s the subset of agents that play strategy x in 
s hence for a social system σ m η the normative system 
η agc 
s only implements the restrictions for those agents that 
choose to cooperate in gς note that this is the same as η agd 
s 
the normative system that excludes all the restrictions of agents that 
play d in gς we then define the utility functions ui for each 
i ∈ ag as 
ui s δi k η agc 
s 
so for example if sd is a collection of strategies in which every 
agent defects i e does not comply with the norm then 
ui sd δi k η agd 
sd 
 ui k † η∅ − ui k 
in the same way if sc is a collection of strategies in which every 
agent cooperates i e complies with the norm then 
ui sc δi k η agd 
sc 
 ui k † η ∅ ui k † η 
we can now start to investigate some properties of normative 
system games 
example continued for our example system we have 
displayed the different u values for our multi agent system with the 
norm η i e { s s t t } as the second table of figure for 
instance the pair in the matrix under the entry s c d 
is obtained as follows u c d δ k η agc 
c d 
u k † η agc 
c d − u k the first term of this is the 
utility of in the system k where we implement η for the 
cooperating agent i e only this means that the transitions are 
r \ { s s } in this system still ϕ 
 a e♦p is the highest 
goal for agent this is the same utility for as in k and hence 
δ k η agc 
c d agent of course benefits if agent 
complies with η while does not his utility would be since 
η agc 
c d is in fact η 
 individually rational normative systems 
a normative system is individually rational if every agent would 
fare better if the normative system were imposed than otherwise 
this is a necessary although not sufficient condition on a norm to 
expect that everybody respects it note that η of our example is 
individually rational for both and although this is not a stable 
situation given that the other plays c i is better of by playing 
d we can easily characterise individually rationality with respect 
to the corresponding game in strategic form as follows let σ 
m η be a social system then the following are equivalent 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
f xk 
 
s 
s 
s 
s 
s 
s k− 
s k 
t x 
f x 
t x 
f x 
t xk 
figure the kripke structure produced in the reduction of 
theorem all transitions are associated with agent the only 
initial state is s 
 η is individually rational in m 
 ∀i ∈ ag ui sc ui sd in the game gς 
the decision problem associated with individually rational 
normative systems is as follows 
individually rational normative system irns 
given multi-agent system m 
question does there exist an individually rational 
normative system for m 
theorem irns is np-complete even in one-agent systems 
proof for membership of np guess a normative system η 
and verify that it is individually rational since η ⊆ r we will be 
able to guess it in nondeterministic polynomial time to verify that 
it is individually rational we check that for all i we have ui k † 
η ui k computing k † η is just set subtraction so can be 
done in polynomial time while determining the value of ui k for 
any k can be done with a polynomial number of model checking 
calls each of which requires only time polynomial in the k and γ 
hence verifying that ui k † η ui k requires only polynomial 
time 
for np-hardness we reduce sat p given a sat instance 
ϕ over boolean variables x xk we produce an instance of 
irns as follows first we define a single agent a { } for each 
boolean variable xi in the sat instance we create two boolean 
variables t xi and f xi in the irns instance we then create a 
kripke structure kϕ with k states as shown in figure arcs 
in this graph correspond to transitions in kϕ let ϕ∗ 
be the result 
of systematically substituting for every boolean variable xi in ϕ 
the ctl expression e ft xi next consider the following 
formulae 
k 
i 
e f t xi ∨ f xi 
k 
i 
¬ e ft xi ∧ e ff xi 
we then define the goal hierarchy for all agent as follows 
γ 
γ ∧ ∧ ϕ∗ 
we claim there is an individually rational normative system for the 
instance so constructed iff ϕ is satisfiable first notice that any 
individually rational normative system must force γ to be true 
since in the original system we do not have γ 
for the ⇒ direction if there is an individually rational normative 
system η then we construct a satisfying assignment for ϕ by 
considering the arcs that are forbidden by η formula ensures that 
we must forbid an arc to either a t xi or a f xi state for all 
variables xi but ensures that we cannot forbid arcs to both so if 
we forbid an arc to a t xi state then in the corresponding valuation 
for ϕ we make xi false while if we forbid an arc to a f xi state 
then we make xi true the fact that ϕ∗ 
is part of the goal ensures 
that the normative system is indeed a valuation for ϕ 
for ⇐ note that for any satisfying valuation for ϕ we can 
construct an individually rational normative system η as follows if 
the valuation makes xi true we forbid the arc to the f xi state 
while if the valuation makes xi false we forbid the arc to the t xi 
state the resulting normative system ensures γ and is thus 
individually rational 
notice that the kripke structure constructed in the reduction 
contains just a single agent and so the theorem is proven 
 pareto efficient normative systems 
pareto efficiency is a basic measure of how good a particular 
outcome is for a group of agents p intuitively an outcome 
is pareto efficient if there is no other outcome that makes every 
agent better off in our framework suppose we are given a social 
system σ m η and asked whether η is pareto efficient this 
amounts to asking whether or not there is some other normative 
system η such that every agent would be better off under η than 
with η if η makes every agent better off than η then we say η 
pareto dominates η the decision problem is as follows 
pareto efficient normative system pens 
given multi-agent system m and normative system η 
over m 
question is η pareto efficient for m 
theorem pens is co-np-complete even for one-agent 
systems 
proof let m and η be as in the theorem we show that the 
complement problem to pens which we refer to as pareto 
dominated is np-complete in this problem we are given m and η 
and we are asked whether η is pareto dominated i e whether or not 
there exists some η over m such that η makes every agent better 
off than η for membership of np simply guess a normative system 
η and verify that for all i ∈ a we have ui k † η ui k † η 
- verifying requires a polynomial number of model checking 
problems each of which takes polynomial time since η ⊆ r the 
normative system can be guessed in non-deterministic polynomial 
time for np-hardness we reduce irns which we know to be 
npcomplete from theorem given an instance m of irns we let m 
in the instance of pareto dominated be as in the irns instance 
and define the normative system for pareto dominated to be η∅ 
the empty normative system now it is straightforward that there 
exists a normative system η which pareto dominates η∅ in m iff 
there exist an individually rational normative system in m since 
the complement problem is np-complete it follows that pens is 
co-np-complete 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
η η η η η η η η η 
u k † η 
u k † η 
table utilities for all possible norms in our example 
how about pareto efficient norms for our toy example settling 
this question amounts to finding the dominant normative systems 
among η η∅ η η η defined before and η { s t } η 
{ t s } η { s s t s } η { t t s t } and η 
{ s t t s } the utilities for each system are given in table 
from this we infer that the pareto efficient norms are η η η η 
and η note that η prohibits the resource to be passed from one 
agent to another and this is not good for any agent since we have 
chosen s 
 {s t} no agent can be sure to ever get the resource 
i e goal ϕi 
 is not true in k † η 
 nash implementation normative systems 
the most famous solution concept in game theory is of course 
nash equilibrium p a collection of strategies one for each 
agent is said to form a nash equilibrium if no agent can benefit by 
doing anything other than playing its strategy under the 
assumption that the other agents play theirs nash equilibria are important 
because they provide stable solutions to the problem of what 
strategy an agent should play note that in our toy example although 
η is individually rational for each agent it is not a nash 
equilibrium since given this norm it would be beneficial for agent to 
deviate and likewise for in our framework we say a social 
system σ m η where η η∅ is a nash implementation if 
sc i e everyone complying with the normative system forms a 
nash equilibrium in the game gς the intuition is that if σ is a 
nash implementation then complying with the normative system 
is a reasonable solution for all concerned there can be no 
benefit to deviating from it indeed there is a positive incentive for all 
to comply if σ is not a nash implementation then the normative 
system is unlikely to succeed since compliance is not rational for 
some agents our choice of terminology is deliberately chosen to 
reflect the way the term nash implementation is used in 
implementation theory or mechanism design p where a game 
designer seeks to achieve some outcomes by designing the rules of 
the game such that these outcomes are equilibria 
nash implementation ni 
given multi-agent system m 
question does there exist a non-empty normative 
system η over m such that m η forms a nash 
implementation 
verifying that a particular social system forms a nash 
implementation can be done in polynomial time - it amounts to checking 
∀i ∈ a ui k † η ≥ ui k † η {i} 
this clearly requires only a polynomial number of model checking 
calls each of which requires only polynomial time 
theorem the ni problem is np-complete even for 
twoagent systems 
proof for membership of np simply guess a normative 
system η and check that it forms a nash implementation since η ⊆ r 
guessing can be done in non-deterministic polynomial time and as 
s k 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
t x 
f x 
t x 
f x 
t xk 
f xk 
 
 
t x 
f x 
t x 
f x 
t xk 
f xk 
 
s 
figure reduction for theorem 
we argued above verifying that it forms a nash implementation 
can be done in polynomial time 
for np-hardness we reduce sat suppose we are given a sat 
instance ϕ over boolean variables x xk then we construct an 
instance of ni as follows we create two agents a { } for 
each boolean variable xi we create two boolean variables t xi 
and f xi and we then define a kripke structure as shown in 
figure with s being the only initial state the arc labelling in 
figure gives the α function and each state is labelled with the 
propositions that are true in that state for each boolean variable xi we 
define the formulae xi and x⊥ 
i as follows 
xi e f t xi ∧ e f e f t xi ∧ a f ¬f xi 
x⊥ 
i e f f xi ∧ e f e f f xi ∧ a f ¬t xi 
let ϕ∗ 
be the formula obtained from ϕ by systematically 
substituting xi for xi each agent has three goals γi for both 
i ∈ { } while 
γ 
k 
i 
 e f t xi ∧ e f f xi 
γ e fe f 
k 
i 
 e f t xi ∧ e f f xi 
and finally for both agents γi being the conjunction of the 
following formulae 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
k 
i 
 xi ∨ x⊥ 
i 
k 
i 
¬ xi ∧ x⊥ 
i 
k 
i 
¬ e f t xi ∧ e f f xi 
ϕ∗ 
 
we denote the multi-agent system so constructed by mϕ now 
we prove that the sat instance ϕ is satisfiable iff mϕ has a nash 
implementation normative system 
for the ⇒ direction suppose ϕ is satisfiable and let x be a 
satisfying valuation i e a set of boolean variables making ϕ true 
we can extract from x a nash implementation normative system η 
as follows if xi ∈ x then η includes the arc from s to the state 
in which f xi is true and also includes the arc from s k 
to the state in which f xi is true if xi ∈ x then η includes the 
arc from s to the state in which t xi is true and also includes 
the arc from s k to the state in which t xi is true no 
other arcs apart from those so defined as included in η notice 
that η is individually rational for both agents if they both comply 
with the normative system then they will have their γi goals 
achieved which they do not in the basic system to see that η 
forms a nash implementation observe that if either agent defects 
from η then neither will have their γi goals achieved agent 
strictly prefers c c over d c and agent strictly prefers 
 c c over c d 
for the ⇐ direction suppose there exists a nash implementation 
normative system η in which case η ∅ then ϕ is satisfiable 
for suppose not then the goals γi are not achievable by any 
normative system by construction now since η must forbid at 
least one transition then at least one agent would fail to have its 
γi goal achieved if it complied so at least one would do better 
by defecting i e not complying with η but this contradicts the 
assumption that η is a nash implementation i e that c c forms 
a nash equilibrium 
this result is perhaps of some technical interest beyond the specific 
concerns of the present paper since it is related to two problems 
that are of wider interest the complexity of mechanism design 
and the complexity of computing nash equilibria 
 richer goal languages 
it is interesting to consider what happens to the complexity of 
the problems we consider above if we allow richer languages for 
goals in particular ctl 
∗ 
 the main difference is that 
determining ui k in a given multi-agent system m when such a goal 
language is used involves solving a pspace-complete problem since 
model checking for ctl 
∗ 
is pspace-complete in fact it seems 
that for each of the three problems we consider above the 
corresponding problem under the assumption of a ctl 
∗ 
representation 
for goals is also pspace-complete it cannot be any easier since 
determining the utility of a particular kripke structure involves 
solving a pspace-complete problem to see membership in pspace 
we can exploit the fact that pspace npspace p and so 
we can guess the desired normative system applying a pspace 
verification procedure to check that it has the desired properties 
 conclusions 
social norms are supposed to restrict our behaviour of course 
such a restriction does not have to be bad the fact that an agent s 
behaviour is restricted may seem a limitation but there may be 
benefits if he can assume that others will also constrain their behaviour 
the question then for an agent is how to be sure that others will 
comply with a norm and for a system designer how to be sure 
that the system will behave socially that is according to its norm 
game theory is a very natural tool to analyse and answer these 
questions which involve strategic considerations and we have 
proposed a way to translate key questions concerning logic-based 
normative systems to game theoretical questions we have proposed 
a logical framework to reason about such scenarios and we have 
given some computational costs for settling some of the main 
questions about them of course our approach is in many senses open 
for extension or enrichment an obvious issue is to consider is the 
complexity of the questions we give for more practical 
representations of models cf and to consider other classes of allowable 
goals 
 references 
 t agotnes w van der hoek j a rodriguez-aguilar 
c sierra and m wooldridge on the logic of normative 
systems in proc ijcai- hyderabad india 
 r alur t a henzinger and o kupferman 
alternating-time temporal logic jnl of the acm 
 - 
 k binmore game theory and the social contract volume 
 playing fair the mit press cambridge ma 
 k binmore game theory and the social contract volume 
 just playing the mit press cambridge ma 
 v conitzer and t sandholm complexity of mechanism 
design in proc uai edmonton canada 
 v conitzer and t sandholm complexity results about nash 
equilibria in proc ijcai- pp - acapulco 
mexico 
 c daskalakis p w goldberg and c h papadimitriou the 
complexity of computing a nash equilibrium in proc 
stoc seattle wa 
 e a emerson temporal and modal logic in handbook of 
theor comp sci vol b pages - elsevier 
 e a emerson and j y halpern  sometimes and  not 
never revisited on branching time versus linear time 
temporal logic jnl of the acm - 
 d fitoussi and m tennenholtz choosing social laws for 
multi-agent systems minimality and simplicity artificial 
intelligence - - 
 m j osborne and a rubinstein a course in game theory 
the mit press cambridge ma 
 c h papadimitriou computational complexity 
addison-wesley reading ma 
 y shoham and m tennenholtz on the synthesis of useful 
social laws for artificial agent societies in proc aaai san 
diego ca 
 y shoham and m tennenholtz on social laws for artificial 
agent societies off-line design in computational theories 
of interaction and agency pages - the mit press 
cambridge ma 
 w van der hoek m roberts and m wooldridge social 
laws in alternating time effectiveness feasibility and 
synthesis synthese 
 m wooldridge and w van der hoek on obligations and 
normative ability jnl of appl logic - 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
implementing commitment-based interactions∗ 
michael winikoff 
school of computer science and it 
rmit university 
melbourne australia 
michael winikoff rmit edu au 
abstract 
although agent interaction plays a vital role in mas and 
messagecentric approaches to agent interaction have their drawbacks present 
agent-oriented programming languages do not provide support for 
implementing agent interaction that is flexible and robust instead 
messages are provided as a primitive building block in this 
paper we consider one approach for modelling agent interactions the 
commitment machines framework this framework supports 
modelling interactions at a higher level using social commitments 
resulting in more flexible interactions we investigate how 
commitmentbased interactions can be implemented in conventional agent-oriented 
programming languages the contributions of this paper are a 
mapping from a commitment machine to a collection of bdi-style 
plans extensions to the semantics of bdi programming languages 
and an examination of two issues that arise when distributing 
commitment machines turn management and race conditions and 
solutions to these problems 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligencemultiagent systems i artificial intelligence programming 
languages and software 
general terms 
design 
 introduction 
agents are social and agent interaction plays a vital role in 
multiagent systems consequently design and implementation of agent 
interaction is an important research topic 
the standard approach for designing agent interactions is 
messagecentric interactions are defined by interaction protocols that give 
the permissible sequences of messages specified using notations 
such as finite state machines petri nets or agent uml 
it has been argued that this message-centric approach to 
interaction design is not a good match for intelligent agents intelligent 
agents should exhibit the ability to persist in achieving their goals 
in the face of failure robustness by trying different approaches 
 flexibility on the other hand when following an interaction 
protocol an agent has limited flexibility and robustness the ability to 
persistently try alternative means to achieving the interaction s aim 
is limited to those options that the protocol s designer provided and 
in practice message-centric design processes do not tend to lead to 
protocols that are flexible or robust 
recognising these limitations of the traditional approach to 
designing agent interactions a number of approaches have been 
proposed in recent years that move away from message-centric 
interaction protocols and instead consider designing agent interactions 
using higher-level concepts such as social commitments 
 or interaction goals there has also been work on richer 
forms of interaction in specific settings such as teams of 
cooperative agents 
however although there has been work on designing flexible and 
robust agent interactions there has been virtually no work on 
providing programming language support for implementing such 
interactions current agent oriented programming languages 
 aopls do not provide support for implementing flexible and robust 
agent interactions using higher-level concepts than messages 
indeed modern aopls with virtually no exceptions provide 
only simple message sending as the basis for implementing agent 
interaction 
this paper presents what to the best of our knowledge is the 
second aopl to support high-level flexible and robust agent 
interaction implementation the first such language staple was 
proposed a few years ago but is not described in detail and is 
arguably impractical for use by non-specialists due to its logical 
basis and heavy reliance on temporal and modal logic 
this paper presents a scheme for extending bdi-like aopls 
to support direct implementation of agent interactions that are 
designed using yolum singh s commitment machine cm 
framework in the remainder of this paper we briefly review 
commitment machines and present a simple abstraction of bdi aopls 
which lies in the common subset of languages such as jason apl 
and can we then present a scheme for translating commitment 
machines to this language and indicate how the language needs 
to be extended to support this we then extend our scheme to 
address a range of issues concerned with distribution including turn 
tracking and race conditions 
 background 
 commitment machines 
the aim of the commitment machine framework is to allow for 
the definition of interactions that are more flexible than traditional 
message-centric approaches a commitment machine cm 
specifies an interaction between entities e g agents services 
processes in terms of actions that change the interaction state this 
interact state consists of fluents predicates that change value over 
time but also social commitments both base-level and conditional 
a base-level social commitment is an undertaking by debtor a to 
creditor b to bring about condition p denoted c a b p this is 
sometimes abbreviated to c p where it is not important to specify 
the identities of the entities in question for example a 
commitment by customer c to merchant m to make the fluent paid true 
would be written as c c m paid 
a conditional social commitment is an undertaking by debtor a 
to creditor b that should condition q become true a will then 
commit to bringing about condition p this is denoted by cc a b q p 
and where the identity of the entities involved is unimportant or 
obvious is abbreviated to cc q p where the arrow is a 
reminder of the causal link between q becoming true and the creation 
of a commitment to make p true for example a commitment to 
make the fluent paid true once goods have been received would be 
written cc goods paid 
the semantics of commitments both base-level and conditional 
is defined with rules that specify how commitments change over 
time for example the commitment c p or cc q p is 
discharged when p becomes true and the commitment cc q p is 
replaced by c p when q becomes true in this paper we use the 
more symmetric semantics proposed by and subsequently 
reformalised by in brief these semantics deal with a number of 
more complex cases such as where commitments are created when 
conditions already hold if p holds when cc p q is meant to 
be created then c q is created instead of cc p q 
an interaction is defined by specifying the entities involved the 
possible contents of the interaction state both fluents and 
commitments and most importantly the actions that each entity can 
perform along with the preconditions and effects of each action 
specified as add and delete lists 
a commitment machine cm defines a range of possible 
interactions that each start in some state 
 and perform actions until 
reaching a final state a final state is one that has no base-level 
commitments one way of visualising the interactions that are 
possible with a given commitment machine is to generate the finite 
state machine corresponding to the cm for example figure gives 
the fsm 
corresponding to the netbill commitment machine 
a simple cm where a customer c and merchant m attempt to 
trade using the following actions 
 
 
unlike standard interaction protocols or finite state machines 
there is no designated initial state for the interaction 
 
the finite state machine is software-generated the nodes and 
connections were computed by an implementation of the axioms 
 available from http www winikoff net cm and were then laid out by 
graphviz http www graphviz org 
 
we use the notation a x p ⇒ e to indicate that action a is 
performed by entity x has precondition p with p omitted if 
empty and effect e 
 sendrequest c ⇒ request 
 sendquote m ⇒ offer 
where offer ≡ promisegoods ∧ promisereceipt and 
promisegoods ≡ cc m c accept goods and 
promisereceipt ≡ cc m c pay receipt 
 sendaccept c ⇒ accept 
where accept ≡ cc c m goods pay 
 sendgoods m ⇒ promisereceipt ∧ goods 
where promisereceipt ≡ cc m c pay receipt 
 sendepo c goods ⇒ pay 
 sendreceipt m pay ⇒ receipt 
the commitment accept is the customer s promise to pay once 
goods have been sent promisegoods is the merchant s promise 
to send the goods once the customer accepts and promisereceipt 
is the merchant s promise to send a receipt once payment has been 
made 
as seen in figure commitment machines can support a range 
of interaction sequences 
 an abstract agent programminglanguage 
agent programming languages in the bdi tradition e g dmars 
jam prs um-prs jack agentspeak l jason apl can 
jadex define agent behaviour in terms of event-triggered plans 
where each plan specifies what it is triggered by under what 
situations it can be considered to be applicable defined using a so-called 
context condition and a plan body a sequence of steps that can 
include posting events which in turn triggers further plans given 
a collection of plans and an event e that has been posted the agent 
first collects all plans types that are triggered by that event the 
relevant plans then evaluates the context conditions of these plans to 
obtain a set of applicable plan instances one of these is chosen 
and is executed 
we now briefly define the formal syntax and semantics of a 
simple abstract bdi agent programming language saapl this 
language is intended to be an abstraction that is in the common 
subset of such languages as jason chapter apl 
chapter and can thus it is intentionally incomplete in some 
areas for instance it doesn t commit to a particular mechanism for 
dealing with plan failure since different mechanisms are used by 
different aopls 
an agent program denoted by π consists of a collection of plan 
clauses of the form e c ← p where e is an event c is a context 
condition a logical formula over the agent s beliefs and p is the 
plan body the plan body is built up from the following constructs 
we have the empty step which always succeeds and does nothing 
operations to add b and delete −b beliefs sending a message 
m to agent n ↑n 
m and posting an event 
 e these can be 
sequenced p p 
c b c ∧ c c ∨ c ¬c ∃x c 
p b −b e ↑n 
m p p 
formal semantics for this language is given in figure this 
semantics is based on the semantics for agentspeak given by 
which in turn is based on the semantics for can the 
semantics is in the style of plotkin s structural operational semantics 
and assumes that operations exist that check whether a condition 
 
we use ↓n 
m as short hand for the event corresponding to 
receiving message m from agent n 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure finite state machine for netbill shaded final states 
follows from a belief set that add a belief to a belief set and that 
delete a belief from a belief set in the case of beliefs being a set of 
ground atoms these operations are respectively consequence 
checking b c and set addition b ∪ {b} and deletion b \ {b} 
more sophisticated belief management methods may be used but 
are not considered here 
we define a basic configuration s q n b p where q is a 
 global message queue modelled as a sequence 
where messages 
are added at one end and removed from the other end n is the 
name of the agent b is the beliefs of the agent and p is the plan 
body being executed i e the intention we also define an agent 
configuration where instead of a single plan body p there is a set 
of plan instances γ finally a complete mas is a pair q as of a 
global message queue q and a set of agent configurations without 
the queue q the global message queue is a sequence of triplets 
of the form sender recipient message 
a transition s −→ s specifies that executing s a single step 
yields s we annotate the arrow with an indication of whether 
the configuration in question is basic an agent configuration or a 
mas configuration the transition relation is defined using rules 
of the form s −→ s or of the form 
s −→ sr 
s −→ sr the latter are 
conditional with the top numerator being the premise and the bottom 
 denominator being the conclusion 
note that there is non-determinism in saapl e g the choice 
of plan to execute from a set of applicable plans this is resolved 
by using selection functions so selects one of the applicable plan 
instances to handle a given event si selects which of the plan 
instances that can be executed should be executed next and sa 
selects which agent should execute a step next 
 implementing commitment-based 
interactions 
in this section we present a mapping from a commitment 
machine to a collection of saapl programs one for each role we 
begin by considering the simple case of two interacting agents and 
 
the operator is used to denote sequence concatenation 
assume that the agents take turns to act in section we relax these 
assumptions 
each action a x p ⇒ e is mapped to a number of plans 
there is a plan for agent x with context condition p that 
performs the action i e applies the effects e to the agent s beliefs 
and sends a message to the other agent and a plan for the other 
agent that updates its state when a message is received from x 
for example given the action sendaccept c ⇒ accept we have 
the following plans where each plan is preceded by m or c 
to indicate which agent that plan belongs to note that where the 
identify of the sender respectively recipient is obvious i e the 
other agent we abbreviate ↑n 
m to ↑m resp ↓n 
m to ↓m turn 
taking is captured through the event ı short for interact the 
agent that is active has an ı event that is being handled handling 
the event involves sending a message to the other agent and then 
doing nothing until a response is received 
c ı true ← accept ↑sendaccept 
m ↓sendaccept true ← accept ı 
if the action has a non-trivial precondition then there are two plans 
in the recipient one to perform the action if possible and another 
to report an error if the action s precondition doesn t hold we 
return to this in section for example the action sendreceipt m 
pay ⇒ receipt generates the following plans 
m ı pay ← receipt ↑sendreceipt 
c ↓sendreceipt pay ← receipt ı 
c ↓sendreceipt ¬pay ← report error 
in addition to these plans we also need plans to start and finish 
the interaction an interaction can be completed whenever there 
are no base-level commitments so both agents have the following 
plans 
ı ¬∃p c p ← ↑done 
↓done ¬∃p c p ← 
↓done ∃p c p ← report error 
an interaction is started by setting up an agent s initial beliefs and 
then having it begin to interact exactly how to do this depends 
on the agent platform e g the agent platform in question may 
offer a simple way to load beliefs from a file a generic approach 
that is a little cumbersome but is portable is to send each of the 
agents involved in the interaction a sequence of init messages each 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
q n b b 
basic 
−→ q n b ∪ {b} 
q n b −b 
basic 
−→ q n b \ {b} 
δ {piθ ti ci ← pi ∈ π ∧ tiθ e ∧ b ciθ} 
q n b e 
basic 
−→ q n b so δ 
q n b p 
basic 
−→ q n b p 
q n b p p 
basic 
−→ q n b p p 
q n b p 
basic 
−→ q n b p 
q n b ↑nb m 
basic 
−→ q n nb m n b 
q na n m q 
q n b γ 
agent 
−→ q n b γ ∪ {↓na m} 
p si γ q n b p 
basic 
−→ q n b p 
q n b γ 
agent 
−→ q n b γ \ {p} ∪ {p } 
p si γ p 
q n b γ 
agent 
−→ q n b γ \ {p} 
n b γ sa as q n b γ 
agent 
−→ q n b γ 
q as 
mas 
−→ q as ∪ { n b γ } \ { n b γ } 
figure operational semantics for saapl 
containing a belief to be added and then send one of the agents a 
start message which begins the interaction both agents thus have 
the following two plans 
↓init b true ← b 
↓start true ← ı 
figure gives the saapl programs for both merchant and 
customer that implement the netbill protocol for conciseness the 
error reporting plans are omitted 
we now turn to refining the context conditions there are three 
refinements that we consider firstly we need to prevent 
performing actions that have no effect on the interaction state secondly 
an agent may want to specify that certain actions that it is able to 
perform should not be performed unless additional conditions hold 
for example the customer may not want to agree to the merchant s 
offer unless the goods have a certain price or property thirdly the 
context conditions of the plans that terminate the interaction need to 
be refined in order to avoid terminating the interaction prematurely 
for each plan of the form ı p ← e ↑m we replace the 
context condition p with the enhanced condition p ∧ p ∧ ¬e where 
p is any additional conditions that the agent wishes to impose 
and ¬e is the negation of the effects of the action for 
example the customer s payment plan becomes assuming no additional 
conditions i e no p ı goods ∧ ¬pay ← pay ↑sendepo 
for each plan of the form ↓m p ← e ı we could add ¬e to 
the precondition but this is redundant since it is already checked 
by the performer of the action and if the action has no effect then 
customer s plans 
ı true ← request ↑sendrequest 
ı true ← accept ↑sendaccept 
ı goods ← pay ↑sendepo 
↓sendquote true ← promisegoods 
 promisereceipt ı 
↓sendgoods true ← promisereceipt goods ı 
↓sendreceipt pay ← receipt ı 
merchant s plans 
ı true ← promisegoods 
 promisereceipt ↑sendquote 
ı true ← promisereceipt goods ↑sendgoods 
ı pay ← receipt ↑sendreceipt 
↓sendrequest true ← request ı 
↓sendaccept true ← accept ı 
↓sendepo goods ← pay ı 
shared plans i e plans of both agents 
ı ¬∃p c p ← ↑done 
↓done ¬∃p c p ← 
↓init b true ← b 
↓start true ← ı 
where 
accept ≡ cc goods pay 
promisegoods ≡ cc accept goods 
promisereceipt ≡ cc pay receipt 
offer ≡ promisegoods ∧ promisereceipt 
figure saapl implementation of netbill 
the sender won t perform it and send the message see also the 
discussion in section 
when specifying additional conditions p some care needs to 
be taken to avoid situations where progress cannot be made because 
the only action s possible are prevented by additional conditions 
one way of indicating preference between actions in many agent 
platforms is to reorder the agent s plans this is clearly safe since 
actions are not prevented just considered in a different order 
the third refinement of context conditions concerns the plans 
that terminate the interaction in the commitment machine 
framework any state that has no base-level commitment is final in that 
the interaction may end there or it may continue however only 
some of these final states are desirable final states which final 
states are considered to be desirable depends on the domain and 
the desired interaction outcome in the netbill example the 
desirable final state is one where the goods have been sent and paid 
for and a receipt issued i e goods ∧ pay ∧ receipt in order to 
prevent an agent from terminating the interaction too early we add 
this as a precondition to the termination plan 
ı goods ∧ pay ∧ receipt ∧ ¬∃p c p ← ↑done 
figure shows the plans that are changed from figure 
in order to support the realisation of cms we need to change 
saapl in a number of ways these changes which are discussed 
below can be applied to existing bdi languages to make them 
commitment machine supportive we present the three changes 
explain what they involve and for each change explain how the 
change was implemented using the apl agent oriented 
programming language the three changes are 
 extending the beliefs of the agent so that they can contain 
commitments 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
customer s plans 
ı ¬request ← request ↑sendrequest 
ı ¬accept ← accept ↑sendaccept 
ı goods ∧ ¬pay ← pay ↑sendepo 
merchant s plans 
ı ¬offer ← promisegoods promisereceipt 
↑sendquote 
ı ¬ promisereceipt ∧ goods ← 
 promisereceipt goods ↑sendgoods 
ı pay ∧ ¬receipt ← receipt ↑sendreceipt 
where 
accept ≡ cc goods pay 
promisegoods ≡ cc accept goods 
promisereceipt ≡ cc pay receipt 
offer ≡ promisegoods ∧ promisereceipt 
figure saapl implementation of netbill with refined 
context conditions changed plans only 
 changing the definition of to encompass implied 
commitments and 
 whenever a belief is added updating existing commitments 
according to the rules of commitment dynamics 
extending the notion of beliefs to encompass commitments in 
fact requires no change in agent platforms that are prolog-like and 
support terms as beliefs e g jason apl can however other 
agent platforms do require an extension for example jack which 
is an extension of java would require changes to support 
commitments that can be nested in the case of apl no change is needed 
to support this 
whenever a context condition contains commitments 
determining whether the context condition is implied by the agent s beliefs 
 b c needs to take into account the notion of implied 
commitments in brief a commitment can be considered to follow 
from a belief set b if the commitment is in the belief set c ∈ b 
but also under other conditions for example a commitment to pay 
c pay can be considered to be implied by a belief set containing 
pay because the commitment may have held and been discharged 
when pay was made true similar rules apply for conditional 
commitments these rules which were introduced in were 
subsequently re-formalised in a simpler form by resulting in the 
four inference rules in the bottom part of figure 
the change that needs to be made to saapl to support 
commitment machine implementations is to extend the definition of to 
include these four rules for apl this was realised by having each 
agent include the following prolog clauses 
holds x - clause x true 
holds c p - holds p 
holds c p - clause cc q p true holds q 
holds cc q - holds q 
holds cc q - holds c q 
the first clause simply says that anything holds if it is in agent s 
beliefs clause x true is true if x is a fact the 
remaining four clauses correspond respectively to the inference rules c 
c cc and cc to use these rules we then modify context 
conditions in our program so that instead of writing for 
example cc m c pay receipt we write holds cc m c 
pay receipt 
b norm b ∪ {b} 
q n b b −→ q n b 
function norm b 
b ← b 
for each b ∈ b do 
if b c p ∧ b p then b ← b \ {b} 
elseif b cc p q then 
if b q then b ← b \ {b} 
elseif b p then b ← b \ {b} ∪ {c q } 
elseif b c q then b ← b \ {b} 
endif 
endif 
endfor 
return b 
end function 
b p 
b c p 
c 
cc q p ∈ b b q 
b p 
c 
b cc p q 
b q 
cc 
b c q 
b cc p q 
cc 
figure new operational semantics 
the final change is to update commitments when a belief is 
added formally this is done by modifying the semantic rule for 
belief addition so that it applies an algorithm to update 
commitments the modified rule and algorithm which mirrors the 
definition of norm in can be found in the top part of figure 
for apl this final change was achieved by manually inserting 
update after updating beliefs and defining the following rules 
for update 
update - c p and holds p 
 {deletec p update } 
update - cc p q and holds q 
 {deletecc p q update } 
update - cc p q and holds p 
 {deletecc p q addc q update } 
update - cc p q and holds c q 
 {deletecc p q update } 
update - true skip 
where deletec and deletecc delete respectively a base-level 
and conditional commitment and addc adds a base-level 
commitment 
one aspect that doesn t require a change is linking commitments 
and actions this is because commitments don t trigger actions 
directly they may trigger actions indirectly but in general their effect 
is to prevent completion of an interaction while there are 
outstanding base level commitments 
figure shows the message sequences from a number of runs of 
a apl implementation of the netbill commitment machine 
 in 
order to illustrate the different possible interactions the code was 
modified so that each agent selected randomly from the actions 
that it could perform and a number of runs were made with the 
customer as the initiator and then with the merchant as the 
initiator there are other possible sequences of messages not shown 
 
source code is available from http www winikoff net cm 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure sample runs from apl implementation alternating turns 
including the obvious one request quote accept goods payment 
receipt and then done 
one minor difference between the apl implementation and 
saapl concerns the semantics of messages in the semantics of 
saapl and of most aopls receiving a message is treated as an 
event however in apl receiving a message is modelled as the 
addition to the agent s beliefs of a fact indicating that the message 
was received thus in the apl implementation we have pg 
rules that are triggered by these beliefs rather than by any event 
one issue with this approach is that the belief remains there so we 
need to ensure that the belief in question is either deleted once 
handled or that we modify preconditions of plans to avoid handling it 
more than once in our implementation we delete these received 
beliefs when they are handled to avoid duplicate handling of 
messages 
 beyond two participants 
generalising to more than two interaction participants requires 
revisiting how turn management is done since it is no longer 
possible to assume alternating turns 
in fact perhaps surprisingly even in the two participant setting 
an alternating turn setup is an unreasonable assumption for 
example consider the path in figure from state to sendgoods 
then to state sendaccept the result in an alternating turn 
setup is a dead-end there is only a single possible action in state 
 namely sendepo but this action is done by the customer and 
it is the merchant s turn to act figure shows the fsm for netbill 
with alternating initiative 
a solution to this problem that works in this example but doesn t 
generalise 
 is to weaken the alternating turn taking regime by 
allowing an agent to act twice in a row if its second action is driven 
by a commitment 
a general solution is to track whose turn it is to act this can be 
done by working out which agents have actions that are able to be 
performed in the current state if there is only a single active agent 
then it is clearly that agent s turn to act however if more than 
one agent is active then somehow the agents need to work out who 
should act next working this out by negotiation is not a particularly 
good solution for two reasons firstly this negotiation has to be 
done at every step of the interaction where more than one agent is 
active in the netbill this applies to seven out of sixteen states so 
it is highly desirable to have a light-weight mechanism for doing 
this secondly it is not clear how the negotiation can avoid an 
infinite regress situation you go first no you go first 
without imposing some arbitrary rule it is also possible to resolve 
who should act by imposing an arbitrary rule for example that the 
customer always acts in preference to the merchant or that each 
agent has a numerical priority perhaps determined by the order in 
which they joined the interaction that determines who acts 
an alternative solution which exploits the symmetrical 
properties of commitment machines is to not try and manage turn taking 
 
consider actions a c ⇒ p a c ⇒ q and a m p ∧ 
q ⇒ r 
figure netbill with alternating initiative 
instead of tracking and controlling whose turn it is we simply allow 
the agents to act freely and rely on the properties of the interaction 
space to ensure that things work out a notion that we shall make 
precise and prove in the remainder of this section 
the issue with having multiple agents be active simultaneously 
is that instead of all agents agreeing on the current interaction state 
agents can be in different states this can be visualised as each 
agent having its own copy of the fsm that it navigates through 
where it is possible for agents to follow different paths through the 
fsm the two specific issues that need to be addressed are 
 can agents end up in different final states 
 can an agent be in a position where an error occurs because 
it cannot perform an action corresponding to a received 
message 
we will show that because actions commute under certain 
assumptions agents cannot end up in different final states and 
furthermore that errors cannot occur again under certain 
assumptions 
by actions commute we mean that the state resulting from 
performing a sequence of actions a an is the same regardless of 
the order in which the actions are performed this means that even 
if agents take different paths through the fsm they still end up in 
the same resulting state because once all messages have been 
processed all agents will have performed the same set of actions this 
addresses the issue of ending up in different final states we return 
to the possibility of errors occurring shortly 
definition monotonicity an action is monotonic if it does not 
delete 
any fluents or commitments a commitment machine is 
 
that is directly deletes it is fine to discharge commitments by 
adding fluents commitments 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
monotonic if all of its actions are monotonic adapted from 
definition 
theorem if a and a are monotonic actions then performing 
a followed by a has the same effect on the agent s beliefs as 
performing a followed by a adapted from theorem 
this assumes that both actions can be performed however it is 
possible for the performance of a to disable a from being done 
for example if a has the effect p and a has precondition 
¬p then although both actions may be enabled in the initial state 
they cannot be performed in either order we can prevent this by 
ensuring that actions preconditions do not contain negation or 
implication since a monotonic action cannot result in a precondition 
that is negation-free becoming false note that this restriction only 
applies to the original action precondition p not to any additional 
preconditions imposed by the agent p this is because only p 
is used to determine whether another agent is able to perform the 
action 
thus monotonic cms with preconditions that do not contain 
negations have actions that commute however in fact the 
restriction to monotonic cms is unnecessarily strong all that is needed 
is that whenever there is a choice of agent that can act then the 
possible actions are monotonic if there is only a single agent that 
can act then no restriction is needed on the actions they may or 
may not be monotonic 
definition locally monotonic a commitment machine is 
locally monotonic if for any state s either a only a single agent 
has actions that can be performed or b all actions that can be 
performed in s are monotonic 
theorem in a locally monotonic cm once all messages have 
been processed all agents will be in the same state furthermore 
no errors can occur 
proof once all messages have been processed we have that all 
agents will have performed the same action set perhaps in a 
different order the essence of the proof is to argue that as long as 
agents haven t yet converged to the same state all actions must 
be monotonic and hence that these actions commute and cannot 
disable any other actions 
consider the first point of divergence where an agent performs 
action a and at the same time another agent call it xb performs 
action b clearly this state has actions of more than one agent 
enabled so since the cm is locally monotonic the relevant actions 
must be monotonic therefore after doing a the action b must 
still be enabled and so the message to do b can be processed by 
updating the recipient agent s beliefs with the effects of b 
furthermore because monotonic actions commute the result of doing 
a before b is the same as doing b before a 
s 
a 
−−−−−→ sa 
 
 
yb b 
 
 
y 
sb −−−−−→ 
a 
sab 
however what happens if the next action after a is not b but 
c because b is enabled and c is not done by agent xb see 
below we must have that c is also monotonic and hence a the 
result of doing a and b and c is the same regardless of the order 
in which the three actions are done and b c doesn t disable b 
so b can still be done after c 
s 
a 
−−−−−→ sa 
c 
−−−−−→ sac 
 
 
yb b 
 
 
y b 
 
 
y 
sb −−−−−→ 
a 
sab −−−−−→ 
c 
sabc 
the reason why c cannot be done by xb is that messages are 
processed in the order of their arrival 
 from the perspective of 
xb the action b was done before c and therefore from any other 
agent s perspective the message saying that b was done must be 
received and processed before a message saying that c is done 
this argument can be extended to show that once agents start 
taking different paths through the fsm all actions taken until the 
point where they converge on a single state must be monotonic 
and hence it is always possible to converge because actions aren t 
disabled so the interaction is error free and the resulting state 
once convergence occurs is the same because monotonic actions 
commute 
this theorem gives a strong theoretical guarantee that not 
doing turn management will not lead to disaster this is analogous 
to proving that disabling all traffic lights would not lead to any 
accidents and is only possible because the refined cm axioms are 
symmetrical 
based on this theorem the generic transformation from cm to 
code should allow agents to act freely which is achieved by simply 
changing ı p ∧ p ∧ ¬e ← e ↑a to 
ı p ∧ p ∧ ¬e ← e ↑a ı 
for example instead of ı ¬request ← request ↑sendrequest 
we have ı ¬request ← request ↑sendrequest ı 
one consequence of the theorem is that it is not necessary to 
ensure that agents process messages before continuing to 
interact however in order to avoid unnecessary parallelism which can 
make debugging harder it may still be desirable to process 
messages before performing actions 
figure shows a number of runs from the apl implementation 
that has been modified to allow free non-alternating interaction 
 discussion 
we have presented a scheme for mapping commitment machines 
to bdi platforms using saapl as an exemplar identified three 
changes that needed to be made to saapl to support cm-based 
interaction and shown that turn management can be avoided in 
cmbased interaction provided the cm is locally monotonic the three 
changes to saapl and the translation scheme from commitment 
machine to bdi plans are both applicable to any bdi language 
as we have mentioned in section there has been some work 
on designing flexible and robust agent interaction but virtually no 
work on implementing flexible and robust interactions 
we have already discussed staple another piece of 
work that is relevant is the work by cheong and winikoff on their 
hermes methodology although the main focus of their work is 
a pragmatic design methodology they also provide guidelines for 
implementing hermes designs using bdi platforms specifically 
jadex however since hermes does not yield a design that is 
formal it is only possible to generate skeleton code that then needs 
to be completed also they do not address the turn taking issue 
how to decide which agent acts when more than one agent is able 
to act 
 
we also assume that the communication medium does not deliver 
messages out of order which is the case for e g tcp 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure sample runs from apl implementation non-alternating turns 
the work of kremer and flores e g also uses 
commitments and deals with implementation however they provide 
infrastructure support casa rather than a programming language 
and do not appear to provide assistance to a programmer seeking to 
implement agents 
although we have implemented the netbill interaction using 
 apl the changes to the semantics were done by modifying our 
netbill apl program rather than by modifying the apl 
implementation itself clearly it would be desirable to modify the 
semantics of apl or of another language directly by changing 
the implementation also although we have not done so it should 
be clear that the translation from a cm to its implementation could 
easily be automated 
another area for further work is to look at how the assumptions 
required to ensure that actions commute can be relaxed 
finally there is a need to perform empirical evaluation there 
has already been some work on comparing hermes with a 
conventional message-centric approach to designing interaction and 
this has shown that using hermes results in designs that are 
significantly more flexible and robust it would be interesting to 
compare commitment machines with hermes but since 
commitment machines are a framework not a design methodology we 
need to compare hermes with a methodology for designing 
interactions that results in commitment machines 
 references 
 r h bordini m dastani j dix and a e f seghrouchni 
editors multi-agent programming languages platforms 
and applications springer 
 c cheong and m winikoff hermes designing 
goal-oriented agent interactions in proceedings of the th 
international workshop on agent-oriented software 
engineering aose- july 
 c cheong and m winikoff hermes implementing 
goal-oriented agent interactions in proceedings of the third 
international workshop on programming multi-agent 
systems promas july 
 c cheong and m winikoff hermes versus prometheus a 
comparative evaluation of two agent interaction design 
approaches submitted for publication 
 p r cohen and h j levesque teamwork nous 
 - 
 m dastani j van der ham and f dignum communication 
for goal directed agents in proceedings of the agent 
communication languages and conversation policies 
workshop 
 f p dignum and g a vreeswijk towards a testbed for 
multi-party dialogues in advances in agent communication 
pages - springer lncs 
 r kremer and r flores using a performative subsumption 
lattice to support commitment-based conversations in 
f dignum v dignum s koenig s kraus m p singh and 
m wooldridge editors autonomous agents and multi-agent 
systems aamas pages - acm press 
 s kumar and p r cohen staple an agent programming 
language based on the joint intention theory in proceedings 
of the third international joint conference on autonomous 
agents multi-agent systems aamas pages 
 - acm press july 
 s kumar m j huber and p r cohen representing and 
executing protocols as joint actions in proceedings of the 
first international joint conference on autonomous agents 
and multi-agent systems pages - bologna italy 
 - july acm press 
 m tambe and w zhang towards flexible teamwork in 
persistent teams extended report journal of autonomous 
agents and multi-agent systems special issue on 
best of icmas 
 m winikoff an agentspeak meta-interpreter and its 
applications in third international workshop on 
programming multi-agent systems promas pages 
 - springer lncs post-proceedings 
 
 m winikoff designing commitment-based agent 
interactions in proceedings of the ieee wic acm 
international conference on intelligent agent technology 
 iat- 
 m winikoff implementing flexible and robust agent 
interactions using distributed commitment machines 
multiagent and grid systems 
 m winikoff w liu and j harland enhancing 
commitment machines in j leite a omicini p torroni 
and p yolum editors declarative agent languages and 
technologies ii number in lecture notes in artificial 
intelligence lnai pages - springer 
 m winikoff l padgham j harland and j thangarajah 
declarative procedural goals in intelligent agent systems 
in proceedings of the eighth international conference on 
principles of knowledge representation and reasoning 
 kr toulouse france 
 p yolum towards design tools for protocol development in 
f dignum v dignum s koenig s kraus m p singh and 
m wooldridge editors autonomous agents and multi-agent 
systems aamas pages - acm press 
 p yolum and m p singh flexible protocol specification and 
execution applying event calculus planning using 
commitments in proceedings of the st joint conference on 
autonomous agents and multiagent systems aamas 
pages - 
 p yolum and m p singh reasoning about commitments in 
the event calculus an approach for specifying and executing 
protocols annals of mathematics and artificial intelligence 
 amai 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
learning and joint deliberation through argumentation in 
multi-agent systems 
santi ontañón 
ccl cognitive computing lab 
georgia institute of technology 
atlanta ga 
santi cc gatech edu 
enric plaza 
iiia artificial intelligence research institute 
csic spanish council for scientific research 
campus uab bellaterra catalonia 
 spain 
enric iiia csic es 
abstract 
in this paper we will present an argumentation framework for 
learning agents amal designed for two purposes for joint 
deliberation and for learning from communication the amal 
framework is completely based on learning from examples the argument 
preference relation the argument generation policy and the 
counterargument generation policy are case-based techniques for join 
deliberation learning agents share their experience by forming a 
committee to decide upon some joint decision we experimentally 
show that the argumentation among committees of agents improves 
both the individual and joint performance for learning from 
communication an agent engages into arguing with other agents in 
order to contrast its individual hypotheses and receive 
counterexamples the argumentation process improves their learning scope and 
individual performance 
categories and subject descriptors 
i artificial intelligence learning i artificial 
intelligence distributed artificial intelligence-multiagent systems 
intelligent agents 
 introduction 
argumentation frameworks for multi-agent systems can be used 
for different purposes like joint deliberation persuasion 
negotiation and conflict resolution in this paper we will present an 
argumentation framework for learning agents and show that it can be 
used for two purposes joint deliberation and learning from 
communication 
argumentation-based joint deliberation involves discussion over 
the outcome of a particular situation or the appropriate course of 
action for a particular situation learning agents are capable of 
learning from experience in the sense that past examples situations and 
their outcomes are used to predict the outcome for the situation 
at hand however since individual agents experience may be 
limited individual knowledge and prediction accuracy is also limited 
thus learning agents that are capable of arguing their individual 
predictions with other agents may reach better prediction accuracy 
after such an argumentation process 
most existing argumentation frameworks for multi-agent 
systems are based on deductive logic or some other deductive logic 
formalism specifically designed to support argumentation such as 
default logic usually an argument is seen as a logical 
statement while a counterargument is an argument offered in opposition 
to another argument agents use a preference relation to 
resolve conflicting arguments however logic-based argumentation 
frameworks assume agents with preloaded knowledge and 
preference relation in this paper we focus on an argumentation-based 
multi-agent learning amal framework where both knowledge 
and preference relation are learned from experience thus we 
consider a scenario with agents that work in the same domain using 
a shared ontology are capable of learning from examples and 
 communicate using an argumentative framework 
having learning capabilities allows agents effectively use a 
specific form of counterargument namely the use of 
counterexamples counterexamples offer the possibility of agents learning 
during the argumentation process moreover learning agents allow 
techniques that use learnt experience to generate adequate 
arguments and counterarguments specifically we will need to address 
two issues how to define a technique to generate arguments 
and counterarguments from examples and how to define a 
preference relation over two conflicting arguments that have been 
induced from examples 
this paper presents a case-based approach to address both 
issues the agents use case-based reasoning cbr to learn from 
past cases where a case is a situation and its outcome in order 
to predict the outcome of a new situation we propose an 
argumentation protocol inside the amal framework at supports agents 
in reaching a joint prediction over a specific situation or problem 
- moreover the reasoning needed to support the argumentation 
process will also be based on cases in particular we present two 
case-based measures one for generating the arguments and 
counterarguments adequate to a particular situation and another for 
determining preference relation among arguments finally we 
evaluate if argumentation between learning agents can produce a 
joint prediction that improves over individual learning performance 
and if learning from the counterexamples conveyed during the 
argumentation process increases the individual performance with 
precisely those cases being used while arguing among them 
the paper is structured as follows section discusses the 
relation among argumentation collaboration and learning then 
section introduces our multi-agent cbr mac framework and the 
notion of justified prediction after that section formally 
defines our argumentation framework sections and present our 
case-based preference relation and argument generation policies 
respectively later section presents the argumentation protocol in 
our amal framework after that section presents an 
exemplification of the argumentation framework finally section presents 
an empirical evaluation of our two main hypotheses the paper 
closes with related work and conclusions sections 
 argumentation collaboration 
and learning 
both learning and collaboration are ways in which an agent can 
improve individual performance in fact there is a clear parallelism 
between learning and collaboration in multi-agent systems since 
both are ways in which agents can deal with their shortcomings 
let us show which are the main motivations that an agent can have 
to learn or to collaborate 
 motivations to learn 
- increase quality of prediction 
- increase efficiency 
- increase the range of solvable problems 
 motivations to collaborate 
- increase quality of prediction 
- increase efficiency 
- increase the range of solvable problems 
- increase the range of accessible resources 
looking at the above lists of motivation we can easily see that 
learning and collaboration are very related in multi-agent systems 
in fact with the exception of the last item in the motivations to 
collaborate list they are two extremes of a continuum of strategies 
to improve performance an agent may choose to increase 
performance by learning by collaborating or by finding an intermediate 
point that combines learning and collaboration in order to improve 
performance 
in this paper we will propose amal an argumentation 
framework for learning agents and will also also show how amal can be 
used both for learning from communication and for solving 
problems in a collaborative way 
 agents can solve problems in a collaborative way via 
engaging an argumentation process about the prediction for the 
situation at hand using this collaboration the prediction 
can be done in a more informed way since the information 
known by several agents has been taken into account 
 agents can also learn from communication with other agents 
by engaging an argumentation process agents that engage 
in such argumentation processes can learn from the 
arguments and counterexamples received from other agents and 
use this information for predicting the outcomes of future 
situations 
in the rest of this paper we will propose an argumentation 
framework and show how it can be used both for learning and for solving 
problems in a collaborative way 
 multi-agent cbr systems 
a multi-agent case based reasoning system mac m 
{ a c an cn } is a multi-agent system composed of a 
{ai an} a set of cbr agents where each agent ai ∈ a 
possesses an individual case base ci each individual agent ai 
in a mac is completely autonomous and each agent ai has 
access only to its individual and private case base ci a case base 
ci {c cm} is a collection of cases agents in a mac 
system are able to individually solve problems but they can also 
collaborate with other agents to solve problems 
in this framework we will restrict ourselves to analytical tasks 
i e tasks like classification where the solution of a problem is 
achieved by selecting a solution class from an enumerated set of 
solution classes in the following we will note the set of all the 
solution classes by s {s sk } therefore a case c p s is 
a tuple containing a case description p and a solution class s ∈ s 
in the following we will use the terms problem and case 
description indistinctly moreover we will use the dot notation to refer to 
elements inside a tuple e g to refer to the solution class of a case 
c we will write c s 
therefore we say a group of agents perform joint deliberation 
when they collaborate to find a joint solution by means of an 
argumentation process however in order to do so an agent has to 
be able to justify its prediction to the other agents i e generate an 
argument for its predicted solution that can be examined and 
critiqued by the other agents the next section addresses this issue 
 justified predictions 
both expert systems and cbr systems may have an explanation 
component in charge of justifying why the system has 
provided a specific answer to the user the line of reasoning of the 
system can then be examined by a human expert thus increasing 
the reliability of the system 
most of the existing work on explanation generation focuses on 
generating explanations to be provided to the user however in our 
approach we use explanations or justifications as a tool for 
improving communication and coordination among agents we are 
interested in justifications since they can be used as arguments 
for that purpose we will benefit from the ability of some machine 
learning methods to provide justifications 
a justification built by a cbr method after determining that the 
solution of a particular problem p was sk is a description that 
contains the relevant information from the problem p that the cbr 
method has considered to predict sk as the solution of p in 
particular cbr methods work by retrieving similar cases to the problem 
at hand and then reusing their solutions for the current problem 
expecting that since the problem and the cases are similar the 
solutions will also be similar thus if a cbr method has retrieved a set 
of cases c cn to solve a particular problem p the justification 
built will contain the relevant information from the problem p that 
made the cbr system retrieve that particular set of cases i e it 
will contain the relevant information that p and c cn have in 
common 
for example figure shows a justification build by a cbr 
system for a toy problem in the following sections we will show 
justifications for real problems in the figure a problem has two 
attributes traffic light and cars passing the retrieval mechanism 
of the cbr system notices that by considering only the attribute 
traffic light it can retrieve two cases that predict the same 
solution wait thus since only this attribute has been used it is the 
only one appearing in the justification the values of the rest of 
attributes are irrelevant since whatever their value the solution class 
would have been the same 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
problem 
traffic light red 
cars passing no 
case 
traffic light red 
cars passing no 
solution wait 
case 
traffic light red 
cars passing yes 
solution wait 
case 
traffic light green 
cars passing yes 
solution wait 
case 
traffic light green 
cars passing no 
solution cross 
retrieved 
cases 
solution wait 
justification 
traffic light red 
figure an example of justification generation in a cbr system notice that since the only relevant feature to decide is traffic light 
 the only one used to retrieve cases it is the only one appearing in the justification 
in general the meaning of a justification is that all or most of 
the cases in the case base of an agent that satisfy the justification 
 i e all the cases that are subsumed by the justification belong to 
the predicted solution class in the rest of the paper we will use 
to denote the subsumption relation in our work we use lid a 
cbr method capable of building symbolic justifications such as the 
one exemplified in figure when an agent provides a justification 
for a prediction the agent generates a justified prediction 
definition a justified prediction is a tuple j a p 
s d where agent a considers s the correct solution for problem 
p and that prediction is justified a symbolic description d such 
that j d j p 
justifications can have many uses for cbr systems in this 
paper we are going to use justifications as arguments in order to 
allow learning agents to engage in argumentation processes 
 arguments and 
counterarguments 
for our purposes an argument α generated by an agent a is 
composed of a statement s and some evidence d supporting s as 
correct in the remainder of this section we will see how this 
general definition of argument can be instantiated in specific kind of 
arguments that the agents can generate in the context of mac 
systems agents argue about predictions for new problems and can 
provide two kinds of information a specific cases p s and b 
justified predictions a p s d using this information we can 
define three types of arguments justified predictions 
counterarguments and counterexamples 
a justified prediction α is generated by an agent ai to argue that 
ai believes that the correct solution for a given problem p is α s 
and the evidence provided is the justification α d in the 
example depicted in figure an agent ai may generate the argument 
α ai p wait traffic light red meaning that the agent ai 
believes that the correct solution for p is wait because the attribute 
traffic light equals red 
a counterargument β is an argument offered in opposition to 
another argument α in our framework a counterargument 
consists of a justified prediction aj p s d generated by an agent 
aj with the intention to rebut an argument α generated by another 
agent ai that endorses a solution class s different from that of 
α s for the problem at hand and justifies this with a justification 
d in the example in figure if an agent generates the argument 
α ai p walk cars passing no an agent that thinks that 
the correct solution is wait might answer with the counterargument 
β aj p wait cars passing no ∧ traffic light red 
meaning that although there are no cars passing the traffic light is red 
and the street cannot be crossed 
a counterexample c is a case that contradicts an argument α 
thus a counterexample is also a counterargument one that states 
that a specific argument α is not always true and the evidence 
provided is the case c specifically for a case c to be a 
counterexample of an argument α the following conditions have to be met 
α d c and α s c s i e the case must satisfy the justification 
α d and the solution of c must be different than the predicted by 
α 
by exchanging arguments and counterarguments including 
counterexamples agents can argue about the correct solution of a given 
problem i e they can engage a joint deliberation process 
however in order to do so they need a specific interaction protocol a 
preference relation between contradicting arguments and a 
decision policy to generate counterarguments including 
counterexamples in the following sections we will present these elements 
 preference relation 
a specific argument provided by an agent might not be consistent 
with the information known to other agents or even to some of the 
information known by the agent that has generated the justification 
due to noise in training data for that reason we are going to 
define a preference relation over contradicting justified predictions 
based on cases basically we will define a confidence measure for 
each justified prediction that takes into account the cases owned by 
each agent and the justified prediction with the highest confidence 
will be the preferred one 
the idea behind case-based confidence is to count how many of 
the cases in an individual case base endorse a justified prediction 
and how many of them are counterexamples of it the more the 
endorsing cases the higher the confidence and the more the 
counterexamples the lower the confidence specifically to assess the 
confidence of a justified prediction α an agent obtains the set of 
cases in its individual case base that are subsumed by α d with 
them an agent ai obtains the y aye and n nay values 
 y ai 
α {c ∈ ci α d c p ∧ α s c s} is the number 
of cases in the agent s case base subsumed by the justification 
α d that belong to the solution class α s 
 nai 
α {c ∈ ci α d c p ∧ α s c s} is the number 
of cases in the agent s case base subsumed by justification 
α d that do not belong to that solution class 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
 
 
 
 
 
- 
- 
figure confidence of arguments is evaluated by contrasting them against the case bases of the agents 
an agent estimates the confidence of an argument as 
cai α 
y ai 
α 
 y ai 
α nai 
α 
i e the confidence on a justified prediction is the number of 
endorsing cases divided by the number of endorsing cases plus 
counterexamples notice that we add to the denominator this is to avoid 
giving excessively high confidences to justified predictions whose 
confidence has been computed using a small number of cases 
notice that this correction follows the same idea than the laplace 
correction to estimate probabilities figure illustrates the individual 
evaluation of the confidence of an argument in particular three 
endorsing cases and one counterexample are found in the case base 
of agents ai giving an estimated confidence of 
moreover we can also define the joint confidence of an argument 
α as the confidence computed using the cases present in the case 
bases of all the agents in the group 
c α i y ai 
α 
 i y ai 
α nai 
α 
notice that to collaboratively compute the joint confidence the 
agents only have to make public the aye and nay values locally 
computed for a given argument 
in our framework agents use this joint confidence as the 
preference relation a justified prediction α is preferred over another one 
β if c α ≥ c β 
 generation of arguments 
in our framework arguments are generated by the agents from 
cases using learning methods any learning method able to 
provide a justified prediction can be used to generate arguments for 
instance decision trees and lid are suitable learning methods 
specifically in the experiments reported in this paper agents use 
lid thus when an agent wants to generate an argument 
endorsing that a specific solution class is the correct solution for a problem 
p it generates a justified prediction as explained in section 
for instance figure shows a real justification generated by 
lid after solving a problem p in the domain of marine sponges 
identification in particular figure shows how when an agent 
receives a new problem to solve in this case a new sponge to 
determine its order the agent uses lid to generate an argument 
 consisting on a justified prediction using the cases in the case 
base of the agent the justification shown in figure can be 
interpreted saying that the predicted solution is hadromerida 
because the smooth form of the megascleres of the spiculate 
skeleton of the sponge is of type tylostyle the spikulate skeleton of the 
sponge has no uniform length and there is no gemmules in the 
external features of the sponge thus the argument generated will 
be α a p hadromerida d 
 generation of counterarguments 
as previously stated agents may try to rebut arguments by 
generating counterargument or by finding counterexamples let us 
explain how they can be generated 
an agent ai wants to generate a counterargument β to rebut an 
argument α when α is in contradiction with the local case base of 
ai moreover while generating such counterargument β ai 
expects that β is preferred over α for that purpose we will present 
a specific policy to generate counterarguments based on the 
specificity criterion 
the specificity criterion is widely used in deductive frameworks 
for argumentation and states that between two conflicting 
arguments the most specific should be preferred since it is in 
principle more informed thus counterarguments generated based on 
the specificity criterion are expected to be preferable since they are 
more informed to the arguments they try to rebut however there 
is no guarantee that such counterarguments will always win since 
as we have stated in section agents in our framework use a 
preference relation based on joint confidence moreover one may think 
that it would be better that the agents generate counterarguments 
based on the joint confidence preference relation however it is not 
obvious how to generate counterarguments based on joint 
confidence in an efficient way since collaboration is required in order to 
evaluate joint confidence thus the agent generating the 
counterargument should constantly communicate with the other agents at 
each step of the induction algorithm used to generate 
counterarguments presently one of our future research lines 
thus in our framework when an agent wants to generate a 
counterargument β to an argument α β has to be more specific than α 
 i e α d β d 
the generation of counterarguments using the specificity 
criterion imposes some restrictions over the learning method although 
lid or id can be easily adapted for this task for instance lid is 
an algorithm that generates a description starting from scratch and 
heuristically adding features to that term thus at every step the 
description is made more specific than in the previous step and the 
number of cases that are subsumed by that description is reduced 
when the description covers only or almost only cases of a 
single solution class lid terminates and predicts that solution class 
to generate a counterargument to an argument α lid just has to 
use as starting point the description α d instead of starting from 
scratch in this way the justification provided by lid will always 
be subsumed by α d and thus the resulting counterargument will 
be more specific than α however notice that lid may sometimes 
not be able to generate counterarguments since lid may not be 
able to specialize the description α d any further or because the 
agent ai has no case inci that is subsumed by α d figure shows 
how an agent a that disagreed with the argument shown in 
figure generates a counterargument using lid moreover figure 
shows the generation of a counterargument β 
 for the argument α 
 
 in figure that is a specialization of α 
 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
solution hadromerida 
justification d 
sponge 
spikulate 
skeleton 
external 
features 
external features 
gemmules no 
spikulate skeleton 
megascleres 
uniform length no 
megascleres 
smooth form tylostyle 
case base 
of a 
lid 
new 
sponge 
p 
figure example of a real justification generated by lid in the marine sponges data set 
specifically in our experiments when an agent ai wants to rebut 
an argument α uses the following policy 
 agent ai uses lid to try to find a counterargument β more 
specific than α if found β is sent to the other agent as a 
counterargument of α 
 if not found then ai searches for a counterexample c ∈ ci 
of α if a case c is found then c is sent to the other agent as 
a counterexample of α 
 if no counterexamples are found then ai cannot rebut the 
argument α 
 argumentation-based 
multi-agent learning 
the interaction protocol of amal allows a group of agents a 
 an to deliberate about the correct solution of a problem p by 
means of an argumentation process if the argumentation process 
arrives to a consensual solution the joint deliberation ends 
otherwise a weighted vote is used to determine the joint solution 
moreover amal also allows the agents to learn from the 
counterexamples received from other agents 
the amal protocol consists on a series of rounds in the initial 
round each agent states which is its individual prediction for p 
then at each round an agent can try to rebut the prediction made 
by any of the other agents the protocol uses a token passing 
mechanism so that agents one at a time can send counterarguments or 
counterexamples if they disagree with the prediction made by any 
other agent specifically each agent is allowed to send one 
counterargument or counterexample each time he gets the token notice 
that this restriction is just to simplify the protocol and that it does 
not restrict the number of counterargument an agent can sent since 
they can be delayed for subsequent rounds when an agent 
receives a counterargument or counterexample it informs the other 
agents if it accepts the counterargument and changes its 
prediction or not moreover agents have also the opportunity to answer 
to counterarguments when they receive the token by trying to 
generate a counterargument to the counterargument 
when all the agents have had the token once the token returns 
to the first agent and so on if at any time in the protocol all the 
agents agree or during the last n rounds no agent has generated 
any counterargument the protocol ends moreover if at the end of 
the argumentation the agents have not reached an agreement then 
a voting mechanism that uses the confidence of each prediction as 
weights is used to decide the final solution thus amal follows 
the same mechanism as human committees first each individual 
member of a committee exposes his arguments and discuses those 
of the other members joint deliberation and if no consensus is 
reached then a voting mechanism is required 
at each iteration agents can use the following performatives 
 assert α the justified prediction held during the next round 
will be α an agent can only hold a single prediction at each 
round thus is multiple asserts are send only the last one is 
considered as the currently held prediction 
 rebut β α the agent has found a counterargument β to the 
prediction α 
we will define ht αt 
 αt 
n as the predictions that each 
of the n agents hold at a round t moreover we will also define 
contradict αt 
i {α ∈ ht α s αt 
i s} as the set of 
contradicting arguments for an agent ai in a round t i e the set of 
arguments at round t that support a different solution class than αt 
i 
the protocol is initiated because one of the agents receives a 
problem p to be solved after that the agent informs all the other 
agents about the problem p to solve and the protocol starts 
 at round t each one of the agents individually solves p 
and builds a justified prediction using its own cbr method 
then each agent ai sends the performative assert α 
i to 
the other agents thus the agents know h α 
i α 
n 
once all the predictions have been sent the token is given to 
the first agent a 
 at each round t other than the agents check whether their 
arguments in ht agree if they do the protocol moves to step 
 moreover if during the last n rounds no agent has sent any 
counterexample or counterargument the protocol also moves 
to step otherwise the agent ai owner of the token tries 
to generate a counterargument for each of the opposing 
arguments in contradict αt 
i ⊆ ht see section then the 
counterargument βt 
i against the prediction αt 
j with the 
lowest confidence c αt 
j is selected since αt 
j is the prediction 
more likely to be successfully rebutted 
 if βt 
i is a counterargument then ai locally compares 
αt 
i with βt 
i by assessing their confidence against its 
individual case base ci see section notice that ai is 
comparing its previous argument with the 
counterargument that ai itself has just generated and that is about 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
sponge 
spikulate 
skeleton 
external 
features 
external features 
gemmules no 
growing 
spikulate skeleton 
megascleres 
uniform length no 
megascleres 
smooth form tylostyle 
growing 
grow massive 
case base 
of a 
lid 
solution astrophorida 
justification d 
figure generation of a counterargument using lid in the sponges data set 
to send to aj if cai βt 
i cai αt 
i then ai 
considers that βt 
i is stronger than its previous argument 
changes its argument to βt 
i by sending assert βt 
i to 
the rest of the agents the intuition behind this is that 
since a counterargument is also an argument ai checks 
if the newly counterargument is a better argument than 
the one he was previously holding and rebut βt 
i 
αt 
j to aj otherwise i e cai βt 
i ≤ cai αt 
i ai 
will send only rebut βt 
i αt 
j to aj in any of the two 
situations the protocol moves to step 
 if βt 
i is a counterexample c then ai sends rebut c αt 
j 
to aj the protocol moves to step 
 if ai cannot generate any counterargument or 
counterexample the token is sent to the next agent a new 
round t starts and the protocol moves to state 
 the agent aj that has received the counterargument βt 
i 
locally compares it against its own argument αt 
j by locally 
assessing their confidence if caj βt 
i caj αt 
j then 
aj will accept the counterargument as stronger than its own 
argument and it will send assert βt 
i to the other agents 
otherwise i e caj βt 
i ≤ caj αt 
j aj will not accept 
the counterargument and will inform the other agents 
accordingly any of the two situations start a new round t 
ai sends the token to the next agent and the protocol moves 
back to state 
 the agent aj that has received the counterexample c retains 
it into its case base and generates a new argument αt 
j that 
takes into account c and informs the rest of the agents by 
sending assert αt 
j to all of them then ai sends the 
token to the next agent a new round t starts and the 
protocol moves back to step 
 the protocol ends yielding a joint prediction as follows if 
the arguments in ht agree then their prediction is the joint 
prediction otherwise a voting mechanism is used to decide 
the joint prediction the voting mechanism uses the joint 
confidence measure as the voting weights as follows 
s arg max 
sk∈s 
αi∈ht αi s sk 
c αi 
moreover in order to avoid infinite iterations if an agent sends 
twice the same argument or counterargument to the same agent the 
message is not considered 
 exemplification 
let us consider a system composed of three agents a a and 
a one of the agents a receives a problem p to solve and 
decides to use amal to solve it for that reason invites a and a to 
take part in the argumentation process they accept the invitation 
and the argumentation protocol starts 
initially each agent generates its individual prediction for p and 
broadcasts it to the other agents thus all of them can compute 
h α 
 α 
 α 
 in particular in this example 
 α 
 a p hadromerida d 
 α 
 a p astrophorida d 
 α 
 a p axinellida d 
a starts owning the token and tries to generate 
counterarguments for α 
 and α 
 but does not succeed however it has one 
counterexample c for α 
 thus a sends the the message rebut 
c α 
 to a a incorporates c into its case base and tries to 
solve the problem p again now taking c into consideration a 
comes up with the justified prediction α 
 a p hadromerida 
d and broadcasts it to the rest of the agents with the message 
assert α 
 thus all of them know the new h α 
 α 
 α 
 
round starts and a gets the token a tries to generate 
counterarguments for α 
 and α 
 and only succeeds to generate a 
counterargument β 
 a p astrophorida d against α 
 the 
counterargument is sent to a with the message rebut β 
 α 
 
agent a receives the counterargument and assesses its local 
confidence the result is that the individual confidence of the 
counterargument β 
 is lower than the local confidence of α 
 therefore a 
does not accept the counterargument and thus h α 
 α 
 α 
 
round starts and a gets the token a generates a 
counterargument β 
 a p hadromerida d for α 
 and sends it to 
a with the message rebut β 
 α 
 agent a receives the 
counterargument and assesses its local confidence the result is that the 
local confidence of the counterargument β 
 is higher than the local 
confidence of α 
 therefore a accepts the counterargument and 
informs the rest of the agents with the message assert β 
 after 
that h α 
 β 
 α 
 
at round since all the agents agree all the justified 
predictions in h predict hadromerida as the solution class the 
protocol ends and a the agent that received the problem considers 
hadromerida as the joint solution for the problem p 
 experimental evaluation 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
sponge 
 
 
 
 
 
 
 
 
 
 
amal 
voting 
individual 
soybean 
 
 
 
 
 
 
 
 
 
amal 
voting 
individual 
figure individual and joint accuracy for to agents 
in this section we empirically evaluate the amal argumentation 
framework we have made experiments in two different data sets 
soybean from the uci machine learning repository and sponge a 
relational data set the soybean data set has examples and 
solution classes while the sponge data set has examples and 
solution classes in an experimental run the data set is divided in 
sets the training set and the test set the training set examples are 
distributed among different agents without replication i e there 
is no example shared by two agents in the testing stage problems 
in the test set arrive randomly to one of the agents and their goal is 
to predict the correct solution 
the experiments are designed to test two hypotheses h that 
argumentation is a useful framework for joint deliberation and can 
improve over other typical methods such as voting and h that 
learning from communication improves the individual performance 
of a learning agent participating in an argumentation process 
moreover we also expect that the improvement achieved from 
argumentation will increase as the number of agents participating in the 
argumentation increases since more information will be taken into 
account 
concerning h argumentation is a useful framework for joint 
deliberation we ran experiments using and agents 
respectively in all experiments each agent has a of the training 
data since the training is always distributed among agents 
figure shows the result of those experiments in the sponge and 
soybean data sets classification accuracy is plotted in the 
vertical axis and in the horizontal axis the number of agents that took 
part in the argumentation processes is shown for each number of 
agents three bars are shown individual voting and amal the 
individual bar shows the average accuracy of individual agents 
predictions the voting bar shows the average accuracy of the joint 
prediction achieved by voting but without any argumentation and 
finally the amal bar shows the average accuracy of the joint 
prediction using argumentation the results shown are the average of 
 -fold cross validation runs 
figure shows that collaboration voting and amal 
outperforms individual problem solving moreover as we expected the 
accuracy improves as more agents collaborate since more 
information is taken into account we can also see that amal always 
outperforms standard voting proving that joint decisions are based 
on better information as provided by the argumentation process 
for instance the joint accuracy for agents in the sponge data 
set is of for amal and for voting while individual 
accuracy is just moreover the improvement achieved by 
amal over voting is even larger in the soybean data set the 
reason is that the soybean data set is more difficult in the sense that 
agents need more data to produce good predictions these 
experimental results show that amal effectively exploits the opportunity 
for improvement the accuracy is higher only because more agents 
have changed their opinion during argumentation otherwise they 
would achieve the same result as voting 
concerning h learning from communication in argumentation 
processes improves individual prediction we ran the following 
experiment initially we distributed a of the training set among 
the five agents after that the rest of the cases in the training set is 
sent to the agents one by one when an agent receives a new 
training case it has several options the agent can discard it the agent 
can retain it or the agent can use it for engaging an argumentation 
process figure shows the result of that experiment for the two 
data sets figure contains three plots where nl not learning 
shows accuracy of an agent with no learning at all l learning 
shows the evolution of the individual classification accuracy when 
agents learn by retaining the training cases they individually 
receive notice that when all the training cases have been retained at 
 the accuracy should be equal to that of figure for 
individual agents and finally lfc learning from communication shows 
the evolution of the individual classification accuracy of learning 
agents that also learn by retaining those counterexamples received 
during argumentation i e they learn both from training examples 
and counterexamples 
figure shows that if an agent ai learns also from 
communication ai can significantly improve its individual performance with 
just a small number of additional cases those selected as relevant 
counterexamples for ai during argumentation for instance in 
the soybean data set individual agents have achieved an accuracy 
of when they also learn from communication versus an 
accuracy of when they only learn from their individual 
experience the number of cases learnt from communication depends 
on the properties of the data set in the sponges data set agents 
have retained only very few additional cases and significantly 
improved individual accuracy namely they retain cases in 
average compared to the cases retained if they do not learn from 
communication in the soybean data set more counterexamples are 
learnt to significantly improve individual accuracy namely they 
retain cases in average compared to cases retained if 
they do not learn from communication finally the fact that both 
data sets show a significant improvement points out the adaptive 
nature of the argumentation-based approach to learning from 
communication the useful cases are selected as counterexamples and 
no more than those needed and they have the intended effect 
 related work 
concerning cbr in a multi-agent setting the first research was 
on negotiated case retrieval among groups of agents our 
work on multi-agent case-based learning started in later 
mc ginty and smyth presented a multi-agent collaborative cbr 
approach ccbr for planning finally another interesting 
approach is multi-case-base reasoning mcbr that deals with 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
sponge 
 
 
 
 
 
 
 
lfc 
l 
nl 
soybean 
 
 
 
 
 
 
 
 
 
lfc 
l 
nl 
figure learning from communication resulting from argumentation in a system composed of agents 
distributed systems where there are several case bases available for 
the same task and addresses the problems of cross-case base 
adaptation the main difference is that our mac approach is a way to 
distribute the reuse process of cbr using a voting system while 
retrieve is performed individually by each agent the other 
multiagent cbr approaches however focus on distributing the retrieve 
process 
research on mas argumentation focus on several issues like a 
logics protocols and languages that support argumentation b 
argument selection and c argument interpretation approaches for 
logic and languages that support argumentation include defeasible 
logic and bdi models although argument selection is a 
key aspect of automated argumentation see and most 
research has been focused on preference relations among arguments 
in our framework we have addressed both argument selection and 
preference relations using a case-based approach 
 conclusions and future work 
in this paper we have presented an argumentation-based 
framework for multi-agent learning specifically we have presented 
amal a framework that allows a group of learning agents to 
argue about the solution of a given problem and we have shown how 
the learning capabilities can be used to generate arguments and 
counterarguments the experimental evaluation shows that the 
increased amount of information provided to the agents by the 
argumentation process increases their predictive accuracy and specially 
when an adequate number of agents take part in the argumentation 
the main contributions of this work are a an argumentation 
framework for learning agents b a case-based preference relation 
over arguments based on computing an overall confidence 
estimation of arguments c a case-based policy to generate 
counterarguments and select counterexamples and d an argumentation-based 
approach for learning from communication 
finally in the experiments presented here a learning agent would 
retain all counterexamples submitted by the other agent however 
this is a very simple case retention policy and we will like to 
experiment with more informed policies - with the goal that individual 
learning agents could significantly improve using only a small set 
of cases proposed by other agents finally our approach is focused 
on lazy learning and future works aims at incorporating eager 
inductive learning inside the argumentative framework for learning 
from communication 
 references 
 agnar aamodt and enric plaza case-based reasoning 
foundational issues methodological variations and system 
approaches artificial intelligence communications 
 - 
 e armengol and e plaza lazy induction of descriptions for 
relational case-based learning in ecml pages - 
 
 gerhard brewka dynamic argument systems a formal 
model of argumentation processes based on situation 
calculus journal of logic and computation - 
 
 carlos i chesñevar and guillermo r simari formalizing 
defeasible argumentation using labelled deductive 
systems journal of computer science technology 
 - 
 d leake and r sooriamurthi automatically selecting 
strategies for multi-case-base reasoning in s craw and 
a preece editors eccbr pages - berlin 
 springer verlag 
 francisco j martín enric plaza and josep-lluis arcos 
knowledge and experience reuse through communications 
among competent peer agents international journal of 
software engineering and knowledge engineering 
 - 
 lorraine mcginty and barry smyth collaborative 
case-based reasoning applications in personalized route 
planning in i watson and q yang editors iccbr number 
 in lnai pages - springer-verlag 
 santi ontañón and enric plaza justification-based 
multiagent learning in icml pages - morgan 
kaufmann 
 enric plaza eva armengol and santiago ontañón the 
explanatory power of symbolic similarity in case-based 
reasoning artificial intelligence review - 
 
 david poole on the comparison of theories preferring the 
most specific explanation in ijcai- pages - 
 
 m v nagendra prassad victor r lesser and susan lander 
retrieval and reasoning in distributed case bases technical 
report umass computer science department 
 k sycara s kraus and a evenchik reaching agreements 
through argumentation a logical model and implementation 
artificial intelligence journal - 
 n r jennings s parsons c sierra agents that reason and 
negotiate by arguing journal of logic and computation 
 - 
 bruce a wooley explanation component of software 
systems acm crossroads 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a multi-agent system for building dynamic ontologies 
kévin ottens 
∗ 
irit université paul sabatier 
 route de narbonne 
f- toulouse 
ottens irit fr 
marie-pierre gleizes 
irit université paul sabatier 
 route de narbonne 
f- toulouse 
gleizes irit fr 
pierre glize 
irit université paul sabatier 
 route de narbonne 
f- toulouse 
glize irit fr 
abstract 
ontologies building from text is still a time-consuming task which 
justifies the growth of ontology learning our system named 
dynamo is designed along this domain but following an original 
approach based on an adaptive multi-agent architecture in this paper 
we present a distributed hierarchical clustering algorithm core of 
our approach it is evaluated and compared to a more conventional 
centralized algorithm we also present how it has been improved 
using a multi-criteria approach with those results in mind we 
discuss the limits of our system and add as perspectives the 
modifications required to reach a complete ontology building solution 
categories and subject descriptors 
i artificial intelligence distributed artificial 
intelligencemultiagent systems 
general terms 
algorithms experimentation 
 introduction 
nowadays it is well established that ontologies are needed for 
semantic web knowledge management b b for knowledge 
management ontologies are used to annotate documents and to 
enhance the information retrieval but building an ontology manually 
is a slow tedious costly complex and time consuming process 
currently a real challenge lies in building them automatically or 
semi-automatically and keeping them up to date it would mean 
creating dynamic ontologies and it justifies the emergence of 
ontology learning techniques 
our research focuses on dynamo an acronym of dynamic 
ontologies a tool based on an adaptive multi-agent system to 
construct and maintain an ontology from a domain specific set of texts 
our aim is not to build an exhaustive general hierarchical ontology 
but a domain specific one we propose a semi-automated tool since 
an external resource is required the ontologist an ontologist is 
a kind of cognitive engineer or analyst who is using information 
from texts and expert interviews to design ontologies 
in the multi-agent field ontologies generally enable agents to 
understand each other they re sometimes used to ease the 
ontology building process in particular for collaborative contexts 
but they rarely represent the ontology itself most works 
interested in the construction of ontologies propose the refinement of 
ontologies this process consists in using an existing ontology and 
building a new one from it this approach is different from our 
approach because dynamo starts from scratch researchers working 
on the construction of ontologies from texts claim that the work to 
be automated requires external resources such as a dictionary 
or web access in our work we propose an interaction between 
the ontologist and the system our external resource lies both in the 
texts and the ontologist 
this paper first presents in section the big picture of the 
dynamo system in particular the motives that led to its creation and 
its general architecture then in section we discuss the 
distributed clustering algorithm used in dynamo and compare it to 
a more classic centralized approach section is dedicated to some 
enhancement of the agents behavior that got designed by taking 
into account criteria ignored by clustering and finally in section 
 we discuss the limitations of our approach and explain how it 
will be addressed in further work 
 dynamo overview 
 ontology as a multi-agent system 
dynamo aims at reducing the need for manual actions in 
processing the text analysis results and at suggesting a concept 
network kick-off in order to build ontologies more efficiently the 
chosen approach is completely original to our knowledge and uses 
an adaptive multi-agent system this choice comes from the 
qualities offered by multi-agent system they can ease the interactive 
design of a system in our case a conceptual network they 
allow its incremental building by progressively taking into account 
new data coming from text analysis and user interaction and last 
but not least they can be easily distributed across a computer 
network 
dynamo takes a syntactical and terminological analysis of texts 
as input it uses several criteria based on statistics computed from 
the linguistic contexts of terms to create and position the concepts 
as output dynamo provides to the analyst a hierarchical 
organization of concepts the multi-agent system itself that can be 
validated refined of modified until he she obtains a satisfying state of 
 
 - - - - rps c ifaamas 
the semantic network 
an ontology can be seen as a stable map constituted of 
conceptual entities represented here by agents linked by labelled 
relations thus our approach considers an ontology as a type of 
equilibrium between its concept-agents where their forces are 
defined by their potential relationships the ontology modification 
is a perturbation of the previous equilibrium by the appearance or 
disappearance of agents or relationships in this way a dynamic 
ontology is a self-organizing process occurring when new texts are 
included into the corpus or when the ontologist interacts with it 
to support the needed flexibility of such a system we use a 
selforganizing multi-agent system based on a cooperative approach 
we followed the adelfe method proposed to drive the design 
of this kind of multi-agent system it justifies how we designed 
some of the rules used by our agents in order to maximize the 
cooperation degree within dynamo s multi-agent system 
 proposed architecture 
in this section we present our system architecture it addresses 
the needs of knowledge engineering in the context of dynamic 
ontology management and maintenance when the ontology is linked 
to a document collection 
the dynamo system consists of three parts cf figure 
 a term network obtained thanks to a term extraction tool 
used to preprocess the textual corpus 
 a multi-agent system which uses the term network to make a 
hierarchical clustering in order to obtain a taxonomy of 
concepts 
 an interface allowing the ontologist to visualize and control 
the clustering process 
 
ontologist 
interface 
system 
concept agent term 
term network 
terms 
extraction 
tool 
figure system architecture 
the term extractor we use is syntex a software that has 
efficiently been used for ontology building tasks we mainly 
selected it because of its robustness and the great amount of 
information extracted in particular it creates a head-expansion network 
which has already proven to be interesting for a clustering system 
 in such a network each term is linked to its head term 
and 
 
i e the maximum sub-phrase located as head of the term 
its expansion term 
 and also to all the terms for which it is a head 
or an expansion term for example knowledge engineering from 
text has knowledge engineering as head term and text as 
expansion term moreover knowledge engineering is composed of 
 knowledge as head term and engineering as expansion term 
with dynamo the term network obtained as the output of the 
extractor is stored in a database for each term pair we assume that it 
is possible to compute a similarity value in order to make a 
clustering because of the nature of the data we are only focusing 
on similarity computation between objects described thanks to 
binary variables that means that each item is described by the 
presence or absence of a characteristic set in the case of terms 
we are generally dealing with their usage contexts with syntex 
those contexts are identified by terms and characterized by some 
syntactic relations 
the dynamo multi-agent system implements the distributed 
clustering algorithm described in detail in section and the rules 
described in section it is designed to be both the system 
producing the resulting structure and the structure itself it means that 
each agent represent a class in the taxonomy then the system 
output is the organization obtained from the interaction between 
agents while taking into account feedback coming from the 
ontologist when he she modifies the taxonomy given his needs or 
expertise 
 distributed clustering 
this section presents the distributed clustering algorithm used in 
dynamo for the sake of understanding and because of its 
evaluation in section we recall the basic centralized algorithm used 
for a hierarchical ascending clustering in a non metric space when 
a symmetrical similarity measure is available which is the 
case of the measures used in our system 
algorithm centralized hierarchical ascending clustering 
algorithm 
data list l of items to organize as a hierarchy 
result root r of the hierarchy 
while length l do 
max ← 
a ← nil 
b ← nil 
for i ← to length l do 
i ← l i 
for j ← i to length l do 
j ← l j 
sim ← similarity i j 
if sim max then 
max ← sim 
a ← i 
b ← j 
end 
end 
end 
remove a l 
remove b l 
append a b l 
end 
r ← l 
in algorithm for each clustering step the pair of the most 
similar elements is determined those two elements are grouped in a 
cluster and the resulting class is appended to the list of remaining 
elements this algorithm stops when the list has only one element 
left 
 
i e the maximum sub-phrase located as tail of the term 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the hierarchy resulting from algorithm is always a binary tree 
because of the way grouping is done moreover grouping the most 
similar elements is equivalent to moving them away from the least 
similar ones our distributed algorithm is designed relying on those 
two facts it is executed concurrently in each of the agents of the 
system 
note that in the following of this paper we used for both 
algorithms an anderberg similarity with α and an average 
link clustering strategy those choices have an impact on the 
resulting tree but they impact neither the global execution of the 
algorithm nor its complexity 
we now present the distributed algorithm used in our system it 
is bootstrapped in the following way 
 a top agent having no parent is created it will be the root of 
the resulting taxonomy 
 an agent is created for each term to be positioned in the 
taxonomy they all have top as parent 
once this basic structure is set the algorithm runs until it reaches 
equilibrium and then provides the resulting taxonomy 
ak− ak ana a 
p 
 
a 
figure distributed classification step 
the process first step figure is triggered when an agent here 
ak has more than one brother since we want to obtain a binary 
tree then it sends a message to its parent p indicating its most 
dissimilar brother here a then p receives the same kind of 
message from each of its children in the following this kind of 
message will be called a vote 
ak− ak ana a 
p 
p 
 
p 
p 
figure distributed clustering step 
next when p has got messages from all its children it starts the 
second step figure thanks to the received messages indicating 
the preferences of its children p can determine three sub-groups 
among its children 
 the child which got the most votes by its brothers that is 
the child being the most dissimilar from the greatest number 
of its brothers in case of a draw one of the winners is chosen 
randomly here a 
 the children that allowed the election of the first group that 
is the agents which chose their brother of the first group as 
being the most dissimilar one here ak to an 
 the remaining children here a to ak− 
then p creates a new agent p having p as parent and asks 
agents from the second group here agents ak to an to make it 
their new parent 
ak− ak ana a 
p 
p 
 
figure distributed clustering step 
finally step figure is trivial the children rejected by p 
 here agent a to an take its message into account and choose p 
as their new parent the hierarchy just created a new intermediate 
level 
note that this algorithm generally converges since the number of 
brothers of an agent drops when an agent has only one remaining 
brother its activity stops although it keeps processing messages 
coming from its children however in a few cases we can reach 
a circular conflict in the voting procedure when for example a 
votes against b b against c and c against a with the current 
system no decision can be taken the current procedure should be 
improved to address this probably using a ranked voting method 
 quantitative evaluation 
now we evaluate the properties of our distributed algorithm it 
requires to begin with a quantitative evaluation based on its 
complexity while comparing it with the algorithm from the previous 
section 
its theoretical complexity is calculated for the worst case by 
considering the similarity computation operation as elementary for 
the distributed algorithm the worst case means that for each run 
only a two-item group can be created under those conditions for a 
given dataset of n items we can determine the amount of similarity 
computations 
for algorithm we note l length l then the most enclosed 
 for loop is run l − i times and its body has the only similarity 
computation so its cost is l−i the second for loop is ran l times 
for i ranging from to l then its cost is 
pl 
i l − i which can 
be simplified in l× l− 
 
 finally for each run of the while loop 
l is decreased from n to which gives us t n as the amount of 
similarity computations for algorithm 
t n 
nx 
l 
l × l − 
 
 
for the distributed algorithm at a given step each one of the l 
agents evaluates the similarity with its l − brothers so each steps 
has a l × l − cost then groups are created and another vote 
occurs with l decreased by one since we assume worst case only 
groups of size or l − are built since l is equal to n on first run 
we obtain tdist n as the amount of similarity computations for the 
distributed algorithm 
tdist n 
nx 
l 
l × l − 
both algorithms then have an o n 
 complexity but in the 
worst case the distributed algorithm does twice the number of 
el the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
ementary operations done by the centralized algorithm this gap 
comes from the local decision making in each agent because of 
this the similarity computations are done twice for each agent pair 
we could conceive that an agent sends its computation result to its 
peer but it would simply move the problem by generating more 
communication in the system 
 
 
 
 
 
 
 
 
 
 
 
amountofcomparisons 
amount of input terms 
 distributed algorithm on average with min and max 
 logarithmic polynomial 
 centralized algorithm 
figure experimental results 
in a second step the average complexity of the algorithm has 
been determined by experiments the multi-agent system has been 
executed with randomly generated input data sets ranging from ten 
to one hundred terms the given value is the average of 
comparisons made for one hundred of runs without any user interaction 
it results in the plots of figure the algorithm is then more 
efficient on average than the centralized algorithm and its average 
complexity is below the worst case it can be explained by the low 
probability that a data set forces the system to create only minimal 
groups two items or maximal n − elements for each step of 
reasoning curve number represents the logarithmic polynomial 
minimizing the error with curve number the highest degree term 
of this polynomial is in n 
log n then our distributed algorithm 
has a o n 
log n complexity on average finally let s note the 
reduced variation of the average performances with the maximum 
and the minimum in the worst case for terms the variation is 
of for an average of around which shows 
the good stability of the system 
 qualitative evaluation 
although the quantitative results are interesting the real 
advantage of this approach comes from more qualitative characteristics 
that we will present in this section all are advantages obtained 
thanks to the use of an adaptive multi-agent system 
the main advantage to the use of a multi-agent system for a 
clustering task is to introduce dynamic in such a system the ontologist 
can make modifications and the hierarchy adapts depending on the 
request it is particularly interesting in a knowledge engineering 
context indeed the hierarchy created by the system is meant to be 
modified by the ontologist since it is the result of a statistic 
computation during the necessary look at the texts to examine the 
usage contexts of terms the ontologist will be able to interpret 
the real content and to revise the system proposal it is extremely 
difficult to realize this with a centralized black-box approach in 
most cases one has to find which reasoning step generated the error 
and to manually modify the resulting class unfortunately in this 
case all the reasoning steps that occurred after the creation of the 
modified class are lost and must be recalculated by taking the 
modification into account that is why a system like asium tries to 
soften the problem with a system-user collaboration by showing to 
the ontologist the created classes after each step of reasoning but 
the ontologist can make a mistake and become aware of it too late 
figure concept agent tree after autonomous stabilization of 
the system 
in order to illustrate our claims we present an example thanks to 
a few screenshots from the working prototype tested on a medical 
related corpus by using test data and letting the system work by 
itself we obtain the hierarchy from figure after stabilization it is 
clear that the concept described by the term lésion lesion is 
misplaced it happens that the similarity computations place it closer to 
 femme woman and chirurgien surgeon than to infection 
 gastro-entérite gastro-enteritis and hépatite hepatitis this 
wrong position for lesion is explained by the fact that without 
ontologist input the reasoning is only done on statistics criteria 
figure concept agent tree after ontologist modification 
then the ontologist replaces the concept in the right branch by 
affecting conceptagent as its new parent the name 
 conceptagent x is automatically given to a concept agent that is not 
described by a term the system reacts by itself and refines the 
clustering hierarchy to obtain a binary tree by creating 
 conceptagent the new stable state if the one of figure 
this system-user coupling is necessary to build an ontology but 
no particular adjustment to the distributed algorithm principle is 
needed since each agent does an autonomous local processing and 
communicates with its neighborhood by messages 
moreover this algorithm can de facto be distributed on a 
computer network the communication between agents is then done by 
sending messages and each one keeps its decision autonomy then 
a system modification to make it run networked would not require 
to adjust the algorithm on the contrary it would only require to 
rework the communication layer and the agent creation process since 
in our current implementation those are not networked 
 multi-criteria hierarchy 
in the previous sections we assumed that similarity can be 
computed for any term pair but as soon as one uses real data this 
property is not verified anymore some terms do not have any 
similarity value with any extracted term moreover for leaf nodes it is 
sometimes interesting to use other means to position them in the 
hierarchy for this low level structuring ontologists generally base 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
their choices on simple heuristics using this observation we built 
a new set of rules which are not based on similarity to support low 
level structuring 
 adding head coverage rules 
in this case agents can act with a very local point of view simply 
by looking at the parent child relation each agent can try to 
determine if its parent is adequate it is possible to guess this because 
each concept agent is described by a set of terms and thanks to the 
 head-expansion term network 
in the following tx will be the set of terms describing concept 
agent x and head tx the set of all the terms that are head of at 
least one element of tx thanks to those two notations we can 
describe the parent adequacy function a p c between a parent p 
and a child c 
a p c 
 tp ∩ head tc 
 tp ∪ head tc 
 
then the best parent for c is the p agent that maximizes a p c 
an agent unsatisfied by its parent can then try to find a better one 
by evaluating adequacy with candidates we designed a 
complementary algorithm to drive this search 
when an agent c is unsatisfied by its parent p it evaluates 
a bi c with all its brothers noted bi the one maximizing a bi c 
is then chosen as the new parent 
figure concept agent tree after autonomous stabilization of 
the system without head coverage rule 
we now illustrate this rule behavior with an example figure 
shows the state of the system after stabilization on test data we 
can notice that hépatite viral viral hepatitis is still linked to the 
taxonomy root it is caused by the fact that there is no similarity 
value between the viral hepatitis term and any of the term of the 
other concept agents 
figure concept agent tree after activation of the head 
coverage rule 
after activating the head coverage rule and letting the system 
stabilize again we obtain figure we can see that viral hepatitis 
slipped through the branch leading to hepatitis and chose it as its 
new parent it is a sensible default choice since viral hepatitis is 
a more specific term than hepatitis 
this rule tends to push agents described by a set of term to 
become leafs of the concept tree it addresses our concern to improve 
the low level structuring of our taxonomy but obviously our agents 
lack a way to backtrack in case of modifications in the taxonomy 
which would make them be located in the wrong branch that is 
one of the point where our system still has to be improved by adding 
another set of rules 
 on using several criteria 
in the previous sections and examples we only used one 
algorithm at a time the distributed clustering algorithm tends to 
introduce new layers in the taxonomy while the head coverage 
algorithm tends to push some of the agents toward the leafs of the 
taxonomy it obviously raises the question on how to deal with 
multiple criteria in our taxonomy building and how agents 
determine their priorities at a given time 
the solution we chose came from the search for minimizing non 
cooperation within the system in accordance with the adelfe 
method each agent computes three non cooperation degrees and 
chooses its current priority depending on which degree is the 
highest for a given agent a having a parent p a set of brothers bi 
and which received a set of messages mk having the priority pk 
the three non cooperation degrees are 
 μh a − a p a is the head coverage non 
cooperation degree determined by the head coverage of the parent 
 μb a max − similarity a bi is the 
 brotherhood non cooperation degree determined by the worst brother 
of a regarding similarities 
 μm a max pk is the message non cooperation 
degree determined by the most urgent message received 
then the non cooperation degree μ a of agent a is 
μ a max μh a μb a μm a 
then we have three cases determining which kind of action a will 
choose 
 if μ a μh a then a will use the head coverage 
algorithm we detailed in the previous subsection 
 if μ a μb a then a will use the distributed clustering 
algorithm see section 
 if μ a μm a then a will process mk immediately in 
order to help its sender 
those three cases summarize the current activities of our agents 
they have to find the best parent for them μ a μh a 
improve the structuring through clustering μ a μb a and 
process other agent messages μ a μm a in order to help them 
fulfill their own goals 
 experimental complexity revisited 
we evaluated the experimental complexity of the whole 
multiagent system when all the rules are activated in this case the 
metric used is the number of messages exchanged in the system once 
again the system has been executed with input data sets ranging 
from ten to one hundred terms the given value is the average of 
message amount sent in the system as a whole for one hundred runs 
without user interaction it results in the plots of figure 
curve number represents the average of the value obtained 
curve number represents the average of the value obtained when 
only the distributed clustering algorithm is activated not the full 
rule set curve number represents the polynomial minimizing the 
error with curve number the highest degree term of this 
polynomial is in n 
 then our multi-agent system has a o n 
 complexity 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
 
 
 
 
 
amountofmessages 
amount of input terms 
 dynamo all rules on average with min and max 
 distributed clustering only on average 
 cubic polynomial 
figure experimental results 
on average moreover let s note the very small variation of the 
average performances with the maximum and the minimum in the 
worst case for terms the variation is of for an average 
of around which proves the excellent stability of 
the system 
finally the extra head coverage rules are a real improvement on 
the distributed algorithm alone they introduce more constraints 
and stability point is reached with less interactions and decision 
making by the agents it means that less messages are exchanged 
in the system while obtaining a tree of higher quality for the 
ontologist 
 discussion perspectives 
 current limitation of our approach 
the most important limitation of our current algorithm is that 
the result depends on the order the data gets added when the 
system works by itself on a fixed data set given during initialization 
the final result is equivalent to what we could obtain with a 
centralized algorithm on the contrary adding a new item after a first 
stabilization has an impact on the final result 
figure concept agent tree after autonomous stabilization 
of the system 
to illustrate our claims we present another example of the 
working system by using test data and letting the system work by itself 
we obtain the hierarchy of figure after stabilization 
figure concept agent tree after taking in account 
 hepatitis 
then the ontologist interacts with the system and adds a new 
concept described by the term hepatitis and linked to the root 
the system reacts and stabilizes we then obtain figure as a 
result hepatitis is located in the right branch but we have not 
obtained the same organization as the figure of the previous 
example we need to improve our distributed algorithm to allow a 
concept to move along a branch we are currently working on the 
required rules but the comparison with centralized algorithm will 
become very difficult in particular since they will take into account 
criteria ignored by the centralized algorithm 
 pruning for ontologies building 
in section we presented the distributed clustering algorithm 
used in the dynamo system since this work was first based on this 
algorithm it introduced a clear bias toward binary trees as a result 
but we have to keep in mind that we are trying to obtain taxonomies 
which are more refined and concise although the head coverage 
rule is an improvement because it is based on how the ontologists 
generally work it only addresses low level structuring but not the 
intermediate levels of the tree 
by looking at figure it is clear that some pruning could be 
done in the taxonomy in particular since lésion moved 
 conceptagent could be removed it is not needed anymore 
moreover the branch starting with conceptagent clearly respects the 
constraint to make a binary tree but it would be more useful to the 
user in a more compact and meaningful form in this case 
 conceptagent and conceptagent could probably be merged 
currently our system has the necessary rules to create 
intermediate levels in the taxonomy or to have concepts shifting towards 
the leaf as we pointed it is not enough so new rules are needed to 
allow removing nodes from the tree or move them toward the root 
most of the work needed to develop those rules consists in finding 
the relevant statistic information that will support the ontologist 
 conclusion 
after being presented as a promising solution ensuring model 
quality and their terminological richness ontology building from 
textual corpus analysis is difficult and costly it requires analyst 
supervising and taking in account the ontology aim using 
natural languages processing tools ease the knowledge localization in 
texts through language uses that said those tools produce a huge 
amount of lexical or grammatical data which is not trivial to 
examine in order to define conceptual elements our contribution lies in 
this step of the modeling process from texts before any attempts to 
normalize or formalize the result 
we proposed an approach based on an adaptive multi-agent 
system to provide the ontologist with a first taxonomic structure of 
concepts our system makes use of a terminological network 
resulting from an analysis made by syntex the current state of our 
software allows to produce simple structures to propose them to 
the ontologist and to make them evolve depending on the 
modifications he made performances of the system are interesting and 
some aspects are even comparable to their centralized counterpart 
its strengths are mostly qualitative since it allows more subtle user 
interactions and a progressive adaptation to new linguistic based 
information 
from the point of view of ontology building this work is a first 
step showing the relevance of our approach it must continue both 
to ensure a better robustness during classification and to obtain 
richer structures semantic wise than simple trees from this 
improvements we are mostly focusing on the pruning to obtain better 
taxonomies we re currently working on the criterion to trigger 
the complementary actions of the structure changes applied by our 
clustering algorithm in other words this algorithm introduces 
inthe sixth intl joint conf on autonomous agents and multi-agent systems aamas 
termediate levels and we need to be able to remove them if 
necessary in order to reach a dynamic equilibrium 
also from the multi-agent engineering point of view their use 
in a dynamic ontology context has shown its relevance this 
dynamic ontologies can be seen as complex problem solving in such 
a case self-organization through cooperation has been an efficient 
solution and more generally it s likely to be interesting for other 
design related tasks even if we re focusing only on knowledge 
engineering in this paper of course our system still requires more 
evaluation and validation work to accurately determine the 
advantages and flaws of this approach we re planning to work on such 
benchmarking in the near future 
 references 
 h assadi construction of a regional ontology from text and 
its use within a documentary system proceedings of the 
international conference on formal ontology and 
information systems - fois pages - 
 n aussenac-gilles and d sörgel text analysis for ontology 
and terminology engineering journal of applied ontology 
 
 j bao and v honavar collaborative ontology building with 
wiki nt proceedings of the workshop on evaluation of 
ontology-based tools eon 
 c bernon v camps m -p gleizes and g picard 
agent-oriented methodologies chapter engineering 
self-adaptive multi-agent systems the adelfe 
methodology pages - idea group publishing 
 c brewster f ciravegna and y wilks background and 
foreground knowledge in dynamic ontology construction 
semantic web workshop sigir august 
 d faure and c nedellec a corpus-based conceptual 
clustering method for verb frames and ontology acquisition 
lrec workshop on adapting lexical and corpus resources to 
sublanguages and applications 
 f gandon ontology engineering a survey and a return on 
experience inria 
 j -p georgé g picard m -p gleizes and p glize living 
design for open computational systems th ieee 
international workshops on enabling technologies 
infrastructure for collaborative enterprises pages - 
june 
 m -p gleizes v camps and p glize a theory of emergent 
computation based on cooperative self-organization for 
adaptive artificial systems fourth european congress of 
systems science september 
 j heflin and j hendler dynamic ontologies on the web 
american association for artificial intelligence conference 
 
 s le moigno j charlet d bourigault and m -c jaulent 
terminology extraction from text to build an ontology in 
surgical intensive care proceedings of the amia 
annual symposium 
 k lister l sterling and k taveter reconciling 
ontological differences by assistant agents aamas 
may 
 a maedche ontology learning for the semantic web 
kluwer academic publisher 
 a maedche and s staab mining ontologies from text 
ekaw pages - 
 c d manning and h schütze foundations of statistical 
natural language processing the mit press cambridge 
massachusetts 
 h v d parunak r rohwer t c belding and 
s brueckner dynamic decentralized any-time hierarchical 
clustering th annual international acm sigir 
conference on research development on information 
retrieval august 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
organizational self-design in semi-dynamic environments 
sachin kamboj 
∗ 
and keith s decker 
department of computer and information sciences 
university of delaware 
newark de 
{kamboj decker} cis udel edu 
abstract 
organizations are an important basis for coordination in 
multiagent systems however there is no best way to organize and all 
ways of organizing are not equally effective attempting to 
optimize an organizational structure depends strongly on 
environmental features including problem characteristics available resources 
and agent capabilities if the environment is dynamic the 
environmental conditions or the problem task structure may change over 
time this precludes the use of static design-time generated 
organizational structures in such systems on the other hand for many 
real environments the problems are not totally unique either 
certain characteristics and conditions change slowly if at all and these 
can have an important effect in creating stable organizational 
structures 
organizational-self design osd has been proposed as an 
approach for constructing suitable organizational structures at 
runtime we extend the existing osd approach to include 
worthoriented domains model other resources in addition to only 
processor resources and build in robustness into the organization we 
then evaluate our approach against the contract-net approach and 
show that our osd agents perform better are more efficient and 
more flexible to changes in the environment 
categories and subject descriptors 
i distributed artificial intelligence multiagent systems 
general terms 
algorithms design performance experimentation 
 introduction 
in this paper we are primarily interested in the organizational 
design of a multiagent system - the roles enacted by the agents 
∗primary author is a student 
the coordination between the roles and the number and assignment 
of roles and resources to the individual agents the organizational 
design is complicated by the fact that there is no best way to 
organize and all ways of organizing are not equally effective 
instead the optimal organizational structure depends both on the 
problem at hand and the environmental conditions under which the 
problem needs to be solved the environmental conditions may 
not be known a priori or may change over time which would 
preclude the use of a static organizational structure on the other hand 
all problem instances and environmental conditions are not always 
unique which would render inefficient the use of a new bespoke 
organizational structure for every problem instance 
organizational self-design osd has been proposed 
as an approach to designing organizations at run-time in which 
the agents are responsible for generating their own organizational 
structures we believe that osd is especially suited to the above 
scenario in which the environment is semi-dynamic as the agents 
can adapt to changes in the task structures and environmental 
conditions while still being able to generate relatively stable 
organizational structures that exploit the common characteristics across 
problem instances 
in our approach as in we define two operators for osd 
- agent spawning and composition - when an agent becomes 
overloaded it spawns off a new agent to handle part of its task 
load responsibility when an agent lies idle for an extended period 
of time it may decide to compose with another agent 
we use tæms as the underlying representation for our 
problem solving requests tæms task analysis environment 
modeling and simulation is a computational framework for 
representing and reasoning about complex task environments in which 
tasks problems are represented using extended hierarchical task 
structures the root node of the task structure represents the 
high-level goal that the agent is trying to achieve the sub-nodes of 
a node represent the subtasks and methods that make up the 
highlevel task the leaf nodes are at the lowest level of abstraction and 
represent executable methods - the primitive actions that the agents 
can perform the executable methods themselves may have 
multiple outcomes with different probabilities and different 
characteristics such as quality cost and duration tæms also allows 
various mechanisms for specifying subtask variations and alternatives 
i e each node in tæms is labeled with a characteristic 
accumulation function that describes how many or which subgoals or sets of 
subgoals need to be achieved in order to achieve a particular 
higherlevel goal tæms has been used to model many different 
problemsolving environments including distributed sensor networks 
information gathering hospital scheduling ems and military planning 
 
the main contributions of this paper are as follows 
 we extend existing osd approaches to use tæms as the 
underlying problem representation which allows us to model 
and use osd for worth-oriented domains this in turn 
allows us to reason about alternative task and role 
assignments that make different quality cost tradeoffs and generate 
different organizational structures and uncertainties in the 
execution of tasks 
 we model the use of resources other than only processor 
resources 
 we incorporate robustness into the organizational structures 
 related work 
the concept of osd is not new and has been around since 
the work of corkill and lesser on the dvmt system even 
though the concept was not fully developed by them more 
recently dignum et al have described osd in the context of the 
reorganization of agent societies and attempt to classify the various 
kinds of reorganization possible according to the the reason for 
reorganization the type of reorganization and who is responsible for 
the reorganization decision according to their scheme the type of 
reorganization done by our agents falls into the category of 
structural changes and the reorganization decision can be described as 
shared command 
our research primarily builds on the work done by gasser and 
ishida in which they use osd in the context of a 
production system in order to perform adaptive work allocation and load 
balancing in their approach they define two organizational 
primitives - composition and decomposition which are similar to our 
organizational primitives for agent spawning and composition the 
main difference between their work and our work is that we use 
tæms as the underlying representation for our problems which 
allows firstly the representation of a larger more general class of 
problems and secondly quantitative reasoning over task structures 
the latter also allows us to incorporate different design-to-criteria 
schedulers 
horling and lesser present a different top-down approach to 
osd that also uses tæms as the underlying representation 
however their approach assumes a fixed number of agents with 
designated and fixed roles osd is used in their work to change the 
interaction patterns between the agents and results in the agents 
using different subtasks or different resources to achieve their goals 
we also extend on the work done by sycara et al on agent 
cloning which is another approach to resource allocation and load 
balancing in this approach the authors present agent cloning as 
a possible response to agent overload - if an agent detects that it 
is overloaded and that there are spare unused resources in the 
system the agent clones itself and gives its clone some part of its 
task load hence agent cloning can be thought of as akin to agent 
spawning in our approach however the two approaches are 
different in that there is no specialization of the agents in the 
formerthe cloned agents are perfect replicas of the original agents and 
fulfill the same roles and responsibilities as the original agents in our 
approach on the other hand the spawned agents are specialized on 
a subpart of the spawning agent s task structure which is no longer 
the responsibility of the spawning agent hence our approach also 
deals with explicit organization formation and the coordination of 
the agents tasks which are not handled by their approach 
other approaches to osd include the work of so and durfee 
 who describe a top-down model of osd in the context of 
cooperative distributive problem solving cdps and barber and 
martin who describe an adaptive decision making framework 
in which agents are able to reorganize decision-making groups by 
dynamically changing who makes the decisions for a particular 
goal and who must carry out these decisions the latter work is 
primarily concerned with coordination decisions and can be used 
to complement our osd work which primarily deals with task and 
resource allocation 
 task and resource model 
to ground our discussion of osd we now formally describe 
our task and resource model in our model the primary input to 
the multi-agent system mas is an ordered set of problem 
solving requests or task instances p p p pn where each 
problem solving request pi can be represented using the tuple 
 ti ai di in this scheme ti is the underlying tæms task 
structure ai ∈ n 
is the arrival time and di ∈ n 
is the deadline 
of the ith 
task instance 
 the mas has no prior knowledge about 
the task ti before the arrival time ai in order for the mas to 
accrue quality the task ti must be completed before the deadline 
di 
furthermore every underlying task structure ti can be 
represented using the tuple t τ m q e r ρ c where 
 t is the set of tasks the tasks are non-leaf nodes in a 
tæms task structure and are used to denote goals that the 
agents must achieve tasks have a characteristic 
accumulation function see below and are themselves composed of 
other subtasks and or methods that need to be achieved in 
order to achieve the goal represented by that task formally 
each task tj can be represented using the pair qj sj where 
qj ∈ q and sj ⊂ t ∪ m for our convenience we 
define two functions subtasks task t → p t ∪ m and 
supertasks tæms node t ∪ m → p t that return 
the subtasks and supertasks of a tæms node respectively 
 
 τ ∈ t is the root of the task structure i e the highest level 
goal that the organization is trying to achieve the quality 
accrued on a problem is equal to the quality of task τ 
 m is the set executable methods i e m 
{m m mn} where each method mk 
is represented using the outcome distribution 
{ o p o p om pm } in the pair ol pl 
ol is an outcome and pl is the probability that executing mk 
will result in the outcome ol furthermore each outcome 
ol is represented using the triple ql cl dl where ql is the 
quality distribution cl is the cost distribution and dl is the 
duration distribution of outcome ol each discrete 
distribution is itself a set of pairs { n p n p nn pn } 
where pi ∈ 
is the probability that the outcome will have 
a quality cost duration of nl ∈ n depending on the type of 
distribution and 
pm 
i pl 
 q is the set of quality characteristic accumulation functions 
 cafs the cafs determine how a task group accrues 
quality given the quality accrued by its subtasks methods for 
our research we use four cafs min max sum and 
exactly one see for formal definitions 
 e is the set of non-local effects again see for formal 
definitions 
 r is the set of resources 
 ρ is a mapping from an executable method and resource to 
the quantity of that resource needed by an agent to 
schedule execute that method that is ρ method resource 
m × r → n 
 
n is the set of natural numbers including zero and n 
is the set 
of positive natural numbers excluding zero 
 
p is the power set of set i e the set of all subsets of a set 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 c is a mapping from a resource to the cost of that resource 
that is c resource r → n 
we also make the following set of assumptions in our research 
 the agents in the mas are drawn from the infinite set a 
{a a a } that is we do not assume a fixed set of 
agents - instead agents are created spawned and destroyed 
 combined as needed 
 all problem solving requests have the same underlying task 
structure i e ∃t∀iti t where t is the task structure of 
the problem that the mas is trying to solve we believe that 
this assumption holds for many of the practical problems that 
we have in mind because tæms task structures are 
basically high-level plans for achieving some goal in which the 
steps required for achieving the goal-as well as the possible 
contingency situations-have been pre-computed offline and 
represented in the task structure because it represents many 
contingencies alternatives uncertain characteristics and 
runtime flexible choices the same underlying task structure 
can play out very differently across specific instances 
 all resources are exclusive i e only one agent may use a 
resource at any given time furthermore we assume that 
each agent has to own the set of resources that it 
needseven though the resource ownership can change during the 
evolution of the organization 
 all resources are non-consumable 
 organizational self design 
 agent roles and relationships 
the organizational structure is primarily composed of roles and 
the relationships between the roles one or more agents may enact 
a particular role and one or more roles must be enacted by every 
agent the roles may be thought of as the parts played by the agents 
enacting the roles in the solution to the problem and reflect the 
long-term commitments made by the agents in question to a certain 
course of action that includes task responsibility authority and 
mechanisms for coordination the relationships between the roles 
are the coordination relationships that exist between the subparts of 
a problem 
in our approach the organizational design is directly contingent 
on the task structure and the environmental conditions under which 
the problems need to be solved we define a role as a tæms 
subtree rooted at a particular node hence the set t ∪ m 
encompasses the space of all possible roles note by definition a role 
may consist of one or more other sub- roles as a particular tæms 
node may itself be made up of one or more subtrees hence we will 
use the terms role task node and task interchangeably 
we also differentiate between local and managed non-local 
roles local roles are roles that are the sole responsibility of a 
single agent that is the agent concerned is responsible for solving all 
the subproblems of the tree rooted at that node for such roles the 
agent concerned can do one or more subtasks solely at its 
discretion and without consultation with any other agent managed roles 
on the other hand must be coordinated between two or more agents 
as such roles will have two or more descendent local roles that are 
the responsibility of two or more separate agents any of the 
existing coordination mechanisms such as gpgp can be used to 
achieve this coordination 
formally if the function type agent tæms node a× t ∪ 
m → {local managed unassigned} returns the type of the 
responsibility of the agent towards the specified role then 
type a r local ⇐⇒ 
∀ri∈subtasks r type a ri local 
type a r managed ⇐⇒ 
 ∃a ∃r r ∈ subtasks r ∧ type a r managed ∨ 
 ∃a ∃a ∃r ∃r a a ∧ r r ∧ 
 r ∈ subtasks r ∧ r ∈ subtasks r ∧ 
 type a r local ∧ type a r local 
 organization formation and adaptation 
to form or adapt their organizational structure the agents use 
two organizational primitives agent spawning and composition 
these two primitives result in a change in the assignment of roles 
to the agents agent spawning is the generation of a new agent to 
handle a subset of the roles of the spawning agent agent 
composition on the other hand is orthogonal to agent spawning and 
involves the merging of two or more agents together - the 
combined agent is responsible for enacting all the roles of the agents 
being merged 
in order to participate in the formation and adaption of an 
organization the agents need to explicitly represent and reason about 
the role assignments hence as a part of its organizational 
knowledge each agent keeps a list of the local roles that it is enacting and 
the non-local roles that it is managing note that each agent only 
has limited organizational knowledge and is individually 
responsible for spawning off or combining with another agent as needed 
based on its estimate of its performance so far 
to see how the organizational primitives work we first describe 
four rules that can be thought of as the organizational invariants 
which will always hold before and after any organizational change 
 for a local role all the descendent nodes of that role will be 
local 
type a r local ⇒ 
∀ri∈subtasks r type a ri local 
 similarly for a managed non-local role all the ascendent 
nodes of that role will be managed 
type a r managed ⇒ 
∀ri∈supertasks r ∃ai ai ∈ a ∧ type ai ri managed 
 if two local roles that are enacted by two different agents 
share a common ancestor that ancestor will be a managed 
role 
 type a r local ∧ type a r local ∧ 
 a a ∧ r r ⇒ 
∀ri∈ supertasks r ∩supertasks r ∃ai ai ∈ a ∧ 
 type ai ri managed 
 if all the direct descendants of a role are local and the sole 
responsibility of a single agent that role will be a local role 
∃a∃r∀ri∈subtasks r a ∈ a ∧ r ∈ t ∪ m ∧ 
 type a ri local ⇒ 
 type a r local 
when a new agent is spawned the agent doing the spawning will 
assign one or more of its local roles to the newly spawned agent 
 algorithm to preserve invariant rules and the spawning 
agent will change the type of all the ascendent roles of the nodes 
assigned to the newly spawned agent from local to managed note 
that the spawning agent is only changing its local organizational 
knowledge and not the global organizational knowledge at the 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
same time the spawning agent is taking on the task of managing 
the previously local roles similarly the newly spawned agent will 
only know of its just assigned local roles 
when an agent the composing agent decides to compose with 
another agent the composed agent the organizational knowledge 
of the composing agent is merged with the organizational 
knowledge of the composed agent to do this the composed agent takes 
on the roles of all the local and managed tasks of the composing 
agent care is taken to preserve the organizational invariant rules 
and 
algorithm spawnagent spawningagent a → a 
 localroles ← {r ⊆ t ∪ m type spawningagent 
r local} 
 newagent ← createnewagent 
 newagentroles ← findrolesforspawnedagent 
 localroles 
 for role in newagentroles do 
 type newagent role ← local 
 type spawningagent role ← unassigned 
 preserveorganizationalinvariants 
 return newagent 
algorithm findrolesforspawnedagent 
 spawningagentroles t ∪ m → t ∪ m 
 r ← spawningagentroles 
 selectedroles ← nil 
 for roleset in p r − {φ r} do 
 if cost r roleset cost r selectedroles then 
 selectedroles ← roleset 
 return selectedroles 
algorithm getresourcecost roles t ∪ m → 
 m ← roles ∩ m 
 cost ← 
 for resource in r do 
 maxresourceusage ← 
 for method in m do 
 if ρ method resource maxresourceusage then 
 max ← ρ method resource 
 cost ← cost 
 c resource × maxresourceusage 
 return cost 
 role allocation during spawning 
one of the key questions that the agent doing the spawning needs 
to answer is - which of its local-roles should it assign to the newly 
spawned agent and which of its local roles should it keep to 
itself the onus of answering this question falls on the 
findrolesforspawnedagent function shown in algorithm above this 
function takes the set of local roles that are the responsibility of the 
spawning agent and returns a subset of those roles for allocation 
to the newly spawned agent this subset is selected based on the 
results of a cost function as is evident from line of the algorithm 
since the use of different cost functions will result in different 
organizational structures and since we have no a priori reason to believe 
that one cost function will out-perform the other we evaluated the 
performance of three different cost functions based on the 
following three different heuristics 
algorithm getexpectedduration roles t ∪ m → n 
 m ← roles ∩ m 
 exptduration ← 
 for outcome q c d outcomeprob in m do 
 exptoutcomeduration ← 
 for n p in d do 
 exptoutcomeduration ← n × p 
 exptduration ← exptduration 
 exptoutcomeduration × outcomeprob 
 return exptduration 
allocating top-most roles first this heuristic always breaks 
up at the top-most nodes first that is if the nodes of a task 
structure were numbered starting from the root in a breadth-first 
fashion then this heuristic would select the local-role of the spawning 
agent that had the lowest number and breakup that node by 
allocating one of its subtasks to the newly spawned agent we 
selected this heuristic because a it is the simplest to implement b 
fastest to run the role allocation can be done in constant time 
without the need of a search through the task structure and c it makes 
sense from a human-organizational perspective as this heuristic 
corresponds to dividing an organization along functional lines 
minimizing total resources this heuristic attempts to 
minimize the total cost of the resources needed by the agents in the 
organization to execute their roles if r be the local roles of the 
spawning agent and r be the subset of roles being evaluated for 
allocation to the newly spawned agent the cost function for this 
heuristic is given by cost r r ← getresourcecost r − 
r getresourcecost r 
balancing execution time this heuristic attempts to allocate 
roles in a way that tries to ensure that each agent has an equal 
amount of work to do for each potential role allocation this 
heuristic works by calculating the absolute value of the difference 
between the expected duration of its own roles after spawning and 
the expected duration of the roles of the newly spawned agent 
if this difference is close to zero then the both the agents have 
roughly the same amount of work to do formally if r be the 
local roles of the spawning agent and r be the subset of roles 
being evaluated for allocation to the newly spawned agent then 
the cost function for this heuristic is given by cost r r ← 
 getexpectedduration r−r −getexpectedduration r 
to evaluate these heuristics we ran a series of experiments that 
tested the performance of the resultant organization on randomly 
generated task structures the results are given in section 
 reasons for organizational change 
as organizational change is expensive requiring clock cycles 
allocation deallocation of resources etc we want a stable 
organizational structure that is suited to the task and environmental 
conditions at hand hence we wish to change the organizational 
structure only if the task structure and or environmental conditions 
change also to allow temporary changes to the environmental 
conditions to be overlooked we want the probability of an 
organizational change to be inversely proportional to the time since the last 
organizational change if this time is relatively short the agents are 
still adjusting to the changes in the environment - hence the 
probability of an agent initiating an organizational change should be 
high similarly if the time since the last organizational change is 
relatively large we wish to have a low probability of organizational 
change 
to allow this variation in probability of organizational change 
we use simulated annealing to determine the probability of 
keepthe sixth intl joint conf on autonomous agents and multi-agent systems aamas 
ing an existing organizational structure this probability is 
calculated using the annealing formula p e− δe 
kt where δe is the 
amount of overload underload t is the time since the last 
organizational change and k is a constant the mechanism of 
computing δe is different for agent spawning than for agent composition 
and is described below from this formula if t is large p or the 
probability of keeping the existing organizational structure is large 
note that the value of p is capped at a certain threshold in order to 
prevent the organization from being too sluggish in its reaction to 
environmental change 
to compute if agent spawning is necessary we use the annealing 
equation with δe 
α∗slack 
where α is a constant and slack is 
the difference between the total time available for completion of 
the outstanding tasks and the sum of the expected time required for 
completion of each task on the task queue also if the amount of 
slack is negative immediate agent spawning will occur without use 
of the annealing equation 
to calculate if agent composition is necessary we again use the 
simulated annealing equation however in this case δe β ∗ 
idle time where β is a constant and idle time is the amount 
of time for which the agent was idle if the agent has been sitting 
idle for a long period of time δe is large which implies that p 
the probability of keeping the existing organizational structure is 
low 
 organization and robustness 
there are two approaches commonly used to achieve robustness 
in multiagent systems 
 the survivalist approach which involves replicating 
domain agents in order to allow the replicas to take over should 
the original agents fail and 
 the citizen approach which involves the use of special 
monitoring agents called sentinel agents in order to detect 
agent failure and dynamically startup new agents in lieu of 
the failed ones 
the advantage of the survivalist approach is that recovery is 
relatively fast since the replicas are pre-existing in the organization 
and can take over as soon as a failure is detected the advantages 
of the citizen approach are that it requires fewer resources little 
modification to the existing organizational structure and 
coordination protocol and is simpler to implement 
both of these approaches can be applied to achieve robustness in 
our osd agents and it is not clear which approach would be better 
rather a thorough empirical evaluation of both approaches would 
be required in this paper we present the citizen approach as it has 
been shown by to have a better performance than the survivalist 
approach in the contract net protocol and leave the presentation 
and evaluation of the survivalist approach to a future paper 
to implement the citizen approach we designed special 
monitoring agents that periodically poll the domain agents by sending 
them are you alive messages that the agents must respond to if 
an agent fails it will not respond to such messages - the 
monitoring agents can then create a new agent and delegate the 
responsibilities of the dead agent to the new agent 
this delegation of responsibilities is non-trivial as the 
monitoring agents do not have access to the internal state of the domain 
agents which is itself composed of two components - the 
organizational knowledge and the task information the former consists 
of the information about the local and managerial roles of the agent 
while the latter is composed of the methods that are being 
scheduled and executed and the tasks that have been delegated to other 
agents this state information can only be deduced by monitoring 
and recording the messages being sent and received by the domain 
agents for example in order to deduce the organizational 
knowledge the monitoring agents need to keep a track of the spawn and 
compose messages sent by the agents in order to trigger the 
spawning and composition operations respectively the deduction 
process is particularly complicated in the case of the task information 
as the monitoring agents do not have access to the private 
schedules of the domain agents the details are beyond the scope of this 
paper 
 evaluation 
to evaluate our approach we ran a series of experiments that 
simulated the operation of both the osd agents and the contract 
net agents on various task structures with varied arrival rates and 
deadlines at the start of each experiment a random tæms task 
structure was generated with a specified depth and branching 
factor during the course of the experiment a series of task instances 
 problems arrive at the organization and must be completed by the 
agents before their specified deadlines 
to directly compare the osd approach with the contract net 
approach each experiment was repeated several times - using osd 
agents on the first run and a different number of contract net agents 
on each subsequent run we were careful to use the same task 
structure task arrival times task deadlines and random numbers for each 
of these trials 
we divided the experiments into two groups experiments in 
which the environment was static fixed task arrival rates and 
deadlines and experiments in which the environment was dynamic 
 varying arrival rates and or deadlines 
the two graphs in figure show the average performance of the 
osd organization against the contract net organizations with 
 and agents the results shown are the averages of running 
 experiments of those experiments had a static environment 
with a fixed task arrival time of cycles and a deadline window of 
 cycles the remaining experiments had a varying task arrival 
rate - the task arrival rate was changed from cycles to cycles 
and back to cycles after every tasks in all the experiments 
the task structures were randomly generated with a maximum depth 
of and a maximum branching factor of the runtime of all the 
experiments was cycles 
we tested several hypotheses relating to the comparative 
performance of our osd approach using the wilcoxon matched-pair 
signed-rank tests matched-pair signifies that we are comparing 
the performance of each system on precisely the same randomized 
task set within each separate experiment the tested hypothesis are 
the osd organization requires fewer agents to complete an 
equal or larger number of tasks when compared to the 
contract net organization to test this hypothesis we tested the 
stronger null hypothesis that states that the contract net agents 
complete more tasks this null hypothesis is rejected for all contract 
net organizations with less than agents static p 
dynamic p for large contract net organizations the number 
of tasks completed is statistically equivalent to the number 
completed by the osd agents however the number of agents used by 
the osd organization is smaller agents in the static case and 
 agents in the dynamic case versus contract net agents 
 
thus the original hypothesis that osd requires fewer agents to 
 
these values should not be construed as an indication of the 
scalability of our approach we have tested our approach on 
organizations with more than agents which is significantly greater than 
the number of agents needed for the kind of applications that we 
have in mind i e web service choreography efficient dynamic use 
of grid computing distributed information gathering etc 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
figure graph comparing the average performance of the 
osd organization with the contract net organizations with 
 and agents the error bars show the standard 
deviations 
complete an equal or larger number of tasks is upheld 
the osd organizations achieve an equal or greater average 
quality than the contract net organizations the null 
hypothesis is that the contract net agents achieve a greater average quality 
we can reject the null hypothesis for contract net organizations with 
less than agents static p dynamic p for 
larger contract net organizations the average quality is statistically 
equivalent to that achieved by osd 
the osd agents have a lower average response time as 
compared to the contract net agents the null hypothesis that osd 
has the same or higher response time is rejected for all contract net 
organizations static p dynamic p 
the osd agents send less messages than the contract net 
agents the null hypothesis that osd sends the same or more 
messages is rejected for all contract net organizations p 
in all cases except contract net agents in a static environment 
where p 
hence as demonstrated by the above tests our agents perform 
better than the contract net agents as they complete a larger number 
of tasks achieve a greater quality and also have a lower response 
time and communication overhead these results make intuitive 
sense given our goals for the osd approach we expected the osd 
organizations to have a faster average response time and to send 
less messages because the agents in the osd organization are not 
wasting time and messages sending bid requests and replying to 
bids the quality gained on the tasks is directly dependent on the 
criteria heuristic bet tf mr rand 
number of agents 
no-org-changes 
total-messages-sent 
resource-cost 
tasks-completed 
average-quality 
average-response-time 
average-runtime 
average-turnaround-time 
table the number of times that each heuristic performed 
the best or statistically equivalent to the best for each of the 
performance criteria heuristic key bet is balancing 
execution time tf is topmost first mr is minimizing resources and 
rand is a random allocation strategy in which every tæms 
node has a uniform probability of being selected for allocation 
number of tasks completed hence the more the number of tasks 
completed the greater average quality the results of testing the 
first hypothesis were slightly more surprising it appears that due 
to the inherent inefficiency of the contract net protocol in bidding 
for each and every task instance a greater number of agents are 
needed to complete an equal number of tasks 
next we evaluated the performance of the three heuristics for 
allocating tasks some preliminary experiments that are not reported 
here due to space constraints demonstrated the lack of a clear 
winner amongst the three heuristics for most of the performance 
criteria that we evaluated we suspected this to be the case because 
different heuristics are better for different task structures and 
environmental conditions and since each experiment starts with a different 
random task structure we couldn t find one allocation strategy that 
always dominated the other for all the performance criteria 
to determine which heuristic performs the best given a set of 
task structures environmental conditions and performance criteria 
we performed a series of experiments that were controlled using 
the following five variables 
 the depth of the task structure was varied from to 
 the branching factor was varied from to 
 the probability of any given task node having a min caf 
was varied from to in increments of the 
probability of any node having a sum caf was in turn modified 
to ensure that the probabilities add up to 
 
 the arrival rate from to cycles in increments of 
 the deadline slack from to in increments of 
each experiment was repeated times with a new task 
structure being generated each time - these experiments formed an 
experimental set hence all the experiments in an experimental set 
had the same values for the exogenous variables that were used to 
control the experiment note that a static environment was used in 
each of these experiments as we wanted to see the performance of 
the arrival rate and deadline slack on each of the three heuristics 
also the results of any experiment in which the osd organization 
consisted of a single agent ware culled from the results similarly 
 
since our preliminary analysis led is to believe that the number 
of max and exactly one cafs in a task structure have a 
minimal effect on the performance of the allocation strategies being 
evaluated we set the probabilities of the max and exactly one 
cafs to in order to reduce the combinatorial explosion of the full 
factorial experimental design 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
experiments in which the generated task structures were 
unsatisfiable given the deadline constraints were removed from the final 
results if any experimental set had more than experiments thus 
removed the whole set was ignored for performing the evaluation 
the final evaluation was done on experimental sets 
we tested the potential of these three heuristics on the following 
performance criteria 
 the average number of agents used 
 the total number of organizational changes 
 the total messages sent by all the agents 
 the total resource cost of the organization 
 the number of tasks completed 
 the average quality accrued the average quality is defined 
as the total quality accrued during the experimental run 
divided by the sum of the number of tasks completed and the 
number of tasks failed 
 the average response time of the organization the response 
time of a task is defined as the difference between the time 
at which any agent in the organization starts working on 
the task the start time and the time at which the task was 
generated the generation time hence the response time 
is equivalent to the wait time for tasks that are never 
attempted started the response time is set at final runtime 
minus the generation time 
 the average runtime of the tasks attempted by the 
organization this time is defined as the difference between the time 
at which the task completed or failed and the start time for 
tasks that were never stated this time is set to zero 
 the turnaround time is defined as the sum of the response 
time and runtime of a task 
except for the number of tasks completed and the average 
quality accrued lower values for the various performance criteria 
indicate better performance again we ran the wilcoxon matched-pair 
signed-rank tests on the experiments in each of the experimental 
sets the null hypothesis in each case was that there is no 
difference between the pair of heuristics for the performance criteria 
under consideration we were interested in the cases in which we 
could reject the null hypothesis with confidence p 
we noted the number of times that a heuristic performed the best 
or was in a group that performed statistically better than the rest 
these counts are given in tables and 
the number of experimental sets in which each heuristic 
performed the best or statistically equivalent to the best is shown in 
table the breakup of these numbers into the number of times 
that each heuristic performed better than all the other heuristics and 
 the number of times each heuristic was statistically equivalent 
to another group of heuristics all of which performed the best is 
shown in table both of these tables allow us to glean important 
information about the performance of the three heuristics 
particularly interesting were the following results 
 whereas balancing execution time bet used the 
lowest number of agents in largest number of experimental sets 
 in most of these cases experimental sets it was 
statistically equivalent to topmost first tf when these 
two heuristics didn t perform equally there was an almost 
even split between the number of experimental sets in which 
one outperformed the other 
we believe this was the case because bet always bifurcates 
the agents into two agents that have a more or less equal task 
load this often results in organizations that have an even 
figure graph demonstrating the robustness of the citizen 
approach the baseline shows the number of tasks completed 
in the absence of any failure 
number of agents - none of which are small 
enough to 
combine into a larger agent with tf on the other hand a 
large agent can successively spawn off smaller agents until it 
and the spawned agents are small enough to complete their 
tasks before the deadlines - this often results in 
organizations with an odd number of agents that is less than those 
used by bet 
 as expected bet achieved the lowest number of 
organizational changes in the largest number of experimental sets in 
fact it was over ten times as good as its second best 
competitor tf this shows that if the agents are conscientious 
in their initial task allocation there is a lesser need for 
organizational change later on especially for static environments 
 a particularly interesting yet easily explainable result was 
that of the average response time we found that the 
minimizing resources mr heuristic performed the best when it 
came to minimizing the average response time this can be 
explained by the fact the mr heuristic is extremely greedy 
and prefers to spawn off small agents that have a tiny 
resource footprint so as to minimize the total increase in the 
resource cost to the organization at the time of spawning 
whereas most of these small agents might compose with 
other agents over time the presence of a single small agent 
is sufficient to reduce the response time 
in fact the mr heuristic is not the most effective heuristic 
when it comes to minimizing the resource-cost of the 
organization - in fact it only outperforms a random task resource 
allocation we believe this is in part due to the greedy 
nature of this heuristic and in part because of the fact that all 
spawning and composition operations only use local 
information we believe that using some non-local information 
about the resource allocation might help in making better 
decisions something that we plan to look at in the future 
finally we evaluated the performance of the citizens approach to 
robustness as applied to our osd mechanism figure as 
expected as the probability of failure increases the number of agents 
failing during a run also increases this results in a slight decrease 
in the number of tasks completed which can be explained by the 
fact that whenever an agent fails its looses whatever work it was 
doing at the time the newly created agent that fills in for the failed 
 
for this discussion small agents are agents that have a low 
expected duration for their local roles as calculated by algorithm 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
criteria heuristic bet tf mr rand bet tf bet rand mr rand tf mr bet tf mr all 
number of agents 
no-org-changes 
total-messages-sent 
resource-cost 
tasks-completed 
average-quality 
average-response-time 
average-runtime 
average-turnaround-time 
table table showing the number of times that each individual heuristic performed the best and the number of times that a certain 
group of statistically equivalent heuristics performed the best only the more interesting heuristic groupings are shown all shows 
the number of experimental sets in which there was no statistical difference between the three heuristics and a random allocation 
strategy 
one must redo the work thus wasting precious time which might 
not be available close to a deadline 
as a part of our future research we wish to firstly evaluate the 
survivalist approach to robustness the survivalist approach might 
actually be better than the citizen approach for higher 
probabilities of agent failure as the replicated agents may be processing the 
task structures in parallel and can take over the moment the 
original agents fail - thus saving time around tight deadlines also 
we strongly believe that the optimal organizational structure may 
vary depending on the probability of failure and the desired level 
of robustness for example one way of achieving a higher level 
of robustness in the survivalist approach given a large numbers of 
agent failures would be to relax the task deadlines however such 
a relaxation would result in the system using fewer agents in order 
to conserve resources which in turn would have a detrimental 
effect on the robustness therefore towards this end we have begun 
exploring the robustness properties of task structures and the ways 
in which the organizational design can be modified to take such 
properties into account 
 conclusion 
in this paper we have presented a run-time approach to 
organization in which the agents use organizational self-design to come up 
with a suitable organizational structure we have also evaluated the 
performance of the organizations generated by the agents following 
our approach with the bespoke organization formation that takes 
place in the contract net protocol and have demonstrated that our 
approach is better than the contract net approach as evident by the 
larger number of tasks completed larger quality achieved and lower 
response time finally we tested the performance of three different 
resource allocation heuristics on various performance metrics and 
also evaluated the robustness of our approach 
 references 
 k s barber and c e martin dynamic reorganization of 
decision-making groups in agents pages - 
new york ny usa 
 k m carley and l gasser computational organization 
theory in g wiess editor multiagent systems a modern 
approach to distributed artificial intelligence pages 
 - mit press 
 w chen and k s decker the analysis of coordination in 
an information system application - emergency medical 
services in lecture notes in computer science lncs 
number pages - springer-verlag may 
 d corkill and v lesser the use of meta-level control for 
coordination in a distributed problem solving network 
proceedings of the eighth international joint conference on 
artificial intelligence pages - august 
 k s decker environment centered analysis and design of 
coordination mechanisms ph d thesis dept of comp 
science university of massachusetts amherst may 
 k s decker and j li coordinating mutually exclusive 
resources using gpgp autonomous agents and multi-agent 
systems - 
 c dellarocas and m klein an experimental evaluation of 
domain-independent fault handling services in open 
multi-agent systems proceedings of the international 
conference on multi-agent systems icmas- july 
 
 v dignum f dignum and l sonenberg towards dynamic 
reorganization of agent societies in proceedings of ceas 
workshop on coordination in emergent agent societies at 
ecai pages - valencia spain september 
 b horling b benyo and v lesser using self-diagnosis to 
adapt organizational structures in agents pages 
 - new york ny usa acm press 
 t ishida l gasser and m yokoo organization self-design 
of distributed production systems ieee transactions on 
knowledge and data engineering - 
 v r lesser et al evolution of the gpgp tæms 
domain-independent coordination framework autonomous 
agents and multi-agent systems - - 
 o marin p sens j briot and z guessoum towards 
adaptive fault tolerance for distributed multi-agent systems 
proceedings of ersads may 
 o shehory k sycara et al agent cloning an approach to 
agent mobility and resource allocation ieee 
communications magazine - 
 y so and e durfee an organizational self-design model for 
organizational change in aaai- workshop on ai and 
theories of groups and organizations pages - 
washington d c july 
 t wagner coordination decision support assistants 
 coordinators technical report - baa 
 t wagner and v lesser design-to-criteria scheduling 
real-time agent control proc of aaai spring 
symposium on real-time autonomous systems - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
negotiation by abduction and relaxation 
chiaki sakama 
dept computer and communication sciences 
wakayama university 
sakaedani wakayama japan 
sakama sys wakayama-u ac jp 
katsumi inoue 
national institute of informatics 
 - - hitotsubashi chiyoda-ku 
tokyo japan 
ki nii ac jp 
abstract 
this paper studies a logical framework for automated 
negotiation between two agents we suppose an agent who has 
a knowledge base represented by a logic program then 
we introduce methods of constructing counter-proposals in 
response to proposals made by an agent to this end we 
combine the techniques of extended abduction in artificial 
intelligence and relaxation in cooperative query answering 
for databases these techniques are respectively used for 
producing conditional proposals and neighborhood proposals 
in the process of negotiation we provide a negotiation 
protocol based on the exchange of these proposals and develop 
procedures for computing new proposals 
categories and subject descriptors 
f mathematical logic logic and constraint 
programming i distributed artificial intelligence 
multiagent systems 
general terms 
theory 
 introduction 
automated negotiation has been received increasing 
attention in multi-agent systems and a number of frameworks 
have been proposed in different contexts 
 for instance negotiation usually proceeds in a 
series of rounds and each agent makes a proposal at every 
round an agent that received a proposal responds in two 
ways one is a critique which is a remark as to whether 
or not parts of the proposal is accepted the other is a 
counter-proposal which is an alternative proposal made in 
response to a previous proposal 
to see these proposals in one-to-one negotiation suppose 
the following negotiation dialogue between a buyer agent b 
and a seller agent s bi or si represents an utterance of 
b or s in the i-th round 
b i want to buy a personal computer of the brand b 
with the specification of cpu ghz memory mb 
hdd gb and a dvd-rw driver i want to get it 
at the price under usd 
s we can provide a pc with the requested specification 
if you pay for it by cash in this case however service 
points are not added for this special discount 
b i cannot pay it by cash 
s in a normal price the requested pc costs usd 
b i cannot accept the price my budget is under 
usd 
s we can provide another computer with the requested 
specification except that it is made by the brand b 
the price is exactly usd 
b i do not want a pc of the brand b instead i can 
downgrade a driver from dvd-rw to cd-rw in my 
initial proposal 
s ok i accept your offer 
in this dialogue in response to the opening proposal b 
the counter-proposal s is returned in the rest of the 
dialogue b b s are critiques while s s b are 
counterproposals 
critiques are produced by evaluating a proposal in a 
knowledge base of an agent in contrast making counter-proposals 
involves generating an alternative proposal which is more 
favorable to the responding agent than the original one 
it is known that there are two ways of producing 
counterproposals extending the initial proposal or amending part 
of the initial proposal according to the first type 
appears in the dialogue a i propose that you provide me 
with service x b i propose that i provide you with 
service x if you provide me with service z the second type 
is in the dialogue a i propose that i provide you with 
service y if you provide me with service x b i propose 
that i provide you with service x if you provide me with 
service z a negotiation proceeds by iterating such 
give-andtake dialogues until it reaches an agreement disagreement 
in those dialogues agents generate counter- proposals by 
reasoning on their own goals or objectives the objective 
of the agent a in the above dialogues is to obtain service 
x the agent b proposes conditions to provide the 
service in the process of negotiation however it may happen 
that agents are obliged to weaken or change their initial 
goals to reach a negotiated compromise in the dialogue of 
 
 - - - - rps c ifaamas 
a buyer agent and a seller agent presented above a buyer 
agent changes its initial goal by downgrading a driver from 
dvd-rw to cd-rw such behavior is usually represented 
as specific meta-knowledge of an agent or specified as 
negotiation protocols in particular problems currently there is 
no computational logic for automated negotiation which has 
general inference rules for producing counter- proposals 
the purpose of this paper is to mechanize a process of 
building counter- proposals in one-to-one negotiation 
dialogues we suppose an agent who has a knowledge base 
represented by a logic program we then introduce 
methods for generating three different types of proposals first 
we use the technique of extended abduction in artificial 
intelligence to construct a conditional proposal as an 
extension of the original one second we use the technique 
of relaxation in cooperative query answering for databases 
 to construct a neighborhood proposal as an amendment 
of the original one third combining extended abduction 
and relaxation conditional neighborhood proposals are 
constructed as amended extensions of the original proposal we 
develop a negotiation protocol between two agents based on 
the exchange of these counter-proposals and critiques we 
also provide procedures for computing proposals in logic 
programming 
this paper is organized as follows section introduces 
a logical framework used in this paper section presents 
methods for constructing proposals and provides a 
negotiation protocol section provides methods for computing 
proposals in logic programming section discusses related 
works and section concludes the paper 
 preliminaries 
logic programs considered in this paper are extended 
disjunctive programs edp an edp or simply a program 
is a set of rules of the form 
l · · · ll ← ll lm not lm not ln 
 n ≥ m ≥ l ≥ where each li is a positive negative 
literal i e a or ¬a for an atom a and not is negation as 
failure naf not l is called an naf-literal the symbol 
 represents disjunction the left-hand side of the rule 
is the head and the right-hand side is the body for each 
rule r of the above form head r body 
 r and body− 
 r 
denote the sets of literals {l ll} {ll lm} and 
{lm ln} respectively also not body− 
 r denotes 
the set of naf-literals {not lm not ln} a 
disjunction of literals and a conjunction of naf- literals in a rule 
are identified with its corresponding sets of literals a rule 
r is often written as head r ← body 
 r not body− 
 r or 
head r ← body r where body r body 
 r ∪not body− 
 r 
a rule r is disjunctive if head r contains more than one 
literal a rule r is an integrity constraint if head r ∅ and 
r is a fact if body r ∅ a program is naf-free if no 
rule contains naf-literals two rules literals are identified 
with respect to variable renaming a substitution is a 
mapping from variables to terms θ {x t xn tn} where 
x xn are distinct variables and each ti is a term 
distinct from xi given a conjunction g of naf- literals gθ 
denotes the conjunction obtained by applying θ to g a 
program rule or literal is ground if it contains no variable 
a program p with variables is a shorthand of its ground 
instantiation ground p the set of ground rules obtained 
from p by substituting variables in p by elements of its 
herbrand universe in every possible way 
the semantics of an edp is defined by the answer set 
semantics let lit be the set of all ground literals in 
the language of a program suppose a program p and a 
set of literals s ⊆ lit then the reduct p s 
is the 
program which contains the ground rule head r ← body 
 r 
iff there is a rule r in ground p such that body− 
 r ∩s ∅ 
given an naf-free edp p cn p denotes the smallest set 
of ground literals which is i closed under p i e for 
every ground rule r in ground p body r ⊆ cn p implies 
head r ∩ cn p ∅ and ii logically closed i e it is 
either consistent or equal to lit given an edp p and a set 
s of literals s is an answer set of p if s cn p s 
 a 
program has none one or multiple answer sets in general 
an answer set is consistent if it is not lit a program p is 
consistent if it has a consistent answer set otherwise p is 
inconsistent 
abductive logic programming introduces a mechanism 
of hypothetical reasoning to logic programming an 
abductive framework used in this paper is the extended 
abduction introduced by inoue and sakama an abductive 
program is a pair p h where p is an edp and h is 
a set of literals called abducibles when a literal l ∈ h 
contains variables any instance of l is also an abducible 
an abductive program p h is consistent if p is 
consistent throughout the paper abductive programs are 
assumed to be consistent unless stated otherwise let g 
l lm not lm not ln be a conjunction where 
all variables in g are existentially quantified at the front and 
range-restricted i e every variable in lm ln appears 
in l lm a set s of ground literals satisfies the 
conjunction g if { l θ lmθ } ⊆ s and { lm θ lnθ }∩ 
s ∅ for some ground instance gθ with a substitution θ 
let p h be an abductive program and g a conjunction 
as above a pair e f is an explanation of an observation 
g in p h if 
 p \ f ∪ e has an answer set which satisfies g 
 p \ f ∪ e is consistent 
 e and f are sets of ground literals such that e ⊆ h\p 
and f ⊆ h ∩ p 
when p \ f ∪ e has an answer set s satisfying the above 
three conditions s is called a belief set of an abductive 
program p h satisfying g with respect to e f note 
that if p has a consistent answer set s satisfying g s 
is also a belief set of p h satisfying g with respect to 
 e f ∅ ∅ extended abduction introduces removes 
hypotheses to from a program to explain an observation 
note that normal abduction as in considers only 
introducing hypotheses to explain an observation an 
explanation e f of an observation g is called minimal if for 
any explanation e f of g e ⊆ e and f ⊆ f imply 
e e and f f 
example consider the abductive program p h 
p flies x ← bird x not ab x 
ab x ← broken-wing x 
bird tweety ← bird opus ← 
broken-wing tweety ← 
h broken-wing x 
the observation g flies tweety has the minimal 
explanation e f ∅ {broken-wing tweety } 
 
this defines credulous explanations skeptical 
explanations are used in 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 negotiation 
 conditional proposals by abduction 
we suppose an agent who has a knowledge base 
represented by an abductive program p h a program p 
consists of two types of knowledge belief b and desire d 
where b represents objective knowledge of an agent while 
d represents subjective knowledge in general we define 
p b ∪ d but do not distinguish b and d if such 
distinction is not important in the context in contrast abducibles 
h are used for representing permissible conditions to make 
a compromise in the process of negotiation 
definition a proposal g is a conjunction of literals 
and naf-literals 
l lm not lm not ln 
where every variable in g is existentially quantified at the 
front and range-restricted in particular g is called a 
critique if g accept or g reject where accept and reject 
are the reserved propositions a counter-proposal is a 
proposal made in response to a proposal 
definition a proposal g is accepted in an 
abductive program p h if p has an answer set satisfying g 
when a proposal is not accepted abduction is used for 
seeking conditions to make it acceptable 
definition let p h be an abductive program 
and g a proposal if e f is a minimal explanation of 
gθ for some substitution θ in p h the conjunction g 
gθ e not f 
is called a conditional proposal for g where e not f 
represents the conjunction a ak not ak not al 
for e {a ak} and f { ak al } 
proposition let p h be an abductive program 
and g a proposal if g is a conditional proposal there is a 
belief set s of p h satisfying g 
proof when g gθ e not f p \ f ∪ e has a 
consistent answer set s satisfying gθ and e ∩ f ∅ in 
this case s satisfies gθ e not f 
a conditional proposal g provides a minimal requirement 
for accepting the proposal g if gθ has multiple minimal 
explanations several conditional proposals exist accordingly 
when e f ∅ ∅ a conditional proposal is used as a 
new proposal made in response to the proposal g 
example an agent seeks a position of a research 
assistant at the computer department of a university with 
the condition that the salary is at least usd per year 
the agent makes his her request as the proposal 
g assist compt dept salary x x ≥ 
the university has the abductive program p h 
p salary ← assist compt dept not has phd 
salary ← assist compt dept has phd 
salary ← assist math dept 
salary ← system admin compt dept 
 
for notational convenience we often include mathematical 
 in equations in proposals programs they are written by 
literals for instance x ≥ y by geq x y with a suitable 
definition of the predicate geq 
employee x ← assist x 
employee x ← system admin x 
assist compt dept assist math dept 
 system admin compt dept ← 
h has phd 
where available positions are represented by disjunction 
according to p the base salary of a research assistant at the 
computer department is usd but if he she has phd 
it is usd in this case e f {has phd} ∅ 
becomes the minimal explanation of gθ assist compt dept 
salary with θ { x } then the 
conditional proposal made by the university becomes 
assist compt dept salary has phd 
 neighborhood proposals by relaxation 
when a proposal is unacceptable an agent tries to 
construct a new counter-proposal by weakening constraints in 
the initial proposal we use techniques of relaxation for 
this purpose relaxation is used as a technique of 
cooperative query answering in databases when an original 
query fails in a database relaxation expands the scope of 
the query by relaxing the constraints in the query this 
allows the database to return neighborhood answers which 
are related to the original query we use the technique for 
producing proposals in the process of negotiation 
definition let p h be an abductive program 
and g a proposal then g is relaxed to g in the following 
three ways 
anti-instantiation construct g such that g θ g for 
some substitution θ 
dropping conditions construct g such that g ⊂ g 
goal replacement if g is a conjunction g g where 
g and g are conjunctions and there is a rule l ← 
g in p such that g θ g for some substitution θ 
then build g as lθ g here lθ is called a replaced 
literal 
in each case every variable in g is existentially quantified 
at the front and range-restricted 
anti-instantiation replaces constants or terms with fresh 
variables dropping conditions eliminates some conditions 
in a proposal goal replacement replaces the condition g 
in g with a literal lθ in the presence of a rule l ← g in p 
under the condition g θ g all these operations 
generalize proposals in different ways each g obtained by these 
operations is called a relaxation of g it is worth noting 
that these operations are also used in the context of 
inductive generalization the relaxed proposal can produce 
new offers which are neighbor to the original proposal 
definition let p h be an abductive program 
and g a proposal 
 let g be a proposal obtained by anti-instantiation if 
p has an answer set s which satisfies g θ for some 
substitution θ and g θ g g θ is called a neighborhood 
proposal by anti-instantiation 
 let g be a proposal obtained by dropping conditions 
if p has an answer set s which satisfies g θ for some 
substitution θ g θ is called a neighborhood proposal by 
dropping conditions 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 let g be a proposal obtained by goal replacement 
for a replaced literal l ∈ g and a rule h ← b in 
p such that l hσ and g \ {l} ∪ bσ g for 
some substitution σ put g g \ {l} ∪ bσ if 
p has an answer set s which satisfies g θ for some 
substitution θ g θ is called a neighborhood proposal 
by goal replacement 
example cont example given the proposal 
g assist compt dept salary x x ≥ 
 g assist w salary x x ≥ is produced by 
substituting compt dept with a variable w as 
g θ assist math dept salary 
with θ { w math dept } is satisfied by an answer 
set of p g θ becomes a neighborhood proposal by 
anti-instantiation 
 g assist compt dept salary x is produced by 
dropping the salary condition x ≥ as 
g θ assist compt dept salary 
with θ { x } is satisfied by an answer set of 
p g θ becomes a neighborhood proposal by 
dropping conditions 
 g employee compt dept salary x x ≥ is 
produced by replacing assist compt dept with 
employee compt dept using the rule employee x ← 
assist x in p by g and the rule employee x ← 
system admin x in p g sys admin compt dept 
salary x x ≥ is produced as 
g θ sys admin compt dept salary 
with θ { x } is satisfied by an answer set 
of p g θ becomes a neighborhood proposal by goal 
replacement 
finally extended abduction and relaxation are combined to 
produce conditional neighborhood proposals 
definition let p h be an abductive program 
and g a proposal 
 let g be a proposal obtained by either anti-instantiation 
or dropping conditions if e f is a minimal 
explanation of g θ g for some substitution θ the 
conjunction g θ e not f is called a conditional neighborhood 
proposal by anti-instantiation dropping conditions 
 let g be a proposal obtained by goal replacement 
suppose g as in definition if e f is a 
minimal explanation of g θ for some substitution θ 
the conjunction g θ e not f is called a conditional 
neighborhood proposal by goal replacement 
a conditional neighborhood proposal reduces to a 
neighborhood proposal when e f ∅ ∅ 
 negotiation protocol 
a negotiation protocol defines how to exchange proposals 
in the process of negotiation this section presents a 
negotiation protocol in our framework we suppose one-to-one 
negotiation between two agents who have a common 
ontology and the same language for successful communication 
definition a proposal l lm not lm not ln 
violates an integrity constraint ← body 
 r not body− 
 r if 
for any substitution θ there is a substitution σ such that 
body 
 r σ ⊆ { l θ lmθ } body− 
 r σ∩{ l θ lmθ } 
∅ and body− 
 r σ ⊆ { lm θ lnθ } 
integrity constraints are conditions which an agent should 
satisfy so that they are used to explain why an agent does 
not accept a proposal 
a negotiation proceeds in a series of rounds each i-th 
round i ≥ consists of a proposal gi 
 made by one agent 
ag and another proposal gi 
 made by the other agent ag 
definition let p h be an abductive program 
of an agent ag and gi 
 a proposal made by ag at the i-th 
round a critique set of ag at the i-th round is a set 
csi 
 p gj 
 csi− 
 p gj− 
 ∪ { r r is an integrity 
constraint in p and gj 
 violates r } 
where j i − or i and cs 
 p g 
 cs 
 p g 
 ∅ 
a critique set of an agent ag accumulates integrity 
constraints which are violated by proposals made by another 
agent ag csi 
 p gj 
 is defined in the same manner 
definition let pk hk be an abductive program 
of an agent agk and gj 
a proposal which is not a critique 
made by any agent at the j ≤ i -th round a negotiation set 
of agk at the i-th round is a triple nsi 
k si 
c si 
n si 
cn 
where si 
c is the set of conditional proposals si 
n is the set 
of neighborhood proposals and si 
cn is the set of conditional 
neighborhood proposals produced by gj 
and pk hk 
a negotiation set represents the space of possible proposals 
made by an agent si 
x x ∈ {c n cn} accumulates proposals 
produced by gj 
 ≤ j ≤ i according to definitions 
and note that an agent can construct counter-proposals 
by modifying its own previous proposals or another agent s 
proposals an agent agk accumulates proposals that are 
made by agk but are rejected by another agent in the failed 
proposal set fp i 
k at the i-th round where fp 
k ∅ 
suppose two agents ag and ag who have abductive 
programs p h and p h respectively given a 
proposal g 
 which is satisfied by an answer set of p a 
negotiation starts in response to the proposal gi 
 made by ag 
at the i-th round ag behaves as follows 
 if gi 
 accept an agreement is reached and 
negotiation ends in success 
 else if gi 
 reject put fp i 
 fpi− 
 ∪{gi− 
 } where 
{g 
 } ∅ proceed to the step b 
 else if p has an answer set satisfying gi 
 ag returns 
gi 
 accept to ag negotiation ends in success 
 otherwise ag behaves as follows put fp i 
 fpi− 
 
 a if gi 
 violates an integrity constraint in p return 
the critique gi 
 reject to ag together with the 
critique set csi 
 p gi 
 
 b otherwise construct nsi 
 as follows 
 i produce si 
c let μ si 
c { p p ∈ si 
c \ fpi 
 and 
p satisfies the constraints in csi 
 p gi− 
 } 
if μ si 
c ∅ select one from μ si 
c and propose 
it as gi 
 to ag otherwise go to ii 
 ii produce si 
n if μ si 
n ∅ select one from μ si 
n 
and propose it as gi 
 to ag otherwise go to iii 
 iii produce si 
cn if μ si 
cn ∅ select one from 
μ si 
cn and propose it as gi 
 to ag otherwise 
negotiation ends in failure this means that ag 
can make no counter-proposal or every 
counterproposal made by ag is rejected by ag 
in the step a ag rejects the proposal gi 
 and returns 
the reason of rejection as a critique set this helps for ag 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
in preparing a next counter-proposal in the step b ag 
constructs a new proposal in its construction ag should 
take care of the critique set csi 
 p gi− 
 which 
represents integrity constraints if any accumulated in previous 
rounds that ag must satisfy also fp i 
 is used for 
removing proposals which have been rejected construction of 
si 
x x ∈ {c n cn} in nsi 
 is incrementally done by adding 
new counter-proposals produced by gi 
 or gi− 
 to si− 
x for 
instance si 
n in nsi 
 is computed as 
si 
n si− 
n ∪{ p p is a neighborhood proposal made by gi 
 } 
∪ { p p is a neighborhood proposal made by gi− 
 } 
where s 
n ∅ that is si 
n is constructed from si− 
n by 
adding new proposals which are obtained by modifying the 
proposal gi 
 made by ag at the i-th round or modifying 
the proposal gi− 
 made by ag at the i − -th round si 
c 
and si 
cn are obtained as well 
in the above protocol an agent produces si 
c at first 
secondly si 
n and finally si 
cn this strategy seeks conditions 
which satisfy the given proposal prior to neighborhood 
proposals which change the original one another strategy 
which prefers neighborhood proposals to conditional ones 
is also considered conditional neighborhood proposals are 
to be considered in the last place since they differ from the 
original one to the maximal extent the above protocol 
produces the candidate proposals in si 
x for each x ∈ {c n cn} 
at once we can consider a variant of the protocol in which 
each proposal in si 
x is constructed one by one see 
example 
the above protocol is repeatedly applied to each one of 
the two negotiating agents until a negotiation ends in 
success failure formally the above negotiation protocol has 
the following properties 
theorem let ag and ag be two agents having 
abductive programs p h and p h respectively 
 if p h and p h are function-free i e both 
pi and hi contain no function symbol any 
negotiation will terminate 
 if a negotiation terminates with agreement on a 
proposal g both p h and p h have belief sets 
satisfying g 
proof when an abductive program is function-free 
abducibles and negotiation sets are both finite moreover if 
a proposal is once rejected it is not proposed again by the 
function μ thus negotiation will terminate in finite steps 
 when a proposal g is made by ag p h has a 
belief set satisfying g if the agent ag accepts the proposal 
g it is satisfied by an answer set of p which is also a belief 
set of p h 
example suppose a buying-selling situation in the 
introduction a seller agent has the abductive program 
ps hs in which ps consists of belief bs and desire ds 
bs pc b g m g pc b g m g ← 
dvd-rw cd-rw ← 
ds normal price ← 
pc b g m g dvd-rw 
normal price ← 
pc b g m g cd-rw 
normal price ← 
pc b g m g dvd-rw 
price x ← normal price x add point 
price x ∗ ← 
normal price x pay cash not add point 
add point ← 
hs add point pay cash 
here and represent selection of products the atom 
pc b g m g represents that the seller agent has 
a pc of the brand b such that cpu is ghz memory is 
 mb and hdd is gb prices of products are 
represented as desire of the seller the rules - are normal 
prices of products a normal price is a selling price on the 
condition that service points are added on the other 
hand a discount price is applied if the paying method is cash 
and no service point is added the fact represents 
the addition of service points this service would be 
withdrawn in case of discount prices so add point is specified as 
an abducible 
a buyer agent has the abductive program pb hb in 
which pb consists of belief bb and desire db 
bb drive ← dvd-rw 
drive ← cd-rw 
price x ← 
db pc b g m g ← 
dvd-rw ← 
cd-rw ← not dvd-rw 
← pay cash 
← price x x 
hb dvd-rw 
rules - are the buyer s desire among them 
and impose constraints for buying a pc a dvd-rw 
is specified as an abducible which is subject to concession 
 st round first the following proposal is given by the 
buyer agent 
g 
b pc b g m g dvd-rw price x x ≤ 
as ps has no answer set which satisfies g 
b the seller agent 
cannot accept the proposal the seller takes an action of 
making a counter-proposal and performs abduction as a 
result the seller finds the minimal explanation e f 
 { pay cash } { add point } which explains g 
b θ with θ 
{ x } the seller constructs the conditional proposal 
g 
s pc b g m g dvd-rw price 
pay cash not add point 
and offers it to the buyer 
 nd round the buyer does not accept g 
s because he she 
cannot pay it by cash the buyer then returns the 
critique g 
b reject to the seller together with the critique set 
cs 
b pb g 
s { } in response to this the seller tries 
to make another proposal which satisfies the constraint in 
this critique set as g 
s is stored in fp 
s and no other 
conditional proposal satisfying the buyer s requirement exists 
the seller produces neighborhood proposals he she relaxes 
g 
b by dropping x ≤ in the condition and produces 
pc b g m g dvd-rw price x 
as ps has an answer set which satisfies 
g 
s pc b g m g dvd-rw price 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the seller offers g 
s as a new counter-proposal 
 rd round the buyer does not accept g 
s because he she 
cannot pay more than usd the buyer again 
returns the critique g 
b reject to the seller together with 
the critique set cs 
b pb g 
s cs 
b pb g 
s ∪ { } the 
seller then considers another proposal by replacing b with 
a variable w g 
b now becomes 
pc w g m g dvd-rw price x x ≤ 
as ps has an answer set which satisfies 
g 
s pc b g m g dvd-rw price 
the seller offers g 
s as a new counter-proposal 
 th round the buyer does not accept g 
s because a pc of 
the brand b is out of his her interest and pb has no answer 
set satisfying g 
s then the buyer makes a concession by 
changing his her original goal the buyer relaxes g 
b by goal 
replacement using the rule in pb and produces 
pc b g m g drive price x x ≤ 
using the following proposal is produced 
pc b g m g cd-rw price x x ≤ 
as pb \ { dvd-rw } has a consistent answer set satisfying the 
above proposal the buyer proposes the conditional 
neighborhood proposal 
g 
b pc b g m g cd-rw not dvd-rw 
price x x ≤ 
to the seller agent since ps also has an answer set satisfying 
g 
b the seller accepts it and sends the message g 
s accept 
to the buyer thus the negotiation ends in success 
 computation 
in this section we provide methods of computing 
proposals in terms of answer sets of programs we first introduce 
some definitions from 
definition given an abductive program p h the 
set ur of update rules is defined as 
ur { l ← not l l ← not l l ∈ h } 
∪ { l ← l l ∈ h \ p } 
∪ { −l ← not l l ∈ h ∩ p } 
where l l and −l are new atoms uniquely associated 
with every l ∈ h the atoms l and −l are called update 
atoms 
by the definition the atom l becomes true iff l is not 
true the pair of rules l ← not l and l ← not l specify 
the situation that an abducible l is true or not when 
p x ∈ h and p a ∈ p but p t ∈ p for t a the rule 
 l ← l precisely becomes p t ← p t for any t a in 
this case the rule is shortly written as p x ← p x x a 
generally the rule becomes p x ← p x x t x 
tn for n such instances the rule l ← l derives the atom 
 l if an abducible l which is not in p is to be true in 
contrast the rule −l ← not l derives the atom −l if an 
abducible l which is in p is not to be true thus update 
atoms represent the change of truth values of abducibles in 
a program that is l means the introduction of l while 
−l means the deletion of l when an abducible l contains 
variables the associated update atom l or −l is supposed 
to have exactly the same variables in this case an update 
atom is semantically identified with its ground instances 
the set of all update atoms associated with the abducibles 
in h is denoted by uh and uh uh 
∪ uh− 
where 
uh 
 resp uh− 
 is the set of update atoms of the form 
 l resp −l 
definition given an abductive program p h its 
update program up is defined as the program 
up p \ h ∪ ur 
an answer set s of up is called u-minimal if there is no 
answer set t of up such that t ∩ uh ⊂ s ∩ uh 
by the definition u-minimal answer sets exist whenever 
up has answer sets update programs are used for 
computing minimal explanations of an observation given an 
observation g as a conjunction of literals and naf-literals 
possibly containing variables we introduce a new ground 
literal o together with the rule o ← g in this case o 
has an explanation e f iff g has the same explanation 
with this replacement an observation is assumed to be a 
ground literal without loss of generality in what follows 
e 
 { l l ∈ e } and f − 
 { −l l ∈ f } for e ⊆ h 
and f ⊆ h 
proposition let p h be an abductive 
program up its update program and g a ground literal 
representing an observation then a pair e f is an 
explanation of g iff up ∪ { ← not g } has a consistent answer set 
s such that e 
 s ∩ uh 
and f− 
 s ∩ uh− 
 in 
particular e f is a minimal explanation iff s is a u-minimal 
answer set 
example to explain the observation g flies t 
in the program p of example first construct the update 
program up of p 
up flies x ← bird x not ab x 
ab x ← broken-wing x 
bird t ← bird o ← 
broken-wing x ← not broken-wing x 
broken-wing x ← not broken-wing x 
 broken-wing x ← broken-wing x x t 
−broken-wing t ← not broken-wing t 
next consider the program up ∪ { ← not flies t } it has 
the single u-minimal answer set s { bird t bird o flies t 
flies o broken-wing t broken-wing o −broken-wing t } 
the unique minimal explanation e f ∅ {broken-wing t } 
of g is expressed by the update atom −broken-wing t in 
s ∩ uh− 
 
proposition let p h be an abductive program 
and g a ground literal representing an observation if p ∪ 
{ ← not g } has a consistent answer set s g has the 
minimal explanation e f ∅ ∅ and s satisfies g 
now we provide methods for computing counter- proposals 
first conditional proposals are computed as follows 
input an abductive program p h a proposal g 
output a set sc of proposals 
if g is a ground literal compute its minimal 
explanation e f in p h using the update program put 
g e not f in sc else if g is a conjunction possibly 
containing variables consider the abductive program 
 
t represents tweety and o represents opus 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
p ∪{ o ← g } h with a ground literal o compute 
a minimal explanation of o in p ∪ { o ← g } h 
using its update program if o has a minimal 
explanation e f with a substitution θ for variables in g 
put gθ e not f in sc 
next neighborhood proposals are computed as follows 
input an abductive program p h a proposal g 
output a set sn of proposals 
 neighborhood proposals by anti-instantiation 
construct g by anti-instantiation for a ground 
literal o if p ∪ { o ← g } ∪ { ← not o } has a 
consistent answer set satisfying g θ with a substitution θ 
and g θ g put g θ in sn 
 neighborhood proposals by dropping conditions 
construct g by dropping conditions if g is a ground 
literal and the program p ∪ { ← not g } has a 
consistent answer set put g in sn else if g is a 
conjunction possibly containing variables do the following 
for a ground literal o if p ∪{ o ← g }∪{ ← not o } 
has a consistent answer set satisfying g θ with a 
substitution θ put g θ in sn 
 neighborhood proposals by goal replacement 
construct g by goal replacement if g is a ground 
literal and there is a rule h ← b in p such that g hσ 
and bσ g for some substitution σ put g bσ 
if p ∪ { ← not g } has a consistent answer set 
satisfying g θ with a substitution θ put g θ in sn else 
if g is a conjunction possibly containing variables 
do the following for a replaced literal l ∈ g if 
there is a rule h ← b in p such that l hσ and 
 g \ {l} ∪ bσ g for some substitution σ put 
g g \ {l} ∪ bσ for a ground literal o if 
p ∪ { o ← g } ∪ { ← not o } has a consistent answer 
set satisfying g θ with a substitution θ put g θ in 
sn 
theorem the set sc resp sn computed above 
coincides with the set of conditional proposals resp 
neighborhood proposals 
proof the result for sc follows from definition and 
proposition the result for sn follows from definition 
and proposition 
conditional neighborhood proposals are computed by 
combining the above two procedures those proposals are 
computed at each round note that the procedure for computing 
sn contains some nondeterministic choices for instance 
there are generally several candidates of literals to relax in 
a proposal also there might be several rules in a program 
for the usage of goal replacement in practice an agent can 
prespecify literals in a proposal for possible relaxation or 
rules in a program for the usage of goal replacement 
 related work 
as there are a number of literature on automated 
negotiation this section focuses on comparison with negotiation 
frameworks based on logic and argumentation 
sadri et al use abductive logic programming as a 
representation language of negotiating agents agents negotiate 
using common dialogue primitives called dialogue moves 
each agent has an abductive logic program in which a 
sequence of dialogues are specified by a program a dialogue 
protocol is specified as constraints and dialogue moves are 
specified as abducibles the behavior of agents is regulated 
by an observe-think-act cycle once a dialogue move is 
uttered by an agent another agent that observed the utterance 
thinks and acts using a proof procedure their approach 
and ours both employ abductive logic programming as a 
platform of agent reasoning but the use of it is quite 
different first they use abducibles to specify dialogue primitives 
of the form tell utterer receiver subject identifier time 
while we use abducibles to specify arbitrary permissible 
hypotheses to construct conditional proposals second a 
program pre-specifies a plan to carry out in order to achieve a 
goal together with available missing resources in the 
context of resource-exchanging problems this is in contrast 
with our method in which possible counter-proposals are 
newly constructed in response to a proposal made by an 
agent third they specify a negotiation policy inside a 
program as integrity constraints while we give a protocol 
independent of individual agents they provide an 
operational model that completely specifies the behavior of agents 
in terms of agent cycle we do not provide such a complete 
specification of the behavior of agents our primary interest 
is to mechanize construction of proposals 
bracciali and torroni formulate abductive agents that 
have knowledge in abductive logic programs to explain 
an observation two agents communicate by exchanging 
integrity constraints in the process of communication an 
agent can revise its own integrity constraints according to 
the information provided by the other agent a set ic 
of integrity constraints relaxes a set ic or ic tightens 
ic if any observation that can be proved with respect to 
ic can also be proved with respect to ic for instance 
ic ← a b c relaxes ic ← a b thus they use relaxation 
for weakening the constraints in an abductive logic program 
in contrast we use relaxation for weakening proposals and 
three different relaxation methods anti-instantiation 
dropping conditions and goal replacement are considered their 
goal is to explain an observation by revising integrity 
constraints of an agent through communication while we use 
integrity constraints for communication to explain critiques 
and help other agents in making counter-proposals 
meyer et al introduce a logical framework for 
negotiating agents they introduce two different modes of 
negotiation concession and adaptation they provide rational 
postulates to characterize negotiated outcomes between two 
agents and describe methods for constructing outcomes 
they provide logical conditions for negotiated outcomes to 
satisfy but they do not describe a process of negotiation nor 
negotiation protocols moreover they represent agents by 
classical propositional theories which is different from our 
abductive logic programming framework 
foo et al model one-to-one negotiation as a one-time 
encounter between two extended logic programs an agent 
offers an answer set of its program and their mutual deal is 
regarded as a trade on their answer sets starting from the 
initial agreement set s∩t for an answer set s of an agent and 
an answer set t of another agent each agent extends this 
set to reflect its own demand while keeping consistency with 
demand of the other agent their algorithm returns new 
programs having answer sets which are consistent with each 
other and keep the agreement set the work is extended to 
repeated encounters in in their framework two agents 
exchange answer sets to produce a common belief set which 
is different from our framework of exchanging proposals 
there are a number of proposals for negotiation based 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
on argumentation an advantage of argumentation-based 
negotiation is that it constructs a proposal with arguments 
supporting the proposal the existence of arguments is 
useful to convince other agents of reasons why an agent offers 
 counter- proposals or returns critiques parsons et al 
develop a logic of argumentation-based negotiation among 
bdi agents in one-to-one negotiation an agent a generates 
a proposal together with its arguments and passes it to 
another agent b the proposal is evaluated by b which 
attempts to build arguments against it if it conflicts with 
b s interest b informs a of its objection by sending back 
its attacking argument in response to this a tries to find 
an alternative way of achieving its original objective or a 
way of persuading b to drop its objection if either type of 
argument can be found a will submit it to b if b finds no 
reason to reject the new proposal it will be accepted and 
the negotiation ends in success otherwise the process is 
iterated in this negotiation processes the agent a never 
changes its original objective so that negotiation ends in 
failure if a fails to find an alternative way of achieving the 
original objective in our framework when a proposal is 
rejected by another agent an agent can weaken or change 
its objective by abduction and relaxation our framework 
does not have a mechanism of argumentation but reasons 
for critiques can be informed by responding critique sets 
kakas and moraitis propose a negotiation protocol 
which integrates abduction within an argumentation 
framework a proposal contains an offer corresponding to the 
negotiation object together with supporting information 
representing conditions under which this offer is made 
supporting information is computed by abduction and is used 
for constructing conditional arguments during the process 
of negotiation in their negotiation protocol when an agent 
cannot satisfy its own goal the agent considers the other 
agent s goal and searches for conditions under which the 
goal is acceptable our present approach differs from theirs 
in the following points first they use abduction to seek 
conditions to support arguments while we use abduction 
to seek conditions for proposals to accept second in their 
negotiation protocol counter-proposals are chosen among 
candidates based on preference knowledge of an agent at 
meta-level which represents policy under which an agent 
uses its object-level decision rules according to situations 
in our framework counter-proposals are newly constructed 
using abduction and relaxation the method of 
construction is independent of particular negotiation protocols as 
 abduction or abductive logic programming used 
in negotiation is mostly based on normal abduction in 
contrast our approach is based on extended abduction which 
can not only introduce hypotheses but remove them from a 
program this is another important difference 
relaxation and neighborhood query answering are devised 
to make databases cooperative with their users in this 
sense those techniques have the spirit similar to cooperative 
problem solving in multi-agent systems as far as the 
authors know however there is no study which applies those 
technique to agent negotiation 
 conclusion 
in this paper we proposed a logical framework for 
negotiating agents to construct proposals in the process of 
negotiation we combined the techniques of extended abduction 
and relaxation it was shown that these two operations are 
used for general inference rules in producing proposals we 
developed a negotiation protocol between two agents based 
on exchange of proposals and critiques and provided 
procedures for computing proposals in abductive logic 
programming this enables us to realize automated negotiation on 
top of the existing answer set solvers the present 
framework does not have a mechanism of selecting an optimal 
 counter- proposal among different alternatives to 
compare and evaluate proposals an agent must have preference 
knowledge of candidate proposals further elaboration to 
maximize the utility of agents is left for future study 
 references 
 l amgoud s parsons and n maudet arguments 
dialogue and negotiation in proc ecai- 
pp - ios press 
 a bracciali and p torroni a new framework for 
knowledge revision of abductive agents through their 
interaction in proc clima-iv computational logic 
in multi-agent systems lnai pp - 
 w chen m zhang and n foo repeated negotiation 
of logic programs in proc th workshop on 
nonmonotonic reasoning action and change 
 w w chu q chen and r -c lee cooperative 
query answering via type abstraction hierarchy in 
cooperating knowledge based systems s m deen ed 
pp - springer 
 n foo t meyer y zhang and d zhang 
negotiating logic programs in proc th workshop on 
nonmonotonic reasoning action and change 
 t gaasterland p godfrey and j minker relaxation 
as a platform for cooperative answering journal of 
intelligence information systems - 
 m gelfond and v lifschitz classical negation in logic 
programs and disjunctive databases new generation 
computing - 
 k inoue and c sakama abductive framework for 
nonmonotonic theory change in proc ijcai- 
pp - morgan kaufmann 
 a c kakas r a kowalski and f toni the role of 
abduction in logic programming in handbook of logic 
in ai and logic programming d m gabbay et al 
 eds vol pp - oxford university press 
 a c kakas and p moraitis adaptive agent 
negotiation via argumentation in proc aamas- 
pp - acm press 
 t meyer n foo r kwok and d zhang logical 
foundation of negotiation outcome concession and 
adaptation in proc aaai- pp - mit press 
 r s michalski a theory and methodology of 
inductive learning in machine learning an artificial 
intelligence approach r s michalski et al eds 
pp - morgan kaufmann 
 s parsons c sierra and n jennings agents that 
reason and negotiate by arguing journal of logic and 
computation - 
 f sadri f toni and p torroni an abductive logic 
programming architecture for negotiating agents in 
proc th european conf on logics in ai lnai 
pp - springer 
 c sakama and k inoue an abductive framework for 
computing knowledge base updates theory and practice 
of logic programming - 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a q-decomposition and bounded rtdp approach to 
resource allocation 
pierrick plamondon and brahim 
chaib-draa 
computer science software engineering dept 
laval university 
québec canada 
{plamon chaib} damas ift ulaval ca 
abder rezak benaskeur 
decision support systems section 
defence r d canada - valcartier 
québec canada 
abderrezak benaskeur drdc-rddc gc ca 
abstract 
this paper contributes to solve effectively stochastic 
resource allocation problems known to be np-complete to 
address this complex resource management problem a 
qdecomposition approach is proposed when the resources 
which are already shared among the agents but the actions 
made by an agent may influence the reward obtained by 
at least another agent the q-decomposition allows to 
coordinate these reward separated agents and thus permits to 
reduce the set of states and actions to consider on the other 
hand when the resources are available to all agents no 
qdecomposition is possible and we use heuristic search in 
particular the bounded real-time dynamic programming 
 bounded rtdp is used bounded rtdp concentrates the 
planning on significant states only and prunes the action 
space the pruning is accomplished by proposing tight 
upper and lower bounds on the value function 
categories and subject descriptors 
i artificial intelligence problem solving control 
methods and search i artificial intelligence 
distributed artificial intelligence 
general terms 
algorithms performance experimentation 
 introduction 
this paper aims to contribute to solve complex stochastic 
resource allocation problems in general resource 
allocation problems are known to be np-complete in such 
problems a scheduling process suggests the action i e 
resources to allocate to undertake to accomplish certain tasks 
according to the perfectly observable state of the 
environment when executing an action to realize a set of tasks 
the stochastic nature of the problem induces probabilities 
on the next visited state in general the number of states 
is the combination of all possible specific states of each task 
and available resources in this case the number of 
possible actions in a state is the combination of each individual 
possible resource assignment to the tasks the very high 
number of states and actions in this type of problem makes 
it very complex 
there can be many types of resource allocation problems 
firstly if the resources are already shared among the agents 
and the actions made by an agent does not influence the 
state of another agent the globally optimal policy can be 
computed by planning separately for each agent a second 
type of resource allocation problem is where the resources 
are already shared among the agents but the actions made 
by an agent may influence the reward obtained by at least 
another agent to solve this problem efficiently we adapt 
qdecomposition proposed by russell and zimdars in our 
q-decomposition approach a planning agent manages each 
task and all agents have to share the limited resources the 
planning process starts with the initial state s in s each 
agent computes their respective q-value then the 
planning agents are coordinated through an arbitrator to find 
the highest global q-value by adding the respective possible 
q-values of each agents when implemented with heuristic 
search since the number of states and actions to consider 
when computing the optimal policy is exponentially reduced 
compared to other known approaches q-decomposition 
allows to formulate the first optimal decomposed heuristic 
search algorithm in a stochastic environments 
on the other hand when the resources are available to 
all agents no q-decomposition is possible a common 
way of addressing this large stochastic problem is by 
using markov decision processes mdps and in particular 
real-time search where many algorithms have been 
developed recently for instance real-time dynamic 
programming rtdp lrtdp hdp and lao are all 
state-of-the-art heuristic search approaches in a stochastic 
environment because of its anytime quality an interesting 
approach is rtdp introduced by barto et al which 
updates states in trajectories from an initial state s to a goal 
state sg rtdp is used in this paper to solve efficiently a 
constrained resource allocation problem 
rtdp is much more effective if the action space can be 
pruned of sub-optimal actions to do this mcmahan et 
 
 - - - - rps c ifaamas 
al smith and simmons and singh and cohn 
proposed solving a stochastic problem using a rtdp type 
heuristic search with upper and lower bounds on the value 
of states mcmahan et al and smith and simmons 
suggested in particular an efficient trajectory of state 
updates to further speed up the convergence when given upper 
and lower bounds this efficient trajectory of state updates 
can be combined to the approach proposed here since this 
paper focusses on the definition of tight bounds and efficient 
state update for a constrained resource allocation problem 
on the other hand the approach by singh and cohn is 
suitable to our case and extended in this paper using in 
particular the concept of marginal revenue to elaborate 
tight bounds this paper proposes new algorithms to define 
upper and lower bounds in the context of a rtdp heuristic 
search approach our marginal revenue bounds are 
compared theoretically and empirically to the bounds proposed 
by singh and cohn also even if the algorithm used to 
obtain the optimal policy is rtdp our bounds can be used 
with any other algorithm to solve an mdp the only 
condition on the use of our bounds is to be in the context of 
stochastic constrained resource allocation the problem is 
now modelled 
 problem formulation 
a simple resource allocation problem is one where there 
are the following two tasks to realize ta {wash the 
dishes} and ta {clean the floor} these two tasks are 
either in the realized state or not realized state to realize the 
tasks two type of resources are assumed res {brush} 
and res {detergent} a computer has to compute the 
optimal allocation of these resources to cleaner robots to realize 
their tasks in this problem a state represents a 
conjunction of the particular state of each task and the available 
resources the resources may be constrained by the amount 
that may be used simultaneously local constraint and in 
total global constraint furthermore the higher is the 
number of resources allocated to realize a task the higher is 
the expectation of realizing the task for this reason when 
the specific states of the tasks change or when the number 
of available resources changes the value of this state may 
change 
when executing an action a in state s the specific states 
of the tasks change stochastically and the remaining 
resource are determined with the resource available in s 
subtracted from the resources used by action a if the resource 
is consumable indeed our model may consider 
consumable and non-consumable resource types a consumable 
resource type is one where the amount of available resource 
is decreased when it is used on the other hand a 
nonconsumable resource type is one where the amount of 
available resource is unchanged when it is used for example a 
brush is a non-consumable resource while the detergent is 
a consumable resource 
 resource allocation as a mdps 
in our problem the transition function and the reward 
function are both known a markov decision process mdp 
framework is used to model our stochastic resource 
allocation problem mdps have been widely adopted by researchers 
today to model a stochastic process this is due to the fact 
that mdps provide a well-studied and simple yet very 
expressive model of the world an mdp in the context of a 
resource allocation problem with limited resources is defined 
as a tuple res t a s a p w r where 
 res res res res is a finite set of resource 
types available for a planning process each resource 
type may have a local resource constraint lres on 
the number that may be used in a single step and 
a global resource constraint gres on the number that 
may be used in total the global constraint only 
applies for consumable resource types resc and the 
local constraints always apply to consumable and 
nonconsumable resource types 
 t a is a finite set of tasks with ta ∈ t a to be 
accomplished 
 s is a finite set of states with s ∈ s a state s is 
a tuple t a res res resc which is the 
characteristic of each unaccomplished task ta ∈ t a in the 
environment and the available consumable resources 
sta is the specific state of task ta also s contains a 
non empty set sg ⊆ s of goal states a goal state is a 
sink state where an agent stays forever 
 a is a finite set of actions or assignments the 
actions a ∈ a s applicable in a state are the 
combination of all resource assignments that may be executed 
according to the state s in particular a is simply an 
allocation of resources to the current tasks and ata is 
the resource allocation to task ta the possible actions 
are limited by lres and gres 
 transition probabilities pa s s for s ∈ s and a ∈ 
a s 
 w wta is the relative weight criticality of each 
task 
 state rewards r rs 
ta∈t a 
rsta ← sta × wta the 
relative reward of the state of a task rsta is the product 
of a real number sta by the weight factor wta for 
our problem a reward of × wta is given when the 
state of a task sta is in an achieved state and in 
all other cases 
 a discount preference factor γ which is a real 
number between and 
a solution of an mdp is a policy π mapping states s into 
actions a ∈ a s in particular πta s is the action i e 
resources to allocate that should be executed on task ta 
considering the global state s in this case an optimal 
policy is one that maximizes the expected total reward for 
accomplishing all tasks the optimal value of a state v s is 
given by 
v s r s max 
a∈a s 
γ 
s ∈s 
pa s s v s 
where the remaining consumable resources in state s are 
resc \ res a where res a are the consumable resources 
used by action a indeed since an action a is a resource 
assignment resc \ res a is the new set of available resources 
after the execution of action a furthermore one may 
compute the q-values q a s of each state action pair using the 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
following equation 
q a s r s γ 
s ∈s 
pa s s max 
a ∈a s 
q a s 
where the optimal value of a state is v s max 
a∈a s 
q a s 
the policy is subjected to the local resource constraints 
res π s ≤ lres∀ s ∈ s and ∀ res ∈ res the global 
constraint is defined according to all system trajectories 
tra ∈ t ra a system trajectory tra is a possible sequence 
of state-action pairs until a goal state is reached under the 
optimal policy π for example state s is entered which may 
transit to s or to s according to action a the two 
possible system trajectories are s a s and s a s 
the global resource constraint is res tra ≤ gres∀ tra ∈ 
t ra and ∀ res ∈ resc where res tra is a function which 
returns the resources used by trajectory tra since the 
available consumable resources are represented in the state space 
this condition is verified by itself in other words the model 
is markovian as the history has not to be considered in the 
state space furthermore the time is not considered in the 
model description but it may also include a time horizon 
by using a finite horizon mdp since resource allocation in 
a stochastic environment is np-complete heuristics should 
be employed q-decomposition which decomposes a 
planning problem to many agents to reduce the computational 
complexity associated to the state and or action spaces is 
now introduced 
 q-decomposition for resource allocation 
there can be many types of resource allocation problems 
firstly if the resources are already shared among the agents 
and the actions made by an agent does not influence the 
state of another agent the globally optimal policy can be 
computed by planning separately for each agent 
a second type of resource allocation problem is where 
the resources are already shared among the agents but the 
actions made by an agent may influence the reward obtained 
by at least another agent for instance a group of agents 
which manages the oil consummated by a country falls in 
this group these agents desire to maximize their specific 
reward by consuming the right amount of oil however all 
the agents are penalized when an agent consumes oil because 
of the pollution it generates another example of this type 
comes from our problem of interest explained in section 
 which is a naval platform which must counter incoming 
missiles i e tasks by using its resources i e weapons 
movements in some scenarios it may happens that the 
missiles can be classified in two types those requiring a 
set of resources res and those requiring a set of resources 
res this can happen depending on the type of missiles 
their range and so on in this case two agents can plan for 
both set of tasks to determine the policy however there 
are interaction between the resource of res and res so 
that certain combination of resource cannot be assigned in 
particular if an agent i allocate resources resi to the first 
set of tasks t ai and agent i allocate resources resi to 
second set of tasks t ai the resulting policy may include 
actions which cannot be executed together 
to result these conflicts we use q-decomposition 
proposed by russell and zimdars in the context of 
reinforcement learning the primary assumption underlying 
qdecomposition is that the overall reward function r can be 
additively decomposed into separate rewards ri for each 
distinct agent i ∈ ag where ag is the number of agents that 
is r i∈ag ri it requires each agent to compute a value 
from its perspective for every action to coordinate with 
each other each agent i reports its action values qi ai si 
for each state si ∈ si to an arbitrator at each learning 
iteration the arbitrator then chooses an action maximizing 
the sum of the agent q-values for each global state s ∈ s 
the next time state s is updated an agent i considers the 
value as its respective contribution or q-value to the global 
maximal q-value that is qi ai si is the value of a state 
such that it maximizes maxa∈a s i∈ag qi ai si the fact 
that the agents use a determined q-value as the value of a 
state is an extension of the sarsa on-policy algorithm to 
q-decomposition russell and zimdars called this approach 
local sarsa in this way an ideal compromise can be found 
for the agents to reach a global optimum indeed rather 
than allowing each agent to choose the successor action each 
agent i uses the action ai executed by the arbitrator in the 
successor state si 
qi ai si ri si γ 
si∈si 
pai si si qi ai si 
 
where the remaining consumable resources in state si are 
resci \ resi ai for a resource allocation problem russell 
and zimdars demonstrated that local sarsa converges 
to the optimum also in some cases this form of agent 
decomposition allows the local q-functions to be expressed 
by a much reduced state and action space 
for our resource allocation problem described briefly in 
this section q-decomposition can be applied to generate an 
optimal solution indeed an optimal bellman backup can 
be applied in a state as in algorithm in line of the 
qdec-backup function each agent managing a task 
computes its respective q-value here qi ai s determines the 
optimal q-value of agent i in state s an agent i uses as 
the value of a possible state transition s the q-value for 
this agent which determines the maximal global q-value for 
state s as in the original q-decomposition approach in 
brief for each visited states s ∈ s each agent computes its 
respective q-values with respect to the global state s so 
the state space is the joint state space of all agents some 
of the gain in complexity to use q-decomposition resides in 
the 
si∈si 
pai si s part of the equation an agent considers 
as a possible state transition only the possible states of the 
set of tasks it manages since the number of states is 
exponential with the number of tasks using q-decomposition 
should reduce the planning time significantly furthermore 
the action space of the agents takes into account only their 
available resources which is much less complex than a 
standard action space which is the combination of all possible 
resource allocation in a state for all agents 
then the arbitrator functionalities are in lines to 
the global q-value is the sum of the q-values produced by 
each agent managing each task as shown in line 
considering the global action a in this case when an action of an 
agent i cannot be executed simultaneously with an action 
of another agent i the global action is simply discarded 
from the action space a s line simply allocate the 
current value with respect to the highest global q-value as in 
a standard bellman backup then the optimal policy and 
q-value of each agent is updated in lines and to the 
sub-actions ai and specific q-values qi ai s of each agent 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
for action a 
algorithm the q-decomposition bellman backup 
 function qdec-backup s 
 v s ← 
 for all i ∈ ag do 
 for all ai ∈ ai s do 
 qi ai s ← ri s γ 
si 
∈si 
pai si s qi ai s 
{where qi ai s hi s when s is not yet visited 
and s has resci \ resi ai remaining consumable 
resources for each agent i} 
 end for 
 end for 
 for all a ∈ a s do 
 q a s ← 
 for all i ∈ ag do 
 q a s ← q a s qi ai s 
 end for 
 if q a s v s then 
 v s ← q a s 
 for all i ∈ ag do 
 πi s ← ai 
 qi ai s ← qi ai s 
 end for 
 end if 
 end for 
a standard bellman backup has a complexity of o a × 
 sag where sag is the number of joint states for all agents 
excluding the resources and a is the number of joint 
actions on the other hand the q-decomposition bellman 
backup has a complexity of o ag × ai × si a × 
 ag where si is the number of states for an agent i 
excluding the resources and ai is the number of actions 
for an agent i since sag is combinatorial with the 
number of tasks so si s also a is combinatorial with 
the number of resource types if the resources are already 
shared among the agents the number of resource type for 
each agent will usually be lower than the set of all 
available resource types for all agents in these circumstances 
 ai a in a standard bellman backup a is multiplied 
by sag which is much more complex than multiplying a 
by ag with the q-decomposition bellman backup thus 
the q-decomposition bellman backup is much less complex 
than a standard bellman backup furthermore the 
communication cost between the agents and the arbitrator is 
null since this approach does not consider a geographically 
separated problem 
however when the resources are available to all agents 
no q-decomposition is possible in this case bounded 
realtime dynamic programming bounded-rtdp permits to 
focuss the search on relevant states and to prune the action 
space a by using lower and higher bound on the value of 
states bounded-rtdp is now introduced 
 bounded-rtdp 
bonet and geffner proposed lrtdp as an improvement 
to rtdp lrtdp is a simple dynamic programming 
algorithm that involves a sequence of trial runs each starting in 
the initial state s and ending in a goal or a solved state 
each lrtdp trial is the result of simulating the policy π while 
updating the values v s using a bellman backup equation 
 over the states s that are visited h s is a heuristic which 
define an initial value for state s this heuristic has to be 
admissible - the value given by the heuristic has to 
overestimate or underestimate the optimal value v s when 
the objective function is maximized or minimized for 
example an admissible heuristic for a stochastic shortest 
path problem is the solution of a deterministic shortest path 
problem indeed since the problem is stochastic the 
optimal value is lower than for the deterministic version it has 
been proven that lrtdp given an admissible initial 
heuristic on the value of states cannot be trapped in loops and 
eventually yields optimal values the convergence is 
accomplished by means of a labeling procedure called 
checksolved s this procedure tries to label as solved each 
traversed state in the current trajectory when the initial 
state is labelled as solved the algorithm has converged 
in this section a bounded version of rtdp 
 boundedrtdp is presented in algorithm to prune the action space 
of sub-optimal actions this pruning enables to speed up the 
convergence of lrtdp bounded-rtdp is similar to rtdp 
except there are two distinct initial heuristics for unvisited 
states s ∈ s hl s and hu s also the checksolved s 
 procedure can be omitted because the bounds can provide 
the labeling of a state as solved on the one hand hl s 
defines a lower bound on the value of s such that the optimal 
value of s is higher than hl s for its part hu s defines an 
upper bound on the value of s such that the optimal value 
of s is lower than hu s 
the values of the bounds are computed in lines and 
 of the bounded-backup function computing these two 
q-values is made simultaneously as the state transitions are 
the same for both q-values only the values of the state 
transitions change thus having to compute two q-values 
instead of one does not augment the complexity of the 
approach in fact smith and simmons state that the 
additional time to compute a bellman backup for two bounds 
instead of one is no more than which is also what we 
obtained in particular l s is the lower bound of state s 
while u s is the upper bound of state s similarly ql a s 
is the q-value of the lower bound of action a in state s while 
qu a s is the q-value of the upper bound of action a in 
state s using these two bounds allow significantly 
reducing the action space a indeed in lines and of the 
bounded-backup function if qu a s ≤ l s then action 
a may be pruned from the action space of s in line 
of this function a state can be labeled as solved if the 
difference between the lower and upper bounds is lower than 
 when the execution goes back to the bounded-rtdp 
function the next state in line has a fixed number of 
consumable resources available resc determined in line 
in brief picknextstate res selects a none-solved state 
s reachable under the current policy which has the highest 
bellman error u s − l s finally in lines to a 
backup is made in a backward fashion on all visited state 
of a trajectory when this trajectory has been made this 
strategy has been proven as efficient 
as discussed by singh and cohn this type of 
algorithm has a number of desirable anytime characteristics if 
an action has to be picked in state s before the algorithm 
has converged while multiple competitive actions remains 
the action with the highest lower bound is picked since 
the upper bound for state s is known it may be estimated 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
algorithm the bounded-rtdp algorithm adapted 
from and 
 function bounded-rtdp s 
 returns a value function v 
 repeat 
 s ← s 
 visited ← null 
 repeat 
 visited push s 
 bounded-backup s 
 resc ← resc \ {π s } 
 s ← s picknextstate resc 
 until s is a goal 
 while visited null do 
 s ← visited pop 
 bounded-backup s 
 end while 
 until s is solved or a s ∀ s ∈ s reachable from 
s 
 return v 
algorithm the bounded bellman backup 
 function bounded-backup s 
 for all a ∈ a s do 
 qu a s ← r s γ 
s ∈s 
pa s s u s 
 ql a s ← r s γ 
s ∈s 
pa s s l s 
{where l s ← hl s and u s ← hu s when s 
is not yet visited and s has resc \ res a remaining 
consumable resources} 
 if qu a s ≤ l s then 
 a s ← a s \ res a 
 end if 
 end for 
 l s ← max 
a∈a s 
ql a s 
 u s ← max 
a∈a s 
qu a s 
 π s ← arg max 
a∈a s 
ql a s 
 if u s − l s then 
 s ← solved 
 end if 
how far the lower bound is from the optimal if the 
difference between the lower and upper bound is too high one 
can choose to use another greedy algorithm of one s choice 
which outputs a fast and near optimal solution 
furthermore if a new task dynamically arrives in the environment 
it can be accommodated by redefining the lower and 
upper bounds which exist at the time of its arrival singh 
and cohn proved that an algorithm that uses 
admissible lower and upper bounds to prune the action space is 
assured of converging to an optimal solution 
the next sections describe two separate methods to define 
hl s and hu s first of all the method of singh and cohn 
 is briefly described then our own method proposes 
tighter bounds thus allowing a more effective pruning of 
the action space 
 singh and cohn s bounds 
singh and cohn defined lower and upper bounds to 
prune the action space their approach is pretty 
straightforward first of all a value function is computed for all 
tasks to realize using a standard rtdp approach then 
using these task-value functions a lower bound hl and 
upper bound hu can be defined in particular hl s 
max 
ta∈t a 
vta sta and hu s 
ta∈t a 
vta sta for readability 
the upper bound by singh and cohn is named singhu and 
the lower bound is named singhl the admissibility of these 
bounds has been proven by singh and cohn such that the 
upper bound always overestimates the optimal value of each 
state while the lower bound always underestimates the 
optimal value of each state to determine the optimal policy 
π singh and cohn implemented an algorithm very similar 
to bounded-rtdp which uses the bounds to initialize l s 
and u s the only difference between bounded-rtdp and 
the rtdp version of singh and cohn is in the stopping 
criteria singh and cohn proposed that the algorithm terminates 
when only one competitive action remains for each state or 
when the range of all competitive actions for any state are 
bounded by an indifference parameter bounded-rtdp 
labels states for which u s − l s as solved and the 
convergence is reached when s is solved or when only one 
competitive action remains for each state this stopping 
criteria is more effective since it is similar to the one used 
by smith and simmons and mcmahan et al brtdp 
in this paper the bounds defined by singh and cohn and 
implemented using bounded-rtdp define the singh-rtdp 
approach the next sections propose to tighten the bounds 
of singh-rtdp to permit a more effective pruning of the 
action space 
 reducing the upper bound 
singhu includes actions which may not be possible to 
execute because of resource constraints which overestimates 
the upper bound to consider only possible actions our 
upper bound named maxu is introduced 
hu s max 
a∈a s 
ta∈t a 
qta ata sta 
where qta ata sta is the q-value of task ta for state sta 
and action ata computed using a standard lrtdp approach 
theorem the upper bound defined by equation is 
admissible 
proof the local resource constraints are satisfied because 
the upper bound is computed using all global possible 
actions a however hu s still overestimates v s because 
the global resource constraint is not enforced indeed each 
task may use all consumable resources for its own purpose 
doing this produces a higher value for each task than the 
one obtained when planning for all tasks globally with the 
shared limited resources 
computing the maxu bound in a state has a 
complexity of o a × t a and o t a for singhu a standard 
bellman backup has a complexity of o a × s since 
 a × t a a × s the computation time to determine the 
upper bound of a state which is done one time for each 
visited state is much less than the computation time required 
to compute a standard bellman backup for a state which is 
usually done many times for each visited state thus the 
computation time of the upper bound is negligible 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 increasing the lower bound 
the idea to increase singhl is to allocate the resources 
a priori among the tasks when each task has its own set 
of resources each task may be solved independently the 
lower bound of state s is hl s 
ta∈t a 
lowta sta where 
lowta sta is a value function for each task ta ∈ t a such 
that the resources have been allocated a priori the 
allocation a priori of the resources is made using marginal revenue 
which is a highly used concept in microeconomics and 
has recently been used for coordination of a decentralized 
mdp in brief marginal revenue is the extra revenue that 
an additional unit of product will bring to a firm thus 
for a stochastic resource allocation problem the marginal 
revenue of a resource is the additional expected value it 
involves the marginal revenue of a resource res for a task ta 
in a state sta is defined as following 
mrta sta max 
ata∈a sta 
qta ata sta − 
max 
ata∈a sta 
qta ata res ∈ ata sta 
the concept of marginal revenue of a resource is used in 
algorithm to allocate the resources a priori among the 
tasks which enables to define the lower bound value of a 
state in line of the algorithm a value function is 
computed for all tasks in the environment using a standard 
lrtdp approach these value functions which are also 
used for the upper bound are computed considering that 
each task may use all available resources the line 
initializes the valueta variable this variable is the estimated 
value of each task ta ∈ t a in the beginning of the 
algorithm no resources are allocated to a specific task thus the 
valueta variable is initialized to for all ta ∈ t a then in 
line a resource type res consumable or non-consumable 
is selected to be allocated here a domain expert may 
separate all available resources in many types or parts to be 
allocated the resources are allocated in the order of its 
specialization in other words the more a resource is 
efficient on a small group of tasks the more it is allocated early 
allocating the resources in this order improves the quality 
of the resulting lower bound the line computes the 
marginal revenue of a consumable resource res for each task 
ta ∈ t a for a non-consumable resource since the resource 
is not considered in the state space all other reachable states 
from sta consider that the resource res is still usable the 
approach here is to sum the difference between the real value 
of a state to the maximal q-value of this state if resource res 
cannot be used for all states in a trajectory given by the 
policy of task ta this heuristic proved to obtain good results 
but other ones may be tried for example monte-carlo 
simulation in line the marginal revenue is updated in 
function of the resources already allocated to each task r sgta 
is the reward to realize task ta thus vta sta −valueta 
r sgta 
is 
the residual expected value that remains to be achieved 
knowing current allocation to task ta and normalized by 
the reward of realizing the tasks the marginal revenue is 
multiplied by this term to indicate that the more a task 
has a high residual value the more its marginal revenue is 
going to be high then a task ta is selected in line with 
the highest marginal revenue adjusted with residual value 
in line the resource type res is allocated to the group 
of resources resta of task ta afterwards line 
recomalgorithm the marginal revenue lower bound algorithm 
 function revenue-bound s 
 returns a lower bound lowt a 
 for all ta ∈ t a do 
 vta ←lrtdp sta 
 valueta ← 
 end for 
 s ← s 
 repeat 
 res ← select a resource type res ∈ res 
 for all ta ∈ t a do 
 if res is consumable then 
 mrta sta ← vta sta − vta sta res \ res 
 else 
 mrta sta ← 
 repeat 
 mrta sta ← mrta sta 
vta sta max 
 ata∈a sta res ∈ata 
qta ata sta 
 sta ← sta picknextstate resc 
 until sta is a goal 
 s ← s 
 end if 
 mrrvta sta ← mrta sta × vta sta −valueta 
r sgta 
 end for 
 ta ← task ta ∈ t a which maximize mrrvta sta 
 resta ← resta {res} 
 temp ← ∅ 
 if res is consumable then 
 temp ← res 
 end if 
 valueta ← valueta vta sta − valueta × 
max 
ata∈a sta res 
qta ata sta temp 
vta sta 
 
 until all resource types res ∈ res are assigned 
 for all ta ∈ t a do 
 lowta ←lrtdp sta resta 
 end for 
 return lowt a 
putes valueta the first part of the equation to compute 
valueta represents the expected residual value for task ta 
this term is multiplied by 
max 
ata∈a sta 
qta ata sta res 
vta sta 
 which 
is the ratio of the efficiency of resource type res in other 
words valueta is assigned to valueta the residual value 
× the value ratio of resource type res for a consumable 
resource the q-value consider only resource res in the state 
space while for a non-consumable resource no resources are 
available 
all resource types are allocated in this manner until res is 
empty all consumable and non-consumable resource types 
are allocated to each task when all resources are allocated 
the lower bound components lowta of each task are 
computed in line when the global solution is computed 
the lower bound is as follow 
hl s max singhl max 
a∈a s 
ta∈t a 
lowta sta 
we use the maximum of the singhl bound and the sum 
of the lower bound components lowta thus 
marginalrevenue ≥ singhl in particular the singhl bound may 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
be higher when a little number of tasks remain as the 
components lowta are computed considering s for example if 
in a subsequent state only one task remains the bound of 
singhl will be higher than any of the lowta components 
the main difference of complexity between singhl and 
revenue-bound is in line where a value for each task 
has to be computed with the shared resource however 
since the resource are shared the state space and action 
space is greatly reduced for each task reducing greatly the 
calculus compared to the value functions computed in line 
 which is done for both singhl and revenue-bound 
theorem the lower bound of equation is 
admissible 
proof lowta sta is computed with the resource being 
shared summing the lowta sta value functions for each 
ta ∈ t a does not violates the local and global resource 
constraints indeed as the resources are shared the tasks 
cannot overuse them thus hl s is a realizable policy and an 
admissible lower bound 
 discussion and experiments 
the domain of the experiments is a naval platform which 
must counter incoming missiles i e tasks by using its 
resources i e weapons movements for the experiments 
 randomly resource allocation problems were generated 
for each approach and possible number of tasks in our 
problem sta thus each task can be in four distinct 
states there are two types of states firstly states where 
actions modify the transition probabilities and then there 
are goal states the state transitions are all stochastic 
because when a missile is in a given state it may always transit 
in many possible states in particular each resource type 
has a probability to counter a missile between and 
depending on the state of the task when a missile is not 
countered it transits to another state which may be 
preferred or not to the current state where the most preferred 
state for a task is when it is countered the effectiveness 
of each resource is modified randomly by ± at the start 
of a scenario there are also local and global resource 
constraints on the amount that may be used for the local 
constraints at most resource of each type can be 
allocated to execute tasks in a specific state this constraint is 
also present on a real naval platform because of sensor and 
launcher constraints and engagement policies furthermore 
for consumable resources the total amount of available 
consumable resource is between and for each type the 
global constraint is generated randomly at the start of a 
scenario for each consumable resource type the number of 
resource type has been fixed to where there are 
consumable resource types and non-consumable resources types 
for this problem a standard lrtdp approach has been 
implemented a simple heuristic has been used where the 
value of an unvisited state is assigned as the value of a goal 
state such that all tasks are achieved this way the value 
of each unvisited state is assured to overestimate its real 
value since the value of achieving a task ta is the highest 
the planner may get for ta since this heuristic is pretty 
straightforward the advantages of using better heuristics are 
more evident nevertheless even if the lrtdp approach uses 
a simple heuristic still a huge part of the state space is not 
visited when computing the optimal policy the approaches 
described in this paper are compared in figures and 
lets summarize these approaches here 
 qdec-lrtdp the backups are computed using the 
qdec-backup function algorithm but in a lrtdp 
context in particular the updates made in the 
checksolved function are also made using the the 
qdecbackup function 
 lrtdp-up the upper bound of maxu is used for 
lrtdp 
 singh-rtdp the singhl and singhu bounds are 
used for bounded-rtdp 
 mr-rtdp the revenue-bound and maxu bounds 
are used for bounded-rtdp 
to implement qdec-lrtdp we divided the set of tasks 
in two equal parts the set of task t ai managed by agent 
i can be accomplished with the set of resources resi while 
the second set of task t ai managed by agent agi can be 
accomplished with the set of resources resi resi had one 
consumable resource type and one non-consumable resource 
type while resi had two consumable resource types and 
one non-consumable resource type when the number of 
tasks is odd one more task was assigned to t ai there are 
constraint between the group of resource resi and resi 
such that some assignments are not possible these 
constraints are managed by the arbitrator as described in 
section q-decomposition permits to diminish the planning 
time significantly in our problem settings and seems a very 
efficient approach when a group of agents have to allocate 
resources which are only available to themselves but the 
actions made by an agent may influence the reward obtained 
by at least another agent 
to compute the lower bound of revenue-bound all 
available resources have to be separated in many types or 
parts to be allocated for our problem we allocated each 
resource of each type in the order of of its specialization like 
we said when describing the revenue-bound function 
in terms of experiments notice that the lrtdp lrtdp-up 
and approaches for resource allocation which doe not prune 
the action space are much more complex for instance it 
took an average of seconds to plan for the lrtdp-up 
approach with six tasks see figure the singh-rtdp 
approach diminished the planning time by using a lower 
and upper bound to prune the action space mr-rtdp 
further reduce the planning time by providing very tight 
initial bounds in particular singh-rtdp needed seconds 
in average to solve problem with six tasks and mr-rtdp 
required seconds indeed the time reduction is quite 
significant compared to lrtdp-up which demonstrates the 
efficiency of using bounds to prune the action space 
furthermore we implemented mr-rtdp with the singhu 
bound and this was slightly less efficient than with the 
maxu bound we also implemented mr-rtdp with the 
singhl bound and this was slightly more efficient than 
singh-rtdp from these results we conclude that the 
difference of efficiency between mr-rtdp and singh-rtdp is 
more attributable to the marginal-revenue lower bound 
that to the maxu upper bound indeed when the number 
of task to execute is high the lower bounds by singh-rtdp 
takes the values of a single task on the other hand the 
lower bound of mr-rtdp takes into account the value of all 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
 
 
 
 
 
 
 
timeinseconds 
number of tasks 
lrtdp 
qdec-lrtdp 
figure efficiency of q-decomposition lrtdp 
and lrtdp 
 
 
 
 
 
 
 
 
timeinseconds 
number of tasks 
lrtdp 
lrtdp-up 
singh-rtdp 
mr-rtdp 
figure efficiency of mr-rtdp compared to 
singh-rtdp 
task by using a heuristic to distribute the resources 
indeed an optimal allocation is one where the resources are 
distributed in the best way to all tasks and our lower bound 
heuristically does that 
 conclusion 
the experiments have shown that q-decomposition seems 
a very efficient approach when a group of agents have to 
allocate resources which are only available to themselves 
but the actions made by an agent may influence the reward 
obtained by at least another agent 
on the other hand when the available resource are 
shared no q-decomposition is possible and we proposed 
tight bounds for heuristic search in this case the 
planning time of bounded-rtdp which prunes the action space 
is significantly lower than for lrtdp furthermore the 
marginal revenue bound proposed in this paper compares 
favorably to the singh and cohn approach 
boundedrtdp with our proposed bounds may apply to a wide range 
of stochastic environments the only condition for the use 
our bounds is that each task possesses consumable and or 
non-consumable limited resources 
an interesting research avenue would be to experiment 
our bounds with other heuristic search algorithms for 
instance frtdp and brtdp are both efficient 
heuristic search algorithms in particular both these approaches 
proposed an efficient state trajectory updates when given 
upper and lower bounds our tight bounds would enable 
for both frtdp and brtdp to reduce the number of backup 
to perform before convergence finally the bounded-rtdp 
function prunes the action space when qu a s ≤ l s as 
singh and cohn suggested frtdp and brtdp could 
also prune the action space in these circumstances to 
further reduce their planning time 
 references 
 a barto s bradtke and s singh learning to act 
using real-time dynamic programming artificial 
intelligence - 
 a beynier and a i mouaddib an iterative algorithm 
for solving constrained decentralized markov decision 
processes in proceeding of the twenty-first national 
conference on artificial intelligence aaai- 
 b bonet and h geffner faster heuristic search 
algorithms for planning with uncertainty and full 
feedback in proceedings of the eighteenth 
international joint conference on artificial 
intelligence ijcai- august 
 b bonet and h geffner labeled lrtdp approach 
improving the convergence of real-time dynamic 
programming in proceeding of the thirteenth 
international conference on automated planning 
scheduling icaps- pages - trento italy 
 
 e a hansen and s zilberstein lao a heuristic 
search algorithm that finds solutions with loops 
artificial intelligence - - 
 h b mcmahan m likhachev and g j gordon 
bounded real-time dynamic programming rtdp with 
monotone upper bounds and performance guarantees 
in icml proceedings of the twenty-second 
international conference on machine learning pages 
 - new york ny usa acm press 
 r s pindyck and d l rubinfeld microeconomics 
prentice hall 
 g a rummery and m niranjan on-line q-learning 
using connectionist systems technical report 
cued finfeng tr cambridge university 
engineering department 
 s j russell and a zimdars q-decomposition for 
reinforcement learning agents in icml pages 
 - 
 s singh and d cohn how to dynamically merge 
markov decision processes in advances in neural 
information processing systems volume pages 
 - cambridge ma usa mit press 
 t smith and r simmons focused real-time dynamic 
programming for mdps squeezing more out of a 
heuristic in proceedings of the twenty-first national 
conference on artificial intelligence aaai boston 
usa 
 w zhang modeling and solving a resource allocation 
problem with soft constraint techniques technical 
report wucs- - washington university 
saint-louis missouri 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
dynamics based control 
with an application to area-sweeping problems 
zinovi rabinovich 
engineering and computer 
science 
hebrew university of 
jerusalem 
jerusalem israel 
nomad cs huji ac il 
jeffrey s rosenschein 
engineering and computer 
science 
hebrew university of 
jerusalem 
jerusalem israel 
jeff cs huji ac il 
gal a kaminka 
the maverick group 
department of computer 
science 
bar ilan university israel 
galk cs biu ac il 
abstract 
in this paper we introduce dynamics based control dbc an 
approach to planning and control of an agent in stochastic 
environments unlike existing approaches which seek to optimize 
expected rewards e g in partially observable markov decision 
problems pomdps dbc optimizes system behavior towards 
specified system dynamics we show that a recently developed planning 
and control approach extended markov tracking emt is an 
instantiation of dbc emt employs greedy action selection to 
provide an efficient control algorithm in markovian environments we 
exploit this efficiency in a set of experiments that applied 
multitarget emt to a class of area-sweeping problems searching for 
moving targets we show that such problems can be naturally 
defined and efficiently solved using the dbc framework and its emt 
instantiation 
categories and subject descriptors 
i problem solving control methods and search 
control theory i robotics i distributed artificial 
intelligence intelligent agents 
general terms 
algorithms theory 
 introduction 
planning and control constitutes a central research area in 
multiagent systems and artificial intelligence in recent years partially 
observable markov decision processes pomdps have 
become a popular formal basis for planning in stochastic 
environments in this framework the planning and control problem is often 
addressed by imposing a reward function and computing a policy 
 of choosing actions that is optimal in the sense that it will result 
in the highest expected utility while theoretically attractive the 
complexity of optimally solving a pomdp is prohibitive 
we take an alternative view of planning in stochastic 
environments we do not use a state-based reward function but instead 
optimize over a different criterion a transition-based specification 
of the desired system dynamics the idea here is to view 
planexecution as a process that compels a stochastic system to change 
and a plan as a dynamic process that shapes that change according 
to desired criteria we call this general planning framework 
dynamics based control dbc 
in dbc the goal of a planning or control process becomes to 
ensure that the system will change in accordance with specific 
 potentially stochastic target dynamics as actual system behavior 
may deviate from that which is specified by target dynamics due 
to the stochastic nature of the system planning in such 
environments needs to be continual in a manner similar to classical 
closed-loop controllers here optimality is measured in terms 
of probability of deviation magnitudes 
in this paper we present the structure of dynamics based 
control we show that the recently developed extended markov 
tracking emt approach is subsumed by dbc with emt 
employing greedy action selection which is a specific 
parameterization among the options possible within dbc emt is an efficient 
instantiation of dbc 
to evaluate dbc we carried out a set of experiments applying 
multi-target emt to the tag game this is a variant on the 
area sweeping problem where an agent is trying to tag a moving 
target quarry whose position is not known with certainty 
experimental data demonstrates that even with a simple model of the 
environment and a simple design of target dynamics high success 
rates can be produced both in catching the quarry and in surprising 
the quarry as expressed by the observed entropy of the controlled 
agent s position 
the paper is organized as follows in section we motivate dbc 
using area-sweeping problems and discuss related work section 
introduces the dynamics based control dbc structure and its 
specialization to markovian environments this is followed by a 
review of the extended markov tracking emt approach as a 
dbc-structured control regimen in section that section also 
discusses the limitations of emt-based control relative to the 
general dbc framework experimental settings and results are then 
presented in section section provides a short discussion of 
the overall approach and section gives some concluding remarks 
and directions for future work 
 
 - - - - rps c ifaamas 
 motivation and related work 
many real-life scenarios naturally have a stochastic target 
dynamics specification especially those domains where there exists 
no ultimate goal but rather system behavior with specific 
properties that has to be continually supported for example security 
guards perform persistent sweeps of an area to detect any sign of 
intrusion cunning thieves will attempt to track these sweeps and 
time their operation to key points of the guards motion it is thus 
advisable to make the guards motion dynamics appear irregular 
and random 
recent work by paruchuri et al has addressed such 
randomization in the context of single-agent and distributed pomdps the 
goal in that work was to generate policies that provide a measure of 
action-selection randomization while maintaining rewards within 
some acceptable levels our focus differs from this work in that 
dbc does not optimize expected rewards-indeed we do not 
consider rewards at all-but instead maintains desired dynamics 
 including but not limited to randomization 
the game of tag is another example of the applicability of the 
approach it was introduced in the work by pineau et al there 
are two agents that can move about an area which is divided into a 
grid the grid may have blocked cells holes into which no agent 
can move one agent the hunter seeks to move into a cell 
occupied by the other the quarry such that they are co-located this is 
a successful tag the quarry seeks to avoid the hunter agent and 
is always aware of the hunter s position but does not know how the 
hunter will behave which opens up the possibility for a hunter to 
surprise the prey the hunter knows the quarry s probabilistic law 
of motion but does not know its current location tag is an instance 
of a family of area-sweeping pursuit-evasion problems 
in the hunter modeled the problem using a pomdp a 
reward function was defined to reflect the desire to tag the quarry 
and an action policy was computed to optimize the reward 
collected over time due to the intractable complexity of determining 
the optimal policy the action policy computed in that paper was 
essentially an approximation 
in this paper instead of formulating a reward function we use 
emt to solve the problem by directly specifying the target 
dynamics in fact any search problem with randomized motion the 
socalled class of area sweeping problems can be described through 
specification of such target system dynamics dynamics based 
control provides a natural approach to solving these problems 
 dynamics based control 
the specification of dynamics based control dbc can be 
broken into three interacting levels environment design level user 
level and agent level 
 environment design level is concerned with the formal 
specification and modeling of the environment for 
example this level would specify the laws of physics within the 
system and set its parameters such as the gravitation 
constant 
 user level in turn relies on the environment model produced 
by environment design to specify the target system 
dynamics it wishes to observe the user level also specifies the 
estimation or learning procedure for system dynamics and the 
measure of deviation in the museum guard scenario above 
these would correspond to a stochastic sweep schedule and a 
measure of relative surprise between the specified and actual 
sweeping 
 agent level in turn combines the environment model from 
the environment design level the dynamics estimation 
procedure the deviation measure and the target dynamics 
specification from user level to produce a sequence of actions 
that create system dynamics as close as possible to the 
targeted specification 
as we are interested in the continual development of a stochastic 
system such as happens in classical control theory and 
continual planning as well as in our example of museum sweeps 
the question becomes how the agent level is to treat the 
deviation measurements over time to this end we use a probability 
threshold-that is we would like the agent level to maximize the 
probability that the deviation measure will remain below a certain 
threshold 
specific action selection then depends on system formalization 
one possibility would be to create a mixture of available system 
trends much like that which happens in behavior-based robotic 
architectures the other alternative would be to rely on the 
estimation procedure provided by the user level-to utilize the 
environment design level model of the environment to choose actions 
so as to manipulate the dynamics estimator into believing that a 
certain dynamics has been achieved notice that this manipulation is 
not direct but via the environment thus for strong enough 
estimator algorithms successful manipulation would mean a successful 
simulation of the specified target dynamics i e beyond discerning 
via the available sensory input 
dbc levels can also have a back-flow of information see 
figure for instance the agent level could provide data about 
target dynamics feasibility allowing the user level to modify the 
requirement perhaps focusing on attainable features of system 
behavior data would also be available about the system response to 
different actions performed combined with a dynamics estimator 
defined by the user level this can provide an important tool for the 
environment model calibration at the environment design level 
userenv design agent 
model 
ideal dynamics 
estimator 
estimator 
dynamics feasibility 
system response data 
figure data flow of the dbc framework 
extending upon the idea of actor-critic algorithms dbc 
data flow can provide a good basis for the design of a learning 
algorithm for example the user level can operate as an exploratory 
device for a learning algorithm inferring an ideal dynamics target 
from the environment model at hand that would expose and verify 
most critical features of system behavior in this case feasibility 
and system response data from the agent level would provide key 
information for an environment model update in fact the 
combination of feasibility and response data can provide a basis for the 
application of strong learning algorithms such as em 
 dbc for markovian environments 
for a partially observable markovian environment dbc can 
be specified in a more rigorous manner notice how dbc discards 
rewards and replaces it by another optimality criterion structural 
differences are summarized in table 
 environment design level is to specify a tuple 
 s a t o ω s where 
- s is the set of all possible environment states 
- s is the initial state of the environment which can also 
be viewed as a probability distribution over s 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
- a is the set of all possible actions applicable in the 
environment 
- t is the environment s probabilistic transition function 
t s ×a → π s that is t s a s is the 
probability that the environment will move from state s to state 
s under action a 
- o is the set of all possible observations this is what 
the sensor input would look like for an outside observer 
- ω is the observation probability function 
ω s × a × s → π o 
that is ω o s a s is the probability that one will 
observe o given that the environment has moved from 
state s to state s under action a 
 user level in the case of markovian environment operates 
on the set of system dynamics described by a family of 
conditional probabilities f {τ s × a → π s } thus 
specification of target dynamics can be expressed by q ∈ f 
and the learning or tracking algorithm can be represented as 
a function l o× a×o ∗ 
→ f that is it maps sequences 
of observations and actions performed so far into an estimate 
τ ∈ f of system dynamics 
there are many possible variations available at the user level 
to define divergence between system dynamics several of 
them are 
- trace distance or l distance between two 
distributions p and q defined by 
d p · q · 
 
 x 
 p x − q x 
- fidelity measure of distance 
f p · q · 
x 
p x q x 
- kullback-leibler divergence 
dkl p · q · 
x 
p x log 
p x 
q x 
notice that the latter two are not actually metrics over the 
space of possible distributions but nevertheless have 
meaningful and important interpretations for instance 
kullbackleibler divergence is an important tool of information 
theory that allows one to measure the price of encoding an 
information source governed by q while assuming that it is 
governed by p 
the user level also defines the threshold of dynamics 
deviation probability θ 
 agent level is then faced with a problem of selecting a 
control signal function a∗ 
to satisfy a minimization problem as 
follows 
a∗ 
 arg min 
a 
pr d τa q θ 
where d τa q is a random variable describing deviation of 
the dynamics estimate τa created by l under control signal 
a from the ideal dynamics q implicit in this minimization 
problem is that l is manipulated via the environment based 
on the environment model produced by the environment 
design level 
 dbc view of the state space 
it is important to note the complementary view that dbc and 
pomdps take on the state space of the environment pomdps 
regard state as a stationary snap-shot of the environment 
whatever attributes of state sequencing one seeks are reached through 
properties of the control process in this case reward accumulation 
this can be viewed as if the sequencing of states and the attributes 
of that sequencing are only introduced by and for the controlling 
mechanism-the pomdp policy 
dbc concentrates on the underlying principle of state 
sequencing the system dynamics dbc s target dynamics specification can 
use the environment s state space as a means to describe discern 
and preserve changes that occur within the system as a result 
dbc has a greater ability to express state sequencing properties 
which are grounded in the environment model and its state space 
definition 
for example consider the task of moving through rough terrain 
towards a goal and reaching it as fast as possible pomdps would 
encode terrain as state space points while speed would be ensured 
by negative reward for every step taken without reaching the 
goalaccumulating higher reward can be reached only by faster motion 
alternatively the state space could directly include the notion of 
speed for pomdps this would mean that the same concept is 
encoded twice in some sense directly in the state space and 
indirectly within reward accumulation now even if the reward 
function would encode more and finer details of the properties of 
motion the pomdp solution will have to search in a much larger 
space of policies while still being guided by the implicit concept 
of the reward accumulation procedure 
on the other hand the tactical target expression of variations and 
correlations between position and speed of motion is now grounded 
in the state space representation in this situation any further 
constraints e g smoothness of motion speed limits in different 
locations or speed reductions during sharp turns are explicitly and 
uniformly expressed by the tactical target and can result in faster 
and more effective action selection by a dbc algorithm 
 emt-based control as a dbc 
recently a control algorithm was introduced called emt-based 
control which instantiates the dbc framework although it 
provides an approximate greedy solution in the dbc sense initial 
experiments using emt-based control have been encouraging 
 emt-based control is based on the markovian environment 
definition as in the case with pomdps but its user and agent 
levels are of the markovian dbc type of optimality 
 user level of emt-based control defines a limited-case 
target system dynamics independent of action 
qemt s → π s 
it then utilizes the kullback-leibler divergence measure to 
compose a momentary system dynamics estimator-the 
extended markov tracking emt algorithm the algorithm 
keeps a system dynamics estimate τt 
emt that is capable of 
explaining recent change in an auxiliary bayesian system 
state estimator from pt− to pt and updates it conservatively 
using kullback-leibler divergence since τt 
emt and pt− t 
are respectively the conditional and marginal probabilities 
over the system s state space explanation simply means 
that 
pt s 
s 
τt 
emt s s pt− s 
and the dynamics estimate update is performed by solving a 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
table structure of pomdp vs dynamics-based control in markovian environment 
level 
approach 
mdp markovian dbc 
environment 
 s a t o ω where 
s - set of states 
a - set of actions 
design 
t s × a → π s - transition 
o - observation set 
ω s × a × s → π o 
user 
r s × a × s → r q s × a → π s 
f π∗ 
 → r l o ot → τ 
r - reward function q - ideal dynamics 
f - reward remodeling l - dynamics estimator 
θ - threshold 
agent π∗ 
 arg max 
π 
e γt 
rt π∗ 
 arg min 
π 
prob d τ q θ 
minimization problem 
τt 
emt h pt pt− τt− 
emt 
 arg min 
τ 
dkl τ × pt− τt− 
emt × pt− 
s t 
pt s 
s 
 τ × pt− s s 
pt− s 
s 
 τ × pt− s s 
 agent level in emt-based control is suboptimal with 
respect to dbc though it remains within the dbc 
framework performing greedy action selection based on 
prediction of emt s reaction the prediction is based on the 
environment model provided by the environment design level 
so that if we denote by ta the environment s transition 
function limited to action a and pt− is the auxiliary bayesian 
system state estimator then the emt-based control choice is 
described by 
a∗ 
 arg min 
a∈a 
dkl h ta × pt pt τt 
emt qemt × pt− 
note that this follows the markovian dbc framework precisely 
the rewarding optimality of pomdps is discarded and in its place 
a dynamics estimator emt in this case is manipulated via action 
effects on the environment to produce an estimate close to the 
specified target system dynamics yet as we mentioned naive 
emtbased control is suboptimal in the dbc sense and has several 
additional limitations that do not exist in the general dbc framework 
 discussed in section 
 multi-target emt 
at times there may exist several behavioral preferences for 
example in the case of museum guards some art items are more 
heavily guarded requiring that the guards stay more often in their 
close vicinity on the other hand no corner of the museum is to 
be left unchecked which demands constant motion successful 
museum security would demand that the guards adhere to and 
balance both of these behaviors for emt-based control this would 
mean facing several tactical targets {qk}k 
k and the question 
becomes how to merge and balance them a balancing mechanism 
can be applied to resolve this issue 
note that emt-based control while selecting an action creates 
a preference vector over the set of actions based on their predicted 
performance with respect to a given target if these preference 
vectors are normalized they can be combined into a single unified 
preference this requires replacement of standard emt-based action 
selection by the algorithm below 
 given 
- a set of target dynamics {qk}k 
k 
- vector of weights w k 
 select action as follows 
- for each action a ∈ a predict the future state 
distribution ¯pa 
t ta ∗ pt 
- for each action compute 
da h ¯pa 
t pt pdt 
- for each a ∈ a and qk tactical target denote 
v a k dkl da qk pt 
 
let vk a 
zk 
v a k where zk 
a∈a 
v a k is 
a normalization factor 
- select a∗ 
 arg min 
a 
k 
k w k vk a 
the weights vector w w wk allows the additional 
tuning of importance among target dynamics without the need 
to redesign the targets themselves this balancing method is also 
seamlessly integrated into the emt-based control flow of 
operation 
 emt-based control limitations 
emt-based control is a sub-optimal in the dbc sense 
representative of the dbc structure it limits the user by forcing emt to 
be its dynamic tracking algorithm and replaces agent optimization 
by greedy action selection this kind of combination however is 
common for on-line algorithms although further development of 
emt-based controllers is necessary evidence so far suggests that 
even the simplest form of the algorithm possesses a great deal of 
power and displays trends that are optimal in the dbc sense of the 
word 
there are two further emt-specific limitations to emt-based 
control that are evident at this point both already have partial 
solutions and are subjects of ongoing research 
the first limitation is the problem of negative preference in the 
pomdp framework for example this is captured simply through 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
the appearance of values with different signs within the reward 
structure for emt-based control however negative preference 
means that one would like to avoid a certain distribution over 
system development sequences emt-based control however 
concentrates on getting as close as possible to a distribution avoidance is 
thus unnatural in native emt-based control 
the second limitation comes from the fact that standard 
environment modeling can create pure sensory actions-actions that do 
not change the state of the world and differ only in the way 
observations are received and the quality of observations received since 
the world state does not change emt-based control would not be 
able to differentiate between different sensory actions 
notice that both of these limitations of emt-based control are 
absent from the general dbc framework since it may have a 
tracking algorithm capable of considering pure sensory actions and 
unlike kullback-leibler divergence a distribution deviation measure 
that is capable of dealing with negative preference 
 emt playing tag 
the game of tag was first introduced in it is a single agent 
problem of capturing a quarry and belongs to the class of area 
sweeping problems an example domain is shown in figure 
 
 
 
 
 
 
 q a 
 
figure tag domain an agent a attempts to seek and 
capture a quarry q 
the game of tag extremely limits the agent s perception so that 
the agent is able to detect the quarry only if they are co-located in 
the same cell of the grid world in the classical version of the game 
co-location leads to a special observation and the  tag action can 
be performed we slightly modify this setting the moment that 
both agents occupy the same cell the game ends as a result both 
the agent and its quarry have the same motion capability which 
allows them to move in four directions north south east and 
west these form a formal space of actions within a markovian 
environment 
the state space of the formal markovian environment is described 
by the cross-product of the agent and quarry s positions for 
figure it would be s {s s } × {s s } 
the effects of an action taken by the agent are deterministic but 
the environment in general has a stochastic response due to the 
motion of the quarry with probability q 
 
it stays put and with 
probability − q it moves to an adjacent cell further away from the 
 
in our experiments this was taken to be q 
agent so for the instance shown in figure and q 
p q s q s a s 
p q s q s a s 
p q s q s a s 
p q s q s a s 
although the evasive behavior of the quarry is known to the 
agent the quarry s position is not the only sensory information 
available to the agent is its own location 
we use emt and directly specify the target dynamics for the 
game of tag we can easily formulate three major trends catching 
the quarry staying mobile and stalking the quarry this results in 
the following three target dynamics 
tcatch at si qt sj at sa ∝ 
 si sj 
 otherwise 
tmobile at si qt so at sj ∝ 
 si sj 
 otherwise 
tstalk at si qt so at sj ∝ 
dist si so 
note that none of the above targets are directly achievable for 
instance if qt s and at s there is no action that can move 
the agent to at s as required by the tcatch target dynamics 
we ran several experiments to evaluate emt performance in the 
tag game three configurations of the domain shown in figure 
were used each posing a different challenge to the agent due to 
partial observability in each setting a set of runs was performed 
with a time limit of steps in every run the initial position of 
both the agent and its quarry was selected at random this means 
that as far as the agent was concerned the quarry s initial position 
was uniformly distributed over the entire domain cell space 
we also used two variations of the environment observability 
function in the first version observability function mapped all 
joint positions of hunter and quarry into the position of the hunter as 
an observation in the second only those joint positions in which 
hunter and quarry occupied different locations were mapped into 
the hunter s location the second version in fact utilized and 
expressed the fact that once hunter and quarry occupy the same cell 
the game ends 
the results of these experiments are shown in table 
balancing the catch move and stalk target dynamics described in 
the previous section by the weight vector emt 
produced stable performance in all three domains 
although direct comparisons are difficult to make the emt 
performance displayed notable efficiency vis-`a-vis the pomdp 
approach in spite of a simple and inefficient matlab implementation 
of the emt algorithm the decision time for any given step 
averaged significantly below second in all experiments for the 
irregular open arena domain which proved to be the most difficult 
experiment runs bounded by steps each a total of steps 
were completed in slightly under hours that is over × 
online steps took an order of magnitude less time than the offline 
computation of pomdp policy in the significance of this 
differential is made even more prominent by the fact that should the 
environment model parameters change the online nature of emt 
would allow it to maintain its performance time while the pomdp 
policy would need to be recomputed requiring yet again a large 
overhead of computation 
we also tested the behavior cell frequency entropy empirical 
measures from trial data as figure and figure show 
empir the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
a 
q 
q a 
 
 
 
 
 
a 
q 
figure these configurations of the tag game space were used a multiple dead-end b irregular open arena c circular corridor 
table performance of the emt-based solution in three tag 
game domains and two observability models i omniposition 
quarry ii quarry is not at hunter s position 
model domain capture e steps time step 
i 
dead-ends msec 
arena msec 
circle msec 
ii 
dead-ends msec 
arena msec 
circle msec 
ical entropy grows with the length of interaction for runs where 
the quarry was not captured immediately the entropy reaches 
between and 
for different runs and scenarios as the agent 
actively seeks the quarry the entropy never reaches its maximum 
one characteristic of the entropy graph for the open arena 
scenario particularly caught our attention in the case of the 
omniposition quarry observation model near the maximum limit of trial 
length steps entropy suddenly dropped further analysis of 
the data showed that under certain circumstances a fluctuating 
behavior occurs in which the agent faces equally viable versions of 
quarry-following behavior since the emt algorithm has greedy 
action selection and the state space does not encode any form of 
commitment not even speed or acceleration the agent is locked 
within a small portion of cells it is essentially attempting to 
simultaneously follow several courses of action all of which are 
consistent with the target dynamics this behavior did not occur in our 
second observation model since it significantly reduced the set of 
eligible courses of action-essentially contributing to tie-breaking 
among them 
 discussion 
the design of the emt solution for the tag game exposes the 
core difference in approach to planning and control between emt 
or dbc on the one hand and the more familiar pomdp approach 
on the other pomdp defines a reward structure to optimize and 
influences system dynamics indirectly through that optimization 
emt discards any reward scheme and instead measures and 
influences system dynamics directly 
 
entropy was calculated using log base equal to the number of 
possible locations within the domain this properly scales entropy 
expression into the range for all domains 
thus for the tag game we did not search for a reward function 
that would encode and express our preference over the agent s 
behavior but rather directly set three heuristic behavior preferences 
as the basis for target dynamics to be maintained experimental 
data shows that these targets need not be directly achievable via the 
agent s actions however the ratio between emt performance and 
achievability of target dynamics remains to be explored 
the tag game experiment data also revealed the different 
emphasis dbc and pomdps place on the formulation of an environment 
state space pomdps rely entirely on the mechanism of reward 
accumulation maximization i e formation of the action selection 
procedure to achieve necessary state sequencing dbc on the 
other hand has two sources of sequencing specification through 
the properties of an action selection procedure and through direct 
specification within the target dynamics the importance of the 
second source was underlined by the tag game experiment data 
in which the greedy emt algorithm applied to a pomdp-type 
state space specification failed since target description over such a 
state space was incapable of encoding the necessary behavior 
tendencies e g tie-breaking and commitment to directed motion 
the structural differences between dbc and emt in 
particular and pomdps prohibits direct performance comparison and 
places them on complementary tracks each within a suitable niche 
for instance pomdps could be seen as a much more natural 
formulation of economic sequential decision-making problems while 
emt is a better fit for continual demand for stochastic change as 
happens in many robotic or embodied-agent problems 
the complementary properties of pomdps and emt can be 
further exploited there is recent interest in using pomdps in hybrid 
solutions in which the pomdps can be used together with 
other control approaches to provide results not easily achievable 
with either approach by itself dbc can be an effective partner in 
such a hybrid solution for instance pomdps have prohibitively 
large off-line time requirements for policy computation but can 
be readily used in simpler settings to expose beneficial behavioral 
trends this can serve as a form of target dynamics that are provided 
to emt in a larger domain for on-line operation 
 conclusions and future work 
in this paper we have presented a novel perspective on the 
process of planning and control in stochastic environments in the form 
of the dynamics based control dbc framework dbc 
formulates the task of planning as support of a specified target system 
dynamics which describes the necessary properties of change within 
the environment optimality of dbc plans of action are measured 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
 
 
 
 
 
 
 
 
steps 
entropy 
dead−ends 
 
 
 
 
 
 
 
 
 
 
steps 
entropy 
arena 
 
 
 
 
 
 
 
 
 
 
steps 
entropy 
circle 
figure observation model i omniposition quarry entropy development with length of tag game for the three experiment 
scenarios a multiple dead-end b irregular open arena c circular corridor 
 
 
 
 
 
 
 
 
 
 
steps 
entropy 
dead−ends 
 
 
 
 
 
 
 
 
 
 
steps 
entropy 
arena 
 
 
 
 
 
 
 
 
 
 
steps 
entropy 
circle 
figure observation model ii quarry not observed at hunter s position entropy development with length of tag game for the 
three experiment scenarios a multiple dead-end b irregular open arena c circular corridor 
with respect to the deviation of actual system dynamics from the 
target dynamics 
we show that a recently developed technique of extended markov 
tracking emt is an instantiation of dbc in fact emt can 
be seen as a specific case of dbc parameterization which employs 
a greedy action selection procedure 
since emt exhibits the key features of the general dbc 
framework as well as polynomial time complexity we used the 
multitarget version of emt to demonstrate that the class of area 
sweeping problems naturally lends itself to dynamics-based 
descriptions as instantiated by our experiments in the tag game 
domain 
as enumerated in section emt has a number of 
limitations such as difficulty in dealing with negative dynamic 
preference this prevents direct application of emt to such problems 
as rendezvous-evasion games e g however dbc in 
general has no such limitations and readily enables the formulation 
of evasion games in future work we intend to proceed with the 
development of dynamics-based controllers for these problems 
 acknowledgment 
the work of the first two authors was partially supported by 
israel science foundation grant and the third author was 
partially supported by a grant from israel s ministry of science and 
technology 
 references 
 r c arkin behavior-based robotics mit press 
 j a bilmes a gentle tutorial of the em algorithm and its 
application to parameter estimation for gaussian mixture and 
hidden markov models technical report tr- - 
department of electrical engeineering and computer 
science university of california at berkeley 
 t m cover and j a thomas elements of information 
theory wiley 
 m e desjardins e h durfee c l ortiz and m j 
wolverton a survey of research in distributed continual 
planning ai magazine - 
 v r konda and j n tsitsiklis actor-critic algorithms 
siam journal on control and optimization 
 - 
 w s lim a rendezvous-evasion game on discrete locations 
with joint randomization advances in applied probability 
 - december 
 m l littman t l dean and l p kaelbling on the 
complexity of solving markov decision problems in 
proceedings of the th annual conference on uncertainty 
in artificial intelligence uai- pages - 
 o madani s hanks and a condon on the undecidability 
of probabilistic planning and related stochastic optimization 
problems artificial intelligence journal - - 
july 
 r m neal and g e hinton a view of the em algorithm 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
that justifies incremental sparse and other variants in m i 
jordan editor learning in graphical models pages 
 - kluwer academic publishers 
 p paruchuri m tambe f ordonez and s kraus security 
in multiagent systems by policy randomization in 
proceeding of aamas 
 j pineau g gordon and s thrun point-based value 
iteration an anytime algorithm for pomdps in international 
joint conference on artificial intelligence ijcai pages 
 - august 
 m l puterman markov decision processes wiley series in 
probability and mathematical statistics applied probability 
and statistics section wiley-interscience publication new 
york 
 z rabinovich and j s rosenschein extended markov 
tracking with an application to control in the workshop on 
agent tracking modeling other agents from observations 
at the third international joint conference on autonomous 
agents and multiagent systems pages - new-york 
july 
 z rabinovich and j s rosenschein multiagent 
coordination by extended markov tracking in the fourth 
international joint conference on autonomous agents and 
multiagent systems pages - utrecht the 
netherlands july 
 z rabinovich and j s rosenschein on the response of 
emt-based control to interacting targets and models in the 
fifth international joint conference on autonomous agents 
and multiagent systems pages - hakodate japan 
may 
 r f stengel optimal control and estimation dover 
publications 
 m tambe e bowring h jung g kaminka 
r maheswaran j marecki j modi r nair j pearce 
p paruchuri d pynadath p scerri n schurr and 
p varakantham conflicts in teamwork hybrids to the 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
rumours and reputation evaluating multi-dimensional 
trust within a decentralised reputation system 
steven reece 
 alex rogers 
 stephen roberts 
and nicholas r jennings 
 
department of engineering science university of oxford oxford ox pj uk 
{reece sjrob} robots ox ac uk 
 
electronics and computer science university of southampton southampton so bj uk 
{acr nrj} ecs soton ac uk 
abstract 
in this paper we develop a novel probabilistic model of 
computational trust that explicitly deals with correlated multi-dimensional 
contracts our starting point is to consider an agent attempting to 
estimate the utility of a contract and we show that this leads to a 
model of computational trust whereby an agent must determine a 
vector of estimates that represent the probability that any 
dimension of the contract will be successfully fulfilled and a covariance 
matrix that describes the uncertainty and correlations in these 
probabilities we present a formalism based on the dirichlet distribution 
that allows an agent to calculate these probabilities and correlations 
from their direct experience of contract outcomes and we show that 
this leads to superior estimates compared to an alternative approach 
using multiple independent beta distributions we then show how 
agents may use the sufficient statistics of this dirichlet distribution 
to communicate and fuse reputation within a decentralised 
reputation system finally we present a novel solution to the problem 
of rumour propagation within such systems this solution uses the 
notion of private and shared information and provides estimates 
consistent with a centralised reputation system whilst maintaining 
the anonymity of the agents and avoiding bias and overconfidence 
categories and subject descriptors 
i distributed artificial intelligence intelligent agents 
general terms 
algorithms design theory 
 introduction 
the role of computational models of trust within multi-agent 
systems in particular and open distributed systems in general has 
recently generated a great deal of research interest in such systems 
agents must typically choose between interaction partners and in 
this context trust can be viewed to provide a means for agents to 
represent and estimate the reliability with which these interaction 
partners will fulfill their commitments to date however much of 
the work within this area has used domain specific or ad-hoc trust 
metrics and has focused on providing heuristics to evaluate and 
update these metrics using direct experience and reputation reports 
from other agents see for a review 
recent work has attempted to place the notion of computational 
trust within the framework of probability theory this 
approach allows many of the desiderata of computational trust models 
to be addressed through principled means in particular i it 
allows agents to update their estimates of the trustworthiness of a 
supplier as they acquire direct experience ii it provides a 
natural framework for agents to express their uncertainty this 
trustworthiness and iii it allows agents to exchange combine and filter 
reputation reports received from other agents 
whilst this approach is attractive it is somewhat limited in that it 
has so far only considered single dimensional outcomes i e whether 
the contract has succeeded or failed in its entirety however in 
many real world settings the success or failure of an interaction may 
be decomposed into several dimensions this presents the 
challenge of combining these multiple dimensions into a single metric 
over which a decision can be made furthermore these dimensions 
will typically also exhibit correlations for example a contract 
within a supply chain may specify criteria for timeliness quality 
and quantity a supplier who is suffering delays may attempt a 
trade-off between these dimensions by supplying the full amount 
late or supplying as much as possible but less than the quantity 
specified within the contract on time thus correlations will 
naturally arise between these dimensions and hence between the 
probabilities that describe the successful fulfillment of each contract 
dimension to date however no such principled framework exists 
to describe these multi-dimensional contracts nor the correlations 
between these dimensions although some ad-hoc models do exist 
- see section for more details 
to rectify this shortcoming in this paper we develop a 
probabilistic model of computational trust that explicitly deals with 
correlated multi-dimensional contracts the starting point for our work 
is to consider how an agent can estimate the utility that it will derive 
from interacting with a supplier here we use standard approaches 
from the literature of data fusion since this is a well developed 
field where the notion of multi-dimensional correlated estimates is 
well established 
 to show that this naturally leads to a trust model 
where the agent must estimate probabilities and correlations over 
 
in this context the multiple dimensions typically represent the 
physical coordinates of a target being tracked and correlations arise 
through the operation and orientation of sensors 
 
 - - - - rps c ifaamas 
multiple dimensions building upon this we then devise a novel 
trust model that addresses the three desiderata discussed above in 
more detail in this paper we extend the state of the art in four key 
ways 
 we devise a novel multi-dimensional probabilistic trust model 
that enables an agent to estimate the expected utility of a 
contract by estimating i the probability that each contract 
dimension will be successfully fulfilled and ii the 
correlations between these estimates 
 we present an exact probabilistic model based upon the 
dirichlet distribution that allows agents to use their direct 
experience of contract outcomes to calculate the probabilities and 
correlations described above we then benchmark this 
solution and show that it leads to good estimates 
 we show that agents can use the sufficient statistics of this 
dirichlet distribution in order to exchange reputation reports 
with one another the sufficient statistics represent 
aggregations of their direct experience and thus express contract 
outcomes in a compact format with no loss of information 
 we show that while being efficient the aggregation of 
contract outcomes can lead to double counting and rumour 
propagation in decentralised reputation systems thus we present 
a novel solution based upon the idea of private and shared 
information we show that it yields estimates consistent with a 
centralised reputation system whilst maintaining the anonymity 
of the agents and avoiding overconfidence 
the remainder of this paper is organised as follows in section we 
review related work in section we present our notation for a 
single dimensional contract before introducing our multi-dimensional 
trust model in section in sections and we discuss 
communicating reputation and present our solution to rumour propagation 
in decentralised reputation systems we conclude in section 
 related work 
the need for a multi-dimensional trust model has been recognised 
by a number of researchers sabater and sierra present a model of 
reputation in which agents form contracts based on multiple 
variables such as delivery date and quality and define impressions as 
subjective evaluations of the outcome of these contracts they 
provide heuristic approaches to combining these impressions to form 
a measure they call subjective reputation 
likewise griffiths decomposes overall trust into a number of 
different dimensions such as success cost timeliness and 
quality in his case each dimension is scored as a real number 
that represents a comparative value with no strong semantic 
meaning he develops an heuristic rule to update these values based 
on the direct experiences of the individual agent and an heuristic 
function that takes the individual trust dimensions and generates a 
single scalar that is then used to select between suppliers whilst 
he comments that the trust values could have some associated 
confidence level heuristics for updating these levels are not presented 
gujral et al take a similar approach and present a trust model 
over multiple domain specific dimensions they define 
multidimensional goal requirements and evaluate an expected payoff 
based on a supplier s estimated behaviour these estimates are 
however simple aggregations over the direct experience of several 
agents and there is no measure of the uncertainty nevertheless 
they show that agents who select suppliers based on these multiple 
dimensions outperform those who consider just a single one 
by contrast a number of researchers have presented more 
principled computational trust models based on probability theory albeit 
limited to a single dimension jøsang and ismail describe the beta 
reputation system whereby the reputation of an agent is compiled 
from the positive and negative reports from other agents who have 
interacted with it the beta distribution represents a natural 
choice for representing these binary outcomes and it provides a 
principled means of representing uncertainty moreover they 
provide a number of extensions to this initial model including an 
approach to exchanging reputation reports using the sufficient 
statistics of the beta distribution methods to discount the opinions of 
agents who themselves have low reputation ratings and techniques 
to deal with reputations that may change over time 
likewise teacy et al use the beta distribution to describe an 
agent s belief in the probability that another agent will 
successfully fulfill its commitments they present a formalism using 
a beta distribution that allows the agent to estimate this probability 
based upon its direct experience and again they use the sufficient 
statistics of this distribution to communicate this estimate to other 
agents they provide a number of extensions to this initial model 
and in particular they consider that agents may not always 
truthfully report their trust estimates thus they present a principled 
approach to detecting and removing inconsistent reports 
our work builds upon these more principled approaches 
however the starting point of our approach is to consider an agent that 
is attempting to estimate the expected utility of a contract we show 
that estimating this expected utility requires that an agent must 
estimate the probability with which the supplier will fulfill its contract 
in the single-dimensional case this naturally leads to a trust model 
using the beta distribution as per jøsang and ismail and teacy et 
al however we then go on to extend this analysis to multiple 
dimensions where we use the natural extension of the beta 
distribution namely the dirichlet distribution to represent the agent s 
belief over multiple dimensions 
 single-dimensional trust 
before presenting our multi-dimensional trust model we first 
introduce the notation and formalism that we will use by describing the 
more familiar single dimensional case we consider an agent who 
must decide whether to engage in a future contract with a supplier 
this contract will lead to some outcome o and we consider that 
o if the contract is successfully fulfilled and o if not 
 
in order for the agent to make a rational decision it should 
consider the utility that it will derive from this contract we assume 
that in the case that the contract is successfully fulfilled the agent 
derives a utility u o otherwise it receives no utility 
 now 
given that the agent is uncertain of the reliability with which the 
supplier will fulfill the contract it should consider the expected 
utility that it will derive e u and this is given by 
e u p o u o 
where p o is the probability that the supplier will successfully 
fulfill the contract however whilst u o is known by the 
agent p o is not the best the agent can do is to determine 
a distribution over possible values of p o given its direct 
experience of previous contract outcomes given that it has been 
able to do so it can then determine an estimate of the expected 
utility 
of the contract e e u and a measure of its uncertainty 
in this expected utility var e u this uncertainty is important 
since a risk averse agent may make a decision regarding a contract 
 
note that we only consider binary contract outcomes although 
extending this to partial outcomes is part of our future work 
 
clearly this can be extended to the case where some utility is 
derived from an unsuccessful outcome 
 
note that this is often called the expected expected utility and 
this is the notation that we adopt here 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
not only on its estimate of the expected utility of the contract but 
also on the probability that the expected utility will exceed some 
minimum amount these two properties are given by 
e e u ˆp o u o 
var e u var p o u o 
 
where ˆp o and var p o are the estimate and 
uncertainty of the probability that a contract will be successfully 
fulfilled and are calculated from the distribution over possible values 
of p o that the agent determines from its direct experience 
the utility based approach that we present here provides an 
attractive motivation for this model of teacy et al 
now in the case of binary contract outcomes the beta 
distribution is the natural choice to represent the distribution over possible 
values of p o since within bayesian statistics this well known 
to be the conjugate prior for binomial observations by adopting 
the beta distribution we can calculate ˆp o and var p o 
using standard results and thus if an agent observed n previous 
contracts of which n were successfully fulfilled then 
ˆp o 
n 
n 
and 
var p o 
 n n − n 
 n n 
note that as expected the greater the number of contracts the agent 
observes the smaller the variance term var p o and thus 
the less the uncertainty regarding the probability that a contract will 
be successfully fulfilled ˆp o 
 multi-dimensional trust 
we now extend the description above to consider contracts 
between suppliers and agents that are represented by multiple 
dimensions and hence the success or failure of a contract can be 
decomposed into the success or failure of each separate 
dimension consider again the example of the supply chain that 
specifies the timeliness quantity and quality of the goods that are to 
be delivered thus within our trust model oa now 
indicates a successful outcome over dimension a of the contract and 
oa indicates an unsuccessful one a contract outcome x 
is now composed of a vector of individual contract part outcomes 
 e g x {oa ob oc } 
given a multi-dimensional contract whose outcome is described 
by the vector x we again consider that in order for an agent to 
make a rational decision it should consider the utility that it will 
derive from this contract to this end we can make the general 
statement that the expected utility of a contract is given by 
e u p x u x t 
 
where p x is a joint probability distribution over all possible 
contract outcomes 
p x 
⎛ 
⎜ 
⎜ 
⎜ 
⎝ 
p oa ob oc 
p oa ob oc 
p oa ob oc 
 
⎞ 
⎟ 
⎟ 
⎟ 
⎠ 
 
and u x is the utility derived from these possible outcomes 
u x 
⎛ 
⎜ 
⎜ 
⎜ 
⎝ 
u oa ob oc 
u oa ob oc 
u oa ob oc 
 
⎞ 
⎟ 
⎟ 
⎟ 
⎠ 
 
as before whilst u x is known to the agent the probability 
distribution p x is not rather given the agent s direct experience 
of the supplier the agent can determine a distribution over possible 
values for p x in the single dimensional case a beta distribution 
was the natural choice over possible values of p o in the 
multi-dimensional case where p x itself is a vector of 
probabilities the corresponding natural choice is the dirichlet distribution 
since this is a conjugate prior for multinomial proportions 
given this distribution the agent is then able to calculate an 
estimate of the expected utility of a contract as before this estimate 
is itself represented by an expected value given by 
e e u ˆp x u x t 
 
and a variance describing the uncertainty in this expected utility 
var e u u x cov p x u x t 
 
where 
cov p x e p x − ˆp x p x − ˆp x t 
 
thus whilst the single dimensional case naturally leads to a trust 
model in which the agents attempt to derive an estimate of 
probability that a contract will be successfully fulfilled ˆp o along 
with a scalar variance that describes the uncertainty in this 
probability var p o in this case the agents must derive an 
estimate of a vector of probabilities ˆp x along with a covariance 
matrix cov p x that represents the uncertainty in p x given 
the observed contractual outcomes at this point it is interesting 
to note that the estimate in the single dimensional case ˆp o 
has a clear semantic meaning in relation to trust it is the agent s 
belief in the probability of a supplier successfully fulfilling a 
contract however in the multi-dimensional case the agent must 
determine ˆp x and since this describes the probability of all possible 
contract outcomes including those that are completely un-fulfilled 
this direct semantic interpretation is not present in the next 
section we describe the exemplar utility function that we shall use in 
the remainder of this paper 
 exemplar utility function 
the approach described so far is completely general in that it 
applies to any utility function of the form described above and also 
applies to the estimation of any joint probability distribution in 
the remainder of this paper for illustrative purposes we shall limit 
the discussion to the simplest possible utility function that exhibits 
a dependence upon the correlations between the contract 
dimensions that is we consider the case that expected utility is 
dependent only on the marginal probabilities of each contract dimension 
being successfully fulfilled rather than the full joint probabilities 
u x 
⎛ 
⎜ 
⎜ 
⎜ 
⎝ 
u oa 
u ob 
u oc 
 
⎞ 
⎟ 
⎟ 
⎟ 
⎠ 
 
thus ˆp x is a vector estimate of the probability of each contract 
dimension being successfully fulfilled and maintains the clear 
semantic interpretation seen in the single dimensional case 
ˆp x 
⎛ 
⎜ 
⎜ 
⎜ 
⎝ 
ˆp oa 
ˆp ob 
ˆp oc 
 
⎞ 
⎟ 
⎟ 
⎟ 
⎠ 
 
the correlations between the contract dimensions affect the 
uncertainty in the expected utility to see this consider the covariance 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
matrix that describes this uncertainty cov p x is now given by 
cov p x 
⎛ 
⎜ 
⎜ 
⎜ 
⎝ 
va cab cac 
cab vb cbc 
cac cbc vc 
 
 
 
⎞ 
⎟ 
⎟ 
⎟ 
⎠ 
 
in this matrix the diagonal terms va vb and vc represent the 
uncertainties in p oa p ob and p oc within 
p x the off-diagonal terms cab cac and cbc represent the 
correlations between these probabilities in the next section we use 
the dirichlet distribution to calculate both ˆp x and cov p x 
from an agent s direct experience of previous contract outcomes 
we first illustrate why this is necessary by considering an 
alternative approach to modelling multi-dimensional contracts whereby 
an agent na¨ıvely assumes that the dimensions are independent and 
thus it models each individually by separate beta distributions as 
in the single dimensional case we presented in section this 
is actually equivalent to setting the off-diagonal terms within the 
covariance matrix cov p x to zero however doing so can 
lead an agent to assume that its estimate of the expected utility of 
the contract is more accurate than it actually is to illustrate this 
consider a specific scenario with the following values u oa 
 u ob and va vb in this case 
var e u cab and thus if the correlation cab is 
ignored then the variance in the expected utility is however if 
the contract outcomes are completely correlated then cab and 
var e u is actually thus in order to have an accurate 
estimate of the variance of the expected contract utility and to make 
a rational decision it is essential that the agent is able to 
represent and calculate these correlation terms in the next section we 
describe how an agent may do so using the dirichlet distribution 
 the dirichlet distribution 
in this section we describe how the agent may use its direct 
experience of previous contracts and the standard results of the dirichlet 
distribution to determine an estimate of the probability that each 
contract dimension will be successful fulfilled ˆp x and a 
measure of the uncertainties in these probabilities that expresses the 
correlations between the contract dimensions cov p x 
we first consider the calculation of ˆp x and the diagonal terms 
of the covariance matrix cov p x as described above the 
derivation of these results is identical to the case of the single 
dimensional beta distribution where out of n contract outcomes 
n are successfully fulfilled in the multi-dimensional case 
however we have a vector {na nb nc } that represents the number 
of outcomes for which each of the individual contract dimensions 
were successfully fulfilled thus in terms of the standard dirichlet 
parameters where αa na and α n the agent can 
estimate the probability of this contract dimension being successfully 
fulfilled 
ˆp oa 
αa 
α 
 
na 
n 
and can also calculate the variance in any contract dimension 
va 
αa α − αa 
α 
 α 
 
 na n − na 
 n n 
however calculating the off-diagonal terms within cov p x is 
more complex since it is necessary to consider the correlations 
between the contract dimensions thus for each pair of dimensions 
 i e a and b we must consider all possible combinations of 
contract outcomes and thus we define nab 
ij as the number of contract 
outcomes for which both oa i and ob j for example nab 
 
represents the number of contracts for which oa and ob 
now using the standard dirichlet notation we can define αab 
ij 
nab 
ij for all i and j taking values and and then to calculate 
the cross-correlations between contract pairs a and b we note that 
the dirichlet distribution over pair-wise joint probabilities is 
prob pab kab 
i∈{ } j∈{ } 
p oa i ob j αab 
ij − 
where 
i∈{ } j∈{ } 
p oa i ob j 
and kab is a normalising constant from this we can derive 
pair-wise probability estimates and variances 
e p oa i ob j 
αab 
ij 
α 
 
v p oa i ob j 
αab 
ij α − αab 
ij 
α 
 α 
 
where 
α 
i∈{ } j∈{ } 
αab 
ij 
and in fact α n where n is the total number of contracts 
observed likewise we can express the covariance in these 
pairwise probabilities in similar terms 
c p oa i ob j p oa m ob n 
−αab 
ij αab 
mn 
α 
 α 
finally we can use the expression 
p oa 
j∈{ } 
p oa ob j 
to determine the covariance cab to do so we first simplify the 
notation by defining v ab 
ij v p oa i ob j and cab 
ijmn 
c p oa i ob j p oa m ob n the covariance for the 
probability of positive contract outcomes is then the covariance 
between j∈{ } p oa ob j and i∈{ } p oa i ob 
 and thus 
cab cab 
 cab 
 cab 
 v ab 
 
thus given a set of contract outcomes that represent the agent s 
previous interactions with a supplier we may use the dirichlet 
distribution to calculate the mean and variance of the probability of 
any contract dimension being successfully fulfilled i e ˆp oa 
and va in addition by a somewhat more complex procedure we 
can also calculate the correlations between these probabilities i e 
cab this allows us to calculate an estimate of the probability that 
any contract dimension will be successfully fulfilled ˆp x and 
also represent the uncertainty and correlations in these probabilities 
by the covariance matrix cov p x in turn these results may be 
used to calculate the estimate and uncertainty in the expected 
utility of the contract in the next section we present empirical results 
that show that in practise this formalism yields significant 
improvements in these estimates compared to the na¨ıve approximation 
using multiple independent beta distributions 
 empirical comparison 
in order to evaluate the effectiveness of our formalism and show 
the importance of the off-diagonal terms in cov p x we 
compare two approaches 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
− − 
 
 
 
 
correlation ρ 
var e u 
dirichlet distribution 
indepedent beta distributions 
− − 
 
 
 
 
 
x 
 
correlation ρ 
information i 
dirichlet distribution 
indepedent beta distributions 
figure plots showing i the variance of the expected contract 
utility and ii the information content of the estimates 
computed using the dirichlet distribution and multiple independent 
beta distributions results are averaged over 
runs and the 
error bars show the standard error in the mean 
 dirichlet distribution we use the full dirichlet 
distribution as described above to calculate ˆp x and cov p x 
including all its off-diagonal terms that represent the 
correlations between the contract dimensions 
 independent beta distributions we use independent beta 
distributions to represent each contract dimension in order 
to calculate ˆp x and then as described earlier we 
approximate cov p x and ignore the correlations by setting all 
the off-diagonal terms to zero 
we consider a two-dimensional case where u oa and 
u ob since this allows us to plot ˆp x and cov p x 
as ellipses in a two-dimensional plane and thus explain the 
differences between the two approaches specifically we initially 
allocate the agent some previous contract outcomes that represents its 
direct experience with a supplier the number of contracts is drawn 
uniformly between and and the actual contract outcomes are 
drawn from an arbitrary joint distribution intended to induce 
correlations between the contract dimensions for each set of 
contracts we use the approaches described above to calculate ˆp x 
and cov p x and hence the variance in the expected contract 
utility var e u in addition we calculate a scalar measure of the 
information content i of the covariance matrix cov p x which 
is a standard way of measuring the uncertainty encoded within the 
covariance matrix more specifically we calculate the 
determinant of the inverse of the covariance matrix 
i det cov p x − 
 
and note that the larger the information content the more precise 
ˆp x will be and thus the better the estimate of the expected 
utility that the agent is able to calculate finally we use the results 
 
 
 
 
 
 
 
p o 
p o 
a 
b 
dirichlet distribution 
indepedent beta distributions 
figure examples of ˆp x and cov p x plotted as second 
standard error ellipses 
presented in section to calculate the actual correlation ρ 
associated with this particular set of contract outcomes 
ρ 
cab 
√ 
vavb 
 
where cab va and vb are calculated as described in section 
the results of this analysis are shown in figure here we show 
the values of i and var e u calculated by the agents plotted 
against the correlation of the contract outcomes ρ that constituted 
their direct experience the results are averaged over 
 
simulation runs note that as expected when the dimensions of the 
contract outcomes are uncorrelated i e ρ then both approaches 
give the same results however the value of using our formalism 
with the full dirichlet distribution is shown when the correlation 
between the dimensions increases either negatively or positively 
as can be seen if we approximate the dirichlet distribution with 
multiple independent beta distributions all of the correlation 
information contained within the covariance matrix cov p x is 
lost and thus the information content of the matrix is much lower 
the loss of this correlation information leads the variance of the 
expected utility of the contract to be incorrect either over or under 
estimated depending on the correlation 
 with the exact amount of 
mis-estimation depending on the actual utility function chosen i e 
the values of u oa and u ob 
in addition in figure we illustrate an example of the estimates 
calculated through both methods for a single exemplar set of 
contract outcomes we represent the probability estimates ˆp x and 
the covariance matrix cov p x in the standard way as an 
ellipse that is ˆp x determines the position of the center of 
the ellipse cov p x defines its size and shape note that whilst 
the ellipse resulting from the full dirichlet formalism accurately 
reflects the true distribution samples of which are plotted as points 
that calculated by using multiple independent beta distributions 
 and thus ignoring the correlations results in a much larger ellipse 
that does not reflect the true distribution the larger size of this 
ellipse is a result of the off-diagonal terms of the covariance matrix 
being set to zero and corresponds to the agent miscalculating the 
uncertainty in the probability of each contract dimension being 
fulfilled this in turn leads it to miscalculate the uncertainty in the 
expected utility of a contract shown in figure as var e u 
 communicating reputation 
having described how an individual agent can use its own direct 
experience of contract outcomes in order to estimate the 
probabil 
note that the plots are not smooth due to the fact that given a 
limited number of contract outcomes then the mean of va and vb 
do not vary smoothly with ρ 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
ity that a multi-dimensional contract will be successfully fulfilled 
we now go on to consider how agents within an open multi-agent 
system can communicate these estimates to one another this is 
commonly referred to as reputation and allows agents with limited 
direct experience of a supplier to make rational decisions 
both jøsang and ismail and teacy et al present models whereby 
reputation is communicated between agents using the sufficient 
statistics of the beta distribution this approach is attractive since 
these sufficient statistics are simple aggregations of contract 
outcomes more precisely they are simply the total number of 
contracts observed n and the number of these that were successfully 
fulfilled n under the probabilistic framework of the beta 
distribution reputation reports in this form may simply be aggregated with 
an agent s own direct experience in order to gain a more precise 
estimate based on a larger set of contract outcomes 
we can immediately extend this approach to the multi-dimensional 
case considered here by requiring that the agents exchange the 
sufficient statistics of the dirichlet distribution instead of the beta 
distribution in this case for each pair of dimensions i e a and b the 
agents must communicate a vector of contract outcomes n which 
are the sufficient statistics of the dirichlet distribution given by 
n nab 
ij ∀a b i ∈ { } j ∈ { } 
thus an agent is able to communicate the sufficient statistics of 
its own dirichlet distribution in terms of just d d − numbers 
 where d is the number of contract dimensions for instance in 
the case of three dimensions n is given by 
n nab 
 nab 
 nab 
 nab 
 nac 
 nac 
 nac 
 nac 
 nbc 
 nbc 
 nbc 
 nbc 
 
and hence large sets of contract outcomes may be communicated 
within a relatively small message size with no loss of information 
again agents receiving these sufficient statistics may simply 
aggregate them with their own direct experience in order to gain a 
more precise estimate of the trustworthiness of a supplier 
finally we note that whilst it is not the focus of our work here 
by adopting the same principled approach as jøsang and ismail and 
teacy et al many of the techniques that they have developed such 
as discounting reports from unreliable agents and filtering 
inconsistent reports from selfish agents may be directly applied within 
this multi-dimensional model however we now go on to consider 
a new issue that arises in both the single and multi-dimensional 
models namely the problems that arise when such aggregated 
sufficient statistics are propagated within decentralised agent networks 
 rumour propagation 
within reputation systems 
in the previous section we described the use of sufficient 
statistics to communicate reputation and we showed that by aggregating 
contract outcomes together into these sufficient statistics a large 
number of contract outcomes can be represented and 
communicated in a compact form whilst this is an attractive property it 
can be problematic in practise since the individual provenance of 
each contract outcome is lost in the aggregation thus to ensure an 
accurate estimate the reputation system must ensure that each 
observation of a contract outcome is included within the aggregated 
statistics no more than once 
within a centralised reputation system where all agents report 
their direct experience to a trusted center such double counting of 
contract outcomes is easy to avoid however in a decentralised 
reputation system where agents communicate reputation to one 
another and aggregate their direct experience with these reputation 
reports on-the-fly avoiding double counting is much more difficult 
a a 
a 
¨ 
¨¨ 
¨¨ 
¨¨b 
e 
t 
n 
n 
n n 
figure example of rumour propagation in a decentralised 
reputation system 
for example consider the case shown in figure where three 
agents a a each with some direct experience of a supplier 
share reputation reports regarding this supplier if agent a were 
to provide its estimate to agents a and a in the form of the 
sufficient statistics of its dirichlet distribution then these agents can 
aggregate these contract outcomes with their own and thus obtain 
more precise estimates if at a later stage agent a were to send 
its aggregate vector of contract outcomes to agent a then agent 
a being unaware of the full history of exchanges may attempt to 
combine these contract outcomes with its own aggregated vector 
however since both vectors contain a contribution from agent a 
these will be counted twice in the final aggregated vector and will 
result in a biased and overconfident estimate this is termed 
rumour propagation or data incest in the data fusion literature 
one possible solution would be to uniquely identify the source of 
each contract outcome and then propagate each vector along with 
its label through the network agents can thus identify identical 
observations that have arrived through different routes and after 
removing the duplicates can aggregate these together to form their 
estimates whilst this appears to be attractive in principle for a 
number of reasons it is not always a viable solution in practise 
firstly agents may not actually wish to have their uniquely labelled 
contract outcomes passed around an open system since such 
information may have commercial or practical significance that could 
be used to their disadvantage as such agents may only be willing 
to exchange identifiable contract outcomes with a small number of 
other agents perhaps those that they have some sort of reciprocal 
relationship with secondly the fact that there is no aggregation 
of the contract outcomes as they pass around the network means 
that the message size increases over time and the ultimate size of 
these messages is bounded only by the number of agents within the 
system possibly an extremely large number for a global system 
finally it may actually be difficult to assign globally agreeable 
consistent and unique labels for each agent within an open system 
in the next section we develop a novel solution to the problem of 
rumour propagation within decentralised reputation systems our 
solution is based on an approach developed within the area of target 
tracking and data fusion it avoids the need to uniquely identify 
an agent it allows agents to restrict the number of other agents 
who they reveal their private estimates to and yet it still allows 
information to propagate throughout the network 
 private and shared information 
our solution to rumour propagation within decentralised reputation 
systems introduces the notion of private information that an agent 
knows it has not communicated to any other agent and shared 
information that has been communicated to or received from 
another agent thus the agent can decompose its contract outcome 
vector n into two vectors a private one np that has not been 
communicated to another agent and a shared one ns that has 
been shared with or received from another agent 
n np ns 
now whenever an agent communicates reputation it 
communicates both its private and shared vectors separately both the 
origthe sixth intl joint conf on autonomous agents and multi-agent systems aamas 
inating and receiving agents then update their two vectors 
appropriately to understand this consider the case that agent aα sends 
its private and shared contract outcome vectors nα 
p and nα 
s to 
agent aβ that itself has private and shared contract outcomes nβ 
p 
and nβ 
s each agent updates its vectors of contract outcomes 
according to the following procedure 
 originating agent once the originating agent has sent both 
its shared and private contract outcome vectors to another 
agent its private information is no longer private thus it 
must remove the contract outcomes that were in its private 
vector and add them into its shared vector 
nα 
s ← nα 
s nα 
p 
nα 
p ← ∅ 
 receiving agent the goal of the receiving agent is to 
accumulate the largest number contract outcomes since this will 
result in the most precise estimate without including shared 
information from both itself and the other agent since this 
may result in double counting of contract outcomes it has 
two choices depending on the total number of contract 
outcomes 
within its own shared vector nβ 
s and within that of 
the originating agent nα 
s thus it updates its vector 
according to the procedure below 
- nβ 
s nα 
s if the receiving agent s shared vector 
represents a greater number of contract outcomes than that 
of the shared vector of the originating agent then the 
agent combines its shared vector with the private 
vector of the originating agent 
nβ 
s ← nβ 
s nα 
p 
nβ 
p unchanged 
- nβ 
s nα 
s alternatively if the receiving agent s shared 
vector represents a smaller number contract outcomes 
than that of the shared vector of the originating agent 
then the receiving agent discards its own shared vector 
and forms a new one from both the private and shared 
vectors of the originating agent 
nβ 
s ← nα 
s nα 
p 
nβ 
p unchanged 
in the case that nβ 
s nα 
s then either option is 
appropriate once the receiving agent has updated its sets it uses the 
contract outcomes within both to form its trust estimate if 
agents receive several vectors simultaneously this approach 
generalises to the receiving agent using the largest shared 
vector and the private vectors of itself and all the originating 
agents to form its new shared vector 
this procedure has a number of attractive properties firstly since 
contract outcomes in an agent s shared vector are never combined 
with those in the shared vector of another agent outcomes that 
originated from the same agent are never combined together and 
thus rumour propagation is completely avoided however since 
the receiving agent may discard its own shared vector and adopt 
the shared vector of the originating agent information is still 
propagated around the network moreover since contract outcomes are 
aggregated together within the private and shared vectors the 
message size is constant and does not increase as the number of 
interactions increases finally an agent only communicates its own 
private contract outcomes to its immediate neighbours if this agent 
 
note that this may be calculated from n nab 
 nab 
 nab 
 nab 
 
subsequently passes it on it does so as unidentifiable aggregated 
information within its shared information thus an agent may limit 
the number of agents with which it is willing to reveal 
identifiable contract outcomes and yet these contract outcomes can still 
propagate within the network and thus improve estimates of other 
agents next we demonstrate empirically that these properties can 
indeed be realised in practise 
 empirical comparison 
in order to evaluate the effectiveness of this procedure we 
simulated random networks consisting of ten agents each agent has 
some direct experience of interacting with a supplier as described 
in section at each iteration of the simulation it interacts with 
its immediate neighbours and exchanges reputation reports through 
the sufficient statistics of their dirichlet distributions we compare 
our solution to two of the most obvious decentralised alternatives 
 private and shared information the agents follow the 
procedure described in the previous section that is they 
maintain separate private and shared vectors of contract 
outcomes and at each iteration they communicate both these 
vectors to their immediate neighbours 
 rumour propagation the agents do not differentiate 
between private and shared contract outcomes at the first 
iteration they communicate all of the contract outcomes that 
constitute their direct experience in subsequent iterations 
they propagate contract outcomes that they receive from any 
of the neighbours to all their other immediate neighbours 
 private information only the agents only communicate 
the contract outcomes that constitute their direct experience 
in all cases at each iteration the agents use the dirichlet 
distribution in order to calculate their trust estimates we compare these 
three decentralised approaches to a centralised reputation system 
 centralised reputation all the agents pass their direct 
experience to a centralised reputation system that aggregates 
them together and passes this estimate back to each agent 
this centralised solution makes the most effective use of 
information available in the network however most real world 
problems demand decentralised solutions due to scalability 
modularity and communication concerns thus this centralised solution 
is included since it represents the optimal case and allows us to 
benchmark our decentralised solution 
the results of these comparisons are shown in figure here 
we show the sum of the information content of each agent s 
covariance matrix calculated as discussed earlier in section for 
each of these four different approaches we first note that where 
private information only is communicated there is no change in 
information after the first iteration once each agent has received the 
direct experience of its immediate neighbours no further increase 
in information can be achieved this represents the minimum 
communication and it exhibits the lowest total information of the four 
cases next we note that in the case of rumour propagation the 
information content increases continually and rapidly exceeds the 
centralised reputation result the fact that the rumour propagation 
case incorrectly exceeds this limit indicates that it is continuously 
counting the same contract outcomes as they cycle around the 
network in the belief that they are independent events finally we 
note that using private and shared information represents a 
compromise between the private information only case and the centralised 
reputation case information is still allowed to propagate around 
the network however rumours are eliminated 
as before we also plot a single instance of the trust estimates 
from one agent i e ˆp x and cov p x as a set of ellipses on a 
 the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
 
 
 
 
 
 
 
 
 
iteration 
information i 
private shared information 
rumour propagation 
private information only 
centralised reputation 
figure sum of information over all agents as a function of 
the communication iteration 
two-dimensional plane along with samples from the true 
distribution as expected the centralised reputation system achieves the 
best estimate of the true distribution since it uses the direct 
experience of all agents the private information only case shows the 
largest ellipse since it propagates the least information around the 
network the rumour propagation case shows the smallest ellipse 
but it is inconsistent with the actual distribution p x thus 
propagating rumours around the network and double counting contract 
outcomes in the belief that they are independent events results in 
an overconfident estimate however we note that our solution 
using separate vectors of private and shared information allows us to 
propagate more information than the private information only case 
but we completely avoid the problems of rumour propagation 
finally we consider the effect that this has on the agents 
calculation of the expected utility of the contract we assume the same 
utility function as used in section i e u oa and 
u ob and in table we present the estimate of the 
expected utility and its standard deviation calculated for all four cases 
by a single agent at iteration five after communication has ceased 
to have any further effect for all methods other than rumour 
propagation we note that the rumour propagation case is clearly 
inconsistent with the centralised reputation system since its standard 
deviation is too small and does not reflect the true uncertainty in 
the expected utility given the contract outcomes however we 
observe that our solution represents the closest case to the centralised 
reputation system and thus succeeds in propagating information 
throughout the network whilst also avoiding bias and 
overconfidence the exact difference between it and the centralised 
reputation system depends upon the topology of the network and the 
history of exchanges that take place within it 
 conclusions 
in this paper we addressed the need for a principled probabilistic 
model of computational trust that deals with contracts that have 
multiple correlated dimensions our starting point was an agent 
estimating the expected utility of a contract and we showed that this 
leads to a model of computational trust that uses the dirichlet 
distribution to calculate a trust estimate from the direct experience of an 
agent we then showed how agents may use the sufficient statistics 
of this dirichlet distribution to represent and communicate 
reputation within a decentralised reputation system and we presented a 
solution to rumour propagation within these systems 
our future work in this area is to extend the exchange of 
reputation to the case where contracts are not homogeneous that 
is not all agents observe the same contract dimensions this is 
a challenging extension since in this case the sufficient statistics 
of the dirichlet distribution can not be used directly however by 
 
 
 
 
 
 
 
 
p o 
p o 
a 
b 
private shared information 
rumour propagation 
private information only 
centralised reputation 
figure instances of ˆp x and cov p x plotted as second 
standard error ellipses after communication iterations 
method e e u ± var e u 
private and shared information ± 
rumour propagation ± 
private information only ± 
centralised reputation ± 
table estimated expected utility and its standard error as 
calculated by a single agent after communication iterations 
addressing this challenge we hope to be able to apply these 
techniques to a setting in which a suppliers provides a range of services 
whose failures are correlated and agents only have direct 
experiences of different subsets of these services 
 acknowledgements 
this research was undertaken as part of the aladdin autonomous 
learning agents for decentralised data and information networks 
project and is jointly funded by a bae systems and epsrc 
strategic partnership ep c 
 references 
 y bar-shalom x r li and t kirubarajan estimation with applications to 
tracking and navigation wiley interscience 
 c boutilier the foundations of expected expected utility in proc of the th 
int joint conf on on artificial intelligence pages - acapulco 
mexico 
 m evans n hastings and b peacock statistical distributions john wiley 
 sons inc 
 n griffiths task delegation using experience-based multi-dimensional trust 
in proc of the th int joint conf on autonomous agents and multiagent 
systems pages - new york usa 
 n gukrai d deangelis k k fullam and k s barber modelling 
multi-dimensional trust in proc of the th int workshop on trust in agent 
systems hakodate japan 
 a jøsang and r ismail the beta reputation system in proc of the th bled 
electronic commerce conf pages - bled slovenia 
 e m maximilien and m p singh agent-based trust model involving 
multiple qualities in proc of the th int joint conf on autonomous agents 
and multiagent systems pages - utrecht the netherlands 
 s d ramchurn d hunyh and n r jennings trust in multi-agent systems 
knowledge engineering review - 
 s reece and s roberts robust low-bandwidth multi-vehicle mapping in 
proc of the th int conf on information fusion philadelphia usa 
 j sabater and c sierra regret a reputation model for gregarious 
societies in proc of the th workshop on deception fraud and trust in agent 
societies pages - montreal canada 
 w t l teacy j patel n r jennings and m luck travos trust and 
reputation in the context of inaccurate information sources autonomous 
agents and multi-agent systems - 
 s utete network management in decentralised sensing systems phd thesis 
university of oxford uk 
the sixth intl joint conf on autonomous agents and multi-agent systems aamas 
