Context-Sensitive Information Retrieval Using
Implicit Feedback
Xuehua Shen
Department of Computer
Science
University of Illinois at
Urbana-Champaign
Bin Tan
Department of Computer
Science
University of Illinois at
Urbana-Champaign
ChengXiang Zhai
Department of Computer
Science
University of Illinois at
Urbana-Champaign
ABSTRACT
A major limitation of most existing retrieval models and systems
is that the retrieval decision is made based solely on the query and
document collection; information about the actual user and search
context is largely ignored. In this paper, we study how to 
exploit implicit feedback information, including previous queries and
clickthrough information, to improve retrieval accuracy in an 
interactive information retrieval setting. We propose several 
contextsensitive retrieval algorithms based on statistical language models
to combine the preceding queries and clicked document summaries
with the current query for better ranking of documents. We use
the TREC AP data to create a test collection with search context
information, and quantitatively evaluate our models using this test
set. Experiment results show that using implicit feedback, 
especially the clicked document summaries, can improve retrieval 
performance substantially.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms
Algorithms
1. INTRODUCTION
In most existing information retrieval models, the retrieval 
problem is treated as involving one single query and a set of documents.
From a single query, however, the retrieval system can only have
very limited clue about the user"s information need. An optimal 
retrieval system thus should try to exploit as much additional context
information as possible to improve retrieval accuracy, whenever it
is available. Indeed, context-sensitive retrieval has been identified
as a major challenge in information retrieval research[2].
There are many kinds of context that we can exploit. Relevance
feedback [14] can be considered as a way for a user to provide
more context of search and is known to be effective for 
improving retrieval accuracy. However, relevance feedback requires that
a user explicitly provides feedback information, such as specifying
the category of the information need or marking a subset of 
retrieved documents as relevant documents. Since it forces the user
to engage additional activities while the benefits are not always 
obvious to the user, a user is often reluctant to provide such feedback
information. Thus the effectiveness of relevance feedback may be
limited in real applications.
For this reason, implicit feedback has attracted much attention 
recently [11, 13, 18, 17, 12]. In general, the retrieval results using the
user"s initial query may not be satisfactory; often, the user would
need to revise the query to improve the retrieval/ranking accuracy
[8]. For a complex or difficult information need, the user may need
to modify his/her query and view ranked documents with many 
iterations before the information need is completely satisfied. In such
an interactive retrieval scenario, the information naturally available
to the retrieval system is more than just the current user query and
the document collection - in general, all the interaction history can
be available to the retrieval system, including past queries, 
information about which documents the user has chosen to view, and even
how a user has read a document (e.g., which part of a document the
user spends a lot of time in reading). We define implicit feedback
broadly as exploiting all such naturally available interaction history
to improve retrieval results.
A major advantage of implicit feedback is that we can improve
the retrieval accuracy without requiring any user effort. For 
example, if the current query is java, without knowing any extra
information, it would be impossible to know whether it is intended
to mean the Java programming language or the Java island in 
Indonesia. As a result, the retrieved documents will likely have both
kinds of documents - some may be about the programming 
language and some may be about the island. However, any particular
user is unlikely searching for both types of documents. Such an
ambiguity can be resolved by exploiting history information. For
example, if we know that the previous query from the user is cgi
programming, it would strongly suggest that it is the programming
language that the user is searching for.
Implicit feedback was studied in several previous works. In [11],
Joachims explored how to capture and exploit the clickthrough 
information and demonstrated that such implicit feedback 
information can indeed improve the search accuracy for a group of 
people. In [18], a simulation study of the effectiveness of different
implicit feedback algorithms was conducted, and several retrieval
models designed for exploiting clickthrough information were 
proposed and evaluated. In [17], some existing retrieval algorithms are
adapted to improve search results based on the browsing history of
a user. Other related work on using context includes personalized
search [1, 3, 4, 7, 10], query log analysis [5], context factors [12],
and implicit queries [6].
While the previous work has mostly focused on using 
clickthrough information, in this paper, we use both clickthrough 
information and preceding queries, and focus on developing new
context-sensitive language models for retrieval. Specifically, we
develop models for using implicit feedback information such as
query and clickthrough history of the current search session to 
improve retrieval accuracy. We use the KL-divergence retrieval model
[19] as the basis and propose to treat context-sensitive retrieval as
estimating a query language model based on the current query and
any search context information. We propose several statistical 
language models to incorporate query and clickthrough history into
the KL-divergence model.
One challenge in studying implicit feedback models is that there
does not exist any suitable test collection for evaluation. We thus
use the TREC AP data to create a test collection with implicit 
feedback information, which can be used to quantitatively evaluate 
implicit feedback models. To the best of our knowledge, this is the
first test set for implicit feedback. We evaluate the proposed 
models using this data set. The experimental results show that using
implicit feedback information, especially the clickthrough data, can
substantially improve retrieval performance without requiring 
additional effort from the user.
The remaining sections are organized as follows. In Section 2,
we attempt to define the problem of implicit feedback and introduce
some terms that we will use later. In Section 3, we propose several
implicit feedback models based on statistical language models. In
Section 4, we describe how we create the data set for implicit 
feedback experiments. In Section 5, we evaluate different implicit 
feedback models on the created data set. Section 6 is our conclusions
and future work.
2. PROBLEM DEFINITION
There are two kinds of context information we can use for 
implicit feedback. One is short-term context, which is the immediate
surrounding information which throws light on a user"s current 
information need in a single session. A session can be considered as a
period consisting of all interactions for the same information need.
The category of a user"s information need (e.g., kids or sports), 
previous queries, and recently viewed documents are all examples of
short-term context. Such information is most directly related to the
current information need of the user and thus can be expected to be
most useful for improving the current search. In general, short-term
context is most useful for improving search in the current session,
but may not be so helpful for search activities in a different 
session. The other kind of context is long-term context, which refers
to information such as a user"s education level and general interest,
accumulated user query history and past user clickthrough 
information; such information is generally stable for a long time and is
often accumulated over time. Long-term context can be applicable
to all sessions, but may not be as effective as the short-term context
in improving search accuracy for a particular session. In this paper,
we focus on the short-term context, though some of our methods
can also be used to naturally incorporate some long-term context.
In a single search session, a user may interact with the search
system several times. During interactions, the user would 
continuously modify the query. Therefore for the current query Qk 
(except for the first query of a search session) , there is a query history,
HQ = (Q1, ..., Qk−1) associated with it, which consists of the 
preceding queries given by the same user in the current session. Note
that we assume that the session boundaries are known in this paper.
In practice, we need techniques to automatically discover session
boundaries, which have been studied in [9, 16]. Traditionally, the
retrieval system only uses the current query Qk to do retrieval. But
the short-term query history clearly may provide useful clues about
the user"s current information need as seen in the java example
given in the previous section. Indeed, our previous work [15] has
shown that the short-term query history is useful for improving 
retrieval accuracy.
In addition to the query history, there may be other short-term
context information available. For example, a user would 
presumably frequently click some documents to view. We refer to data
associated with these actions as clickthrough history. The 
clickthrough data may include the title, summary, and perhaps also the
content and location (e.g., the URL) of the clicked document. 
Although it is not clear whether a viewed document is actually 
relevant to the user"s information need, we may safely assume that
the displayed summary/title information about the document is 
attractive to the user, thus conveys information about the user"s 
information need. Suppose we concatenate all the displayed text 
information about a document (usually title and summary) together, we
will also have a clicked summary Ci in each round of retrieval. In
general, we may have a history of clicked summaries C1, ..., Ck−1.
We will also exploit such clickthrough history HC = (C1, ..., Ck−1)
to improve our search accuracy for the current query Qk. Previous
work has also shown positive results using similar clickthrough 
information [11, 17].
Both query history and clickthrough history are implicit 
feedback information, which naturally exists in interactive information
retrieval, thus no additional user effort is needed to collect them. In
this paper, we study how to exploit such information (HQ and HC ),
develop models to incorporate the query history and clickthrough
history into a retrieval ranking function, and quantitatively evaluate
these models.
3. LANGUAGE MODELS FOR 
CONTEXTSENSITIVEINFORMATIONRETRIEVAL
Intuitively, the query history HQ and clickthrough history HC
are both useful for improving search accuracy for the current query
Qk. An important research question is how we can exploit such 
information effectively. We propose to use statistical language 
models to model a user"s information need and develop four specific
context-sensitive language models to incorporate context 
information into a basic retrieval model.
3.1 Basic retrieval model
We use the Kullback-Leibler (KL) divergence method [19] as
our basic retrieval method. According to this model, the retrieval
task involves computing a query language model θQ for a given
query and a document language model θD for a document and then
computing their KL divergence D(θQ||θD), which serves as the
score of the document.
One advantage of this approach is that we can naturally 
incorporate the search context as additional evidence to improve our 
estimate of the query language model.
Formally, let HQ = (Q1, ..., Qk−1) be the query history and
the current query be Qk. Let HC = (C1, ..., Ck−1) be the 
clickthrough history. Note that Ci is the concatenation of all clicked
documents" summaries in the i-th round of retrieval since we may
reasonably treat all these summaries equally. Our task is to estimate
a context query model, which we denote by p(w|θk), based on the
current query Qk, as well as the query history HQ and clickthrough
history HC . We now describe several different language models for
exploiting HQ and HC to estimate p(w|θk). We will use c(w, X)
to denote the count of word w in text X, which could be either a
query or a clicked document"s summary or any other text. We will
use |X| to denote the length of text X or the total number of words
in X.
3.2 Fixed Coefficient Interpolation (FixInt)
Our first idea is to summarize the query history HQ with a 
unigram language model p(w|HQ) and the clickthrough history HC
with another unigram language model p(w|HC ). Then we linearly
interpolate these two history models to obtain the history model
p(w|H). Finally, we interpolate the history model p(w|H) with
the current query model p(w|Qk). These models are defined as
follows.
p(w|Qi) =
c(w, Qi)
|Qi|
p(w|HQ) =
1
k − 1
i=k−1
i=1
p(w|Qi)
p(w|Ci) =
c(w, Ci)
|Ci|
p(w|HC ) =
1
k − 1
i=k−1
i=1
p(w|Ci)
p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ)
p(w|θk) = αp(w|Qk) + (1 − α)p(w|H)
where β ∈ [0, 1] is a parameter to control the weight on each 
history model, and where α ∈ [0, 1] is a parameter to control the
weight on the current query and the history information.
If we combine these equations, we see that
p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)]
That is, the estimated context query model is just a fixed coefficient
interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ).
3.3 Bayesian Interpolation (BayesInt)
One possible problem with the FixInt approach is that the 
coefficients, especially α, are fixed across all the queries. But intuitively,
if our current query Qk is very long, we should trust the current
query more, whereas if Qk has just one word, it may be beneficial
to put more weight on the history. To capture this intuition, we treat
p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed
data to estimate a context query model using Bayesian estimator.
The estimated model is given by
p(w|θk) =
c(w, Qk) + µp(w|HQ) + νp(w|HC )
|Qk| + µ + ν
=
|Qk|
|Qk| + µ + ν
p(w|Qk)+
µ + ν
|Qk| + µ + ν
[
µ
µ + ν
p(w|HQ)+
ν
µ + ν
p(w|HC )]
where µ is the prior sample size for p(w|HQ) and ν is the prior
sample size for p(w|HC ). We see that the only difference between
BayesInt and FixInt is the interpolation coefficients are now 
adaptive to the query length. Indeed, when viewing BayesInt as FixInt,
we see that α = |Qk|
|Qk|+µ+ν
, β = ν
ν+µ
, thus with fixed µ and ν,
we will have a query-dependent α. Later we will show that such an
adaptive α empirically performs better than a fixed α.
3.4 Online Bayesian Updating (OnlineUp)
Both FixInt and BayesInt summarize the history information by
averaging the unigram language models estimated based on 
previous queries or clicked summaries. This means that all previous
queries are treated equally and so are all clicked summaries. 
However, as the user interacts with the system and acquires more 
knowledge about the information in the collection, presumably, the 
reformulated queries will become better and better. Thus assigning 
decaying weights to the previous queries so as to trust a recent query
more than an earlier query appears to be reasonable. Interestingly,
if we incrementally update our belief about the user"s information
need after seeing each query, we could naturally obtain decaying
weights on the previous queries. Since such an incremental online
updating strategy can be used to exploit any evidence in an 
interactive retrieval system, we present it in a more general way.
In a typical retrieval system, the retrieval system responds to
every new query entered by the user by presenting a ranked list
of documents. In order to rank documents, the system must have
some model for the user"s information need. In the KL divergence
retrieval model, this means that the system must compute a query
model whenever a user enters a (new) query. A principled way of
updating the query model is to use Bayesian estimation, which we
discuss below.
3.4.1 Bayesian updating
We first discuss how we apply Bayesian estimation to update a
query model in general. Let p(w|φ) be our current query model
and T be a new piece of text evidence observed (e.g., T can be a
query or a clicked summary). To update the query model based on
T, we use φ to define a Dirichlet prior parameterized as
Dir(µT p(w1|φ), ..., µT p(wN |φ))
where µT is the equivalent sample size of the prior. We use 
Dirichlet prior because it is a conjugate prior for multinomial 
distributions. With such a conjugate prior, the predictive distribution of φ
(or equivalently, the mean of the posterior distribution of φ is given
by
p(w|φ) =
c(w, T) + µT p(w|φ)
|T| + µT
(1)
where c(w, T) is the count of w in T and |T| is the length of T.
Parameter µT indicates our confidence in the prior expressed in
terms of an equivalent text sample comparable with T. For 
example, µT = 1 indicates that the influence of the prior is equivalent to
adding one extra word to T.
3.4.2 Sequential query model updating
We now discuss how we can update our query model over time
during an interactive retrieval process using Bayesian estimation.
In general, we assume that the retrieval system maintains a current
query model φi at any moment. As soon as we obtain some implicit
feedback evidence in the form of a piece of text Ti, we will update
the query model.
Initially, before we see any user query, we may already have
some information about the user. For example, we may have some
information about what documents the user has viewed in the past.
We use such information to define a prior on the query model,
which is denoted by φ0. After we observe the first query Q1, we
can update the query model based on the new observed data Q1.
The updated query model φ1 can then be used for ranking 
documents in response to Q1. As the user views some documents, the
displayed summary text for such documents C1 (i.e., clicked 
summaries) can serve as some new data for us to further update the
query model to obtain φ1. As we obtain the second query Q2 from
the user, we can update φ1 to obtain a new model φ2. In general,
we may repeat such an updating process to iteratively update the
query model.
Clearly, we see two types of updating: (1) updating based on a
new query Qi; (2) updating based on a new clicked summary Ci. In
both cases, we can treat the current model as a prior of the context
query model and treat the new observed query or clicked summary
as observed data. Thus we have the following updating equations:
p(w|φi) =
c(w, Qi) + µip(w|φi−1)
|Qi| + µi
p(w|φi) =
c(w, Ci) + νip(w|φi)
|Ci| + νi
where µi is the equivalent sample size for the prior when updating
the model based on a query, while νi is the equivalent sample size
for the prior when updating the model based on a clicked summary.
If we set µi = 0 (or νi = 0) we essentially ignore the prior model,
thus would start a completely new query model based on the query
Qi (or the clicked summary Ci). On the other hand, if we set µi =
+∞ (or νi = +∞) we essentially ignore the observed query (or
the clicked summary) and do not update our model. Thus the model
remains the same as if we do not observe any new text evidence. In
general, the parameters µi and νi may have different values for
different i. For example, at the very beginning, we may have very
sparse query history, thus we could use a smaller µi, but later as the
query history is richer, we can consider using a larger µi. But in
our experiments, unless otherwise stated, we set them to the same
constants, i.e., ∀i, j, µi = µj, νi = νj.
Note that we can take either p(w|φi) or p(w|φi) as our context
query model for ranking documents. This suggests that we do not
have to wait until a user enters a new query to initiate a new round
of retrieval; instead, as soon as we collect clicked summary Ci, we
can update the query model and use p(w|φi) to immediately rerank
any documents that a user has not yet seen.
To score documents after seeing query Qk, we use p(w|φk), i.e.,
p(w|θk) = p(w|φk)
3.5 Batch Bayesian updating (BatchUp)
If we set the equivalent sample size parameters to fixed 
constant, the OnlineUp algorithm would introduce a decaying factor
- repeated interpolation would cause the early data to have a low
weight. This may be appropriate for the query history as it is 
reasonable to believe that the user becomes better and better at query
formulation as time goes on, but it is not necessarily appropriate for
the clickthrough information, especially because we use the 
displayed summary, rather than the actual content of a clicked 
document. One way to avoid applying a decaying interpolation to
the clickthrough data is to do OnlineUp only for the query history
Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first
buffer all the clickthrough data together and use the whole chunk
of clickthrough data to update the model generated through 
running OnlineUp on previous queries. The updating equations are as
follows.
p(w|φi) =
c(w, Qi) + µip(w|φi−1)
|Qi| + µi
p(w|ψi) =
i−1
j=1 c(w, Cj) + νip(w|φi)
i−1
j=1 |Cj| + νi
where µi has the same interpretation as in OnlineUp, but νi now
indicates to what extent we want to trust the clicked summaries. As
in OnlineUp, we set all µi"s and νi"s to the same value. And to rank
documents after seeing the current query Qk, we use
p(w|θk) = p(w|ψk)
4. DATA COLLECTION
In order to quantitatively evaluate our models, we need a data set
which includes not only a text database and testing topics, but also
query history and clickthrough history for each topic. Since there
is no such data set available to us, we have to create one. There
are two choices. One is to extract topics and any associated query
history and clickthrough history for each topic from the log of a
retrieval system (e.g., search engine). But the problem is that we
have no relevance judgments on such data. The other choice is to
use a TREC data set, which has a text database, topic description
and relevance judgment file. Unfortunately, there are no query 
history and clickthrough history data. We decide to augment a TREC
data set by collecting query history and clickthrough history data.
We select TREC AP88, AP89 and AP90 data as our text database,
because AP data has been used in several TREC tasks and has 
relatively complete judgments. There are altogether 242918 news 
articles and the average document length is 416 words. Most articles
have titles. If not, we select the first sentence of the text as the 
title. For the preprocessing, we only do case folding and do not do
stopword removal or stemming.
We select 30 relatively difficult topics from TREC topics 1-150.
These 30 topics have the worst average precision performance among
TREC topics 1-150 according to some baseline experiments using
the KL-Divergence model with Bayesian prior smoothing [20]. The
reason why we select difficult topics is that the user then would
have to have several interactions with the retrieval system in order
to get satisfactory results so that we can expect to collect a 
relatively richer query history and clickthrough history data from the
user. In real applications, we may also expect our models to be
most useful for such difficult topics, so our data collection strategy
reflects the real world applications well.
We index the TREC AP data set and set up a search engine and
web interface for TREC AP news articles. We use 3 subjects to do
experiments to collect query history and clickthrough history data.
Each subject is assigned 10 topics and given the topic descriptions
provided by TREC. For each topic, the first query is the title of
the topic given in the original TREC topic description. After the
subject submits the query, the search engine will do retrieval and
return a ranked list of search results to the subject. The subject will
browse the results and maybe click one or more results to browse
the full text of article(s). The subject may also modify the query to
do another search. For each topic, the subject composes at least 4
queries. In our experiment, only the first 4 queries for each topic
are used. The user needs to select the topic number from a 
selection menu before submitting the query to the search engine so that
we can easily detect the session boundary, which is not the focus of
our study. We use a relational database to store user interactions,
including the submitted queries and clicked documents. For each
query, we store the query terms and the associated result pages.
And for each clicked document, we store the summary as shown
on the search result page. The summary of the article is query 
dependent and is computed online using fixed-length passage retrieval
(KL divergence model with Bayesian prior smoothing).
Among 120 (4 for each of 30 topics) queries which we study in
the experiment, the average query length is 3.71 words. Altogether
there are 91 documents clicked to view. So on average, there are
around 3 clicks per topic. The average length of clicked summary
FixInt BayesInt OnlineUp BatchUp
Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0)
MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs
q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317
q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150
q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100
Improve. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3%
q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483
q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067
Improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4%
q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933
q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250
Improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4%
Table 1: Effect of using query history and clickthrough data for document ranking.
is 34.4 words. Among 91 clicked documents, 29 documents are
judged relevant according to TREC judgment file. This data set is
publicly available 1
.
5. EXPERIMENTS
5.1 Experiment design
Our major hypothesis is that using search context (i.e., query 
history and clickthrough information) can help improve search 
accuracy. In particular, the search context can provide extra information
to help us estimate a better query model than using just the current
query. So most of our experiments involve comparing the retrieval
performance using the current query only (thus ignoring any 
context) with that using the current query as well as the search context.
Since we collected four versions of queries for each topic, we
make such comparisons for each version of queries. We use two
performance measures: (1) Mean Average Precision (MAP): This
is the standard non-interpolated average precision and serves as a
good measure of the overall ranking accuracy. (2) Precision at 20
documents (pr@20docs): This measure does not average well, but
it is more meaningful than MAP and reflects the utility for users
who only read the top 20 documents. In all cases, the reported
figure is the average over all of the 30 topics.
We evaluate the four models for exploiting search context (i.e.,
FixInt, BayesInt, OnlineUp, and BatchUp). Each model has 
precisely two parameters (α and β for FixInt; µ and ν for others).
Note that µ and ν may need to be interpreted differently for 
different methods. We vary these parameters and identify the optimal
performance for each method. We also vary the parameters to study
the sensitivity of our algorithms to the setting of the parameters.
5.2 Result analysis
5.2.1 Overall effect of search context
We compare the optimal performances of four models with those
using the current query only in Table 1. A row labeled with qi is
the baseline performance and a row labeled with qi + HQ + HC
is the performance of using search context. We can make several
observations from this table:
1. Comparing the baseline performances indicates that on average
reformulated queries are better than the previous queries with the
performance of q4 being the best. Users generally formulate better
and better queries.
2. Using search context generally has positive effect, especially
when the context is rich. This can be seen from the fact that the
1
http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip
improvement for q4 and q3 is generally more substantial compared
with q2. Actually, in many cases with q2, using the context may
hurt the performance, probably because the history at that point is
sparse. When the search context is rich, the performance 
improvement can be quite substantial. For example, BatchUp achieves
92.4% improvement in the mean average precision over q3 and
77.2% improvement over q4. (The generally low precisions also
make the relative improvement deceptively high, though.)
3. Among the four models using search context, the performances
of FixInt and OnlineUp are clearly worse than those of BayesInt
and BatchUp. Since BayesInt performs better than FixInt and the
main difference between BayesInt and FixInt is that the former uses
an adaptive coefficient for interpolation, the results suggest that 
using adaptive coefficient is quite beneficial and a Bayesian style 
interpolation makes sense. The main difference between OnlineUp
and BatchUp is that OnlineUp uses decaying coefficients to 
combine the multiple clicked summaries, while BatchUp simply 
concatenates all clicked summaries. Therefore the fact that BatchUp
is consistently better than OnlineUp indicates that the weights for
combining the clicked summaries indeed should not be decaying.
While OnlineUp is theoretically appealing, its performance is 
inferior to BayesInt and BatchUp, likely because of the decaying 
coefficient. Overall, BatchUp appears to be the best method when we
vary the parameter settings.
We have two different kinds of search context - query history
and clickthrough data. We now look into the contribution of each
kind of context.
5.2.2 Using query history only
In each of four models, we can turn off the clickthrough 
history data by setting parameters appropriately. This allows us to
evaluate the effect of using query history alone. We use the same
parameter setting for query history as in Table 1. The results are
shown in Table 2. Here we see that in general, the benefit of using
query history is very limited with mixed results. This is different
from what is reported in a previous study [15], where using query
history is consistently helpful. Another observation is that the 
context runs perform poorly at q2, but generally perform (slightly) 
better than the baselines for q3 and q4. This is again likely because
at the beginning the initial query, which is the title in the original
TREC topic description, may not be a good query; indeed, on 
average, performances of these first-generation queries are clearly
poorer than those of all other user-formulated queries in the later
generations. Yet another observation is that when using query 
history only, the BayesInt model appears to be better than other 
models. Since the clickthrough data is ignored, OnlineUp and BatchUp
FixInt BayesInt OnlineUp BatchUp
Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞)
MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs
q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150
q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967
Improve. -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9%
q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483
q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450
Improve -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2%
q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933
q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917
Improve -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8%
Table 2: Effect of using query history only for document ranking.
µ 0 0.5 1 2 3 4 5 6 7 8 9
q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164
q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335
q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513
Table 3: Average Precision of BatchUp using query history only
are essentially the same algorithm. The displayed results thus 
reflect the variation caused by parameter µ. A smaller setting of 2.0
is seen better than a larger value of 5.0. A more complete picture
of the influence of the setting of µ can be seen from Table 3, where
we show the performance figures for a wider range of values of µ.
The value of µ can be interpreted as how many words we regard
the query history is worth. A larger value thus puts more weight
on the history and is seen to hurt the performance more when the
history information is not rich. Thus while for q4 the best 
performance tends to be achieved for µ ∈ [2, 5], only when µ = 0.5 we
see some small benefit for q2. As we would expect, an excessively
large µ would hurt the performance in general, but q2 is hurt most
and q4 is barely hurt, indicating that as we accumulate more and
more query history information, we can put more and more weight
on the history information. This also suggests that a better strategy
should probably dynamically adjust parameters according to how
much history information we have.
The mixed query history results suggest that the positive effect
of using implicit feedback information may have largely come from
the use of clickthrough history, which is indeed true as we discuss
in the next subsection.
5.2.3 Using clickthrough history only
We now turn off the query history and only use the clicked 
summaries plus the current query. The results are shown in Table 4. We
see that the benefit of using clickthrough information is much more
significant than that of using query history. We see an overall 
positive effect, often with significant improvement over the baseline. It
is also clear that the richer the context data is, the more 
improvement using clicked summaries can achieve. Other than some 
occasional degradation of precision at 20 documents, the improvement
is fairly consistent and often quite substantial.
These results show that the clicked summary text is in general
quite useful for inferring a user"s information need. Intuitively, 
using the summary text, rather than the actual content of the 
document, makes more sense, as it is quite possible that the document
behind a seemingly relevant summary is actually non-relevant.
29 out of the 91 clicked documents are relevant. Updating the
query model based on such summaries would bring up the ranks
of these relevant documents, causing performance improvement.
However, such improvement is really not beneficial for the user as
the user has already seen these relevant documents. To see how
much improvement we have achieved on improving the ranks of
the unseen relevant documents, we exclude these 29 relevant 
documents from our judgment file and recompute the performance of
BayesInt and the baseline using the new judgment file. The results
are shown in Table 5. Note that the performance of the baseline
method is lower due to the removal of the 29 relevant documents,
which would have been generally ranked high in the results. From
Table 5, we see clearly that using clicked summaries also helps 
improve the ranks of unseen relevant documents significantly.
Query BayesInt(µ = 0, ν = 5.0)
MAP pr@20docs
q2 0.0263 0.100
q2 + HC 0.0314 0.100
Improve. 19.4% 0%
q3 0.0331 0.125
q3 + HC 0.0661 0.178
Improve 99.7% 42.4%
q4 0.0442 0.165
q4 + HC 0.0739 0.188
Improve 67.2% 13.9%
Table 5: BayesInt evaluated on unseen relevant documents
One remaining question is whether the clickthrough data is still
helpful if none of the clicked documents is relevant. To answer
this question, we took out the 29 relevant summaries from our
clickthrough history data HC to obtain a smaller set of clicked
summaries HC , and re-evaluated the performance of the BayesInt
method using HC with the same setting of parameters as in 
Table 4. The results are shown in Table 6. We see that although the
improvement is not as substantial as in Table 4, the average 
precision is improved across all generations of queries. These results
should be interpreted as very encouraging as they are based on only
62 non-relevant clickthroughs. In reality, a user would more likely
click some relevant summaries, which would help bring up more
relevant documents as we have seen in Table 4 and Table 5.
FixInt BayesInt OnlineUp BatchUp
Query (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i < k, µi = +∞) (µ = 0, ν = 15)
MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs
q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150
q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167
Improve. 3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5%
q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483
q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.0513 0.1650
Improve 72.4% 32.6% 81.9% 37.1% 47.7% 19.2% 21.9% 11.3%
q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930
q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050
Improve 66.2% 15.5% 72.6% 18.1% 44.0% 14.7% 16.2% 6.1%
Table 4: Effect of using clickthrough data only for document ranking.
Query BayesInt(µ = 0, ν = 5.0)
MAP pr@20docs
q2 0.0312 0.1150
q2 + HC 0.0313 0.0950
Improve. 0.3% -17.4%
q3 0.0421 0.1483
q3 + HC 0.0521 0.1820
Improve 23.8% 23.0%
q4 0.0536 0.1930
q4 + HC 0.0620 0.1850
Improve 15.7% -4.1%
Table 6: Effect of using only non-relevant clickthrough data
5.2.4 Additive effect of context information
By comparing the results across Table 1, Table 2 and Table 4,
we can see that the benefit of the query history information and
that of clickthrough information are mostly additive, i.e., 
combining them can achieve better performance than using each alone,
but most improvement has clearly come from the clickthrough 
information. In Table 7, we show this effect for the BatchUp method.
5.2.5 Parameter sensitivity
All four models have two parameters to control the relative weights
of HQ, HC , and Qk, though the parameterization is different from
model to model. In this subsection, we study the parameter 
sensitivity for BatchUp, which appears to perform relatively better than
others. BatchUp has two parameters µ and ν.
We first look at µ. When µ is set to 0, the query history is not
used at all, and we essentially just use the clickthrough data 
combined with the current query. If we increase µ, we will gradually
incorporate more information from the previous queries. In Table 8,
we show how the average precision of BatchUp changes as we vary
µ with ν fixed to 15.0, where the best performance of BatchUp is
achieved. We see that the performance is mostly insensitive to the
change of µ for q3 and q4, but is decreasing as µ increases for q2.
The pattern is also similar when we set ν to other values.
In addition to the fact that q1 is generally worse than q2, q3, and
q4, another possible reason why the sensitivity is lower for q3 and
q4 may be that we generally have more clickthrough data 
available for q3 and q4 than for q2, and the dominating influence of the
clickthrough data has made the small differences caused by µ less
visible for q3 and q4.
The best performance is generally achieved when µ is around
2.0, which means that the past query information is as useful as
about 2 words in the current query. Except for q2, there is clearly
some tradeoff between the current query and the previous queries
Query MAP pr@20docs
q2 0.0312 0.1150
q2 + HQ 0.0287 0.0967
Improve. -8.0% -15.9%
q2 + HC 0.0344 0.1167
Improve. 10.3% 1.5%
q2 + HQ + HC 0.0342 0.1100
Improve. 9.6% -4.3%
q3 0.0421 0.1483
q3 + HQ 0.0455 0.1450
Improve 8.1% -2.2%
q3 + HC 0.0513 0.1650
Improve 21.9% 11.3%
q3 + HQ + HC 0.0810 0.2067
Improve 92.4% 39.4%
q4 0.0536 0.1930
q4 + HQ 0.0552 0.1917
Improve 3.0% -0.8%
q4 + HC 0.0623 0.2050
Improve 16.2% 6.1%
q4 + HQ + HC 0.0950 0.2250
Improve 77.2% 16.4%
Table 7: Additive benefit of context information
and using a balanced combination of them achieves better 
performance than using each of them alone.
We now turn to the other parameter ν. When ν is set to 0, we
only use the clickthrough data; When ν is set to +∞, we only use
the query history and the current query. With µ set to 2.0, where
the best performance of BatchUp is achieved, we vary ν and show
the results in Table 9. We see that the performance is also not very
sensitive when ν ≤ 30, with the best performance often achieved
at ν = 15. This means that the combined information of query
history and the current query is as useful as about 15 words in the
clickthrough data, indicating that the clickthrough information is
highly valuable.
Overall, these sensitivity results show that BatchUp not only 
performs better than other methods, but also is quite robust.
6. CONCLUSIONS AND FUTURE WORK
In this paper, we have explored how to exploit implicit 
feedback information, including query history and clickthrough history
within the same search session, to improve information retrieval
performance. Using the KL-divergence retrieval model as the 
basis, we proposed and studied four statistical language models for
context-sensitive information retrieval, i.e., FixInt, BayesInt, 
OnlineUp and BatchUp. We use TREC AP Data to create a test set
µ 0 1 2 3 4 5 6 7 8 9 10
MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219
q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750
MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788
q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000
MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929
q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333
Table 8: Sensitivity of µ in BatchUp
ν 0 1 2 5 10 15 30 100 300 500
MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290
q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967
MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491
q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550
MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625
q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033
Table 9: Sensitivity of ν in BatchUp
for evaluating implicit feedback models. Experiment results show
that using implicit feedback, especially clickthrough history, can
substantially improve retrieval performance without requiring any
additional user effort.
The current work can be extended in several ways: First, we
have only explored some very simple language models for 
incorporating implicit feedback information. It would be interesting to
develop more sophisticated models to better exploit query history
and clickthrough history. For example, we may treat a clicked 
summary differently depending on whether the current query is a 
generalization or refinement of the previous query. Second, the 
proposed models can be implemented in any practical systems. We are
currently developing a client-side personalized search agent, which
will incorporate some of the proposed algorithms. We will also do
a user study to evaluate effectiveness of these models in the real
web search. Finally, we should further study a general retrieval
framework for sequential decision making in interactive 
information retrieval and study how to optimize some of the parameters in
the context-sensitive retrieval models.
7. ACKNOWLEDGMENTS
This material is based in part upon work supported by the 
National Science Foundation under award numbers IIS-0347933 and
IIS-0428472. We thank the anonymous reviewers for their useful
comments.
8. REFERENCES
[1] E. Adar and D. Karger. Haystack: Per-user information
environments. In Proceedings of CIKM 1999, 1999.
[2] J. Allan and et al. Challenges in information retrieval and
language modeling. Workshop at University of Amherst,
2002.
[3] K. Bharat. Searchpad: Explicit capture of search context to
support web search. In Proceeding of WWW 2000, 2000.
[4] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.
Relevance feedback and personalization: A language
modeling perspective. In Proeedings of Second DELOS
Workshop: Personalisation and Recommender Systems in
Digital Libraries, 2001.
[5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. Probabilistic
query expansion using query logs. In Proceedings of WWW
2002, 2002.
[6] S. T. Dumais, E. Cutrell, R. Sarin, and E. Horvitz. Implicit
queries (IQ) for contextualized search (demo description). In
Proceedings of SIGIR 2004, page 594, 2004.
[7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan,
G. Wolfman, and E. Ruppin. Placing search in context: The
concept revisited. In Proceedings of WWW 2002, 2001.
[8] C. Huang, L. Chien, and Y. Oyang. Query session based term
suggestion for interactive web search. In Proceedings of
WWW 2001, 2001.
[9] X. Huang, F. Peng, A. An, and D. Schuurmans. Dynamic
web log session identification with statistical language
models. Journal of the American Society for Information
Science and Technology, 55(14):1290-1303, 2004.
[10] G. Jeh and J. Widom. Scaling personalized web search. In
Proceeding of WWW 2003, 2003.
[11] T. Joachims. Optimizing search engines using clickthrough
data. In Proceedings of SIGKDD 2002, 2002.
[12] D. Kelly and N. J. Belkin. Display time as implicit feedback:
Understanding task effects. In Proceedings of SIGIR 2004,
2004.
[13] D. Kelly and J. Teevan. Implicit feedback for inferring user
preference. SIGIR Forum, 32(2), 2003.
[14] J. Rocchio. Relevance feedback information retrieval. In The
Smart Retrieval System-Experiments in Automatic Document
Processing, pages 313-323, Kansas City, MO, 1971.
Prentice-Hall.
[15] X. Shen and C. Zhai. Exploiting query history for document
ranking in interactive information retrieval (poster). In
Proceedings of SIGIR 2003, 2003.
[16] S. Sriram, X. Shen, and C. Zhai. A session-based search
engine (poster). In Proceedings of SIGIR 2004, 2004.
[17] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web
search based on user profile constructed without any effort
from users. In Proceedings of WWW 2004, 2004.
[18] R. W. White, J. M. Jose, C. J. van Rijsbergen, and
I. Ruthven. A simulated study of implicit feedback models.
In Proceedings of ECIR 2004, pages 311-326, 2004.
[19] C. Zhai and J. Lafferty. Model-based feedback in the
KL-divergence retrieval model. In Proceedings of CIKM
2001, 2001.
[20] C. Zhai and J. Lafferty. A study of smoothing methods for
language models applied to ad-hoc information retrieval. In
Proceedings of SIGIR 2001, 2001.
Event Threading within News Topics
Ramesh Nallapati, Ao Feng, Fuchun Peng, James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
nmramesh,aofeng,fuchun,allan @cs.umass.edu
ABSTRACT
With the overwhelming volume of online news available today,
there is an increasing need for automatic techniques to analyze and
present news to the user in a meaningful and efficient manner. 
Previous research focused only on organizing news stories by their
topics into a flat hierarchy. We believe viewing a news topic as a
flat collection of stories is too restrictive and inefficient for a user
to understand the topic quickly.
In this work, we attempt to capture the rich structure of events
and their dependencies in a news topic through our event models.
We call the process of recognizing events and their dependencies
event threading. We believe our perspective of modeling the 
structure of a topic is more effective in capturing its semantics than a flat
list of on-topic stories.
We formally define the novel problem, suggest evaluation 
metrics and present a few techniques for solving the problem. Besides
the standard word based features, our approaches take into account
novel features such as temporal locality of stories for event 
recognition and time-ordering for capturing dependencies. Our 
experiments on a manually labeled data sets show that our models 
effectively identify the events and capture dependencies among them.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering
General Terms
Algorithms, Experimentation, Measurement
1. INTRODUCTION
News forms a major portion of information disseminated in the
world everyday. Common people and news analysts alike are very
interested in keeping abreast of new things that happen in the news,
but it is becoming very difficult to cope with the huge volumes
of information that arrives each day. Hence there is an increasing
need for automatic techniques to organize news stories in a way that
helps users interpret and analyze them quickly. This problem is 
addressed by a research program called Topic Detection and Tracking
(TDT) [3] that runs an open annual competition on standardized
tasks of news organization.
One of the shortcomings of current TDT evaluation is its view of
news topics as flat collection of stories. For example, the detection
task of TDT is to arrange a collection of news stories into clusters
of topics. However, a topic in news is more than a mere collection
of stories: it is characterized by a definite structure of inter-related
events. This is indeed recognized by TDT which defines a topic as
‘a set of news stories that are strongly related by some seminal 
realworld event" where an event is defined as ‘something that happens
at a specific time and location" [3]. For example, when a bomb
explodes in a building, that is the seminal event that triggers the
topic. Other events in the topic may include the rescue attempts,
the search for perpetrators, arrests and trials and so on. We see
that there is a pattern of dependencies between pairs of events in
the topic. In the above example, the event of rescue attempts is
‘influenced" by the event of bombing and so is the event of search
for perpetrators.
In this work we investigate methods for modeling the structure
of a topic in terms of its events. By structure, we mean not only
identifying the events that make up a topic, but also establishing
dependencies-generally causal-among them. We call the 
process of recognizing events and identifying dependencies among
them event threading, an analogy to email threading that shows
connections between related email messages. We refer to the 
resulting interconnected structure of events as the event model of the
topic. Although this paper focuses on threading events within an
existing news topic, we expect that such event based dependency
structure more accurately reflects the structure of news than strictly
bounded topics do. From a user"s perspective, we believe that our
view of a news topic as a set of interconnected events helps him/her
get a quick overview of the topic and also allows him/her navigate
through the topic faster.
The rest of the paper is organized as follows. In section 2, we
discuss related work. In section 3, we define the problem and use
an example to illustrate threading of events within a news topic. In
section 4, we describe how we built the corpus for our problem.
Section 5 presents our evaluation techniques while section 6 
describes the techniques we use for modeling event structure. In 
section 7 we present our experiments and results. Section 8 concludes
the paper with a few observations on our results and comments on
future work.
446
2. RELATED WORK
The process of threading events together is related to threading
of electronic mail only by name for the most part. Email usually
incorporates a strong structure of referenced messages and 
consistently formatted subject headings-though information retrieval
techniques are useful when the structure breaks down [7]. Email
threading captures reference dependencies between messages and
does not attempt to reflect any underlying real-world structure of
the matter under discussion.
Another area of research that looks at the structure within a topic
is hierarchical text classification of topics [9, 6]. The hierarchy
within a topic does impose a structure on the topic, but we do not
know of an effort to explore the extent to which that structure 
reflects the underlying event relationships.
Barzilay and Lee [5] proposed a content structure modeling
technique where topics within text are learnt using unsupervised
methods, and a linear order of these topics is modeled using hidden
Markov models. Our work differs from theirs in that we do not 
constrain the dependency to be linear. Also their algorithms are tuned
to work on specific genres of topics such as earthquakes, accidents,
etc., while we expect our algorithms to generalize over any topic.
In TDT, researchers have traditionally considered topics as 
flatclusters [1]. However, in TDT-2003, a hierarchical structure of
topic detection has been proposed and [2] made useful attempts
to adopt the new structure. However this structure still did not 
explicitly model any dependencies between events.
In a work closest to ours, Makkonen [8] suggested modeling
news topics in terms of its evolving events. However, the paper
stopped short of proposing any models to the problem. Other 
related work that dealt with analysis within a news topic includes
temporal summarization of news topics [4].
3. PROBLEM DEFINITION AND NOTATION
In this work, we have adhered to the definition of event and topic
as defined in TDT. We present some definitions (in italics) and our
interpretations (regular-faced) below for clarity.
1. Story: A story is a news article delivering some information
to users. In TDT, a story is assumed to refer to only a single
topic. In this work, we also assume that each story discusses
a single event. In other words, a story is the smallest atomic
unit in the hierarchy (topic event story). Clearly, both
the assumptions are not necessarily true in reality, but we
accept them for simplicity in modeling.
2. Event: An event is something that happens at some specific
time and place [10]. In our work, we represent an event by
a set of stories that discuss it. Following the assumption of
atomicity of a story, this means that any set of distinct events
can be represented by a set of non-overlapping clusters of
news stories.
3. Topic: A set of news stories strongly connected by a seminal
event. We expand on this definition and interpret a topic as
a series of related events. Thus a topic can be represented
by clusters of stories each representing an event and a set of
(directed or undirected) edges between pairs of these clusters
representing the dependencies between these events. We will
describe this representation of a topic in more detail in the
next section.
4. Topic detection and tracking (TDT) :Topic detection 
detects clusters of stories that discuss the same topic; Topic
tracking detects stories that discuss a previously known topic [3].
Thus TDT concerns itself mainly with clustering stories into
topics that discuss them.
5. Event threading: Event threading detects events within in a
topic, and also captures the dependencies among the events.
Thus the main difference between event threading and TDT
is that we focus our modeling effort on microscopic events
rather than larger topics. Additionally event threading 
models the relatedness or dependencies between pairs of events
in a topic while TDT models topics as unrelated clusters of
stories.
We first define our problem and representation of our model
formally and then illustrate with the help of an example. We are
given a set of Ò news stories Ë ×½ ¡ ¡ ¡ ×Ò on a given topic
Ì and their time of publication. We define a set of events
½ ¡ ¡ ¡ Ñ with the following constraints:
¾ ¾
Ë (1)
× Ø (2)
× ¾ × Ø × ¾ (3)
While the first constraint says that each event is an element in the
power set of S, the second constraint ensures that each story can
belong to at most one event. The last constraint tells us that every
story belongs to one of the events in . In fact this allows us to
define a mapping function from stories to events as follows:
´× µ iff × ¾ (4)
Further, we also define a set of directed edges ´ µ
which denote dependencies between events. It is important to 
explain what we mean by this directional dependency: While the 
existence of an edge itself represents relatedness of two events, the
direction could imply causality or temporal-ordering. By causal
dependency we mean that the occurrence of event B is related to
and is a consequence of the occurrence of event A. By temporal 
ordering, we mean that event B happened after event A and is related
to A but is not necessarily a consequence of A. For example, 
consider the following two events: ‘plane crash" (event A) and 
‘subsequent investigations" (event B) in a topic on a plane crash incident.
Clearly, the investigations are a result of the crash. Hence an 
arrow from A to B falls under the category of causal dependency.
Now consider the pair of events ‘Pope arrives in Cuba"(event A)
and ‘Pope meets Castro"(event B) in a topic that discusses Pope"s
visit to Cuba. Now events A and B are closely related through their
association with the Pope and Cuba but event B is not necessarily
a consequence of the occurrence of event A. An arrow in such 
scenario captures what we call time ordering. In this work, we do not
make an attempt to distinguish between these two kinds of 
dependencies and our models treats them as identical. A simpler (and
hence less controversial) choice would be to ignore direction in the
dependencies altogether and consider only undirected edges. This
choice definitely makes sense as a first step but we chose the former
since we believe directional edges make more sense to the user as
they provide a more illustrative flow-chart perspective to the topic.
To make the idea of event threading more concrete, consider the
example of TDT3 topic 30005, titled ‘Osama bin Laden"s 
Indictment" (in the 1998 news). This topic has 23 stories which form 5
events. An event model of this topic can be represented as in figure
1. Each box in the figure indicates an event in the topic of Osama"s
indictment. The occurrence of event 2, namely ‘Trial and 
Indictment of Osama" is dependent on the event of ‘evidence gathered
by CIA", i.e., event 1. Similarly, event 2 influences the occurrences
of events 3, 4 and 5, namely ‘Threats from Militants", ‘Reactions
447
from Muslim World" and ‘announcement of reward". Thus all the
dependencies in the example are causal.
Extending our notation further, we call an event A a parent of B
and B the child of A, if ´ µ ¾ . We define an event model
Å ´ µ to be a tuple of the set of events and set of 
dependencies.
Trial and
(5)
(3)
(4)
CIA announces reward
Muslim world
Reactions from
Islamic militants
Threats from
(2)
(1)
Osama
Indictment of
CIA
gathered by
Evidence
Figure 1: An event model of TDT topic ‘Osama bin Laden"s
indictment".
Event threading is strongly related to topic detection and 
tracking, but also different from it significantly. It goes beyond topics,
and models the relationships between events. Thus, event 
threading can be considered as a further extension of topic detection and
tracking and is more challenging due to at least the following 
difficulties.
1. The number of events is unknown.
2. The granularity of events is hard to define.
3. The dependencies among events are hard to model.
4. Since it is a brand new research area, no standard evaluation
metrics and benchmark data is available.
In the next few sections, we will describe our attempts to tackle
these problems.
4. LABELED DATA
We picked 28 topics from the TDT2 corpus and 25 topics from
the TDT3 corpus. The criterion we used for selecting a topic is that
it should contain at least 15 on-topic stories from CNN headline
news. If the topic contained more than 30 CNN stories, we picked
only the first 30 stories to keep the topic short enough for 
annotators. The reason for choosing only CNN as the source is that the
stories from this source tend to be short and precise and do not tend
to digress or drift too far away from the central theme. We believe
modeling such stories would be a useful first step before dealing
with more complex data sets.
We hired an annotator to create truth data. Annotation includes
defining the event membership for each story and also the 
dependencies. We supervised the annotator on a set of three topics that
we did our own annotations on and then asked her to annotate the
28 topics from TDT2 and 25 topics from TDT3.
In identifying events in a topic, the annotator was asked to broadly
follow the TDT definition of an event, i.e., ‘something that happens
at a specific time and location". The annotator was encouraged to
merge two events A and B into a single event C if any of the 
stories discusses both A and B. This is to satisfy our assumption that
each story corresponds to a unique event. The annotator was also
encouraged to avoid singleton events, events that contain a single
news story, if possible. We realized from our own experience that
people differ in their perception of an event especially when the
number of stories in that event is small. As part of the guidelines,
we instructed the annotator to assign titles to all the events in each
topic. We believe that this would help make her understanding of
the events more concrete. We however, do not use or model these
titles in our algorithms.
In defining dependencies between events, we imposed no 
restrictions on the graph structure. Each event could have single, 
multiple or no parents. Further, the graph could have cycles or 
orphannodes. The annotator was however instructed to assign a 
dependency from event A to event B if and only if the occurrence of B
is ‘either causally influenced by A or is closely related to A and
follows A in time".
From the annotated topics, we created a training set of 26 topics
and a test set of 27 topics by merging the 28 topics from TDT2 and
25 from TDT3 and splitting them randomly. Table 1 shows that the
training and test sets have fairly similar statistics.
Feature Training set Test set
Num. topics 26 27
Avg. Num. Stories/Topic 28.69 26.74
Avg. Doc. Len. 64.60 64.04
Avg. Num. Stories/Event 5.65 6.22
Avg. Num. Events/Topic 5.07 4.29
Avg. Num. Dependencies/Topic 3.07 2.92
Avg. Num. Dependencies/Event 0.61 0.68
Avg. Num. Days/Topic 30.65 34.48
Table 1: Statistics of annotated data
5. EVALUATION
A system can generate some event model Å¼ ´
¼ ¼µ using
certain algorithms, which is usually different from the truth model
Å ´ µ (we assume the annotator did not make any 
mistake). Comparing a system event model Å¼ with the true model
Å requires comparing the entire event models including their 
dependency structure. And different event granularities may bring
huge discrepancy between Å¼ and Å. This is certainly non-trivial
as even testing whether two graphs are isomorphic has no known
polynomial time solution. Hence instead of comparing the actual
structure we examine a pair of stories at a time and verify if the
system and true labels agree on their event-memberships and 
dependencies. Specifically, we compare two kinds of story pairs:
¯ Cluster pairs ( ´Åµ): These are the complete set of 
unordered pairs ´× × µ of stories × and × that fall within the
same event given a model Å. Formally,
´Åµ ´× × µ × × ¾ Ë ´× µ ´× µ (5)
where is the function in Å that maps stories to events as
defined in equation 4.
¯ Dependency pairs ( ´Åµ): These are the set of all ordered
pairs of stories ´× × µ such that there is a dependency from
the event of × to the event of × in the model Å.
´Åµ ´× × µ ´ ´× µ ´× µµ ¾ (6)
Note the story pair is ordered here, so ´× × µ is not 
equivalent to ´× × µ. In our evaluation, a correct pair with wrong
448
(B->D)
Cluster pairs
(A,C)
Dependency pairs
(A->B)
(C->B)
(B->D)
D,E
D,E
(D,E)
(D,E)
(A->C) (A->E)
(B->C) (B->E)
(B->E)
Cluster precision: 1/2
Cluster Recall: 1/2
Dependency Recall: 2/6
Dependency Precision: 2/4
(A->D)
True event model System event model
A,B
C
A,C B
Cluster pairs
(A,B)
Dependency pairs
Figure 2: Evaluation measures
direction will be considered a mistake. As we mentioned 
earlier in section 3, ignoring the direction may make the 
problem simpler, but we will lose the expressiveness of our 
representation.
Given these two sets of story pairs corresponding to the true
event model Å and the system event model Å¼, we define recall
and precision for each category as follows.
¯ Cluster Precision (CP): It is the probability that two 
randomly selected stories × and × are in the same true-event
given that they are in the same system event.
È È´ ´× µ ´× µ
¼´× µ
¼´× µµ
´Åµ ´Å¼µ
´Å¼µ
(7)
where ¼ is the story-event mapping function corresponding
to the model Å¼.
¯ Cluster Recall(CR): It is the probability that two randomly
selected stories × and × are in the same system-event given
that they are in the same true event.
Ê È´
¼´× µ
¼´× µ ´× µ ´× µµ
´Åµ ´Å¼µ
´Åµ
(8)
¯ Dependency Precision(DP): It is the probability that there is
a dependency between the events of two randomly selected
stories × and × in the true model Å given that they have a
dependency in the system model Å¼. Note that the direction
of dependency is important in comparison.
È È´´ ´× µ ´× µµ ¾ ´
¼´× µ
¼´× µµ ¾
¼µ
´Åµ ´Å¼µ
´Å¼µ
(9)
¯ Dependency Recall(DR): It is the probability that there is
a dependency between the events of two randomly selected
stories × and × in the system model Å¼ given that they have
a dependency in the true model Å. Again, the direction of
dependency is taken into consideration.
Ê È´´
¼´× µ
¼´× µµ ¾
¼ ´ ´× µ ´× µµ ¾ µ
´Åµ ´Å¼µ
´Åµ
(10)
The measures are illustrated by an example in figure 2. We also
combine these measures using the well known F1-measure 
commonly used in text classification and other research areas as shown
below.
¾ ¢ È ¢ Ê
È · Ê
¾ ¢ È ¢ Ê
È · Ê
Â ¾ ¢ ¢
·
(11)
where and are the cluster and dependency F1-measures
respectively and Â is the Joint F1-measure (Â ) that we use to
measure the overall performance.
6. TECHNIQUES
The task of event modeling can be split into two parts: clustering
the stories into unique events in the topic and constructing 
dependencies among them. In the following subsections, we describe
techniques we developed for each of these sub-tasks.
6.1 Clustering
Each topic is composed of multiple events, so stories must be
clustered into events before we can model the dependencies among
them. For simplicity, all stories in the same topic are assumed to
be available at one time, rather than coming in a text stream. This
task is similar to traditional clustering but features other than word
distributions may also be critical in our application.
In many text clustering systems, the similarity between two 
stories is the inner product of their tf-idf vectors, hence we use it as
one of our features. Stories in the same event tend to follow 
temporal locality, so the time stamp of each story can be a useful feature.
Additionally, named-entities such as person and location names are
another obvious feature when forming events. Stories in the same
event tend to be related to the same person(s) and locations(s).
In this subsection, we present an agglomerative clustering 
algorithm that combines all these features. In our experiments, 
however, we study the effect of each feature on the performance 
separately using modified versions of this algorithm.
6.1.1 Agglomerative clustering with
time decay (ACDT)
We initialize our events to singleton events (clusters), i.e., each
cluster contains exactly one story. So the similarity between two
events, to start with, is exactly the similarity between the 
corresponding stories. The similarity Û×ÙÑ´×½ ×¾µ between two 
stories ×½ and ×¾ is given by the following formula:
Û×ÙÑ´×½ ×¾µ ½ Ó×´×½ ×¾µ · ¾ÄÓ ´×½ ×¾µ · ¿È Ö´×½ ×¾µ
(12)
Here ½, ¾, ¿ are the weights on different features. In this work,
we determined them empirically, but in the future, one can 
consider more sophisticated learning techniques to determine them.
Ó×´×½ ×¾µ is the cosine similarity of the term vectors. ÄÓ ´×½ ×¾µ
is 1 if there is some location that appears in both stories, otherwise
it is 0. È Ö´×½ ×¾µ is similarly defined for person name.
We use time decay when calculating similarity of story pairs,
i.e., the larger time difference between two stories, the smaller their
similarities. The time period of each topic differs a lot, from a few
days to a few months. So we normalize the time difference using
the whole duration of that topic. The time decay adjusted similarity
449
× Ñ´×½ ×¾µ is given by
× Ñ´×½ ×¾µ Û×ÙÑ´×½ ×¾µ
 « Ø½ Ø¾
Ì (13)
where Ø½ and Ø¾ are the time stamps for story 1 and 2 respectively.
T is the time difference between the earliest and the latest story in
the given topic. « is the time decay factor.
In each iteration, we find the most similar event pair and merge
them. We have three different ways to compute the similarity 
between two events Ù and Ú:
¯ Average link: In this case the similarity is the average of the
similarities of all pairs of stories between Ù and Ú as shown
below:
× Ñ´ Ù Ú µ
È×Ù¾ Ù
È×Ú¾ Ú × Ñ´×Ù ×Ú µ
Ù Ú
(14)
¯ Complete link: The similarity between two events is given
by the smallest of the pair-wise similarities.
× Ñ´ Ù Ú µ Ñ Ò
×Ù¾ Ù ×Ú¾ Ú
× Ñ´×Ù ×Ú µ (15)
¯ Single link: Here the similarity is given by the best similarity
between all pairs of stories.
× Ñ´ Ù Ú µ Ñ Ü
×Ù¾ Ù ×Ú¾ Ú
× Ñ´×Ù ×Ú µ (16)
This process continues until the maximum similarity falls below
the threshold or the number of clusters is smaller than a given 
number.
6.2 Dependency modeling
Capturing dependencies is an extremely hard problem because
it may require a ‘deeper understanding" of the events in question.
A human annotator decides on dependencies not just based on the
information in the events but also based on his/her vast repertoire
of domain-knowledge and general understanding of how things 
operate in the world. For example, in Figure 1 a human knows ‘Trial
and indictment of Osama" is influenced by ‘Evidence gathered by
CIA" because he/she understands the process of law in general.
We believe a robust model should incorporate such domain 
knowledge in capturing dependencies, but in this work, as a first step, we
will rely on surface-features such as time-ordering of news stories
and word distributions to model them. Our experiments in later 
sections demonstrate that such features are indeed useful in capturing
dependencies to a large extent.
In this subsection, we describe the models we considered for 
capturing dependencies. In the rest of the discussion in this subsection,
we assume that we are already given the mapping ¼ Ë and
we focus only on modeling the edges ¼. First we define a couple
of features that the following models will employ.
First we define a 1-1 time-ordering function Ø Ë ½ ¡ ¡ ¡ Ò
that sorts stories in ascending order by their time of publication.
Now, the event-time-ordering function Ø is defined as follows.
Ø ½ ¡ ¡ ¡ Ñ × Ø
Ù Ú ¾ Ø ´ Ùµ Ø ´ Úµ ´µ Ñ Ò
×Ù¾ Ù
Ø´×Ùµ Ñ Ò
×Ú¾ Ú
Ø´×Úµ
(17)
In other words, Ø time-orders events based on the time-ordering of
their respective first stories.
We will also use average cosine similarity between two events as
a feature and it is defined as follows.
Ú Ë Ñ´ Ù Ú µ
È×Ù¾ Ù
È×Ú¾ Ú Ó×´×Ù ×Ú µ
Ù Ú
(18)
6.2.1 Complete-Link model
In this model, we assume that there are dependencies between all
pairs of events. The direction of dependency is determined by the
time-ordering of the first stories in the respective events. Formally,
the system edges are defined as follows.
¼ ´ Ù Ú µ Ø ´ Ùµ Ø ´ Ú µ (19)
where Ø is the event-time-ordering function. In other words, the
dependency edge is directed from event Ù to event Ú , if the first
story in event Ù is earlier than the first story in event Ú . We point
out that this is not to be confused with the complete-link algorithm
in clustering. Although we use the same names, it will be clear
from the context which one we refer to.
6.2.2 Simple Thresholding
This model is an extension of the complete link model with an
additional constraint that there is a dependency between any two
events Ù and Ú only if the average cosine similarity between
event Ù and event Ú is greater than a threshold Ì. Formally,
¼ ´ Ù Úµ Ú Ë Ñ´ Ù Ú µ Ì
Ø ´ Ùµ Ø ´ Ú µ (20)
6.2.3 Nearest Parent Model
In this model, we assume that each event can have at most one
parent. We define the set of dependencies as follows.
¼ ´ Ù Úµ Ú Ë Ñ´ Ù Ú µ Ì
Ø ´ Úµ Ø ´ Ùµ · ½ (21)
Thus, for each event Ú , the nearest parent model considers only
the event preceding it as defined by Ø as a potential candidate. The
candidate is assigned as the parent only if the average similarity
exceeds a pre-defined threshold Ì.
6.2.4 Best Similarity Model
This model also assumes that each event can have at most one
parent. An event Ú is assigned a parent Ù if and only if Ù is
the most similar earlier event to Ú and the similarity exceeds a
threshold Ì. Mathematically, this can be expressed as:
¼ ´ Ù Ú µ Ú Ë Ñ´ Ù Úµ Ì
Ù Ö Ñ Ü
Û Ø ´ Ûµ Ø ´ Úµ
Ú Ë Ñ´ Û Ú µ
(22)
6.2.5 Maximum Spanning Tree model
In this model, we first build a maximum spanning tree (MST) 
using a greedy algorithm on the following fully connected weighted,
undirected graph whose vertices are the events and whose edges
are defined as follows:
´ Ù Ú µ Û´ Ù Ú µ Ú Ë Ñ´ Ù Úµ (23)
Let ÅËÌ´ µ be the set of edges in the maximum spanning tree of
¼. Now our directed dependency edges are defined as follows.
¼ ´ Ù Ú µ ´ Ù Ú µ ¾ ÅËÌ´ µ Ø ´ Ùµ Ø ´ Úµ
Ú Ë Ñ´ Ù Ú µ Ì (24)
450
Thus in this model, we assign dependencies between the most 
similar events in the topic.
7. EXPERIMENTS
Our experiments consists of three parts. First we modeled only
the event clustering part (defining the mapping function ¼) using
clustering algorithms described in section 6.1. Then we modeled
only the dependencies by providing to the system the true clusters
and running only the dependency algorithms of section 6.2. Finally,
we experimented with combinations of clustering and dependency
algorithms to produce the complete event model. This way of 
experimentation allows us to compare the performance of our 
algorithms in isolation and in association with other components. The
following subsections present the three parts of our 
experimentation.
7.1 Clustering
We have tried several variations of the Ì algorithm to study
the effects of various features on the clustering performance. All
the parameters are learned by tuning on the training set. We also
tested the algorithms on the test set with parameters fixed at their
optimal values learned from training. We used agglomerative 
clusModel best T CP CR CF P-value
cos+1-lnk 0.15 0.41 0.56 
0.43cos+all-lnk 0.00 0.40 0.62 
0.45cos+Loc+avg-lnk 0.07 0.37 0.74 
0.45cos+Per+avg-lnk 0.07 0.39 0.70 
0.46cos+TD+avg-lnk 0.04 0.45 0.70 0.53 2.9e-4*
cos+N(T)+avg-lnk - 0.41 0.62 0.48 7.5e-2
cos+N(T)+T+avg-lnk 0.03 0.42 0.62 0.49 2.4e-2*
cos+TD+N(T)+avg-lnk - 0.44 0.66 0.52 7.0e-3*
cos+TD+N(T)+T+avg-lnk 0.03 0.47 0.64 0.53 1.1e-3*
Baseline(cos+avg-lnk) 0.05 0.39 0.67 
0.46Table 2: Comparison of agglomerative clustering algorithms
(training set)
tering based on only cosine similarity as our clustering baseline.
The results on the training and test sets are in Table 2 and 3 
respectively. We use the Cluster F1-measure (CF) averaged over all topics
as our evaluation criterion.
Model CP CR CF P-value
cos+1-lnk 0.43 0.49 
0.39cos+all-lnk 0.43 0.62 
0.47cos+Loc+avg-lnk 0.37 0.73 
0.45cos+Per+avg-lnk 0.44 0.62 
0.45cos+TD+avg-lnk 0.48 0.70 0.54 0.014*
cos+N(T)+avg-lnk 0.41 0.71 0.51 0.31
cos+N(T)+T+avg-lnk 0.43 0.69* 0.52 0.14
cos+TD+N(T)+avg-lnk 0.43 0.76 0.54 0.025*
cos+TD+N(T)+T+avg-lnk 0.47 0.69 0.54 0.0095*
Baseline(cos+avg-lnk) 0.44 0.67 
0.50Table 3: Comparison of agglomerative clustering algorithms
(test set)
P-value marked with a £ means that it is a statistically significant
improvement over the baseline (95% confidence level, one tailed
T-test). The methods shown in table 2 and 3 are:
¯ Baseline: tf-idf vector weight, cosine similarity, average link
in clustering. In equation 12, ½ ½, ¾ ¿ ¼. And
« ¼ in equation 13. This F-value is the maximum obtained
by tuning the threshold.
¯ cos+1-lnk: Single link comparison (see equation 16) is used
where similarity of two clusters is the maximum of all story
pairs, other configurations are the same as the baseline run.
¯ cos+all-lnk: Complete link algorithm of equation 15 is used.
Similar to single link but it takes the minimum similarity of
all story pairs.
¯ cos+Loc+avg-lnk: Location names are used when 
calculating similarity. ¾ ¼ ¼ in equation 12. All algorithms
starting from this one use average link (equation 14), since
single link and complete link do not show any improvement
of performance.
¯ cos+Per+avg-lnk: ¿ ¼ ¼ in equation 12, i.e., we put
some weight on person names in the similarity.
¯ cos+TD+avg-lnk: Time Decay coefficient « ½ in equation
13, which means the similarity between two stories will be
decayed to ½ if they are at different ends of the topic.
¯ cos+N(T)+avg-lnk: Use the number of true events to control
the agglomerative clustering algorithm. When the number
of clusters is fewer than that of truth events, stop merging
clusters.
¯ cos+N(T)+T+avg-lnk: similar to N(T) but also stop 
agglomeration if the maximal similarity is below the threshold Ì.
¯ cos+TD:+N(T)+avg-lnk: similar to N(T) but the similarities
are decayed, « ½ in equation 13.
¯ cos+TD+N(T)+T+avg-lnk: similar to TD+N(Truth) but 
calculation halts when the maximal similarity is smaller than
the threshold Ì.
Our experiments demonstrate that single link and complete link
similarities perform worse than average link, which is reasonable
since average link is less sensitive to one or two story pairs. We
had expected locations and person names to improve the result, but
it is not the case. Analysis of topics shows that many on-topic
stories share the same locations or persons irrespective of the event
they belong to, so these features may be more useful in identifying
topics rather than events. Time decay is successful because events
are temporally localized, i.e., stories discussing the same event tend
to be adjacent to each other in terms of time. Also we noticed
that providing the number of true events improves the performance
since it guides the clustering algorithm to get correct granularity.
However, for most applications, it is not available. We used it only
as a cheat experiment for comparison with other algorithms. On
the whole, time decay proved to the most powerful feature besides
cosine similarity on both training and test sets.
7.2 Dependencies
In this subsection, our goal is to model only dependencies. We
use the true mapping function and by implication the true events
Î . We build our dependency structure ¼ using all the five 
models described in section 6.2. We first train our models on the 26
training topics. Training involves learning the best threshold Ì
for each of the models. We then test the performances of all the
trained models on the 27 test topics. We evaluate our performance
451
using the average values of Dependency Precision (DP), 
Dependency Recall (DR) and Dependency F-measure (DF). We consider
the complete-link model to be our baseline since for each event, it
trivially considers all earlier events to be parents.
Table 4 lists the results on the training set. We see that while all
the algorithms except MST outperform the baseline complete-link
algorithm , the nearest Parent algorithm is statistically significant
from the baseline in terms of its DF-value using a one-tailed paired
T-test at 95% confidence level.
Model best Ì DP DR DF P-value
Nearest Parent 0.025 0.55 0.62 0.56 0.04*
Best Similarity 0.02 0.51 0.62 0.53 0.24
MST 0.0 0.46 0.58 
0.48Simple Thresh. 0.045 0.45 0.76 0.52 0.14
Complete-link - 0.36 0.93 
0.48Table 4: Results on the training set: Best Ì is the optimal value
of the threshold Ì. * indicates the corresponding model is 
statistically significant compared to the baseline using a one-tailed,
paired T-test at 95% confidence level.
In table 5 we present the comparison of the models on the test
set. Here, we do not use any tuning but set the threshold to the
corresponding optimal values learned from the training set. The 
results throw some surprises: The nearest parent model, which was
significantly better than the baseline on training set, turns out to be
worse than the baseline on the test set. However all the other 
models are better than the baseline including the best similarity which
is statistically significant. Notice that all the models that perform
better than the baseline in terms of DF, actually sacrifice their 
recall performance compared to the baseline, but improve on their
precision substantially thereby improving their performance on the
DF-measure.
We notice that both simple-thresholding and best similarity are
better than the baseline on both training and test sets although the
improvement is not significant. On the whole, we observe that the
surface-level features we used capture the dependencies to a 
reasonable level achieving a best value of 0.72 DF on the test set.
Although there is a lot of room for improvement, we believe this is
a good first step.
Model DP DR DF P-value
Nearest Parent 0.61 0.60 
0.60Best Similarity 0.71 0.74 0.72 0.04*
MST 0.70 0.68 0.69 0.22
Simple Thresh. 0.57 0.75 0.64 0.24
Baseline (Complete-link) 0.50 0.94 
0.63Table 5: Results on the test set
7.3 Combining Clustering and Dependencies
Now that we have studied the clustering and dependency 
algorithms in isolation, we combine the best performing algorithms and
build the entire event model. Since none of the dependency 
algorithms has been shown to be consistently and significantly better
than the others, we use all of them in our experimentation. From
the clustering techniques, we choose the best performing Cos+TD.
As a baseline, we use a combination of the baselines in each 
components, i.e., cos for clustering and complete-link for dependencies.
Note that we need to retrain all the algorithms on the training
set because our objective function to optimize is now JF, the joint
F-measure. For each algorithm, we need to optimize both the 
clustering threshold and the dependency threshold. We did this 
empirically on the training set and the optimal values are listed in table
6.
The results on the training set, also presented in table 6, indicate
that cos+TD+Simple-Thresholding is significantly better than the
baseline in terms of the joint F-value JF, using a one-tailed paired 
Ttest at 95% confidence level. On the whole, we notice that while the
clustering performance is comparable to the experiments in section
7.1, the overall performance is undermined by the low dependency
performance. Unlike our experiments in section 7.2 where we had
provided the true clusters to the system, in this case, the system
has to deal with deterioration in the cluster quality. Hence the 
performance of the dependency algorithms has suffered substantially
thereby lowering the overall performance.
The results on the test set present a very similar story as shown
in table 7. We also notice a fair amount of consistency in the 
performance of the combination algorithms. cos+TD+Simple-Thresholding
outperforms the baseline significantly. The test set results also point
to the fact that the clustering component remains a bottleneck in
achieving an overall good performance.
8. DISCUSSION AND CONCLUSIONS
In this paper, we have presented a new perspective of modeling
news topics. Contrary to the TDT view of topics as flat 
collection of news stories, we view a news topic as a relational structure
of events interconnected by dependencies. In this paper, we also
proposed a few approaches for both clustering stories into events
and constructing dependencies among them. We developed a 
timedecay based clustering approach that takes advantage of 
temporallocalization of news stories on the same event and showed that it
performs significantly better than the baseline approach based on
cosine similarity. Our experiments also show that we can do fairly
well on dependencies using only surface-features such as 
cosinesimilarity and time-stamps of news stories as long as true events
are provided to the system. However, the performance deteriorates
rapidly if the system has to discover the events by itself. Despite
that discouraging result, we have shown that our combined 
algorithms perform significantly better than the baselines.
Our results indicate modeling dependencies can be a very hard
problem especially when the clustering performance is below ideal
level. Errors in clustering have a magnifying effect on errors in 
dependencies as we have seen in our experiments. Hence, we should
focus not only on improving dependencies but also on clustering at
the same time.
As part of our future work, we plan to investigate further into
the data and discover new features that influence clustering as well
as dependencies. And for modeling dependencies, a probabilistic
framework should be a better choice since there is no definite 
answer of yes/no for the causal relations among some events. We also
hope to devise an iterative algorithm which can improve clustering
and dependency performance alternately as suggested by one of
the reviewers. We also hope to expand our labeled corpus further
to include more diverse news sources and larger and more complex
event structures.
Acknowledgments
We would like to thank the three anonymous reviewers for their
valuable comments. This work was supported in part by the Center
452
Model Cluster T Dep. T CP CR CF DP DR DF JF P-value
cos+TD+Nearest-Parent 0.055 0.02 0.51 0.53 0.49 0.21 0.19 0.19 
0.27cos+TD+Best-Similarity 0.04 0.02 0.45 0.70 0.53 0.21 0.33 0.23 
0.32cos+TD+MST 0.04 0.00 0.45 0.70 0.53 0.22 0.35 0.25 
0.33cos+TD+Simple-Thresholding 0.065 0.02 0.56 0.47 0.48 0.23 0.61 0.32 0.38 0.0004*
Baseline (cos+Complete-link) 0.10 - 0.58 0.31 0.38 0.20 0.67 0.30 
0.33Table 6: Combined results on the training set
Model CP CR CF DP DR DF JF P-value
cos+TD+Nearest Parent 0.57 0.50 0.50 0.27 0.19 0.21 
0.30cos+TD+Best Similarity 0.48 0.70 0.54 0.31 0.27 0.26 
0.35cos+TD+MST 0.48 0.70 0.54 0.31 0.30 0.28 
0.37cos+TD+Simple Thresholding 0.60 0.39 0.44 0.32 0.66 0.42 0.43 0.0081*
Baseline (cos+Complete-link) 0.66 0.27 0.36 0.30 0.72 0.43 
0.39Table 7: Combined results on the test set
for Intelligent Information Retrieval and in part by 
SPAWARSYSCENSD grant number N66001-02-1-8903. Any opinions, findings and
conclusions or recommendations expressed in this material are the
authors" and do not necessarily reflect those of the sponsor.
9. REFERENCES
[1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. Topic detection and tracking pilot study: Final
report. In Proceedings of the DARPA Broadcast News
Transcription and Understanding Workshop, pages 194-218,
1998.
[2] J. Allan, A. Feng, and A. Bolivar. Flexible intrinsic
evaluation of hierarchical clustering for tdt. volume In the
Proc. of the ACM Twelfth International Conference on
Information and Knowledge Management, pages 263-270,
Nov 2003.
[3] James Allan, editor. Topic Detection and Tracking:Event
based Information Organization. Kluwer Academic
Publishers, 2000.
[4] James Allan, Rahul Gupta, and Vikas Khandelwal. Temporal
summaries of new topics. In Proceedings of the 24th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 10-18. ACM
Press, 2001.
[5] Regina Barzilay and Lillian Lee. Catching the drift:
Probabilistic content models, with applications to generation
and summarization. In Proceedings of Human Language
Technology Conference and North American Chapter of the
Association for Computational Linguistics(HLT-NAACL),
pages 113-120, 2004.
[6] D. Lawrie and W. B. Croft. Discovering and comparing topic
hierarchies. In Proceedings of RIAO 2000 Conference, pages
314-330, 1999.
[7] David D. Lewis and Kimberly A. Knowles. Threading
electronic mail: a preliminary study. Inf. Process. Manage.,
33(2):209-217, 1997.
[8] Juha Makkonen. Investigations on event evolution in tdt. In
Proceedings of HLT-NAACL 2003 Student Workshop, pages
43-48, 2004.
[9] Aixin Sun and Ee-Peng Lim. Hierarchical text classification
and evaluation. In Proceedings of the 2001 IEEE
International Conference on Data Mining, pages 521-528.
IEEE Computer Society, 2001.
[10] Yiming Yang, Jaime Carbonell, Ralf Brown, Thomas Pierce,
Brian T. Archibald, and Xin Liu. Learning approaches for
detecting and tracking news events. In IEEE Intelligent
Systems Special Issue on Applications of Intelligent
Information Retrieval, volume 14 (4), pages 32-43, 1999.
453
Relaxed Online SVMs for Spam Filtering
D. Sculley
Tufts University
Department of Computer Science
161 College Ave., Medford, MA USA
dsculleycs.tufts.edu
Gabriel M. Wachman
Tufts University
Department of Computer Science
161 College Ave., Medford, MA USA
gwachm01cs.tufts.edu
ABSTRACT
Spam is a key problem in electronic communication, 
including large-scale email systems and the growing number of
blogs. Content-based filtering is one reliable method of 
combating this threat in its various forms, but some academic
researchers and industrial practitioners disagree on how best
to filter spam. The former have advocated the use of 
Support Vector Machines (SVMs) for content-based filtering,
as this machine learning methodology gives state-of-the-art
performance for text classification. However, similar 
performance gains have yet to be demonstrated for online spam
filtering. Additionally, practitioners cite the high cost of
SVMs as reason to prefer faster (if less statistically robust)
Bayesian methods. In this paper, we offer a resolution to this
controversy. First, we show that online SVMs indeed give
state-of-the-art classification performance on online spam
filtering on large benchmark data sets. Second, we show
that nearly equivalent performance may be achieved by a
Relaxed Online SVM (ROSVM) at greatly reduced 
computational cost. Our results are experimentally verified on
email spam, blog spam, and splog detection tasks.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval - spam
General Terms
Measurement, Experimentation, Algorithms
1. INTRODUCTION
Electronic communication is increasingly plagued by 
unwanted or harmful content known as spam. The most well
known form of spam is email spam, which remains a major
problem for large email systems. Other forms of spam are
also becoming problematic, including blog spam, in which
spammers post unwanted comments in blogs [21], and splogs,
which are fake blogs constructed to enable link spam with
the hope of boosting the measured importance of a given
webpage in the eyes of automated search engines [17]. There
are a variety of methods for identifying these many forms
of spam, including compiling blacklists of known spammers,
and conducting link analysis.
The approach of content analysis has shown particular
promise and generality for combating spam. In content 
analysis, the actual message text (often including hyper-text and
meta-text, such as HTML and headers) is analyzed using
machine learning techniques for text classification to 
determine if the given content is spam. Content analysis has
been widely applied in detecting email spam [11], and has
also been used for identifying blog spam [21] and splogs [17].
In this paper, we do not explore the related problem of link
spam, which is currently best combated by link analysis [13].
1.1 An Anti-Spam Controversy
The anti-spam community has been divided on the choice
of the best machine learning method for content-based spam
detection. Academic researchers have tended to favor the
use of Support Vector Machines (SVMs), a statistically 
robust machine learning method [7] which yields 
state-of-theart performance on general text classification [14]. However,
SVMs typically require training time that is quadratic in the
number of training examples, and are impractical for 
largescale email systems. Practitioners requiring content-based
spam filtering have typically chosen to use the faster (if
less statistically robust) machine learning method of Naive
Bayes text classification [11, 12, 20]. This Bayesian method
requires only linear training time, and is easily implemented
in an online setting with incremental updates. This allows a
deployed system to easily adapt to a changing environment
over time. Other fast methods for spam filtering include
compression models [1] and logistic regression [10]. It has
not yet been empirically demonstrated that SVMs give 
improved performance over these methods in an online spam
detection setting [4].
1.2 Contributions
In this paper, we address the anti-spam controversy and
offer a potential resolution. We first demonstrate that 
online SVMs do indeed provide state-of-the-art spam detection
through empirical tests on several large benchmark data sets
of email spam. We then analyze the effect of the tradeoff
parameter in the SVM objective function, which shows that
the expensive SVM methodology may, in fact, be overkill for
spam detection. We reduce the computational cost of SVM
learning by relaxing this requirement on the maximum 
margin in online settings, and create a Relaxed Online SVM,
ROSVM, appropriate for high performance content-based
spam filtering in large-scale settings.
2. SPAM AND ONLINE SVMS
The controversy between academics and practitioners in
spam filtering centers on the use of SVMs. The former 
advocate their use, but have yet to demonstrate strong 
performance with SVMs on online spam filtering. Indeed, the
results of [4] show that, when used with default parameters,
SVMs actually perform worse than other methods. In this
section, we review the basic workings of SVMs and describe
a simple Online SVM algorithm. We then show that Online
SVMs indeed achieve state-of-the-art performance on 
filtering email spam, blog comment spam, and splogs, so long as
the tradeoff parameter C is set to a high value. However, the
cost of Online SVMs turns out to be prohibitive for 
largescale applications. These findings motivate our proposal of
Relaxed Online SVMs in the following section.
2.1 Background: SVMs
SVMs are a robust machine learning methodology which
has been shown to yield state-of-the-art performance on text
classification [14]. by finding a hyperplane that separates
two classes of data in data space while maximizing the 
margin between them.
We use the following notation to describe SVMs, which
draws from [23]. A data set X contains n labeled example
vectors {(x1, y1) . . . (xn, yn)}, where each xi is a vector 
containing features describing example i, and each yi is the class
label for that example. In spam detection, the classes spam
and ham (i.e., not spam) are assigned the numerical class
labels +1 and −1, respectively. The linear SVMs we employ
in this paper use a hypothesis vector w and bias term b to
classify a new example x, by generating a predicted class
label f(x):
f(x) = sign(< w, x > +b)
SVMs find the hypothesis w, which defines the separating
hyperplane, by minimizing the following objective function
over all n training examples:
τ(w, ξ) =
1
2
||w||2
+ C
nX
i=i
ξi
under the constraints that
∀i = {1..n} : yi(< w, xi > +b) ≥ 1 − ξi, ξi ≥ 0
In this objective function, each slack variable ξi shows the
amount of error that the classifier makes on a given example
xi. Minimizing the sum of the slack variables corresponds
to minimizing the loss function on the training data, while
minimizing the term 1
2
||w||2
corresponds to maximizing the
margin between the two classes [23]. These two optimization
goals are often in conflict; the tradeoff parameter C 
determines how much importance to give each of these tasks.
Linear SVMs exploit data sparsity to classify a new 
instance in O(s) time, where s is the number of non-zero 
features. This is the same classification time as other linear
Given: data set X = (x1, y1), . . . , (xn, yn), C, m:
Initialize w := 0, b := 0, seenData := { }
For Each xi ∈ X do:
Classify xi using f(xi) = sign(< w, xi > +b)
IF yif(xi) < 1
Find w , b using SMO on seenData,
using w, b as seed hypothesis.
Add xi to seenData
done
Figure 1: Pseudo code for Online SVM.
classifiers, and as Naive Bayesian classification. Training
SVMs, however, typically takes O(n2
) time, for n training
examples. A variant for linear SVMs was recently proposed
which trains in O(ns) time [15], but because this method
has a high constant, we do not explore it here.
2.2 Online SVMs
In many traditional machine learning applications, SVMs
are applied in batch mode. That is, an SVM is trained on
an entire set of training data, and is then tested on a 
separate set of testing data. Spam filtering is typically tested
and deployed in an online setting, which proceeds 
incrementally. Here, the learner classifies a new example, is told if
its prediction is correct, updates its hypothesis accordingly,
and then awaits a new example. Online learning allows a
deployed system to adapt itself in a changing environment.
Re-training an SVM from scratch on the entire set of 
previously seen data for each new example is cost prohibitive.
However, using an old hypothesis as the starting point for
re-training reduces this cost considerably. One method of 
incremental and decremental SVM learning was proposed in
[2]. Because we are only concerned with incremental 
learning, we apply a simpler algorithm for converting a batch
SVM learner into an online SVM (see Figure 1 for 
pseudocode), which is similar to the approach of [16].
Each time the Online SVM encounters an example that
was poorly classified, it retrains using the old hypothesis as
a starting point. Note that due to the Karush-Kuhn-Tucker
(KKT) conditions, it is not necessary to re-train on 
wellclassified examples that are outside the margins [23].
We used Platt"s SMO algorithm [22] as a core SVM solver,
because it is an iterative method that is well suited to 
converge quickly from a good initial hypothesis. Because 
previous work (and our own initial testing) indicates that binary
feature values give the best results for spam filtering [20,
9], we optimized our implementation of the Online SMO to
exploit fast inner-products with binary vectors. 1
2.3 Feature Mapping Spam Content
Extracting machine learning features from text may be
done in a variety of ways, especially when that text may
include hyper-content and meta-content such as HTML and
header information. However, previous research has shown
that simple methods from text classification, such as bag
of words vectors, and overlapping character-level n-grams,
can achieve strong results [9]. Formally, a bag of words 
vector is a vector x with a unique dimension for each possible
1
Our source code is freely available at
www.cs.tufts.edu/∼dsculley/onlineSMO.
1
0.999
0.995
0.99
0.985
0.98
0.1 1 10 100 1000
ROCArea
C
2-grams
3-grams
4-grams
words
Figure 2: Tuning the Tradeoff Parameter C. Tests
were conducted with Online SMO, using binary 
feature vectors, on the spamassassin data set of 6034
examples. Graph plots C versus Area under the
ROC curve.
word, defined as a contiguous substring of non-whitespace
characters. An n-gram vector is a vector x with a unique
dimension for each possible substring of n total characters.
Note that n-grams may include whitespace, and are 
overlapping. We use binary feature scoring, which has been shown
to be most effective for a variety of spam detection 
methods [20, 9]. We normalize the vectors with the Euclidean
norm. Furthermore, with email data, we reduce the impact
of long messages (for example, with attachments) by 
considering only the first 3,000 characters of each string. For blog
comments and splogs, we consider the whole text, 
including any meta-data such as HTML tags, as given. No other
feature selection or domain knowledge was used.
2.4 Tuning the Tradeoff Parameter, C
The SVM tradeoff parameter C must be tuned to balance
the (potentially conflicting) goals of maximizing the 
margin and minimizing the training error. Early work on SVM
based spam detection [9] showed that high values of C give
best performance with binary features. Later work has not
always followed this lead: a (low) default setting of C was
used on splog detection [17], and also on email spam [4].
Following standard machine learning practice, we tuned C
on separate tuning data not used for later testing. We used
the publicly available spamassassin email spam data set,
and created an online learning task by randomly interleaving
all 6034 labeled messages to create a single ordered set.
For tuning, we performed a coarse parameter search for C
using powers of ten from .0001 to 10000. We used the Online
SVM described above, and tested both binary bag of words
vectors and n-gram vectors with n = {2, 3, 4}. We used the
first 3000 characters of each message, which included header
information, body of the email, and possibly attachments.
Following the recommendation of [6], we use Area under
the ROC curve as our evaluation measure. The results (see
Figure 2) agree with [9]: there is a plateau of high 
performance achieved with all values of C ≥ 10, and performance
degrades sharply with C < 1. For the remainder of our 
experiments with SVMs in this paper, we set C = 100. We
will return to the observation that very high values of C do
not degrade performance as support for the intuition that
relaxed SVMs should perform well on spam.
Table 1: Results for Email Spam filtering with 
Online SVM on benchmark data sets. Score reported
is (1-ROCA)%, where 0 is optimal.
trec05p-1 trec06p
OnSVM: words 0.015 (.011-.022) 0.034 (.025-.046)
3-grams 0.011 (.009-.015) 0.025 (.017-.035)
4-grams 0.008 (.007-.011) 0.023 (.017-.032)
SpamProbe 0.059 (.049-.071) 0.092 (.078-.110)
BogoFilter 0.048 (.038-.062) 0.077 (.056-.105)
TREC Winners 0.019 (.015-.023) 0.054 (.034-.085)
53-Ensemble 0.007 (.005-.008) 0.020 (.007-.050)
Table 2: Results for Blog Comment Spam Detection
using SVMs and Leave One Out Cross Validation.
We report the same performance measures as in the
prior work for meaningful comparison.
accuracy precision recall
SVM C = 100: words 0.931 0.946 0.954
3-grams 0.951 0.963 0.965
4-grams 0.949 0.967 0.956
Prior best method 0.83 0.874 0.874
2.5 Email Spam and Online SVMs
With C tuned on a separate tuning set, we then tested the
performance of Online SVMs in spam detection. We used
two large benchmark data sets of email spam as our test
corpora. These data sets are the 2005 TREC public data set
trec05p-1 of 92,189 messages, and the 2006 TREC public
data sets, trec06p, containing 37,822 messages in English.
(We do not report our strong results on the trec06c corpus
of Chinese messages as there have been questions raised over
the validity of this test set.) We used the canonical ordering
provided with each of these data sets for fair comparison.
Results for these experiments, with bag of words vectors
and and n-gram vectors appear in Table 1. To compare our
results with previous scores on these data sets, we use the
same (1-ROCA)% measure described in [6], which is one 
minus the area under the ROC curve, expressed as a percent.
This measure shows the percent chance of error made by
a classifier asserting that one message is more likely to be
spam than another. These results show that Online SVMs
do give state of the art performance on email spam. The
only known system that out-performs the Online SVMs on
the trec05p-1 data set is a recent ensemble classifier which
combines the results of 53 unique spam filters [19]. To
our knowledge, the Online SVM has out-performed every
other single filter on these data sets, including those using
Bayesian methods [5, 3], compression models [5, 3], logistic
regression [10], and perceptron variants [3], the TREC 
competition winners [5, 3], and open source email spam filters
BogoFilter v1.1.5 and SpamProbe v1.4d.
2.6 Blog Comment Spam and SVMs
Blog comment spam is similar to email spam in many 
regards, and content-based methods have been proposed for
detecting these spam comments [21]. However, large 
benchmark data sets of labeled blog comment spam do not yet 
exist. Thus, we run experiments on the only publicly available
data set we know of, which was used in content-based blog
Table 3: Results for Splog vs. Blog Detection using
SVMs and Leave One Out Cross Validation. We
report the same evaluation measures as in the prior
work for meaningful comparison.
features precision recall F1
SVM C = 100: words 0.921 0.870 0.895
3-grams 0.904 0.866 0.885
4-grams 0.928 0.876 0.901
Prior SVM with: words 0.887 0.864 0.875
4-grams 0.867 0.844 0.855
words+urls 0.893 0.869 0.881
comment spam detection experiments by [21]. Because of
the small size of the data set, and because prior researchers
did not conduct their experiments in an on-line setting, we
test the performance of linear SVMs using leave-one-out
cross validation, with SVM-Light, a standard open-source
SVM implementation [14]. We use the parameter setting
C = 100, with the same feature space mappings as above.
We report accuracy, precision, and recall to compare these to
the results given on the same data set by [21]. These results
(see Table 2) show that SVMs give superior performance on
this data set to the prior methodology.
2.7 Splogs and SVMs
As with blog comment spam, there is not yet a large, 
publicly available benchmark corpus of labeled splog detection
test data. However, the authors of [17] kindly provided us
with the labeled data set of 1,389 blogs and splogs that they
used to test content-based splog detection using SVMs. The
only difference between our methodology and that of [17] is
that they used default parameters for C, which SVM-Light
sets to 1
avg||x||2
. (For normalized vectors, this default value
sets C = 1.) They also tested several domain-informed 
feature mappings, such as giving special features to url tags.
For our experiments, we used the same feature mappings
as above, and tested the effect of setting C = 100. As with
the methodology of [17], we performed leave one out cross
validation for apples-to-apples comparison on this data. The
results (see Table 3) show that a high value of C produces
higher performance for the same feature space mappings,
and even enables the simple 4-gram mapping to out-perform
the previous best mapping which incorporated domain 
knowledge by using words and urls.
2.8 Computational Cost
The results presented in this section demonstrate that 
linfeatures trec06p trec05p-1
words 12196s 66478s
3-grams 44605s 128924s
4-grams 87519s 242160s
corpus size 32822 92189
Table 4: Execution time for Online SVMs with email
spam detection, in CPU seconds. These times do
not include the time spent mapping strings to 
feature vectors. The number of examples in each data
set is given in the last row as corpus size.
A
B
Figure 3: Visualizing the effect of C. 
Hyperplane A maximizes the margin while accepting a
small amount of training error. This corresponds
to setting C to a low value. Hyperplane B 
accepts a smaller margin in order to reduce 
training error. This corresponds to setting C to a high
value. Content-based spam filtering appears to do
best with high values of C.
ear SVMs give state of the art performance on content-based
spam filtering. However, this performance comes at a price.
Although the blog comment spam and splog data sets are
too small for the quadratic training time of SVMs to 
appear problematic, the email data sets are large enough to
illustrate the problems of quadratic training cost.
Table 4 shows computation time versus data set size for
each of the online learning tasks (on same system). The
training cost of SVMs are prohibitive for large-scale content
based spam detection, or a large blog host. In the 
following section, we reduce this cost by relaxing the expensive
requirements of SVMs.
3. RELAXED ONLINE SVMS (ROSVM)
One of the main benefits of SVMs is that they find a 
decision hyperplane that maximizes the margin between classes
in the data space. Maximizing the margin is expensive,
typically requiring quadratic training time in the number
of training examples. However, as we saw in the previous
section, the task of content-based spam detection is best
achieved by SVMs with a high value of C. Setting C to a
high value for this domain implies that minimizing 
training loss is more important than maximizing the margin (see
Figure 3).
Thus, while SVMs do create high performance spam 
filters, applying them in practice is overkill. The full margin
maximization feature that they provide is unnecessary, and
relaxing this requirement can reduce computational cost.
We propose three ways to relax Online SVMs:
• Reduce the size of the optimization problem by only
optimizing over the last p examples.
• Reduce the number of training updates by only 
training on actual errors.
• Reduce the number of iterations in the iterative SVM
Given: dataset X = (x1, y1), . . . , (xn, yn), C, m, p:
Initialize w := 0, b := 0, seenData := { }
For Each xi ∈ X do:
Classify xi using f(xi) = sign(< w, xi > +b)
If yif(xi) < m
Find w , b with SMO on seenData,
using w, b as seed hypothesis.
set (w, b) := (w",b")
If size(seenData) > p
remove oldest example from seenData
Add xi to seenData
done
Figure 4: Pseudo-code for Relaxed Online SVM.
solver by allowing an approximate solution to the 
optimization problem.
As we describe in the remainder of this subsection, all of
these methods trade statistical robustness for reduced 
computational cost. Experimental results reported in the 
following section show that they equal or approach the 
performance of full Online SVMs on content-based spam detection.
3.1 Reducing Problem Size
In the full Online SVMs, we re-optimize over the full set
of seen data on every update, which becomes expensive as
the number of seen data points grows. We can bound this
expense by only considering the p most recent examples for
optimization (see Figure 4 for pseudo-code).
Note that this is not equivalent to training a new SVM
classifier from scratch on the p most recent examples, 
because each successive optimization problem is seeded with
the previous hypothesis w [8]. This hypothesis may contain
values for features that do not occur anywhere in the p most
recent examples, and these will not be changed. This allows
the hypothesis to remember rare (but informative) features
that were learned further than p examples in the past.
Formally, the optimization problem is now defined most
clearly in the dual form [23]. In this case, the original 
softmargin SVM is computed by maximizing at example n:
W (α) =
nX
i=1
αi −
1
2
nX
i,j=1
αiαjyiyj < xi, xj >,
subject to the previous constraints [23]:
∀i ∈ {1, . . . , n} : 0 ≤ αi ≤ C and
nX
i=1
αiyi = 0
To this, we add the additional lookback buffer constraint
∀j ∈ {1, . . . , (n − p)} : αj = cj
where cj is a constant, fixed as the last value found for αj
while j > (n − p). Thus, the margin found by an 
optimization is not guaranteed to be one that maximizes the margin
for the global data set of examples {x1, . . . , xn)}, but rather
one that satisfies a relaxed requirement that the margin be
maximized over the examples { x(n−p+1), . . . , xn}, subject
to the fixed constraints on the hyperplane that were found
in previous optimizations over examples {x1, . . . , x(n−p)}.
(For completeness, when p ≥ n, define (n − p) = 1.) This
set of constraints reduces the number of free variables in the
optimization problem, reducing computational cost.
3.2 Reducing Number of Updates
As noted before, the KKT conditions show that a well
classified example will not change the hypothesis; thus it is
not necessary to re-train when we encounter such an 
example. Under the KKT conditions, an example xi is considered
well-classified when yif(xi) > 1. If we re-train on every
example that is not well-classified, our hyperplane will be
guaranteed to be optimal at every step.
The number of re-training updates can be reduced by 
relaxing the definition of well classified. An example xi is
now considered well classified when yif(xi) > M, for some
0 ≤ M ≤ 1. Here, each update still produces an optimal 
hyperplane. The learner may encounter an example that lies
within the margins, but farther from the margins than M.
Such an example means the hypothesis is no longer globally
optimal for the data set, but it is considered good enough
for continued use without immediate retraining.
This update procedure is similar to that used by 
variants of the Perceptron algorithm [18]. In the extreme case,
we can set M = 0, which creates a mistake driven Online
SVM. In the experimental section, we show that this 
version of Online SVMs, which updates only on actual errors,
does not significantly degrade performance on content-based
spam detection, but does significantly reduce cost.
3.3 Reducing Iterations
As an iterative solver, SMO makes repeated passes over
the data set to optimize the objective function. SMO has
one main loop, which can alternate between passing over
the entire data set, or the smaller active set of current 
support vectors [22]. Successive iterations of this loop bring
the hyperplane closer to an optimal value. However, it is
possible that these iterations provide less benefit than their
expense justifies. That is, a close first approximation may
be good enough. We introduce a parameter T to control the
maximum number of iterations we allow. As we will see in
the experimental section, this parameter can be set as low
as 1 with little impact on the quality of results, providing
computational savings.
4. EXPERIMENTS
In Section 2, we argued that the strong performance on
content-based spam detection with SVMs with a high value
of C show that the maximum margin criteria is overkill, 
incurring unnecessary computational cost. In Section 3, we
proposed ROSVM to address this issue, as both of these
methods trade away guarantees on the maximum margin 
hyperplane in return for reduced computational cost. In this
section, we test these methods on the same benchmark data
sets to see if state of the art performance may be achieved by
these less costly methods. We find that ROSVM is capable
of achieving these high levels of performance with greatly
reduced cost. Our main tests on content-based spam 
detection are performed on large benchmark sets of email data.
We then apply these methods on the smaller data sets of
blog comment spam and blogs, with similar performance.
4.1 ROSVM Tests
In Section 3, we proposed three approaches for reducing
the computational cost of Online SMO: reducing the 
prob0.005
0.01
0.025
0.05
0.1
10 100 1000 10000 100000
(1-ROCA)%
Buffer Size
trec05p-1
trec06p
0
50000
100000
150000
200000
250000
10 100 1000 10000 100000
CPUSec.
Buffer Size
trec05p-1
trec06p
Figure 5: Reduced Size Tests.
lem size, reducing the number of optimization iterations,
and reducing the number of training updates. Each of these
approaches relax the maximum margin criteria on the global
set of previously seen data. Here we test the effect that each
of these methods has on both effectiveness and efficiency. In
each of these tests, we use the large benchmark email data
sets, trec05p-1 and trec06p.
4.1.1 Testing Reduced Size
For our first ROSVM test, we experiment on the effect
of reducing the size of the optimization problem by only
considering the p most recent examples, as described in the
previous section. For this test, we use the same 4-gram 
mappings as for the reference experiments in Section 2, with the
same value C = 100. We test a range of values p in a coarse
grid search. Figure 5 reports the effect of the buffer size p in
relationship to the (1-ROCA)% performance measure (top),
and the number of CPU seconds required (bottom).
The results show that values of p < 100 do result in 
degraded performance, although they evaluate very quickly.
However, p values from 500 to 10,000 perform almost as
well as the original Online SMO (represented here as p =
100, 000), at dramatically reduced computational cost.
These results are important for making state of the art
performance on large-scale content-based spam detection
practical with online SVMs. Ordinarily, the training time
would grow quadratically with the number of seen examples.
However, fixing a value of p ensures that the training time
is independent of the size of the data set. Furthermore, a
lookback buffer allows the filter to adjust to concept drift.
0.005
0.01
0.025
0.05
0.1
10521
(1-ROCA)%
Max Iters.
trec06p
trec05p-1
50000
100000
150000
200000
250000
10521
CPUSec.
Max Iters.
trec06p
trec05p-1
Figure 6: Reduced Iterations Tests.
4.1.2 Testing Reduced Iterations
In the second ROSVM test, we experiment with reducing
the number of iterations. Our initial tests showed that the
maximum number of iterations used by Online SMO was
rarely much larger than 10 on content-based spam detection;
thus we tested values of T = {1, 2, 5, ∞}. Other parameters
were identical to the original Online SVM tests.
The results on this test were surprisingly stable (see 
Figure 6). Reducing the maximum number of SMO iterations
per update had essentially no impact on classification 
performance, but did result in a moderate increase in speed. This
suggests that any additional iterations are spent attempting
to find improvements to a hyperplane that is already very
close to optimal. These results show that for content-based
spam detection, we can reduce computational cost by 
allowing only a single SMO iteration (that is, T = 1) with
effectively equivalent performance.
4.1.3 Testing Reduced Updates
For our third ROSVM experiment, we evaluate the impact
of adjusting the parameter M to reduce the total number of
updates. As noted before, when M = 1, the hyperplane is
globally optimal at every step. Reducing M allows a slightly
inconsistent hyperplane to persist until it encounters an 
example for which it is too inconsistent. We tested values of
M from 0 to 1, at increments of 0.1. (Note that we used
p = 10000 to decrease the cost of evaluating these tests.)
The results for these tests are appear in Figure 7, and
show that there is a slight degradation in performance with
reduced values of M, and that this degradation in 
performance is accompanied by an increase in efficiency. Values of
0.005
0.01
0.025
0.05
0.1
0 0.2 0.4 0.6 0.8 1
(1-ROCA)%
M
trec05p-1
trec06p
5000
10000
15000
20000
25000
30000
35000
40000
0 0.2 0.4 0.6 0.8 1
CPUSec.
M
trec05p-1
trec06p
Figure 7: Reduced Updates Tests.
M > 0.7 give effectively equivalent performance as M = 1,
and still reduce cost.
4.2 Online SVMs and ROSVM
We now compare ROSVM against Online SVMs on the
email spam, blog comment spam, and splog detection tasks.
These experiments show comparable performance on these
tasks, at radically different costs. In the previous section,
the effect of the different relaxation methods was tested
separately. Here, we tested these methods together to 
create a full implementation of ROSVM. We chose the values
p = 10000, T = 1, M = 0.8 for the email spam detection
tasks. Note that these parameter values were selected as
those allowing ROSVM to achieve comparable performance
results with Online SVMs, in order to test total difference
in computational cost. The splog and blog data sets were
much smaller, so we set p = 100 for these tasks to allow
meaningful comparisons between the reduced size and full
size optimization problems. Because these values were not
hand-tuned, both generalization performance and runtime
results are meaningful in these experiments.
4.2.1 Experimental Setup
We compared Online SVMs and ROSVM on email spam,
blog comment spam, and splog detection. For the email
spam, we used the two large benchmark corpora, trec05p-1
and trec06p, in the standard online ordering. We randomly
ordered both the blog comment spam corpus and the splog
corpus to create online learning tasks. Note that this is a
different setting than the leave-one-out cross validation task
presented on these corpora in Section 2 - the results are
not directly comparable. However, this experimental design
Table 5: Email Spam Benchmark Data. These 
results compare Online SVM and ROSVM on email
spam detection, using binary 4-gram feature space.
Score reported is (1-ROCA)%, where 0 is optimal.
trec05p-1 trec05p-1 trec06p trec06p
(1-ROC)% CPUs (1-ROC)% CPUs
OnSVM 0.0084 242,160 0.0232 87,519
ROSVM 0.0090 24,720 0.0240 18,541
Table 6: Blog Comment Spam. These results 
comparing Online SVM and ROSVM on blog comment
spam detection using binary 4-gram feature space.
Acc. Prec. Recall F1 CPUs
OnSVM 0.926 0.930 0.962 0.946 139
ROSVM 0.923 0.925 0.965 0.945 11
does allow meaningful comparison between our two online
methods on these content-based spam detection tasks.
We ran each method on each task, and report the results
in Tables 5, 6, and 7. Note that the CPU time reported for
each method was generated on the same computing system.
This time reflects only the time needed to complete online
learning on tokenized data. We do not report the time taken
to tokenize the data into binary 4-grams, as this is the same
additive constant for all methods on each task. In all cases,
ROSVM was significantly less expensive computationally.
4.3 Discussion
The comparison results shown in Tables 5, 6, and 7 are
striking in two ways. First, they show that the performance
of Online SVMs can be matched and even exceeded by 
relaxed margin methods. Second, they show a dramatic 
disparity in computational cost. ROSVM is an order of 
magnitude more efficient than the normal Online SVM, and gives
comparable results. Furthermore, the fixed lookback buffer
ensures that the cost of each update does not depend on the
size of the data set already seen, unlike Online SVMs. Note
the blog and splog data sets are relatively small, and results
on these data sets must be considered preliminary. Overall,
these results show that there is no need to pay the high cost
of SVMs to achieve this level of performance on 
contentbased detection of spam. ROSVMs offer a far cheaper 
alternative with little or no performance loss.
5. CONCLUSIONS
In the past, academic researchers and industrial 
practitioners have disagreed on the best method for online 
contentbased detection of spam on the web. We have presented one
resolution to this debate. Online SVMs do, indeed, 
proTable 7: Splog Data Set. These results compare
Online SVM and ROSVM on splog detection using
binary 4-gram feature space.
Acc. Prec. Recall F1 CPUs
OnSVM 0.880 0.910 0.842 0.874 29353
ROSVM 0.878 0.902 0.849 0.875 1251
duce state-of-the-art performance on this task with proper
adjustment of the tradeoff parameter C, but with cost that
grows quadratically with the size of the data set. The high
values of C required for best performance with SVMs show
that the margin maximization of Online SVMs is overkill for
this task. Thus, we have proposed a less expensive 
alternative, ROSVM, that relaxes this maximum margin 
requirement, and produces nearly equivalent results. These 
methods are efficient enough for large-scale filtering of 
contentbased spam in its many forms.
It is natural to ask why the task of content-based spam 
detection gets strong performance from ROSVM. After all, not
all data allows the relaxation of SVM requirements. We 
conjecture that email spam, blog comment spam, and splogs all
share the characteristic that a subset of features are 
particularly indicative of content being either spam or not spam.
These indicative features may be sparsely represented in the
data set, because of spam methods such as word obfuscation,
in which common spam words are intentionally misspelled in
an attempt to reduce the effectiveness of word-based spam
detection. Maximizing the margin may cause these sparsely
represented features to be ignored, creating an overall 
reduction in performance. It appears that spam data is highly
separable, allowing ROSVM to be successful with high 
values of C and little effort given to maximizing the margin.
Future work will determine how applicable relaxed SVMs
are to the general problem of text classification.
Finally, we note that the success of relaxed SVM methods
for content-based spam detection is a result that depends
on the nature of spam data, which is potentially subject to
change. Although it is currently true that ham and spam
are linearly separable given an appropriate feature space,
this assumption may be subject to attack. While our current
methods appear robust against primitive attacks along these
lines, such as the good word attack [24], we must explore the
feasibility of more sophisticated attacks.
6. REFERENCES
[1] A. Bratko and B. Filipic. Spam filtering using
compression models. Technical Report IJS-DP-9227,
Department of Intelligent Systems, Jozef Stefan
Institute, L jubljana, Slovenia, 2005.
[2] G. Cauwenberghs and T. Poggio. Incremental and
decremental support vector machine learning. In
NIPS, pages 409-415, 2000.
[3] G. V. Cormack. TREC 2006 spam track overview. In
To appear in: The Fifteenth Text REtrieval
Conference (TREC 2006) Proceedings, 2006.
[4] G. V. Cormack and A. Bratko. Batch and on-line
spam filter comparison. In Proceedings of the Third
Conference on Email and Anti-Spam (CEAS), 2006.
[5] G. V. Cormack and T. R. Lynam. TREC 2005 spam
track overview. In The Fourteenth Text REtrieval
Conference (TREC 2005) Proceedings, 2005.
[6] G. V. Cormack and T. R. Lynam. On-line supervised
spam filter evaluation. Technical report, David R.
Cheriton School of Computer Science, University of
Waterloo, Canada, February 2006.
[7] N. Cristianini and J. Shawe-Taylor. An introduction to
support vector machines. Cambridge University Press,
2000.
[8] D. DeCoste and K. Wagstaff. Alpha seeding for
support vector machines. In KDD "00: Proceedings of
the sixth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 345-349,
2000.
[9] H. Drucker, V. Vapnik, and D. Wu. Support vector
machines for spam categorization. IEEE Transactions
on Neural Networks, 10(5):1048-1054, 1999.
[10] J. Goodman and W. Yin. Online discriminative spam
filter training. In Proceedings of the Third Conference
on Email and Anti-Spam (CEAS), 2006.
[11] P. Graham. A plan for spam. 2002.
[12] P. Graham. Better bayesian filtering. 2003.
[13] Z. Gyongi and H. Garcia-Molina. Spam: It"s not just
for inboxes anymore. Computer, 38(10):28-34, 2005.
[14] T. Joachims. Text categorization with suport vector
machines: Learning with many relevant features. In
ECML "98: Proceedings of the 10th European
Conference on Machine Learning, pages 137-142,
1998.
[15] T. Joachims. Training linear svms in linear time. In
KDD "06: Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 217-226, 2006.
[16] J. Kivinen, A. Smola, and R. Williamson. Online
learning with kernels. In Advances in Neural
Information Processing Systems 14, pages 785-793.
MIT Press, 2002.
[17] P. Kolari, T. Finin, and A. Joshi. SVMs for the
blogosphere: Blog identification and splog detection.
AAAI Spring Symposium on Computational
Approaches to Analyzing Weblogs, 2006.
[18] W. Krauth and M. M´ezard. Learning algorithms with
optimal stability in neural networks. Journal of
Physics A, 20(11):745-752, 1987.
[19] T. Lynam, G. Cormack, and D. Cheriton. On-line
spam filter fusion. In SIGIR "06: Proceedings of the
29th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 123-130, 2006.
[20] V. Metsis, I. Androutsopoulos, and G. Paliouras.
Spam filtering with naive bayes - which naive bayes?
Third Conference on Email and Anti-Spam (CEAS),
2006.
[21] G. Mishne, D. Carmel, and R. Lempel. Blocking blog
spam with language model disagreement. Proceedings
of the 1st International Workshop on Adversarial
Information Retrieval on the Web (AIRWeb), May
2005.
[22] J. Platt. Sequenital minimal optimization: A fast
algorithm for training support vector machines. In
B. Scholkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods - Support Vector
Learning. MIT Press, 1998.
[23] B. Scholkopf and A. Smola. Learning with Kernels:
Support Vector Machines, Regularization,
Optimization, and Beyond. MIT Press, 2001.
[24] G. L. Wittel and S. F. Wu. On attacking statistical
spam filters. CEAS: First Conference on Email and
Anti-Spam, 2004.
Impedance Coupling in Content-targeted Advertising
Berthier Ribeiro-Neto
Computer Science Department
Federal University of Minas Gerais
Belo Horizonte, Brazil
berthier@dcc.ufmg.br
Marco Cristo
Computer Science Department
Federal University of Minas Gerais
Belo Horizonte, Brazil
marco@dcc.ufmg.br
Paulo B. Golgher
Akwan Information Technologies
Av. Abra˜ao Caram 430 - Pampulha
Belo Horizonte, Brazil
golgher@akwan.com.br
Edleno Silva de Moura
Computer Science Department
Federal University of Amazonas
Manaus, Brazil
edleno@dcc.ufam.edu.br
ABSTRACT
The current boom of the Web is associated with the revenues
originated from on-line advertising. While search-based 
advertising is dominant, the association of ads with a Web
page (during user navigation) is becoming increasingly 
important. In this work, we study the problem of associating
ads with a Web page, referred to as content-targeted 
advertising, from a computer science perspective. We assume that
we have access to the text of the Web page, the keywords
declared by an advertiser, and a text associated with the
advertiser"s business. Using no other information and 
operating in fully automatic fashion, we propose ten strategies
for solving the problem and evaluate their effectiveness. Our
methods indicate that a matching strategy that takes into
account the semantics of the problem (referred to as AAK
for ads and keywords) can yield gains in average precision
figures of 60% compared to a trivial vector-based strategy.
Further, a more sophisticated impedance coupling strategy,
which expands the text of the Web page to reduce 
vocabulary impedance with regard to an advertisement, can yield
extra gains in average precision of 50%. These are first 
results. They suggest that great accuracy in content-targeted
advertising can be attained with appropriate algorithms.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: 
Information Search and Retrieval; I.5.3 [Pattern Recognition]:
Applications-Text processing
General Terms
Algorithms, Experimentation
1. INTRODUCTION
The emergence of the Internet has opened up new 
marketing opportunities. In fact, a company has now the possibility
of showing its advertisements (ads) to millions of people at a
low cost. During the 90"s, many companies invested heavily
on advertising in the Internet with apparently no concerns
about their investment return [16]. This situation radically
changed in the following decade when the failure of many
Web companies led to a dropping in supply of cheap venture
capital and a considerable reduction in on-line advertising
investments [15,16].
It was clear then that more effective strategies for on-line
advertising were required. For that, it was necessary to take
into account short-term and long-term interests of the users
related to their information needs [9,14]. As a consequence,
many companies intensified the adoption of intrusive 
techniques for gathering information of users mostly without
their consent [8]. This raised privacy issues which 
stimulated the research for less invasive measures [16].
More recently, Internet information gatekeepers as, for 
example, search engines, recommender systems, and 
comparison shopping services, have employed what is called paid
placement strategies [3]. In such methods, an advertiser
company is given prominent positioning in advertisement
lists in return for a placement fee. Amongst these methods,
the most popular one is a non-intrusive technique called 
keyword targeted marketing [16]. In this technique, keywords
extracted from the user"s search query are matched against
keywords associated with ads provided by advertisers. A
ranking of the ads, which also takes into consideration the
amount that each advertiser is willing to pay, is computed.
The top ranked ads are displayed in the search result page
together with the answers for the user query.
The success of keyword targeted marketing has motivated
information gatekeepers to offer their advertisement services
in different contexts. For example, as shown in Figure 1,
relevant ads could be shown to users directly in the pages of
information portals. The motivation is to take advantage of
496
the users immediate information interests at browsing time.
The problem of matching ads to a Web page that is browsed,
which we also refer to as content-targeted advertising [1],
is different from that of keyword marketing. In this case,
instead of dealing with users" keywords, we have to use the
contents of a Web page to decide which ads to display.
Figure 1: Example of content-based advertising in
the page of a newspaper. The middle slice of the
page shows the beginning of an article about the
launch of a DVD movie. At the bottom slice, we can
see advertisements picked for this page by Google"s
content-based advertising system, AdSense.
It is important to notice that paid placement 
advertising strategies imply some risks to information gatekeepers.
For instance, there is the possibility of a negative impact
on their credibility which, at long term, can demise their
market share [3]. This makes investments in the quality of
ad recommendation systems even more important to 
minimize the possibility of exhibiting ads unrelated to the user"s
interests. By investing in their ad systems, information 
gatekeepers are investing in the maintenance of their credibility
and in the reinforcement of a positive user attitude towards
the advertisers and their ads [14]. Further, that can 
translate into higher clickthrough rates that lead to an increase in
revenues for information gatekeepers and advertisers, with
gains to all parts [3].
In this work, we focus on the problem of content-targeted
advertising. We propose new strategies for associating ads
with a Web page. Five of these strategies are referred to as
matching strategies. They are based on the idea of matching
the text of the Web page directly to the text of the ads and
its associated keywords. Five other strategies, which we here
introduce, are referred to as impedance coupling strategies.
They are based on the idea of expanding the Web page with
new terms to facilitate the task of matching ads and Web
pages. This is motivated by the observation that there is 
frequently a mismatch between the vocabulary of a Web page
and the vocabulary of an advertisement. We say that there
is a vocabulary impedance problem and that our technique
provides a positive effect of impedance coupling by reducing
the vocabulary impedance. Further, all our strategies rely
on information that is already available to information 
gatekeepers that operate keyword targeted advertising systems.
Thus, no other data from the advertiser is required.
Using a sample of a real case database with over 93,000
ads and 100 Web pages selected for testing, we evaluate our
ad recommendation strategies. First, we evaluate the five
matching strategies. They match ads to a Web page 
using a standard vector model and provide what we may call
trivial solutions. Our results indicate that a strategy that
matches the ad plus its keywords to a Web page, requiring
the keywords to appear in the Web page, provides 
improvements in average precision figures of roughly 60% relative
to a strategy that simply matches the ads to the Web page.
Such strategy, which we call AAK (for ads and keywords),
is then taken as our baseline.
Following we evaluate the five impedance coupling 
strategies. They are based on the idea of expanding the ad and
the Web page with new terms to reduce the vocabulary
impedance between their texts. Our results indicate that it
is possible to generate extra improvements in average 
precision figures of roughly 50% relative to the AAK strategy.
The paper is organized as follows. In section 2, we 
introduce five matching strategies to solve content-targeted
advertising. In section 3, we present our impedance 
coupling strategies. In section 4, we describe our experimental
methodology and datasets and discuss our results. In 
section 5 we discuss related work. In section 6 we present our
conclusions.
2. MATCHING STRATEGIES
Keyword advertising relies on matching search queries to
ads and its associated keywords. Context-based 
advertising, which we address here, relies on matching ads and its
associated keywords to the text of a Web page.
Given a certain Web page p, which we call triggering page,
our task is to select advertisements related to the contents
of p. Without loss of generality, we consider that an 
advertisement ai is composed of a title, a textual description,
and a hyperlink. To illustrate, for the first ad by Google
shown in Figure 1, the title is Star Wars Trilogy Full,
the description is Get this popular DVD free. Free w/ free
shopping. Sign up now, and the hyperlink points to the site
www.freegiftworld.com. Advertisements can be grouped
by advertisers in groups called campaigns, such that a 
campaign can have one or more advertisements.
Given our triggering page p and a set A of ads, a simple
way of ranking ai ∈ A with regard to p is by matching the
contents of p to the contents of ai. For this, we use the vector
space model [2], as discussed in the immediately following.
In the vector space model, queries and documents are 
represented as weighted vectors in an n-dimensional space. Let
wiq be the weight associated with term ti in the query q
and wij be the weight associated with term ti in the 
document dj. Then, q = (w1q, w2q, ..., wiq, ..., wnq) and dj =
(w1j, w2j, ..., wij, ..., wnj) are the weighted vectors used to
represent the query q and the document dj. These weights
can be computed using classic tf-idf schemes. In such schemes,
weights are taken as the product between factors that 
quantify the importance of a term in a document (given by the
term frequency, or tf, factor) and its rarity in the whole 
collection (given by the inverse document factor, or idf, factor),
see [2] for details. The ranking of the query q with regard
to the document dj is computed by the cosine similarity
497
formula, that is, the cosine of the angle between the two
corresponding vectors:
sim(q, dj) =
q • dj
|q| × |dj|
=
Pn
i=1 wiq · wij
qPn
i=1 w2
iq
qPn
i=1 w2
ij
(1)
By considering p as the query and ai as the document, we
can rank the ads with regard to the Web page p. This is our
first matching strategy. It is represented by the function AD
given by:
AD(p, ai) = sim(p, ai)
where AD stands for direct match of the ad, composed by
title and description and sim(p, ai) is computed according
to Eq. (1).
In our second method, we use other source of evidence
provided by the advertisers: the keywords. With each 
advertisement ai an advertiser associates a keyword ki, which
may be composed of one or more terms. We denote the
association between an advertisement ai and a keyword ki
as the pair (ai, ki) ∈ K, where K is the set of associations
made by the advertisers. In the case of keyword targeted
advertising, such keywords are used to match the ads to the
user queries. In here, we use them to match ads to the Web
page p. This provides our second method for ad matching
given by:
KW(p, ai) = sim(p, ki)
where (ai, ki) ∈ K and KW stands for match the ad 
keywords.
We notice that most of the keywords selected by 
advertisers are also present in the ads associated with those 
keywords. For instance, in our advertisement test collection,
this is true for 90% of the ads. Thus, instead of using the
keywords as matching devices, we can use them to emphasize
the main concepts in an ad, in an attempt to improve our
AD strategy. This leads to our third method of ad matching
given by:
AD KW(p, ai) = sim(p, ai ∪ ki)
where (ai, ki) ∈ K and AD KW stands for match the ad and
its keywords.
Finally, it is important to notice that the keyword ki 
associated with ai could not appear at all in the triggering page
p, even when ai is highly ranked. However, if we assume that
ki summarizes the main topic of ai according to an 
advertiser viewpoint, it can be interesting to assure its presence
in p. This reasoning suggests that requiring the occurrence
of the keyword ki in the triggering page p as a condition
to associate ai with p might lead to improved results. This
leads to two extra matching strategies as follows:
ANDKW(p, ai) =

sim(p, ai) if ki p
0 if otherwise
AD ANDKW(p, ai) = AAK(p, ai) =

sim(p, ai ∪ ki) if ki p
0 if otherwise
where (ai, ki) ∈ K, ANDKW stands for match the ad keywords
and force their appearance, and AD ANDKW (or AAK for ads
and keywords) stands for match the ad, its keywords, and
force their appearance.
As we will see in our results, the best among these simple
methods is AAK. Thus, it will be used as baseline for our
impedance coupling strategies which we now discuss.
3. IMPEDANCE COUPLING STRATEGIES
Two key issues become clear as one plays with the 
contenttargeted advertising problem. First, the triggering page 
normally belongs to a broader contextual scope than that of the
advertisements. Second, the association between a good 
advertisement and the triggering page might depend on a topic
that is not mentioned explicitly in the triggering page.
The first issue is due to the fact that Web pages can be
about any subject and that advertisements are concise in
nature. That is, ads tend to be more topic restricted than
Web pages. The second issue is related to the fact that, as
we later discuss, most advertisers place a small number of
advertisements. As a result, we have few terms describing
their interest areas. Consequently, these terms tend to be
of a more general nature. For instance, a car shop probably
would prefer to use car instead of super sport to describe
its core business topic. As a consequence, many specific
terms that appear in the triggering page find no match in
the advertisements. To make matters worst, a page might
refer to an entity or subject of the world through a label
that is distinct from the label selected by an advertiser to
refer to the same entity.
A consequence of these two issues is that vocabularies of
pages and ads have low intersection, even when an ad is
related to a page. We cite this problem from now on as
the vocabulary impedance problem. In our experiments, we
realized that this problem limits the final quality of direct
matching strategies. Therefore, we studied alternatives to
reduce the referred vocabulary impedance.
For this, we propose to expand the triggering pages with
new terms. Figure 2 illustrates our intuition. We already
know that the addition of keywords (selected by the 
advertiser) to the ads leads to improved results. We say that a
keyword reduces the vocabulary impedance by providing an
alternative matching path. Our idea is to add new terms
(words) to the Web page p to also reduce the vocabulary
impedance by providing a second alternative matching path.
We refer to our expansion technique as impedance coupling.
For this, we proceed as follows.
expansion
terms keyword
vocabulary impedance
triggering
page p ad
Figure 2: Addition of new terms to a Web page to
reduce the vocabulary impedance.
An advertiser trying to describe a certain topic in a concise
way probably will choose general terms to characterize that
topic. To facilitate the matching between this ad and our
triggering page p, we need to associate new general terms
with p. For this, we assume that Web documents similar
to the triggering page p share common topics. Therefore,
498
by inspecting the vocabulary of these similar documents we
might find good terms for better characterizing the main
topics in the page p. We now describe this idea using a
Bayesian network model [10,11,13] depicted in Figure 3.
R
D0 D1 Dj Dk
T1 T2 T3 Ti Tm
... ...
... ...
Figure 3: Bayesian network model for our 
impedance coupling technique.
In our model, which is based on the belief network in [11],
the nodes represent pieces of information in the domain.
With each node is associated a binary random variable,
which takes the value 1 to mean that the corresponding 
entity (a page or terms) is observed and, thus, relevant in our
computations. In this case, we say that the information was
observed. Node R represents the page r, a new 
representation for the triggering page p. Let N be the set of the k
most similar documents to the triggering page, including the
triggering page p itself, in a large enough Web collection C.
Root nodes D0 through Dk represent the documents in N,
that is, the triggering page D0 and its k nearest neighbors,
D1 through Dk, among all pages in C. There is an edge
from node Dj to node R if document dj is in N. Nodes
T1 through Tm represent the terms in the vocabulary of C.
There is an edge from node Dj to a node Ti if term ti occurs
in document dj. In our model, the observation of the pages
in N leads to the observation of a new representation of the
triggering page p and to a set of terms describing the main
topics associated with p and its neighbors.
Given these definitions, we can now use the network to
determine the probability that a term ti is a good term for
representing a topic of the triggering page p. In other words,
we are interested in the probability of observing the final
evidence regarding a term ti, given that the new 
representation of the page p has been observed, P(Ti = 1|R = 1).
This translates into the following equation1
:
P(Ti|R) =
1
P(R)
X
d
P(Ti|d)P(R|d)P(d) (2)
where d represents the set of states of the document nodes.
Since we are interested just in the states in which only a
single document dj is observed and P(d) can be regarded as
a constant, we can rewrite Eq. (2) as:
P(Ti|R) =
ν
P(R)
kX
j=0
P(Ti|dj)P(R|dj) (3)
where dj represents the state of the document nodes in
which only document dj is observed and ν is a constant
1
To simplify our notation we represent the probabilities
P(X = 1) as P(X) and P(X = 0) as P(X).
associated with P(dj). Eq. (3) is the general equation to
compute the probability that a term ti is related to the 
triggering page. We now define the probabilities P(Ti|dj) and
P(R|dj) as follows:
P(Ti|dj) = η wij (4)
P(R|dj) =

(1 − α) j = 0
α sim(r, dj) 1 ≤ j ≤ k
(5)
where η is a normalizing constant, wij is the weight 
associated with term ti in the document dj, and sim(p, dj) is
given by Eq. (1), i.e., is the cosine similarity between p and
dj. The weight wij is computed using a classic tf-idf scheme
and is zero if term ti does not occur in document dj. Notice
that P(Ti|dj) = 1 − P(Ti|dj) and P(R|dj) = 1 − P(R|dj).
By defining the constant α, it is possible to determine how
important should be the influence of the triggering page p
to its new representation r. By substituting Eq. (4) and
Eq. (5) into Eq. (3), we obtain:
P(Ti|R) = ρ ((1 − α) wi0 + α
kX
j=1
wij sim(r, dj)) (6)
where ρ = η ν is a normalizing constant.
We use Eq. (6) to determine the set of terms that will
compose r, as illustrated in Figure 2. Let ttop be the top
ranked term according to Eq. (6). The set r is composed
of the terms ti such that P (Ti|R)
P (Ttop|R)
≥ β, where β is a given
threshold. In our experiments, we have used β = 0.05. 
Notice that the set r might contain terms that already occur
in p. That is, while we will refer to the set r as expansion
terms, it should be clear that p ∩ r = ∅.
By using α = 0, we simply consider the terms originally
in page p. By increasing α, we relax the context of the page
p, adding terms from neighbor pages, turning page p into its
new representation r. This is important because, sometimes,
a topic apparently not important in the triggering page offers
a good opportunity for advertising. For example, consider
a triggering page that describes a congress in London about
digital photography. Although London is probably not an
important topic in this page, advertisements about hotels
in London would be appropriate. Thus, adding hotels to
page p is important. This suggests using α > 0, that is,
preserving the contents of p and using the terms in r to
expand p.
In this paper, we examine both approaches. Thus, in our
sixth method we match r, the set of new expansion terms,
directly to the ads, as follows:
AAK T(p, ai) = AAK(r, ai)
where AAK T stands for match the ad and keywords to the
set r of expansion terms.
In our seventh method, we match an expanded page p to
the ads as follows:
AAK EXP(p, ai) = AAK(p ∪ r, ai)
where AAK EXP stands for match the ad and keywords to
the expanded triggering page.
499
To improve our ad placement methods, other external
source that we can use is the content of the page h pointed to
by the advertisement"s hyperlink, that is, its landing page.
After all, this page comprises the real target of the ad and
perhaps could present a more detailed description of the
product or service being advertised. Given that the 
advertisement ai points to the landing page hi, we denote this
association as the pair (ai, hi) ∈ H, where H is the set of
associations between the ads and the pages they point to.
Our eighth method consists of matching the triggering page
p to the landing pages pointed to by the advertisements, as
follows:
H(p, ai) = sim(p, hi)
where (ai, hi) ∈ H and H stands for match the hyperlink
pointed to by the ad.
We can also combine this information with the more 
promising methods previously described, AAK and AAK EXP as 
follows. Given that (ai, hi) ∈ H and (ai, ki) ∈ K, we have our
last two methods:
AAK H(p, ai) =

sim(p, ai ∪ hi ∪ ki) if ki p
0 if otherwise
AAK EXP H(p, ai) =

sim(p ∪ r, ai ∪ hi ∪ ki) if ki (p ∪ r)
0 if otherwise
where AAK H stands for match ads and keywords also 
considering the page pointed by the ad and AAH EXP H stands
for match ads and keywords with expanded triggering page,
also considering the page pointed by the ad.
Notice that other combinations were not considered in this
study due to space restrictions. These other combinations
led to poor results in our experimentation and for this reason
were discarded.
4. EXPERIMENTS
4.1 Methodology
To evaluate our ad placement strategies, we performed
a series of experiments using a sample of a real case ad
collection with 93,972 advertisements, 1,744 advertisers, and
68,238 keywords2
. The advertisements are grouped in 2,029
campaigns with an average of 1.16 campaigns per advertiser.
For the strategies AAK T and AAK EXP, we had to 
generate a set of expansion terms. For that, we used a database
of Web pages crawled by the TodoBR search engine [12]
(http://www.todobr.com.br/). This database is composed
of 5,939,061 pages of the Brazilian Web, under the domain
.br. For the strategies H, AAK H, and AAK EXP H, we also
crawled the pages pointed to by the advertisers. No other
filtering method was applied to these pages besides the 
removal of HTML tags.
Since we are initially interested in the placement of 
advertisements in the pages of information portals, our test 
collection was composed of 100 pages extracted from a 
Brazilian newspaper. These are our triggering pages. They were
crawled in such a way that only the contents of their 
articles was preserved. As we have no preferences for particular
2
Data in portuguese provided by an on-line advertisement
company that operates in Brazil.
topics, the crawled pages cover topics as diverse as politics,
economy, sports, and culture.
For each of our 100 triggering pages, we selected the top
three ranked ads provided by each of our 10 ad placement
strategies. Thus, for each triggering page we select no more
than 30 ads. These top ads were then inserted in a pool
for that triggering page. Each pool contained an average of
15.81 advertisements. All advertisements in each pool were
submitted to a manual evaluation by a group of 15 users.
The average number of relevant advertisements per page
pool was 5.15. Notice that we adopted the same pooling
method used to evaluate the TREC Web-based collection [6].
To quantify the precision of our results, we used 11-point
average figures [2]. Since we are not able to evaluate the
entire ad collection, recall values are relative to the set of
evaluated advertisements.
4.2 Tuning Idf factors
We start by analyzing the impact of different idf factors
in our advertisement collection. Idf factors are important
because they quantify how discriminative is a term in the
collection. In our ad collection, idf factors can be computed
by taking ads, advertisers or campaigns as documents. To
exemplify, consider the computation of ad idf for a term
ti that occurs 9 times in a collection of 100 ads. Then, the
inverse document frequency of ti is given by:
idfi = log
100
9
Hence, we can compute ad, advertiser or campaign idf 
factors. As we observe in Figure 4, for the AD strategy, the best
ranking is obtained by the use of campaign idf, that is, by
calculating our idf factor so that it discriminates campaigns.
Similar results were obtained for all the other methods.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 0.2 0.4 0.6 0.8 1
precision
recall
Campaign idf
Advertiser idf
Ad idf
Figure 4: Precision-recall curves obtained for the
AD strategy using ad, advertiser, and campaign idf
factors.
This reflects the fact that terms might be better 
discriminators for a business topic than for an specific ad. This
effect can be accomplished by calculating the factor relative
to idf advertisers or campaigns instead of ads. In fact, 
campaign idf factors yielded the best results. Thus, they will be
used in all the experiments reported from now on.
500
4.3 Results
Matching Strategies
Figure 5 displays the results for the matching strategies 
presented in Section 2. As shown, directly matching the 
contents of the ad to the triggering page (AD strategy) is not so
effective. The reason is that the ad contents are very noisy.
It may contain messages that do not properly describe the
ad topics such as requisitions for user actions (e.g, visit our
site) and general sentences that could be applied to any
product or service (e.g, we delivery for the whole 
country). On the other hand, an advertiser provided keyword
summarizes well the topic of the ad. As a consequence, the
KW strategy is superior to the AD and AD KW strategies. This
situation changes when we require the keywords to appear
in the target Web page. By filtering out ads whose keywords
do not occur in the triggering page, much noise is discarded.
This makes ANDKW a better alternative than KW. Further, in
this new situation, the contents of the ad becomes useful
to rank the most relevant ads making AD ANDKW (or AAK for
ads and keywords) the best among all described methods.
For this reason, we adopt AAK as our baseline in the next set
of experiments.
0
0.1
0.2
0.3
0.4
0.5
0.6
0 0.2 0.4 0.6 0.8 1
precision
recall
AAK
ANDKW
KW
AD_KW
AD
Figure 5: Comparison among our five matching
strategies. AAK (ads and keywords) is superior.
Table 1 illustrates average precision figures for Figure 5.
We also present actual hits per advertisement slot. We call
hit an assignment of an ad (to the triggering page) that
was considered relevant by the evaluators. We notice that
our AAK strategy provides a gain in average precision of 60%
relative to the trivial AD strategy. This shows that careful
consideration of the evidence related to the problem does
pay off.
Impedance Coupling Strategies
Table 2 shows top ranked terms that occur in a page 
covering Argentinean wines produced using grapes derived from
the Bordeaux region of France. The p column includes the
top terms for this page ranked according to our tf-idf 
weighting scheme. The r column includes the top ranked 
expansion terms generated according to Eq. (6). Notice that the
expansion terms not only emphasize important terms of the
target page (by increasing their weights) such as wines and
Methods Hits 11-pt average
#1 #2 #3 total score gain(%)
AD 41 32 13 86 0.104
AD KW 51 28 17 96 0.106 +1.9
KW 46 34 28 108 0.125 +20.2
ANDKW 49 37 35 121 0.153 +47.1
AD ANDKW (AAK) 51 48 39 138 0.168 +61.5
Table 1: Average precision figures, corresponding to
Figure 5, for our five matching strategies. Columns
labelled #1, #2, and #3 indicate total of hits in
first, second, and third advertisement slots, 
respectively. The AAK strategy provides improvements of
60% relative to the AD strategy.
Rank p r
term score term score
1 argentina 0.090 wines 0.251
2 obtained* 0.047 wine* 0.140
3 class* 0.036 whites 0.091
4 whites 0.035 red* 0.057
5 french* 0.031 grape 0.051
6 origin* 0.029 bordeaux 0.045
7 france* 0.029 acideness* 0.038
8 grape 0.017 argentina 0.037
9 sweet* 0.016 aroma* 0.037
10 country* 0.013 blanc* 0.036
...
35 wines 0.010 
-...
Table 2: Top ranked terms for the triggering page
p according to our tf-idf weighting scheme and top
ranked terms for r, the expansion terms for p, 
generated according to Eq. (6). Ranking scores were
normalized in order to sum up to 1. Terms marked
with ‘*" are not shared by the sets p and r.
whites, but also reveal new terms related to the main topic
of the page such as aroma and red. Further, they avoid
some uninteresting terms such as obtained and country.
Figure 6 illustrates our results when the set r of 
expansion terms is used. They show that matching the ads to
the terms in the set r instead of to the triggering page p
(AAK T strategy) leads to a considerable improvement over
our baseline, AAK. The gain is even larger when we use the
terms in r to expand the triggering page (AAK EXP method).
This confirms our hypothesis that the triggering page could
have some interesting terms that should not be completely
discarded.
Finally, we analyze the impact on the ranking of using the
contents of pages pointed by the ads. Figure 7 displays our
results. It is clear that using only the contents of the pages
pointed by the ads (H strategy) yields very poor results.
However, combining evidence from the pages pointed by the
ads with our baseline yields improved results. Most 
important, combining our best strategy so far (AAK EXP) with
pages pointed by ads (AAK EXP H strategy) leads to superior
results. This happens because the two additional sources
of evidence, expansion terms and pages pointed by the ads,
are distinct and complementary, providing extra and 
valuable information for matching ads to a Web page.
501
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.2 0.4 0.6 0.8 1
precision
recall
AAK_EXP
AAK_T
AAK
Figure 6: Impact of using a new representation for
the triggering page, one that includes expansion
terms.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.2 0.4 0.6 0.8 1
precision
recall
AAK_EXP_H
AAK_H
AAK
H
Figure 7: Impact of using the contents of the page
pointed by the ad (the hyperlink).
Figure 8 and Table 3 summarize all results described in
this section. In Figure 8 we show precision-recall curves
and in Table 3 we show 11-point average figures. We also
present actual hits per advertisement slot and gains in 
average precision relative to our baseline, AAK. We notice that
the highest number of hits in the first slot was generated by
the method AAK EXP. However, the method with best 
overall retrieval performance was AAK EXP H, yielding a gain in
average precision figures of roughly 50% over the baseline
(AAK).
4.4 Performance Issues
In a keyword targeted advertising system, ads are assigned
at query time, thus the performance of the system is a very
important issue. In content-targeted advertising systems,
we can associate ads with a page at publishing (or 
updating) time. Also, if a new ad comes in we might consider
assigning this ad to already published pages in oﬄine mode.
That is, we might design the system such that its 
performance depends fundamentally on the rate that new pages
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.2 0.4 0.6 0.8 1
precision
recall
AAK_EXP_H
AAK_EXP
AAK_T
AAK_H
AAK
H
Figure 8: Comparison among our ad placement
strategies.
Methods Hits 11-pt average
#1 #2 #3 total score gain(%)
H 28 5 6 39 0.026 -84.3
AAK 51 48 39 138 0.168
AAK H 52 50 46 148 0.191 +13.5
AAK T 65 49 43 157 0.226 +34.6
AAK EXP 70 52 53 175 0.242 +43.8
AAK EXP H 64 61 51 176 0.253 +50.3
Table 3: Results for our impedance coupling 
strategies.
are published and the rate that ads are added or modified.
Further, the data needed by our strategies (page crawling,
page expansion, and ad link crawling) can be gathered and
processed oﬄine, not affecting the user experience. Thus,
from this point of view, the performance is not critical and
will not be addressed in this work.
5. RELATED WORK
Several works have stressed the importance of relevance
in advertising. For example, in [14] it was shown that 
advertisements that are presented to users when they are not
interested on them are viewed just as annoyance. Thus,
in order to be effective, the authors conclude that 
advertisements should be relevant to consumer concerns at the
time of exposure. The results in [9] enforce this conclusion
by pointing out that the more targeted the advertising, the
more effective it is.
Therefore it is not surprising that other works have 
addressed the relevance issue. For instance, in [8] it is proposed
a system called ADWIZ that is able to adapt online 
advertisement to a user"s short-term interests in a non-intrusive
way. Contrary to our work, ADWIZ does not directly use
the content of the page viewed by the user. It relies on search
keywords supplied by the user to search engines and on the
URL of the page requested by the user. On the other hand,
in [7] the authors presented an intrusive approach in which
an agent sits between advertisers and the user"s browser 
allowing a banner to be placed into the currently viewed page.
In spite of having the opportunity to use the page"s content,
502
the agent infers relevance based on category information and
user"s private information collected along the time.
In [5] the authors provide a comparison between the 
ranking strategies used by Google and Overture for their keyword
advertising systems. Both systems select advertisements by
matching them to the keywords provided by the user in a
search query and rank the resulting advertisement list 
according to the advertisers" willingness to pay. In 
particular, Google approach also considers the clickthrough rate
of each advertisement as an additional evidence for its 
relevance. The authors conclude that Google"s strategy is better
than that used by Overture. As mentioned before, the 
ranking problem in keyword advertising is different from that of
content-targeted advertising. Instead of dealing with 
keywords provided by users in search queries, we have to deal
with the contents of a page which can be very diffuse.
Finally, the work in [4] focuses on improving search 
engine results in a TREC collection by means of an automatic
query expansion method based on kNN [17]. Such method
resembles our expansion approach presented in section 3.
Our method is different from that presented by [4]. They
expand user queries applied to a document collection with
terms extracted from the top k documents returned as 
answer to the query in the same collection. In our case, we
use two collections: an advertisement and a Web collection.
We expand triggering pages with terms extracted from the
Web collection and then we match these expanded pages to
the ads from the advertisement collection. By doing this, we
emphasize the main topics of the triggering pages, increasing
the possibility of associating relevant ads with them.
6. CONCLUSIONS
In this work we investigated ten distinct strategies for 
associating ads with a Web page that is browsed 
(contenttargeted advertising). Five of our strategies attempt to
match the ads directly to the Web page. Because of that,
they are called matching strategies. The other five 
strategies recognize that there is a vocabulary impedance problem
among ads and Web pages and attempt to solve the problem
by expanding the Web pages and the ads with new terms.
Because of that they are called impedance coupling 
strategies.
Using a sample of a real case database with over 93 
thousand ads, we evaluated our strategies. For the five matching
strategies, our results indicated that planned consideration
of additional evidence (such as the keywords provided by the
advertisers) yielded gains in average precision figures (for
our test collection) of 60%. This was obtained by a 
strategy called AAK (for ads and keywords), which is taken as
the baseline for evaluating our more advanced impedance
coupling strategies.
For our five impedance coupling strategies, the results 
indicate that additional gains in average precision of 50% (now
relative to the AAK strategy) are possible. These were 
generated by expanding the Web page with new terms (obtained
using a sample Web collection containing over five million
pages) and the ads with the contents of the page they point
to (a hyperlink provided by the advertisers).
These are first time results that indicate that high quality
content-targeted advertising is feasible and practical.
7. ACKNOWLEDGEMENTS
This work was supported in part by the GERINDO 
project, grant MCT/CNPq/CT-INFO 552.087/02-5, by CNPq
grant 300.188/95-1 (Berthier Ribeiro-Neto), and by CNPq
grant 303.576/04-9 (Edleno Silva de Moura). Marco Cristo
is supported by Fucapi, Manaus, AM, Brazil.
8. REFERENCES
[1] The Google adwords. Google content-targeted advertising.
http://adwords.google.com/select/ct_faq.html, November
2004.
[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information
Retrieval. Addison-Wesley-Longman, 1st edition, 1999.
[3] H. K. Bhargava and J. Feng. Paid placement strategies for
internet search engines. In Proceedings of the eleventh
international conference on World Wide Web, pages 117-123.
ACM Press, 2002.
[4] E. P. Chan, S. Garcia, and S. Roukos. Trec-5 ad-hoc retrieval
using k nearest-neighbors re-scoring. In The Fifth Text
REtrieval Conference (TREC-5). National Institute of
Standards and Technology (NIST), November 1996.
[5] J. Feng, H. K. Bhargava, and D. Pennock. Comparison of
allocation rules for paid placement advertising in search
engines. In Proceedings of the 5th international conference on
Electronic commerce, pages 294-299. ACM Press, 2003.
[6] D. Hawking, N. Craswell, and P. B. Thistlewaite. Overview of
TREC-7 very large collection track. In The Seventh Text
REtrieval Conference (TREC-7), pages 91-104, Gaithersburg,
Maryland, USA, November 1998.
[7] Y. Kohda and S. Endo. Ubiquitous advertising on the www:
merging advertisement on the browser. Comput. Netw. ISDN
Syst., 28(7-11):1493-1499, 1996.
[8] M. Langheinrich, A. Nakamura, N. Abe, T. Kamba, and
Y. Koseki. Unintrusive customization techniques for web
advertising. Comput. Networks, 31(11-16):1259-1272, 1999.
[9] T. P. Novak and D. L. Hoffman. New metrics for new media:
toward the development of web measurement standards. World
Wide Web J., 2(1):213-246, 1997.
[10] J. Pearl. Probabilistic Reasoning in Intelligent Systems:
Networks of plausible inference. Morgan Kaufmann Publishers,
2nd edition, 1988.
[11] B. Ribeiro-Neto and R. Muntz. A belief network model for IR.
In Proceedings of the 19th Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, pages 253-260, Zurich, Switzerland, August 1996.
[12] A. Silva, E. Veloso, P. Golgher, B. Ribeiro-Neto, A. Laender,
and N. Ziviani. CobWeb - a crawler for the brazilian web. In
Proceedings of the String Processing and Information
Retrieval Symposium (SPIRE"99), pages 184-191, Cancun,
Mexico, September 1999.
[13] H. Turtle and W. B. Croft. Evaluation of an inference
network-based retrieval model. ACM Transactions on
Information Systems, 9(3):187-222, July 1991.
[14] C. Wang, P. Zhang, R. Choi, and M. Daeredita. Understanding
consumers attitude toward advertising. In Eighth Americas
Conference on Information Systems, pages 1143-1148, August
2002.
[15] M. Weideman. Ethical issues on content distribution to digital
consumers via paid placement as opposed to website visibility
in search engine results. In The Seventh ETHICOMP
International Conference on the Social and Ethical Impacts
of Information and Communication Technologies, pages
904-915. Troubador Publishing Ltd, April 2004.
[16] M. Weideman and T. Haig-Smith. An investigation into search
engines as a form of targeted advert delivery. In Proceedings of
the 2002 annual research conference of the South African
institute of computer scientists and information technologists
on Enablement through technology, pages 258-258. South
African Institute for Computer Scientists and Information
Technologists, 2002.
[17] Y. Yang. Expert network: Effective and efficient learning from
human decisions in text categorization and retrieval. In W. B.
Croft and e. C. J. van Rijsbergen, editors, Proceedings of the
17rd annual international ACM SIGIR conference on
Research and development in information retrieval, pages
13-22. Springer-Verlag, 1994.
503
Knowledge-intensive Conceptual Retrieval and Passage
Extraction of Biomedical Literature
Wei Zhou, Clement Yu
Department of Computer Science
University of Illinois at Chicago
wzhou8@uic.edu,
yu@cs.uic.edu
Neil Smalheiser, Vetle Torvik
Department of Psychiatry and
Psychiatric Institute (MC912)
University of Illinois at Chicago
{neils, vtorvik}@uic.edu
Jie Hong
Division of Epidemiology and
Biostatistics, School of Public health
University of Illinois at Chicago
jhong20@uic.edu
ABSTRACT
This paper presents a study of incorporating domain-specific
knowledge (i.e., information about concepts and relationships
between concepts in a certain domain) in an information retrieval
(IR) system to improve its effectiveness in retrieving biomedical
literature. The effects of different types of domain-specific
knowledge in performance contribution are examined. Based on
the TREC platform, we show that appropriate use of 
domainspecific knowledge in a proposed conceptual retrieval model
yields about 23% improvement over the best reported result in
passage retrieval in the Genomics Track of TREC 2006.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval - retrieval models, query formulation, information
filtering. H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing - thesauruses.
General Terms
Algorithms, Performance, Experimentation.
1. INTRODUCTION
Biologists search for literature on a daily basis. For most
biologists, PubMed, an online service of U.S. National Library of
Medicine (NLM), is the most commonly used tool for searching
the biomedical literature. PubMed allows for keyword search by
using Boolean operators. For example, if one desires documents on
the use of the drug propanolol in the disease hypertension, a
typical PubMed query might be propanolol AND hypertension,
which will return all the documents having the two keywords.
Keyword search in PubMed is effective if the query is well-crafted
by the users using their expertise. However, information needs of
biologists, in some cases, are expressed as complex questions
[8][9], which PubMed is not designed to handle. While NLM does
maintain an experimental tool for free-text queries [6], it is still
based on PubMed keyword search.
The Genomics track of the 2006 Text REtrieval Conference
(TREC) provides a common platform to assess the methods and
techniques proposed by various groups for biomedical information
retrieval. The queries were collected from real biologists and they
are expressed as complex questions, such as How do mutations in
the Huntingtin gene affect Huntington"s disease?. The document
collection contains 162,259 Highwire full-text documents in
HTML format. Systems from participating groups are expected to
find relevant passages within the full-text documents. A passage is
defined as any span of text that does not include the HTML
paragraph tag (i.e., <P> or </P>).
We approached the problem by utilizing domain-specific
knowledge in a conceptual retrieval model. Domain-specific
knowledge, in this paper, refers to information about concepts and
relationships between concepts in a certain domain. We assume
that appropriate use of domain-specific knowledge might improve
the effectiveness of retrieval. For example, given a query What is
the role of gene PRNP in the Mad Cow Disease?, expanding the
gene symbol PRNP with its synonyms Prp, PrPSc, and
prion protein, more relevant documents might be retrieved.
PubMed and many other biomedical systems [8][9][10][13] also
make use of domain-specific knowledge to improve retrieval
effectiveness.
Intuitively, retrieval on the level of concepts should outperform
bag-of-words approaches, since the semantic relationships
among words in a concept are utilized. In some recent studies
[13][15], positive results have been reported for this hypothesis. In
this paper, concepts are entry terms of the ontology Medical
Subject Headings (MeSH), a controlled vocabulary maintained by
NLM for indexing biomedical literature, or gene symbols in the
Entrez gene database also from NLM. A concept could be a word,
such as the gene symbol PRNP, or a phrase, such as Mad cow
diseases. In the conceptual retrieval model presented in this
paper, the similarity between a query and a document is measured
on both concept and word levels.
This paper makes two contributions:
1. We propose a conceptual approach to utilize domain-specific
knowledge in an IR system to improve its effectiveness in
retrieving biomedical literature. Based on this approach, our
system achieved significant improvement (23%) over the best
reported result in passage retrieval in the Genomics track of
TREC 2006.
2. We examine the effects of utilizing concepts and of different
types of domain-specific knowledge in performance
contribution.
This paper is organized as follows: problem statement is given in
the next section. The techniques are introduced in section 3. In
section 4, we present the experimental results. Related works are
given in section 5 and finally, we conclude the paper in section 6.
2. PROBLEM STATEMENT
We describe the queries, document collection and the system
output in this section.
The query set used in the Genomics track of TREC 2006 consists
of 28 questions collected from real biologists. As described in [8],
these questions all have the following general format:
Biological object (1..m)
Relationship
←⎯⎯⎯⎯→ Biological process (1..n) (1)
where a biological object might be a gene, protein, or gene
mutation and a biological process can be a physiological process
or disease. A question might involve multiple biological objects
(m) and multiple biological processes (n). These questions were
derived from four templates (Table 2).
Table 2 Query templates and examples in the Genomics track
of TREC 2006
Template Example
What is the role of gene in
disease?
What is the role of DRD4 in
alcoholism?
What effect does gene have
on biological process?
What effect does the insulin
receptor gene have on
tumorigenesis?
How do genes interact in
organ function?
How do HMG and HMGB1
interact in hepatitis?
How does a mutation in
gene influence biological
process?
How does a mutation in Ret
influence thyroid function?
Features of the queries: 1) They are different from the typical
Web queries and the PubMed queries, both of which usually
consist of 1 to 3 keywords; 2) They are generated from structural
templates which can be used by a system to identify the query
components, the biological object or process.
The document collection contains 162,259 Highwire full-text
documents in HTML format.
The output of the system is a list of passages ranked according to
their similarities with the query. A passage is defined as any span
of text that does not include the HTML paragraph tag (i.e., <P> or
</P>). A passage could be a part of a sentence, a sentence, a set of
consecutive sentences or a paragraph (i.e., the whole span of text
that are inside of <P> and </P> HTML tags).
This is a passage-level information retrieval problem with the
attempt to put biologists in contexts where relevant information is
provided.
3. TECHNIQUES AND METHODS
We approached the problem by first retrieving the top-k most
relevant paragraphs, then extracting passages from these
paragraphs, and finally ranking the passages. In this process, we
employed several techniques and methods, which will be
introduced in this section. First, we give two definitions:
Definition 3.1 A concept is 1) a entry term in the MeSH
ontology, or 2) a gene symbol in the Entrez gene database. This
definition of concept can be generalized to include other
biomedical dictionary terms.
Definition 3.2 A semantic type is a category defined in the
Semantic Network of the Unified Medical Language System
(UMLS) [14]. The current release of the UMLS Semantic Network
contains 135 semantic types such as Disease or Syndrome. Each
entry term in the MeSH ontology is assigned one or more semantic
types. Each gene symbol in the Entrez gene database maps to the
semantic type Gene or Genome. In addition, these semantic
types are linked by 54 relationships. For example, Antibiotic
prevents Disease or Syndrome. These relationships among
semantic types represent general biomedical knowledge. We
utilized these semantic types and their relationships to identify
related concepts.
The rest of this section is organized as follows: in section 3.1, we
explain how the concepts are identified within a query. In section
3.2, we specify five different types of domain-specific knowledge
and introduce how they are compiled. In section 3.3, we present
our conceptual IR model. Finally, our strategy for passage
extraction is described in section 3.4.
3.1 Identifying concepts within a query
A concept, defined in Definition 3.1, is a gene symbol or a MeSH
term. We make use of the query templates to identify gene
symbols. For example, the query How do HMG and HMGB1
interact in hepatitis? is derived from the template How do genes
interact in organ function?. In this case, HMG and HMGB1
will be identified as gene symbols. In cases where the query
templates are not provided, programs for recognition of gene
symbols within texts are needed.
We use the query translation functionality of PubMed to extract
MeSH terms in a query. This is done by submitting the whole
query to PubMed, which will then return a file in which the MeSH
terms in the query are labeled. In Table 3.1, three MeSH terms
within the query What is the role of gene PRNP in the Mad cow
disease? are found in the PubMed translation: "encephalopathy,
bovine spongiform" for Mad cow disease, genes for gene,
and role for role.
Table 3.1 The PubMed translation of the query "What is the
role of gene PRNP in the Mad cow disease?".
Term PubMed translation
Mad cow
disease
"bovine spongiform encephalopathy"[Text Word]
OR "encephalopathy, bovine spongiform"[MeSH
Terms] OR Mad cow disease[Text Word]
gene
("genes"[TIAB] NOT Medline[SB]) OR
"genes"[MeSH Terms] OR gene[Text Word]
role "role"[MeSH Terms] OR role[Text Word]
3.2 Compiling domain-specific knowledge
In this paper, domain-specific knowledge refers to information
about concepts and their relationships in a certain domain. We
used five types of domain-specific knowledge in the domain of
genomics:
Type 1. Synonyms (terms listed in the thesauruses that refer to
the same meaning)
Type 2. Hypernyms (more generic terms, one level only)
Type 3. Hyponyms (more specific terms, one level only)
Type 4. Lexical variants (different forms of the same concept,
such as abbreviations. They are commonly used in the
literature, but might not be listed in the thesauruses)
Type 5. Implicitly related concepts (terms that are semantically
related and also co-occur more frequently than being
independent in the biomedical texts)
Knowledge of type 1-3 is retrieved from the following two
thesauruses: 1) MeSH, a controlled vocabulary maintained by
NLM for indexing biomedical literature. The 2007 version of
MeSH contains information about 190,000 concepts. These
concepts are organized in a tree hierarchy; 2) Entrez Gene, one of
the most widely used searchable databases of genes. The current
version of Entrez Gene contains information about 1.7 million
genes. It does not have a hierarchy. Only synonyms are retrieved
from Entrez Gene. The compiling of type 4-5 knowledge is
introduced in section 3.2.1 and 3.2.2, respectively.
3.2.1 Lexical variants
Lexical variants of gene symbols
New gene symbols and their lexical variants are regularly
introduced into the biomedical literature [7]. However, many
reference databases, such as UMLS and Entrez Gene, may not be
able to keep track of all this kind of variants. For example, for the
gene symbol "NF-kappa B", at least 5 different lexical variants can
be found in the biomedical literature: "NF-kappaB", "NFkappaB",
"NFkappa B", "NF-kB", and "NFkB", three of which are not in the
current UMLS and two not in the Entrez Gene. [3][21] have shown
that expanding gene symbols with their lexical variants improved
the retrieval effectiveness of their biomedical IR systems. In our
system, we employed the following two strategies to retrieve
lexical variants of gene symbols.
Strategy I: This strategy is to automatically generate lexical
variants according to a set of manually crafted heuristics [3][21].
For example, given a gene symbol PLA2, a variant PLAII is
generated according to the heuristic that Roman numerals and
Arabic numerals are convertible when naming gene symbols.
Another variant, PLA 2, is also generated since a hyphen or a
space could be inserted at the transition between alphabetic and
numerical characters in a gene symbol.
Strategy II: This strategy is to retrieve lexical variants from an
abbreviation database. ADAM [22] is an abbreviation database
which covers frequently used abbreviations and their definitions
(or long-forms) within MEDLINE, the authoritative repository of
citations from the biomedical literature maintained by the NLM.
Given a query How does nucleoside diphosphate kinase (NM23)
contribute to tumor progression?, we first identify the
abbreviation NM23 and its long-form nucleoside diphosphate
kinase using the abbreviation identification program from [4].
Searching the long-form nucleoside diphosphate kinase in
ADAM, other abbreviations, such as NDPK or NDK, are
retrieved. These abbreviations are considered as the lexical
variants of NM23.
Lexical variants of MeSH concepts
ADAM is used to obtain the lexical variants of MeSH concepts as
well. All the abbreviations of a MeSH concept in ADAM are
considered as lexical variants to each other. In addition, those
long-forms that share the same abbreviation with the MeSH
concept and are different by an edit distance of 1 or 2 are also
considered as its lexical variants. As an example, "human
papilloma viruses" and "human papillomaviruses" have the same
abbreviation HPV in ADAM and their edit distance is 1. Thus
they are considered as lexical variants to each other. The edit
distance between two strings is measured by the minimum number
of insertions, deletions, and substitutions of a single character
required to transform one string into the other [12].
3.2.2 Implicitly related concepts
Motivation: In some cases, there are few documents in the
literature that directly answer a given query. In this situation, those
documents that implicitly answer their questions or provide
supporting information would be very helpful. For example, there
are few documents in PubMed that directly answer the query
"What is the role of the genes HNF4 and COUP-tf I in the
suppression in the function of the liver?". However, there exist
some documents about the role of "HNF4" and "COUP-tf I" in
regulating "hepatitis B virus" transcription. It is very likely that the
biologists would be interested in these documents because
"hepatitis B virus" is known as a virus that could cause serious
damage to the function of liver. In the given example, "hepatitis B
virus" is not a synonym, hypernym, hyponym, nor a lexical variant
of any of the query concepts, but it is semantically related to the
query concepts according to the UMLS Semantic Network. We
call this type of concepts implicitly related concepts of the
query. This notion is similar to the B-term used in [19] for
relating two disjoint literatures for biomedical hypothesis
generation. The difference is that we utilize the semantic
relationships among query concepts to exclusively focus on
concepts of certain semantic types.
A query q in format (1) of section 2 can be represented by
q = (A, C)
where A is the set of biological objects and C is the set of
biological processes. Those concepts that are semantically related
to both A and C according to the UMLS Semantic Network are
considered as the implicitly related concepts of the query. In the
above example, A = {HNF4, COUP-tf I}, C = {function of
liver}, and "hepatitis B virus" is one of the implicitly related
concepts.
We make use of the MEDLINE database to extract the implicitly
related concepts. The 2006 version of MEDLINE database
contains citations (i.e., abstracts, titles, and etc.) of over 15 million
biomedical articles. Each document in MEDLINE is manually
indexed by a list of MeSH terms to describe the topics covered by
that document. Implicitly related concepts are extracted and
ranked in the following steps:
Step 1. Let list_A be the set of MeSH terms that are 1) used for
indexing those MEDLINE citations having A, and 2) semantically
related to A according to the UMLS Semantic Network. Similarly,
list_C is created for C. Concepts in B = list_A ∩ list_C are
considered as implicitly related concepts of the query.
Step 2. For each concept b∈B, compute the association between
b and A using the mutual information measure [5]:
P( , )
( , ) log
P( )P( )
b A
I b A
b A
=
where P(x) = n/N, n is the number of MEDLINE citations having x
and N is the size of MEDLINE. A large value for I(b, A) means
that b and A co-occur much more often than being independent.
I(b, C) is computed similarly.
Step 3. Let r(b) = (I(b, A), I(b, C)), for b∈ B. Given b1, b2 ∈ B,
we say r(b1) ≤ r(b2) if I(b1, A) ≤ I(b2, A) and I(b1, C) ≤ I(b2, C).
Then the association between b and the query q is measured by:
{ : and ( ) ( )}
( , )
{ : and ( ) ( )}
x x B r x r b
score b q
x x B r b r x
∈ ≤
=
∈ ≤
(2)
The numerator in Formula 2 is the number of the concepts in B
that are associated with both A and C equally with or less than b.
The denominator is the number of the concepts in B that are
associated with both A and C equally with or more than b. Figure
3.2.2 shows the top 4 implicitly related concepts for the sample
query.
Figure 3.2.2 Top 4 implicitly related concepts for the query
"How do interactions between HNF4 and COUP-TF1 suppress
liver function?".
In Figure 3.2.2, the top 4 implicitly related concepts are all highly
associated with liver: Hepatocytes are liver cells;
Hepatoblastoma is a malignant liver neoplasm occurring in
young children; the vast majority of Gluconeogenesis takes
place in the liver; and Hepatitis B virus is a virus that could
cause serious damage to the function of liver.
The top-k ranked concepts in B are used for query expansion: if
I(b, A) ≥ I(b, C), then b is considered as an implicit related
concept of A. A document having b but not A will receive a partial
weight of A. The expansion is similar for C when I(b, A) < I(b, C).
3.3 Conceptual IR model
We now discuss our conceptual IR model. We first give the basic
conceptual IR model in section 3.3.1. Then we explain how the
domain-specific knowledge is incorporated in the model using
query expansion in section 3.3.2. A pseudo-feedback strategy is
introduced in section 3.3.3. In section 3.3.4, we give a strategy to
improve the ranking by avoiding incorrect match of abbreviations.
3.3.1 Basic model
Given a query q and a document d, our model measures two
similarities, concept similarity and word similarity:
( , ) ( , )( , ) ( , )
concept word
sim q d sim q d sim q d=
Concept similarity
Two vectors are derived from a query q,
1 2
1 11 12 1
2 21 22 2
( , )
( , ,..., )
( , ,..., )
m
n
q v v
v c c c
v c c c
=
=
=
where v1 is a vector of concepts describing the biological object(s)
and v2 is a vector of concepts describing the biological process(es).
Given a vector of concepts v, let s(v) be the set of concepts in v.
The weight of vi is then measured by:
( ) max{log : ( ) ( ) and 0}i i v
v
N
w v s v s v n
n
= ⊆ >
where v is a vector that contains a subset of concepts in vi and nv is
the number of documents having all the concepts in v.
The concept similarity between q and d is then computed by
2
1
( )( , ) i i
concept i
w vsim q d α
=
= ×∑
where αi is a parameter to indicate the completeness of vi that
document d has covered. αi is measured by:
and i
i
i
c
c d c v
c
c v
idf
idf
α
∈ ∈
∈
=
∑
∑
(3)
where idfc is the inverse document frequency of concept c.
An example: suppose we have a query How does Nurr-77 delete
T cells before they migrate to the spleen or lymph nodes and how
does this impact autoimmunity?. After identifying the concepts in
the query, we have:
1
2
('Nurr-77')
('T cells', 'spleen', 'autoimmunity', 'lymph nodes')
v
v
=
=
Suppose that some document frequencies of different
combinations of concepts are as follows:
25 df('Nurr-77')
0 df('T cells', 'spleen', 'autoimmunity', 'lymph nodes')
326 df('T cells', 'spleen', 'autoimmunity')
82 df('spleen', 'autoimmunity', 'lymph nodes')
147 df('T cells', 'autoimmunity', 'lymph nodes')
2332 df('T cells', 'spleen', 'lymph nodes')
The weight of vi is then computed by (note that there does not exist
a document having all the concepts in v2):
1
2
( ) log( / 25)
( ) log( /82)
w v N
w v N
=
=
.
Now suppose a document d contains concepts ‘Nurr-77", 'T cells',
'spleen', and 'lymph nodes', but not ‘autoimmunity", then the value
of parameter αi is computed as follows:
1
2
1
('T cells')+ ('spleen')+ ('lymph nodes')
('T cells')+ ('spleen')+ ('lymph nodes')+ ('autoimmunity')
idf idf idf
idf idf idf idf
α
α
=
=
Word similarity
The similarity between q and d on the word level is computed
using Okapi [17]:
10.5 ( 1)
log( )( , )
0.5word w q
N n k tf
sim q d
n K tf∈
− + +
=
+ +
∑ (4)
where N is the size of the document collection; n is the number of
documents containing w; K=k1 × ((1-b)+b × dl/avdl) and k1=1.2,
C
Function
of Liver
Implicitly related concepts (B)
Hepatocytes
Hepatoblastoma
Gluconeogenesis
Hepatitis B virus
HNF4 and
COUP-tf I
A
b=0.75 are constants. dl is the document length of d and avdl is the
average document length; tf is the term frequency of w within d.
The model
Given two documents d1 and d2, we say 1 2( , ) ( , )sim q d sim q d> or
d1 will be ranked higher than d2, with respect to the same query q,
if either
1) 1 2( , ) ( , )
concept concept
sim q d sim q d> or
2) 1 2 1 2and( , ) ( , ) ( , ) ( , )
concept concept word word
sim q d sim q d sim q d sim q d= >
This conceptual IR model emphasizes the similarity on the concept
level. A similar model but applied to non-biomedical domain has
been given in [15].
3.3.2 Incorporating domain-specific knowledge
Given a concept c, a vector u is derived by incorporating its
domain-specific knowledge:
1 2 3( , , , )u c u u u=
where u1 is a vector of its synonyms, hyponyms, and lexical
variants; u2 is a vector of its hypernyms; and u3 is a vector of its
implicitly related concepts. An occurrence of any term in u1 will
be counted as an occurrence of c. idfc in Formula 3 is updated as:
1,
logc
c u
N
D
idf =
1,c uD is the set of documents having c or any term in 1u . The
weight that a document d receives from u is given by:
max{ : and }tw t u t d∈ ∈
where wt = β .cidf× The weighting factor β is an empirical tuning
parameter determined as:
1. β = 1 if t is the original concept, its synonym, its hyponym, or
its lexical variant;
2. β = 0.95 if t is a hypernym;
3. β = 0.90× (k-i+1)/k if t is an implicitly related concept. k is
the number of selected top ranked implicitly related concepts
(see section 3.2.2); i is the position of t in the ranking of
implicitly related concepts.
3.3.3 Pseudo-feedback
Pseudo-feedback is a technique commonly used to improve
retrieval performance by adding new terms into the original query.
We used a modified pseudo-feedback strategy described in [2].
Step 1. Let C be the set of concepts in the top 15 ranked
documents. For each concept c in C, compute the similarity
between c and the query q, the computation of sim(q,c) can be
found in [2].
Step 2. The top-k ranked concepts by sim(q,c) are selected.
Step 3. Associate each selected concept c' with the concept cq in
q that 1) has the same semantic type as c', and 2) is most related to
c' among all the concepts in q. The association between c' and cq
is computed by:
P( ', )
( ', ) log
P( ')P( )
q
q
q
c c
I c c
c c
=
where P(x) = n/N, n is the number of documents having x and N is
the size of the document collection. A document having c' but not
cq receives a weight given by: (0.5× (k-i+1)/k) ,qcidf× where i is the
position of c' in the ranking of step 2.
3.3.4 Avoid incorrect match of abbreviations
Some gene symbols are very short and thus ambiguous. For
example, the gene symbol APC could be the abbreviation for
many non-gene long-forms, such as air pollution control,
aerobic plate count, or argon plasma coagulation. This step is
to avoid incorrect match of abbreviations in the top ranked
documents.
Given an abbreviation X with the long-form L in the query, we
scan the top-k ranked (k=1000) documents and when a document
is found with X, we compare L with all the long-forms of X in that
document. If none of these long-forms is equal or close to L (i.e.,
the edit distance between L and the long-form of X in that
document is 1 or 2), then the concept similarity of X is subtracted.
3.4 Passage extraction
The goal of passage extraction is to highlight the most relevant
fragments of text in paragraphs. A passage is defined as any span
of text that does not include the HTML paragraph tag (i.e., <P> or
</P>). A passage could be a part of a sentence, a sentence, a set of
consecutive sentences or a paragraph (i.e., the whole span of text
that are inside of <P> and </P> HTML tags). It is also possible to
have more than one relevant passage in a single paragraph. Our
strategy for passage extraction assumes that the optimal passage(s)
in a paragraph should have all the query concepts that the whole
paragraph has. Also they should have higher density of query
concepts than other fragments of text in the paragraph.
Suppose we have a query q and a paragraph p represented by a
sequence of sentences 1 2... .np s s s= Let C be the set of concepts in
q that occur in p and S = Φ.
Step 1. For each sequence of consecutive sentences 1... ,i i js s s+ 1 ≤
i ≤ j ≤ n, let S = S 1{ ... }i i js s s+∪ if 1...i i js s s+ satisfies that:
1) Every query concept in C occurs in 1...i i js s s+ and
2) There does not exist k, such that i < k < j and every query
concept in C occurs in 1...i i ks s s+ or 1 2... .k k js s s+ +
Condition 1 requires 1...i i js s s+ having all the query concepts in p
and condition 2 requires 1...i i js s s+ be the minimal.
Step 2. Let 1min{ 1: ... }i i jL j i s s s S+= − + ∈ . For every
1...i i js s s+ in S, let 1{ ... }i i jS S s s s+= − if (j - i + 1) > L. This step is to
remove those sequences of sentences in S that have lower density
of query concepts.
Step 3. For every two sequences of consecutive
sentences 1 1 1 2 2 21 1... , and ...i i j i i js s s S s s s S+ +∈ ∈ , if
1 2 1 2
2 1
, and
1
i i j j
i j
≤ ≤
≤ +
(5)
then do
Repeat this step until for every two sequences of consecutive
sentences in S, condition (5) does not apply. This step is to merge
those sequences of sentences in S that are adjacent or overlapped.
Finally the remaining sequences of sentences in S are returned as
the optimal passages in the paragraph p with respect to the query.
1 1 2
1 1
2 2 2
1
1 1
1
{ ... }
{ ... }
{ ... }
i i j
i i j
i i j
S S s s s
S S s s s
S S s s s
+
+
+
= ∪
= −
= −
4. EXPERIMENTAL RESULTS
The evaluation of our techniques and the experimental results are
given in this section. We first describe the datasets and evaluation
metrics used in our experiments and then present the results.
4.1 Data sets and evaluation metrics
Our experiments were performed on the platform of the Genomics
track of TREC 2006. The document collection contains 162,259
full-text documents from 49 Highwire biomedical journals. The set
of queries consists of 28 queries collected from real biologists.
The performance is measured on three different levels (passage,
aspect, and document) to provide better insight on how the
question is answered from different perspectives. Passage MAP:
As described in [8], this is a character-based precision calculated
as follows: At each relevant retrieved passage, precision will be
computed as the fraction of characters overlapping with the gold
standard passages divided by the total number of characters
included in all nominated passages from this system for the topic
up until that point. Similar to regular MAP, relevant passages that
were not retrieved will be added into the calculation as well, with
precision set to 0 for relevant passages not retrieved. Then the
mean of these average precisions over all topics will be calculated
to compute the mean average passage precision. Aspect MAP: A
question could be addressed from different aspects. For example,
the question what is the role of gene PRNP in the Mad cow
disease? could be answered from aspects like Diagnosis,
Neurologic manifestations, or Prions/Genetics. This measure
indicates how comprehensive the question is answered. Document
MAP: This is the standard IR measure. The precision is measured
at every point where a relevant document is obtained and then
averaged over all relevant documents to obtain the average
precision for a given query. For a set of queries, the mean of the
average precision for all queries is the MAP of that IR system.
The output of the system is a list of passages ranked according to
their similarities with the query. The performances on the three
levels are then calculated based on the ranking of the passages.
4.2 Results
The Wilcoxon signed-rank test was employed to determine the
statistical significance of the results. In the tables of the following
sections, statistically significant improvements (at the 5% level)
are marked with an asterisk.
4.2.1 Conceptual IR model vs. term-based model
The initial baseline was established using word similarity only
computed by the Okapi (Formula 4). Another run based on our
basic conceptual IR model was performed without using query
expansion, pseudo-feedback, or abbreviation correction. The
experimental result is shown in Table 4.2.1. Our basic conceptual
IR model significantly outperforms the Okapi on all three levels,
which suggests that, although it requires additional efforts to
identify concepts, retrieval on the concept level can achieve
substantial improvements over purely term-based retrieval model.
4.2.2 Contribution of different types of knowledge
A series of experiments were performed to examine how each type
of domain-specific knowledge contributes to the retrieval
performance. A new baseline was established using the basic
conceptual IR model without incorporating any type of 
domainspecific knowledge. Then five runs were conducted by adding
each individual type of domain-specific knowledge. We also
conducted a run by adding all types of domain-specific knowledge.
Results of these experiments are shown in Table 4.2.2.
We found that any available type of domain-specific
knowledge improved the performance in passage retrieval. The
biggest improvement comes from the lexical variants, which is
consistent with the result reported in [3]. This result also indicates
that biologists are likely to use different variants of the same
concept according to their own writing preferences and these
variants might not be collected in the existing biomedical
thesauruses. It also suggests that the biomedical IR systems can
benefit from the domain-specific knowledge extracted from the
literature by text mining systems.
Synonyms provided the second biggest improvement.
Hypernyms, hyponyms, and implicitly related concepts provided
similar degrees of improvement. The overall performance is an
accumulative result of adding different types of domain-specific
knowledge and it is better than any individual addition. It is clearly
shown that the performance is significantly improved (107% on
passage level, 63.1% on aspect level, and 49.6% on document
level) when the domain-specific knowledge is appropriately
incorporated. Although it is not explicitly shown in Table 4.2.3,
different types of domain-specific knowledge affect different
subsets of queries. More specifically, each of these types (with the
exception of the lexical variants which affects a large number of
queries) affects only a few queries. But for those affected queries,
their improvement is significant. As a consequence, the
accumulative improvement is very significant.
4.2.3 Pseudo-feedback and abbreviation correction
Using the Baseline+All in Table 4.2.2 as a new baseline, the
contribution of abbreviation correction and pseudo-feedback is
given in Table 4.2.3. There is little improvement by avoiding
incorrect matching of abbreviations. The pseudo-feedback
contributed about 4.6% improvement in passage retrieval.
4.2.4 Performance compared with best-reported results
We compared our result with the results reported in the Genomics
track of TREC 2006 [8] on the conditions that 1) systems are
automatic systems and 2) passages are extracted from paragraphs.
The performance of our system relative to the best reported results
is shown in Table 4.2.4 (in TREC 2006, some systems returned the
whole paragraphs as passages. As a consequence, excellent
retrieval results were obtained on document and aspect levels at
the expense of performance on the passage level. We do not
include the results of such systems here).
Table 4.2.4 Performance compared with best-reported results.
Passage MAP Aspect MAP Document MAP
Best reported results 0.1486 0.3492 0.5320
Our results 0.1823 0.3811 0.5391
Improvement 22.68% 9.14% 1.33%
The best reported results in the first row of Table 4.2.4 on three
levels (passage, aspect, and document) are from different systems.
Our result is from a single run on passage retrieval in which it is
better than the best reported result by 22.68% in passage retrieval
and at the same time, 9.14% better in aspect retrieval, and 1.33%
better in document retrieval (Since the average precision of each
individual query was not reported, we can not apply the Wilcoxon
signed-rank test to calculate the significance of difference between
our performance and the best reported result.).
Table 4.2.1 Basic conceptual IR model vs. term-based model
Run Passage Aspect Document
MAP Imprvd qs # (%) MAP Imprvd qs # (%) MAP Imprvd qs # (%)
Okapi 0.064 N/A 0.175 N/A 0.285 N/A
Basic conceptual IR model 0.084* (+31.3%) 17 (65.4%) 0.233* (+33.1%) 12 (46.2%) 0.359* (+26.0%) 15 (57.7%)
Table 4.2.2 Contribution of different types of domain-specific knowledge
Run Passage Aspect Document
MAP Imprvd qs # (%) MAP Imprvd qs # (%) MAP Imprvd qs # (%)
Baseline
= Basic conceptual IR model
0.084 N/A 0.233 N/A 0.359 N/A
Baseline+Synonyms 0.105 (+25%) 11 (42.3%) 0.246 (+5.6%) 9 (34.6%) 0.420 (+17%) 13 (50%)
Baseline+Hypernyms 0.088 (+4.8%) 11 (42.3%) 0.225 (-3.4%) 9 (34.6%) 0.390 (+8.6%) 16 (61.5%)
Baseline+Hyponyms 0.087 (+3.6%) 10 (38.5%) 0.217 (-6.9%) 7 (26.9%) 0.389 (+8.4%) 10 (38.5%)
Baseline+Variants 0.150* (+78.6%) 16 (61.5%) 0.348* (+49.4%) 13 (50%) 0.495* (+37.9%) 10 (38.5%)
Baseline+Related 0.086 (+2.4%) 9 (34.6%) 0.220 (-5.6%) 9 (34.6%) 0.387 (+7.8%) 13 (50%)
Baseline+All 0.174* (107%) 25 (96.2%) 0.380* (+63.1%) 19 (73.1%) 0.537* (+49.6%) 14 (53.8%)
Table 4.2.3 Contribution of abbreviation correction and pseudo-feedback
Run Passage Aspect Document
MAP Imprvd qs # (%) MAP Imprvd qs # (%) MAP Imprvd qs # (%)
Baseline+All 0.174 N/A 0.380 N/A 0.537 N/A
Baseline+All+Abbr 0.175 (+0.6%) 5 (19.2%) 0.375 (-1.3%) 4 (15.4%) 0.535 (-0.4%) 4 (15.4%)
Baseline+All+Abbr+PF 0.182 (+4.6%) 10 (38.5%) 0.381 (+0.3%) 6 (23.1%) 0.539 (+0.4%) 9 (34.6%)
A separate experiment has been done using a second testbed, the
ad-hoc Task of TREC Genomics 2005, to evaluate our
knowledge-intensive conceptual IR model for document retrieval
of biomedical literature. The overall performance in terms of MAP
is 35.50%, which is about 22.92% above the best reported result
[9]. Notice that the performance was only measured on the
document level for the ad-hoc Task of TREC Genomics 2005.
5. RELATED WORKS
Many studies used manually-crafted thesauruses or knowledge
databases created by text mining systems to improve retrieval
effectiveness based on either word-statistical retrieval systems or
conceptual retrieval systems.
[11][1] assessed query expansion using the UMLS
Metathesaurus. Based on a word-statistical retrieval system, [11]
used definitions and different types of thesaurus relationships for
query expansion and a deteriorated performance was reported. [1]
expanded queries with phrases and UMLS concepts determined by
the MetaMap, a program which maps biomedical text to UMLS
concepts, and no significant improvement was shown. We used
MeSH, Entrez gene, and other non-thesaurus knowledge resources
such as an abbreviation database for query expansion. A critical
difference between our work and those in [11][1] is that our
retrieval model is based on concepts, not on individual words.
The Genomics track in TREC provides a common platform to
evaluate methods and techniques proposed by various groups for
biomedical information retrieval. As summarized in [8][9][10],
many groups utilized domain-specific knowledge to improve
retrieval effectiveness. Among these groups, [3] assessed both
thesaurus-based knowledge, such as gene information, and non
thesaurus-based knowledge, such as lexical variants of gene
symbols, for query expansion. They have shown that query
expansion with acronyms and lexical variants of gene symbols
produced the biggest improvement, whereas, the query expansion
with gene information from gene databases deteriorated the
performance. [21] used a similar approach for generating lexical
variants of gene symbols and reported significant improvements.
Our system utilized more types of domain-specific knowledge,
including hyponyms, hypernyms and implicitly related concepts.
In addition, under the conceptual retrieval framework, we
examined more comprehensively the effects of different types of
domain-specific knowledge in performance contribution.
[20][15] utilized WordNet, a database of English words and
their lexical relationships developed by Princeton University, for
query expansion in the non-biomedical domain. In their studies,
queries were expanded using the lexical semantic relations such as
synonyms, hypernyms, or hyponyms. Little benefit has been
shown in [20]. This has been due to ambiguity of the query terms
which have different meanings in different contexts. When these
synonyms having multiple meanings are added to the query,
substantial irrelevant documents are retrieved. In the biomedical
domain, this kind of ambiguity of query terms is relatively less
frequent, because, although the abbreviations are highly
ambiguous, general biomedical concepts usually have only one
meaning in the thesaurus, such as UMLS, whereas a term in
WordNet usually have multiple meanings (represented as synsets
in WordNet). Besides, we have implemented a post-ranking step
to reduce the number of incorrect matches of abbreviations, which
will hopefully decrease the negative impact caused by the
abbreviation ambiguity. Besides, we have implemented a 
postranking step to reduce the number of incorrect matches of
abbreviations, which will hopefully decrease the negative impact
caused by the abbreviation ambiguity. The retrieval model in [15]
emphasized the similarity between a query and a document on the
phrase level assuming that phrases are more important than
individual words when retrieving documents. Although the
assumption is similar, our conceptual model is based on the
biomedical concepts, not phrases.
[13] presented a good study of the role of knowledge in the
document retrieval of clinical medicine. They have shown that
appropriate use of semantic knowledge in a conceptual retrieval
framework can yield substantial improvements. Although the
retrieval model is similar, we made a study in the domain of
genomics, in which the problem structure and task knowledge is
not as well-defined as in the domain of clinical medicine [18].
Also, our similarity function is very different from that in [13].
In summary, our approach differs from previous works in four
important ways: First, we present a case study of conceptual
retrieval in the domain of genomics, where many knowledge
resources can be used to improve the performance of biomedical
IR systems. Second, we have studied more types of 
domainspecific knowledge than previous researchers and carried out more
comprehensive experiments to look into the effects of different
types of domain-specific knowledge in performance contribution.
Third, although some of the techniques seem similar to previously
published ones, they are actually quite different in details. For
example, in our pseudo-feedback process, we require that the unit
of feedback is a concept and the concept has to be of the same
semantic type as a query concept. This is to ensure that our
conceptual model of retrieval can be applied. As another example,
the way in which implicitly related concepts are extracted in this
paper is significantly different from that given in [19]. Finally, our
conceptual IR model is actually based on complex concepts
because some biomedical meanings, such as biological processes,
are represented by multiple simple concepts.
6. CONCLUSION
This paper proposed a conceptual approach to utilize 
domainspecific knowledge in an IR system to improve its effectiveness in
retrieving biomedical literature. We specified five different types
of domain-specific knowledge (i.e., synonyms, hyponyms,
hypernyms, lexical variants, and implicitly related concepts) and
examined their effects in performance contribution. We also
evaluated other two techniques, pseudo-feedback and abbreviation
correction. Experimental results have shown that appropriate use
of domain-specific knowledge in a conceptual IR model yields
significant improvements (23%) in passage retrieval over the best
known results. In our future work, we will explore the use of other
existing knowledge resources, such as UMLS and the Wikipedia,
and evaluate techniques such as disambiguation of gene symbols
for improving retrieval effectiveness. The application of our
conceptual IR model in other domains such as clinical medicine
will be investigated.
7. ACKNOWLEDGMENTS
insightful discussion.
8. REFERENCES
[1] Aronson A.R., Rindflesch T.C. Query expansion using the
UMLS Metathesaurus. Proc AMIA Annu Fall Symp. 1997.
485-9.
[2] Baeza-Yates R., Ribeiro-Neto B. Modern Information
Retrieval. Addison-Wesley, 1999, 129-131.
[3] Buttcher S., Clarke C.L.A., Cormack G.V. Domain-specific
synonym expansion and validation for biomedical
information retrieval (MultiText experiments for TREC
2004). TREC"04.
[4] Chang J.T., Schutze H., Altman R.B. Creating an online
dictionary of abbreviations from MEDLINE. Journal of the
American Medical Informatics Association. 2002 9(6).
[5] Church K.W., Hanks P. Word association norms, mutual
information and lexicography. Computational Linguistics.
1990;16:22, C29.
[6] Fontelo P., Liu F., Ackerman M. askMEDLINE: a free-text,
natural language query tool for MEDLINE/PubMed. BMC
Med Inform Decis Mak. 2005 Mar 10;5(1):5.
[7] Fukuda K., Tamura A., Tsunoda T., Takagi T. Toward
information extraction: identifying protein names from
biological papers. Pac Symp Biocomput. 1998;:707-18.
[8] Hersh W.R., and etc. TREC 2006 Genomics Track Overview.
TREC"06.
[9] Hersh W.R., and etc. TREC 2005 Genomics Track Overview.
In TREC"05.
[10] Hersh W.R., and etc. TREC 2004 Genomics Track Overview.
In TREC"04.
[11] Hersh W.R., Price S., Donohoe L. Assessing thesaurus-based
query expansion using the UMLS Metathesaurus. Proc AMIA
Symp. 344-8. 2000.
[12] Levenshtein, V. Binary codes capable of correcting deletions,
insertions, and reversals. Soviet Physics - Doklady 10, 10
(1996), 707-710.
[13] Lin J., Demner-Fushman D. The Role of Knowledge in
Conceptual Retrieval: A Study in the Domain of Clinical
Medicine. SIGIR"06. 99-06.
[14] Lindberg D., Humphreys B., and McCray A. The Unified
Medical Language System. Methods of Information in
Medicine. 32(4):281-291, 1993.
[15] Liu S., Liu F., Yu C., and Meng W.Y. An Effective
Approach to Document Retrieval via Utilizing WordNet and
Recognizing Phrases. SIGIR"04. 266-272
[16] Proux D., Rechenmann F., Julliard L., Pillet V.V., Jacq B.
Detecting Gene Symbols and Names in Biological Texts: A
First Step toward Pertinent Information Extraction. Genome
Inform Ser Workshop Genome Inform. 1998;9:72-80.
[17] Robertson S.E., Walker S. Okapi/Keenbow at TREC-8. NIST
Special Publication 500-246: TREC 8.
[18] Sackett D.L., and etc. Evidence-Based Medicine: How to
Practice and Teach EBM. Churchill Livingstone. Second
edition, 2000.
[19] Swanson,D.R., Smalheiser,N.R. An interactive system for
finding complemen-tary literatures: a stimulus to scientific
discovery. Artificial Intelligence, 1997; 91,183-203.
[20] Voorhees E. Query expansion using lexical-semantic
relations. SIGIR 1994. 61-9
[21] Zhong M., Huang X.J. Concept-based biomedical text
retrieval. SIGIR"06. 723-4
[22] Zhou W., Torvik V.I., Smalheiser N.R. ADAM: Another
Database of Abbreviations in MEDLINE. Bioinformatics.
2006; 22(22): 2813-2818.
Cross-Lingual Query Suggestion Using Query Logs
of Different Languages
Wei Gao1*
, Cheng Niu2
, Jian-Yun Nie3
, Ming Zhou2
, Jian Hu2
, Kam-Fai Wong1
,
Hsiao-Wuen Hon2
1
The Chinese University of Hong Kong, Hong Kong, China
{wgao, kfwong}@se.cuhk.edu.hk
2
Microsoft Research Asia, Beijing, China
{chengniu, mingzhou, jianh, hon}@microsoft.com
3
Université de Montréal, Montréal, QC, Canada
nie@iro.umontreal.ca
ABSTRACT
Query suggestion aims to suggest relevant queries for a given
query, which help users better specify their information needs.
Previously, the suggested terms are mostly in the same language
of the input query. In this paper, we extend it to cross-lingual
query suggestion (CLQS): for a query in one language, we suggest
similar or relevant queries in other languages. This is very
important to scenarios of cross-language information retrieval
(CLIR) and cross-lingual keyword bidding for search engine
advertisement. Instead of relying on existing query translation
technologies for CLQS, we present an effective means to map the
input query of one language to queries of the other language in the
query log. Important monolingual and cross-lingual information
such as word translation relations and word co-occurrence
statistics, etc. are used to estimate the cross-lingual query
similarity with a discriminative model. Benchmarks show that the
resulting CLQS system significantly outperforms a baseline
system based on dictionary-based query translation. Besides, the
resulting CLQS is tested with French to English CLIR tasks on
TREC collections. The results demonstrate higher effectiveness
than the traditional query translation methods.
Categories and Subject Descriptors
H.3.3 [Information storage and retrieval]: Information Search
and Retrieval - Query formulation
General Terms
Algorithms, Performance, Experimentation, Theory.
1. INTRODUCTION
Query suggestion is a functionality to help users of a search
engine to better specify their information need, by narrowing
down or expanding the scope of the search with synonymous
queries and relevant queries, or by suggesting related queries that
have been frequently used by other users. Search engines, such as
Google, Yahoo!, MSN, Ask Jeeves, all have implemented query
suggestion functionality as a valuable addition to their core search
method. In addition, the same technology has been leveraged to
recommend bidding terms to online advertiser in the 
pay-forperformance search market [12].
Query suggestion is closely related to query expansion which
extends the original query with new search terms to narrow the
scope of the search. But different from query expansion, query
suggestion aims to suggest full queries that have been formulated
by users so that the query integrity and coherence are preserved in
the suggested queries.
Typical methods for query suggestion exploit query logs and
document collections, by assuming that in the same period of
time, many users share the same or similar interests, which can be
expressed in different manners [12, 14, 26]. By suggesting the
related and frequently used formulations, it is hoped that the new
query can cover more relevant documents. However, all of the
existing studies dealt with monolingual query suggestion and to
our knowledge, there is no published study on cross-lingual query
suggestion (CLQS). CLQS aims to suggest related queries but in a
different language. It has wide applications on World Wide Web:
for cross-language search or for suggesting relevant bidding terms
in a different language. 1
CLQS can be approached as a query translation problem, i.e., to
suggest the queries that are translations of the original query.
Dictionaries, large size of parallel corpora and existing
commercial machine translation systems can be used for
translation. However, these kinds of approaches usually rely on
static knowledge and data. It cannot effectively reflect the quickly
shifting interests of Web users. Moreover, there are some
problems with translated queries in target language. For instance,
the translated terms can be reasonable translations, but they are
not popularly used in the target language. For example, the French
query aliment biologique is translated into biologic food by
Google translation tool2
, yet the correct formulation nowadays
should be organic food. Therefore, there exist many mismatch
cases between the translated terms and the really used terms in
target language. This mismatch makes the suggested terms in the
target language ineffective.
A natural thinking of solving this mismatch is to map the
queries in the source language and the queries in the target
language, by using the query log of a search engine. We exploit
the fact that the users of search engines in the same period of time
have similar interests, and they submit queries on similar topics in
different languages. As a result, a query written in a source
language likely has an equivalent in a query log in the target
language. In particular, if the user intends to perform CLIR, then
original query is even more likely to have its correspondent
included in the target language query log. Therefore, if a
candidate for CLQS appears often in the query log, then it is more
likely the appropriate one to be suggested.
In this paper, we propose a method of calculating the similarity
between source language query and the target language query by
exploiting, in addition to the translation information, a wide
spectrum of bilingual and monolingual information, such as term
co-occurrences, query logs with click-through data, etc. A
discriminative model is used to learn the cross-lingual query
similarity based on a set of manually translated queries. The
model is trained by optimizing the cross-lingual similarity to best
fit the monolingual similarity between one query and the other
query"s translation. Besides being benchmarked as an independent
module, the resulting CLQS system is tested as a new means of
query translation in CLIR task on TREC collections. The results
show that this new translation method is more effective than the
traditional query translation method.
The remainder of this paper is organized as follows: Section 2
introduces the related work; Section 3 describes in detail the
discriminative model for estimating cross-lingual query similarity;
Section 4 presents a new CLIR approach using cross-lingual query
suggestion as a bridge across language boundaries. Section 5
discusses the experiments and benchmarks; finally, the paper is
concluded in Section 6.
2. RELATED WORK
Most approaches to CLIR perform a query translation followed by
a monolingual IR. Typically, queries are translated either using a
bilingual dictionary [22], a machine translation software [9] or a
parallel corpus [20].
Despite the various types of resources used, out-of-vocabulary
(OOV) words and translation disambiguation are the two major
bottlenecks for CLIR [20]. In [7, 27], OOV term translations are
mined from the Web using a search engine. In [17], bilingual
knowledge is acquired based on anchor text analysis. In addition,
word co-occurrence statistics in the target language has been
leveraged for translation disambiguation [3, 10, 11, 19].
2
http://www.google.com/language_tools
Nevertheless, it is arguable that accurate query translation may
not be necessary for CLIR. Indeed, in many cases, it is helpful to
introduce words even if they are not direct translations of any
query word, but are closely related to the meaning of the query.
This observation has led to the development of cross-lingual query
expansion (CLQE) techniques [2, 16, 18]. [2] reports the
enhancement on CLIR by post-translation expansion. [16]
develops a cross-lingual relevancy model by leveraging the 
crosslingual co-occurrence statistics in parallel texts. [18] makes
performance comparison on multiple CLQE techniques, including
pre-translation expansion and post-translation expansion.
However, there is lack of a unified framework to combine the
wide spectrum of resources and recent advances of mining
techniques for CLQE.
CLQS is different from CLQE in that it aims to suggest full
queries that have been formulated by users in another language.
As CLQS exploits up-to-date query logs, it is expected that for
most user queries, we can find common formulations on these
topics in the query log in the target language. Therefore, CLQS
also plays a role of adapting the original query formulation to the
common formulations of similar topics in the target language.
Query logs have been successfully used for monolingual IR [8,
12, 15, 26], especially in monolingual query suggestions [12] and
relating the semantically relevant terms for query expansion [8,
15]. In [1], the target language query log has been exploited to
help query translation in CLIR.
3. ESTIMATING CROSS-LINGUAL
QUERY SIMILARITY
A search engine has a query log containing user queries in
different languages within a certain period of time. In addition to
query terms, click-through information is also recorded. Therefore,
we know which documents have been selected by users for each
query. Given a query in the source language, our CLQS task is to
determine one or several similar queries in the target language
from the query log.
The key problem with cross-lingual query suggestion is how to
learn a similarity measure between two queries in different
languages. Although various statistical similarity measures have
been studied for monolingual terms [8, 26], most of them are
based on term co-occurrence statistics, and can hardly be applied
directly in cross-lingual settings.
In order to define a similarity measure across languages, one
has to use at least one translation tool or resource. So the measure
is based on both translation relation and monolingual similarity. In
this paper, as our purpose is to provide up-to-date query similarity
measure, it may not be sufficient to use only a static translation
resource. Therefore, we also integrate a method to mine possible
translations on the Web. This method is particularly useful for
dealing with OOV terms.
Given a set of resources of different natures, the next question
is how to integrate them in a principled manner. In this paper, we
propose a discriminative model to learn the appropriate similarity
measure. The principle is as follows: we assume that we have a
reasonable monolingual query similarity measure. For any training
query example for which a translation exists, its similarity
measure (with any other query) is transposed to its translation.
Therefore, we have the desired cross-language similarity value for
this example. Then we use a discriminative model to learn the
cross-language similarity function which fits the best these
examples.
In the following sections, let us first describe the detail of the
discriminative model for cross-lingual query similarity estimation.
Then we introduce all the features (monolingual and cross-lingual
information) that we will use in the discriminative model.
3.1 Discriminative Model for Estimating
Cross-Lingual Query Similarity
In this section, we propose a discriminative model to learn 
crosslingual query similarities in a principled manner. The principle is
as follows: for a reasonable monolingual query similarity between
two queries, a cross-lingual correspondent can be deduced
between one query and another query"s translation. In other
words, for a pair of queries in different languages, their 
crosslingual similarity should fit the monolingual similarity between
one query and the other query"s translation. For example, the
similarity between French query pages jaunes (i.e., yellow
page in English) and English query telephone directory should
be equal to the monolingual similarity between the translation of
the French query yellow page and telephone directory. There
are many ways to obtain a monolingual similarity measure
between terms, e.g., term co-occurrence based mutual information
and 2
χ . Any of them can be used as the target for the cross-lingual
similarity function to fit. In this way, cross-lingual query
similarity estimation is formulated as a regression task as follows:
Given a source language query fq , a target language query eq ,
and a monolingual query similarity MLsim , the corresponding
cross-lingual query similarity CLsim is defined as follows:
),(),( eqMLefCL qTsimqqsim f
= (1)
where fqT is the translation of fq in the target language.
Based on Equation (1), it would be relatively easy to create a
training corpus. All it requires is a list of query translations. Then
an existing monolingual query suggestion system can be used to
automatically produce similar query to each translation, and create
the training corpus for cross-lingual similarity estimation. Another
advantage is that it is fairly easy to make use of arbitrary
information sources within a discriminative modeling framework
to achieve optimal performance.
In this paper, support vector machine (SVM) regression
algorithm [25] is used to learn the cross-lingual term similarity
function. Given a vector of feature functions f between fq and
eq , ),( efCL ttsim is represented as an inner product between a
weight vector and the feature vector in a kernel space as follows:
)),((),( efefCL ttfwttsim φ•= (2)
where φ is the mapping from the input feature space onto the
kernel space, and wis the weight vector in the kernel space which
will be learned by the SVM regression training. Once the weight
vector is learned, the Equation (2) can be used to estimate the
similarity between queries of different languages.
We want to point out that instead of regression, one can
definitely simplify the task as a binary or ordinal classification, in
which case CLQS can be categorized according to discontinuous
class labels, e.g., relevant and irrelevant, or a series of levels of
relevancies, e.g., strongly relevant, weakly relevant, and
irrelevant. In either case, one can resort to discriminative
classification approaches, such as an SVM or maximum entropy
model, in a straightforward way. However, the regression
formalism enables us to fully rank the suggested queries based on
the similarity score given by Equation (1).
The Equations (1) and (2) construct a regression model for
cross-lingual query similarity estimation. In the following
sections, the monolingual query similarity measure (see Section
3.2) and the feature functions used for SVM regression (see
Section 3.3) will be presented.
3.2 Monolingual Query Similarity Measure
Based on Click-through Information
Any monolingual term similarity measure can be used as the
regression target. In this paper, we select the monolingual query
similarity measure presented in [26] which reports good
performance by using search users" click-through information in
query logs. The reason to choose this monolingual similarity is
that it is defined in a similar context as ours − according to a user
log that reflects users" intention and behavior. Therefore, we can
expect that the cross-language term similarity learned from it can
also reflect users" intention and expectation.
Following [26], our monolingual query similarity is defined by
combining both query content-based similarity and click-through
commonality in the query log.
First the content similarity between two queries p and q is
defined as follows:
))(),((
),(
),(
qknpknMax
qpKN
qpsimilarity content = (3)
where )( xkn is the number of keywords in a query x, ),( qpKN is
the number of common keywords in the two queries.
Secondly, the click-through based similarity is defined as
follows,
))(),((
),(
),(
qrdprdMax
qpRD
qpsimilarity throughclick =−
(4)
where )(xrd is the number of clicked URLs for a query x, and
),( qpRD is the number of common URLs clicked for two queries.
Finally, the similarity between two queries is a linear
combination of the content-based and click-through-based
similarities, and is presented as follows:
),(*
),(*),(
qpsimilarity
qpsimilarityqpsimilarity
throughclick
content
−
+=
β
α (5)
where α and β are the relative importance of the two similarity
measures. In this paper, we set ,4.0=α and 6.0=β following the
practice in [26]. Queries with similarity measure higher than a
threshold with another query will be regarded as relevant
monolingual query suggestions (MLQS) for the latter. In this
paper, the threshold is set as 0.9 empirically.
3.3 Features Used for Learning Cross-Lingual
Query Similarity Measure
This section presents the extraction of candidate relevant queries
from the log with the assistance of various monolingual and
bilingual resources. Meanwhile, feature functions over source
query and the cross-lingual relevant candidates are defined. Some
of the resources being used here, such as bilingual lexicon and
parallel corpora, were for query translation in previous work. But
note that we employ them here as an assistant means for finding
relevant candidates in the log rather than for acquiring accurate
translations.
3.3.1 Bilingual Dictionary
In this subsection, a built-in-house bilingual dictionary containing
120,000 unique entries is used to retrieve candidate queries. Since
multiple translations may be associated with each source word,
co-occurrence based translation disambiguation is performed [3,
10]. The process is presented as follows:
Given an input query }{ ,2,1 fnfff wwwq K= in the source
language, for each query term fiw , a set of unique translations are
provided by the bilingual dictionary D : },,{)( ,2,1 imiifi tttwD K= .
Then the cohesion between the translations of two query terms is
measured using mutual information which is computed as follows:
)()(
),(
log),()( ,
klij
klij
klijklij
tPtP
ttP
ttPttMI = (6)
where .
)(
)(,
),(
),(
N
tC
tP
N
ttC
ttP
klij
klij ==
Here ),( yxC is the number of queries in the log containing both
x and y , )(xC is the number of queries containing term x , and
N is the total number of queries in the log.
Based on the term-term cohesion defined in Equation (6), all the
possible query translations are ranked using the summation of the
term-term cohesion ∑≠
=
kiki
klijqdict ttMITS f
,,
),()( . The set of
top-4 query translations is denoted as )( fqTS . For each possible
query translation )( fqTST∈ , we retrieve all the queries containing
the same keywords as T from the target language log. The
retrieved queries are candidate target queries, and are assigned
)(TSdict
as the value of the feature Dictionary-based Translation
Score.
3.3.2 Parallel Corpora
Parallel corpora are precious resources for bilingual knowledge
acquisition. Different from the bilingual dictionary, the bilingual
knowledge learned from parallel corpora assigns probability for
each translation candidate which is useful in acquiring dominant
query translations.
In this paper, the Europarl corpus (a set of parallel French and
English texts from the proceedings of the European Parliament) is
used. The corpus is first sentence aligned. Then word alignments
are derived by training an IBM translation model 1 [4] using
GIZA++ [21]. The learned bilingual knowledge is used to extract
candidate queries from the query log. The process is presented as
follows:
Given a pair of queries, fq in the source language and eq in the
target language, the Bi-Directional Translation Score is defined as
follows:
)|()|(),( 111 feIBMefIBMefIBM qqpqqpqqS = (7)
where )|(1 xypIBM
is the word sequence translation probability
given by IBM model 1 which has the following form:
∏∑= =+
=
||
1
||
0
||1 )|(
)1|(|
1
)|(
y
j
x
i
ijyIBM xyp
x
xyp (8)
where )|( ij xyp is the word to word translation probability
derived from the word-aligned corpora.
The reason to use bidirectional translation probability is to deal
with the fact that common words can be considered as possible
translations of many words. By using bidirectional translation, we
test whether the translation words can be translated back to the
source words. This is helpful to focus on the translation
probability onto the most specific translation candidates.
Now, given an input query fq , the top 10 queries }{ eq with the
highest bidirectional translation scores with fq are retrieved from
the query log, and ),(1 efIBM qqS in Equation (7) is assigned as the
value for the feature Bi-Directional Translation Score.
3.3.3 Online Mining for Related Queries
OOV word translation is a major knowledge bottleneck for query
translation and CLIR. To overcome this knowledge bottleneck,
web mining has been exploited in [7, 27] to acquire 
EnglishChinese term translations based on the observation that Chinese
terms may co-occur with their English translations in the same
web page. In this section, this web mining approach is adapted to
acquire not only translations but semantically related queries in
the target language.
It is assumed that if a query in the target language co-occurs
with the source query in many web pages, they are probably
semantically related. Therefore, a simple method is to send the
source query to a search engine (Google in our case) for Web
pages in the target language in order to find related queries in the
target language. For instance, by sending a French query pages
jaunes to search for English pages, the English snippets
containing the key words yellow pages or telephone directory
will be returned. However, this simple approach may induce
significant amount of noise due to the non-relevant returns from
the search engine. In order to improve the relevancy of the
bilingual snippets, we extend the simple approach by the
following query modification: the original query is used to search
with the dictionary-based query keyword translations, which are
unified by the ∧ (and) ∨ (OR) operators into a single Boolean
query. For example, for a given query abcq = where the set of
translation entries in the dictionary of for a is },,{ 321 aaa , b is
},{ 21 bb and c is }{ 1c , we issue 121321 )()( cbbaaaq ∧∨∧∨∨∧ as
one web query.
From the returned top 700 snippets, the most frequent 10 target
queries are identified, and are associated with the feature
Frequency in the Snippets.
Furthermore, we use Co-Occurrence Double-Check (CODC)
Measure to weight the association between the source and target
queries. CODC Measure is proposed in [6] as an association
measure based on snippet analysis, named Web Search with
Double Checking (WSDC) model. In WSDC model, two objects a
and b are considered to have an association if b can be found by
using a as query (forward process), and a can be found by using b
as query (backward process) by web search. The forward process
counts the frequency of b in the top N snippets of query a, denoted
as )@( abfreq . Similarly, the backward process count the
frequency of a in the top N snippets of query b, denoted
as )@( bafreq . Then the CODC association score is defined as
follows:
⎪
⎩
⎪
⎨
⎧ =×
=
⎥
⎥
⎦
⎤
⎢
⎢
⎣
⎡
×
otherwise,
0)@()@(if,0
),(
)(
)@(
)(
)@(
log
α
e
ef
f
fe
qfreq
qqfreq
qfreq
qqfreq
effe
efCODC
e
qqfreqqqfreq
qqS (9)
CODC measures the association of two terms in the range
between 0 and 1, where under the two extreme cases, eq and fq
are of no association when 0)@( =fe qqfreq
or 0)@( =ef qqfreq , and are of the strongest association when
)()@( ffe qfreqqqfreq = and )()@( eef qfreqqqfreq = . In
our experiment, α is set at 0.15 following the practice in [6].
Any query eq mined from the Web will be associated with a
feature CODC Measure with ),( efCODC qqS as its value.
3.3.4 Monolingual Query Suggestion
For all the candidate queries 0Q being retrieved using dictionary
(see Section 3.3.1), parallel data (see Section 3.3.2) and web
mining (see Section 3.3.3), monolingual query suggestion system
(described in Section 3.1) is called to produce more related
queries in the target language. For each target query eq , its
monolingual source query )( eML qSQ is defined as the query in
0Q with the highest monolingual similarity with eq , i.e.,
),(maxarg)( 0 eeMLQqeML qqsimqSQ e
′= ∈′
(10)
Then the monolingual similarity between eq and )( eML qSQ is
used as the value of the eq "s Monolingual Query Suggestion
Feature. For any target query 0Qq∈ , its Monolingual Query
Suggestion Feature is set as 1.
For any query 0Qqe ∉ , its values of Dictionary-based
Translation Score, Bi-Directional Translation Score, Frequency
in the Snippet, and CODC Measure are set to be equal to the
feature values of )( eML qSQ .
3.4 Estimating Cross-lingual Query Similarity
In summary, four categories of features are used to learn the 
crosslingual query similarity. SVM regression algorithm [25] is used to
learn the weights in Equation (2). In this paper, LibSVM toolkit
[5] is used for the regression training.
In the prediction stage, the candidate queries will be ranked
using the cross-lingual query similarity score computed in terms
of )),((),( efefCL ttfwttsim φ•= , and the queries with
similarity score lower than a threshold will be regarded as 
nonrelevant. The threshold is learned using a development data set by
fitting MLQS"s output.
4. CLIR BASED ON CROSS-LINGUAL
QUERY SUGGESTION
In Section 3, we presented a discriminative model for cross lingual
query suggestion. However, objectively benchmarking a query
suggestion system is not a trivial task. In this paper, we propose to
use CLQS as an alternative to query translation, and test its
effectiveness in CLIR tasks. The resulting good performance of
CLIR corresponds to the high quality of the suggested queries.
Given a source query fq , a set of relevant queries }{ eq in the
target language are recommended using the cross-lingual query
suggestion system. Then a monolingual IR system based on the
BM25 model [23] is called using each }{ eqq∈ as queries to
retrieve documents. Then the retrieved documents are re-ranked
based on the sum of the BM25 scores associated with each
monolingual retrieval.
5. PERFORMACNCE EVALUATION
In this section, we will benchmark the cross-lingual query
suggestion system, comparing its performance with monolingual
query suggestion, studying the contribution of various information
sources, and testing its effectiveness when being used in CLIR
tasks.
5.1 Data Resources
In our experiments, French and English are selected as the source
and target language respectively. Such selection is due to the fact
that large scale query logs are readily available for these two
languages. A one-month English query log (containing 7 million
unique English queries with occurrence frequency more than 5) of
MSN search engine is used as the target language log. And a
monolingual query suggestion system is built based on it. In
addition, 5,000 French queries are selected randomly from a
French query log (containing around 3 million queries), and are
manually translated into English by professional French-English
translators. Among the 5,000 French queries, 4,171 queries have
their translations in the English query log, and are used for CLQS
training and testing. Furthermore, among the 4,171 French
queries, 70% are used for cross-lingual query similarity training,
10% are used as the development data to determine the relevancy
threshold, and 20% are used for testing. To retrieve the 
crosslingual related queries, a built-in-house French-English bilingual
lexicon (containing 120,000 unique entries) and the Europarl
corpus are used.
Besides benchmarking CLQS as an independent system, the
CLQS is also tested as a query translation system for CLIR
tasks. Based on the observation that the CLIR performance
heavily relies on the quality of the suggested queries, this
benchmark measures the quality of CLQS in terms of its
effectiveness in helping CLIR. To perform such benchmark, we
use the documents of TREC6 CLIR data (AP88-90 newswire,
750MB) with officially provided 25 short French-English queries
pairs (CL1-CL25). The selection of this data set is due to the fact
that the average length of the queries are 3.3 words long, which
matches the web query logs we use to train CLQS.
5.2 Performance of Cross-lingual Query
Suggestion
Mean-square-error (MSE) is used to measure the regression error
and it is defined as follows:
( )2
),(),(
1
∑ −=
i
eiqMLeifiCL qTsimqqsim
l
MSE fi
where l is the total number of cross-lingual query pairs in the
testing data.
As described in Section 3.4, a relevancy threshold is learned
using the development data, and only CLQS with similarity value
above the threshold is regarded as truly relevant to the input
query. In this way, CLQS can also be benchmarked as a
classification task using precision (P) and recall (R) which are
defined as follows:
CLQS
MLQSCLQS
P
S
SS I
= ,
MLQS
MLQSCLQS
R
S
SS I
=
where CLQSS is the set of relevant queries suggested by CLQS,
MLQSS is the set of relevant queries suggested by MLQS (see
Section 3.2).
The benchmarking results with various feature configurations
are shown in Table 1.
Regression Classification
Features
MSE P R
DD 0.274 0.723 0.098
DD+PC 0.224 0.713 0.125
DD+PC+
Web
0.115 0.808 0.192
DD+PC+
Web+ML
QS
0.174 0.796 0.421
Table 1. CLQS performance with different feature settings
(DD: dictionary only; DD+PC: dictionary and parallel corpora;
DD+PC+Web: dictionary, parallel corpora, and web mining;
DD+PC+Web+MLQS: dictionary, parallel corpora, web mining
and monolingual query suggestion)
Table 1 reports the performance comparison with various
feature settings. The baseline system (DD) uses a conventional
query translation approach, i.e., a bilingual dictionary with 
cooccurrence-based translation disambiguation. The baseline system
only covers less than 10% of the suggestions made by MLQS.
Using additional features obviously enables CLQS to generate
more relevant queries. The most significant improvement on recall
is achieved by exploiting MLQS. The final CLQS system is able
to generate 42% of the queries suggested by MLQS. Among all
the feature combinations, there is no significant change in
precision. This indicates that our methods can improve the recall
by effectively leveraging various information sources without
losing the accuracy of the suggestions.
Besides benchmarking CLQS by comparing its output with
MLQS output, 200 French queries are randomly selected from the
French query log. These queries are double-checked to make sure
that they are not in the CLQS training corpus. Then CLQS system
is used to suggest relevant English queries for them. On average,
for each French query, 8.7 relevant English queries are suggested.
Then the total 1,740 suggested English queries are manually
checked by two professional English/French translators with
cross-validation. Among the 1,747 suggested queries, 1,407
queries are recognized as relevant to the original ones, hence the
accuracy is 80.9%. Figure 1 shows an example of CLQS of the
French query terrorisme international (international terrorism
in English).
5.3 CLIR Performance
In this section, CLQS is tested with French to English CLIR tasks.
We conduct CLIR experiments using the TREC 6 CLIR dataset
described in Section 5.1. The CLIR is performed using a query
translation system followed by a BM25-based [23] monolingual
IR module. The following three different systems have been used
to perform query translation: (1) CLQS: our CLQS system; (2)
MT: Google French to English machine translation system; (3)
DT: a dictionary based query translation system using 
cooccurrence statistics for translation disambiguation. The
translation disambiguation algorithm is presented in Section 3.3.1.
Besides, the monolingual IR performance is also reported as a
reference. The average precision of the four IR systems are
reported in Table 2, and the 11-point precision-recall curves are
shown in Figure 2.
Table 2. Average precision of CLIR on TREC 6 Dataset
(Monolingual: monolingual IR system; MT: CLIR based on
machine translation; DT: CLIR based on dictionary
translation; CLQS: CLQS-based CLIR)
IR System Average Precision % of Monolingual IR
Monolingual 0.266 100%
MT 0.217 81.6%
DT 0.186 69.9%
CLQS 0.233 87.6%
Figure 1. An example of CLQS of the French query
terrorisme international
international terrorism (0.991); what is terrorism (0.943);
counter terrorism (0.920); terrorist (0.911);
terrorist attacks (0.898); international terrorist (0.853);
world terrorism (0.845); global terrorism (0.833);
transnational terrorism (0.821); human rights (0.811);
terrorist groups (0. 777); patterns of global terrorism (0.762)
september 11 (0.734)
11-point P-R curves (TREC6)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Precison
Monolingual
MT
DT
CLQS
The benchmark shows that using CLQS as a query translation
tool outperforms CLIR based on machine translation by 7.4%,
outperforms CLIR based on dictionary translation by 25.2%, and
achieves 87.6% of the monolingual IR performance.
The effectiveness of CLQS lies in its ability in suggesting
closely related queries besides accurate translations. For example,
for the query CL14 terrorisme international (international
terrorism), although the machine translation tool translates the
query correctly, CLQS system still achieves higher score by
recommending many additional related terms such as global
terrorism, world terrorism, etc. (as shown in Figure 1). Another
example is the query La pollution causée par l'automobile (air
pollution due to automobile) of CL6. The MT tool provides the
translation the pollution caused by the car, while CLQS system
enumerates all the possible synonyms of car, and suggest the
following queries car pollution, auto pollution, automobile
pollution. Besides, other related queries such as global
warming are also suggested. For the query CL12 La culture
écologique (organic farming), the MT tool fails to generate the
correct translation. Although the correct translation is neither in
our French-English dictionary, CLQS system generates organic
farm as a relevant query due to successful web mining.
The above experiment demonstrates the effectiveness of using
CLQS to suggest relevant queries for CLIR enhancement. A
related research is to perform query expansion to enhance CLIR
[2, 18]. So it is very interesting to compare the CLQS approach
with the conventional query expansion approaches. Following
[18], post-translation expansion is performed based on 
pseudorelevance feedback (PRF) techniques. We first perform CLIR in
the same way as before. Then we use the traditional PRF
algorithm described in [24] to select expansion terms. In our
experiments, the top 10 terms are selected to expand the original
query, and the new query is used to search the collection for the
second time. The new CLIR performance in terms of average
precision is shown in Table 3. The 11-point P-R curves are drawn
in Figure 3.
Although being enhanced by pseudo-relevance feedback, the
CLIR using either machine translation or dictionary-based query
translation still does not perform as well as CLQS-based
approach. Statistical t-test [13] is conducted to indicate whether
the CLQS-based CLIR performs significantly better. Pair-wise 
pvalues are shown in Table 4. Clearly, CLQS significantly
outperforms MT and DT without PRF as well as DT+PRF, but its
superiority over MT+PRF is not significant. However, when
combined with PRF, CLQS significant outperforms all the other
methods. This indicates the higher effectiveness of CLQS in
related term identification by leveraging a wide spectrum of
resources. Furthermore, post-translation expansion is capable of
improving CLQS-based CLIR. This is due to the fact that CLQS
and pseudo-relevance feedback are leveraging different categories
of resources, and both approaches can be complementary.
IR System AP without PRF AP with PRF
Monolingual 0.266 (100%) 0.288 (100%)
MT 0.217 (81.6%) 0.222 (77.1%)
DT 0.186 (69.9%) 0.220 (76.4%)
CLQS 0.233 (87.6%) 0.259 (89.9%)
11-point P-R curves with pseudo relevance feedback (TREC6)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Precison
Monolingual
MT
DT
CLQS
MT DT MT+PRF DT+PRF
CLQS 0.0298 3.84e-05 0.1472 0.0282
CLQS+PR
F
0.0026 2.63e-05 0.0094 0.0016
6. CONCLUSIONS
In this paper, we proposed a new approach to cross-lingual query
suggestion by mining relevant queries in different languages from
query logs. The key solution to this problem is to learn a 
crosslingual query similarity measure by a discriminative model
exploiting multiple monolingual and bilingual resources. The
model is trained based on the principle that cross-lingual
similarity should best fit the monolingual similarity between one
query and the other query"s translation.
Figure 2. 11 points precision-recall on TREC6 CLIR data set
Figure 3. 11 points precision-recall on TREC6 CLIR
dataset with pseudo relevance feedback
Table 3. Comparison of average precision (AP) on TREC 6
without and with post-translation expansion. (%) are the
relative percentages over the monolingual IR performance
Table 4. The results of pair-wise significance t-test. Here 
pvalue < 0.05 is considered statistically significant
The baseline CLQS system applies a typical query translation
approach, using a bilingual dictionary with co-occurrence-based
translation disambiguation. This approach only covers 10% of the
relevant queries suggested by an MLQS system (when the exact
translation of the original query is given). By leveraging
additional resources such as parallel corpora, web mining and 
logbased monolingual query expansion, the final system is able to
cover 42% of the relevant queries suggested by an MLQS system
with precision as high as 79.6%.
To further test the quality of the suggested queries, CLQS system
is used as a query translation system in CLIR tasks.
Benchmarked using TREC 6 French to English CLIR task, CLQS
demonstrates higher effectiveness than the traditional query
translation methods using either bilingual dictionary or
commercial machine translation tools.
The improvement on TREC French to English CLIR task by
using CLQS demonstrates the high quality of the suggested
queries. This also shows the strong correspondence between the
input French queries and English queries in the log. In the future,
we will build CLQS system between languages which may be
more loosely correlated, e.g., English and Chinese, and study the
CLQS performance change due to the less strong correspondence
among queries in such languages.
7. REFERENCES
[1] Ambati, V. and Rohini., U. Using Monolingual Clickthrough
Data to Build Cross-lingual Search Systems. In Proceedings
of New Directions in Multilingual Information Access
Workshop of SIGIR 2006.
[2] Ballestors, L. A. and Croft, W. B. Phrasal Translation and
Query Expansion Techniques for Cross-Language
Information Retrieval. In Proc. SIGIR 1997, pp. 84-91.
[3] Ballestors, L. A. and Croft, W. B. Resolving Ambiguity for
Cross-Language Retrieval. In Proc. SIGIR 1998, pp. 64-71.
[4] Brown, P. F., Pietra, D. S. A., Pietra, D. V. J., and Mercer, R.
L. The Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics, 
19(2):263311, 1993.
[5] Chang, C. C. and Lin, C. LIBSVM: a Library for Support
Vector Machines (Version 2.3). 2001.
http://citeseer.ist.psu.edu/chang01libsvm.html
[6] Chen, H.-H., Lin, M.-S., and Wei, Y.-C. Novel Association
Measures Using Web Search with Double Checking. In Proc.
COLING/ACL 2006, pp. 1009-1016.
[7] Cheng, P.-J., Teng, J.-W., Chen, R.-C., Wang, J.-H., Lu, 
W.H., and Chien, L.-F. Translating Unknown Queries with Web
Corpora for Cross-Language Information Retrieval. In Proc.
SIGIR 2004, pp. 146-153.
[8] Cui, H., Wen, J. R., Nie, J.-Y., and Ma, W. Y. Query
Expansion by Mining User Logs. IEEE Trans. on Knowledge
and Data Engineering, 15(4):829-839, 2003.
[9] Fujii A. and Ishikawa, T. Applying Machine Translation to
Two-Stage Cross-Language Information Retrieval. In
Proceedings of 4th Conference of the Association for
Machine Translation in the Americas, pp. 13-24, 2000.
[10] Gao, J. F., Nie, J.-Y., Xun, E., Zhang, J., Zhou, M., and
Huang, C. Improving query translation for CLIR using
statistical Models. In Proc. SIGIR 2001, pp. 96-104.
[11] Gao, J. F., Nie, J.-Y., He, H., Chen, W., and Zhou, M.
Resolving Query Translation Ambiguity using a Decaying
Co-occurrence Model and Syntactic Dependence Relations.
In Proc. SIGIR 2002, pp. 183-190.
[12] Gleich, D., and Zhukov, L. SVD Subspace Projections for
Term Suggestion Ranking and Clustering. In Technical
Report, Yahoo! Research Labs, 2004.
[13] Hull, D. Using Statistical Testing in the Evaluation of
Retrieval Experiments. In Proc. SIGIR 1993, pp. 329-338.
[14] Jeon, J., Croft, W. B., and Lee, J. Finding Similar Questions
in Large Question and Answer Archives. In Proc. CIKM
2005, pp. 84-90.
[15] Joachims, T. Optimizing Search Engines Using Clickthrough
Data. In Proc. SIGKDD 2002, pp. 133-142.
[16] Lavrenko, V., Choquette, M., and Croft, W. B. Cross-Lingual
Relevance Models. In Proc. SIGIR 2002, pp. 175-182.
[17] Lu, W.-H., Chien, L.-F., and Lee, H.-J. Anchor Text Mining
for Translation Extraction of Query Terms. In Proc. SIGIR
2001, pp. 388-389.
[18] McNamee, P. and Mayfield, J. Comparing Cross-Language
Query Expansion Techniques by Degrading Translation
Resources. In Proc. SIGIR 2002, pp. 159-166.
[19] Monz, C. and Dorr, B. J. Iterative Translation
Disambiguation for Cross-Language Information Retrieval.
In Proc. SIGIR 2005, pp. 520-527.
[20] Nie, J.-Y., Simard, M., Isabelle, P., and Durand, R. 
CrossLanguage Information Retrieval based on Parallel Text and
Automatic Mining of Parallel Text from the Web. In Proc.
SIGIR 1999, pp. 74-81.
[21] Och, F. J. and Ney, H. A Systematic Comparison of Various
Statistical Alignment Models. Computational Linguistics,
29(1):19-51, 2003.
[22] Pirkola, A., Hedlund, T., Keshusalo, H., and Järvelin, K.
Dictionary-Based Cross-Language Information Retrieval:
Problems, Methods, and Research Findings. Information
Retrieval, 4(3/4):209-230, 2001.
[23] Robertson, S. E., Walker, S., Hancock-Beaulieu, M. M., and
Gatford, M. OKAPI at TREC-3. In Proc.TREC-3, pp. 
200225, 1995.
[24] Robertson, S. E. and Jones, K. S. Relevance Weighting of
Search Terms. Journal of the American Society of
Information Science, 27(3):129-146, 1976.
[25] Smola, A. J. and Schölkopf, B. A. Tutorial on Support Vector
Regression. Statistics and Computing, 14(3):199-222, 2004.
[26] Wen, J. R., Nie, J.-Y., and Zhang, H. J. Query Clustering
Using User Logs. ACM Trans. Information Systems,
20(1):59-81, 2002.
[27] Zhang, Y. and Vines, P. Using the Web for Automated
Translation Extraction in Cross-Language Information
Retrieval. In Proc. SIGIR 2004, pp. 162-169.
A Study of Factors Affecting the Utility of
Implicit Relevance Feedback
Ryen W. White
Human-Computer Interaction Laboratory
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
ryen@umd.edu
Ian Ruthven
Department of Computer and
Information Sciences
University of Strathclyde
Glasgow, Scotland. G1 1XH.
ir@cis.strath.ac.uk
Joemon M. Jose
Department of Computing Science
University of Glasgow
Glasgow, Scotland. G12 8RZ.
jj@dcs.gla.ac.uk
ABSTRACT
Implicit relevance feedback (IRF) is the process by which a search
system unobtrusively gathers evidence on searcher interests from their
interaction with the system. IRF is a new method of gathering
information on user interest and, if IRF is to be used in operational IR
systems, it is important to establish when it performs well and when it
performs poorly. In this paper we investigate how the use and
effectiveness of IRF is affected by three factors: search task
complexity, the search experience of the user and the stage in the
search. Our findings suggest that all three of these factors contribute
to the utility of IRF.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]
General Terms
Experimentation, Human Factors.
1. INTRODUCTION
Information Retrieval (IR) systems are designed to help searchers
solve problems. In the traditional interaction metaphor employed by
Web search systems such as Yahoo! and MSN Search, the system
generally only supports the retrieval of potentially relevant documents
from the collection. However, it is also possible to offer support to
searchers for different search activities, such as selecting the terms to
present to the system or choosing which search strategy to adopt [3,
8]; both of which can be problematic for searchers.
As the quality of the query submitted to the system directly affects the
quality of search results, the issue of how to improve search queries
has been studied extensively in IR research [6]. Techniques such as
Relevance Feedback (RF) [11] have been proposed as a way in which
the IR system can support the iterative development of a search query
by suggesting alternative terms for query modification. However, in
practice RF techniques have been underutilised as they place an
increased cognitive burden on searchers to directly indicate relevant
results [10].
Implicit Relevance Feedback (IRF) [7] has been proposed as a way in
which search queries can be improved by passively observing
searchers as they interact. IRF has been implemented either through
the use of surrogate measures based on interaction with documents
(such as reading time, scrolling or document retention) [7] or using
interaction with browse-based result interfaces [5]. IRF has been
shown to display mixed effectiveness because the factors that are good
indicators of user interest are often erratic and the inferences drawn
from user interaction are not always valid [7].
In this paper we present a study into the use and effectiveness of IRF
in an online search environment. The study aims to investigate the
factors that affect IRF, in particular three research questions: (i) is the
use of and perceived quality of terms generated by IRF affected by the
search task? (ii) is the use of and perceived quality of terms generated
by IRF affected by the level of search experience of system users? (iii)
is IRF equally used and does it generate terms that are equally useful
at all search stages? This study aims to establish when, and under what
circumstances, IRF performs well in terms of its use and the query
modification terms selected as a result of its use.
The main experiment from which the data are taken was designed to
test techniques for selecting query modification terms and techniques
for displaying retrieval results [13]. In this paper we use data derived
from that experiment to study factors affecting the utility of IRF.
2. STUDY
In this section we describe the user study conducted to address our
research questions.
2.1 Systems
Our study used two systems both of which suggested new query terms
to the user. One system suggested terms based on the user"s
interaction (IRF), the other used Explicit RF (ERF) asking the user to
explicitly indicate relevant material. Both systems used the same term
suggestion algorithm, [15], and used a common interface.
2.1.1 Interface Overview
In both systems, retrieved documents are represented at the interface
by their full-text and a variety of smaller, query-relevant
representations, created at retrieval time. We used the Web as the test
collection in this study and Google1
as the underlying search engine.
Document representations include the document title and a summary
of the document; a list of top-ranking sentences (TRS) extracted from
the top documents retrieved, scored in relation to the query, a sentence
in the document summary, and each summary sentence in the context
it occurs in the document (i.e., with the preceding and following
sentence). Each summary sentence and top-ranking sentence is
regarded as a representation of the document. The default display
contains the list of top-ranking sentences and the list of the first ten
document titles. Interacting with a representation guides searchers to a
different representation from the same document, e.g., moving the
mouse over a document title displays a summary of the document.
This presentation of progressively more information from documents
to aid relevance assessments has been shown to be effective in earlier
work [14, 16]. In Appendix A we show the complete interface to the
IRF system with the document representations marked and in
Appendix B we show a fragment from the ERF interface with the
checkboxes used by searchers to indicate relevant information. Both
systems provide an interactive query expansion feature by suggesting
new query terms to the user. The searcher has the responsibility for
choosing which, if any, of these terms to add to the query. The
searcher can also add or remove terms from the query at will.
2.1.2 Explicit RF system
This version of the system implements explicit RF. Next to each
document representation are checkboxes that allow searchers to mark
individual representations as relevant; marking a representation is an
indication that its contents are relevant. Only the representations
marked relevant by the user are used for suggesting new query terms.
This system was used as a baseline against which the IRF system
could be compared.
2.1.3 Implicit RF system
This system makes inferences about searcher interests based on the
information with which they interact. As described in Section 2.1.1
interacting with a representation highlights a new representation from
the same document. To the searcher this is a way they can find out
more information from a potentially interesting source. To the implicit
RF system each interaction with a representation is interpreted as an
implicit indication of interest in that representation; interacting with a
representation is assumed to be an indication that its contents are
relevant. The query modification terms are selected using the same
algorithm as in the Explicit RF system. Therefore the only difference
between the systems is how relevance is communicated to the system.
The results of the main experiment [13] indicated that these two
systems were comparable in terms of effectiveness.
2.2 Tasks
Search tasks were designed to encourage realistic search behaviour by
our subjects. The tasks were phrased in the form of simulated work
task situations [2], i.e., short search scenarios that were designed to
reflect real-life search situations and allow subjects to develop
personal assessments of relevance. We devised six search topics (i.e.,
applying to university, allergies in the workplace, art galleries in
Rome, Third Generation mobile phones, Internet music piracy and
petrol prices) based on pilot testing with a small representative group
of subjects. These subjects were not involved in the main experiment.
For each topic, three versions of each work task situation were
devised, each version differing in their predicted level of task
complexity. As described in [1] task complexity is a variable that
affects subject perceptions of a task and their interactive behaviour,
e.g., subjects perform more filtering activities with highly complex
search tasks. By developing tasks of different complexity we can
assess how the nature of the task affects the subjects" interactive
behaviour and hence the evidence supplied to IRF algorithms. Task
complexity was varied according to the methodology described in [1],
specifically by varying the number of potential information sources
and types of information required, to complete a task. In our pilot
tests (and in a posteriori analysis of the main experiment results) we
verified that subjects reporting of individual task complexity matched
our estimation of the complexity of the task.
Subjects attempted three search tasks: one high complexity, one
moderate complexity and one low complexity2
. They were asked to
read the task, place themselves in the situation it described and find
the information they felt was required to complete the task. Figure 1
shows the task statements for three levels of task complexity for one
of the six search topics.
HC Task: High Complexity
Whilst having dinner with an American colleague, they comment on the
high price of petrol in the UK compared to other countries, despite large
volumes coming from the same source. Unaware of any major differences,
you decide to find out how and why petrol prices vary worldwide.
MC Task: Moderate Complexity
Whilst out for dinner one night, one of your friends" guests is complaining
about the price of petrol and the factors that cause it. Throughout the night
they seem to be complaining about everything they can, reducing the
credibility of their earlier statements so you decide to research which
factors actually are important in determining the price of petrol in the UK.
LC Task: Low Complexity
While out for dinner one night, your friend complains about the rising
price of petrol. However, as you have not been driving for long, you are
unaware of any major changes in price. You decide to find out how the
price of petrol has changed in the UK in recent years.
Figure 1. Varying task complexity (Petrol Prices topic).
2.3 Subjects
156 volunteers expressed an interest in participating in our study. 48
subjects were selected from this set with the aim of populating two
groups, each with 24 subjects: inexperienced (infrequent/
inexperienced searchers) and experienced (frequent/ experienced
searchers). Subjects were not chosen and classified into their groups
until they had completed an entry questionnaire that asked them about
their search experience and computer use.
The average age of the subjects was 22.83 years (maximum 51,
minimum 18, σ = 5.23 years) and 75% had a university diploma or a
higher degree. 47.91% of subjects had, or were pursuing, a
qualification in a discipline related to Computer Science. The subjects
were a mixture of students, researchers, academic staff and others,
with different levels of computer and search experience. The subjects
were divided into the two groups depending on their search
experience, how often they searched and the types of searches they
performed. All were familiar with Web searching, and some with
searching in other domains.
2.4 Methodology
The experiment had a factorial design; with 2 levels of search
experience, 3 experimental systems (although we only report on the
findings from the ERF and IRF systems) and 3 levels of search task
complexity. Subjects attempted one task of each complexity,
2
The main experiment from which these results are drawn had a third
comparator system which had a different interface. Each subject
carried out three tasks, one on each system. We only report on the
results from the ERF and IRF systems as these are the only pertinent
ones for this paper.
switched systems after each task and used each system once. The
order in which systems were used and search tasks attempted was
randomised according to a Latin square experimental design.
Questionnaires used Likert scales, semantic differentials and 
openended questions to elicit subject opinions [4]. System logging was
also used to record subject interaction.
A tutorial carried out prior to the experiment allowed subjects to use a
non-feedback version of the system to attempt a practice task before
using the first experimental system. Experiments lasted between 
oneand-a-half and two hours, dependent on variables such as the time
spent completing questionnaires. Subjects were offered a 5 minute
break after the first hour. In each experiment:
i. the subject was welcomed and asked to read an introduction to
the experiments and sign consent forms. This set of instructions
was written to ensure that each subject received precisely the
same information.
ii. the subject was asked to complete an introductory questionnaire.
This contained questions about the subject"s education, general
search experience, computer experience and Web search
experience.
iii. the subject was given a tutorial on the interface, followed by a
training topic on a version of the interface with no RF.
iv. the subject was given three task sheets and asked to choose one
task from the six topics on each sheet. No guidelines were given
to subjects when choosing a task other than they could not
choose a task from any topic more than once. Task complexity
was rotated by the experimenter so each subject attempted one
high complexity task, one moderate complexity task and one low
complexity task.
v. the subject was asked to perform the search and was given 15
minutes to search. The subject could terminate a search early if
they were unable to find any more information they felt helped
them complete the task.
vi. after completion of the search, the subject was asked to complete
a post-search questionnaire.
vii. the remaining tasks were attempted by the subject, following
steps v. and vi.
viii. the subject completed a post-experiment questionnaire and
participated in a post-experiment interview.
Subjects were told that their interaction may be used by the IRF
system to help them as they searched. They were not told which
behaviours would be used or how it would be used.
We now describe the findings of our analysis.
3. FINDINGS
In this section we use the data derived from the experiment to answer
our research questions about the effect of search task complexity,
search experience and stage in search on the use and effectiveness of
IRF. We present our findings per research question. Due to the
ordinal nature of much of the data non-parametric statistical testing is
used in this analysis and the level of significance is set to p < .05,
unless otherwise stated. We use the method proposed by [12] to
determine the significance of differences in multiple comparisons and
that of [9] to test for interaction effects between experimental
variables, the occurrence of which we report where appropriate. All
Likert scales and semantic differentials were on a 5-point scale where
a rating closer to 1 signifies more agreement with the attitude
statement. The category labels HC, MC and LC are used to denote the
high, moderate and low complexity tasks respectively. The highest, or
most positive, values in each table are shown in bold. Our analysis
uses data from questionnaires, post-experiment interviews and
background system logging on the ERF and IRF systems.
3.1 Search Task
Searchers attempted three search tasks of varying complexity, each on
a different experimental system. In this section we present an analysis
on the use and usefulness of IRF for search tasks of different
complexities. We present our findings in terms of the RF provided by
subjects and the terms recommended by the systems.
3.1.1 Feedback
We use questionnaires and system logs to gather data on subject
perceptions and provision of RF for different search tasks. In the 
postsearch questionnaire subjects were asked about how RF was conveyed
using differentials to elicit their opinion on:
1. the value of the feedback technique: How you conveyed relevance
to the system (i.e. ticking boxes or viewing information) was: easy /
difficult, effective/ ineffective, useful"/not useful.
2. the process of providing the feedback: How you conveyed relevance
to the system made you feel: comfortable/uncomfortable, in
control/not in control.
The average obtained differential values are shown in Table 1 for IRF
and each task category. The value corresponding to the differential
All represents the mean of all differentials for a particular attitude
statement. This gives some overall understanding of the subjects"
feelings which can be useful as the subjects may not answer individual
differentials very precisely. The values for ERF are included for
reference in this table and all other tables and figures in the Findings
section. Since the aim of the paper is to investigate situations in which
IRF might perform well, not a direct comparison between IRF and
ERF, we make only limited comparisons between these two types of
feedback.
Table 1. Subject perceptions of RF method (lower = better).
Each cell in Table 1 summarises the subject responses for 16 
tasksystem pairs (16 subjects who ran a high complexity (HC) task on the
ERF system, 16 subjects who ran a medium complexity (MC) task on
the ERF system, etc). Kruskal-Wallis Tests were applied to each
differential for each type of RF3
. Subject responses suggested that
3
Since this analysis involved many differentials, we use a Bonferroni
correction to control the experiment-wise error rate and set the alpha
level (α) to .0167 and .0250 for both statements 1. and 2.
respectively, i.e., .05 divided by the number of differentials. This
correction reduces the number of Type I errors i.e., rejecting null
hypotheses that are true.
Explicit RF Implicit RF
Differential
HC MC LC HC MC LC
Easy 2.78 2.47 2.12 1.86 1.81 1.93
Effective 2.94 2.68 2.44 2.04 2.41 2.66
Useful 2.76 2.51 2.16 1.91 2.37 2.56
All (1) 2.83 2.55 2.24 1.94 2.20 2.38
Comfortable 2.27 2.28 2.35 2.11 2.15 2.16
In control 2.01 1.97 1.93 2.73 2.68 2.61
All (2) 2.14 2.13 2.14 2.42 2.42 2.39
IRF was most effective and useful for more complex search tasks4
and that the differences in all pair-wise comparisons between tasks
were significant5
. Subject perceptions of IRF elicited using the other
differentials did not appear to be affected by the complexity of the
search task6
. To determine whether a relationship exists between the
effectiveness and usefulness of the IRF process and task complexity
we applied Spearman"s Rank Order Correlation Coefficient to
participant responses. The results of this analysis suggest that the
effectiveness of IRF and usefulness of IRF are both related to task
complexity; as task complexity increases subject preference for IRF
also increases7
.
On the other hand, subjects felt ERF was more effective and useful
for low complexity tasks8
. Their verbal reporting of ERF, where
perceived utility and effectiveness increased as task complexity
decreased, supports this finding. In tasks of lower complexity the
subjects felt they were better able to provide feedback on whether or
not documents were relevant to the task.
We analyse interaction logs generated by both interfaces to investigate
the amount of RF subjects provided. To do this we use a measure of
search precision that is the proportion of all possible document
representations that a searcher assessed, divided by the total number
they could assess. In ERF this is the proportion of all possible
representations that were marked relevant by the searcher, i.e., those
representations explicitly marked relevant. In IRF this is the
proportion of representations viewed by a searcher over all possible
representations that could have been viewed by the searcher. This
proportion measures the searcher"s level of interaction with a
document, we take it to measure the user"s interest in the document:
the more document representations viewed the more interested we
assume a user is in the content of the document.
There are a maximum of 14 representations per document: 4 
topranking sentences, 1 title, 1 summary, 4 summary sentences and 4
summary sentences in document context. Since the interface shows
document representations from the top-30 documents, there are 420
representations that a searcher can assess. Table 2 shows proportion
of representations provided as RF by subjects.
Table 2. Feedback and documents viewed.
Explicit RF Implicit RF
Measure
HC MC LC HC MC LC
Proportion
Feedback
2.14 2.39 2.65 21.50 19.36 15.32
Documents
Viewed
10.63 10.43 10.81 10.84 12.19 14.81
For IRF there is a clear pattern: as complexity increases the subjects
viewed fewer documents but viewed more representations for each
document. This suggests a pattern where users are investigating
retrieved documents in more depth. It also means that the amount of
4
effective: χ2
(2) = 11.62, p = .003; useful: χ2
(2) = 12.43, p = .002
5
Dunn"s post-hoc tests (multiple comparison using rank sums); all Z ≥
2.88, all p ≤ .002
6
all χ2
(2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests)
7
effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009
8
effective: χ2
(2) = 7.01, p = .03; useful: χ2
(2) = 6.59, p = .037
(Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥
2.34, all p ≤ .01 (Dunn"s post-hoc tests)
feedback varies based on the complexity of the search task. Since IRF
is based on the interaction of the searcher, the more they interact, the
more feedback they provide. This has no effect on the number of RF
terms chosen, but may affect the quality of the terms selected.
Correlation analysis revealed a strong negative correlation between the
number of documents viewed and the amount of feedback searchers
provide9
; as the number of documents viewed increases the proportion
of feedback falls (searchers view less representations of each
document). This may be a natural consequence of their being less
time to view documents in a time constrained task environment but as
we will show as complexity changes, the nature of information
searchers interact with also appears to change. In the next section we
investigate the effect of task complexity on the terms chosen as a
result of IRF.
3.1.2 Terms
The same RF algorithm was used to select query modification terms in
all systems [16]. We use subject opinions of terms recommended by
the systems as a measure of the effectiveness of IRF with respect to
the terms generated for different search tasks. To test this, subjects
were asked to complete two semantic differentials that completed the
statement: The words chosen by the system were:
relevant/irrelevant and useful/not useful. Table 3 presents
average responses grouped by search task.
Table 3. Subject perceptions of system terms (lower = better).
Explicit RF Implicit RF
Differential
HC MC LC HC MC LC
Relevant 2.50 2.46 2.41 1.94 2.35 2.68
Useful 2.61 2.61 2.59 2.06 2.54 2.70
Kruskal-Wallis Tests were applied within each type of RF. The
results indicate that the relevance and usefulness of the terms chosen
by IRF is affected by the complexity of the search task; the terms
chosen are more relevant and useful when the search task is more
complex. 10
Relevant here, was explained as being related to their task
whereas useful was for terms that were seen as being helpful in the
search task. For ERF, the results indicate that the terms generated are
perceived to be more relevant and useful for less complex search
tasks; although differences between tasks were not significant11
. This
suggests that subject perceptions of the terms chosen for query
modification are affected by task complexity. Comparison between
ERF and IRF shows that subject perceptions also vary for different
types of RF12
.
As well as using data on relevance and utility of the terms chosen, we
used data on term acceptance to measure the perceived value of the
terms suggested. Explicit and Implicit RF systems made
recommendations about which terms could be added to the original
search query. In Table 4 we show the proportion of the top six terms
9
r = −0.696, p = .001 (Pearson"s Correlation Coefficient)
10
relevant: χ2
(2) = 13.82, p = .001; useful: χ2
(2) = 11.04, p = .004; α
= .025
11
all χ2
(2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test)
12
all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test)
13
that were shown to the searcher that were added to the search
query, for each type of task and each type of RF.
Table 4. Term Acceptance (percentage of top six terms).
Explicit RF Implicit RFProportion
of terms HC MC LC HC MC LC
Accepted 65.31 67.32 68.65 67.45 67.24 67.59
The average number of terms accepted from IRF is approximately the
same across all search tasks and generally the same as that of ERF14
.
As Table 2 shows, subjects marked fewer documents relevant for
highly complex tasks . Therefore, when task complexity increases the
ERF system has fewer examples of relevant documents and the
expansion terms generated may be poorer. This could explain the
difference in the proportion of recommended terms accepted in ERF
as task complexity increases. For IRF there is little difference in how
many of the recommended terms were chosen by subjects for each
level of task complexity15
. Subjects may have perceived IRF terms as
more useful for high complexity tasks but this was not reflected in the
proportion of IRF terms accepted. Differences may reside in the
nature of the terms accepted; future work will investigate this issue.
3.1.3 Summary
In this section we have presented an investigation on the effect of
search task complexity on the utility of IRF. From the results there
appears to be a strong relation between the complexity of the task and
the subject interaction: subjects preferring IRF for highly complex
tasks. Task complexity did not affect the proportion of terms accepted
in either RF method, despite there being a difference in how
relevant and useful subjects perceived the terms to be for different
complexities; complexity may affect term selection in ways other than
the proportion of terms accepted.
3.2 Search Experience
Experienced searchers may interact differently and give different
types of evidence to RF than inexperienced searchers. As such, levels
of search experience may affect searchers" use and perceptions of IRF.
In our experiment subjects were divided into two groups based on
their level of search experience, the frequency with which they
searched and the types of searches they performed. In this section we
use their perceptions and logging to address the next research
question; the relationship between the usefulness and use of IRF and
the search experience of experimental subjects. The data are the same
as that analysed in the previous section, but here we focus on search
experience rather than the search task.
3.2.1 Feedback
We analyse the results from the attitude statements described at the
beginning of Section 3.1.1. (i.e., How you conveyed relevance to the
system was… and How you conveyed relevance to the system made
you feel…). These differentials elicited opinion from experimental
subjects about the RF method used. In Table 5 we show the mean
average responses for inexperienced and experienced subject groups
on ERF and IRF; 24 subjects per cell.
13
This was the smallest number of query modification terms that were
offered in both systems.
14
all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test)
15
ERF: χ2
(2) = 3.67, p = .16; IRF: χ2
(2) = 2.55, p = .28 
(KruskalWallis Tests)
Table 5. Subject perceptions of RF method (lower = better).
The results demonstrate a strong preference in inexperienced subjects
for IRF; they found it more easy and effective than experienced
subjects. 16
The differences for all other IRF differentials were not
statistically significant. For all differentials, apart from in control,
inexperienced subjects generally preferred IRF over ERF17
.
Inexperienced subjects also felt that IRF was more difficult to control
than experienced subjects18
. As these subjects have less search
experience they may be less able to understand RF processes and may
be more comfortable with the system gathering feedback implicitly
from their interaction. Experienced subjects tended to like ERF more
than inexperienced subjects and felt more comfortable with this
feedback method19
. It appears from these results that experienced
subjects found ERF more useful and were more at ease with the ERF
process.
In a similar way to Section 3.1.1 we analysed the proportion of
feedback that searchers provided to the experimental systems. Our
analysis suggested that search experience does not affect the amount
of feedback subjects provide20
.
3.2.2 Terms
We used questionnaire responses to gauge subject opinion on the
relevance and usefulness of the terms from the perspective of
experienced and inexperienced subjects. Table 6 shows the average
differential responses obtained from both subject groups.
Table 6. Subject perceptions of system terms (lower = better).
Explicit RF Implicit RF
Differential
Inexp. Exp. Inexp. Exp.
Relevant 2.58 2.44 2.33 2.21
Useful 2.88 2.63 2.33 2.23
The differences between subject groups were significant21
.
Experienced subjects generally reacted to the query modification
terms chosen by the system more positively than inexperienced
16
easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α =
.0167 (Mann-Whitney Tests)
17
all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test)
18
U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test)
19
T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test)
20
ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 
(MannWhitney Tests)
21
ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024
Explicit RF Implicit RF
Differential
Inexp. Exp. Inexp. Exp.
Easy 2.46 2.46 1.84 1.98
Effective 2.75 2.63 2.32 2.43
Useful 2.50 2.46 2.28 2.27
All (1) 2.57 2.52 2.14 2.23
Comfortable 2.46 2.14 2.05 2.24
In control 1.96 1.98 2.73 2.64
All (2) 2.21 2.06 2.39 2.44
subjects. This finding was supported by the proportion of query
modification terms these subjects accepted. In the same way as in
Section 3.1.2, we analysed the number of query modification terms
recommended by the system that were used by experimental subjects.
Table 7 shows the average number of accepted terms per subject
group.
Table 7. Term Acceptance (percentage of top six terms).
Explicit RF Implicit RFProportion
of terms Inexp. Exp. Inexp. Exp.
Accepted 63.76 70.44 64.43 71.35
Our analysis of the data show that differences between subject groups
for each type of RF are significant; experienced subjects accepted
more expansion terms regardless of type of RF. However, the
differences between the same groups for different types of RF are not
significant; subjects chose roughly the same percentage of expansion
terms offered irrespective of the type of RF22
.
3.2.3 Summary
In this section we have analysed data gathered from two subject
groups - inexperienced searchers and experienced searchers - on how
they perceive and use IRF. The results indicate that inexperienced
subjects found IRF more easy and effective than experienced
subjects, who in turn found the terms chosen as a result of IRF more
relevant and useful. We also showed that inexperienced subjects
generally accepted less recommended terms than experienced
subjects, perhaps because they were less comfortable with RF or
generally submitted shorter search queries. Search experience appears
to affect how subjects use the terms recommended as a result of the
RF process.
3.3 Search Stage
From our observations of experimental subjects as they searched we
conjectured that RF may be used differently at different times during a
search. To test this, our third research question concerned the use and
usefulness of IRF during the course of a search. In this section we
investigate whether the amount of RF provided by searchers or the
proportion of terms accepted are affected by how far through their
search they are. For the purposes of this analysis a search begins when
a subject poses the first query to the system and progresses until they
terminate the search or reach the maximum allowed time for a search
task of 15 minutes. We do not divide tasks based on this limit as
subjects often terminated their search in less than 15 minutes.
In this section we use data gathered from interaction logs and subject
opinions to investigate the extent to which RF was used and the extent
to which it appeared to benefit our experimental subjects at different
stages in their search
3.3.1 Feedback
The interaction logs for all searches on the Explicit RF and Implicit
RF were analysed and each search is divided up into nine equal length
time slices. This number of slices gave us an equal number per stage
and was a sufficient level of granularity to identify trends in the
results. Slices 1 - 3 correspond to the start of the search, 4 - 6 to the
middle of the search and 7 - 9 to the end. In Figure 2 we plot the
measure of precision described in Section 3.1.1 (i.e., the proportion
of all possible representations that were provided as RF) at each of the
22
IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013
nine slices, per search task, averaged across all subjects; this allows us
to see how the provision of RF was distributed during a search. The
total amount of feedback for a single RF method/task complexity
pairing across all nine slices corresponds to the value recorded in the
first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine
slices of Figure 2 is 21.50%). To simplify the statistical analysis and
comparison we use the grouping of start, middle and end.
0 1 2 3 4 5 6 7 8 9
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
Slice
Search"precision"(%oftotalrepsprovidedasRF)
Explicit RF/HC
Explicit RF/MC
Explicit RF/LC
Implicit RF/HC
Implicit RF/MC
Implicit RF/LC
Figure 2. Distribution of RF provision per search task.
Figure 2 appears to show the existence of a relationship between the
stage in the search and the amount of relevance information provided
to the different types of feedback algorithm. These are essentially
differences in the way users are assessing documents. In the case of
ERF subjects provide explicit relevance assessments throughout most
of the search, but there is generally a steep increase in the end phase
towards the completion of the search23
.
When using the IRF system, the data indicates that at the start of the
search subjects are providing little relevance information24
, which
corresponds to interacting with few document representations. At this
stage the subjects are perhaps concentrating more on reading the
retrieved results. Implicit relevance information is generally offered
extensively in the middle of the search as they interact with results and
it then tails off towards the end of the search. This would appear to
correspond to stages of initial exploration, detailed analysis of
document representations and storage and presentation of findings.
Figure 2 also shows the proportion of feedback for tasks of different
complexity. The results appear to show a difference25
in how IRF is
used that relates to the complexity of the search task. More
specifically, as complexity increases it appears as though subjects take
longer to reach their most interactive point. This suggests that task
complexity affects how IRF is distributed during the search and that
they may be spending more time initially interpreting search results
for more complex tasks.
23
IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005
(Dunn"s post-hoc tests).
24
Although increasing toward the end of the start stage.
25
Although not statistically significant; χ2
(2) = 3.54, p = .17
(Friedman Rank Sum Test)
3.3.2 Terms
The terms recommended by the system are chosen based on the
frequency of their occurrence in the relevant items. That is, 
nonstopword, non-query terms occurring frequently in search results
regarded as relevant are likely to be recommended to the searcher for
query modification. Since there is a direct association between the RF
and the terms selected we use the number of terms accepted by
searchers at different points in the search as an indication of how
effective the RF has been up until the current point in the search. In
this section we analysed the average number of terms from the top six
terms recommended by Explicit RF and Implicit RF over the course of
a search. The average proportion of the top six recommended terms
that were accepted at each stage are shown in Table 8; each cell
contains data from all 48 subjects.
Table 8. Term Acceptance (proportion of top six terms).
Explicit RF Implicit RFProportion
of terms start middle end start middle end
Accepted 66.87 66.98 67.34 61.85 68.54 73.22
The results show an apparent association between the stage in the
search and the number of feedback terms subjects accept. Search
stage affects term acceptance in IRF but not in ERF26
. The further
into a search a searcher progresses, the more likely they are to accept
terms recommended via IRF (significantly more than ERF27
). A
correlation analysis between the proportion of terms accepted at each
search stage and cumulative RF (i.e., the sum of all precision at each
slice in Figure 2 up to and including the end of the search stage)
suggests that in both types of RF the quality of system terms improves
as more RF is provided28
.
3.3.3 Summary
The results from this section indicate that the location in a search
affects the amount of feedback given by the user to the system, and
hence the amount of information that the RF mechanism has to decide
which terms to offer the user. Further, trends in the data suggest that
the complexity of the task affects how subjects provide IRF and the
proportion of system terms accepted.
4. DISCUSSION AND IMPLICATIONS
In this section we discuss the implications of the findings presented in
the previous section for each research question.
4.1 Search Task
The results of our study showed that ERF was preferred for less
complex tasks and IRF for more complex tasks. From observations
and subject comments we perceived that when using ERF systems
subjects generally forgot to provide the feedback but also employed
different criteria during the ERF process (i.e., they were assessing
relevance rather than expressing an interest). When the search was
more complex subjects rarely found results they regarded as
completely relevant. Therefore they struggled to find relevant
26
ERF: χ2
(2) = 2.22, p = .33; IRF: χ2
(2) = 7.73, p = .021 (Friedman
Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥
1.77, all p ≤ .038 (Dunn"s post-hoc tests)
27
all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test)
28
IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson
Correlation Coefficient)
information and were unable to communicate RF to the search system.
In these situations subjects appeared to prefer IRF as they do not need
to make a relevance decision to obtain the benefits of RF, i.e., term
suggestions, whereas in ERF they do.
The association between RF method and task complexity has
implications for the design of user studies of RF systems and the RF
systems themselves. It implies that in the design of user studies
involving ERF or IRF systems care should be taken to include tasks of
varying complexities, to avoid task bias. Also, in the design of search
systems it implies that since different types of RF may be appropriate
for different task complexities then a system that could automatically
detect complexity could use both ERF and IRF simultaneously to
benefit the searcher. For example, on the IRF system we noticed that
as task complexity falls search behaviour shifts from results interface
to retrieved documents. Monitoring such interaction across a number
of studies may lead to a set of criteria that could help IR systems
automatically detect task complexity and tailor support to suit.
4.2 Search Experience
We analysed the affect of search experience on the utility of IRF. Our
analysis revealed a general preference across all subjects for IRF over
ERF. That is, the average ratings assigned to IRF were generally more
positive than those assigned to ERF. However, IRF was generally
liked by both subject groups (perhaps because it removed the burden
of providing relevance information) and ERF was generally preferred
by experienced subjects more than inexperienced subjects (perhaps
because it allowed them to specify which results were used by the
system when generating term recommendations).
All subjects felt more in control with ERF than IRF, but for
inexperienced subjects this did not appear to affect their overall
preferences29
. These subjects may understand the RF process less, but
may be more willing to sacrifice control over feedback in favour of
IRF, a process that they perceive more positively.
4.3 Search Stage
We also analysed the effects of search stage on the use and usefulness
of IRF. Through analysis of this nature we can build a more complete
picture of how searchers used RF and how this varies based on the RF
method. The results suggest that IRF is used more in the middle of
the search than at the beginning or end, whereas ERF is used more
towards the end. The results also show the effects of task complexity
on the IRF process and how rapidly subjects reach their most
interactive point. Without an analysis of this type it would not have
been possible to establish the existence of such patterns of behaviour.
The findings suggest that searchers interact differently for IRF and
ERF. Since ERF is not traditionally used until toward the end of the
search it may be possible to incorporate both IRF and ERF into the
same IR system, with IRF being used to gather evidence until subjects
decide to use ERF. The development of such a system represents part
of our ongoing work in this area.
5. CONCLUSIONS
In this paper we have presented an investigation of Implicit Relevance
Feedback (IRF). We aimed to answer three research questions about
factors that may affect the provision and usefulness of IRF. These
factors were search task complexity, the subjects" search experience
and the stage in the search. Our overall conclusion was that all factors
29
This may also be true for experienced subjects, but the data we have
is insufficient to draw this conclusion.
appear to have some effect on the use and effectiveness of IRF,
although the interaction effects between factors are not statistically
significant.
Our conclusions per each research question are: (i) IRF is generally
more useful for complex search tasks, where searchers want to focus
on the search task and get new ideas for their search from the system,
(ii) IRF is preferred to ERF overall and generally preferred by
inexperienced subjects wanting to reduce the burden of providing RF,
and (iii) within a single search session IRF is affected by temporal
location in a search (i.e., it is used in the middle, not the beginning or
end) and task complexity.
Studies of this nature are important to establish the circumstances
where a promising technique such as IRF are useful and those when it
is not. It is only after such studies have been run and analysed in this
way can we develop an understanding of IRF that allow it to be
successfully implemented in operational IR systems.
6. REFERENCES
[1] Bell, D.J. and Ruthven, I. (2004). Searchers' assessments of task
complexity for web searching. Proceedings of the 26th European
Conference on Information Retrieval, 57-71.
[2] Borlund, P. (2000). Experimental components for the evaluation
of interactive information retrieval systems. Journal of
Documentation. 56(1): 71-90.
[3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).
Strategic help for user interfaces for information retrieval.
Journal of the American Society for Information Science and
Technology. 53(5): 343-358.
[4] Busha, C.H. and Harter, S.P., (1980). Research methods in
librarianship: Techniques and interpretation. Library and
information science series. New York: Academic Press.
[5] Campbell, I. and Van Rijsbergen, C.J. (1996). The ostensive
model of developing information needs. Proceedings of the 3rd
International Conference on Conceptions of Library and
Information Science, 251-268.
[6] Harman, D., (1992). Relevance feedback and other query
modification techniques. In Information retrieval: Data
structures and algorithms. New York: Prentice-Hall.
[7] Kelly, D. and Teevan, J. (2003). Implicit feedback for inferring
user preference. SIGIR Forum. 37(2): 18-28.
[8] Koenemann, J. and Belkin, N.J. (1996). A case for interaction: A
study of interactive information retrieval behavior and
effectiveness. Proceedings of the ACM SIGCHI Conference on
Human Factors in Computing Systems, 205-212.
[9] Meddis, R., (1984). Statistics using ranks: A unified approach.
Oxford: Basil Blackwell, 303-308.
[10] Morita, M. and Shinoda, Y. (1994). Information filtering based
on user behavior analysis and best match text retrieval.
Proceedings of the 17th Annual ACM SIGIR Conference on
Research and Development in Information Retrieval, 272-281.
[11] Salton, G. and Buckley, C. (1990). Improving retrieval
performance by relevance feedback. Journal of the American
Society for Information Science. 41(4): 288-297.
[12] Siegel, S. and Castellan, N.J. (1988). Nonparametric statistics for
the behavioural sciences. 2nd ed. Singapore: McGraw-Hill.
[13] White, R.W. (2004). Implicit feedback for interactive information
retrieval. Unpublished Doctoral Dissertation, University of
Glasgow, Glasgow, United Kingdom.
[14] White, R.W., Jose, J.M. and Ruthven, I. (2005). An implicit
feedback approach for interactive information retrieval,
Information Processing and Management, in press.
[15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J.
(2004). A simulated study of implicit feedback models.
Proceedings of the 26th European Conference on Information
Retrieval, 311-326.
[16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W.
(2000). The impact of fluid documents on reading and browsing:
An observational study. Proceedings of the ACM SIGCHI
Conference on Human Factors in Computing Systems, 249-256.
Appendix B. Checkboxes to mark
relevant document titles in the
Explicit RF system.
Appendix A. Interface to Implicit RF system.
1. Top-Ranking Sentence 2. Title 3. Summary 4. Summary Sentence 5. Sentence in Context
2
3
4
5
1
Downloading Textual Hidden Web Content Through
Keyword Queries
Alexandros Ntoulas
UCLA Computer Science
ntoulas@cs.ucla.edu
Petros Zerfos
UCLA Computer Science
pzerfos@cs.ucla.edu
Junghoo Cho
UCLA Computer Science
cho@cs.ucla.edu
ABSTRACT
An ever-increasing amount of information on the Web today is
available only through search interfaces: the users have to type in a
set of keywords in a search form in order to access the pages from
certain Web sites. These pages are often referred to as the Hidden
Web or the Deep Web. Since there are no static links to the Hidden
Web pages, search engines cannot discover and index such pages
and thus do not return them in the results. However, according to
recent studies, the content provided by many Hidden Web sites is
often of very high quality and can be extremely valuable to many
users.
In this paper, we study how we can build an effective Hidden Web
crawler that can autonomously discover and download pages from
the Hidden Web. Since the only entry point to a Hidden Web site
is a query interface, the main challenge that a Hidden Web crawler
has to face is how to automatically generate meaningful queries to
issue to the site. Here, we provide a theoretical framework to 
investigate the query generation problem for the Hidden Web and we
propose effective policies for generating queries automatically. Our
policies proceed iteratively, issuing a different query in every 
iteration. We experimentally evaluate the effectiveness of these policies
on 4 real Hidden Web sites and our results are very promising. For
instance, in one experiment, one of our policies downloaded more
than 90% of a Hidden Web site (that contains 14 million 
documents) after issuing fewer than 100 queries.
Categories and Subject Descriptors: H.3.7 [Information Systems]:
Digital Libraries; H.3.1 [Information Systems]: Content Analysis
and Indexing; H.3.3 [Information Systems]: Information Search
and Retrieval.
General Terms: Algorithms, Performance, Design.
1. INTRODUCTION
Recent studies show that a significant fraction of Web content
cannot be reached by following links [7, 12]. In particular, a large
part of the Web is hidden behind search forms and is reachable
only when users type in a set of keywords, or queries, to the forms.
These pages are often referred to as the Hidden Web [17] or the
Deep Web [7], because search engines typically cannot index the
pages and do not return them in their results (thus, the pages are
essentially hidden from a typical Web user).
According to many studies, the size of the Hidden Web increases
rapidly as more organizations put their valuable content online
through an easy-to-use Web interface [7]. In [12], Chang et al.
estimate that well over 100,000 Hidden-Web sites currently exist
on the Web. Moreover, the content provided by many Hidden-Web
sites is often of very high quality and can be extremely valuable
to many users [7]. For example, PubMed hosts many high-quality
papers on medical research that were selected from careful 
peerreview processes, while the site of the US Patent and Trademarks
Office1
makes existing patent documents available, helping 
potential inventors examine prior art.
In this paper, we study how we can build a Hidden-Web crawler2
that can automatically download pages from the Hidden Web, so
that search engines can index them. Conventional crawlers rely
on the hyperlinks on the Web to discover pages, so current search
engines cannot index the Hidden-Web pages (due to the lack of
links). We believe that an effective Hidden-Web crawler can have
a tremendous impact on how users search information on the Web:
• Tapping into unexplored information: The Hidden-Web
crawler will allow an average Web user to easily explore the
vast amount of information that is mostly hidden at present.
Since a majority of Web users rely on search engines to discover
pages, when pages are not indexed by search engines, they are
unlikely to be viewed by many Web users. Unless users go 
directly to Hidden-Web sites and issue queries there, they cannot
access the pages at the sites.
• Improving user experience: Even if a user is aware of a 
number of Hidden-Web sites, the user still has to waste a significant
amount of time and effort, visiting all of the potentially relevant
sites, querying each of them and exploring the result. By making
the Hidden-Web pages searchable at a central location, we can
significantly reduce the user"s wasted time and effort in 
searching the Hidden Web.
• Reducing potential bias: Due to the heavy reliance of many Web
users on search engines for locating information, search engines
influence how the users perceive the Web [28]. Users do not
necessarily perceive what actually exists on the Web, but what
is indexed by search engines [28]. According to a recent 
article [5], several organizations have recognized the importance of
bringing information of their Hidden Web sites onto the surface,
and committed considerable resources towards this effort. Our
1
US Patent Office: http://www.uspto.gov
2
Crawlers are the programs that traverse the Web automatically and
download pages for search engines.
100
Figure 1: A single-attribute search interface
Hidden-Web crawler attempts to automate this process for 
Hidden Web sites with textual content, thus minimizing the 
associated costs and effort required.
Given that the only entry to Hidden Web pages is through
querying a search form, there are two core challenges to 
implementing an effective Hidden Web crawler: (a) The crawler has to
be able to understand and model a query interface, and (b) The
crawler has to come up with meaningful queries to issue to the
query interface. The first challenge was addressed by Raghavan
and Garcia-Molina in [29], where a method for learning search 
interfaces was presented. Here, we present a solution to the second
challenge, i.e. how a crawler can automatically generate queries so
that it can discover and download the Hidden Web pages.
Clearly, when the search forms list all possible values for a query
(e.g., through a drop-down list), the solution is straightforward. We
exhaustively issue all possible queries, one query at a time. When
the query forms have a free text input, however, an infinite 
number of queries are possible, so we cannot exhaustively issue all 
possible queries. In this case, what queries should we pick? Can the
crawler automatically come up with meaningful queries without
understanding the semantics of the search form?
In this paper, we provide a theoretical framework to investigate
the Hidden-Web crawling problem and propose effective ways of
generating queries automatically. We also evaluate our proposed
solutions through experiments conducted on real Hidden-Web sites.
In summary, this paper makes the following contributions:
• We present a formal framework to study the problem of 
HiddenWeb crawling. (Section 2).
• We investigate a number of crawling policies for the Hidden
Web, including the optimal policy that can potentially download
the maximum number of pages through the minimum number of
interactions. Unfortunately, we show that the optimal policy is
NP-hard and cannot be implemented in practice (Section 2.2).
• We propose a new adaptive policy that approximates the optimal
policy. Our adaptive policy examines the pages returned from
previous queries and adapts its query-selection policy 
automatically based on them (Section 3).
• We evaluate various crawling policies through experiments on
real Web sites. Our experiments will show the relative 
advantages of various crawling policies and demonstrate their 
potential. The results from our experiments are very promising. In
one experiment, for example, our adaptive policy downloaded
more than 90% of the pages within PubMed (that contains 14
million documents) after it issued fewer than 100 queries.
2. FRAMEWORK
In this section, we present a formal framework for the study of
the Hidden-Web crawling problem. In Section 2.1, we describe our
assumptions on Hidden-Web sites and explain how users interact
with the sites. Based on this interaction model, we present a 
highlevel algorithm for a Hidden-Web crawler in Section 2.2. Finally in
Section 2.3, we formalize the Hidden-Web crawling problem.
2.1 Hidden-Web database model
There exists a variety of Hidden Web sources that provide 
information on a multitude of topics. Depending on the type of 
information, we may categorize a Hidden-Web site either as a textual
database or a structured database. A textual database is a site that
Figure 2: A multi-attribute search interface
mainly contains plain-text documents, such as PubMed and 
LexisNexis (an online database of legal documents [1]). Since 
plaintext documents do not usually have well-defined structure, most
textual databases provide a simple search interface where users
type a list of keywords in a single search box (Figure 1). In 
contrast, a structured database often contains multi-attribute relational
data (e.g., a book on the Amazon Web site may have the fields
title=‘Harry Potter", author=‘J.K. Rowling" and
isbn=‘0590353403") and supports multi-attribute search 
interfaces (Figure 2). In this paper, we will mainly focus on 
textual databases that support single-attribute keyword queries. We
discuss how we can extend our ideas for the textual databases to
multi-attribute structured databases in Section 6.1.
Typically, the users need to take the following steps in order to
access pages in a Hidden-Web database:
1. Step 1. First, the user issues a query, say liver, through the
search interface provided by the Web site (such as the one shown
in Figure 1).
2. Step 2. Shortly after the user issues the query, she is presented
with a result index page. That is, the Web site returns a list of
links to potentially relevant Web pages, as shown in Figure 3(a).
3. Step 3. From the list in the result index page, the user identifies
the pages that look interesting and follows the links. Clicking
on a link leads the user to the actual Web page, such as the one
shown in Figure 3(b), that the user wants to look at.
2.2 A generic Hidden Web crawling algorithm
Given that the only entry to the pages in a Hidden-Web site
is its search from, a Hidden-Web crawler should follow the three
steps described in the previous section. That is, the crawler has
to generate a query, issue it to the Web site, download the result
index page, and follow the links to download the actual pages. In
most cases, a crawler has limited time and network resources, so
the crawler repeats these steps until it uses up its resources.
In Figure 4 we show the generic algorithm for a Hidden-Web
crawler. For simplicity, we assume that the Hidden-Web crawler
issues single-term queries only.3
The crawler first decides which
query term it is going to use (Step (2)), issues the query, and 
retrieves the result index page (Step (3)). Finally, based on the links
found on the result index page, it downloads the Hidden Web pages
from the site (Step (4)). This same process is repeated until all the
available resources are used up (Step (1)).
Given this algorithm, we can see that the most critical decision
that a crawler has to make is what query to issue next. If the
crawler can issue successful queries that will return many matching
pages, the crawler can finish its crawling early on using minimum
resources. In contrast, if the crawler issues completely irrelevant
queries that do not return any matching pages, it may waste all
of its resources simply issuing queries without ever retrieving 
actual pages. Therefore, how the crawler selects the next query can
greatly affect its effectiveness. In the next section, we formalize
this query selection problem.
3
For most Web sites that assume AND for multi-keyword
queries, single-term queries return the maximum number of results.
Extending our work to multi-keyword queries is straightforward.
101
(a) List of matching pages for query liver. (b) The first matching page for liver.
Figure 3: Pages from the PubMed Web site.
ALGORITHM 2.1. Crawling a Hidden Web site
Procedure
(1) while ( there are available resources ) do
// select a term to send to the site
(2) qi = SelectTerm()
// send query and acquire result index page
(3) R(qi) = QueryWebSite( qi )
// download the pages of interest
(4) Download( R(qi) )
(5) done
Figure 4: Algorithm for crawling a Hidden Web site.
S
q1
q
qq
2
34
Figure 5: A set-formalization of the optimal query selection
problem.
2.3 Problem formalization
Theoretically, the problem of query selection can be formalized
as follows: We assume that the crawler downloads pages from a
Web site that has a set of pages S (the rectangle in Figure 5). We
represent each Web page in S as a point (dots in Figure 5). Every
potential query qi that we may issue can be viewed as a subset of S,
containing all the points (pages) that are returned when we issue qi
to the site. Each subset is associated with a weight that represents
the cost of issuing the query. Under this formalization, our goal is to
find which subsets (queries) cover the maximum number of points
(Web pages) with the minimum total weight (cost). This problem
is equivalent to the set-covering problem in graph theory [16].
There are two main difficulties that we need to address in this
formalization. First, in a practical situation, the crawler does not
know which Web pages will be returned by which queries, so the
subsets of S are not known in advance. Without knowing these
subsets the crawler cannot decide which queries to pick to 
maximize the coverage. Second, the set-covering problem is known to
be NP-Hard [16], so an efficient algorithm to solve this problem
optimally in polynomial time has yet to be found.
In this paper, we will present an approximation algorithm that
can find a near-optimal solution at a reasonable computational cost.
Our algorithm leverages the observation that although we do not
know which pages will be returned by each query qi that we issue,
we can predict how many pages will be returned. Based on this 
information our query selection algorithm can then select the best
queries that cover the content of the Web site. We present our 
prediction method and our query selection algorithm in Section 3.
2.3.1 Performance Metric
Before we present our ideas for the query selection problem, we
briefly discuss some of our notation and the cost/performance 
metrics.
Given a query qi, we use P(qi) to denote the fraction of pages
that we will get back if we issue query qi to the site. For example, if
a Web site has 10,000 pages in total, and if 3,000 pages are returned
for the query qi = medicine, then P(qi) = 0.3. We use P(q1 ∧
q2) to represent the fraction of pages that are returned from both
q1 and q2 (i.e., the intersection of P(q1) and P(q2)). Similarly, we
use P(q1 ∨ q2) to represent the fraction of pages that are returned
from either q1 or q2 (i.e., the union of P(q1) and P(q2)).
We also use Cost(qi) to represent the cost of issuing the query
qi. Depending on the scenario, the cost can be measured either in
time, network bandwidth, the number of interactions with the site,
or it can be a function of all of these. As we will see later, our
proposed algorithms are independent of the exact cost function.
In the most common case, the query cost consists of a number
of factors, including the cost for submitting the query to the site,
retrieving the result index page (Figure 3(a)) and downloading the
actual pages (Figure 3(b)). We assume that submitting a query 
incurs a fixed cost of cq. The cost for downloading the result index
page is proportional to the number of matching documents to the
query, while the cost cd for downloading a matching document is
also fixed. Then the overall cost of query qi is
Cost(qi) = cq + crP(qi) + cdP(qi). (1)
In certain cases, some of the documents from qi may have already
been downloaded from previous queries. In this case, the crawler
may skip downloading these documents and the cost of qi can be
Cost(qi) = cq + crP(qi) + cdPnew(qi). (2)
Here, we use Pnew(qi) to represent the fraction of the new 
documents from qi that have not been retrieved from previous queries.
Later in Section 3.1 we will study how we can estimate P(qi) and
Pnew(qi) to estimate the cost of qi.
Since our algorithms are independent of the exact cost function,
we will assume a generic cost function Cost(qi) in this paper. When
we need a concrete cost function, however, we will use Equation 2.
Given the notation, we can formalize the goal of a Hidden-Web
crawler as follows:
102
PROBLEM 1. Find the set of queries q1, . . . , qn that maximizes
P(q1 ∨ · · · ∨ qn)
under the constraint
n
i=1
Cost(qi) ≤ t.
Here, t is the maximum download resource that the crawler has.
3. KEYWORD SELECTION
How should a crawler select the queries to issue? Given that the
goal is to download the maximum number of unique documents
from a textual database, we may consider one of the following 
options:
• Random: We select random keywords from, say, an English 
dictionary and issue them to the database. The hope is that a random
query will return a reasonable number of matching documents.
• Generic-frequency: We analyze a generic document corpus 
collected elsewhere (say, from the Web) and obtain the generic 
frequency distribution of each keyword. Based on this generic 
distribution, we start with the most frequent keyword, issue it to the
Hidden-Web database and retrieve the result. We then continue
to the second-most frequent keyword and repeat this process 
until we exhaust all download resources. The hope is that the 
frequent keywords in a generic corpus will also be frequent in the
Hidden-Web database, returning many matching documents.
• Adaptive: We analyze the documents returned from the previous
queries issued to the Hidden-Web database and estimate which
keyword is most likely to return the most documents. Based on
this analysis, we issue the most promising query, and repeat
the process.
Among these three general policies, we may consider the 
random policy as the base comparison point since it is expected to
perform the worst. Between the generic-frequency and the 
adaptive policies, both policies may show similar performance if the
crawled database has a generic document collection without a 
specialized topic. The adaptive policy, however, may perform 
significantly better than the generic-frequency policy if the database has a
very specialized collection that is different from the generic corpus.
We will experimentally compare these three policies in Section 4.
While the first two policies (random and generic-frequency 
policies) are easy to implement, we need to understand how we can 
analyze the downloaded pages to identify the most promising query
in order to implement the adaptive policy. We address this issue in
the rest of this section.
3.1 Estimating the number of matching pages
In order to identify the most promising query, we need to 
estimate how many new documents we will download if we issue the
query qi as the next query. That is, assuming that we have issued
queries q1, . . . , qi−1 we need to estimate P(q1∨· · ·∨qi−1∨qi), for
every potential next query qi and compare this value. In estimating
this number, we note that we can rewrite P(q1 ∨ · · · ∨ qi−1 ∨ qi)
as:
P((q1 ∨ · · · ∨ qi−1) ∨ qi)
= P(q1 ∨ · · · ∨ qi−1) + P(qi) − P((q1 ∨ · · · ∨ qi−1) ∧ qi)
= P(q1 ∨ · · · ∨ qi−1) + P(qi)
− P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1) (3)
In the above formula, note that we can precisely measure P(q1 ∨
· · · ∨ qi−1) and P(qi | q1 ∨ · · · ∨ qi−1) by analyzing 
previouslydownloaded pages: We know P(q1 ∨ · · · ∨ qi−1), the union of
all pages downloaded from q1, . . . , qi−1, since we have already 
issued q1, . . . , qi−1 and downloaded the matching pages.4
We can
also measure P(qi | q1 ∨ · · · ∨ qi−1), the probability that qi 
appears in the pages from q1, . . . , qi−1, by counting how many times
qi appears in the pages from q1, . . . , qi−1. Therefore, we only need
to estimate P(qi) to evaluate P(q1 ∨ · · · ∨ qi). We may consider a
number of different ways to estimate P(qi), including the 
following:
1. Independence estimator: We assume that the appearance of the
term qi is independent of the terms q1, . . . , qi−1. That is, we
assume that P(qi) = P(qi|q1 ∨ · · · ∨ qi−1).
2. Zipf estimator: In [19], Ipeirotis et al. proposed a method to
estimate how many times a particular term occurs in the entire
corpus based on a subset of documents from the corpus. Their
method exploits the fact that the frequency of terms inside text
collections follows a power law distribution [30, 25]. That is,
if we rank all terms based on their occurrence frequency (with
the most frequent term having a rank of 1, second most frequent
a rank of 2 etc.), then the frequency f of a term inside the text
collection is given by:
f = α(r + β)−γ
(4)
where r is the rank of the term and α, β, and γ are constants that
depend on the text collection.
Their main idea is (1) to estimate the three parameters, α, β and
γ, based on the subset of documents that we have downloaded
from previous queries, and (2) use the estimated parameters to
predict f given the ranking r of a term within the subset. For
a more detailed description on how we can use this method to
estimate P(qi), we refer the reader to the extended version of
this paper [27].
After we estimate P(qi) and P(qi|q1 ∨ · · · ∨ qi−1) values, we
can calculate P(q1 ∨ · · · ∨ qi). In Section 3.3, we explain how
we can efficiently compute P(qi|q1 ∨ · · · ∨ qi−1) by maintaining a
succinct summary table. In the next section, we first examine how
we can use this value to decide which query we should issue next
to the Hidden Web site.
3.2 Query selection algorithm
The goal of the Hidden-Web crawler is to download the 
maximum number of unique documents from a database using its 
limited download resources. Given this goal, the Hidden-Web crawler
has to take two factors into account. (1) the number of new 
documents that can be obtained from the query qi and (2) the cost of
issuing the query qi. For example, if two queries, qi and qj, incur
the same cost, but qi returns more new pages than qj, qi is more
desirable than qj. Similarly, if qi and qj return the same number
of new documents, but qi incurs less cost then qj, qi is more 
desirable. Based on this observation, the Hidden-Web crawler may
use the following efficiency metric to quantify the desirability of
the query qi:
Efficiency(qi) =
Pnew(qi)
Cost(qi)
Here, Pnew(qi) represents the amount of new documents returned
for qi (the pages that have not been returned for previous queries).
Cost(qi) represents the cost of issuing the query qi.
Intuitively, the efficiency of qi measures how many new 
documents are retrieved per unit cost, and can be used as an indicator of
4
For exact estimation, we need to know the total number of pages in
the site. However, in order to compare only relative values among
queries, this information is not actually needed.
103
ALGORITHM 3.1. Greedy SelectTerm()
Parameters:
T: The list of potential query keywords
Procedure
(1) Foreach tk in T do
(2) Estimate Efficiency(tk) = Pnew(tk)
Cost(tk)
(3) done
(4) return tk with maximum Efficiency(tk)
Figure 6: Algorithm for selecting the next query term.
how well our resources are spent when issuing qi. Thus, the 
Hidden Web crawler can estimate the efficiency of every candidate qi,
and select the one with the highest value. By using its resources
more efficiently, the crawler may eventually download the 
maximum number of unique documents. In Figure 6, we show the query
selection function that uses the concept of efficiency. In principle,
this algorithm takes a greedy approach and tries to maximize the
potential gain in every step.
We can estimate the efficiency of every query using the 
estimation method described in Section 3.1. That is, the size of the new
documents from the query qi, Pnew(qi), is
Pnew(qi)
= P(q1 ∨ · · · ∨ qi−1 ∨ qi) − P(q1 ∨ · · · ∨ qi−1)
= P(qi) − P(q1 ∨ · · · ∨ qi−1)P(qi|q1 ∨ · · · ∨ qi−1)
from Equation 3, where P(qi) can be estimated using one of the
methods described in section 3. We can also estimate Cost(qi) 
similarly. For example, if Cost(qi) is
Cost(qi) = cq + crP(qi) + cdPnew(qi)
(Equation 2), we can estimate Cost(qi) by estimating P(qi) and
Pnew(qi).
3.3 Efficient calculation of query statistics
In estimating the efficiency of queries, we found that we need to
measure P(qi|q1∨· · ·∨qi−1) for every potential query qi. This 
calculation can be very time-consuming if we repeat it from scratch for
every query qi in every iteration of our algorithm. In this section,
we explain how we can compute P(qi|q1 ∨ · · · ∨ qi−1) efficiently
by maintaining a small table that we call a query statistics table.
The main idea for the query statistics table is that P(qi|q1 ∨· · ·∨
qi−1) can be measured by counting how many times the keyword
qi appears within the documents downloaded from q1, . . . , qi−1.
We record these counts in a table, as shown in Figure 7(a). The
left column of the table contains all potential query terms and the
right column contains the number of previously-downloaded 
documents containing the respective term. For example, the table in 
Figure 7(a) shows that we have downloaded 50 documents so far, and
the term model appears in 10 of these documents. Given this 
number, we can compute that P(model|q1 ∨ · · · ∨ qi−1) = 10
50
= 0.2.
We note that the query statistics table needs to be updated 
whenever we issue a new query qi and download more documents. This
update can be done efficiently as we illustrate in the following 
example.
EXAMPLE 1. After examining the query statistics table of 
Figure 7(a), we have decided to use the term computer as our next
query qi. From the new query qi = computer, we downloaded
20 more new pages. Out of these, 12 contain the keyword model
Term tk N(tk)
model 10
computer 38
digital 50
Term tk N(tk)
model 12
computer 20
disk 18
Total pages: 50 New pages: 20
(a) After q1, . . . , qi−1 (b) New from qi = computer
Term tk N(tk)
model 10+12 = 22
computer 38+20 = 58
disk 0+18 = 18
digital 50+0 = 50
Total pages: 50 + 20 = 70
(c) After q1, . . . , qi
Figure 7: Updating the query statistics table.
q
i1 i−1
q\/ ... \/q
q
i
/
S
Figure 8: A Web site that does not return all the results.
and 18 the keyword disk. The table in Figure 7(b) shows the
frequency of each term in the newly-downloaded pages.
We can update the old table (Figure 7(a)) to include this new
information by simply adding corresponding entries in Figures 7(a)
and (b). The result is shown on Figure 7(c). For example, keyword
model exists in 10 + 12 = 22 pages within the pages retrieved
from q1, . . . , qi. According to this new table, P(model|q1∨· · ·∨qi)
is now 22
70
= 0.3.
3.4 Crawling sites that limit the number of
results
In certain cases, when a query matches a large number of pages,
the Hidden Web site returns only a portion of those pages. For 
example, the Open Directory Project [2] allows the users to see only
up to 10, 000 results after they issue a query. Obviously, this kind
of limitation has an immediate effect on our Hidden Web crawler.
First, since we can only retrieve up to a specific number of pages
per query, our crawler will need to issue more queries (and 
potentially will use up more resources) in order to download all the
pages. Second, the query selection method that we presented in
Section 3.2 assumes that for every potential query qi, we can find
P(qi|q1 ∨ · · · ∨ qi−1). That is, for every query qi we can find the
fraction of documents in the whole text database that contains qi
with at least one of q1, . . . , qi−1. However, if the text database 
returned only a portion of the results for any of the q1, . . . , qi−1 then
the value P(qi|q1 ∨ · · · ∨ qi−1) is not accurate and may affect our
decision for the next query qi, and potentially the performance of
our crawler. Since we cannot retrieve more results per query than
the maximum number the Web site allows, our crawler has no other
choice besides submitting more queries. However, there is a way
to estimate the correct value for P(qi|q1 ∨ · · · ∨ qi−1) in the case
where the Web site returns only a portion of the results.
104
Again, assume that the Hidden Web site we are currently 
crawling is represented as the rectangle on Figure 8 and its pages as
points in the figure. Assume that we have already issued queries
q1, . . . , qi−1 which returned a number of results less than the 
maximum number than the site allows, and therefore we have 
downloaded all the pages for these queries (big circle in Figure 8). That
is, at this point, our estimation for P(qi|q1 ∨· · ·∨qi−1) is accurate.
Now assume that we submit query qi to the Web site, but due to a
limitation in the number of results that we get back, we retrieve the
set qi (small circle in Figure 8) instead of the set qi (dashed circle
in Figure 8). Now we need to update our query statistics table so
that it has accurate information for the next step. That is, although
we got the set qi back, for every potential query qi+1 we need to
find P(qi+1|q1 ∨ · · · ∨ qi):
P(qi+1|q1 ∨ · · · ∨ qi)
=
1
P(q1 ∨ · · · ∨ qi)
· [P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1))+
P(qi+1 ∧ qi) − P(qi+1 ∧ qi ∧ (q1 ∨ · · · ∨ qi−1))] (5)
In the previous equation, we can find P(q1 ∨· · ·∨qi) by 
estimating P(qi) with the method shown in Section 3. Additionally, we
can calculate P(qi+1 ∧ (q1 ∨ · · · ∨ qi−1)) and P(qi+1 ∧ qi ∧ (q1 ∨
· · · ∨ qi−1)) by directly examining the documents that we have
downloaded from queries q1, . . . , qi−1. The term P(qi+1 ∧ qi)
however is unknown and we need to estimate it. Assuming that qi
is a random sample of qi, then:
P(qi+1 ∧ qi)
P(qi+1 ∧ qi)
=
P(qi)
P(qi)
(6)
From Equation 6 we can calculate P(qi+1 ∧ qi) and after we
replace this value to Equation 5 we can find P(qi+1|q1 ∨ · · · ∨ qi).
4. EXPERIMENTAL EVALUATION
In this section we experimentally evaluate the performance of
the various algorithms for Hidden Web crawling presented in this
paper. Our goal is to validate our theoretical analysis through 
realworld experiments, by crawling popular Hidden Web sites of 
textual databases. Since the number of documents that are discovered
and downloaded from a textual database depends on the selection
of the words that will be issued as queries5
to the search interface
of each site, we compare the various selection policies that were
described in section 3, namely the random, generic-frequency, and
adaptive algorithms.
The adaptive algorithm learns new keywords and terms from the
documents that it downloads, and its selection process is driven by
a cost model as described in Section 3.2. To keep our experiment
and its analysis simple at this point, we will assume that the cost for
every query is constant. That is, our goal is to maximize the number
of downloaded pages by issuing the least number of queries. Later,
in Section 4.4 we will present a comparison of our policies based
on a more elaborate cost model. In addition, we use the 
independence estimator (Section 3.1) to estimate P(qi) from downloaded
pages. Although the independence estimator is a simple estimator,
our experiments will show that it can work very well in practice.6
For the generic-frequency policy, we compute the frequency 
distribution of words that appear in a 5.5-million-Web-page corpus
5
Throughout our experiments, once an algorithm has submitted a
query to a database, we exclude the query from subsequent 
submissions to the same database from the same algorithm.
6
We defer the reporting of results based on the Zipf estimation to a
future work.
downloaded from 154 Web sites of various topics [26]. Keywords
are selected based on their decreasing frequency with which they
appear in this document set, with the most frequent one being 
selected first, followed by the second-most frequent keyword, etc.7
Regarding the random policy, we use the same set of words 
collected from the Web corpus, but in this case, instead of selecting
keywords based on their relative frequency, we choose them 
randomly (uniform distribution). In order to further investigate how
the quality of the potential query-term list affects the random-based
algorithm, we construct two sets: one with the 16, 000 most 
frequent words of the term collection used in the generic-frequency
policy (hereafter, the random policy with the set of 16,000 words
will be referred to as random-16K), and another set with the 1 
million most frequent words of the same collection as above (hereafter,
referred to as random-1M). The former set has frequent words that
appear in a large number of documents (at least 10, 000 in our 
collection), and therefore can be considered of high-quality terms.
The latter set though contains a much larger collection of words,
among which some might be bogus, and meaningless.
The experiments were conducted by employing each one of the
aforementioned algorithms (adaptive, generic-frequency, 
random16K, and random-1M) to crawl and download contents from three
Hidden Web sites: The PubMed Medical Library,8
Amazon,9
and
the Open Directory Project[2]. According to the information on
PubMed"s Web site, its collection contains approximately 14 
million abstracts of biomedical articles. We consider these abstracts
as the documents in the site, and in each iteration of the adaptive
policy, we use these abstracts as input to the algorithm. Thus our
goal is to discover as many unique abstracts as possible by 
repeatedly querying the Web query interface provided by PubMed. The
Hidden Web crawling on the PubMed Web site can be considered
as topic-specific, due to the fact that all abstracts within PubMed
are related to the fields of medicine and biology.
In the case of the Amazon Web site, we are interested in 
downloading all the hidden pages that contain information on books.
The querying to Amazon is performed through the Software 
Developer"s Kit that Amazon provides for interfacing to its Web site,
and which returns results in XML form. The generic keyword
field is used for searching, and as input to the adaptive policy we
extract the product description and the text of customer reviews
when present in the XML reply. Since Amazon does not provide
any information on how many books it has in its catalogue, we use
random sampling on the 10-digit ISBN number of the books to 
estimate the size of the collection. Out of the 10, 000 random ISBN
numbers queried, 46 are found in the Amazon catalogue, therefore
the size of its book collection is estimated to be 46
10000
· 1010
= 4.6
million books. It"s also worth noting here that Amazon poses an
upper limit on the number of results (books in our case) returned
by each query, which is set to 32, 000.
As for the third Hidden Web site, the Open Directory Project
(hereafter also referred to as dmoz), the site maintains the links to
3.8 million sites together with a brief summary of each listed site.
The links are searchable through a keyword-search interface. We
consider each indexed link together with its brief summary as the
document of the dmoz site, and we provide the short summaries
to the adaptive algorithm to drive the selection of new keywords
for querying. On the dmoz Web site, we perform two Hidden Web
crawls: the first is on its generic collection of 3.8-million indexed
7
We did not manually exclude stop words (e.g., the, is, of, etc.)
from the keyword list. As it turns out, all Web sites except PubMed
return matching documents for the stop words, such as the.
8
PubMed Medical Library: http://www.pubmed.org
9
Amazon Inc.: http://www.amazon.com
105
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 50 100 150 200
fractionofdocuments
query number
Cumulative fraction of unique documents - PubMed website
adaptive
generic-frequency
random-16K
random-1M
Figure 9: Coverage of policies for Pubmed
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 100 200 300 400 500 600 700
fractionofdocuments
query number
Cumulative fraction of unique documents - Amazon website
adaptive
generic-frequency
random-16K
random-1M
Figure 10: Coverage of policies for Amazon
sites, regardless of the category that they fall into. The other crawl
is performed specifically on the Arts section of dmoz (http://
dmoz.org/Arts), which comprises of approximately 429, 000
indexed sites that are relevant to Arts, making this crawl 
topicspecific, as in PubMed. Like Amazon, dmoz also enforces an upper
limit on the number of returned results, which is 10, 000 links with
their summaries.
4.1 Comparison of policies
The first question that we seek to answer is the evolution of the
coverage metric as we submit queries to the sites. That is, what
fraction of the collection of documents stored in the Hidden Web
site can we download as we continuously query for new words 
selected using the policies described above? More formally, we are
interested in the value of P(q1 ∨ · · · ∨ qi−1 ∨ qi), after we submit
q1, . . . , qi queries, and as i increases.
In Figures 9, 10, 11, and 12 we present the coverage metric for
each policy, as a function of the query number, for the Web sites
of PubMed, Amazon, general dmoz and the art-specific dmoz, 
respectively. On the y-axis the fraction of the total documents 
downloaded from the website is plotted, while the x-axis represents the
query number. A first observation from these graphs is that in 
general, the generic-frequency and the adaptive policies perform much
better than the random-based algorithms. In all of the figures, the
graphs for the random-1M and the random-16K are significantly
below those of other policies.
Between the generic-frequency and the adaptive policies, we can
see that the latter outperforms the former when the site is topic 
spe0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 100 200 300 400 500 600 700
fractionofdocuments
query number
Cumulative fraction of unique documents - dmoz website
adaptive
generic-frequency
random-16K
random-1M
Figure 11: Coverage of policies for general dmoz
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 50 100 150 200 250 300 350 400 450
fractionofdocuments
query number
Cumulative fraction of unique documents - dmoz/Arts website
adaptive
generic-frequency
random-16K
random-1M
Figure 12: Coverage of policies for the Arts section of dmoz
cific. For example, for the PubMed site (Figure 9), the adaptive
algorithm issues only 83 queries to download almost 80% of the
documents stored in PubMed, but the generic-frequency algorithm
requires 106 queries for the same coverage,. For the dmoz/Arts
crawl (Figure 12), the difference is even more substantial: the 
adaptive policy is able to download 99.98% of the total sites indexed in
the Directory by issuing 471 queries, while the frequency-based 
algorithm is much less effective using the same number of queries,
and discovers only 72% of the total number of indexed sites. The
adaptive algorithm, by examining the contents of the pages that it
downloads at each iteration, is able to identify the topic of the site as
expressed by the words that appear most frequently in the result-set.
Consequently, it is able to select words for subsequent queries that
are more relevant to the site, than those preferred by the 
genericfrequency policy, which are drawn from a large, generic collection.
Table 1 shows a sample of 10 keywords out of 211 chosen and 
submitted to the PubMed Web site by the adaptive algorithm, but not
by the other policies. For each keyword, we present the number of
the iteration, along with the number of results that it returned. As
one can see from the table, these keywords are highly relevant to
the topics of medicine and biology of the Public Medical Library,
and match against numerous articles stored in its Web site.
In both cases examined in Figures 9, and 12, the random-based
policies perform much worse than the adaptive algorithm, and the
generic-frequency. It is worthy noting however, that the 
randombased policy with the small, carefully selected set of 16, 000 
quality words manages to download a considerable fraction of 42.5%
106
Iteration Keyword Number of Results
23 department 2, 719, 031
34 patients 1, 934, 428
53 clinical 1, 198, 322
67 treatment 4, 034, 565
69 medical 1, 368, 200
70 hospital 503, 307
146 disease 1, 520, 908
172 protein 2, 620, 938
Table 1: Sample of keywords queried to PubMed exclusively by
the adaptive policy
from the PubMed Web site after 200 queries, while the coverage
for the Arts section of dmoz reaches 22.7%, after 471 queried 
keywords. On the other hand, the random-based approach that makes
use of the vast collection of 1 million words, among which a large
number is bogus keywords, fails to download even a mere 1% of the
total collection, after submitting the same number of query words.
For the generic collections of Amazon and the dmoz sites, shown
in Figures 10 and 11 respectively, we get mixed results: The 
genericfrequency policy shows slightly better performance than the 
adaptive policy for the Amazon site (Figure 10), and the adaptive method
clearly outperforms the generic-frequency for the general dmoz site
(Figure 11). A closer look at the log files of the two Hidden Web
crawlers reveals the main reason: Amazon was functioning in a
very flaky way when the adaptive crawler visited it, resulting in
a large number of lost results. Thus, we suspect that the slightly
poor performance of the adaptive policy is due to this 
experimental variance. We are currently running another experiment to 
verify whether this is indeed the case. Aside from this experimental
variance, the Amazon result indicates that if the collection and the
words that a Hidden Web site contains are generic enough, then the
generic-frequency approach may be a good candidate algorithm for
effective crawling.
As in the case of topic-specific Hidden Web sites, the 
randombased policies also exhibit poor performance compared to the other
two algorithms when crawling generic sites: for the Amazon Web
site, random-16K succeeds in downloading almost 36.7% after 
issuing 775 queries, alas for the generic collection of dmoz, the 
fraction of the collection of links downloaded is 13.5% after the 770th
query. Finally, as expected, random-1M is even worse than 
random16K, downloading only 14.5% of Amazon and 0.3% of the generic
dmoz.
In summary, the adaptive algorithm performs remarkably well in
all cases: it is able to discover and download most of the documents
stored in Hidden Web sites by issuing the least number of queries.
When the collection refers to a specific topic, it is able to identify
the keywords most relevant to the topic of the site and consequently
ask for terms that is most likely that will return a large number of
results . On the other hand, the generic-frequency policy proves to
be quite effective too, though less than the adaptive: it is able to 
retrieve relatively fast a large portion of the collection, and when the
site is not topic-specific, its effectiveness can reach that of 
adaptive (e.g. Amazon). Finally, the random policy performs poorly in
general, and should not be preferred.
4.2 Impact of the initial query
An interesting issue that deserves further examination is whether
the initial choice of the keyword used as the first query issued by
the adaptive algorithm affects its effectiveness in subsequent 
iterations. The choice of this keyword is not done by the selection of the
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 10 20 30 40 50 60
fractionofdocuments
query number
Convergence of adaptive under different initial queries - PubMed website
pubmed
data
information
return
Figure 13: Convergence of the adaptive algorithm using 
different initial queries for crawling the PubMed Web site
adaptive algorithm itself and has to be manually set, since its query
statistics tables have not been populated yet. Thus, the selection is
generally arbitrary, so for purposes of fully automating the whole
process, some additional investigation seems necessary.
For this reason, we initiated three adaptive Hidden Web crawlers
targeting the PubMed Web site with different seed-words: the word
data, which returns 1,344,999 results, the word information
that reports 308, 474 documents, and the word return that 
retrieves 29, 707 pages, out of 14 million. These keywords 
represent varying degrees of term popularity in PubMed, with the first
one being of high popularity, the second of medium, and the third
of low. We also show results for the keyword pubmed, used in
the experiments for coverage of Section 4.1, and which returns 695
articles. As we can see from Figure 13, after a small number of
queries, all four crawlers roughly download the same fraction of
the collection, regardless of their starting point: Their coverages
are roughly equivalent from the 25th query. Eventually, all four
crawlers use the same set of terms for their queries, regardless of
the initial query. In the specific experiment, from the 36th query 
onward, all four crawlers use the same terms for their queries in each
iteration, or the same terms are used off by one or two query 
numbers. Our result confirms the observation of [11] that the choice of
the initial query has minimal effect on the final performance. We
can explain this intuitively as follows: Our algorithm approximates
the optimal set of queries to use for a particular Web site. Once
the algorithm has issued a significant number of queries, it has an
accurate estimation of the content of the Web site, regardless of
the initial query. Since this estimation is similar for all runs of the
algorithm, the crawlers will use roughly the same queries.
4.3 Impact of the limit in the number of results
While the Amazon and dmoz sites have the respective limit of
32,000 and 10,000 in their result sizes, these limits may be larger
than those imposed by other Hidden Web sites. In order to 
investigate how a tighter limit in the result size affects the 
performance of our algorithms, we performed two additional crawls to
the generic-dmoz site: we ran the generic-frequency and adaptive
policies but we retrieved only up to the top 1,000 results for 
every query. In Figure 14 we plot the coverage for the two policies
as a function of the number of queries. As one might expect, by
comparing the new result in Figure 14 to that of Figure 11 where
the result limit was 10,000, we conclude that the tighter limit 
requires a higher number of queries to achieve the same coverage.
For example, when the result limit was 10,000, the adaptive 
pol107
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 500 1000 1500 2000 2500 3000 3500
FractionofUniquePages
Query Number
Cumulative fraction of unique pages downloaded per query - Dmoz Web site (cap limit 1000)
adaptive
generic-frequency
Figure 14: Coverage of general dmoz after limiting the number
of results to 1,000
icy could download 70% of the site after issuing 630 queries, while
it had to issue 2,600 queries to download 70% of the site when
the limit was 1,000. On the other hand, our new result shows that
even with a tight result limit, it is still possible to download most
of a Hidden Web site after issuing a reasonable number of queries.
The adaptive policy could download more than 85% of the site 
after issuing 3,500 queries when the limit was 1,000. Finally, our
result shows that our adaptive policy consistently outperforms the
generic-frequency policy regardless of the result limit. In both 
Figure 14 and Figure 11, our adaptive policy shows significantly larger
coverage than the generic-frequency policy for the same number of
queries.
4.4 Incorporating the document download
cost
For brevity of presentation, the performance evaluation results
provided so far assumed a simplified cost-model where every query
involved a constant cost. In this section we present results regarding
the performance of the adaptive and generic-frequency algorithms
using Equation 2 to drive our query selection process. As we 
discussed in Section 2.3.1, this query cost model includes the cost for
submitting the query to the site, retrieving the result index page,
and also downloading the actual pages. For these costs, we 
examined the size of every result in the index page and the sizes of the
documents, and we chose cq = 100, cr = 100, and cd = 10000,
as values for the parameters of Equation 2, and for the particular
experiment that we ran on the PubMed website. The values that
we selected imply that the cost for issuing one query and retrieving
one result from the result index page are roughly the same, while
the cost for downloading an actual page is 100 times larger. We
believe that these values are reasonable for the PubMed Web site.
Figure 15 shows the coverage of the adaptive and 
genericfrequency algorithms as a function of the resource units used 
during the download process. The horizontal axis is the amount of
resources used, and the vertical axis is the coverage. As it is 
evident from the graph, the adaptive policy makes more efficient use of
the available resources, as it is able to download more articles than
the generic-frequency, using the same amount of resource units.
However, the difference in coverage is less dramatic in this case,
compared to the graph of Figure 9. The smaller difference is due
to the fact that under the current cost metric, the download cost of
documents constitutes a significant portion of the cost. Therefore,
when both policies downloaded the same number of documents,
the saving of the adaptive policy is not as dramatic as before. That
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 5000 10000 15000 20000 25000 30000
FractionofUniquePages
Total Cost (cq=100, cr=100, cd=10000)
Cumulative fraction of unique pages downloaded per cost unit - PubMed Web site
adaptive
frequency
Figure 15: Coverage of PubMed after incorporating the 
document download cost
is, the savings in the query cost and the result index download cost
is only a relatively small portion of the overall cost. Still, we 
observe noticeable savings from the adaptive policy. At the total cost
of 8000, for example, the coverage of the adaptive policy is roughly
0.5 while the coverage of the frequency policy is only 0.3.
5. RELATED WORK
In a recent study, Raghavan and Garcia-Molina [29] present an
architectural model for a Hidden Web crawler. The main focus of
this work is to learn Hidden-Web query interfaces, not to 
generate queries automatically. The potential queries are either provided
manually by users or collected from the query interfaces. In 
contrast, our main focus is to generate queries automatically without
any human intervention.
The idea of automatically issuing queries to a database and 
examining the results has been previously used in different contexts.
For example, in [10, 11], Callan and Connel try to acquire an 
accurate language model by collecting a uniform random sample from
the database. In [22] Lawrence and Giles issue random queries to
a number of Web Search Engines in order to estimate the fraction
of the Web that has been indexed by each of them. In a similar
fashion, Bharat and Broder [8] issue random queries to a set of
Search Engines in order to estimate the relative size and overlap of
their indexes. In [6], Barbosa and Freire experimentally evaluate
methods for building multi-keyword queries that can return a large
fraction of a document collection. Our work differs from the 
previous studies in two ways. First, it provides a theoretical framework
for analyzing the process of generating queries for a database and
examining the results, which can help us better understand the 
effectiveness of the methods presented in the previous work. Second,
we apply our framework to the problem of Hidden Web crawling
and demonstrate the efficiency of our algorithms.
Cope et al. [15] propose a method to automatically detect whether
a particular Web page contains a search form. This work is 
complementary to ours; once we detect search interfaces on the Web
using the method in [15], we may use our proposed algorithms to
download pages automatically from those Web sites.
Reference [4] reports methods to estimate what fraction of a
text database can be eventually acquired by issuing queries to the
database. In [3] the authors study query-based techniques that can
extract relational data from large text databases. Again, these works
study orthogonal issues and are complementary to our work.
In order to make documents in multiple textual databases 
searchable at a central place, a number of harvesting approaches have
108
been proposed (e.g., OAI [21], DP9 [24]). These approaches 
essentially assume cooperative document databases that willingly share
some of their metadata and/or documents to help a third-party search
engine to index the documents. Our approach assumes 
uncooperative databases that do not share their data publicly and whose
documents are accessible only through search interfaces.
There exists a large body of work studying how to identify the
most relevant database given a user query [20, 19, 14, 23, 18]. This
body of work is often referred to as meta-searching or database
selection problem over the Hidden Web. For example, [19] 
suggests the use of focused probing to classify databases into a topical
category, so that given a query, a relevant database can be selected
based on its topical category. Our vision is different from this body
of work in that we intend to download and index the Hidden pages
at a central location in advance, so that users can access all the
information at their convenience from one single location.
6. CONCLUSION AND FUTURE WORK
Traditional crawlers normally follow links on the Web to 
discover and download pages. Therefore they cannot get to the Hidden
Web pages which are only accessible through query interfaces. In
this paper, we studied how we can build a Hidden Web crawler that
can automatically query a Hidden Web site and download pages
from it. We proposed three different query generation policies for
the Hidden Web: a policy that picks queries at random from a list
of keywords, a policy that picks queries based on their frequency
in a generic text collection, and a policy which adaptively picks a
good query based on the content of the pages downloaded from the
Hidden Web site. Experimental evaluation on 4 real Hidden Web
sites shows that our policies have a great potential. In particular, in
certain cases the adaptive policy can download more than 90% of
a Hidden Web site after issuing approximately 100 queries. Given
these results, we believe that our work provides a potential 
mechanism to improve the search-engine coverage of the Web and the
user experience of Web search.
6.1 Future Work
We briefly discuss some future-research avenues.
Multi-attribute Databases We are currently investigating how
to extend our ideas to structured multi-attribute databases. While
generating queries for multi-attribute databases is clearly a more
difficult problem, we may exploit the following observation to 
address this problem: When a site supports multi-attribute queries,
the site often returns pages that contain values for each of the query
attributes. For example, when an online bookstore supports queries
on title, author and isbn, the pages returned from a query
typically contain the title, author and ISBN of corresponding books.
Thus, if we can analyze the returned pages and extract the values
for each field (e.g, title = ‘Harry Potter", author =
‘J.K. Rowling", etc), we can apply the same idea that we
used for the textual database: estimate the frequency of each 
attribute value and pick the most promising one. The main challenge
is to automatically segment the returned pages so that we can 
identify the sections of the pages that present the values corresponding
to each attribute. Since many Web sites follow limited formatting
styles in presenting multiple attributes - for example, most book
titles are preceded by the label Title: - we believe we may learn
page-segmentation rules automatically from a small set of training
examples.
Other Practical Issues In addition to the automatic query 
generation problem, there are many practical issues to be addressed
to build a fully automatic Hidden-Web crawler. For example, in
this paper we assumed that the crawler already knows all query 
interfaces for Hidden-Web sites. But how can the crawler discover
the query interfaces? The method proposed in [15] may be a good
starting point. In addition, some Hidden-Web sites return their 
results in batches of, say, 20 pages, so the user has to click on a
next button in order to see more results. In this case, a fully 
automatic Hidden-Web crawler should know that the first result index
page contains only a partial result and press the next button 
automatically. Finally, some Hidden Web sites may contain an infinite
number of Hidden Web pages which do not contribute much 
significant content (e.g. a calendar with links for every day). In this
case the Hidden-Web crawler should be able to detect that the site
does not have much more new content and stop downloading pages
from the site. Page similarity detection algorithms may be useful
for this purpose [9, 13].
7. REFERENCES
[1] Lexisnexis http://www.lexisnexis.com.
[2] The Open Directory Project, http://www.dmoz.org.
[3] E. Agichtein and L. Gravano. Querying text databases for efficient information
extraction. In ICDE, 2003.
[4] E. Agichtein, P. Ipeirotis, and L. Gravano. Modeling query-based access to text
databases. In WebDB, 2003.
[5] Article on New York Times. Old Search Engine, the Library, Tries to Fit Into a
Google World. Available at: http:
//www.nytimes.com/2004/06/21/technology/21LIBR.html,
June 2004.
[6] L. Barbosa and J. Freire. Siphoning hidden-web data through keyword-based
interfaces. In SBBD, 2004.
[7] M. K. Bergman. The deep web: Surfacing hidden value,http:
//www.press.umich.edu/jep/07-01/bergman.html.
[8] K. Bharat and A. Broder. A technique for measuring the relative size and
overlap of public web search engines. In WWW, 1998.
[9] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. Syntactic
clustering of the web. In WWW, 1997.
[10] J. Callan, M. Connell, and A. Du. Automatic discovery of language models for
text databases. In SIGMOD, 1999.
[11] J. P. Callan and M. E. Connell. Query-based sampling of text databases.
Information Systems, 19(2):97-130, 2001.
[12] K. C.-C. Chang, B. He, C. Li, and Z. Zhang. Structured databases on the web:
Observations and implications. Technical report, UIUC.
[13] J. Cho, N. Shivakumar, and H. Garcia-Molina. Finding replicated web
collections. In SIGMOD, 2000.
[14] W. Cohen and Y. Singer. Learning to query the web. In AAAI Workshop on
Internet-Based Information Systems, 1996.
[15] J. Cope, N. Craswell, and D. Hawking. Automated discovery of search
interfaces on the web. In 14th Australasian conference on Database
technologies, 2003.
[16] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms,
2nd Edition. MIT Press/McGraw Hill, 2001.
[17] D. Florescu, A. Y. Levy, and A. O. Mendelzon. Database techniques for the
world-wide web: A survey. SIGMOD Record, 27(3):59-74, 1998.
[18] B. He and K. C.-C. Chang. Statistical schema matching across web query
interfaces. In SIGMOD Conference, 2003.
[19] P. Ipeirotis and L. Gravano. Distributed search over the hidden web:
Hierarchical database sampling and selection. In VLDB, 2002.
[20] P. G. Ipeirotis, L. Gravano, and M. Sahami. Probe, count, and classify:
Categorizing hidden web databases. In SIGMOD, 2001.
[21] C. Lagoze and H. V. Sompel. The Open Archives Initiative: Building a
low-barrier interoperability framework In JCDL, 2001.
[22] S. Lawrence and C. L. Giles. Searching the World Wide Web. Science,
280(5360):98-100, 1998.
[23] V. Z. Liu, J. C. Richard C. Luo and, and W. W. Chu. Dpro: A probabilistic
approach for hidden web database selection using dynamic probing. In ICDE,
2004.
[24] X. Liu, K. Maly, M. Zubair and M. L. Nelson. DP9-An OAI Gateway Service
for Web Crawlers. In JCDL, 2002.
[25] B. B. Mandelbrot. Fractal Geometry of Nature. W. H. Freeman & Co.
[26] A. Ntoulas, J. Cho, and C. Olston. What"s new on the web? the evolution of the
web from a search engine perspective. In WWW, 2004.
[27] A. Ntoulas, P. Zerfos, and J. Cho. Downloading hidden web content. Technical
report, UCLA, 2004.
[28] S. Olsen. Does search engine"s power threaten web"s independence?
http://news.com.com/2009-1023-963618.html.
[29] S. Raghavan and H. Garcia-Molina. Crawling the hidden web. In VLDB, 2001.
[30] G. K. Zipf. Human Behavior and the Principle of Least-Effort.
Addison-Wesley, Cambridge, MA, 1949.
109
Vocabulary Independent Spoken Term Detection
Jonathan Mamou
IBM Haifa Research Labs
Haifa 31905, Israel
mamou@il.ibm.com
Bhuvana Ramabhadran, Olivier Siohan
IBM T. J. Watson Research Center
Yorktown Heights, N.Y. 10598, USA
{bhuvana,siohan}@us.ibm.com
ABSTRACT
We are interested in retrieving information from speech data
like broadcast news, telephone conversations and roundtable
meetings. Today, most systems use large vocabulary 
continuous speech recognition tools to produce word transcripts;
the transcripts are indexed and query terms are retrieved
from the index. However, query terms that are not part
of the recognizer"s vocabulary cannot be retrieved, and the
recall of the search is affected. In addition to the output
word transcript, advanced systems provide also phonetic
transcripts, against which query terms can be matched 
phonetically. Such phonetic transcripts suffer from lower 
accuracy and cannot be an alternative to word transcripts.
We present a vocabulary independent system that can 
handle arbitrary queries, exploiting the information provided
by having both word transcripts and phonetic transcripts.
A speech recognizer generates word confusion networks and
phonetic lattices. The transcripts are indexed for query 
processing and ranking purpose. The value of the proposed
method is demonstrated by the relative high performance of
our system, which received the highest overall ranking for
US English speech data in the recent NIST Spoken Term
Detection evaluation [1].
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval
General Terms
Algorithms
1. INTRODUCTION
The rapidly increasing amount of spoken data calls for
solutions to index and search this data.
The classical approach consists of converting the speech to
word transcripts using a large vocabulary continuous speech
recognition (LVCSR) tool. In the past decade, most of the
research efforts on spoken data retrieval have focused on 
extending classical IR techniques to word transcripts. Some of
these works have been done in the framework of the NIST
TREC Spoken Document Retrieval tracks and are described
by Garofolo et al. [12]. These tracks focused on retrieval
from a corpus of broadcast news stories spoken by 
professionals. One of the conclusions of those tracks was that
the effectiveness of retrieval mostly depends on the 
accuracy of the transcripts. While the accuracy of automatic
speech recognition (ASR) systems depends on the scenario
and environment, state-of-the-art systems achieved better
than 90% accuracy in transcription of such data. In 2000,
Garofolo et al. concluded that Spoken document retrieval
is a solved problem [12].
However, a significant drawback of such approaches is that
search on queries containing out-of-vocabulary (OOV) terms
will not return any results. OOV terms are missing words
from the ASR system vocabulary and are replaced in the
output transcript by alternatives that are probable, given
the recognition acoustic model and the language model. It
has been experimentally observed that over 10% of user
queries can contain OOV terms [16], as queries often 
relate to named entities that typically have a poor coverage
in the ASR vocabulary. The effects of OOV query terms in
spoken data retrieval are discussed by Woodland et al. [28].
In many applications the OOV rate may get worse over time
unless the recognizer"s vocabulary is periodically updated.
Another approach consists of converting the speech to
phonetic transcripts and representing the query as a 
sequence of phones. The retrieval is based on searching the
sequence of phones representing the query in the phonetic
transcripts. The main drawback of this approach is the 
inherent high error rate of the transcripts. Therefore, such
approach cannot be an alternative to word transcripts, 
especially for in-vocabulary (IV) query terms that are part of
the vocabulary of the ASR system.
A solution would be to combine the two different 
approaches presented above: we index both word transcripts
and phonetic transcripts; during query processing, the 
information is retrieved from the word index for IV terms and
from the phonetic index for OOV terms. We would like to
be able to process also hybrid queries, i.e, queries that 
include both IV and OOV terms. Consequently, we need to
merge pieces of information retrieved from word index and
phonetic index. Proximity information on the occurrences
of the query terms is required for phrase search and for
proximity-based ranking. In classical IR, the index stores for
each occurrence of a term, its offset. Therefore, we cannot
merge posting lists retrieved by phonetic index with those
retrieved by word index since the offset of the occurrences
retrieved from the two different indices are not comparable.
The only element of comparison between phonetic and word
transcripts are the timestamps. No previous work 
combining word and phonetic approach has been done on phrase
search. We present a novel scheme for information retrieval
that consists of storing, during the indexing process, for each
unit of indexing (phone or word) its timestamp. We search
queries by merging the information retrieved from the two
different indices, word index and phonetic index, according
to the timestamps of the query terms. We analyze the 
retrieval effectiveness of this approach on the NIST Spoken
Term Detection 2006 evaluation data [1].
The paper is organized as follows. We describe the audio
processing in Section 2. The indexing and retrieval methods
are presented in section 3. Experimental setup and results
are given in Section 4. In Section 5, we give an overview of
related work. Finally, we conclude in Section 6.
2. AUTOMATIC SPEECH RECOGNITION
SYSTEM
We use an ASR system for transcribing speech data. It
works in speaker-independent mode. For best recognition
results, a speaker-independent acoustic model and a 
language model are trained in advance on data with similar
characteristics.
Typically, ASR generates lattices that can be considered
as directed acyclic graphs. Each vertex in a lattice is 
associated with a timestamp and each edge (u, v) is labeled with
a word or phone hypothesis and its prior probability, which
is the probability of the signal delimited by the timestamps
of the vertices u and v, given the hypothesis. The 1-best
path transcript is obtained from the lattice using dynamic
programming techniques.
Mangu et al. [18] and Hakkani-Tur et al. [13] propose a
compact representation of a word lattice called word 
confusion network (WCN). Each edge (u, v) is labeled with a word
hypothesis and its posterior probability, i.e., the probability
of the word given the signal. One of the main advantages
of WCN is that it also provides an alignment for all of the
words in the lattice. As explained in [13], the three main
steps for building a WCN from a word lattice are as follows:
1. Compute the posterior probabilities for all edges in the
word lattice.
2. Extract a path from the word lattice (which can be
the 1-best, the longest or any random path), and call
it the pivot path of the alignment.
3. Traverse the word lattice, and align all the transitions
with the pivot, merging the transitions that 
correspond to the same word (or label) and occur in the
same time interval by summing their posterior 
probabilities.
The 1-best path of a WCN is obtained from the path 
containing the best hypotheses. As stated in [18], although
WCNs are more compact than word lattices, in general the
1-best path obtained from WCN has a better word accuracy
than the 1-best path obtained from the corresponding word
lattice.
Typical structures of a lattice and a WCN are given in
Figure 1.
Figure 1: Typical structures of a lattice and a WCN.
3. RETRIEVAL MODEL
The main problem with retrieving information from 
spoken data is the low accuracy of the transcription 
particularly on terms of interest such as named entities and 
content words. Generally, the accuracy of a word transcript
is characterized by its word error rate (WER). There are
three kinds of errors that can occur in a transcript: 
substitution of a term that is part of the speech by another
term, deletion of a spoken term that is part of the speech
and insertion of a term that is not part of the speech.
Substitutions and deletions reflect the fact that an 
occurrence of a term in the speech signal is not recognized. These
misses reduce the recall of the search. Substitutions and 
insertions reflect the fact that a term which is not part of the
speech signal appears in the transcript. These misses reduce
the precision of the search.
Search recall can be enhanced by expanding the transcript
with extra words. These words can be taken from the other
alternatives provided by the WCN; these alternatives may
have been spoken but were not the top choice of the ASR.
Such an expansion tends to correct the substitutions and
the deletions and consequently, might improve recall but
will probably reduce precision. Using an appropriate 
ranking model, we can avoid the decrease in precision. Mamou et
al. have presented in [17] the enhancement in the recall and
the MAP by searching on WCN instead of considering only
the 1-best path word transcript in the context of spoken 
document retrieval. We have adapted this model of IV search to
term detection. In word transcripts, OOV terms are deleted
or substituted. Therefore, the usage of phonetic transcripts
is more desirable. However, due to their low accuracy, we
have preferred to use only the 1-best path extracted from the
phonetic lattices. We will show that the usage of phonetic
transcripts tends to improve the recall without affecting the
precision too much, using an appropriate ranking.
3.1 Spoken document detection task
As stated in the STD 2006 evaluation plan [2], the task
consists in finding all the exact matches of a specific query
in a given corpus of speech data. A query is a phrase 
containing several words. The queries are text and not speech.
Note that this task is different from the more classical task of
spoken document retrieval. Manual transcripts of the speech
are not provided but are used by the evaluators to find true
occurrences. By definition, true occurrences of a query are
found automatically by searching the manual transcripts 
using the following rule: the gap between adjacent words in
a query must be less than 0.5 seconds in the corresponding
speech. For evaluating the results, each system output 
occurrence is judged as correct or not according to whether it
is close in time to a true occurrence of the query retrieved
from manual transcripts; it is judged as correct if the 
midpoint of the system output occurrence is less than or equal
to 0.5 seconds from the time span of a true occurrence of
the query.
3.2 Indexing
We have used the same indexing process for WCN and
phonetic transcripts. Each occurrence of a unit of indexing
(word or phone) u in a transcript D is indexed with the
following information:
• the begin time t of the occurrence of u,
• the duration d of the occurrence of u.
In addition, for WCN indexing, we store
• the confidence level of the occurrence of u at the
time t that is evaluated by its posterior probability
Pr(u|t, D),
• the rank of the occurrence of u among the other 
hypotheses beginning at the same time t, rank(u|t, D).
Note that since the task is to find exact matches of the
phrase queries, we have not filtered stopwords and the 
corpus is not stemmed before indexing.
3.3 Search
In the following, we present our approach for 
accomplishing the STD task using the indices described above. The
terms are extracted from the query. The vocabulary of the
ASR system building word transcripts is given. Terms that
are part of this vocabulary are IV terms; the other terms
are OOV. For an IV query term, the posting list is extracted
from the word index. For an OOV query term, the term is
converted to a sequence of phones using a joint maximum
entropy N-gram model [10]. For example, the term prosody
is converted to the sequence of phones (p, r, aa, z, ih,
d, iy). The posting list of each phone is extracted from the
phonetic index.
The next step consists of merging the different posting
lists according to the timestamp of the occurrences in order
to create results matching the query. First, we check that
the words and phones appear in the right order according to
their begin times. Second, we check that the gap in time 
between adjacent words and phones is reasonable. 
Conforming to the requirements of the STD evaluation, the distance
in time between two adjacent query terms must be less than
0.5 seconds. For OOV search, we check that the distance
in time between two adjacent phones of a query term is less
that 0.2 seconds; this value has been determined empirically.
In such a way, we can reduce the effect of insertion errors
since we allow insertions between the adjacent words and
phones. Our query processing does not allow substitutions
and deletions.
Example: Let us consider the phrase query prosody
research. The term prosody is OOV and the term research
is IV. The term prosody is converted to the sequence of
phones (p, r, aa, z, ih, d, iy). The posting list of each
phone is extracted from the phonetic index. We merge the
posting lists of the phones such that the sequence of phones
appears in the right order and the gap in time between the
pairs of phones (p, r), (r, aa), (aa, z), (z, ih), (ih, d), (d, iy) is
less than 0.2 seconds. We obtain occurrences of the term
prosody. The posting list of research is extracted from the
word index and we merge it with the occurrences found for
prosody such that they appear in the right order and the
distance in time between prosody and research is less than
0.5 seconds.
Note that our indexing model allows to search for different
types of queries:
1. queries containing only IV terms using the word index.
2. queries containing only OOV terms using the phonetic
index.
3. keyword queries containing both IV and OOV terms
using the word index for IV terms and the phonetic
index for OOV terms; for query processing, the 
different sets of matches are unified if the query terms have
OR semantics and intersected if the query terms have
AND semantics.
4. phrase queries containing both IV and OOV terms; for
query processing, the posting lists of the IV terms 
retrieved from the word index are merged with the 
posting lists of the OOV terms retrieved from the phonetic
index. The merging is possible since we have stored
the timestamps for each unit of indexing (word and
phone) in both indices.
The STD evaluation has focused on the fourth query type.
It is the hardest task since we need to combine posting lists
retrieved from phonetic and word indices.
3.4 Ranking
Since IV terms and OOV terms are retrieved from two 
different indices, we propose two different functions for scoring
an occurrence of a term; afterward, an aggregate score is 
assigned to the query based on the scores of the query terms.
Because the task is term detection, we do not use a 
document frequency criterion for ranking the occurrences.
Let us consider a query Q = (k0, ..., kn), associated with
a boosting vector B = (B1, ..., Bj). This vector associates
a boosting factor to each rank of the different hypotheses;
the boosting factors are normalized between 0 and 1. If the
rank r is larger than j, we assume Br = 0.
3.4.1 In vocabulary term ranking
For IV term ranking, we extend the work of Mamou et
al. [17] on spoken document retrieval to term detection. We
use the information provided by the word index. We define
the score score(k, t, D) of a keyword k occurring at a time t
in the transcript D, by the following formula:
score(k, t, D) = Brank(k|t,D) × Pr(k|t, D)
Note that 0 ≤ score(k, t, D) ≤ 1.
3.4.2 Out of vocabulary term ranking
For OOV term ranking, we use the information provided
by the phonetic index. We give a higher rank to occurrences
of OOV terms that contain phones close (in time) to each
other. We define a scoring function that is related to the
average gap in time between the different phones. Let us
consider a keyword k converted to the sequence of phones
(pk
0 , ..., pk
l ). We define the normalized score score(k, tk
0 , D)
of a keyword k = (pk
0 , ..., pk
l ), where each pk
i occurs at time
tk
i with a duration of dk
i in the transcript D, by the following
formula:
score(k, tk
0 , D) = 1 −
l
i=1 5 × (tk
i − (tk
i−1 + dk
i−1))
l
Note that according to what we have ex-plained in 
Section 3.3, we have ∀1 ≤ i ≤ l, 0 < tk
i − (tk
i−1 + dk
i−1) <
0.2 sec, 0 < 5 × (tk
i − (tk
i−1 + dk
i−1)) < 1, and consequently,
0 < score(k, tk
0 , D) ≤ 1. The duration of the keyword 
occurrence is tk
l − tk
0 + dk
l .
Example: let us consider the sequence (p, r, aa, z,
ih, d, iy) and two different occurrences of the sequence.
For each phone, we give the begin time and the duration in
second.
Occurrence 1: (p, 0.25, 0.01), (r, 0.36, 0.01), (aa, 0.37, 0.01),
(z, 0.38, 0.01), (ih, 0.39, 0.01), (d, 0.4, 0.01), (iy, 0.52, 0.01).
Occurrence 2: (p, 0.45, 0.01), (r, 0.46, 0.01), (aa, 0.47, 0.01),
(z, 0.48, 0.01), (ih, 0.49, 0.01), (d, 0.5, 0.01), (iy, 0.51, 0.01).
According to our formula, the score of the first occurrence
is 0.83 and the score of the second occurrence is 1. In the
first occurrence, there are probably some insertion or silence
between the phone p and r, and between the phone d and iy.
The silence can be due to the fact that the phones belongs
to two different words ans therefore, it is not an occurrence
of the term prosody.
3.4.3 Combination
The score of an occurrence of a query Q at time t0 in the
document D is determined by the multiplication of the score
of each keyword ki, where each ki occurs at time ti with a
duration di in the transcript D:
score(Q, t0, D) =
n
i=0
score(ki, ti, D)γn
Note that according to what we have ex-plained in 
Section 3.3, we have ∀1 ≤ i ≤ n, 0 < ti −(ti−1 +di−1) < 0.5 sec.
Our goal is to estimate for each found occurrence how
likely the query appears. It is different from classical IR
that aims to rank the results and not to score them. Since
the probability to have a false alarm is inversely proportional
to the length of the phrase query, we have boosted the score
of queries by a γn exponent, that is related to the number
of keywords in the phrase. We have determined empirically
the value of γn = 1/n.
The begin time of the query occurrence is determined by
the begin time t0 of the first query term and the duration
of the query occurrence by tn − t0 + dn.
4. EXPERIMENTS
4.1 Experimental setup
Our corpus consists of the evaluation set provided by NIST
for the STD 2006 evaluation [1]. It includes three 
different source types in US English: three hours of broadcast
news (BNEWS), three hours of conversational telephony
speech (CTS) and two hours of conference room meetings
(CONFMTG). As shown in Section 4.2, these different 
collections have different accuracies. CTS and CONFMTG are
spontaneous speech. For the experiments, we have processed
the query set provided by NIST that includes 1100 queries.
Each query is a phrase containing between one to five terms,
common and rare terms, terms that are in the manual 
transcripts and those that are not. Testing and determination
of empirical values have been achieved on another set of
speech data and queries, the development set, also provided
by NIST.
We have used the IBM research prototype ASR system,
described in [26], for transcribing speech data. We have
produced WCNs for the three different source types. 1-best
phonetic transcripts were generated only for BNEWS and
CTS, since CONFMTG phonetic transcripts have too low
accuracy. We have adapted Juru [7], a full-text search 
library written in Java, to index the transcripts and to store
the timestamps of the words and phones; search results have
been retrieved as described in Section 3.
For each found occurrence of the given query, our system
outputs: the location of the term in the audio recording
(begin time and duration), the score indicating how likely
is the occurrence of query, (as defined in Section 3.4) and a
hard (binary) decision as to whether the detection is 
correct. We measure precision and recall by comparing the
results obtained over the automatic transcripts (only the 
results having true hard decision) to the results obtained over
the reference manual transcripts. Our aim is to evaluate the
ability of the suggested retrieval approach to handle 
transcribed speech data. Thus, the closer the automatic results
to the manual results is, the better the search effectiveness
over the automatic transcripts will be. The results returned
from the manual transcription for a given query are 
considered relevant and are expected to be retrieved with highest
scores. This approach for measuring search effectiveness 
using manual data as a reference is very common in speech
retrieval research [25, 22, 8, 9, 17].
Beside the recall and the precision, we use the evaluation
measures defined by NIST for the 2006 STD evaluation [2]:
the Actual Term-Weighted Value (ATWV) and the 
Maximum Term-Weighted Value (MTWV). The term-weighted
value (TWV) is computed by first computing the miss and
false alarm probabilities for each query separately, then 
using these and an (arbitrarily chosen) prior probability to
compute query-specific values, and finally averaging these
query-specific values over all queries q to produce an overall
system value:
TWV (θ) = 1 − averageq{Pmiss(q, θ) + β × PF A(q, θ)}
where β = C
V
(Pr−1
q − 1). θ is the detection threshold. For
the evaluation, the cost/value ratio, C/V , has been 
determined to 0.1 and the prior probability of a query Prq to
10−4
. Therefore, β = 999.9.
Miss and false alarm probabilities for a given query q are
functions of θ:
Pmiss(q, θ) = 1 −
Ncorrect(q, θ)
Ntrue(q)
PF A(q, θ) =
Nspurious(q, θ)
NNT (q)
corpus WER(%) SUBR(%) DELR(%) INSR(%)
BNEWS WCN 12.7 49 42 9
CTS WCN 19.6 51 38 11
CONFMTG WCN 47.4 47 49 3
Table 1: WER and distribution of the error types over word 1-best path extracted from WCNs for the
different source types.
where:
• Ncorrect(q, θ) is the number of correct detections 
(retrieved by the system) of the query q with a score
greater than or equal to θ.
• Nspurious(q, θ) is the number of spurious detections of
the query q with a score greater than or equal to θ.
• Ntrue(q) is the number of true occurrences of the query
q in the corpus.
• NNT (q) is the number of opportunities for incorrect
detection of the query q in the corpus; it is the 
NonTarget query trials. It has been defined by the 
following formula: NNT (q) = Tspeech − Ntrue(q). Tspeech
is the total amount of speech in the collection (in 
seconds).
ATWV is the actual term-weighted value; it is the 
detection value attained by the system as a result of the system
output and the binary decision output for each putative 
occurrence. It ranges from −∞ to +1. MTWV is the 
maximum term-weighted value over the range of all possible
values of θ. It ranges from 0 to +1.
We have also provided the detection error tradeoff (DET)
curve [19] of miss probability (Pmiss) vs. false alarm 
probability (PF A).
We have used the STDEval tool to extract the relevant
results from the manual transcripts and to compute ATWV,
MTWV and the DET curve.
We have determined empirically the following values for
the boosting vector defined in Section 3.4: Bi = 1
i
.
4.2 WER analysis
We use the word error rate (WER) in order to characterize
the accuracy of the transcripts. WER is defined as follows:
S + D + I
N
× 100
where N is the total number of words in the corpus, and
S, I, and D are the total number of substitution, insertion,
and deletion errors, respectively. The substitution error rate
(SUBR) is defined by
S
S + D + I
× 100.
Deletion error rate (DELR) and insertion error rate (INSR)
are defined in a similar manner.
Table 1 gives the WER and the distribution of the error
types over 1-best path transcripts extracted from WCNs.
The WER of the 1-best path phonetic transcripts is 
approximately two times worse than the WER of word transcripts.
That is the reason why we have not retrieved from phonetic
transcripts on CONFMTG speech data.
4.3 Theta threshold
We have determined empirically a detection threshold θ
per source type and the hard decision of the occurrences
having a score less than θ is set to false; false occurrences
returned by the system are not considered as retrieved and
therefore, are not used for computing ATWV, precision and
recall.
The value of the threshold θ per source type is reported in
Table 2. It is correlated to the accuracy of the transcripts.
Basically, setting a threshold aims to eliminate from the
retrieved occurrences, false alarms without adding misses.
The higher the WER is, the higher the θ threshold should
be.
BNEWS CTS CONFMTG
0.4 0.61 0.91
Table 2: Values of the θ threshold per source type.
4.4 Processing resource profile
We report in Table 3 the processing resource profile. 
Concerning the index size, note that our index is compressed
using IR index compression techniques. The indexing time
includes both audio processing (generation of word and 
phonetic transcripts) and building of the searchable indices.
Index size 0.3267 MB/HS
Indexing time 7.5627 HP/HS
Index Memory Usage 1653.4297 MB
Search speed 0.0041 sec.P/HS
Search Memory Usage 269.1250 MB
Table 3: Processing resource profile. (HS: Hours of
Speech. HP: Processing Hours. sec.P: Processing
seconds)
4.5 Retrieval measures
We compare our approach (WCN phonetic) presented in
Section 4.1 with another approach (1-best-WCN phonetic).
The only difference between these two approaches is that,
in 1-best-WCN phonetic, we index only the 1-best path 
extracted from the WCN instead of indexing all the WCN.
WCN phonetic was our primary system for the evaluation
and 1-best-WCN phonetic was one of our contrastive 
systems. Average precision and recall, MTWV and ATWV on
the 1100 queries are given in Table 4. We provide also the
DET curve for WCN phonetic approach in Figure 2. The
point that maximizes the TWV, the MTWV, is specified on
each curve. Note that retrieval performance has been 
evaluated separately for each source type since the accuracy of
the speech differs per source type as shown in Section 4.2.
As expected, we can see that MTWV and ATWV decrease
in higher WER. The retrieval performance is improved when
measure BNEWS CTS CONFMTG
WCN phonetic ATWV 0.8485 0.7392 0.2365
MTWV 0.8532 0.7408 0.2508
precision 0.94 0.90 0.65
recall 0.89 0.81 0.37
1-best-WCN phonetic ATWV 0.8279 0.7102 0.2381
MTWV 0.8319 0.7117 0.2512
precision 0.95 0.91 0.66
recall 0.84 0.75 0.37
Table 4: ATWV, MTWV, precision and recall per source type.
Figure 2: DET curve for WCN phonetic approach.
using WCNs relatively to 1-best path. It is due to the fact
that miss probability is improved by indexing all the 
hypotheses provided by the WCNs. This observation confirms
the results shown by Mamou et al. [17] in the context of 
spoken document retrieval. The ATWV that we have obtained
is close to the MTWV; we have combined our ranking model
with appropriate threshold θ to eliminate results with lower
score. Therefore, the effect of false alarms added by WCNs
is reduced.
WCN phonetic approach was used in the recent NIST STD
evaluation and received the highest overall ranking among
eleven participants. For comparison, the system that ranked
at the third place, obtained an ATWV of 0.8238 for BNEWS,
0.6652 for CTS and 0.1103 for CONFMTG.
4.6 Influence of the duration of the query on
the retrieval performance
We have analysed the retrieval performance according to
the average duration of the occurrences in the manual 
transcripts. The query set was divided into three different 
quantiles according to the duration; we have reported in Table 5
ATWV and MTWV according to the duration. We can see
that we performed better on longer queries. One of the 
reasons is the fact that the ASR system is more accurate on
long words. Hence, it was justified to boost the score of the
results with the exponent γn, as explained in Section 3.4.3,
according to the length of the query.
quantile 0-33 33-66 66-100
BNEWS ATWV 0.7655 0.8794 0.9088
MTWV 0.7819 0.8914 0.9124
CTS ATWV 0.6545 0.8308 0.8378
MTWV 0.6551 0.8727 0.8479
CONFMTG ATWV 0.1677 0.3493 0.3651
MTWV 0.1955 0.4109 0.3880
Table 5: ATWV, MTWV according to the duration
of the query occurrences per source type.
4.7 OOV vs. IV query processing
We have randomly chosen three sets of queries from the
query sets provided by NIST: 50 queries containing only IV
terms; 50 queries containing only OOV terms; and 50 hybrid
queries containing both IV and OOV terms. The following
experiment has been achieved on the BNEWS collection and
IV and OOV terms has been determined according to the
vocabulary of BNEWS ASR system.
We would like to compare three different approaches of
retrieval: using only word index; using only phonetic index;
combining word and phonetic indices. Table 6 summarizes
the retrieval performance according to each approach and
to each type of queries. Using a word-based approach for
dealing with OOV and hybrid queries affects drastically the
performance of the retrieval; precision and recall are null.
Using a phone-based approach for dealing with IV queries
affects also the performance of the retrieval relatively to the
word-based approach.
As expected, the approach combining word and phonetic
indices presented in Section 3 leads to the same retrieval
performance as the word approach for IV queries and to
the same retrieval performance as the phonetic approach for
OOV queries. This approach always outperforms the others
and it justifies the fact that we need to combine word and
phonetic search.
5. RELATED WORK
In the past decade, the research efforts on spoken data
retrieval have focused on extending classical IR techniques
to spoken documents. Some of these works have been done
in the context of the TREC Spoken Document Retrieval
evaluations and are described by Garofolo et al. [12]. An
LVCSR system is used to transcribe the speech into 1-best
path word transcripts. The transcripts are indexed as clean
text: for each occurrence, its document, its word offset and
additional information are stored in the index. A generic IR
system over the text is used for word spotting and search
as described by Brown et al. [6] and James [14]. This 
stratindex word phonetic word and phonetic
precision recall precision recall precision recall
IV queries 0.8 0.96 0.11 0.77 0.8 0.96
OOV queries 0 0 0.13 0.79 0.13 0.79
hybrid queries 0 0 0.15 0.71 0.89 0.83
Table 6: Comparison of word and phonetic approach on IV and OOV queries
egy works well for transcripts like broadcast news collections
that have a low WER (in the range of 15%-30%) and are
redundant by nature (the same piece of information is 
spoken several times in different manners). Moreover, the 
algorithms have been mostly tested over long queries stated in
plain English and retrieval for such queries is more robust
against speech recognition errors.
An alternative approach consists of using word lattices in
order to improve the effectiveness of SDR. Singhal et al. [24,
25] propose to add some terms to the transcript in order
to alleviate the retrieval failures due to ASR errors. From
an IR perspective, a classical way to bring new terms is
document expansion using a similar corpus. Their approach
consists in using word lattices in order to determine which
words returned by a document expansion algorithm should
be added to the original transcript. The necessity to use a
document expansion algorithm was justified by the fact that
the word lattices they worked with, lack information about
word probabilities.
Chelba and Acero in [8, 9] propose a more compact word
lattice, the position specific posterior lattice (PSPL). This
data structure is similar to WCN and leads to a more 
compact index. The offset of the terms in the speech documents
is also stored in the index. However, the evaluation 
framework is carried out on lectures that are relatively planned,
in contrast to conversational speech. Their ranking model
is based on the term confidence level but does not take into
consideration the rank of the term among the other 
hypotheses. Mamou et al. [17] propose a model for spoken document
retrieval using WCNs in order to improve the recall and the
MAP of the search. However, in the above works, the 
problem of queries containing OOV terms is not addressed.
Popular approaches to deal with OOV queries are based
on sub-words transcripts, where the sub-words are typically
phones, syllables or word fragments (sequences of phones)
[11, 20, 23]. The classical approach consists of using 
phonetic transcripts. The transcripts are indexed in the same
manner as words in using classical text retrieval techniques;
during query processing, the query is represented as a 
sequence of phones. The retrieval is based on searching the
string of phones representing the query in the phonetic 
transcript. To account for the high recognition error rates, some
other systems use richer transcripts like phonetic lattices.
They are attractive as they accommodate high error rate
conditions as well as allow for OOV queries to be used [15,
3, 20, 23, 21, 27]. However, phonetic lattices contain many
edges that overlap in time with the same phonetic label, and
are difficult to index. Moreover, beside the improvement in
the recall of the search, the precision is affected since 
phonetic lattices are often inaccurate. Consequently, phonetic
approaches should be used only for OOV search; for 
searching queries containing also IV terms, this technique affects
the performance of the retrieval in comparison to the word
based approach.
Saraclar and Sproat in [22] show improvement in word
spotting accuracy for both IV and OOV queries, using 
phonetic and word lattices, where a confidence measure of a
word or a phone can be derived. They propose three 
different retrieval strategies: search both the word and the
phonetic indices and unify the two different sets of results;
search the word index for IV queries, search the phonetic 
index for OOV queries; search the word index and if no result
is returned, search the phonetic index. However, no strategy
is proposed to deal with phrase queries containing both IV
and OOV terms. Amir et al. in [5, 4] propose to merge a
word approach with a phonetic approach in the context of
video retrieval. However, the phonetic transcript is obtained
from a text to phonetic conversion of the 1-best path of the
word transcript and is not based on a phonetic decoding of
the speech data.
An important issue to be considered when looking at the
state-of-the-art in retrieval of spoken data, is the lack of a
common test set and appropriate query terms. This paper
uses such a task and the STD evaluation is a good summary
of the performance of different approaches on the same test
conditions.
6. CONCLUSIONS
This work studies how vocabulary independent spoken
term detection can be performed efficiently over different
data sources. Previously, phonetic-based and word-based
approaches have been used for IR on speech data. The 
former suffers from low accuracy and the latter from limited
vocabulary of the recognition system. In this paper, we have
presented a vocabulary independent model of indexing and
search that combines both the approaches. The system can
deal with all kinds of queries although the phrases that need
to combine for the retrieval, information extracted from two
different indices, a word index and a phonetic index. The
scoring of OOV terms is based on the proximity (in time) 
between the different phones. The scoring of IV terms is based
on information provided by the WCNs. We have shown an
improvement in the retrieval performance when using all the
WCN and not only the 1-best path and when using phonetic
index for search of OOV query terms. This approach always
outperforms the other approaches using only word index or
phonetic index.
As a future work, we will compare our model for OOV
search on phonetic transcripts with a retrieval model based
on the edit distance.
7. ACKNOWLEDGEMENTS
Jonathan Mamou is grateful to David Carmel and Ron
Hoory for helpful and interesting discussions.
8. REFERENCES
[1] NIST Spoken Term Detection 2006 Evaluation
Website, http://www.nist.gov/speech/tests/std/.
[2] NIST Spoken Term Detection (STD) 2006 Evaluation
Plan, 
http://www.nist.gov/speech/tests/std/docs/std06-evalplan-v10.pdf.
[3] C. Allauzen, M. Mohri, and M. Saraclar. General
indexation of weighted automata - application to
spoken utterance retrieval. In Proceedings of the
HLT-NAACL 2004 Workshop on Interdiciplinary
Approaches to Speech Indexing and Retrieval, Boston,
MA, USA, 2004.
[4] A. Amir, M. Berg, and H. Permuter. Mutual relevance
feedback for multimodal query formulation in video
retrieval. In MIR "05: Proceedings of the 7th ACM
SIGMM international workshop on Multimedia
information retrieval, pages 17-24, New York, NY,
USA, 2005. ACM Press.
[5] A. Amir, A. Efrat, and S. Srinivasan. Advances in
phonetic word spotting. In CIKM "01: Proceedings of
the tenth international conference on Information and
knowledge management, pages 580-582, New York,
NY, USA, 2001. ACM Press.
[6] M. Brown, J. Foote, G. Jones, K. Jones, and S. Young.
Open-vocabulary speech indexing for voice and video
mail retrieval. In Proceedings ACM Multimedia 96,
pages 307-316, Hong-Kong, November 1996.
[7] D. Carmel, E. Amitay, M. Herscovici, Y. S. Maarek,
Y. Petruschka, and A. Soffer. Juru at TREC 
10Experiments with Index Pruning. In Proceedings of the
Tenth Text Retrieval Conference (TREC-10). National
Institute of Standards and Technology. NIST, 2001.
[8] C. Chelba and A. Acero. Indexing uncertainty for
spoken document search. In Interspeech 2005, pages
61-64, Lisbon, Portugal, 2005.
[9] C. Chelba and A. Acero. Position specific posterior
lattices for indexing speech. In Proceedings of the 43rd
Annual Conference of the Association for
Computational Linguistics (ACL), Ann Arbor, MI,
2005.
[10] S. Chen. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech 2003,
Geneva, Switzerland, 2003.
[11] M. Clements, S. Robertson, and M. Miller. Phonetic
searching applied to on-line distance learning modules.
In Digital Signal Processing Workshop, 2002 and the
2nd Signal Processing Education Workshop.
Proceedings of 2002 IEEE 10th, pages 186-191, 2002.
[12] J. Garofolo, G. Auzanne, and E. Voorhees. The TREC
spoken document retrieval track: A success story. In
Proceedings of the Ninth Text Retrieval Conference
(TREC-9). National Institute of Standards and
Technology. NIST, 2000.
[13] D. Hakkani-Tur and G. Riccardi. A general algorithm
for word graph matrix decomposition. In Proceedings
of the IEEE Internation Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages
596-599, Hong-Kong, 2003.
[14] D. James. The application of classical information
retrieval techniques to spoken documents. PhD thesis,
University of Cambridge, Downing College, 1995.
[15] D. A. James. A system for unrestricted topic retrieval
from radio news broadcasts. In Proc. ICASSP "96,
pages 279-282, Atlanta, GA, 1996.
[16] B. Logan, P. Moreno, J. V. Thong, and E. Whittaker.
An experimental study of an audio indexing system
for the web. In Proceedings of ICSLP, 1996.
[17] J. Mamou, D. Carmel, and R. Hoory. Spoken
document retrieval from call-center conversations. In
SIGIR "06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 51-58,
New York, NY, USA, 2006. ACM Press.
[18] L. Mangu, E. Brill, and A. Stolcke. Finding consensus
in speech recognition: word error minimization and
other applications of confusion networks. Computer
Speech and Language, 14(4):373-400, 2000.
[19] A. Martin, G. Doddington, T. Kamm, M. Ordowski,
and M. Przybocki. The DET curve in assessment of
detection task performance. In Proc. Eurospeech "97,
pages 1895-1898, Rhodes, Greece, 1997.
[20] K. Ng and V. W. Zue. Subword-based approaches for
spoken document retrieval. Speech Commun.,
32(3):157-186, 2000.
[21] Y. Peng and F. Seide. Fast two-stage
vocabulary-independent search in spontaneous speech.
In Acoustics, Speech, and Signal Processing.
Proceedings. (ICASSP). IEEE International
Conference, volume 1, pages 481-484, 2005.
[22] M. Saraclar and R. Sproat. Lattice-based search for
spoken utterance retrieval. In HLT-NAACL 2004:
Main Proceedings, pages 129-136, Boston,
Massachusetts, USA, 2004.
[23] F. Seide, P. Yu, C. Ma, and E. Chang.
Vocabulary-independent search in spontaneous speech.
In ICASSP-2004, IEEE International Conference on
Acoustics, Speech, and Signal Processing, 2004.
[24] A. Singhal, J. Choi, D. Hindle, D. Lewis, and
F. Pereira. AT&T at TREC-7. In Proceedings of the
Seventh Text Retrieval Conference (TREC-7).
National Institute of Standards and Technology.
NIST, 1999.
[25] A. Singhal and F. Pereira. Document expansion for
speech retrieval. In SIGIR "99: Proceedings of the
22nd annual international ACM SIGIR conference on
research and development in information retrieval,
pages 34-41, New York, NY, USA, 1999. ACM Press.
[26] H. Soltau, B. Kingsbury, L. Mangu, D. Povey,
G. Saon, and G. Zweig. The IBM 2004 conversational
telephony system for rich transcription. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), March 2005.
[27] K. Thambiratnam and S. Sridharan. Dynamic match
phone-lattice searches for very fast and accurate
unrestricted vocabulary keyword spotting. In
Acoustics, Speech, and Signal Processing. Proceedings.
(ICASSP). IEEE International Conference, 2005.
[28] P. C. Woodland, S. E. Johnson, P. Jourlin, and K. S.
Jones. Effects of out of vocabulary words in spoken
document retrieval (poster session). In SIGIR "00:
Proceedings of the 23rd annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 372-374, New York, NY,
USA, 2000. ACM Press.
Broad Expertise Retrieval in Sparse Data Environments
Krisztian Balog
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ
Amsterdam, The Netherlands
kbalog@science.uva.nl
Toine Bogers
ILK, Tilburg University
P.O. Box 90153, 5000 LE
Tilburg, The Netherlands
A.M.Bogers@uvt.nl
Leif Azzopardi
Dept. of Computing Science
University of Glasgow,
Glasgow, G12 8QQ
leif@dcs.gla.ac.uk
Maarten de Rijke
ISLA, University of Amsterdam
Kruislaan 403, 1098 SJ
Amsterdam, The Netherlands
mdr@science.uva.nl
Antal van den Bosch
ILK, Tilburg University
P.O. Box 90153, 5000 LE
Tilburg, The Netherlands
Antal.vdnBosch@uvt.nl
ABSTRACT
Expertise retrieval has been largely unexplored on data other than
the W3C collection. At the same time, many intranets of 
universities and other knowledge-intensive organisations offer examples
of relatively small but clean multilingual expertise data, covering
broad ranges of expertise areas. We first present two main 
expertise retrieval tasks, along with a set of baseline approaches based on
generative language modeling, aimed at finding expertise relations
between topics and people. For our experimental evaluation, we
introduce (and release) a new test set based on a crawl of a 
university site. Using this test set, we conduct two series of experiments.
The first is aimed at determining the effectiveness of baseline 
expertise retrieval methods applied to the new test set. The second
is aimed at assessing refined models that exploit characteristic 
features of the new test set, such as the organizational structure of the
university, and the hierarchical structure of the topics in the test set.
Expertise retrieval models are shown to be robust with respect to
environments smaller than the W3C collection, and current 
techniques appear to be generalizable to other settings.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.1 Content 
Analysis and Indexing; H.3.3 Information Search and Retrieval; H.3.4
Systems and Software; H.4 [Information Systems Applications]:
H.4.2 Types of Systems; H.4.m Miscellaneous
General Terms
Algorithms, Measurement, Performance, Experimentation
1. INTRODUCTION
An organization"s intranet provides a means for exchanging 
information between employees and for facilitating employee 
collaborations. To efficiently and effectively achieve this, it is necessary
to provide search facilities that enable employees not only to access
documents, but also to identify expert colleagues.
At the TREC Enterprise Track [22] the need to study and 
understand expertise retrieval has been recognized through the 
introduction of Expert Finding tasks. The goal of expert finding is to
identify a list of people who are knowledgeable about a given topic.
This task is usually addressed by uncovering associations between
people and topics [10]; commonly, a co-occurrence of the name
of a person with topics in the same context is assumed to be 
evidence of expertise. An alternative task, which using the same idea
of people-topic associations, is expert profiling, where the task is to
return a list of topics that a person is knowledgeable about [3].
The launch of the Expert Finding task at TREC has generated a
lot of interest in expertise retrieval, with rapid progress being made
in terms of modeling, algorithms, and evaluation aspects. However,
nearly all of the expert finding or profiling work performed has
been validated experimentally using the W3C collection [24] from
the Enterprise Track. While this collection is currently the only
publicly available test collection for expertise retrieval tasks, it only
represents one type of intranet. With only one test collection it is
not possible to generalize conclusions to other realistic settings.
In this paper we focus on expertise retrieval in a realistic setting
that differs from the W3C setting-one in which relatively small
amounts of clean, multilingual data are available, that cover a broad
range of expertise areas, as can be found on the intranets of 
universities and other knowledge-intensive organizations. Typically, this
setting features several additional types of structure: topical 
structure (e.g., topic hierarchies as employed by the organization), 
organizational structure (faculty, department, ...), as well as multiple
types of documents (research and course descriptions, publications,
and academic homepages). This setting is quite different from the
W3C setting in ways that might impact upon the performance of
expertise retrieval tasks.
We focus on a number of research questions in this paper: Does
the relatively small amount of data available on an intranet affect
the quality of the topic-person associations that lie at the heart of
expertise retrieval algorithms? How do state-of-the-art algorithms
developed on the W3C data set perform in the alternative scenario
of the type described above? More generally, do the lessons from
the Expert Finding task at TREC carry over to this setting? How
does the inclusion or exclusion of different documents affect 
expertise retrieval tasks? In addition to, how can the topical and 
organizational structure be used for retrieval purposes?
To answer our research questions, we first present a set of 
baseline approaches, based on generative language modeling, aimed at
finding associations between topics and people. This allows us to
formulate the expert finding and expert profiling tasks in a uniform
way, and has the added benefit of allowing us to understand the 
relations between the two tasks. For our experimental evaluation, we
introduce a new data set (the UvT Expert Collection) which is 
representative of the type of intranet that we described above. Our 
collection is based on publicly available data, crawled from the 
website of Tilburg University (UvT). This type of data is particularly
interesting, since (1) it is clean, heterogeneous, structured, and 
focused, but comprises a limited number of documents; (2) contains
information on the organizational hierarchy; (3) it is bilingual 
(English and Dutch); and (4) the list of expertise areas of an individual
are provided by the employees themselves. Using the UvT Expert
collection, we conduct two sets of experiments. The first is aimed
at determining the effectiveness of baseline expertise finding and
profiling methods in this new setting. A second group of 
experiments is aimed at extensions of the baseline methods that exploit
characteristic features of the UvT Expert Collection; specifically,
we propose and evaluate refined expert finding and profiling 
methods that incorporate topicality and organizational structure.
Apart from the research questions and data set that we contribute,
our main contributions are as follows. The baseline models 
developed for expertise finding perform well on the new data set. While
on the W3C setting the expert finding task appears to be more 
difficult than profiling, for the UvT data the opposite is the case. We
find that profiling on the UvT data set is considerably more 
difficult than on the W3C set, which we believe is due to the large
(but realistic) number of topical areas that we used for profiling:
about 1,500 for the UvT set, versus 50 in the W3C case. 
Taking the similarity between topics into account can significantly 
improve retrieval performance. The best performing similarity 
measures are content-based, therefore they can be applied on the W3C
(and other) settings as well. Finally, we demonstrate that the 
organizational structure can be exploited in the form of a context model,
improving MAP scores for certain models by up to 70%.
The remainder of this paper is organized as follows. In the next
section we review related work. Then, in Section 3 we provide 
detailed descriptions of the expertise retrieval tasks that we address
in this paper: expert finding and expert profiling. In Section 4 we
present our baseline models, of which the performance is then 
assessed in Section 6 using the UvT data set that we introduce in 
Section 5. Advanced models exploiting specific features of our data are
presented in Section 7 and evaluated in Section 8. We formulate our
conclusions in Section 9.
2. RELATED WORK
Initial approaches to expertise finding often employed databases
containing information on the skills and knowledge of each 
individual in the organization [11]. Most of these tools (usually called 
yellow pages or people-finding systems) rely on people to self-assess
their skills against a predefined set of keywords. For updating 
profiles in these systems in an automatic fashion there is a need for
intelligent technologies [5]. More recent approaches use specific
document sets (such as email [6] or software [18]) to find expertise.
In contrast with focusing on particular document types, there is also
an increased interest in the development of systems that index and
mine published intranet documents as sources of evidence for 
expertise. One such published approach is the P@noptic system [9],
which builds a representation of each person by concatenating all
documents associated with that person-this is similar to Model 1
of Balog et al. [4], who formalize and compare two methods. Balog
et al."s Model 1 directly models the knowledge of an expert from
associated documents, while their Model 2 first locates documents
on the topic and then finds the associated experts. In the reported
experiments the second method performs significantly better when
there are sufficiently many associated documents per candidate.
Most systems that took part in the 2005 and 2006 editions of the
Expert Finding task at TREC implemented (variations on) one of
these two models; see [10, 20]. Macdonald and Ounis [16] propose
a different approach for ranking candidate expertise with respect to
a topic based on data fusion techniques, without using 
collectionspecific heuristics; they find that applying field-based weighting
models improves the ranking of candidates. Petkova and Croft [19]
propose yet another approach, based on a combination of the above
Model 1 and 2, explicitly modeling topics.
Turning to other expert retrieval tasks that can also be addressed
using topic-people associations, Balog and de Rijke [3] addressed
the task of determining topical expert profiles. While their methods
proved to be efficient on the W3C corpus, they require an amount
of data that may not be available in the typical knowledge-intensive
organization. Balog and de Rijke [2] study the related task of 
finding experts that are similar to a small set of experts given as input.
As an aside, creating a textual summary of a person shows
some similarities to biography finding, which has received a 
considerable amount of attention recently; see e.g., [13].
We use generative language modeling to find associations 
between topics and people. In our modeling of expert finding and
profiling we collect evidence for expertise from multiple sources, in
a heterogeneous collection, and integrate it with the co-occurrence
of candidates" names and query terms-the language modeling 
setting allows us to do this in a transparent manner. Our modeling
proceeds in two steps. In the first step, we consider three baseline
models, two taken from [4] (the Models 1 and 2 mentioned above),
and one a refined version of a model introduced in [3] (which we
refer to as Model 3 below); this third model is also similar to the
model described by Petkova and Croft [19]. The models we 
consider in our second round of experiments are mixture models 
similar to contextual language models [1] and to the expanded 
documents of Tao et al. [21]; however, the features that we use for
definining our expansions-including topical structure and 
organizational structure-have not been used in this way before.
3. TASKS
In the expertise retrieval scenario that we envisage, users seeking
expertise within an organization have access to an interface that
combines a search box (where they can search for experts or topics)
with navigational structures (of experts and of topics) that allows
them to click their way to an expert page (providing the profile of a
person) or a topic page (providing a list of experts on the topic).
To feed the above interface, we face two expertise retrieval
tasks, expert finding and expert profiling, that we first define and
then formalize using generative language models. In order to model
either task, the probability of the query topic being associated to a
candidate expert plays a key role in the final estimates for searching
and profiling. By using language models, both the candidates and
the query are characterized by distributions of terms in the 
vocabulary (used in the documents made available by the organization
whose expertise retrieval needs we are addressing).
3.1 Expert finding
Expert finding involves the task of finding the right person with
the appropriate skills and knowledge: Who are the experts on topic
X?. E.g., an employee wants to ascertain who worked on a 
particular project to find out why particular decisions were made without
having to trawl through documentation (if there is any). Or, they
may be in need a trained specialist for consultancy on a specific
problem.
Within an organization there are usually many possible 
candidates who could be experts for given topic. We can state this 
problem as follows:
What is the probability of a candidate ca being an 
expert given the query topic q?
That is, we determine p(ca|q), and rank candidates ca according to
this probability. The candidates with the highest probability given
the query are deemed the most likely experts for that topic. The
challenge is how to estimate this probability accurately. Since the
query is likely to consist of only a few terms to describe the 
expertise required, we should be able to obtain a more accurate estimate
by invoking Bayes" Theorem, and estimating:
p(ca|q) =
p(q|ca)p(ca)
p(q)
, (1)
where p(ca) is the probability of a candidate and p(q) is the 
probability of a query. Since p(q) is a constant, it can be ignored for
ranking purposes. Thus, the probability of a candidate ca being an
expert given the query q is proportional to the probability of a query
given the candidate p(q|ca), weighted by the a priori belief p(ca)
that candidate ca is an expert.
p(ca|q) ∝ p(q|ca)p(ca) (2)
In this paper our main focus is on estimating the probability of
a query given the candidate p(q|ca), because this probability 
captures the extent to which the candidate knows about the query topic.
Whereas the candidate priors are generally assumed to be 
uniformand thus will not influence the ranking-it has been demonstrated
that a sensible choice of priors may improve the performance [20].
3.2 Expert profiling
While the task of expert searching was concerned with 
finding experts given a particular topic, the task of expert profiling
seeks to answer a related question: What topics does a candidate
know about? Essentially, this turns the questions of expert finding
around. The profiling of an individual candidate involves the 
identification of areas of skills and knowledge that they have expertise
about and an evaluation of the level of proficiency in each of these
areas. This is the candidate"s topical profile.
Generally, topical profiles within organizations consist of 
tabular structures which explicitly catalogue the skills and knowledge
of each individual in the organization. However, such practice is
limited by the resources available for defining, creating, 
maintaining, and updating these profiles over time. By focusing on 
automatic methods which draw upon the available evidence within the
document repositories of an organization, our aim is to reduce the
human effort associated with the maintenance of topical profiles1
.
A topical profile of a candidate, then, is defined as a vector where
each element i of the vector corresponds to the candidate ca"s 
expertise on a given topic ki, (i.e., s(ca, ki)). Each topic ki defines a
particular knowledge area or skill that the organization uses to 
define the candidate"s topical profile. Thus, it is assumed that a list of
topics, {k1, . . . , kn}, where n is the number of pre-defined topics,
is given:
profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3)
1
Context and evidence are needed to help users of expertise 
finding systems to decide whom to contact when seeking expertise in a
particular area. Examples of such context are: Who does she work
with? What are her contact details? Is she well-connected, just
in case she is not able to help us herself? What is her role in the
organization? Who is her superior? Collaborators, and affiliations,
etc. are all part of the candidate"s social profile, and can serve as
a background against which the system"s recommendations should
be interpreted. In this paper we only address the problem of 
determining topical profiles, and leave social profiling to further work.
We state the problem of quantifying the competence of a person on
a certain knowledge area as follows:
What is the probability of a knowledge area (ki) being
part of the candidate"s (expertise) profile?
where s(ca, ki) is defined by p(ki|ca). Our task, then, is to 
estimate p(ki|ca), which is equivalent to the problem of obtaining
p(q|ca), where the topic ki is represented as a query topic q, i.e., a
sequence of keywords representing the expertise required.
Both the expert finding and profiling tasks rely on the accurate
estimation of p(q|ca). The only difference derives from the prior
probability that a person is an expert (p(ca)), which can be 
incorporated into the expert finding task. This prior does not apply to
the profiling task since the candidate (individual) is fixed.
4. BASELINE MODELS
In this section we describe our baseline models for estimating
p(q|ca), i.e., associations between topics and people. Both expert
finding and expert profiling boil down to this estimation. We 
employ three models for calculating this probability.
4.1 From topics to candidates
Using Candidate Models: Model 1 Model 1 [4] defines the 
probability of a query given a candidate (p(q|ca)) using standard 
language modeling techniques, based on a multinomial unigram 
language model. For each candidate ca, a candidate language model
θca is inferred such that the probability of a term given θca is 
nonzero for all terms, i.e., p(t|θca) > 0. From the candidate model the
query is generated with the following probability:
p(q|θca) =
Y
t∈q
p(t|θca)n(t,q)
,
where each term t in the query q is sampled identically and 
independently, and n(t, q) is the number of times t occurs in q. The
candidate language model is inferred as follows: (1) an empirical
model p(t|ca) is computed; (2) it is smoothed with background
probabilities. Using the associations between a candidate and a
document, the probability p(t|ca) can be approximated by:
p(t|ca) =
X
d
p(t|d)p(d|ca),
where p(d|ca) is the probability that candidate ca generates a 
supporting document d, and p(t|d) is the probability of a term t 
occurring in the document d. We use the maximum-likelihood estimate
of a term, that is, the normalised frequency of the term t in 
document d. The strength of the association between document d and
candidate ca expressed by p(d|ca) reflects the degree to which the
candidates expertise is described using this document. The 
estimation of this probability is presented later, in Section 4.2.
The candidate model is then constructed as a linear interpolation
of p(t|ca) and the background model p(t) to ensure there are no
zero probabilities, which results in the final estimation:
p(q|θca) = (4)
Y
t∈q
(
(1 − λ)
X
d
p(t|d)p(d|ca)
!
+ λp(t)
)n(t,q)
.
Model 1 amasses all the term information from all the documents
associated with the candidate, and uses this to represent that 
candidate. This model is used to predict how likely a candidate would
produce a query q. This can can be intuitively interpreted as the
probability of this candidate talking about the query topic, where
we assume that this is indicative of their expertise.
Using Document Models: Model 2 Model 2 [4] takes a 
different approach. Here, the process is broken into two parts. Given
a candidate ca, (1) a document that is associated with a candidate
is selected with probability p(d|ca), and (2) from this document a
query q is generated with probability p(q|d). Then the sum over all
documents is taken to obtain p(q|ca), such that:
p(q|ca) =
X
d
p(q|d)p(d|ca). (5)
The probability of a query given a document is estimated by 
inferring a document language model θd for each document d in a
similar manner as the candidate model was inferred:
p(t|θd) = (1 − λ)p(t|d) + λp(t), (6)
where p(t|d) is the probability of the term in the document. The
probability of a query given the document model is:
p(q|θd) =
Y
t∈q
p(t|θd)n(t,q)
.
The final estimate of p(q|ca) is obtained by substituting p(q|d) for
p(q|θd) into Eq. 5 (see [4] for full details). Conceptually, Model 2
differs from Model 1 because the candidate is not directly modeled.
Instead, the document acts like a hidden variable in the process
which separates the query from the candidate. This process is akin
to how a user may search for candidates with a standard search
engine: initially by finding the documents which are relevant, and
then seeing who is associated with that document. By examining a
number of documents the user can obtain an idea of which 
candidates are more likely to discuss the topic q.
Using Topic Models: Model 3 We introduce a third model, Model 3.
Instead of attempting to model the query generation process via
candidate or document models, we represent the query as a topic
language model and directly estimate the probability of the 
candidate p(ca|q). This approach is similar to the model presented
in [3, 19]. As with the previous models, a language model is 
inferred, but this time for the query. We adapt the work of Lavrenko
and Croft [14] to estimate a topic model from the query.
The procedure is as follows. Given a collection of documents
and a query topic q, it is assumed that there exists an unknown
topic model θk that assigns probabilities p(t|θk) to the term 
occurrences in the topic documents. Both the query and the documents
are samples from θk (as opposed to the previous approaches, where
a query is assumed to be sampled from a specific document or 
candidate model). The main task is to estimate p(t|θk), the probability
of a term given the topic model. Since the query q is very sparse,
and as there are no examples of documents on the topic, this 
distribution needs to be approximated. Lavrenko and Croft [14] suggest
a reasonable way of obtaining such an approximation, by assuming
that p(t|θk) can be approximated by the probability of term t given
the query q. We can then estimate p(t|q) using the joint probability
of observing the term t together with the query terms, q1, . . . , qm,
and dividing by the joint probability of the query terms:
p(t|θk) ≈ p(t|q) =
p(t, q1, . . . , qm)
p(q1, . . . , qm)
=
p(t, q1, . . . , qm)
P
t ∈T p(t , q1, . . . , qm)
,
where p(q1, . . . , qm) =
P
t ∈T p(t , q1, . . . , qm), and T is the 
entire vocabulary of terms. In order to estimate the joint probability
p(t, q1, . . . , qm), we follow [14, 15] and assume t and q1, . . . , qm
are mutually independent, once we pick a source distribution from
the set of underlying source distributions U. If we choose U to be
a set of document models. then to construct this set, the query q
would be issued against the collection, and the top n returned are
assumed to be relevant to the topic, and thus treated as samples
from the topic model. (Note that candidate models could be used
instead.) With the document models forming U, the joint 
probability of term and query becomes:
p(t, q1, . . . , qm) =
X
d∈U
p(d)
˘
p(t|θd)
mY
i=1
p(qi|θd)
¯
. (7)
Here, p(d) denotes the prior distribution over the set U, which 
reflects the relevance of the document to the topic. We assume that
p(d) is uniform across U. In order to rank candidates according
to the topic model defined, we use the Kullback-Leibler divergence
metric (KL, [8]) to measure the difference between the candidate
models and the topic model:
KL(θk||θca) =
X
t
p(t|θk) log
p(t|θk)
p(t|θca)
. (8)
Candidates with a smaller divergence from the topic model are 
considered to be more likely experts on that topic. The candidate model
θca is defined in Eq. 4. By using KL divergence instead of the 
probability of a candidate given the topic model p(ca|θk), we avoid
normalization problems.
4.2 Document-candidate associations
For our models we need to be able to estimate the probability
p(d|ca), which expresses the extent to which a document d 
characterizes the candidate ca. In [4], two methods are presented for 
estimating this probability, based on the number of person names 
recognized in a document. However, in our (intranet) setting it is 
reasonable to assume that authors of documents can unambiguously be
identified (e.g., as the author of an article, the teacher assigned to a
course, the owner of a web page, etc.) Hence, we set p(d|ca) to be
1 if candidate ca is author of document d, otherwise the probability
is 0. In Section 6 we describe how authorship can be determined
on different types of documents within the collection.
5. THE UVT EXPERT COLLECTION
The UvT Expert collection used in the experiments in this paper
fits the scenario outlined in Section 3. The collection is based on
the Webwijs (Webwise) system developed at Tilburg University
(UvT) in the Netherlands. Webwijs (http://www.uvt.nl/
webwijs/) is a publicly accessible database of UvT employees
who are involved in research or teaching; currently, Webwijs 
contains information about 1168 experts, each of whom has a page with
contact information and, if made available by the expert, a research
description and publications list. In addition, each expert can 
select expertise areas from a list of 1491 topics and is encouraged to
suggest new topics that need to be approved by the Webwijs editor.
Each topic has a separate page that shows all experts associated
with that topic and, if available, a list of related topics.
Webwijs is available in Dutch and English, and this bilinguality
has been preserved in the collection. Every Dutch Webwijs page
has an English translation. Not all Dutch topics have an English
translation, but the reverse is true: the 981 English topics all have a
Dutch equivalent.
About 42% of the experts teach courses at Tilburg University;
these courses were also crawled and included in the profile. In 
addition, about 27% of the experts link to their academic homepage
from their Webwijs page. These home pages were crawled and
added to the collection. (This means that if experts put the full-text
versions of their publications on their academic homepage, these
were also available for indexing.) We also obtained 1880 full-text
versions of publications from the UvT institutional repository and
Dutch English
no. of experts 1168 1168
no. of experts with ≥ 1 topic 743 727
no. of topics 1491 981
no. of expert-topic pairs 4318 3251
avg. no. of topics/expert 5.8 5.9
max. no. of topics/expert (no. of experts) 60 (1) 35 (1)
min. no. of topics/expert (no. of experts) 1 (74) 1 (106)
avg. no. of experts/topic 2.9 3.3
max. no. of experts/topic (no. of topics) 30 (1) 30 (1)
min. no. of experts/topic (no. of topics) 1 (615) 1 (346)
no. of experts with HP 318 318
no. of experts with CD 318 318
avg. no. of CDs per teaching expert 3.5 3.5
no. of experts with RD 329 313
no. of experts with PUB 734 734
avg. no. of PUBs per expert 27.0 27.0
avg. no. of PUB citations per expert 25.2 25.2
avg. no. of full-text PUBs per expert 1.8 1.8
Table 2: Descriptive statistics of the Dutch and English versions
of the UvT Expert collection.
converted them to plain text. We ran the TextCat [23] language
identifier to classify the language of the home pages and the 
fulltext publications. We restricted ourselves to pages where the 
classifier was confident about the language used on the page.
This resulted in four document types: research descriptions (RD),
course descriptions (CD), publications (PUB; full-text and 
citationonly versions), and academic homepages (HP). Everything was
bundled into the UvT Expert collection which is available at http:
//ilk.uvt.nl/uvt-expert-collection/.
The UvT Expert collection was extracted from a different 
organizational setting than the W3C collection and differs from it in
a number of ways. The UvT setting is one with relatively small
amounts of multilingual data. Document-author associations are
clear and the data is structured and clean. The collection covers a
broad range of expertise areas, as one can typically find on intranets
of universities and other knowledge-intensive institutes. 
Additionally, our university setting features several types of structure 
(topical and organizational), as well as multiple document types. 
Another important difference between the two data sets is that the 
expertise areas in the UvT Expert collection are self-selected instead
of being based on group membership or assignments by others.
Size is another dimension along which the W3C and UvT Expert
collections differ: the latter is the smaller of the two. Also realistic
are the large differences in the amount of information available for
each expert. Utilizing Webwijs is voluntary; 425 Dutch experts
did not select any topics at all. This leaves us with 743 Dutch and
727 English usable expert profiles. Table 2 provides descriptive
statistics for the UvT Expert collection.
Universities tend to have a hierarchical structure that goes from
the faculty level, to departments, research groups, down to the 
individual researchers. In the UvT Expert collection we have 
information about the affiliations of researchers with faculties and
institutes, providing us with a two-level organizational hierarchy.
Tilburg University has 22 organizational units at the faculty level
(including the university office and several research institutes) and
71 departments, which amounts to 3.2 departments per faculty.
As to the topical hierarchy used by Webwijs, 131 of the 1491
topics are top nodes in the hierarchy. This hierarchy has an average
topic chain length of 2.65 and a maximum length of 7 topics.
6. EVALUATION
Below, we evaluate Section 4"s models for expert finding and
profiling onthe UvT Expert collection. We detail our research 
questions and experimental setup, and then present our results.
6.1 Research Questions
We address the following research questions. Both expert finding
and profiling rely on the estimations of p(q|ca). The question is
how the models compare on the different tasks, and in the setting of
the UvT Expert collection. In [4], Model 2 outperformed Model 1
on the W3C collection. How do they compare on our data set? And
how does Model 3 compare to Model 1? What about performance
differences between the two languages in our test collection?
6.2 Experimental Setup
The output of our models was evaluated against the self-assigned
topic labels, which were treated as relevance judgements. Results
were evaluated separately for English and Dutch. For English we
only used topics for which the Dutch translation was available; for
Dutch all topics were considered. The results were averaged for
the queries in the intersection of relevance judgements and results;
missing queries do not contribute a value of 0 to the scores.
We use standard information retrieval measures, such as Mean
Average Precision (MAP) and Mean Reciprocal Rank (MRR). We
also report the percentage of topics (%q) and candidates (%ca) 
covered, for the expert finding and profiling tasks, respectively.
6.3 Results
Table 1 shows the performance of Model 1, 2, and 3 on the 
expert finding and profiling tasks. The rows of the table correspond
to the various document types (RD, CD, PUB, and HP) and to their
combinations. RD+CD+PUB+HP is equivalent to the full 
collection and will be referred as the BASELINE of our experiments.
Looking at Table 1 we see that Model 2 performs the best across
the board. However, when the data is clean and very focused (RD),
Model 3 outperforms it in a number of cases. Model 1 has the
best coverage of candidates (%ca) and topics (%q). The various
document types differ in their characteristics and how they improve
the finding and profiling tasks. Expert profiling benefits much from
the clean data present in the RD and CD document types, while the
publications contribute the most to the expert finding task. Adding
the homepages does not prove to be particularly useful.
When we compare the results across languages, we find that the
coverage of English topics (%q) is higher than of the Dutch ones
for expert finding. Apart from that, the scores fall in the same range
for both languages. For the profiling task the coverage of the 
candidates (%ca) is very similar for both languages. However, the 
performance is substantially better for the English topics.
While it is hard to compare scores across collections, we 
conclude with a brief comparison of the absolute scores in Table 1 to
those reported in [3, 4] on the W3C test set (2005 edition). For
expert finding the MAP scores for Model 2 reported here are about
50% higher than the corresponding figures in [4], while our MRR
scores are slightly below those in [4]. For expert profiling, the 
differences are far more dramatic: the MAP scores for Model 2 
reported here are around 50% below the scores in [3], while the (best)
MRR scores are about the same as those in [3]. The cause for the
latter differences seems to reside in the number of knowledge areas
considered here-approx. 30 times more than in the W3C setting.
7. ADVANCED MODELS
Now that we have developed and assessed basic language 
modeling techniques for expertise retrieval, we turn to refined models
that exploit special features of our test collection.
7.1 Exploiting knowledge area similarity
One way to improve the scoring of a query given a candidate is
to consider what other requests the candidate would satisfy and use
them as further evidence to support the original query, proportional
Expert finding Expert profiling
Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3
%q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR
English
RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337
CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370
PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299
HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287
RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416
RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344
RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329
Dutch
RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327
CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380
PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294
HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255
RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384
RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339
RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328
Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations.
%q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the
expert profiling task). The top and bottom blocks correspond to English and Dutch respectively. The best scores are in boldface.
to how related the other requests are to the original query. This can
be modeled by interpolating between the p(q|ca) and the further
supporting evidence from all similar requests q , as follows:
p (q|ca) = λp(q|ca) + (1 − λ)
X
q
p(q|q )p(q |ca), (9)
where p(q|q ) represents the similarity between the two topics q
and q . To be able to work with similarity methods that are not
necessarily probabilities, we set p(q|q ) = w(q,q )
γ
, where γ is
a normalizing constant, such that γ =
P
q w(q , q ). We 
consider four methods for calculating the similarity score between two
topics. Three approaches are strictly content-based, and establish
similarity by examining co-occurrence patterns of topics within the
collection, while the last approach exploits the hierarchical 
structure of topical areas that may be present within an organization (see
[7] for further examples of integrating word relationships into 
language models).
The Kullback-Leibler (KL) divergence metric defined in Eq. 8
provides a measure of how different or similar two probability 
distributions are. A topic model is inferred for q and q using the
method presented in Section 4.1 to describe the query across the
entire vocabulary. Since a lower KL score means the queries are
more similar, we let w(q, q ) = max(KL(θq||·) − KL(θq||θq )).
Pointwise Mutual Information (PMI, [17]) is a measure of 
association used in information theory to determine the extent of 
independence between variables. The dependence between two queries
is reflected by the SI(q, q ) score, where scores greater than zero
indicate that it is likely that there is a dependence, which we take
to mean that the queries are likely to be similar:
SI(q, q ) = log
p(q, q )
p(q)p(q )
(10)
We estimate the probability of a topic p(q) using the number of
documents relevant to query q within the collection. The joint
probability p(q, q ) is estimated similarly, by using the 
concatenation of q and q as a query. To obtain p(q|q ), we then set
w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0,
because we are only interested in including queries that are similar.
The log-likelihood statistic provides another measure of 
dependence, which is more reliable than the pointwise mutual 
information measure [17]. Let k1 be the number of co-occurrences of q
and q , k2 the number of occurrences of q not co-occurring with q ,
n1 the total number of occurrences of q , and n2 the total number
of topic tokens minus the number of occurrences of q . Then, let
p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2),
(q, q ) = 2( (p1, k1, n1) + (p2, k2, n2)
− (p, k1, n1) − (p, k2, n2)),
where (p, n, k) = k log p + (n − k) log(1 − p). The higher
score indicate that queries are also likely to be similar, thus we set
w(q, q ) = (q, q ).
Finally, we also estimate the similarity of two topics based on
their distance within the topic hierarchy. The topic hierarchy is
viewed as a directed graph, and for all topic-pairs the shortest path
SP(q, q ) is calculated. We set the similarity score to be the 
reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ).
7.2 Contextual information
Given the hierarchy of an organization, the units to which a 
person belong are regarded as a context so as to compensate for data
sparseness. We model it as follows:
p (q|ca) =

1 −
P
ou∈OU(ca) λou

· p(q|ca)
+
P
ou∈OU(ca) λou · p(q|ou),
where OU(ca) is the set of organizational units of which 
candidate ca is a member of, and p(q|o) expresses the strength of the
association between query q and the unit ou. The latter probability
can be estimated using either of the three basic models, by simply
replacing ca with ou in the corresponding equations. An 
organizational unit is associated with all the documents that its members
have authored. That is, p(d|ou) = maxca∈ou p(d|ca).
7.3 A simple multilingual model
For knowledge institutes in Europe, academic or otherwise, a
multilingual (or at least bilingual) setting is typical. The following
model builds on a kind of independence assumption: there is no
spill-over of expertise/profiles across language boundaries. While a
simplification, this is a sensible first approach. That is: p (q|ca) =P
l∈L λl · p(ql|ca), where L is the set of languages used in the
collection, ql is the translation of the query q to language l, and λl is
a language specific smoothing parameter, such that
P
l∈L λl = 1.
8. ADVANCED MODELS: EVALUATION
In this section we present an experimental evaluation of our 
advanced models.
Expert finding Expert profiling
Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3
%q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR
English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329
Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328
Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411
Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates). Best scores for each
model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.
Method Model 1 Model 2 Model 3
MAP MRR MAP MRR MAP MRR
English
BASELINE 0.296 0.454 0.339 0.509 0.221 0.333
KLDIV 0.291 0.453 0.327 0.503 0.219 0.330
PMI 0.291 0.453 0.337 0.509 0.219 0.331
LL 0.319 0.490 0.360 0.524 0.233 0.368
HDIST 0.299 0.465 0.346 0.537 0.219 0.332
Dutch
BASELINE 0.240 0.350 0.271 0.403 0.227 0.389
KLDIV 0.239 0.347 0.253 0.386 0.224 0.385
PMI 0.239 0.350 0.260 0.392 0.227 0.389
LL 0.255 0.372 0.281 0.425 0.231 0.389
HDIST 0.253 0.365 0.271 0.407 0.236 0.402
Method Model 1 Model 2 Model 3
MAP MRR MAP MRR MAP MRR
English
BASELINE 0.485 0.546 0.499 0.548 0.381 0.416
KLDIV 0.510 0.564 0.513 0.558 0.381 0.416
PMI 0.486 0.546 0.495 0.542 0.407 0.451
LL 0.558 0.589 0.586 0.617 0.408 0.453
HDIST 0.507 0.567 0.512 0.563 0.386 0.420
Dutch
BASELINE 0.263 0.313 0.294 0.358 0.262 0.315
KLDIV 0.284 0.336 0.271 0.321 0.261 0.314
PMI 0.265 0.317 0.265 0.316 0.273 0.330
LL 0.312 0.351 0.330 0.377 0.284 0.331
HDIST 0.280 0.327 0.288 0.341 0.266 0.321
Table 4: Performance on the expert finding (top) and profiling
(bottom) tasks, using knowledge area similarities. Runs were
evaluated on the main topics set. Best scores are in boldface.
8.1 Research Questions
Our questions follow the refinements presented in the preceding
section: Does exploiting the knowledge area similarity improve 
effectiveness? Which of the various methods for capturing word 
relationships is most effective? Furthermore, is our way of bringing
in contextual information useful? For which tasks? And finally, is
our simple way of combining the monolingual scores sufficient for
obtaining significant improvements?
8.2 Experimental setup
Given that the self-assessments are also sparse in our collection,
in order to be able to measure differences between the various 
models, we selected a subset of topics, and evaluated (some of the) runs
only on this subset. This set is referred as main topics, and consists
of topics that are located at the top level of the topical hierarchy. (A
main topic has subtopics, but is not a subtopic of any other topic.)
This main set consists of 132 Dutch and 119 English topics. The
relevance judgements were restricted to the main topic set, but were
not expanded with subtopics.
8.3 Exploiting knowledge area similarity
Table 4 presents the results. The four methods used for 
estimating knowledge-area similarity are KL divergence (KLDIV), 
PointLang. Topics Model 1 Model 2 Model 3
MAP MRR MAP MRR MAP MRR
Expert finding
UK ALL 0.423 0.545 0.654 0.799 0.494 0.629
UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699
NL ALL 0.439 0.560 0.672 0.826 0.480 0.630
NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655
Expert profiling
UK ALL 0.240 0.640 0.306 0.778 0.223 0.616
UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587
NL ALL 0.203 0.716 0.254 0.770 0.183 0.627
NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549
Table 5: Evaluating the context models on organizational units.
wise mutual information (PMI), log-likelihood (LL), and distance
within topic hierarchy (HDIST). We managed to improve upon the
baseline in all cases, but the improvement is more noticeable for
the profiling task. For both tasks, the LL method performed best.
The content-based approaches performed consistently better than
HDIST.
8.4 Contextual information
A two level hierarchy of organizational units (faculties and 
institutes) is available in the UvT Expert collection. The unit a person
belongs to is used as a context for that person. First, we evaluated
the models of the organizational units, using all topics (ALL) and
only the main topics (MAIN). An organizational unit is considered
to be relevant for a given topic (or vice versa) if at least one member
of the unit selected the given topic as an expertise area.
Table 5 reports on the results. As far as expert finding goes, given
a topic, the corresponding organizational unit can be identified with
high precision. However, the expert profiling task shows a different
picture: the scores are low, and the task seems hard. The 
explanation may be that general concepts (i.e., our main topics) may belong
to several organizational units.
Second, we performed another evaluation, where we combined
the contextual models with the candidate models (to score 
candidates again). Table 6 reports on the results. We find a positive
impact of the context models only for expert finding. Noticably,
for expert finding (and Model 1), it improves over 50% (for 
English) and over 70% (for Dutch) on MAP. The poor performance
on expert profiling may be due to the fact that context models alone
did not perform very well on the profiling task to begin with.
8.5 Multilingual models
In this subsection we evaluate the method for combining 
results across multiple languages that we described in Section 7.3.
In our setting the set of languages consists of English and Dutch:
L = {UK, NL}. The weights on these languages were set to be
identical (λUK = λNL = 0.5). We performed experiments with
various λ settings, but did not observe significant differences in
performance.
Table 3 reports on the multilingual results, where performance is
evaluated on the full topic set. All three models significantly 
imLang. Method Model 1 Model 2 Model 3
MAP MRR MAP MRR MAP MRR
Expert finding
UK BL 0.296 0.454 0.339 0.509 0.221 0.333
UK CT 0.330 0.491 0.342 0.500 0.228 0.342
NL BL 0.240 0.350 0.271 0.403 0.227 0.389
NL CT 0.251 0.382 0.267 0.410 0.246 0.404
Expert profiling
UK BL 0.485 0.546 0.499 0.548 0.381 0.416
UK CT 0.562 0.620 0.508 0.558 0.440 0.486
NL BL 0.263 0.313 0.294 0.358 0.262 0.315
NL CT 0.330 0.384 0.317 0.387 0.294 0.345
Table 6: Performance of the context models (CT) compared to
the baseline (BL). Best scores are in boldface.
proved over all measures for both tasks. The coverage of topics
and candidates for the expert finding and profiling tasks, 
respectively, is close to 100% in all cases. The relative improvement
of the precision scores ranges from 10% to 80%. These scores
demonstrate that despite its simplicity, our method for combining
results over multiple languages achieves substantial improvements
over the baseline.
9. CONCLUSIONS
In this paper we focused on expertise retrieval (expert finding
and profiling) in a new setting of a typical knowledge-intensive 
organization in which the available data is of high quality, 
multilingual, and covering a broad range of expertise area. Typically, the
amount of available data in such an organization (e.g., a university,
a research institute, or a research lab) is limited when compared to
the W3C collection that has mostly been used for the experimental
evaluation of expertise retrieval so far.
To examine expertise retrieval in this setting, we introduced (and
released) the UvT Expert collection as a representative case of such
knowledge intensive organizations. The new collection reflects the
typical properties of knowledge-intensive institutes noted above and
also includes several features which may are potentially useful for
expertise retrieval, such as topical and organizational structure.
We evaluated how current state-of-the-art models for expert 
finding and profiling performed in this new setting and then refined
these models in order to try and exploit the different 
characteristics within the data environment (language, topicality, and 
organizational structure). We found that current models of expertise
retrieval generalize well to this new environment; in addition we
found that refining the models to account for the differences results
in significant improvements, thus making up for problems caused
by data sparseness issues.
Future work includes setting up manual assessments of 
automatically generated profiles by the employees themselves, especially in
cases where the employees have not provided a profile themselves.
10. ACKNOWLEDGMENTS
Krisztian Balog was supported by the Netherlands Organisation
for Scientific Research (NWO) under project number 220-80-001.
Maarten de Rijke was also supported by NWO under project 
numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 
600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 
640.001.501, 640.002.501, and by the E.U. IST programme of the 6th
FP for RTD under project MultiMATCH contract IST-033104.
The work of Toine Bogers and Antal van den Bosch was funded
by the IOP-MMI-program of SenterNovem / The Dutch Ministry
of Economic Affairs, as part of the `A Propos project.
11. REFERENCES
[1] L. Azzopardi. Incorporating Context in the Language Modeling
Framework for ad-hoc Information Retrieval. PhD thesis, University
of Paisley, 2005.
[2] K. Balog and M. de Rijke. Finding similar experts. In This volume,
2007.
[3] K. Balog and M. de Rijke. Determining expert profiles (with an 
application to expert finding). In IJCAI "07: Proc. 20th Intern. Joint Conf.
on Artificial Intelligence, pages 2657-2662, 2007.
[4] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert
finding in enterprise corpora. In SIGIR "06: Proc. 29th annual 
intern. ACM SIGIR conf. on Research and development in information
retrieval, pages 43-50, 2006.
[5] I. Becerra-Fernandez. The role of artificial intelligence technologies
in the implementation of people-finder knowledge management 
systems. In AAAI Workshop on Bringing Knowledge to Business 
Processes, March 2000.
[6] C. S. Campbell, P. P. Maglio, A. Cozzi, and B. Dom. Expertise 
identification using email communications. In CIKM "03: Proc. twelfth
intern. conf. on Information and knowledge management, pages 
528531, 2003.
[7] G. Cao, J.-Y. Nie, and J. Bai. Integrating word relationships into 
language models. In SIGIR "05: Proc. 28th annual intern. ACM SIGIR
conf. on Research and development in information retrieval, pages
298-305, 2005.
[8] T. M. Cover and J. A. Thomas. Elements of Information Theory.
Wiley-Interscience, 1991.
[9] N. Craswell, D. Hawking, A. M. Vercoustre, and P. Wilkins. P@noptic
expert: Searching for experts not just for documents. In Ausweb, 2001.
[10] N. Craswell, A. de Vries, and I. Soboroff. Overview of the 
TREC2005 Enterprise Track. In The Fourteenth Text REtrieval Conf. Proc.
(TREC 2005), 2006.
[11] T. H. Davenport and L. Prusak. Working Knowledge: How 
Organizations Manage What They Know. Harvard Business School Press,
Boston, MA, 1998.
[12] T. Dunning. Accurate methods for the statistics of surprise and 
coincidence. Computational Linguistics, 19(1):61-74, 1993.
[13] E. Filatova and J. Prager. Tell me what you do and I"ll tell you what
you are: Learning occupation-related activities for biographies. In
HLT/EMNLP, 2005.
[14] V. Lavrenko and W. B. Croft. Relevance based language models. In
SIGIR "01: Proc. 24th annual intern. ACM SIGIR conf. on Research
and development in information retrieval, pages 120-127, 2001.
[15] V. Lavrenko, M. Choquette, and W. B. Croft. Cross-lingual relevance
models. In SIGIR "02: Proc. 25th annual intern. ACM SIGIR conf. on
Research and development in information retrieval, pages 175-182,
2002.
[16] C. Macdonald and I. Ounis. Voting for candidates: adapting data 
fusion techniques for an expert search task. In CIKM "06: Proc. 15th
ACM intern. conf. on Information and knowledge management, pages
387-396, 2006.
[17] C. Manning and H. Sch¨utze. Foundations of Statistical Natural 
Language Processing. The MIT Press, 1999.
[18] A. Mockus and J. D. Herbsleb. Expertise browser: a quantitative 
approach to identifying expertise. In ICSE "02: Proc. 24th Intern. Conf.
on Software Engineering, pages 503-512, 2002.
[19] D. Petkova and W. B. Croft. Hierarchical language models for expert
finding in enterprise corpora. In Proc. ICTAI 2006, pages 599-608,
2006.
[20] I. Soboroff, A. de Vries, and N. Craswell. Overview of the TREC
2006 Enterprise Track. In TREC 2006 Working Notes, 2006.
[21] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information
retrieval with document expansion. In HLT-NAACL 2006, 2006.
[22] TREC. Enterprise track, 2005. URL: http://www.ins.cwi.
nl/projects/trec-ent/wiki/.
[23] G. van Noord. TextCat Language Guesser. URL: http://www.
let.rug.nl/˜vannoord/TextCat/.
[24] W3C. The W3C test collection, 2005. URL: http://research.
microsoft.com/users/nickcr/w3c-summary.html.
Unified Utility Maximization Framework for Resource
Selection
Luo Si
Language Technology Inst.
School of Compute Science
Carnegie Mellon University
Pittsburgh, PA 15213
lsi@cs.cmu.edu
Jamie Callan
Language Technology Inst.
School of Compute Science
Carnegie Mellon University
Pittsburgh, PA 15213
callan@cs.cmu.edu
ABSTRACT
This paper presents a unified utility framework for resource
selection of distributed text information retrieval. This new
framework shows an efficient and effective way to infer the
probabilities of relevance of all the documents across the text
databases. With the estimated relevance information, resource
selection can be made by explicitly optimizing the goals of
different applications. Specifically, when used for database
recommendation, the selection is optimized for the goal of 
highrecall (include as many relevant documents as possible in the
selected databases); when used for distributed document
retrieval, the selection targets the high-precision goal (high
precision in the final merged list of documents). This new model
provides a more solid framework for distributed information
retrieval. Empirical studies show that it is at least as effective as
other state-of-the-art algorithms.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]:
General Terms
Algorithms
1. INTRODUCTION
Conventional search engines such as Google or AltaVista use
ad-hoc information retrieval solution by assuming all the
searchable documents can be copied into a single centralized
database for the purpose of indexing. Distributed information
retrieval, also known as federated search [1,4,7,11,14,22] is
different from ad-hoc information retrieval as it addresses the
cases when documents cannot be acquired and stored in a single
database. For example, Hidden Web contents (also called
invisible or deep Web contents) are information on the Web
that cannot be accessed by the conventional search engines.
Hidden web contents have been estimated to be 2-50 [19] times
larger than the contents that can be searched by conventional
search engines. Therefore, it is very important to search this type
of valuable information.
The architecture of distributed search solution is highly
influenced by different environmental characteristics. In a small
local area network such as small company environments, the
information providers may cooperate to provide corpus statistics
or use the same type of search engines. Early distributed
information retrieval research focused on this type of
cooperative environments [1,8]. On the other side, in a wide
area network such as very large corporate environments or on
the Web there are many types of search engines and it is difficult
to assume that all the information providers can cooperate as
they are required. Even if they are willing to cooperate in these
environments, it may be hard to enforce a single solution for all
the information providers or to detect whether information
sources provide the correct information as they are required.
Many applications fall into the latter type of uncooperative
environments such as the Mind project [16] which integrates
non-cooperating digital libraries or the QProber system [9]
which supports browsing and searching of uncooperative hidden
Web databases. In this paper, we focus mainly on uncooperative
environments that contain multiple types of independent search
engines.
There are three important sub-problems in distributed
information retrieval. First, information about the contents of
each individual database must be acquired (resource
representation) [1,8,21]. Second, given a query, a set of
resources must be selected to do the search (resource selection)
[5,7,21]. Third, the results retrieved from all the selected
resources have to be merged into a single final list before it can
be presented to the end user (retrieval and results merging)
[1,5,20,22].
Many types of solutions exist for distributed information
retrieval. Invisible-web.net1
provides guided browsing of hidden
Web databases by collecting the resource descriptions of these
databases and building hierarchies of classes that group them by
similar topics. A database recommendation system goes a step
further than a browsing system like Invisible-web.net by
recommending most relevant information sources to users"
queries. It is composed of the resource description and the
resource selection components. This solution is useful when the
users want to browse the selected databases by themselves
instead of asking the system to retrieve relevant documents
automatically. Distributed document retrieval is a more
sophisticated task. It selects relevant information sources for
users" queries as the database recommendation system does.
Furthermore, users" queries are forwarded to the corresponding
selected databases and the returned individual ranked lists are
merged into a single list to present to the users.
The goal of a database recommendation system is to select a
small set of resources that contain as many relevant documents
as possible, which we call a high-recall goal. On the other side,
the effectiveness of distributed document retrieval is often
measured by the Precision of the final merged document result
list, which we call a high-precision goal. Prior research
indicated that these two goals are related but not identical [4,21].
However, most previous solutions simply use effective resource
selection algorithm of database recommendation system for
distributed document retrieval system or solve the inconsistency
with heuristic methods [1,4,21].
This paper presents a unified utility maximization framework to
integrate the resource selection problem of both database
recommendation and distributed document retrieval together by
treating them as different optimization goals.
First, a centralized sample database is built by randomly
sampling a small amount of documents from each database with
query-based sampling [1]; database size statistics are also
estimated [21]. A logistic transformation model is learned off
line with a small amount of training queries to map the
centralized document scores in the centralized sample database
to the corresponding probabilities of relevance.
Second, after a new query is submitted, the query can be used to
search the centralized sample database which produces a score
for each sampled document. The probability of relevance for
each document in the centralized sample database can be
estimated by applying the logistic model to each document"s
score. Then, the probabilities of relevance of all the (mostly
unseen) documents among the available databases can be
estimated using the probabilities of relevance of the documents
in the centralized sample database and the database size
estimates.
For the task of resource selection for a database
recommendation system, the databases can be ranked by the
expected number of relevant documents to meet the high-recall
goal. For resource selection for a distributed document retrieval
system, databases containing a small number of documents with
large probabilities of relevance are favored over databases
containing many documents with small probabilities of
relevance. This selection criterion meets the high-precision goal
of distributed document retrieval application. Furthermore, the
Semi-supervised learning (SSL) [20,22] algorithm is applied to
merge the returned documents into a final ranked list.
The unified utility framework makes very few assumptions and
works in uncooperative environments. Two key features make it
a more solid model for distributed information retrieval: i) It
formalizes the resource selection problems of different
applications as various utility functions, and optimizes the utility
functions to achieve the optimal results accordingly; and ii) It
shows an effective and efficient way to estimate the probabilities
of relevance of all documents across databases. Specifically, the
framework builds logistic models on the centralized sample
database to transform centralized retrieval scores to the
corresponding probabilities of relevance and uses the centralized
sample database as the bridge between individual databases and
the logistic model. The human effort (relevance judgment)
required to train the single centralized logistic model does not
scale with the number of databases. This is a large advantage
over previous research, which required the amount of human
effort to be linear with the number of databases [7,15].
The unified utility framework is not only more theoretically
solid but also very effective. Empirical studies show the new
model to be at least as accurate as the state-of-the-art algorithms
in a variety of configurations.
The next section discusses related work. Section 3 describes the
new unified utility maximization model. Section 4 explains our
experimental methodology. Sections 5 and 6 present our
experimental results for resource selection and document
retrieval. Section 7 concludes.
2. PRIOR RESEARCH
There has been considerable research on all the sub-problems of
distributed information retrieval. We survey the most related
work in this section.
The first problem of distributed information retrieval is resource
representation. The STARTS protocol is one solution for
acquiring resource descriptions in cooperative environments [8].
However, in uncooperative environments, even the databases are
willing to share their information, it is not easy to judge whether
the information they provide is accurate or not. Furthermore, it
is not easy to coordinate the databases to provide resource
representations that are compatible with each other. Thus, in
uncooperative environments, one common choice is query-based
sampling, which randomly generates and sends queries to
individual search engines and retrieves some documents to build
the descriptions. As the sampled documents are selected by
random queries, query-based sampling is not easily fooled by
any adversarial spammer that is interested to attract more traffic.
Experiments have shown that rather accurate resource
descriptions can be built by sending about 80 queries and
downloading about 300 documents [1].
Many resource selection algorithms such as gGlOSS/vGlOSS
[8] and CORI [1] have been proposed in the last decade. The
CORI algorithm represents each database by its terms, the
document frequencies and a small number of corpus statistics
(details in [1]). As prior research on different datasets has shown
the CORI algorithm to be the most stable and effective of the
three algorithms [1,17,18], we use it as a baseline algorithm in
this work. The relevant document distribution estimation
(ReDDE [21]) resource selection algorithm is a recent algorithm
that tries to estimate the distribution of relevant documents
across the available databases and ranks the databases
accordingly. Although the ReDDE algorithm has been shown to
be effective, it relies on heuristic constants that are set
empirically [21].
The last step of the document retrieval sub-problem is results
merging, which is the process of transforming database-specific
33
document scores into comparable database-independent
document scores. The semi supervised learning (SSL) [20,22]
result merging algorithm uses the documents acquired by 
querybased sampling as training data and linear regression to learn the
database-specific, query-specific merging models. These linear
models are used to convert the database-specific document
scores into the approximated centralized document scores. The
SSL algorithm has been shown to be effective [22]. It serves as
an important component of our unified utility maximization
framework (Section 3).
In order to achieve accurate document retrieval results, many
previous methods simply use resource selection algorithms that
are effective of database recommendation system. But as
pointed out above, a good resource selection algorithm
optimized for high-recall may not work well for document
retrieval, which targets the high-precision goal. This type of
inconsistency has been observed in previous research [4,21].
The research in [21] tried to solve the problem with a heuristic
method.
The research most similar to what we propose here is the
decision-theoretic framework (DTF) [7,15]. This framework
computes a selection that minimizes the overall costs (e.g.,
retrieval quality, time) of document retrieval system and several
methods [15] have been proposed to estimate the retrieval
quality. However, two points distinguish our research from the
DTF model. First, the DTF is a framework designed specifically
for document retrieval, but our new model integrates two
distinct applications with different requirements (database
recommendation and distributed document retrieval) into the
same unified framework. Second, the DTF builds a model for
each database to calculate the probabilities of relevance. This
requires human relevance judgments for the results retrieved
from each database. In contrast, our approach only builds one
logistic model for the centralized sample database. The
centralized sample database can serve as a bridge to connect the
individual databases with the centralized logistic model, thus the
probabilities of relevance of documents in different databases
can be estimated. This strategy can save large amount of human
judgment effort and is a big advantage of the unified utility
maximization framework over the DTF especially when there
are a large number of databases.
3. UNIFIED UTILITY MAXIMIZATION
FRAMEWORK
The Unified Utility Maximization (UUM) framework is based
on estimating the probabilities of relevance of the (mostly
unseen) documents available in the distributed search
environment. In this section we describe how the probabilities of
relevance are estimated and how they are used by the Unified
Utility Maximization model. We also describe how the model
can be optimized for the high-recall goal of a database
recommendation system and the high-precision goal of a
distributed document retrieval system.
3.1 Estimating Probabilities of Relevance
As pointed out above, the purpose of resource selection is 
highrecall and the purpose of document retrieval is high-precision. In
order to meet these diverse goals, the key issue is to estimate the
probabilities of relevance of the documents in various databases.
This is a difficult problem because we can only observe a
sample of the contents of each database using query-based
sampling. Our strategy is to make full use of all the available
information to calculate the probability estimates.
3.1.1 Learning Probabilities of Relevance
In the resource description step, the centralized sample database
is built by query-based sampling and the database sizes are
estimated using the sample-resample method [21]. At the same
time, an effective retrieval algorithm (Inquery [2]) is applied on
the centralized sample database with a small number (e.g., 50)
of training queries. For each training query, the CORI resource
selection algorithm [1] is applied to select some number
(e.g., 10) of databases and retrieve 50 document ids from each
database. The SSL results merging algorithm [20,22] is used to
merge the results. Then, we can download the top 50 documents
in the final merged list and calculate their corresponding
centralized scores using Inquery and the corpus statistics of the
centralized sample database. The centralized scores are further
normalized (divided by the maximum centralized score for each
query), as this method has been suggested to improve estimation
accuracy in previous research [15]. Human judgment is acquired
for those documents and a logistic model is built to transform
the normalized centralized document scores to probabilities of
relevance as follows:
( )
))(exp(1
))(exp(
|)( _
_
dSba
dSba
drelPdR
ccc
ccc
++
+
== (1)
where )(
_
dSc
is the normalized centralized document score and
ac and bc are the two parameters of the logistic model. These two
parameters are estimated by maximizing the probabilities of
relevance of the training queries. The logistic model provides us
the tool to calculate the probabilities of relevance from
centralized document scores.
3.1.2 Estimating Centralized Document Scores
When the user submits a new query, the centralized document
scores of the documents in the centralized sample database are
calculated. However, in order to calculate the probabilities of
relevance, we need to estimate centralized document scores for
all documents across the databases instead of only the sampled
documents. This goal is accomplished using: the centralized
scores of the documents in the centralized sample database, and
the database size statistics.
We define the database scale factor for the ith
database as the
ratio of the estimated database size and the number of
documents sampled from this database as follows:
^
_
i
i
i
db
db
db samp
N
SF
N
= (2)
where
^
idbN is the estimated database size and _idb sampN is the
number of documents from the ith
database in the centralized
sample database. The intuition behind the database scale factor
is that, for a database whose scale factor is 50, if one document
from this database in the centralized sample database has a
centralized document score of 0.5, we may guess that there are
about 50 documents in that database which have scores of about
0.5. Actually, we can apply a finer non-parametric linear
interpolation method to estimate the centralized document score
curve for each database. Formally, we rank all the sampled
documents from the ith
database by their centralized document
34
scores to get the sampled centralized document score list
{Sc(dsi1), Sc(dsi2), Sc(dsi3),…..} for the ith
database; we assume
that if we could calculate the centralized document scores for all
the documents in this database and get the complete centralized
document score list, the top document in the sampled list would
have rank SFdbi/2, the second document in the sampled list
would rank SFdbi3/2, and so on. Therefore, the data points of
sampled documents in the complete list are: {(SFdbi/2, Sc(dsi1)),
(SFdbi3/2, Sc(dsi2)), (SFdbi5/2, Sc(dsi3)),…..}. Piecewise linear
interpolation is applied to estimate the centralized document
score curve, as illustrated in Figure 1. The complete centralized
document score list can be estimated by calculating the values of
different ranks on the centralized document curve as:
],1[,)(S
^^
c idbij Njd ∈ .
It can be seen from Figure 1 that more sample data points
produce more accurate estimates of the centralized document
score curves. However, for databases with large database scale
ratios, this kind of linear interpolation may be rather inaccurate,
especially for the top ranked (e.g., [1, SFdbi/2]) documents.
Therefore, an alternative solution is proposed to estimate the
centralized document scores of the top ranked documents for
databases with large scale ratios (e.g., larger than 100).
Specifically, a logistic model is built for each of these databases.
The logistic model is used to estimate the centralized document
score of the top 1 document in the corresponding database by
using the two sampled documents from that database with
highest centralized scores.
))()(exp(1
))()(exp(
)(
22110
22110
^
1
iciicii
iciicii
ic
dsSdsS
dsSdsS
dS
ααα
ααα
+++
++
= (3)
0iα , 1iα and 2iα are the parameters of the logistic model. For
each training query, the top retrieved document of each database
is downloaded and the corresponding centralized document
score is calculated. Together with the scores of the top two
sampled documents, these parameters can be estimated.
After the centralized score of the top document is estimated, an
exponential function is fitted for the top part ([1, SFdbi/2]) of the
centralized document score curve as:
]2/,1[)*exp()( 10
^
idbiiijc SFjjdS ∈+= ββ (4)
^
0 1 1log( ( ))i c i iS dβ β= − (5)
)12/(
))(log()((log(
^
11
1
−
−
=
idb
icic
i
SF
dSdsS
β (6)
The two parameters 0iβ and 1iβ are fitted to make sure the
exponential function passes through the two points (1,
^
1)( ic dS )
and (SFdbi/2, Sc(dsi1)). The exponential function is only used to
adjust the top part of the centralized document score curve and
the lower part of the curve is still fitted with the linear
interpolation method described above. The adjustment by fitting
exponential function of the top ranked documents has been
shown empirically to produce more accurate results.
From the centralized document score curves, we can estimate
the complete centralized document score lists accordingly for all
the available databases. After the estimated centralized
document scores are normalized, the complete lists of
probabilities of relevance can be constructed out of the complete
centralized document score lists by Equation 1. Formally for the
ith
database, the complete list of probabilities of relevance is:
],1[,)(R
^^
idbij Njd ∈ .
3.2 The Unified Utility Maximization Model
In this section, we formally define the new unified utility
maximization model, which optimizes the resource selection
problems for two goals of high-recall (database
recommendation) and high-precision (distributed document
retrieval) in the same framework.
In the task of database recommendation, the system needs to
decide how to rank databases. In the task of document retrieval,
the system not only needs to select the databases but also needs
to decide how many documents to retrieve from each selected
database. We generalize the database recommendation selection
process, which implicitly recommends all documents in every
selected database, as a special case of the selection decision for
the document retrieval task. Formally, we denote di as the
number of documents we would like to retrieve from the ith
database and ,.....},{ 21 ddd = as a selection action for all the
databases.
The database selection decision is made based on the complete
lists of probabilities of relevance for all the databases. The
complete lists of probabilities of relevance are inferred from all
the available information specifically sR , which stands for the
resource descriptions acquired by query-based sampling and the
database size estimates acquired by sample-resample; cS stands
for the centralized document scores of the documents in the
centralized sample database.
If the method of estimating centralized document scores and
probabilities of relevance in Section 3.1 is acceptable, then the
most probable complete lists of probabilities of relevance can be
derived and we denote them as 1
^ ^
*
1{(R( ), [1, ]),dbjd j Nθ = ∈
2
^ ^
2(R( ), [1, ]),.......}dbjd j N∈ . Random vector
 
denotes an
arbitrary set of complete lists of probabilities of relevance and
),|( cs SRP θ as the probability of generating this set of lists.
Finally, to each selection action d and a set of complete lists of
Figure 1. Linear interpolation construction of the complete
centralized document score list (database scale factor is 50).
35
probabilities of relevance θ , we associate a utility function
),( dU θ which indicates the benefit from making the d
selection when the true complete lists of probabilities of
relevance are θ .
Therefore, the selection decision defined by the Bayesian
framework is:
θθθ
θ
dSRPdUd cs
d
).|(),(maxarg
*
= (7)
One common approach to simplify the computation in the
Bayesian framework is to only calculate the utility function at
the most probable parameter values instead of calculating the
whole expectation. In other words, we only need to calculate
),( *
dU θ and Equation 7 is simplified as follows:
),(maxarg *
*
θdUd
d
= (8)
This equation serves as the basic model for both the database
recommendation system and the document retrieval system.
3.3 Resource Selection for High-Recall
High-recall is the goal of the resource selection algorithm in
federated search tasks such as database recommendation. The
goal is to select a small set of resources (e.g., less than Nsdb
databases) that contain as many relevant documents as possible,
which can be formally defined as:
=
=
i
N
j
iji
idb
ddIdU
^
1
^
*
)(R)(),( θ (9)
I(di) is the indicator function, which is 1 when the ith
database is
selected and 0 otherwise. Plug this equation into the basic model
in Equation 8 and associate the selected database number
constraint to obtain the following:
sdb
i
i
i
N
j
iji
d
NdItoSubject
ddId
idb
=
=
=
)(:
)(R)(maxarg
^
1
^*
(10)
The solution of this optimization problem is very simple. We
can calculate the expected number of relevant documents for
each database as follows:
=
=
idb
i
N
j
ijRd dN
^
1
^^
)(R (11)
The Nsdb databases with the largest expected number of relevant
documents can be selected to meet the high-recall goal. We call
this the UUM/HR algorithm (Unified Utility Maximization for
High-Recall).
3.4 Resource Selection for High-Precision
High-Precision is the goal of resource selection algorithm in
federated search tasks such as distributed document retrieval. It
is measured by the Precision at the top part of the final merged
document list. This high-precision criterion is realized by the
following utility function, which measures the Precision of
retrieved documents from the selected databases.
=
=
i
d
j
iji
i
ddIdU
1
^
*
)(R)(),( θ (12)
Note that the key difference between Equation 12 and Equation
9 is that Equation 9 sums up the probabilities of relevance of all
the documents in a database, while Equation 12 only considers a
much smaller part of the ranking. Specifically, we can calculate
the optimal selection decision by:
=
=
i
d
j
iji
d
i
ddId
1
^*
)(R)(maxarg (13)
Different kinds of constraints caused by different characteristics
of the document retrieval tasks can be associated with the above
optimization problem. The most common one is to select a fixed
number (Nsdb) of databases and retrieve a fixed number (Nrdoc) of
documents from each selected database, formally defined as:
0,
)(:
)(R)(maxarg
1
^*
≠=
=
=
=
irdoci
sdb
i
i
i
d
j
iji
d
difNd
NdItoSubject
ddId
i
(14)
This optimization problem can be solved easily by calculating
the number of expected relevant documents in the top part of the
each database"s complete list of probabilities of relevance:
=
=
rdoc
i
N
j
ijRdTop dN
1
^^
_ )(R (15)
Then the databases can be ranked by these values and selected.
We call this the UUM/HP-FL algorithm (Unified Utility
Maximization for High-Precision with Fixed Length document
rankings from each selected database).
A more complex situation is to vary the number of retrieved
documents from each selected database. More specifically, we
allow different selected databases to return different numbers of
documents. For simplification, the result list lengths are required
to be multiples of a baseline number 10. (This value can also be
varied, but for simplification it is set to 10 in this paper.) This
restriction is set to simulate the behavior of commercial search
engines on the Web. (Search engines such as Google and
AltaVista return only 10 or 20 document ids for every result
page.) This procedure saves the computation time of calculating
optimal database selection by allowing the step of dynamic
programming to be 10 instead of 1 (more detail is discussed
latterly). For further simplification, we restrict to select at most
100 documents from each database (di<=100) Then, the
selection optimization problem is formalized as follows:
]10..,,2,1,0[,*10
)(:
)(R)(maxarg
_
1
^*
∈=
=
=
=
=
kkd
Nd
NdItoSubject
ddId
i
rdocTotal
i
i
sdb
i
i
i
d
j
iji
d
i
(16)
NTotal_rdoc is the total number of documents to be retrieved.
Unfortunately, there is no simple solution for this optimization
problem as there are for Equations 10 and 14. However, a
36
dynamic programming algorithm can be applied to calculate the
optimal solution. The basic steps of this dynamic programming
method are described in Figure 2. As this algorithm allows
retrieving result lists of varying lengths from each selected
database, it is called UUM/HP-VL algorithm.
After the selection decisions are made, the selected databases are
searched and the corresponding document ids are retrieved from
each database. The final step of document retrieval is to merge
the returned results into a single ranked list with the 
semisupervised learning algorithm. It was pointed out before that the
SSL algorithm maps the database-specific scores into the
centralized document scores and builds the final ranked list
accordingly, which is consistent with all our selection
procedures where documents with higher probabilities of
relevance (thus higher centralized document scores) are selected.
4. EXPERIMENTAL METHODOLOGY
4.1 Testbeds
It is desirable to evaluate distributed information retrieval
algorithms with testbeds that closely simulate the real world
applications.
The TREC Web collections WT2g or WT10g [4,13] provide a
way to partition documents by different Web servers. In this
way, a large number (O(1000)) of databases with rather diverse
contents could be created, which may make this testbed a good
candidate to simulate the operational environments such as open
domain hidden Web. However, two weakness of this testbed are:
i) Each database contains only a small amount of document (259
documents by average for WT2g) [4]; and ii) The contents of
WT2g or WT10g are arbitrarily crawled from the Web. It is not
likely for a hidden Web database to provide personal homepages
or web pages indicating that the pages are under construction
and there is no useful information at all. These types of web
pages are contained in the WT2g/WT10g datasets. Therefore,
the noisy Web data is not similar with that of high-quality
hidden Web database contents, which are usually organized by
domain experts.
Another choice is the TREC news/government data [1,15,17,
18,21]. TREC news/government data is concentrated on
relatively narrow topics. Compared with TREC Web data: i) The
news/government documents are much more similar to the
contents provided by a topic-oriented database than an arbitrary
web page, ii) A database in this testbed is larger than that of
TREC Web data. By average a database contains thousands of
documents, which is more realistic than a database of TREC
Web data with about 250 documents. As the contents and sizes
of the databases in the TREC news/government testbed are more
similar with that of a topic-oriented database, it is a good
candidate to simulate the distributed information retrieval
environments of large organizations (companies) or 
domainspecific hidden Web sites, such as West that provides access to
legal, financial and news text databases [3]. As most current
distributed information retrieval systems are developed for the
environments of large organizations (companies) or 
domainspecific hidden Web other than open domain hidden Web,
TREC news/government testbed was chosen in this work.
Trec123-100col-bysource testbed is one of the most used TREC
news/government testbed [1,15,17,21]. It was chosen in this
work. Three testbeds in [21] with skewed database size
distributions and different types of relevant document
distributions were also used to give more thorough simulation
for real environments.
Trec123-100col-bysource: 100 databases were created from
TREC CDs 1, 2 and 3. They were organized by source and
publication date [1]. The sizes of the databases are not skewed.
Details are in Table 1.
Three testbeds built in [21] were based on the 
trec123-100colbysource testbed. Each testbed contains many small databases
and two large databases created by merging about 10-20 small
databases together.
Input: Complete lists of probabilities of relevance for all
the |DB| databases.
Output: Optimal selection solution for Equation 16.
i) Create the three-dimensional array:
Sel (1..|DB|, 1..NTotal_rdoc/10, 1..Nsdb)
Each Sel (x, y, z) is associated with a selection
decision xyzd , which represents the best selection
decision in the condition: only databases from number 1
to number x are considered for selection; totally y*10
documents will be retrieved; only z databases are
selected out of the x database candidates. And
Sel (x, y, z) is the corresponding utility value by
choosing the best selection.
ii) Initialize Sel (1, 1..NTotal_rdoc/10, 1..Nsdb) with only the
estimated relevance information of the 1st
database.
iii) Iterate the current database candidate i from 2 to |DB|
For each entry Sel (i, y, z):
Find k such that:
)10,min(1:
))()1,,1((maxarg
*10
^
*
yktosubject
dRzkyiSelk
kj
ij
k
≤≤
+−−−=
≤
),,1())()1,,1((
*
*10
^
*
zyiSeldRzkyiSelIf
kj
ij −>+−−−
≤
This means that we should retrieve *
10 k∗ documents
from the ith
database, otherwise we should not select this
database and the previous best solution Sel (i-1, y, z)
should be kept.
Then set the value of iyzd and Sel (i, y, z) accordingly.
iv) The best selection solution is given by _ /10| | Toral rdoc sdbDB N Nd
and the corresponding utility value is Sel (|DB|,
NTotal_rdoc/10, Nsdb).
Figure 2. The dynamic programming optimization
procedure for Equation 16.
Table1: Testbed statistics.
Number of documents Size (MB)
Testbed
Size
(GB) Min Avg Max Min Avg Max
Trec123 3.2 752 10782 39713 28 32 42
Table2: Query set statistics.
Name
TREC
Topic Set
TREC
Topic Field
Average Length
(Words)
Trec123 51-150 Title 3.1
37
Trec123-2ldb-60col (representative): The databases in the
trec123-100col-bysource were sorted with alphabetical order.
Two large databases were created by merging 20 small
databases with the round-robin method. Thus, the two large
databases have more relevant documents due to their large sizes,
even though the densities of relevant documents are roughly the
same as the small databases.
Trec123-AP-WSJ-60col (relevant): The 24 Associated Press
collections and the 16 Wall Street Journal collections in the
trec123-100col-bysource testbed were collapsed into two large
databases APall and WSJall. The other 60 collections were left
unchanged. The APall and WSJall databases have higher
densities of documents relevant to TREC queries than the small
databases. Thus, the two large databases have many more
relevant documents than the small databases.
Trec123-FR-DOE-81col (nonrelevant): The 13 Federal
Register collections and the 6 Department of Energy collections
in the trec123-100col-bysource testbed were collapsed into two
large databases FRall and DOEall. The other 80 collections were
left unchanged. The FRall and DOEall databases have lower
densities of documents relevant to TREC queries than the small
databases, even though they are much larger.
100 queries were created from the title fields of TREC topics
51-150. The queries 101-150 were used as training queries and
the queries 51-100 were used as test queries (details in Table 2).
4.2 Search Engines
In the uncooperative distributed information retrieval
environments of large organizations (companies) or 
domainspecific hidden Web, different databases may use different types
of search engine. To simulate the multiple type-engine
environment, three different types of search engines were used
in the experiments: INQUERY [2], a unigram statistical
language model with linear smoothing [12,20] and a TFIDF
retrieval algorithm with ltc weight [12,20]. All these
algorithms were implemented with the Lemur toolkit [12].
These three kinds of search engines were assigned to the
databases among the four testbeds in a round-robin manner.
5. RESULTS: RESOURCE SELECTION OF
DATABASE RECOMMENDATION
All four testbeds described in Section 4 were used in the
experiments to evaluate the resource selection effectiveness of
the database recommendation system.
The resource descriptions were created using query-based
sampling. About 80 queries were sent to each database to
download 300 unique documents. The database size statistics
were estimated by the sample-resample method [21]. Fifty
queries (101-150) were used as training queries to build the
relevant logistic model and to fit the exponential functions of the
centralized document score curves for large ratio databases
(details in Section 3.1). Another 50 queries (51-100) were used
as test data.
Resource selection algorithms of database recommendation
systems are typically compared using the recall metric nR
[1,17,18,21]. Let B denote a baseline ranking, which is often the
RBR (relevance based ranking), and E as a ranking provided by
a resource selection algorithm. And let Bi and Ei denote the
number of relevant documents in the ith
ranked database of B or
E. Then Rn is defined as follows:
=
=
= k
i i
k
i i
k
B
E
R
1
1
(17)
Usually the goal is to search only a few databases, so our figures
only show results for selecting up to 20 databases.
The experiments summarized in Figure 3 compared the
effectiveness of the three resource selection algorithms, namely
the CORI, ReDDE and UUM/HR. The UUM/HR algorithm is
described in Section 3.3. It can be seen from Figure 3 that the
ReDDE and UUM/HR algorithms are more effective (on the
representative, relevant and nonrelevant testbeds) or as good as
(on the Trec123-100Col testbed) the CORI resource selection
algorithm. The UUM/HR algorithm is more effective than the
ReDDE algorithm on the representative and relevant testbeds
and is about the same as the ReDDE algorithm on the 
Trec123100Col and the nonrelevant testbeds. This suggests that the
UUM/HR algorithm is more robust than the ReDDE algorithm.
It can be noted that when selecting only a few databases on the
Trec123-100Col or the nonrelevant testbeds, the ReDEE
algorithm has a small advantage over the UUM/HR algorithm.
We attribute this to two causes: i) The ReDDE algorithm was
tuned on the Trec123-100Col testbed; and ii) Although the
difference is small, this may suggest that our logistic model of
estimating probabilities of relevance is not accurate enough.
More training data or a more sophisticated model may help to
solve this minor puzzle.
Collections Selected. Collections Selected.
Trec123-100Col Testbed. Representative Testbed.
Collection Selected. Collection Selected.
Relevant Testbed. Nonrelevant Testbed.
Figure 3. Resource selection experiments on the four testbeds.
38
6. RESULTS: DOCUMENT RETRIEVAL
EFFECTIVENESS
For document retrieval, the selected databases are searched and
the returned results are merged into a single final list. In all of
the experiments discussed in this section the results retrieved
from individual databases were combined by the 
semisupervised learning results merging algorithm. This version of
the SSL algorithm [22] is allowed to download a small number
of returned document texts on the fly to create additional
training data in the process of learning the linear models which
map database-specific document scores into estimated
centralized document scores. It has been shown to be very
effective in environments where only short result-lists are
retrieved from each selected database [22]. This is a common
scenario in operational environments and was the case for our
experiments.
Document retrieval effectiveness was measured by Precision at
the top part of the final document list. The experiments in this
section were conducted to study the document retrieval
effectiveness of five selection algorithms, namely the CORI,
ReDDE, UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms.
The last three algorithms were proposed in Section 3. All the
first four algorithms selected 3 or 5 databases, and 50 documents
were retrieved from each selected database. The UUM/HP-FL
algorithm also selected 3 or 5 databases, but it was allowed to
adjust the number of documents to retrieve from each selected
database; the number retrieved was constrained to be from 10 to
100, and a multiple of 10.
The Trec123-100Col and representative testbeds were selected
for document retrieval as they represent two extreme cases of
resource selection effectiveness; in one case the CORI algorithm
is as good as the other algorithms and in the other case it is quite
Table 5. Precision on the representative testbed when 3 databases were selected. (The first baseline is CORI; the second baseline for
UUM/HP methods is UUM/HR.)
Precision at
Doc Rank
CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
5 docs 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%)(-0.9%) 0.5000 (+34.4%)(+7.8%)
10 docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%)(-1.3%) 0.4640 (+36.5%)(+0.9%)
15 docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%)(-1.9%) 0.4413 (+41.4%)(+2.2)
20 docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%)(+4.0%)
30 docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%)(-2.6%) 0.3887 (+53.5%)(+1.0%)
Table 6. Precision on the representative testbed when 5 databases were selected. (The first baseline is CORI; the second baseline for
UUM/HP methods is UUM/HR.)
Precision at
Doc Rank
CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
5 docs 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%)(-6.1%) 0.4520 (+14.1%)(-0.9%)
10 docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%)(+4.2%) 0.4560 (+17.5%)(+6.5%)
15 docs 0.3533 0.3987 (+12.9%) 0.4227 (+19.6%) 0.4440 (+25.7%)(+5.0%) 0.4453 (+26.0%)(+5.4%)
20 docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%)(+3.6%) 0.4350 (+30.6%)(+5.1%)
30 docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%)(-0.7%) 0.4060 (+36.8%)(+1.2%)
Table 3. Precision on the trec123-100col-bysource testbed when 3 databases were selected. (The first baseline is CORI; the second
baseline for UUM/HP methods is UUM/HR.)
Precision at
Doc Rank
CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
5 docs 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%)(+18.1%) 0.4640 (+27.5%)(+17.2%)
10 docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%)(+20.5%) 0.4220 (+25.6%)(+19.9%)
15 docs 0.3253 0.3187 (-2.0%) 0.3347 (+2.9%) 0.3973 (+22.2%)(+15.7%) 0.3920 (+20.5%)(+17.1%)
20 docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)(+13.8%) 0.3700 (+17.8%)(+13.2%)
30 docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%)(+14.8%) 0.3400 (+22.3%)(+14.4%)
Table 4. Precision on the trec123-100col-bysource testbed when 5 databases were selected. (The first baseline is CORI; the second
baseline for UUM/HP methods is UUM/HR.)
Precision at
Doc Rank
CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
5 docs 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%)(+9.4%) 0.4600 (+15.0%)(+7.5%)
10 docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%)(+10.0%) 0.4320 (+13.7%)(+13.7%)
15 docs 0.3560 0.3560 (+0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%)(+5.4%) 0.4080 (+14.6%)(+9.7%)
20 docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%)(+4.5%) 0.3830 (+11.7%)(+7.9%)
30 docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%)(+5.6%) 0.3487 (+7.6%)(+5.3%)
39
a lot worse than the other algorithms. Tables 3 and 4 show the
results on the Trec123-100Col testbed, and Tables 5 and 6 show
the results on the representative testbed.
On the Trec123-100Col testbed, the document retrieval
effectiveness of the CORI selection algorithm is roughly the
same or a little bit better than the ReDDE algorithm but both of
them are worse than the other three algorithms (Tables 3 and 4).
The UUM/HR algorithm has a small advantage over the CORI
and ReDDE algorithms. One main difference between the
UUM/HR algorithm and the ReDDE algorithm was pointed out
before: The UUM/HR uses training data and linear interpolation
to estimate the centralized document score curves, while the
ReDDE algorithm [21] uses a heuristic method, assumes the
centralized document score curves are step functions and makes
no distinction among the top part of the curves. This difference
makes UUM/HR better than the ReDDE algorithm at
distinguishing documents with high probabilities of relevance
from low probabilities of relevance. Therefore, the UUM/HR
reflects the high-precision retrieval goal better than the ReDDE
algorithm and thus is more effective for document retrieval.
The UUM/HR algorithm does not explicitly optimize the
selection decision with respect to the high-precision goal as the
UUM/HP-FL and UUM/HP-VL algorithms are designed to do.
It can be seen that on this testbed, the UUM/HP-FL and
UUM/HP-VL algorithms are much more effective than all the
other algorithms. This indicates that their power comes from
explicitly optimizing the high-precision goal of document
retrieval in Equations 14 and 16.
On the representative testbed, CORI is much less effective than
other algorithms for distributed document retrieval (Tables 5 and
6). The document retrieval results of the ReDDE algorithm are
better than that of the CORI algorithm but still worse than the
results of the UUM/HR algorithm. On this testbed the three
UUM algorithms are about equally effective. Detailed analysis
shows that the overlap of the selected databases between the
UUM/HR, UUM/HP-FL and UUM/HP-VL algorithms is much
larger than the experiments on the Trec123-100Col testbed,
since all of them tend to select the two large databases. This
explains why they are about equally effective for document
retrieval.
In real operational environments, databases may return no
document scores and report only ranked lists of results. As the
unified utility maximization model only utilizes retrieval scores
of sampled documents with a centralized retrieval algorithm to
calculate the probabilities of relevance, it makes database
selection decisions without referring to the document scores
from individual databases and can be easily generalized to this
case of rank lists without document scores. The only adjustment
is that the SSL algorithm merges ranked lists without document
scores by assigning the documents with pseudo-document scores
normalized for their ranks (In a ranked list of 50 documents, the
first one has a score of 1, the second has a score of 0.98 etc)
,which has been studied in [22]. The experiment results on
trec123-100Col-bysource testbed with 3 selected databases are
shown in Table 7. The experiment setting was the same as
before except that the document scores were eliminated
intentionally and the selected databases only return ranked lists
of document ids. It can be seen from the results that the
UUM/HP-FL and UUM/HP-VL work well with databases
returning no document scores and are still more effective than
other alternatives. Other experiments with databases that return
no document scores are not reported but they show similar
results to prove the effectiveness of UUM/HP-FL and 
UUM/HPVL algorithms.
The above experiments suggest that it is very important to
optimize the high-precision goal explicitly in document
retrieval. The new algorithms based on this principle achieve
better or at least as good results as the prior state-of-the-art
algorithms in several environments.
7. CONCLUSION
Distributed information retrieval solves the problem of finding
information that is scattered among many text databases on local
area networks and Internets. Most previous research use
effective resource selection algorithm of database
recommendation system for distributed document retrieval
application. We argue that the high-recall resource selection
goal of database recommendation and high-precision goal of
document retrieval are related but not identical. This kind of
inconsistency has also been observed in previous work, but the
prior solutions either used heuristic methods or assumed
cooperation by individual databases (e.g., all the databases used
the same kind of search engines), which is frequently not true in
the uncooperative environment.
In this work we propose a unified utility maximization model to
integrate the resource selection of database recommendation and
document retrieval tasks into a single unified framework. In this
framework, the selection decisions are obtained by optimizing
different objective functions. As far as we know, this is the first
work that tries to view and theoretically model the distributed
information retrieval task in an integrated manner.
The new framework continues a recent research trend studying
the use of query-based sampling and a centralized sample
database. A single logistic model was trained on the centralized
Table 7. Precision on the trec123-100col-bysource testbed when 3 databases were selected (The first baseline is CORI; the second
baseline for UUM/HP methods is UUM/HR.) (Search engines do not return document scores)
Precision at
Doc Rank
CORI ReDDE UUM/HR UUM/HP-FL UUM/HP-VL
5 docs 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%)(+22.8%) 0.4520 (+28.4%)(+22.8)
10 docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%)(+23.4%) 0.4020 (+21.1%)(+20.4%)
15 docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%) 0.3920 (+21.5%)(+19.5%) 0.3733 (+15.7%)(+13.8%)
20 docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%)(+17.3%) 0.3590 (+18.5%)(+14.7%)
30 docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%)(+12.9%) 0.3273 (+20.0%)(+12.9%)
40
sample database to estimate the probabilities of relevance of
documents by their centralized retrieval scores, while the
centralized sample database serves as a bridge to connect the
individual databases with the centralized logistic model.
Therefore, the probabilities of relevance for all the documents
across the databases can be estimated with very small amount of
human relevance judgment, which is much more efficient than
previous methods that build a separate model for each database.
This framework is not only more theoretically solid but also
very effective. One algorithm for resource selection (UUM/HR)
and two algorithms for document retrieval (UUM/HP-FL and
UUM/HP-VL) are derived from this framework. Empirical
studies have been conducted on testbeds to simulate the
distributed search solutions of large organizations (companies)
or domain-specific hidden Web. Furthermore, the UUM/HP-FL
and UUM/HP-VL resource selection algorithms are extended
with a variant of SSL results merging algorithm to address the
distributed document retrieval task when selected databases do
not return document scores. Experiments have shown that these
algorithms achieve results that are at least as good as the prior
state-of-the-art, and sometimes considerably better. Detailed
analysis indicates that the advantage of these algorithms comes
from explicitly optimizing the goals of the specific tasks.
The unified utility maximization framework is open for different
extensions. When cost is associated with searching the online
databases, the utility framework can be adjusted to automatically
estimate the best number of databases to search so that a large
amount of relevant documents can be retrieved with relatively
small costs. Another extension of the framework is to consider
the retrieval effectiveness of the online databases, which is an
important issue in the operational environments. All of these are
the directions of future research.
ACKNOWLEDGEMENT
This research was supported by NSF grants EIA-9983253 and
IIS-0118767. Any opinions, findings, conclusions, or
recommendations expressed in this paper are the authors", and
do not necessarily reflect those of the sponsor.
REFERENCES
[1] J. Callan. (2000). Distributed information retrieval. In W.B.
Croft, editor, Advances in Information Retrieval. Kluwer
Academic Publishers. (pp. 127-150).
[2] J. Callan, W.B. Croft, and J. Broglio. (1995). TREC and
TIPSTER experiments with INQUERY. Information
Processing and Management, 31(3). (pp. 327-343).
[3] J. G. Conrad, X. S. Guo, P. Jackson and M. Meziou.
(2002). Database selection using actual physical and
acquired logical collection resources in a massive 
domainspecific operational environment. Distributed search over
the hidden web: Hierarchical database sampling and
selection. In Proceedings of the 28th
International
Conference on Very Large Databases (VLDB).
[4] N. Craswell. (2000). Methods for distributed information
retrieval. Ph. D. thesis, The Australian Nation University.
[5] N. Craswell, D. Hawking, and P. Thistlewaite. (1999).
Merging results from isolated search engines. In
Proceedings of 10th Australasian Database Conference.
[6] D. D'Souza, J. Thom, and J. Zobel. (2000). A comparison
of techniques for selecting text collections. In Proceedings
of the 11th Australasian Database Conference.
[7] N. Fuhr. (1999). A Decision-Theoretic approach to
database selection in networked IR. ACM Transactions on
Information Systems, 17(3). (pp. 229-249).
[8] L. Gravano, C. Chang, H. Garcia-Molina, and A. Paepcke.
(1997). STARTS: Stanford proposal for internet 
metasearching. In Proceedings of the 20th ACM-SIGMOD
International Conference on Management of Data.
[9] L. Gravano, P. Ipeirotis and M. Sahami. (2003). QProber:
A System for Automatic Classification of Hidden-Web
Databases. ACM Transactions on Information Systems,
21(1).
[10] P. Ipeirotis and L. Gravano. (2002). Distributed search over
the hidden web: Hierarchical database sampling and
selection. In Proceedings of the 28th International
Conference on Very Large Databases (VLDB).
[11] InvisibleWeb.com. http://www.invisibleweb.com
[12] The lemur toolkit. http://www.cs.cmu.edu/~lemur
[13] J. Lu and J. Callan. (2003). Content-based information
retrieval in peer-to-peer networks. In Proceedings of the
12th International Conference on Information and
Knowledge Management.
[14] W. Meng, C.T. Yu and K.L. Liu. (2002) Building efficient
and effective metasearch engines. ACM Comput. Surv.
34(1).
[15] H. Nottelmann and N. Fuhr. (2003). Evaluating different
method of estimating retrieval quality for resource
selection. In Proceedings of the 25th Annual International
ACM SIGIR Conference on Research and Development in
Information Retrieval.
[16] H., Nottelmann and N., Fuhr. (2003). The MIND
architecture for heterogeneous multimedia federated digital
libraries. ACM SIGIR 2003 Workshop on Distributed
Information Retrieval.
[17] A.L. Powell, J.C. French, J. Callan, M. Connell, and C.L.
Viles. (2000). The impact of database selection on
distributed searching. In Proceedings of the 23rd Annual
International ACM SIGIR Conference on Research and
Development in Information Retrieval.
[18] A.L. Powell and J.C. French. (2003). Comparing the
performance of database selection algorithms. ACM
Transactions on Information Systems, 21(4). (pp. 412-456).
[19] C. Sherman (2001). Search for the invisible web. Guardian
Unlimited.
[20] L. Si and J. Callan. (2002). Using sampled data and
regression to merge search engine results. In Proceedings
of the 25th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval.
[21] L. Si and J. Callan. (2003). Relevant document distribution
estimation method for resource selection. In Proceedings of
the 26th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval.
[22] L. Si and J. Callan. (2003). A Semi-Supervised learning
method to merge search engine results. ACM Transactions
on Information Systems, 21(4). (pp. 457-491).
41
Beyond PageRank: Machine Learning for Static Ranking
Matthew Richardson
Microsoft Research
One Microsoft Way
Redmond, WA 98052
+1 (425) 722-3325
mattri@microsoft.com
Amit Prakash
MSN
One Microsoft Way
Redmond, WA 98052
+1 (425) 705-6015
amitp@microsoft.com
Eric Brill
Microsoft Research
One Microsoft Way
Redmond, WA 98052
+1 (425) 705-4992
brill@microsoft.com
ABSTRACT
Since the publication of Brin and Page"s paper on PageRank,
many in the Web community have depended on PageRank for the
static (query-independent) ordering of Web pages. We show that
we can significantly outperform PageRank using features that are
independent of the link structure of the Web. We gain a further
boost in accuracy by using data on the frequency at which users
visit Web pages. We use RankNet, a ranking machine learning
algorithm, to combine these and other static features based on
anchor text and domain characteristics. The resulting model
achieves a static ranking pairwise accuracy of 67.3% (vs. 56.7%
for PageRank or 50% for random).
Categories and Subject Descriptors
I.2.6 [Artificial Intelligence]: Learning. H.3.3 [Information
Storage and Retrieval]: Information Search and Retrieval.
General Terms
Algorithms, Measurement, Performance, Experimentation.
1. INTRODUCTION
Over the past decade, the Web has grown exponentially in size.
Unfortunately, this growth has not been isolated to good-quality
pages. The number of incorrect, spamming, and malicious (e.g.,
phishing) sites has also grown rapidly. The sheer number of both
good and bad pages on the Web has led to an increasing reliance
on search engines for the discovery of useful information. Users
rely on search engines not only to return pages related to their
search query, but also to separate the good from the bad, and
order results so that the best pages are suggested first.
To date, most work on Web page ranking has focused on
improving the ordering of the results returned to the user 
(querydependent ranking, or dynamic ranking). However, having a good
query-independent ranking (static ranking) is also crucially
important for a search engine. A good static ranking algorithm
provides numerous benefits:
• Relevance: The static rank of a page provides a general
indicator to the overall quality of the page. This is a
useful input to the dynamic ranking algorithm.
• Efficiency: Typically, the search engine"s index is
ordered by static rank. By traversing the index from 
highquality to low-quality pages, the dynamic ranker may
abort the search when it determines that no later page
will have as high of a dynamic rank as those already
found. The more accurate the static rank, the better this
early-stopping ability, and hence the quicker the search
engine may respond to queries.
• Crawl Priority: The Web grows and changes as quickly
as search engines can crawl it. Search engines need a way
to prioritize their crawl-to determine which pages to 
recrawl, how frequently, and how often to seek out new
pages. Among other factors, the static rank of a page is
used to determine this prioritization. A better static rank
thus provides the engine with a higher quality, more 
upto-date index.
Google is often regarded as the first commercially successful
search engine. Their ranking was originally based on the
PageRank algorithm [5][27]. Due to this (and possibly due to
Google"s promotion of PageRank to the public), PageRank is
widely regarded as the best method for the static ranking of Web
pages.
Though PageRank has historically been thought to perform quite
well, there has yet been little academic evidence to support this
claim. Even worse, there has recently been work showing that
PageRank may not perform any better than other simple measures
on certain tasks. Upstill et al. have found that for the task of
finding home pages, the number of pages linking to a page and the
type of URL were as, or more, effective than PageRank [32]. They
found similar results for the task of finding high quality
companies [31]. PageRank has also been used in systems for
TREC"s very large collection and Web track competitions,
but with much less success than had been expected [17]. Finally,
Amento et al. [1] found that simple features, such as the number
of pages on a site, performed as well as PageRank.
Despite these, the general belief remains among many, both
academic and in the public, that PageRank is an essential factor
for a good static rank. Failing this, it is still assumed that using the
link structure is crucial, in the form of the number of inlinks or the
amount of anchor text.
In this paper, we show there are a number of simple url- or 
pagebased features that significantly outperform PageRank (for the
purposes of statically ranking Web pages) despite ignoring the
structure of the Web. We combine these and other static features
using machine learning to achieve a ranking system that is
significantly better than PageRank (in pairwise agreement with
human labels).
A machine learning approach for static ranking has other
advantages besides the quality of the ranking. Because the
measure consists of many features, it is harder for malicious users
to manipulate it (i.e., to raise their page"s static rank to an
undeserved level through questionable techniques, also known as
Web spamming). This is particularly true if the feature set is not
known. In contrast, a single measure like PageRank can be easier
to manipulate because spammers need only concentrate on one
goal: how to cause more pages to point to their page. With an
algorithm that learns, a feature that becomes unusable due to
spammer manipulation will simply be reduced or removed from
the final computation of rank. This flexibility allows a ranking
system to rapidly react to new spamming techniques.
A machine learning approach to static ranking is also able to take
advantage of any advances in the machine learning field. For
example, recent work on adversarial classification [12] suggests
that it may be possible to explicitly model the Web page
spammer"s (the adversary) actions, adjusting the ranking model in
advance of the spammer"s attempts to circumvent it. Another
example is the elimination of outliers in constructing the model,
which helps reduce the effect that unique sites may have on the
overall quality of the static rank. By moving static ranking to a
machine learning framework, we not only gain in accuracy, but
also gain in the ability to react to spammer"s actions, to rapidly
add new features to the ranking algorithm, and to leverage
advances in the rapidly growing field of machine learning.
Finally, we believe there will be significant advantages to using
this technique for other domains, such as searching a local hard
drive or a corporation"s intranet. These are domains where the
link structure is particularly weak (or non-existent), but there are
other domain-specific features that could be just as powerful. For
example, the author of an intranet page and his/her position in the
organization (e.g., CEO, manager, or developer) could provide
significant clues as to the importance of that page. A machine
learning approach thus allows rapid development of a good static
algorithm in new domains.
This paper"s contribution is a systematic study of static features,
including PageRank, for the purposes of (statically) ranking Web
pages. Previous studies on PageRank typically used subsets of the
Web that are significantly smaller (e.g., the TREC VLC2 corpus,
used by many, contains only 19 million pages). Also, the
performance of PageRank and other static features has typically
been evaluated in the context of a complete system for dynamic
ranking, or for other tasks such as question answering. In contrast,
we explore the use of PageRank and other features for the direct
task of statically ranking Web pages.
We first briefly describe the PageRank algorithm. In Section 3 we
introduce RankNet, the machine learning technique used to
combine static features into a final ranking. Section 4 describes
the static features. The heart of the paper is in Section 5, which
presents our experiments and results. We conclude with a
discussion of related and future work.
2. PAGERANK
The basic idea behind PageRank is simple: a link from a Web
page to another can be seen as an endorsement of that page. In
general, links are made by people. As such, they are indicative of
the quality of the pages to which they point - when creating a
page, an author presumably chooses to link to pages deemed to be
of good quality. We can take advantage of this linkage
information to order Web pages according to their perceived
quality.
Imagine a Web surfer who jumps from Web page to Web page,
choosing with uniform probability which link to follow at each
step. In order to reduce the effect of dead-ends or endless cycles
the surfer will occasionally jump to a random page with some
small probability α, or when on a page with no out-links. If
averaged over a sufficient number of steps, the probability the
surfer is on page j at some point in time is given by the formula:
∑∈
+
−
=
ji i
iP
N
jP
B F
)()1(
)( α
α (1)
Where Fi is the set of pages that page i links to, and Bj is the set of
pages that link to page j. The PageRank score for node j is defined
as this probability: PR(j)=P(j). Because equation (1) is recursive,
it must be iteratively evaluated until P(j) converges (typically, the
initial distribution for P(j) is uniform). The intuition is, because a
random surfer would end up at the page more frequently, it is
likely a better page. An alternative view for equation (1) is that
each page is assigned a quality, P(j). A page gives an equal
share of its quality to each page it points to.
PageRank is computationally expensive. Our collection of 5
billion pages contains approximately 370 billion links. Computing
PageRank requires iterating over these billions of links multiple
times (until convergence). It requires large amounts of memory
(or very smart caching schemes that slow the computation down
even further), and if spread across multiple machines, requires
significant communication between them. Though much work has
been done on optimizing the PageRank computation (see e.g.,
[25] and [6]), it remains a relatively slow, computationally
expensive property to compute.
3. RANKNET
Much work in machine learning has been done on the problems of
classification and regression. Let X={xi} be a collection of feature
vectors (typically, a feature is any real valued number), and
Y={yi} be a collection of associated classes, where yi is the class
of the object described by feature vector xi. The classification
problem is to learn a function f that maps yi=f(xi), for all i. When
yi is real-valued as well, this is called regression.
Static ranking can be seen as a regression problem. If we let xi
represent features of page i, and yi be a value (say, the rank) for
each page, we could learn a regression function that mapped each
page"s features to their rank. However, this over-constrains the
problem we wish to solve. All we really care about is the order of
the pages, not the actual value assigned to them.
Recent work on this ranking problem [7][13][18] directly
attempts to optimize the ordering of the objects, rather than the
value assigned to them. For these, let Z={<i,j>} be a collection of
pairs of items, where item i should be assigned a higher value than
item j. The goal of the ranking problem, then, is to learn a
function f such that,
)()(,, ji ffji xxZ >∈∀
708
Note that, as with learning a regression function, the result of this
process is a function (f) that maps feature vectors to real values.
This function can still be applied anywhere that a 
regressionlearned function could be applied. The only difference is the
technique used to learn the function. By directly optimizing the
ordering of objects, these methods are able to learn a function that
does a better job of ranking than do regression techniques.
We used RankNet [7], one of the aforementioned techniques for
learning ranking functions, to learn our static rank function.
RankNet is a straightforward modification to the standard neural
network back-prop algorithm. As with back-prop, RankNet
attempts to minimize the value of a cost function by adjusting
each weight in the network according to the gradient of the cost
function with respect to that weight. The difference is that, while a
typical neural network cost function is based on the difference
between the network output and the desired output, the RankNet
cost function is based on the difference between a pair of network
outputs. That is, for each pair of feature vectors <i,j> in the
training set, RankNet computes the network outputs oi and oj.
Since vector i is supposed to be ranked higher than vector j, the
larger is oj-oi, the larger the cost.
RankNet also allows the pairs in Z to be weighted with a
confidence (posed as the probability that the pair satisfies the
ordering induced by the ranking function). In this paper, we used
a probability of one for all pairs. In the next section, we will
discuss the features used in our feature vectors, xi.
4. FEATURES
To apply RankNet (or other machine learning techniques) to the
ranking problem, we needed to extract a set of features from each
page. We divided our feature set into four, mutually exclusive,
categories: page-level (Page), domain-level (Domain), anchor text
and inlinks (Anchor), and popularity (Popularity). We also
optionally used the PageRank of a page as a feature. Below, we
describe each of these feature categories in more detail.
PageRank
We computed PageRank on a Web graph of 5 billion crawled
pages (and 20 billion known URLs linked to by these pages).
This represents a significant portion of the Web, and is
approximately the same number of pages as are used by
Google, Yahoo, and MSN for their search engines.
Because PageRank is a graph-based algorithm, it is important
that it be run on as large a subset of the Web as possible. Most
previous studies on PageRank used subsets of the Web that are
significantly smaller (e.g. the TREC VLC2 corpus, used by
many, contains only 19 million pages)
We computed PageRank using the standard value of 0.85 for α.
Popularity
Another feature we used is the actual popularity of a Web page,
measured as the number of times that it has been visited by
users over some period of time. We have access to such data
from users who have installed the MSN toolbar and have opted
to provide it to MSN. The data is aggregated into a count, for
each Web page, of the number of users who viewed that page.
Though popularity data is generally unavailable, there are two
other sources for it. The first is from proxy logs. For example, a
university that requires its students to use a proxy has a record
of all the pages they have visited while on campus.
Unfortunately, proxy data is quite biased and relatively small.
Another source, internal to search engines, are records of which
results their users clicked on. Such data was used by the search
engine Direct Hit, and has recently been explored for
dynamic ranking purposes [20]. An advantage of the toolbar
data over this is that it contains information about URL visits
that are not just the result of a search.
The raw popularity is processed into a number of features such
as the number of times a page was viewed and the number of
times any page in the domain was viewed. More details are
provided in section 5.5.
Anchor text and inlinks
These features are based on the information associated with
links to the page in question. It includes features such as the
total amount of text in links pointing to the page (anchor
text), the number of unique words in that text, etc.
Page
This category consists of features which may be determined by
looking at the page (and its URL) alone. We used only eight,
simple features such as the number of words in the body, the
frequency of the most common term, etc.
Domain
This category contains features that are computed as averages
across all pages in the domain. For example, the average
number of outlinks on any page and the average PageRank.
Many of these features have been used by others for ranking Web
pages, particularly the anchor and page features. As mentioned,
the evaluation is typically for dynamic ranking, and we wish to
evaluate the use of them for static ranking. Also, to our
knowledge, this is the first study on the use of actual page
visitation popularity for static ranking. The closest similar work is
on using click-through behavior (that is, which search engine
results the users click on) to affect dynamic ranking (see e.g.,
[20]).
Because we use a wide variety of features to come up with a static
ranking, we refer to this as fRank (for feature-based ranking).
fRank uses RankNet and the set of features described in this
section to learn a ranking function for Web pages. Unless
otherwise specified, fRank was trained with all of the features.
5. EXPERIMENTS
In this section, we will demonstrate that we can out perform
PageRank by applying machine learning to a straightforward set
of features. Before the results, we first discuss the data, the
performance metric, and the training method.
5.1 Data
In order to evaluate the quality of a static ranking, we needed a
gold standard defining the correct ordering for a set of pages.
For this, we employed a dataset which contains human judgments
for 28000 queries. For each query, a number of results are
manually assigned a rating, from 0 to 4, by human judges. The
rating is meant to be a measure of how relevant the result is for
the query, where 0 means poor and 4 means excellent. There
are approximately 500k judgments in all, or an average of 18
ratings per query.
The queries are selected by randomly choosing queries from
among those issued to the MSN search engine. The probability
that a query is selected is proportional to its frequency among all
709
of the queries. As a result, common queries are more likely to be
judged than uncommon queries. As an example of how diverse
the queries are, the first four queries in the training set are chef
schools, chicagoland speedway, eagles fan club, and
Turkish culture. The documents selected for judging are those
that we expected would, on average, be reasonably relevant (for
example, the top ten documents returned by MSN"s search
engine). This provides significantly more information than
randomly selecting documents on the Web, the vast majority of
which would be irrelevant to a given query.
Because of this process, the judged pages tend to be of higher
quality than the average page on the Web, and tend to be pages
that will be returned for common search queries. This bias is good
when evaluating the quality of static ranking for the purposes of
index ordering and returning relevant documents. This is because
the most important portion of the index to be well-ordered and
relevant is the portion that is frequently returned for search
queries. Because of this bias, however, the results in this paper are
not applicable to crawl prioritization. In order to obtain
experimental results on crawl prioritization, we would need
ratings on a random sample of Web pages.
To convert the data from query-dependent to query-independent,
we simply removed the query, taking the maximum over
judgments for a URL that appears in more than one query. The
reasoning behind this is that a page that is relevant for some query
and irrelevant for another is probably a decent page and should
have a high static rank. Because we evaluated the pages on
queries that occur frequently, our data indicates the correct index
ordering, and assigns high value to pages that are likely to be
relevant to a common query.
We randomly assigned queries to a training, validation, or test set,
such that they contained 84%, 8%, and 8% of the queries,
respectively. Each set contains all of the ratings for a given query,
and no query appears in more than one set. The training set was
used to train fRank. The validation set was used to select the
model that had the highest performance. The test set was used for
the final results.
This data gives us a query-independent ordering of pages. The
goal for a static ranking algorithm will be to reproduce this
ordering as closely as possible. In the next section, we describe
the measure we used to evaluate this.
5.2 Measure
We chose to use pairwise accuracy to evaluate the quality of a
static ranking. The pairwise accuracy is the fraction of time that
the ranking algorithm and human judges agree on the ordering of
a pair of Web pages.
If S(x) is the static ranking assigned to page x, and H(x) is the
human judgment of relevance for x, then consider the following
sets:
)}()(:,{ yHxHyx >=pH and )}()(:,{ ySxSyx >=pS
The pairwise accuracy is the portion of Hp that is also contained
in Sp:
p
pp
H
SH ∩
=accuracypairwise
This measure was chosen for two reasons. First, the discrete
human judgments provide only a partial ordering over Web pages,
making it difficult to apply a measure such as the Spearman rank
order correlation coefficient (in the pairwise accuracy measure, a
pair of documents with the same human judgment does not affect
the score). Second, the pairwise accuracy has an intuitive
meaning: it is the fraction of pairs of documents that, when the
humans claim one is better than the other, the static rank
algorithm orders them correctly.
5.3 Method
We trained fRank (a RankNet based neural network) using the
following parameters. We used a fully connected 2 layer network.
The hidden layer had 10 hidden nodes. The input weights to this
layer were all initialized to be zero. The output layer (just a
single node) weights were initialized using a uniform random
distribution in the range [-0.1, 0.1]. We used tanh as the transfer
function from the inputs to the hidden layer, and a linear function
from the hidden layer to the output. The cost function is the
pairwise cross entropy cost function as discussed in section 3.
The features in the training set were normalized to have zero mean
and unit standard deviation. The same linear transformation was
then applied to the features in the validation and test sets.
For training, we presented the network with 5 million pairings of
pages, where one page had a higher rating than the other. The
pairings were chosen uniformly at random (with replacement)
from all possible pairings. When forming the pairs, we ignored the
magnitude of the difference between the ratings (the rating spread)
for the two URLs. Hence, the weight for each pair was constant
(one), and the probability of a pair being selected was
independent of its rating spread.
We trained the network for 30 epochs. On each epoch, the
training pairs were randomly shuffled. The initial training rate was
0.001. At each epoch, we checked the error on the training set. If
the error had increased, then we decreased the training rate, under
the hypothesis that the network had probably overshot. The
training rate at each epoch was thus set to:
Training rate =
1+ε
κ
Where κ is the initial rate (0.001), and ε is the number of times
the training set error has increased. After each epoch, we
measured the performance of the neural network on the validation
set, using 1 million pairs (chosen randomly with replacement).
The network with the highest pairwise accuracy on the validation
set was selected, and then tested on the test set. We report the
pairwise accuracy on the test set, calculated using all possible
pairs.
These parameters were determined and fixed before the static rank
experiments in this paper. In particular, the choice of initial
training rate, number of epochs, and training rate decay function
were taken directly from Burges et al [7].
Though we had the option of preprocessing any of the features
before they were input to the neural network, we refrained from
doing so on most of them. The only exception was the popularity
features. As with most Web phenomenon, we found that the
distribution of site popularity is Zipfian. To reduce the dynamic
range, and hopefully make the feature more useful, we presented
the network with both the unpreprocessed, as well as the
logarithm, of the popularity features (As with the others, the
logarithmic feature values were also normalized to have zero
mean and unit standard deviation).
710
Applying fRank to a document is computationally efficient, taking
time that is only linear in the number of input features; it is thus
within a constant factor of other simple machine learning methods
such as naïve Bayes. In our experiments, computing the fRank for
all five billion Web pages was approximately 100 times faster
than computing the PageRank for the same set.
5.4 Results
As Table 1 shows, fRank significantly outperforms PageRank for
the purposes of static ranking. With a pairwise accuracy of 67.4%,
fRank more than doubles the accuracy of PageRank (relative to
the baseline of 50%, which is the accuracy that would be achieved
by a random ordering of Web pages). Note that one of fRank"s
input features is the PageRank of the page, so we would expect it
to perform no worse than PageRank. The significant increase in
accuracy implies that the other features (anchor, popularity, etc.)
do in fact contain useful information regarding the overall quality
of a page.
Table 1: Basic Results
Technique Accuracy (%)
None (Baseline) 50.00
PageRank 56.70
fRank 67.43
There are a number of decisions that go into the computation of
PageRank, such as how to deal with pages that have no outlinks,
the choice of α, numeric precision, convergence threshold, etc.
We were able to obtain a computation of PageRank from a
completely independent implementation (provided by Marc
Najork) that varied somewhat in these parameters. It achieved a
pairwise accuracy of 56.52%, nearly identical to that obtained by
our implementation. We thus concluded that the quality of the
PageRank is not sensitive to these minor variations in algorithm,
nor was PageRank"s low accuracy due to problems with our
implementation of it.
We also wanted to find how well each feature set performed. To
answer this, for each feature set, we trained and tested fRank
using only that set of features. The results are shown in Table 2.
As can be seen, every single feature set individually outperformed
PageRank on this test. Perhaps the most interesting result is that
the Page-level features had the highest performance out of all the
feature sets. This is surprising because these are features that do
not depend on the overall graph structure of the Web, nor even on
what pages point to a given page. This is contrary to the common
belief that the Web graph structure is the key to finding a good
static ranking of Web pages.
Table 2: Results for individual feature sets.
Feature Set Accuracy (%)
PageRank 56.70
Popularity 60.82
Anchor 59.09
Page 63.93
Domain 59.03
All Features 67.43
Because we are using a two-layer neural network, the features in
the learned network can interact with each other in interesting,
nonlinear ways. This means that a particular feature that appears
to have little value in isolation could actually be very important
when used in combination with other features. To measure the
final contribution of a feature set, in the context of all the other
features, we performed an ablation study. That is, for each set of
features, we trained a network to contain all of the features except
that set. We then compared the performance of the resulting
network to the performance of the network with all of the features.
Table 3 shows the results of this experiment, where the decrease
in accuracy is the difference in pairwise accuracy between the
network trained with all of the features, and the network missing
the given feature set.
Table 3: Ablation study. Shown is the decrease in accuracy
when we train a network that has all but the given set of
features. The last line is shows the effect of removing the
anchor, PageRank, and domain features, hence a model
containing no network or link-based information whatsoever.
Feature Set Decrease in
Accuracy
PageRank 0.18
Popularity 0.78
Anchor 0.47
Page 5.42
Domain
Anchor, PageRank & Domain
0.10
0.60
The results of the ablation study are consistent with the individual
feature set study. Both show that the most important feature set is
the Page-level feature set, and the second most important is the
popularity feature set.
Finally, we wished to see how the performance of fRank
improved as we added features; we wanted to find at what point
adding more feature sets became relatively useless. Beginning
with no features, we greedily added the feature set that improved
performance the most. The results are shown in Table 4. For
example, the fourth line of the table shows that fRank using the
page, popularity, and anchor features outperformed any network
that used the page, popularity, and some other feature set, and that
the performance of this network was 67.25%.
Table 4: fRank performance as feature sets are added. At each
row, the feature set that gave the greatest increase in accuracy
was added to the list of features (i.e., we conducted a greedy
search over feature sets).
Feature Set Accuracy (%)
None 50.00
+Page 63.93
+Popularity 66.83
+Anchor 67.25
+PageRank 67.31
+Domain 67.43
711
Finally, we present a qualitative comparison of PageRank vs.
fRank. In Table 5 are the top ten URLs returned for PageRank and
for fRank. PageRank"s results are heavily weighted towards
technology sites. It contains two QuickTime URLs (Apple"s video
playback software), as well as Internet Explorer and FireFox
URLs (both of which are Web browsers). fRank, on the other
hand, contains more consumer-oriented sites such as American
Express, Target, Dell, etc. PageRank"s bias toward technology can
be explained through two processes. First, there are many pages
with buttons at the bottom suggesting that the site is optimized
for Internet Explorer, or that the visitor needs QuickTime. These
generally link back to, in these examples, the Internet Explorer
and QuickTime download sites. Consequently, PageRank ranks
those pages highly. Though these pages are important, they are
not as important as it may seem by looking at the link structure
alone. One fix for this is to add information about the link to the
PageRank computation, such as the size of the text, whether it was
at the bottom of the page, etc.
The other bias comes from the fact that the population of Web site
authors is different than the population of Web users. Web
authors tend to be technologically-oriented, and thus their linking
behavior reflects those interests. fRank, by knowing the actual
visitation popularity of a site (the popularity feature set), is able to
eliminate some of that bias. It has the ability to depend more on
where actual Web users visit rather than where the Web site
authors have linked.
The results confirm that fRank outperforms PageRank in pairwise
accuracy. The two most important feature sets are the page and
popularity features. This is surprising, as the page features
consisted only of a few (8) simple features. Further experiments
found that, of the page features, those based on the text of the
page (as opposed to the URL) performed the best. In the next
section, we explore the popularity feature in more detail.
5.5 Popularity Data
As mentioned in section 4, our popularity data came from MSN
toolbar users. For privacy reasons, we had access only to an
aggregate count of, for each URL, how many times it was visited
by any toolbar user. This limited the possible features we could
derive from this data. For possible extensions, see section 6.3,
future work.
For each URL in our train and test sets, we provided a feature to
fRank which was how many times it had been visited by a toolbar
user. However, this feature was quite noisy and sparse,
particularly for URLs with query parameters (e.g., 
http://search.msn.com/results.aspx?q=machine+learning&form=QBHP). One
solution was to provide an additional feature which was the
number of times any URL at the given domain was visited by a
toolbar user. Adding this feature dramatically improved the
performance of fRank.
We took this one step further and used the built-in hierarchical
structure of URLs to construct many levels of backoff between the
full URL and the domain. We did this by using the set of features
shown in Table 6.
Table 6: URL functions used to compute the Popularity
feature set.
Function Example
Exact URL cnn.com/2005/tech/wikipedia.html?v=mobile
No Params cnn.com/2005/tech/wikipedia.html
Page wikipedia.html
URL-1 cnn.com/2005/tech
URL-2 cnn.com/2005
…
Domain cnn.com
Domain+1 cnn.com/2005
…
Each URL was assigned one feature for each function shown in
the table. The value of the feature was the count of the number of
times a toolbar user visited a URL, where the function applied to
that URL matches the function applied to the URL in question.
For example, a user"s visit to cnn.com/2005/sports.html would
increment the Domain and Domain+1 features for the URL
cnn.com/2005/tech/wikipedia.html.
As seen in Table 7, adding the domain counts significantly
improved the quality of the popularity feature, and adding the
numerous backoff functions listed in Table 6 improved the
accuracy even further.
Table 7: Effect of adding backoff to the popularity feature set
Features Accuracy (%)
URL count 58.15
URL and Domain counts 59.31
All backoff functions (Table 6) 60.82
Table 5: Top ten URLs for PageRank vs. fRank
PageRank fRank
google.com google.com
apple.com/quicktime/download yahoo.com
amazon.com americanexpress.com
yahoo.com hp.com
microsoft.com/windows/ie target.com
apple.com/quicktime bestbuy.com
mapquest.com dell.com
ebay.com autotrader.com
mozilla.org/products/firefox dogpile.com
ftc.gov bankofamerica.com
712
Backing off to subsets of the URL is one technique for dealing
with the sparsity of data. It is also informative to see how the
performance of fRank depends on the amount of popularity data
that we have collected. In Figure 1 we show the performance of
fRank trained with only the popularity feature set vs. the amount
of data we have for the popularity feature set. Each day, we
receive additional popularity data, and as can be seen in the plot,
this increases the performance of fRank. The relation is
logarithmic: doubling the amount of popularity data provides a
constant improvement in pairwise accuracy.
In summary, we have found that the popularity features provide a
useful boost to the overall fRank accuracy. Gathering more
popularity data, as well as employing simple backoff strategies,
improve this boost even further.
5.6 Summary of Results
The experiments provide a number of conclusions. First, fRank
performs significantly better than PageRank, even without any
information about the Web graph. Second, the page level and
popularity features were the most significant contributors to
pairwise accuracy. Third, by collecting more popularity data, we
can continue to improve fRank"s performance.
The popularity data provides two benefits to fRank. First, we see
that qualitatively, fRank"s ordering of Web pages has a more
favorable bias than PageRank"s. fRank"s ordering seems to
correspond to what Web users, rather than Web page authors,
prefer. Second, the popularity data is more timely than
PageRank"s link information. The toolbar provides information
about which Web pages people find interesting right now,
whereas links are added to pages more slowly, as authors find the
time and interest.
6. RELATED AND FUTURE WORK
6.1 Improvements to PageRank
Since the original PageRank paper, there has been work on
improving it. Much of that work centers on speeding up and
parallelizing the computation [15][25].
One recognized problem with PageRank is that of topic drift: A
page about dogs will have high PageRank if it is linked to by
many pages that themselves have high rank, regardless of their
topic. In contrast, a search engine user looking for good pages
about dogs would likely prefer to find pages that are pointed to by
many pages that are themselves about dogs. Hence, a link that is
on topic should have higher weight than a link that is not.
Richardson and Domingos"s Query Dependent PageRank [29]
and Haveliwala"s Topic-Sensitive PageRank [16] are two
approaches that tackle this problem.
Other variations to PageRank include differently weighting links
for inter- vs. intra-domain links, adding a backwards step to the
random surfer to simulate the back button on most browsers
[24] and modifying the jump probability (α) [3]. See Langville
and Meyer [23] for a good survey of these, and other
modifications to PageRank.
6.2 Other related work
PageRank is not the only link analysis algorithm used for ranking
Web pages. The most well-known other is HITS [22], which is
used by the Teoma search engine [30]. HITS produces a list of
hubs and authorities, where hubs are pages that point to many
authority pages, and authorities are pages that are pointed to by
many hubs. Previous work has shown HITS to perform
comparably to PageRank [1].
One field of interest is that of static index pruning (see e.g.,
Carmel et al. [8]). Static index pruning methods reduce the size of
the search engine"s index by removing documents that are
unlikely to be returned by a search query. The pruning is typically
done based on the frequency of query terms. Similarly, Pandey
and Olston [28] suggest crawling pages frequently if they are
likely to incorrectly appear (or not appear) as a result of a search.
Similar methods could be incorporated into the static rank (e.g.,
how many frequent queries contain words found on this page).
Others have investigated the effect that PageRank has on the Web
at large [9]. They argue that pages with high PageRank are more
likely to be found by Web users, thus more likely to be linked to,
and thus more likely to maintain a higher PageRank than other
pages. The same may occur for the popularity data. If we increase
the ranking for popular pages, they are more likely to be clicked
on, thus further increasing their popularity. Cho et al. [10] argue
that a more appropriate measure of Web page quality would
depend on not only the current link structure of the Web, but also
on the change in that link structure. The same technique may be
applicable to popularity data: the change in popularity of a page
may be more informative than the absolute popularity.
One interesting related work is that of Ivory and Hearst [19].
Their goal was to build a model of Web sites that are considered
high quality from the perspective of content, structure and
navigation, visual design, functionality, interactivity, and overall
experience. They used over 100 page level features, as well as
features encompassing the performance and structure of the site.
This let them qualitatively describe the qualities of a page that
make it appear attractive (e.g., rare use of italics, at least 9 point
font, …), and (in later work) to build a system that assists novel
Web page authors in creating quality pages by evaluating it
according to these features. The primary differences between this
work and ours are the goal (discovering what constitutes a good
Web page vs. ordering Web pages for the purposes of Web
search), the size of the study (they used a dataset of less than 6000
pages vs. our set of 468,000), and our comparison with PageRank.
y = 0.577Ln(x) + 58.283
R
2
= 0.9822
58
58.5
59
59.5
60
60.5
61
1 10 100
Days of Toolbar Data
PairwiseAccuracy
Figure 1: Relation between the amount of popularity data and
the performance of the popularity feature set. Note the x-axis
is a logarithmic scale.
713
Nevertheless, their work provides insights to additional useful
static features that we could incorporate into fRank in the future.
Recent work on incorporating novel features into dynamic ranking
includes that by Joachims et al. [21], who investigate the use of
implicit feedback from users, in the form of which search engine
results are clicked on. Craswell et al. [11] present a method for
determining the best transformation to apply to query independent
features (such as those used in this paper) for the purposes of
improving dynamic ranking. Other work, such as Boyan et al. [4]
and Bartell et al. [2] apply machine learning for the purposes of
improving the overall relevance of a search engine (i.e., the
dynamic ranking). They do not apply their techniques to the
problem of static ranking.
6.3 Future work
There are many ways in which we would like to extend this work.
First, fRank uses only a small number of features. We believe we
could achieve even more significant results with more features. In
particular the existence, or lack thereof, of certain words could
prove very significant (for instance, under construction
probably signifies a low quality page). Other features could
include the number of images on a page, size of those images,
number of layout elements (tables, divs, and spans), use of style
sheets, conforming to W3C standards (like XHTML 1.0 Strict),
background color of a page, etc.
Many pages are generated dynamically, the contents of which may
depend on parameters in the URL, the time of day, the user
visiting the site, or other variables. For such pages, it may be
useful to apply the techniques found in [26] to form a static
approximation for the purposes of extracting features. The
resulting grammar describing the page could itself be a source of
additional features describing the complexity of the page, such as
how many non-terminal nodes it has, the depth of the grammar
tree, etc.
fRank allows one to specify a confidence in each pairing of
documents. In the future, we will experiment with probabilities
that depend on the difference in human judgments between the
two items in the pair. For example, a pair of documents where one
was rated 4 and the other 0 should have a higher confidence than
a pair of documents rated 3 and 2.
The experiments in this paper are biased toward pages that have
higher than average quality. Also, fRank with all of the features
can only be applied to pages that have already been crawled.
Thus, fRank is primarily useful for index ordering and improving
relevance, not for directing the crawl. We would like to
investigate a machine learning approach for crawl prioritization as
well. It may be that a combination of methods is best: for
example, using PageRank to select the best 5 billion of the 20
billion pages on the Web, then using fRank to order the index and
affect search relevancy.
Another interesting direction for exploration is to incorporate
fRank and page-level features directly into the PageRank
computation itself. Work on biasing the PageRank jump vector
[16], and transition matrix [29], have demonstrated the feasibility
and advantages of such an approach. There is reason to believe
that a direct application of [29], using the fRank of a page for its
relevance, could lead to an improved overall static rank.
Finally, the popularity data can be used in other interesting ways.
The general surfing and searching habits of Web users varies by
time of day. Activity in the morning, daytime, and evening are
often quite different (e.g., reading the news, solving problems,
and accessing entertainment, respectively). We can gain insight
into these differences by using the popularity data, divided into
segments of the day. When a query is issued, we would then use
the popularity data matching the time of query in order to do the
ranking of Web pages. We also plan to explore popularity features
that use more than just the counts of how often a page was visited.
For example, how long users tended to dwell on a page, did they
leave the page by clicking a link or by hitting the back button, etc.
Fox et al. did a study that showed that features such as this can be
valuable for the purposes of dynamic ranking [14]. Finally, the
popularity data could be used as the label rather than as a feature.
Using fRank in this way to predict the popularity of a page may
useful for the tasks of relevance, efficiency, and crawl priority.
There is also significantly more popularity data than human
labeled data, potentially enabling more complex machine learning
methods, and significantly more features.
7. CONCLUSIONS
A good static ranking is an important component for today"s
search engines and information retrieval systems. We have
demonstrated that PageRank does not provide a very good static
ranking; there are many simple features that individually out
perform PageRank. By combining many static features, fRank
achieves a ranking that has a significantly higher pairwise
accuracy than PageRank alone. A qualitative evaluation of the top
documents shows that fRank is less technology-biased than
PageRank; by using popularity data, it is biased toward pages that
Web users, rather than Web authors, visit. The machine learning
component of fRank gives it the additional benefit of being more
robust against spammers, and allows it to leverage further
developments in the machine learning community in areas such as
adversarial classification. We have only begun to explore the
options, and believe that significant strides can be made in the
area of static ranking by further experimentation with additional
features, other machine learning techniques, and additional
sources of data.
8. ACKNOWLEDGMENTS
Thank you to Marc Najork for providing us with additional
PageRank computations and to Timo Burkard for assistance with
the popularity data. Many thanks to Chris Burges for providing
code and significant support in using training RankNets. Also, we
thank Susan Dumais and Nick Craswell for their edits and
suggestions.
9. REFERENCES
[1] B. Amento, L. Terveen, and W. Hill. Does authority mean
quality? Predicting expert quality ratings of Web documents.
In Proceedings of the 23rd
Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, 2000.
[2] B. Bartell, G. Cottrell, and R. Belew. Automatic combination
of multiple ranked retrieval systems. In Proceedings of the
17th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, 1994.
[3] P. Boldi, M. Santini, and S. Vigna. PageRank as a function
of the damping factor. In Proceedings of the International
World Wide Web Conference, May 2005.
714
[4] J. Boyan, D. Freitag, and T. Joachims. A machine learning
architecture for optimizing web search engines. In AAAI
Workshop on Internet Based Information Systems, August
1996.
[5] S. Brin and L. Page. The anatomy of a large-scale
hypertextual web search engine. In Proceedings of the
Seventh International Wide Web Conference, Brisbane,
Australia, 1998. Elsevier.
[6] A. Broder, R. Lempel, F. Maghoul, and J. Pederson.
Efficient PageRank approximation via graph aggregation. In
Proceedings of the International World Wide Web
Conference, May 2004.
[7] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N.
Hamilton, G. Hullender. Learning to rank using gradient
descent. In Proceedings of the 22nd
International Conference
on Machine Learning, Bonn, Germany, 2005.
[8] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y.
S. Maarek, and A. Soffer. Static index pruning for
information retrieval systems. In Proceedings of the 24th
Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 43-50,
New Orleans, Louisiana, USA, September 2001.
[9] J. Cho and S. Roy. Impact of search engines on page
popularity. In Proceedings of the International World Wide
Web Conference, May 2004.
[10]J. Cho, S. Roy, R. Adams. Page Quality: In search of an
unbiased web ranking. In Proceedings of the ACM SIGMOD
2005 Conference. Baltimore, Maryland. June 2005.
[11]N. Craswell, S. Robertson, H. Zaragoza, and M. Taylor.
Relevance weighting for query independent evidence. In
Proceedings of the 28th
Annual Conference on Research and
Development in Information Retrieval (SIGIR), August,
2005.
[12]N. Dalvi, P. Domingos, Mausam, S. Sanghai, D. Verma.
Adversarial Classification. In Proceedings of the Tenth
International Conference on Knowledge Discovery and Data
Mining (pp. 99-108), Seattle, WA, 2004.
[13]O. Dekel, C. Manning, and Y. Singer. Log-linear models for
label-ranking. In Advances in Neural Information Processing
Systems 16. Cambridge, MA: MIT Press, 2003.
[14]S. Fox, K S. Fox, K. Karnawat, M. Mydland, S. T. Dumais
and T. White (2005). Evaluating implicit measures to
improve the search experiences. In the ACM Transactions on
Information Systems, 23(2), pp. 147-168. April 2005.
[15]T. Haveliwala. Efficient computation of PageRank. Stanford
University Technical Report, 1999.
[16]T. Haveliwala. Topic-sensitive PageRank. In Proceedings of
the International World Wide Web Conference, May 2002.
[17]D. Hawking and N. Craswell. Very large scale retrieval and
Web search. In D. Harman and E. Voorhees (eds), The
TREC Book. MIT Press.
[18]R. Herbrich, T. Graepel, and K. Obermayer. Support vector
learning for ordinal regression. In Proceedings of the Ninth
International Conference on Artificial Neural Networks, pp.
97-102. 1999.
[19]M. Ivory and M. Hearst. Statistical profiles of highly-rated
Web sites. In Proceedings of the ACM SIGCHI Conference
on Human Factors in Computing Systems, 2002.
[20]T. Joachims. Optimizing search engines using clickthrough
data. In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining (KDD), 2002.
[21]T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G.
Gay. Accurately Interpreting Clickthrough Data as Implicit
Feedback. In Proceedings of the Conference on Research and
Development in Information Retrieval (SIGIR), 2005.
[22]J. Kleinberg. Authoritative sources in a hyperlinked
environment. Journal of the ACM 46:5, pp. 604-32. 1999.
[23]A. Langville and C. Meyer. Deeper inside PageRank.
Internet Mathematics 1(3):335-380, 2004.
[24]F. Matthieu and M. Bouklit. The effect of the back button in
a random walk: application for PageRank. In Alternate track
papers and posters of the Thirteenth International World
Wide Web Conference, 2004.
[25]F. McSherry. A uniform approach to accelerated PageRank
computation. In Proceedings of the International World
Wide Web Conference, May 2005.
[26]Y. Minamide. Static approximation of dynamically generated
Web pages. In Proceedings of the International World Wide
Web Conference, May 2005.
[27]L. Page, S. Brin, R. Motwani, and T. Winograd. The
PageRank citation ranking: Bringing order to the web.
Technical report, Stanford University, Stanford, CA, 1998.
[28]S. Pandey and C. Olston. User-centric Web crawling. In
Proceedings of the International World Wide Web
Conference, May 2005.
[29]M. Richardson and P. Domingos. The intelligent surfer:
probabilistic combination of link and content information in
PageRank. In Advances in Neural Information Processing
Systems 14, pp. 1441-1448. Cambridge, MA: MIT Press,
2002.
[30]C. Sherman. Teoma vs. Google, Round 2. Available from
World Wide Web (http://dc.internet.com/news/article.php/
1002061), 2002.
[31]T. Upstill, N. Craswell, and D. Hawking. Predicting fame
and fortune: PageRank or indegree?. In the Eighth
Australasian Document Computing Symposium. 2003.
[32]T. Upstill, N. Craswell, and D. Hawking. Query-independent
evidence in home page finding. In ACM Transactions on
Information Systems. 2003.
715
Learning User Interaction Models
for Predicting Web Search Result Preferences
Eugene Agichtein
Microsoft Research
eugeneag@microsoft.com
Eric Brill
Microsoft Research
brill@microsoft.com
Susan Dumais
Microsoft Research
sdumais@microsoft.com
Robert Ragno
Microsoft Research
rragno@microsoft.com
ABSTRACT
Evaluating user preferences of web search results is crucial for
search engine development, deployment, and maintenance. We
present a real-world study of modeling the behavior of web search
users to predict web search result preferences. Accurate modeling
and interpretation of user behavior has important applications to
ranking, click spam detection, web search personalization, and
other tasks. Our key insight to improving robustness of
interpreting implicit feedback is to model query-dependent
deviations from the expected noisy user behavior. We show that
our model of clickthrough interpretation improves prediction
accuracy over state-of-the-art clickthrough methods. We
generalize our approach to model user behavior beyond
clickthrough, which results in higher preference prediction
accuracy than models based on clickthrough information alone.
We report results of a large-scale experimental evaluation that
show substantial improvements over published implicit feedback
interpretation methods.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Search process,
relevance feedback.
General Terms
Algorithms, Measurement, Performance, Experimentation.
1. INTRODUCTION
Relevance measurement is crucial to web search and to
information retrieval in general. Traditionally, search relevance is
measured by using human assessors to judge the relevance of
query-document pairs. However, explicit human ratings are
expensive and difficult to obtain. At the same time, millions of
people interact daily with web search engines, providing valuable
implicit feedback through their interactions with the search
results. If we could turn these interactions into relevance
judgments, we could obtain large amounts of data for evaluating,
maintaining, and improving information retrieval systems.
Recently, automatic or implicit relevance feedback has
developed into an active area of research in the information
retrieval community, at least in part due to an increase in
available resources and to the rising popularity of web search.
However, most traditional IR work was performed over
controlled test collections and carefully-selected query sets and
tasks. Therefore, it is not clear whether these techniques will
work for general real-world web search. A significant distinction
is that web search is not controlled. Individual users may behave
irrationally or maliciously, or may not even be real users; all of
this affects the data that can be gathered. But the amount of the
user interaction data is orders of magnitude larger than anything
available in a non-web-search setting. By using the aggregated
behavior of large numbers of users (and not treating each user as
an individual expert) we can correct for the noise inherent in
individual interactions, and generate relevance judgments that
are more accurate than techniques not specifically designed for
the web search setting.
Furthermore, observations and insights obtained in laboratory
settings do not necessarily translate to real world usage. Hence,
it is preferable to automatically induce feedback interpretation
strategies from large amounts of user interactions. Automatically
learning to interpret user behavior would allow systems to adapt
to changing conditions, changing user behavior patterns, and
different search settings. We present techniques to automatically
interpret the collective behavior of users interacting with a web
search engine to predict user preferences for search results. Our
contributions include:
• A distributional model of user behavior, robust to noise
within individual user sessions, that can recover relevance
preferences from user interactions (Section 3).
• Extensions of existing clickthrough strategies to include
richer browsing and interaction features (Section 4).
• A thorough evaluation of our user behavior models, as well
as of previously published state-of-the-art techniques, over
a large set of web search sessions (Sections 5 and 6).
We discuss our results and outline future directions and
various applications of this work in Section 7, which concludes
the paper.
2. BACKGROUND AND RELATED WORK
Ranking search results is a fundamental problem in
information retrieval. The most common approaches in the
context of the web use both the similarity of the query to the
page content, and the overall quality of a page [3, 20]. A 
state-ofthe-art search engine may use hundreds of features to describe a
candidate page, employing sophisticated algorithms to rank
pages based on these features. Current search engines are
commonly tuned on human relevance judgments. Human
annotators rate a set of pages for a query according to perceived
relevance, creating the gold standard against which different
ranking algorithms can be evaluated. Reducing the dependence on
explicit human judgments by using implicit relevance feedback
has been an active topic of research.
Several research groups have evaluated the relationship
between implicit measures and user interest. In these studies,
both reading time and explicit ratings of interest are collected.
Morita and Shinoda [14] studied the amount of time that users
spent reading Usenet news articles and found that reading time
could predict a user"s interest levels. Konstan et al. [13] showed
that reading time was a strong predictor of user interest in their
GroupLens system. Oard and Kim [15] studied whether implicit
feedback could substitute for explicit ratings in recommender
systems. More recently, Oard and Kim [16] presented a
framework for characterizing observable user behaviors using two
dimensions-the underlying purpose of the observed behavior and
the scope of the item being acted upon.
Goecks and Shavlik [8] approximated human labels by
collecting a set of page activity measures while users browsed the
World Wide Web. The authors hypothesized correlations between
a high degree of page activity and a user"s interest. While the
results were promising, the sample size was small and the
implicit measures were not tested against explicit judgments of
user interest. Claypool et al. [6] studied how several implicit
measures related to the interests of the user. They developed a
custom browser called the Curious Browser to gather data, in a
computer lab, about implicit interest indicators and to probe for
explicit judgments of Web pages visited. Claypool et al. found
that the time spent on a page, the amount of scrolling on a page,
and the combination of time and scrolling have a strong positive
relationship with explicit interest, while individual scrolling
methods and mouse-clicks were not correlated with explicit
interest. Fox et al. [7] explored the relationship between implicit
and explicit measures in Web search. They built an instrumented
browser to collect data and then developed Bayesian models to
relate implicit measures and explicit relevance judgments for both
individual queries and search sessions. They found that
clickthrough was the most important individual variable but that
predictive accuracy could be improved by using additional
variables, notably dwell time on a page.
Joachims [9] developed valuable insights into the collection of
implicit measures, introducing a technique based entirely on
clickthrough data to learn ranking functions. More recently,
Joachims et al. [10] presented an empirical evaluation of
interpreting clickthrough evidence. By performing eye tracking
studies and correlating predictions of their strategies with explicit
ratings, the authors showed that it is possible to accurately
interpret clickthrough events in a controlled, laboratory setting. A
more comprehensive overview of studies of implicit measures is
described in Kelly and Teevan [12].
Unfortunately, the extent to which existing research applies to
real-world web search is unclear. In this paper, we build on
previous research to develop robust user behavior interpretation
models for the real web search setting.
3. LEARNING USER BEHAVIOR MODELS
As we noted earlier, real web search user behavior can be
noisy in the sense that user behaviors are only probabilistically
related to explicit relevance judgments and preferences. Hence,
instead of treating each user as a reliable expert, we aggregate
information from many unreliable user search session traces. Our
main approach is to model user web search behavior as if it were
generated by two components: a relevance component - 
queryspecific behavior influenced by the apparent result relevance, and
a background component - users clicking indiscriminately.
Our general idea is to model the deviations from the expected
user behavior. Hence, in addition to basic features, which we
will describe in detail in Section 3.2, we compute derived
features that measure the deviation of the observed feature value
for a given search result from the expected values for a result,
with no query-dependent information. We motivate our
intuitions with a particularly important behavior feature, result
clickthrough, analyzed next, and then introduce our general
model of user behavior that incorporates other user actions
(Section 3.2).
3.1 A Case Study in Click Distributions
As we discussed, we aggregate statistics across many user
sessions. A click on a result may mean that some user found the
result summary promising; it could also be caused by people
clicking indiscriminately. In general, individual user behavior,
clickthrough and otherwise, is noisy, and cannot be relied upon
for accurate relevance judgments. The data set is described in
more detail in Section 5.2. For the present it suffices to note that
we focus on a random sample of 3,500 queries that were
randomly sampled from query logs. For these queries we
aggregate click data over more than 120,000 searches performed
over a three week period. We also have explicit relevance
judgments for the top 10 results for each query.
Figure 3.1 shows the relative clickthrough frequency as a
function of result position. The aggregated click frequency at
result position p is calculated by first computing the frequency of
a click at p for each query (i.e., approximating the probability
that a randomly chosen click for that query would land on
position p). These frequencies are then averaged across queries
and normalized so that relative frequency of a click at the top
position is 1. The resulting distribution agrees with previous
observations that users click more often on top-ranked results.
This reflects the fact that search engines do a reasonable job of
ranking results as well as biases to click top results and 
noisewe attempt to separate these components in the analysis that
follows.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
result position
RelativeClickFrequency
Figure 3.1: Relative click frequency for top 30 result
positions over 3,500 queries and 120,000 searches.
First we consider the distribution of clicks for the relevant
documents for these queries. Figure 3.2 reports the aggregated
click distribution for queries with varying Position of Top
Relevant document (PTR). While there are many clicks above
the first relevant document for each distribution, there are
clearly peaks in click frequency for the first relevant result.
For example, for queries with top relevant result in position 2,
the relative click frequency at that position (second bar) is higher
than the click frequency at other positions for these queries.
Nevertheless, many users still click on the non-relevant results
in position 1 for such queries. This shows a stronger property of
the bias in the click distribution towards top results - users click
more often on results that are ranked higher, even when they are
not relevant.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 5 10
result position
relativeclickfrequency
PTR=1
PTR=2
PTR=3
PTR=5
PTR=10
Background
Figure 3.2: Relative click frequency for queries with varying
PTR (Position of Top Relevant document).
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
1 2 3 5 10
result position
correctedrelativeclickfrequency
PTR=1
PTR=2
PTR=3
PTR=5
PTR=10
Figure 3.3: Relative corrected click frequency for relevant
documents with varying PTR (Position of Top Relevant).
If we subtract the background distribution of Figure 3.1 from the
mixed distribution of Figure 3.2, we obtain the distribution in
Figure 3.3, where the remaining click frequency distribution can
be interpreted as the relevance component of the results. Note that
the corrected click distribution correlates closely with actual
result relevance as explicitly rated by human judges.
3.2 Robust User Behavior Model
Clicks on search results comprise only a small fraction of the
post-search activities typically performed by users. We now
introduce our techniques for going beyond the clickthrough
statistics and explicitly modeling post-search user behavior.
Although clickthrough distributions are heavily biased towards
top results, we have just shown how the ‘relevance-driven" click
distribution can be recovered by correcting for the prior,
background distribution. We conjecture that other aspects of user
behavior (e.g., page dwell time) are similarly distorted. Our
general model includes two feature types for describing user
behavior: direct and deviational where the former is the directly
measured values, and latter is deviation from the expected values
estimated from the overall (query-independent) distributions for
the corresponding directly observed features.
More formally, we postulate that the observed value o of a
feature f for a query q and result r can be expressed as a mixture
of two components:
),,()(),,( frqrelfCfrqo += (1)
where )( fC is the prior background distribution for values of f
aggregated across all queries, and rel(q,r,f) is the component of
the behavior influenced by the relevance of the result r. As
illustrated above with the clickthrough feature, if we subtract the
background distribution (i.e., the expected clickthrough for a
result at a given position) from the observed clickthrough
frequency at a given position, we can approximate the relevance
component of the clickthrough value1
. In order to reduce the
effect of individual user variations in behavior, we average
observed feature values across all users and search sessions for
each query-URL pair. This aggregation gives additional
robustness of not relying on individual noisy user interactions.
In summary, the user behavior for a query-URL pair is
represented by a feature vector that includes both the directly
observed features and the derived, corrected feature values.
We now describe the actual features we use to represent user
behavior.
3.3 Features for Representing User Behavior
Our goal is to devise a sufficiently rich set of features that
allow us to characterize when a user will be satisfied with a web
search result. Once the user has submitted a query, they perform
many different actions (reading snippets, clicking results,
navigating, refining their query) which we capture and
summarize. This information was obtained via opt-in client-side
instrumentation from users of a major web search engine.
This rich representation of user behavior is similar in many
respects to the recent work by Fox et al. [7]. An important
difference is that many of our features are (by design) query
specific whereas theirs was (by design) a general, 
queryindependent model of user behavior. Furthermore, we include
derived, distributional features computed as described above.
The features we use to represent user search interactions are
summarized in Table 3.1. For clarity, we organize the features
into the groups Query-text, Clickthrough, and Browsing.
Query-text features: Users decide which results to examine in
more detail by looking at the result title, URL, and summary - in
some cases, looking at the original document is not even
necessary. To model this aspect of user experience we defined
features to characterize the nature of the query and its relation to
the snippet text. These include features such as overlap between
the words in title and in query (TitleOverlap), the fraction of
words shared by the query and the result summary
(SummaryOverlap), etc.
Browsing features: Simple aspects of the user web page
interactions can be captured and quantified. These features are
used to characterize interactions with pages beyond the results
page. For example, we compute how long users dwell on a page
(TimeOnPage) or domain (TimeOnDomain), and the deviation
of dwell time from expected page dwell time for a query. These
features allows us to model intra-query diversity of page
browsing behavior (e.g., navigational queries, on average, are
likely to have shorter page dwell time than transactional or
informational queries). We include both the direct features and
the derived features described above.
Clickthrough features: Clicks are a special case of user
interaction with the search engine. We include all the features
necessary to learn the clickthrough-based strategies described
in Sections 4.1 and 4.4. For example, for a query-URL pair we
provide the number of clicks for the result (ClickFrequency), as
1
Of course, this is just a rough estimate, as the observed
background distribution also includes the relevance
component.
well as whether there was a click on result below or above the
current URL (IsClickBelow, IsClickAbove). The derived feature
values such as ClickRelativeFrequency and ClickDeviation are
computed as described in Equation 1.
Query-text features
TitleOverlap Fraction of shared words between query and title
SummaryOverlap Fraction of shared words between query and summary
QueryURLOverlap Fraction of shared words between query and URL
QueryDomainOverlap Fraction of shared words between query and domain
QueryLength Number of tokens in query
QueryNextOverlap Average fraction of words shared with next query
Browsing features
TimeOnPage Page dwell time
CumulativeTimeOnPage Cumulative time for all subsequent pages after search
TimeOnDomain Cumulative dwell time for this domain
TimeOnShortUrl Cumulative time on URL prefix, dropping parameters
IsFollowedLink 1 if followed link to result, 0 otherwise
IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise
IsRedirected 1 if initial URL same as final URL, 0 otherwise
IsPathFromSearch 1 if only followed links after query, 0 otherwise
ClicksFromSearch Number of hops to reach page from query
AverageDwellTime Average time on page for this query
DwellTimeDeviation Deviation from overall average dwell time on page
CumulativeDeviation Deviation from average cumulative time on page
DomainDeviation Deviation from average time on domain
ShortURLDeviation Deviation from average time on short URL
Clickthrough features
Position Position of the URL in Current ranking
ClickFrequency Number of clicks for this query, URL pair
ClickRelativeFrequency Relative frequency of a click for this query and URL
ClickDeviation Deviation from expected click frequency
IsNextClicked 1 if there is a click on next position, 0 otherwise
IsPreviousClicked 1 if there is a click on previous position, 0 otherwise
IsClickAbove 1 if there is a click above, 0 otherwise
IsClickBelow 1 if there is click below, 0 otherwise
Table 3.1: Features used to represent post-search interactions
for a given query and search result URL
3.4 Learning a Predictive Behavior Model
Having described our features, we now turn to the actual
method of mapping the features to user preferences. We attempt
to learn a general implicit feedback interpretation strategy
automatically instead of relying on heuristics or insights. We
consider this approach to be preferable to heuristic strategies,
because we can always mine more data instead of relying (only)
on our intuition and limited laboratory evidence. Our general
approach is to train a classifier to induce weights for the user
behavior features, and consequently derive a predictive model of
user preferences. The training is done by comparing a wide range
of implicit behavior measures with explicit user judgments for a
set of queries.
For this, we use a large random sample of queries in the search
query log of a popular web search engine, the sets of results
(identified by URLs) returned for each of the queries, and any
explicit relevance judgments available for each query/result pair.
We can then analyze the user behavior for all the instances where
these queries were submitted to the search engine.
To learn the mapping from features to relevance preferences,
we use a scalable implementation of neural networks, RankNet
[4], capable of learning to rank a set of given items. More
specifically, for each judged query we check if a result link has
been judged. If so, the label is assigned to the query/URL pair and
to the corresponding feature vector for that search result. These
vectors of feature values corresponding to URLs judged relevant
or non-relevant by human annotators become our training set.
RankNet has demonstrated excellent performance in learning to
rank objects in a supervised setting, hence we use RankNet for
our experiments.
4. PREDICTING USER PREFERENCES
In our experiments, we explore several models for predicting
user preferences. These models range from using no implicit
user feedback to using all available implicit user feedback.
Ranking search results to predict user preferences is a
fundamental problem in information retrieval. Most traditional
IR and web search approaches use a combination of page and
link features to rank search results, and a representative 
state-ofthe-art ranking system will be used as our baseline ranker
(Section 4.1). At the same time, user interactions with a search
engine provide a wealth of information. A commonly considered
type of interaction is user clicks on search results. Previous work
[9], as described above, also examined which results were
skipped (e.g., ‘skip above" and ‘skip next") and other related
strategies to induce preference judgments from the users"
skipping over results and not clicking on following results. We
have also added refinements of these strategies to take into
account the variability observed in realistic web scenarios.. We
describe these strategies in Section 4.2.
As clickthroughs are just one aspect of user interaction, we
extend the relevance estimation by introducing a machine
learning model that incorporates clicks as well as other aspects
of user behavior, such as follow-up queries and page dwell time
(Section 4.3). We conclude this section by briefly describing our
baseline - a state-of-the-art ranking algorithm used by an
operational web search engine.
4.1 Baseline Model
A key question is whether browsing behavior can provide
information absent from existing explicit judgments used to train
an existing ranker. For our baseline system we use a 
state-of-theart page ranking system currently used by a major web search
engine. Hence, we will call this system Current for the
subsequent discussion. While the specific algorithms used by the
search engine are beyond the scope of this paper, the algorithm
ranks results based on hundreds of features such as query to
document similarity, query to anchor text similarity, and
intrinsic page quality. The Current web search engine rankings
provide a strong system for comparison and experiments of the
next two sections.
4.2 Clickthrough Model
If we assume that every user click was motivated by a rational
process that selected the most promising result summary, we can
then interpret each click as described in Joachims et al.[10]. By
studying eye tracking and comparing clicks with explicit
judgments, they identified a few basic strategies. We discuss the
two strategies that performed best in their experiments, Skip
Above and Skip Next.
Strategy SA (Skip Above): For a set of results for a query
and a clicked result at position p, all unclicked results
ranked above p are predicted to be less relevant than the
result at p.
In addition to information about results above the clicked
result, we also have information about the result immediately
following the clicked one. Eye tracking study performed by
Joachims et al. [10] showed that users usually consider the result
immediately following the clicked result in current ranking. Their
Skip Next strategy uses this observation to predict that a result
following the clicked result at p is less relevant than the clicked
result, with accuracy comparable to the SA strategy above. For
better coverage, we combine the SA strategy with this extension to
derive the Skip Above + Skip Next strategy:
Strategy SA+N (Skip Above + Skip Next): This strategy
predicts all un-clicked results immediately following a
clicked result as less relevant than the clicked result, and
combines these predictions with those of the SA strategy
above.
We experimented with variations of these strategies, and found
that SA+N outperformed both SA and the original Skip Next
strategy, so we will consider the SA and SA+N strategies in the
rest of the paper. These strategies are motivated and empirically
tested for individual users in a laboratory setting. As we will
show, these strategies do not work as well in real web search
setting due to inherent inconsistency and noisiness of individual
users" behavior.
The general approach for using our clickthrough models
directly is to filter clicks to those that reflect higher-than-chance
click frequency. We then use the same SA and SA+N strategies,
but only for clicks that have higher-than-expected frequency
according to our model. For this, we estimate the relevance
component rel(q,r,f) of the observed clickthrough feature f as the
deviation from the expected (background) clickthrough
distribution )( fC .
Strategy CD (deviation d): For a given query, compute the
observed click frequency distribution o(r, p) for all results r
in positions p. The click deviation for a result r in position p,
dev(r, p) is computed as:
)(),(),( pCproprdev −=
where C(p) is the expected clickthrough at position p. If
dev(r,p)>d, retain the click as input to the SA+N strategy
above, and apply SA+N strategy over the filtered set of click
events.
The choice of d selects the tradeoff between recall and
precision. While the above strategy extends SA and SA+N, it still
assumes that a (filtered) clicked result is preferred over all
unclicked results presented to the user above a clicked position.
However, for informational queries, multiple results may be
clicked, with varying frequency. Hence, it is preferable to
individually compare results for a query by considering the
difference between the estimated relevance components of the
click distribution of the corresponding query results. We now
define a generalization of the previous clickthrough interpretation
strategy:
Strategy CDiff (margin m): Compute deviation dev(r,p) for
each result r1...rn in position p. For each pair of results ri and
rj, predict preference of ri over rj iff dev(ri,pi)-dev(ri,pj)>m.
As in CD, the choice of m selects the tradeoff between recall
and precision. The pairs may be preferred in the original order or
in reverse of it. Given the margin, two results might be effectively
indistinguishable, but only one can possibly be preferred over the
other. Intuitively, CDiff generalizes the skip idea above to include
cases where the user skipped (i.e., clicked less than expected)
on uj and preferred (i.e., clicked more than expected) on ui.
Furthermore, this strategy allows for differentiation within the set
of clicked results, making it more appropriate to noisy user
behavior.
CDiff and CD are complimentary. CDiff is a generalization of
the clickthrough frequency model of CD, but it ignores the
positional information used in CD. Hence, combining the two
strategies to improve coverage is a natural approach:
Strategy CD+CDiff (deviation d, margin m): Union
of CD and CDiff predictions.
Other variations of the above strategies were considered, but
these five methods cover the range of observed performance.
4.3 General User Behavior Model
The strategies described in the previous section generate
orderings based solely on observed clickthrough frequencies. As
we discussed, clickthrough is just one, albeit important, aspect
of user interactions with web search engine results. We now
present our general strategy that relies on the automatically
derived predictive user behavior models (Section 3).
The UserBehavior Strategy: For a given query, each
result is represented with the features in Table 3.1.
Relative user preferences are then estimated using the
learned user behavior model described in Section 3.4.
Recall that to learn a predictive behavior model we used the
features from Table 3.1 along with explicit relevance judgments
as input to RankNet which learns an optimal weighting of
features to predict preferences.
This strategy models user interaction with the search engine,
allowing it to benefit from the wisdom of crowds interacting
with the results and the pages beyond. As our experiments in the
subsequent sections demonstrate, modeling a richer set of user
interactions beyond clickthroughs results in more accurate
predictions of user preferences.
5. EXPERIMENTAL SETUP
We now describe our experimental setup. We first describe
the methodology used, including our evaluation metrics (Section
5.1). Then we describe the datasets (Section 5.2) and the
methods we compared in this study (Section 5.3).
5.1 Evaluation Methodology and Metrics
Our evaluation focuses on the pairwise agreement between
preferences for results. This allows us to compare to previous
work [9,10]. Furthermore, for many applications such as tuning
ranking functions, pairwise preference can be used directly for
training [1,4,9]. The evaluation is based on comparing
preferences predicted by various models to the correct
preferences derived from the explicit user relevance judgments.
We discuss other applications of our models beyond web search
ranking in Section 7.
To create our set of test pairs we take each query and
compute the cross-product between all search results, returning
preferences for pairs according to the order of the associated
relevance labels. To avoid ambiguity in evaluation, we discard
all ties (i.e., pairs with equal label).
In order to compute the accuracy of our preference predictions
with respect to the correct preferences, we adapt the standard
Recall and Precision measures [20]. While our task of computing
pairwise agreement is different from the absolute relevance
ranking task, the metrics are used in the similar way.
Specifically, we report the average query recall and precision.
For our task, Query Precision and Query Recall for a query q are
defined as:
• Query Precision: Fraction of predicted preferences for results
for q that agree with preferences obtained from explicit
human judgment.
• Query Recall: Fraction of preferences obtained from explicit
human judgment for q that were correctly predicted.
The overall Recall and Precision are computed as the average of
Query Recall and Query Precision, respectively. A drawback of
this evaluation measure is that some preferences may be more
valuable than others, which pairwise agreement does not capture.
We discuss this issue further when we consider extensions to the
current work in Section 7.
5.2 Datasets
For evaluation we used 3,500 queries that were randomly
sampled from query logs(for a major web search engine. For each
query the top 10 returned search results were manually rated on a
6-point scale by trained judges as part of ongoing relevance
improvement effort. In addition for these queries we also had user
interaction data for more than 120,000 instances of these queries.
The user interactions were harvested from anonymous
browsing traces that immediately followed a query submitted to
the web search engine. This data collection was part of voluntary
opt-in feedback submitted by users from October 11 through
October 31. These three weeks (21 days) of user interaction data
was filtered to include only the users in the English-U.S. market.
In order to better understand the effect of the amount of user
interaction data available for a query on accuracy, we created
subsets of our data (Q1, Q10, and Q20) that contain different
amounts of interaction data:
• Q1: Human-rated queries with at least 1 click on results
recorded (3500 queries, 28,093 query-URL pairs)
• Q10: Queries in Q1 with at least 10 clicks (1300 queries,
18,728 query-URL pairs).
• Q20: Queries in Q1 with at least 20 clicks (1000 queries total,
12,922 query-URL pairs).
These datasets were collected as part of normal user experience
and hence have different characteristics than previously reported
datasets collected in laboratory settings. Furthermore, the data
size is order of magnitude larger than any study reported in the
literature.
5.3 Methods Compared
We considered a number of methods for comparison. We
compared our UserBehavior model (Section 4.3) to previously
published implicit feedback interpretation techniques and some
variants of these approaches (Section 4.2), and to the current
search engine ranking based on query and page features alone
(Section 4.1). Specifically, we compare the following strategies:
• SA: The Skip Above clickthrough strategy (Section 4.2)
• SA+N: A more comprehensive extension of SA that takes
better advantage of current search engine ranking.
• CD: Our refinement of SA+N that takes advantage of our
mixture model of clickthrough distribution to select trusted
clicks for interpretation (Section 4.2).
• CDiff: Our generalization of the CD strategy that explicitly
uses the relevance component of clickthrough probabilities to
induce preferences between search results (Section 4.2).
• CD+CDiff: The strategy combining CD and CDiff as the
union of predicted preferences from both (Section 4.2).
• UserBehavior: We order predictions based on decreasing
highest score of any page. In our preliminary experiments
we observed that higher ranker scores indicate higher
confidence in the predictions. This heuristic allows us to
do graceful recall-precision tradeoff using the score of the
highest ranked result to threshold the queries (Section 4.3)
• Current: Current search engine ranking (section 4.1). Note
that the Current ranker implementation was trained over a
superset of the rated query/URL pairs in our datasets, but
using the same truth labels as we do for our evaluation.
Training/Test Split: The only strategy for which splitting the
datasets into training and test was required was the
UserBehavior method. To evaluate UserBehavior we train and
validate on 75% of labeled queries, and test on the remaining
25%. The sampling was done per query (i.e., all results for a
chosen query were included in the respective dataset, and there
was no overlap in queries between training and test sets).
It is worth noting that both the ad-hoc SA and SA+N, as well
as the distribution-based strategies (CD, CDiff, and CD+CDiff),
do not require a separate training and test set, since they are
based on heuristics for detecting anomalous click frequencies
for results. Hence, all strategies except for UserBehavior were
tested on the full set of queries and associated relevance
preferences, while UserBehavior was tested on a randomly
chosen hold-out subset of the queries as described above. To
make sure we are not favoring UserBehavior, we also tested all
other strategies on the same hold-out test sets, resulting in the
same accuracy results as testing over the complete datasets.
6. RESULTS
We now turn to experimental evaluation of predicting
relevance preference of web search results. Figure 6.1 shows the
recall-precision results over the Q1 query set (Section 5.2). The
results indicate that previous click interpretation strategies, SA
and SA+N perform suboptimally in this setting, exhibiting
precision 0.627 and 0.638 respectively. Furthermore, there is no
mechanism to do recall-precision trade-off with SA and SA+N,
as they do not provide prediction confidence. In contrast, our
clickthrough distribution-based techniques CD and CD+CDiff
exhibit somewhat higher precision than SA and SA+N (0.648
and 0.717 at Recall of 0.08, maximum achieved by SA or
SA+N).
SA+N
SA
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45
Recall
Precision
SA SA+N
CD CDiff
CD+CDiff UserBehavior
Current
Figure 6.1: Precision vs. Recall of SA, SA+N, CD, CDiff,
CD+CDiff, UserBehavior, and Current relevance prediction
methods over the Q1 dataset.
Interestingly, CDiff alone exhibits precision equal to SA
(0.627) at the same recall at 0.08. In contrast, by combining CD
and CDiff strategies (CD+CDiff method) we achieve the best
performance of all clickthrough-based strategies, exhibiting
precision of above 0.66 for recall values up to 0.14, and higher at
lower recall levels. Clearly, aggregating and intelligently
interpreting clickthroughs, results in significant gain for realistic
web search, than previously described strategies. However, even
the CD+CDiff clickthrough interpretation strategy can be
improved upon by automatically learning to interpret the
aggregated clickthrough evidence.
But first, we consider the best performing strategy,
UserBehavior. Incorporating post-search navigation history in
addition to clickthroughs (Browsing features) results in the
highest recall and precision among all methods compared. Browse
exhibits precision of above 0.7 at recall of 0.16, significantly
outperforming our Baseline and clickthrough-only strategies.
Furthermore, Browse is able to achieve high recall (as high as
0.43) while maintaining precision (0.67) significantly higher than
the baseline ranking.
To further analyze the value of different dimensions of implicit
feedback modeled by the UserBehavior strategy, we consider each
group of features in isolation. Figure 6.2 reports Precision vs.
Recall for each feature group. Interestingly, Query-text alone has
low accuracy (only marginally better than Random). Furthermore,
Browsing features alone have higher precision (with lower
maximum recall achieved) than considering all of the features in
our UserBehavior model. Applying different machine learning
methods for combining classifier predictions may increase
performance of using all features for all recall values.
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45
Recall
Precision
All Features
Clickthrough
Query-text
Browsing
Figure 6.2: Precision vs. recall for predicting relevance with
each group of features individually.
0.65
0.67
0.69
0.71
0.73
0.75
0.77
0.79
0.81
0.83
0.85
0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49
Recall
Precision
CD+CDiff:Q1 UserBehavior:Q1
CD+CDiff:Q10 UserBehavior:Q10
CD+CDiff:Q20 UserBehavior:Q20
Figure 6.3: Recall vs. Precision of CD+CDiff and
UserBehavior for query sets Q1, Q10, and Q20 (queries with
at least 1, at least 10, and at least 20 clicks respectively).
Interestingly, the ranker trained over Clickthrough-only
features achieves substantially higher recall and precision than
human-designed clickthrough-interpretation strategies described
earlier. For example, the clickthrough-trained classifier achieves
0.67 precision at 0.42 Recall vs. the maximum recall of 0.14
achieved by the CD+CDiff strategy.
Our clickthrough and user behavior interpretation strategies
rely on extensive user interaction data. We consider the effects
of having sufficient interaction data available for a query before
proposing a re-ranking of results for that query. Figure 6.3
reports recall-precision curves for the CD+CDiff and
UserBehavior methods for different test query sets with at least
1 click (Q1), 10 clicks (Q10) and 20 clicks (Q20) available per
query. Not surprisingly, CD+CDiff improves with more clicks.
This indicates that accuracy will improve as more user
interaction histories become available, and more queries from
the Q1 set will have comprehensive interaction histories.
Similarly, the UserBehavior strategy performs better for queries
with 10 and 20 clicks, although the improvement is less dramatic
than for CD+CDiff. For queries with sufficient clicks, CD+CDiff
exhibits precision comparable with Browse at lower recall.
0
0.05
0.1
0.15
0.2
7 12 17 21
Days of user interaction data harvested
Recall
CD+CDiff
UserBehavior
Figure 6.4: Recall of CD+CDiff and UserBehavior strategies
at fixed minimum precision 0.7 for varying amounts of user
activity data (7, 12, 17, 21 days).
Our techniques often do not make relevance predictions for
search results (i.e., if no interaction data is available for the
lower-ranked results), consequently maintaining higher precision
at the expense of recall. In contrast, the current search engine
always makes a prediction for every result for a given query. As
a consequence, the recall of Current is high (0.627) at the
expense of lower precision As another dimension of acquiring
training data we consider the learning curve with respect to
amount (days) of training data available. Figure 6.4 reports the
Recall of CD+CDiff and UserBehavior strategies for varying
amounts of training data collected over time. We fixed minimum
precision for both strategies at 0.7 as a point substantially higher
than the baseline (0.625). As expected, Recall of both strategies
improves quickly with more days of interaction data examined.
We now briefly summarize our experimental results. We
showed that by intelligently aggregating user clickthroughs
across queries and users, we can achieve higher accuracy on
predicting user preferences. Because of the skewed distribution
of user clicks our clickthrough-only strategies have high
precision, but low recall (i.e., do not attempt to predict relevance
of many search results). Nevertheless, our CD+CDiff
clickthrough strategy outperforms most recent state-of-the-art
results by a large margin (0.72 precision for CD+CDiff vs. 0.64
for SA+N) at the highest recall level of SA+N.
Furthermore, by considering the comprehensive UserBehavior
features that model user interactions after the search and beyond
the initial click, we can achieve substantially higher precision
and recall than considering clickthrough alone. Our
UserBehavior strategy achieves recall of over 0.43 with precision
of over 0.67 (with much higher precision at lower recall levels),
substantially outperforms the current search engine preference
ranking and all other implicit feedback interpretation methods.
7. CONCLUSIONS AND FUTURE WORK
Our paper is the first, to our knowledge, to interpret 
postsearch user behavior to estimate user preferences in a real web
search setting. We showed that our robust models result in higher
prediction accuracy than previously published techniques.
We introduced new, robust, probabilistic techniques for
interpreting clickthrough evidence by aggregating across users
and queries. Our methods result in clickthrough interpretation
substantially more accurate than previously published results not
specifically designed for web search scenarios. Our methods"
predictions of relevance preferences are substantially more
accurate than the current state-of-the-art search result ranking that
does not consider user interactions. We also presented a general
model for interpreting post-search user behavior that incorporates
clickthrough, browsing, and query features. By considering the
complete search experience after the initial query and click, we
demonstrated prediction accuracy far exceeding that of
interpreting only the limited clickthrough information.
Furthermore, we showed that automatically learning to
interpret user behavior results in substantially better performance
than the human-designed ad-hoc clickthrough interpretation
strategies. Another benefit of automatically learning to interpret
user behavior is that such methods can adapt to changing
conditions and changing user profiles. For example, the user
behavior model on intranet search may be different from the web
search behavior. Our general UserBehavior method would be able
to adapt to these changes by automatically learning to map new
behavior patterns to explicit relevance ratings.
A natural application of our preference prediction models is to
improve web search ranking [1]. In addition, our work has many
potential applications including click spam detection, search
abuse detection, personalization, and domain-specific ranking. For
example, our automatically derived behavior models could be
trained on examples of search abuse or click spam behavior
instead of relevance labels. Alternatively, our models could be
used directly to detect anomalies in user behavior - either due to
abuse or to operational problems with the search engine.
While our techniques perform well on average, our
assumptions about clickthrough distributions (and learning the
user behavior models) may not hold equally well for all queries.
For example, queries with divergent access patterns (e.g., for
ambiguous queries with multiple meanings) may result in
behavior inconsistent with the model learned for all queries.
Hence, clustering queries and learning different predictive models
for each query type is a promising research direction. Query
distributions also change over time, and it would be productive to
investigate how that affects the predictive ability of these models.
Furthermore, some predicted preferences may be more valuable
than others, and we plan to investigate different metrics to capture
the utility of the predicted preferences.
As we showed in this paper, using the wisdom of crowds can
give us accurate interpretation of user interactions even in the
inherently noisy web search setting. Our techniques allow us to
automatically predict relevance preferences for web search results
with accuracy greater than the previously published methods. The
predicted relevance preferences can be used for automatic
relevance evaluation and tuning, for deploying search in new
settings, and ultimately for improving the overall web search
experience.
8. REFERENCES
[1] E. Agichtein, E. Brill, and S. Dumais, Improving Web Search Ranking
by Incorporating User Behavior, in Proceedings of the ACM
Conference on Research and Development on Information Retrieval
(SIGIR), 2006
[2] J. Allan. HARD Track Overview in TREC 2003: High Accuracy
Retrieval from Documents. In Proceedings of TREC 2003, 24-37,
2004.
[3] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web
Search Engine,. In Proceedings of WWW7, 107-117, 1998.
[4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N.
Hamilton, and G. Hullender, Learning to Rank using Gradient
Descent, in Proceedings of the International Conference on Machine
Learning (ICML), 2005
[5] D.M. Chickering, The WinMine Toolkit, Microsoft Technical Report
MSR-TR-2002-103, 2002
[6] M. Claypool, D. Brown, P. Lee and M. Waseda. Inferring user interest,
in IEEE Internet Computing. 2001
[7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.
Evaluating implicit measures to improve the search experience. In
ACM Transactions on Information Systems, 2005
[8] J. Goecks and J. Shavlick. Learning users" interests by unobtrusively
observing their normal behavior. In Proceedings of the IJCAI
Workshop on Machine Learning for Information Filtering. 1999.
[9] T. Joachims, Optimizing Search Engines Using Clickthrough Data, in
Proceedings of the ACM Conference on Knowledge Discovery and
Datamining (SIGKDD), 2002
[10] T. Joachims, L. Granka, B. Pang, H. Hembrooke and G. Gay,
Accurately Interpreting Clickthrough Data as Implicit Feedback, in
Proceedings of the ACM Conference on Research and Development
on Information Retrieval (SIGIR), 2005
[11] T. Joachims, Making Large-Scale SVM Learning Practical. Advances
in Kernel Methods, in Support Vector Learning, MIT Press, 1999
[12] D. Kelly and J. Teevan, Implicit feedback for inferring user preference:
A bibliography. In SIGIR Forum, 2003
[13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon and J. Riedl.
GroupLens: Applying collaborative filtering to usenet news. In
Communications of ACM, 1997.
[14] M. Morita, and Y. Shinoda, Information filtering based on user
behavior analysis and best match text retrieval. In Proceedings of the
ACM Conference on Research and Development on Information
Retrieval (SIGIR), 1994
[15] D. Oard and J. Kim. Implicit feedback for recommender systems. in
Proceedings of AAAI Workshop on Recommender Systems. 1998
[16] D. Oard and J. Kim. Modeling information content using observable
behavior. In Proceedings of the 64th Annual Meeting of the
American Society for Information Science and Technology. 2001
[17] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal
Content on the World Wide Web. In Working with Technology in
Mind: Brunswikian. Resources for Cognitive Science and
Engineering, Oxford University Press, 2004
[18] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from
Implicit Feedback, in Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD), ACM, 2005
[19] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning
from Implicit Feedback, in the ICML Workshop on Learning in Web
Search, 2005
[20] G. Salton and M. McGill. Introduction to modern information
retrieval. McGraw-Hill, 1983
[21] E.M. Voorhees, D. Harman, Overview of TREC, 2001
A Frequency-based and a Poisson-based Definition of the
Probability of Being Informative
Thomas Roelleke
Department of Computer Science
Queen Mary University of London
thor@dcs.qmul.ac.uk
ABSTRACT
This paper reports on theoretical investigations about the
assumptions underlying the inverse document frequency (idf ).
We show that an intuitive idf -based probability function for
the probability of a term being informative assumes disjoint
document events. By assuming documents to be 
independent rather than disjoint, we arrive at a Poisson-based 
probability of being informative. The framework is useful for
understanding and deciding the parameter estimation and
combination in probabilistic retrieval models.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
models
General Terms
Theory
1. INTRODUCTION AND BACKGROUND
The inverse document frequency (idf ) is one of the most
successful parameters for a relevance-based ranking of 
retrieved objects. With N being the total number of 
documents, and n(t) being the number of documents in which
term t occurs, the idf is defined as follows:
idf(t) := − log
n(t)
N
, 0 <= idf(t) < ∞
Ranking based on the sum of the idf -values of the query
terms that occur in the retrieved documents works well, this
has been shown in numerous applications. Also, it is well
known that the combination of a document-specific term
weight and idf works better than idf alone. This approach
is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is
the so-called term frequency of term t in document d. The
idf reflects the discriminating power (informativeness) of a
term, whereas the tf reflects the occurrence of a term.
The idf alone works better than the tf alone does. An 
explanation might be the problem of tf with terms that occur
in many documents; let us refer to those terms as noisy
terms. We use the notion of noisy terms rather than 
frequent terms since frequent terms leaves open whether we
refer to the document frequency of a term in a collection or
to the so-called term frequency (also referred to as 
withindocument frequency) of a term in a document. We 
associate noise with the document frequency of a term in a
collection, and we associate occurrence with the 
withindocument frequency of a term. The tf of a noisy term might
be high in a document, but noisy terms are not good 
candidates for representing a document. Therefore, the removal
of noisy terms (known as stopword removal) is essential
when applying tf . In a tf-idf approach, the removal of 
stopwords is conceptually obsolete, if stopwords are just words
with a low idf .
From a probabilistic point of view, tf is a value with a
frequency-based probabilistic interpretation whereas idf has
an informative rather than a probabilistic interpretation.
The missing probabilistic interpretation of idf is a problem
in probabilistic retrieval models where we combine uncertain
knowledge of different dimensions (e.g.: informativeness of
terms, structure of documents, quality of documents, age
of documents, etc.) such that a good estimate of the 
probability of relevance is achieved. An intuitive solution is a
normalisation of idf such that we obtain values in the 
interval [0; 1]. For example, consider a normalisation based on
the maximal idf -value. Let T be the set of terms occurring
in a collection.
Pfreq (t is informative) :=
idf(t)
maxidf
maxidf := max({idf(t)|t ∈ T}), maxidf <= − log(1/N)
minidf := min({idf(t)|t ∈ T}), minidf >= 0
minidf
maxidf
≤ Pfreq (t is informative) ≤ 1.0
This frequency-based probability function covers the interval
[0; 1] if the minimal idf is equal to zero, which is the case
if we have at least one term that occurs in all documents.
Can we interpret Pfreq , the normalised idf , as the probability
that the term is informative?
When investigating the probabilistic interpretation of the
227
normalised idf , we made several observations related to 
disjointness and independence of document events. These 
observations are reported in section 3. We show in section 3.1
that the frequency-based noise probability n(t)
N
used in the
classic idf -definition can be explained by three assumptions:
binary term occurrence, constant document containment and
disjointness of document containment events. In section 3.2
we show that by assuming independence of documents, we
obtain 1 − e−1
≈ 1 − 0.37 as the upper bound of the noise
probability of a term. The value e−1
is related to the 
logarithm and we investigate in section 3.3 the link to 
information theory. In section 4, we link the results of the previous
sections to probability theory. We show the steps from 
possible worlds to binomial distribution and Poisson distribution.
In section 5, we emphasise that the theoretical framework
of this paper is applicable for both idf and tf . Finally, in
section 6, we base the definition of the probability of 
being informative on the results of the previous sections and
compare frequency-based and Poisson-based definitions.
2. BACKGROUND
The relationship between frequencies, probabilities and
information theory (entropy) has been the focus of many
researchers. In this background section, we focus on work
that investigates the application of the Poisson distribution
in IR since a main part of the work presented in this paper
addresses the underlying assumptions of Poisson.
[4] proposes a 2-Poisson model that takes into account
the different nature of relevant and non-relevant documents,
rare terms (content words) and frequent terms (noisy terms,
function words, stopwords). [9] shows experimentally that
most of the terms (words) in a collection are distributed
according to a low dimension n-Poisson model. [10] uses a
2-Poisson model for including term frequency-based 
probabilities in the probabilistic retrieval model. The non-linear
scaling of the Poisson function showed significant 
improvement compared to a linear frequency-based probability. The
Poisson model was here applied to the term frequency of a
term in a document. We will generalise the discussion by
pointing out that document frequency and term frequency
are dual parameters in the collection space and the 
document space, respectively. Our discussion of the Poisson 
distribution focuses on the document frequency in a collection
rather than on the term frequency in a document.
[7] and [6] address the deviation of idf and Poisson, and
apply Poisson mixtures to achieve better Poisson-based 
estimates. The results proved again experimentally that a 
onedimensional Poisson does not work for rare terms, therefore
Poisson mixtures and additional parameters are proposed.
[3], section 3.3, illustrates and summarises 
comprehensively the relationships between frequencies, probabilities
and Poisson. Different definitions of idf are put into 
context and a notion of noise is defined, where noise is viewed
as the complement of idf . We use in our paper a different
notion of noise: we consider a frequency-based noise that
corresponds to the document frequency, and we consider a
term noise that is based on the independence of document
events.
[11], [12], [8] and [1] link frequencies and probability 
estimation to information theory. [12] establishes a framework
in which information retrieval models are formalised based
on probabilistic inference. A key component is the use of a
space of disjoint events, where the framework mainly uses
terms as disjoint events. The probability of being 
informative defined in our paper can be viewed as the probability
of the disjoint terms in the term space of [12].
[8] address entropy and bibliometric distributions. 
Entropy is maximal if all events are equiprobable and the 
frequency-based Lotka law (N/iλ
is the number of scientists
that have written i publications, where N and λ are 
distribution parameters), Zipf and the Pareto distribution are 
related. The Pareto distribution is the continuous case of the
Lotka and Lotka and Zipf show equivalences. The Pareto
distribution is used by [2] for term frequency normalisation.
The Pareto distribution compares to the Poisson 
distribution in the sense that Pareto is fat-tailed, i. e. Pareto 
assigns larger probabilities to large numbers of events than
Poisson distributions do. This makes Pareto interesting
since Poisson is felt to be too radical on frequent events.
We restrict in this paper to the discussion of Poisson, 
however, our results show that indeed a smoother distribution
than Poisson promises to be a good candidate for improving
the estimation of probabilities in information retrieval.
[1] establishes a theoretical link between tf-idf and 
information theory and the theoretical research on the meaning
of tf-idf clarifies the statistical model on which the different
measures are commonly based. This motivation matches
the motivation of our paper: We investigate theoretically
the assumptions of classical idf and Poisson for a better
understanding of parameter estimation and combination.
3. FROM DISJOINT TO INDEPENDENT
We define and discuss in this section three probabilities:
The frequency-based noise probability (definition 1), the 
total noise probability for disjoint documents (definition 2).
and the noise probability for independent documents 
(definition 3).
3.1 Binary occurrence, constant containment
and disjointness of documents
We show in this section, that the frequency-based noise
probability n(t)
N
in the idf definition can be explained as
a total probability with binary term occurrence, constant
document containment and disjointness of document 
containments.
We refer to a probability function as binary if for all events
the probability is either 1.0 or 0.0. The occurrence 
probability P(t|d) is binary, if P(t|d) is equal to 1.0 if t ∈ d, and
P(t|d) is equal to 0.0, otherwise.
P(t|d) is binary : ⇐⇒ P(t|d) = 1.0 ∨ P(t|d) = 0.0
We refer to a probability function as constant if for all
events the probability is equal. The document containment
probability reflect the chance that a document occurs in a
collection. This containment probability is constant if we
have no information about the document containment or
we ignore that documents differ in containment. 
Containment could be derived, for example, from the size, quality,
age, links, etc. of a document. For a constant containment
in a collection with N documents, 1
N
is often assumed as
the containment probability. We generalise this definition
and introduce the constant λ where 0 ≤ λ ≤ N. The 
containment of a document d depends on the collection c, this
is reflected by the notation P(d|c) used for the containment
228
of a document.
P(d|c) is constant : ⇐⇒ ∀d : P(d|c) =
λ
N
For disjoint documents that cover the whole event space,
we set λ = 1 and obtain
Èd P(d|c) = 1.0. Next, we define
the frequency-based noise probability and the total noise
probability for disjoint documents. We introduce the event
notation t is noisy and t occurs for making the difference
between the noise probability P(t is noisy|c) in a collection
and the occurrence probability P(t occurs|d) in a document
more explicit, thereby keeping in mind that the noise 
probability corresponds to the occurrence probability of a term
in a collection.
Definition 1. The frequency-based term noise 
probability:
Pfreq (t is noisy|c) :=
n(t)
N
Definition 2. The total term noise probability for
disjoint documents:
Pdis (t is noisy|c) :=
d
P(t occurs|d) · P(d|c)
Now, we can formulate a theorem that makes assumptions
explicit that explain the classical idf .
Theorem 1. IDF assumptions: If the occurrence 
probability P(t|d) of term t over documents d is binary, and
the containment probability P(d|c) of documents d is 
constant, and document containments are disjoint events, then
the noise probability for disjoint documents is equal to the
frequency-based noise probability.
Pdis (t is noisy|c) = Pfreq (t is noisy|c)
Proof. The assumptions are:
∀d : (P(t occurs|d) = 1 ∨ P(t occurs|d) = 0) ∧
P(d|c) =
λ
N
∧
d
P(d|c) = 1.0
We obtain:
Pdis (t is noisy|c) =
d|t∈d
1
N
=
n(t)
N
= Pfreq (t is noisy|c)
The above result is not a surprise but it is a 
mathematical formulation of assumptions that can be used to explain
the classical idf . The assumptions make explicit that the
different types of term occurrence in documents (frequency
of a term, importance of a term, position of a term, 
document part where the term occurs, etc.) and the different
types of document containment (size, quality, age, etc.) are
ignored, and document containments are considered as 
disjoint events.
From the assumptions, we can conclude that idf 
(frequencybased noise, respectively) is a relatively simple but strict
estimate. Still, idf works well. This could be explained
by a leverage effect that justifies the binary occurrence and
constant containment: The term occurrence for small 
documents tends to be larger than for large documents, whereas
the containment for small documents tends to be smaller
than for large documents. From that point of view, idf
means that P(t ∧ d|c) is constant for all d in which t occurs,
and P(t ∧ d|c) is zero otherwise. The occurrence and 
containment can be term specific. For example, set P(t∧d|c) =
1/ND(c) if t occurs in d, where ND(c) is the number of 
documents in collection c (we used before just N). We choose a
document-dependent occurrence P(t|d) := 1/NT (d), i. e. the
occurrence probability is equal to the inverse of NT (d), which
is the total number of terms in document d. Next, we choose
the containment P(d|c) := NT (d)/NT (c)·NT (c)/ND(c) where
NT (d)/NT (c) is a document length normalisation (number
of terms in document d divided by the number of terms in
collection c), and NT (c)/ND(c) is a constant factor of the
collection (number of terms in collection c divided by the
number of documents in collection c). We obtain P(t∧d|c) =
1/ND(c).
In a tf-idf -retrieval function, the tf -component reflects
the occurrence probability of a term in a document. This is
a further explanation why we can estimate the idf with a
simple P(t|d), since the combined tf-idf contains the 
occurrence probability. The containment probability corresponds
to a document normalisation (document length 
normalisation, pivoted document length) and is normally attached to
the tf -component or the tf-idf -product.
The disjointness assumption is typical for frequency-based
probabilities. From a probability theory point of view, we
can consider documents as disjoint events, in order to achieve
a sound theoretical model for explaining the classical idf .
But does disjointness reflect the real world where the 
containment of a document appears to be independent of the
containment of another document? In the next section, we
replace the disjointness assumption by the independence 
assumption.
3.2 The upper bound of the noise probability
for independent documents
For independent documents, we compute the probability
of a disjunction as usual, namely as the complement of the
probability of the conjunction of the negated events:
P(d1 ∨ . . . ∨ dN ) = 1 − P(¬d1 ∧ . . . ∧ ¬dN )
= 1 −
d
(1 − P(d))
The noise probability can be considered as the conjunction
of the term occurrence and the document containment.
P(t is noisy|c) := P(t occurs ∧ (d1 ∨ . . . ∨ dN )|c)
For disjoint documents, this view of the noise probability
led to definition 2. For independent documents, we use now
the conjunction of negated events.
Definition 3. The term noise probability for 
independent documents:
Pin (t is noisy|c) :=
d
(1 − P(t occurs|d) · P(d|c))
With binary occurrence and a constant containment P(d|c) :=
λ/N, we obtain the term noise of a term t that occurs in n(t)
documents:
Pin (t is noisy|c) = 1 − 1 −
λ
N
n(t)
229
For binary occurrence and disjoint documents, the 
containment probability was 1/N. Now, with independent 
documents, we can use λ as a collection parameter that controls
the average containment probability. We show through the
next theorem that the upper bound of the noise probability
depends on λ.
Theorem 2. The upper bound of being noisy: If the
occurrence P(t|d) is binary, and the containment P(d|c)
is constant, and document containments are independent
events, then 1 − e−λ
is the upper bound of the noise 
probability.
∀t : Pin (t is noisy|c) < 1 − e−λ
Proof. The upper bound of the independent noise 
probability follows from the limit limN→∞(1 + x
N
)N
= ex
(see
any comprehensive math book, for example, [5], for the 
convergence equation of the Euler function). With x = −λ, we
obtain:
lim
N→∞
1 −
λ
N
N
= e−λ
For the term noise, we have:
Pin (t is noisy|c) = 1 − 1 −
λ
N
n(t)
Pin (t is noisy|c) is strictly monotonous: The noise of a term
tn is less than the noise of a term tn+1, where tn occurs in
n documents and tn+1 occurs in n + 1 documents. 
Therefore, a term with n = N has the largest noise probability.
For a collection with infinite many documents, the upper
bound of the noise probability for terms tN that occur in all
documents becomes:
lim
N→∞
Pin (tN is noisy) = lim
N→∞
1 − 1 −
λ
N
N
= 1 − e−λ
By applying an independence rather a disjointness 
assumption, we obtain the probability e−1
that a term is not noisy
even if the term does occur in all documents. In the disjoint
case, the noise probability is one for a term that occurs in
all documents.
If we view P(d|c) := λ/N as the average containment,
then λ is large for a term that occurs mostly in large 
documents, and λ is small for a term that occurs mostly in small
documents. Thus, the noise of a term t is large if t occurs in
n(t) large documents and the noise is smaller if t occurs in
small documents. Alternatively, we can assume a constant
containment and a term-dependent occurrence. If we 
assume P(d|c) := 1, then P(t|d) := λ/N can be interpreted as
the average probability that t represents a document. The
common assumption is that the average containment or 
occurrence probability is proportional to n(t). However, here
is additional potential: The statistical laws (see [3] on Luhn
and Zipf) indicate that the average probability could follow
a normal distribution, i. e. small probabilities for small n(t)
and large n(t), and larger probabilities for medium n(t).
For the monotonous case we investigate here, the noise of
a term with n(t) = 1 is equal to 1 − (1 − λ/N) = λ/N and
the noise of a term with n(t) = N is close to 1− e−λ
. In the
next section, we relate the value e−λ
to information theory.
3.3 The probability of a maximal informative
signal
The probability e−1
is special in the sense that a signal
with that probability is a signal with maximal information as
derived from the entropy definition. Consider the definition
of the entropy contribution H(t) of a signal t.
H(t) := P(t) · − ln P(t)
We form the first derivation for computing the optimum.
∂H(t)
∂P(t)
= − ln P(t) +
−1
P(t)
· P(t)
= −(1 + ln P(t))
For obtaining optima, we use:
0 = −(1 + ln P(t))
The entropy contribution H(t) is maximal for P(t) = e−1
.
This result does not depend on the base of the logarithm as
we see next:
∂H(t)
∂P(t)
= − logb P(t) +
−1
P(t) · ln b
· P(t)
= −
1
ln b
+ logb P(t) = −
1 + ln P(t)
ln b
We summarise this result in the following theorem:
Theorem 3. The probability of a maximal 
informative signal: The probability Pmax = e−1
≈ 0.37 is the 
probability of a maximal informative signal. The entropy of a
maximal informative signal is Hmax = e−1
.
Proof. The probability and entropy follow from the 
derivation above.
The complement of the maximal noise probability is e−λ
and we are looking now for a generalisation of the entropy
definition such that e−λ
is the probability of a maximal 
informative signal. We can generalise the entropy definition
by computing the integral of λ+ ln P(t), i. e. this derivation
is zero for e−λ
. We obtain a generalised entropy:
−(λ + ln P(t)) d(P(t)) = P(t) · (1 − λ − ln P(t))
The generalised entropy corresponds for λ = 1 to the 
classical entropy. By moving from disjoint to independent 
documents, we have established a link between the complement
of the noise probability of a term that occurs in all 
documents and information theory. Next, we link independent
documents to probability theory.
4. THE LINK TO PROBABILITY THEORY
We review for independent documents three concepts of
probability theory: possible worlds, binomial distribution
and Poisson distribution.
4.1 Possible Worlds
Each conjunction of document events (for each document,
we consider two document events: the document can be
true or false) is associated with a so-called possible world.
For example, consider the eight possible worlds for three
documents (N = 3).
230
world w conjunction
w7 d1 ∧ d2 ∧ d3
w6 d1 ∧ d2 ∧ ¬d3
w5 d1 ∧ ¬d2 ∧ d3
w4 d1 ∧ ¬d2 ∧ ¬d3
w3 ¬d1 ∧ d2 ∧ d3
w2 ¬d1 ∧ d2 ∧ ¬d3
w1 ¬d1 ∧ ¬d2 ∧ d3
w0 ¬d1 ∧ ¬d2 ∧ ¬d3
With each world w, we associate a probability µ(w), which
is equal to the product of the single probabilities of the 
document events.
world w probability µ(w)
w7
 λ
N
¡3
·
 1 − λ
N
¡0
w6
 λ
N
¡2
·
 1 − λ
N
¡1
w5
 λ
N
¡2
·
 1 − λ
N
¡1
w4
 λ
N
¡1
·
 1 − λ
N
¡2
w3
 λ
N
¡2
·
 1 − λ
N
¡1
w2
 λ
N
¡1
·
 1 − λ
N
¡2
w1
 λ
N
¡1
·
 1 − λ
N
¡2
w0
 λ
N
¡0
·
 1 − λ
N
¡3
The sum over the possible worlds in which k documents are
true and N −k documents are false is equal to the 
probability function of the binomial distribution, since the binomial
coefficient yields the number of possible worlds in which k
documents are true.
4.2 Binomial distribution
The binomial probability function yields the probability
that k of N events are true where each event is true with
the single event probability p.
P(k) := binom(N, k, p) :=
N
k
pk
(1 − p)N −k
The single event probability is usually defined as p := λ/N,
i. e. p is inversely proportional to N, the total number of
events. With this definition of p, we obtain for an infinite
number of documents the following limit for the product of
the binomial coefficient and pk
:
lim
N→∞
N
k
pk
=
= lim
N→∞
N · (N −1) · . . . · (N −k +1)
k!
λ
N
k
=
λk
k!
The limit is close to the actual value for k << N. For large
k, the actual value is smaller than the limit.
The limit of (1−p)N −k follows from the limit limN→∞(1+
x
N
)N
= ex
.
lim
N→∞
(1 − p)N−k
= lim
N→∞
1 −
λ
N
N −k
= lim
N→∞
e−λ
· 1 −
λ
N
−k
= e−λ
Again, the limit is close to the actual value for k << N. For
large k, the actual value is larger than the limit.
4.3 Poisson distribution
For an infinite number of events, the Poisson probability
function is the limit of the binomial probability function.
lim
N→∞
binom(N, k, p) =
λk
k!
· e−λ
P(k) = poisson(k, λ) :=
λk
k!
· e−λ
The probability poisson(0, 1) is equal to e−1
, which is the
probability of a maximal informative signal. This shows
the relationship of the Poisson distribution and information
theory.
After seeing the convergence of the binomial distribution,
we can choose the Poisson distribution as an approximation
of the independent term noise probability. First, we define
the Poisson noise probability:
Definition 4. The Poisson term noise probability:
Ppoi (t is noisy|c) := e−λ
·
n(t)
k=1
λk
k!
For independent documents, the Poisson distribution 
approximates the probability of the disjunction for large n(t),
since the independent term noise probability is equal to the
sum over the binomial probabilities where at least one of
n(t) document containment events is true.
Pin (t is noisy|c) =
n(t)
k=1
n(t)
k
pk
(1 − p)N −k
Pin (t is noisy|c) ≈ Ppoi (t is noisy|c)
We have defined a frequency-based and a Poisson-based 
probability of being noisy, where the latter is the limit of the
independence-based probability of being noisy. Before we
present in the final section the usage of the noise 
probability for defining the probability of being informative, we
emphasise in the next section that the results apply to the
collection space as well as to the the document space.
5. THE COLLECTION SPACE AND THE
DOCUMENT SPACE
Consider the dual definitions of retrieval parameters in
table 1. We associate a collection space D × T with a 
collection c where D is the set of documents and T is the set
of terms in the collection. Let ND := |D| and NT := |T|
be the number of documents and terms, respectively. We
consider a document as a subset of T and a term as a subset
of D. Let nT (d) := |{t|d ∈ t}| be the number of terms that
occur in the document d, and let nD(t) := |{d|t ∈ d}| be the
number of documents that contain the term t.
In a dual way, we associate a document space L × T with
a document d where L is the set of locations (also referred
to as positions, however, we use the letters L and l and not
P and p for avoiding confusion with probabilities) and T is
the set of terms in the document. The document dimension
in a collection space corresponds to the location (position)
dimension in a document space.
The definition makes explicit that the classical notion of
term frequency of a term in a document (also referred to as
the within-document term frequency) actually corresponds
to the location frequency of a term in a document. For the
231
space collection document
dimensions documents and terms locations and terms
document/location
frequency
nD(t, c): Number of documents in which term t
occurs in collection c
nL(t, d): Number of locations (positions) at which
term t occurs in document d
ND(c): Number of documents in collection c NL(d): Number of locations (positions) in 
document d
term frequency nT (d, c): Number of terms that document d 
contains in collection c
nT (l, d): Number of terms that location l contains
in document d
NT (c): Number of terms in collection c NT (d): Number of terms in document d
noise/occurrence P(t|c) (term noise) P(t|d) (term occurrence)
containment P(d|c) (document) P(l|d) (location)
informativeness − ln P(t|c) − ln P(t|d)
conciseness − ln P(d|c) − ln P(l|d)
P(informative) ln(P(t|c))/ ln(P(tmin, c)) ln(P(t|d))/ ln(P(tmin, d))
P(concise) ln(P(d|c))/ ln(P(dmin|c)) ln(P(l|d))/ ln(P(lmin|d))
Table 1: Retrieval parameters
actual term frequency value, it is common to use the 
maximal occurrence (number of locations; let lf be the location
frequency).
tf(t, d):=lf(t, d):=
Pfreq (t occurs|d)
Pfreq (tmax occurs|d)
=
nL(t, d)
nL(tmax , d)
A further duality is between informativeness and 
conciseness (shortness of documents or locations): informativeness
is based on occurrence (noise), conciseness is based on 
containment.
We have highlighted in this section the duality between
the collection space and the document space. We 
concentrate in this paper on the probability of a term to be noisy
and informative. Those probabilities are defined in the 
collection space. However, the results regarding the term noise
and informativeness apply to their dual counterparts: term
occurrence and informativeness in a document. Also, the
results can be applied to containment of documents and 
locations.
6. THE PROBABILITY OF BEING 
INFORMATIVE
We showed in the previous sections that the disjointness
assumption leads to frequency-based probabilities and that
the independence assumption leads to Poisson probabilities.
In this section, we formulate a frequency-based definition
and a Poisson-based definition of the probability of being
informative and then we compare the two definitions.
Definition 5. The frequency-based probability of 
being informative:
Pfreq (t is informative|c) :=
− ln n(t)
N
− ln 1
N
= − logN
n(t)
N
= 1 − logN n(t) = 1 −
ln n(t)
ln N
We define the Poisson-based probability of being 
informative analogously to the frequency-based probability of being
informative (see definition 5).
Definition 6. The Poisson-based probability of 
being informative:
Ppoi (t is informative|c) :=
− ln e−λ
·
Èn(t)
k=1
λk
k!
− ln(e−λ · λ)
=
λ − ln
Èn(t)
k=1
λk
k!
λ − ln λ
For the sum expression, the following limit holds:
lim
n(t)→∞
n(t)
k=1
λk
k!
= eλ
− 1
For λ >> 1, we can alter the noise and informativeness 
Poisson by starting the sum from 0, since eλ
>> 1. Then, the
minimal Poisson informativeness is poisson(0, λ) = e−λ
. We
obtain a simplified Poisson probability of being informative:
Ppoi (t is informative|c) ≈
λ − ln
Èn(t)
k=0
λk
k!
λ
= 1 −
ln
Èn(t)
k=0
λk
k!
λ
The computation of the Poisson sum requires an 
optimisation for large n(t). The implementation for this paper
exploits the nature of the Poisson density: The Poisson 
density yields only values significantly greater than zero in an
interval around λ.
Consider the illustration of the noise and 
informativeness definitions in figure 1. The probability functions 
displayed are summarised in figure 2 where the simplified 
Poisson is used in the noise and informativeness graphs. The
frequency-based noise corresponds to the linear solid curve
in the noise figure. With an independence assumption, we
obtain the curve in the lower triangle of the noise figure. By
changing the parameter p := λ/N of the independence 
probability, we can lift or lower the independence curve. The
noise figure shows the lifting for the value λ := ln N ≈
9.2. The setting λ = ln N is special in the sense that the
frequency-based and the Poisson-based informativeness have
the same denominator, namely ln N, and the Poisson sum
converges to λ. Whether we can draw more conclusions from
this setting is an open question.
We can conclude, that the lifting is desirable if we know
for a collection that terms that occur in relatively few 
doc232
0
0.2
0.4
0.6
0.8
1
0 2000 4000 6000 8000 10000
Probabilityofbeingnoisy
n(t): Number of documents with term t
frequency
independence: 1/N
independence: ln(N)/N
poisson: 1000
poisson: 2000
poisson: 1000,2000
0
0.2
0.4
0.6
0.8
1
0 2000 4000 6000 8000 10000
Probabilityofbeinginformative
n(t): Number of documents with term t
frequency
independence: 1/N
independence: ln(N)/N
poisson: 1000
poisson: 2000
poisson: 1000,2000
Figure 1: Noise and Informativeness
Probability function Noise Informativeness
Frequency Pfreq Def n(t)/N ln(n(t)/N)/ ln(1/N)
Interval 1/N ≤ Pfreq ≤ 1.0 0.0 ≤ Pfreq ≤ 1.0
Independence Pin Def 1 − (1 − p)n(t)
ln(1 − (1 − p)n(t)
)/ ln(p)
Interval p ≤ Pin < 1 − e−λ
ln(p) ≤ Pin ≤ 1.0
Poisson Ppoi Def e−λ Èn(t)
k=1
λk
k!
(λ − ln
Èn(t)
k=1
λk
k!
)/(λ − ln λ)
Interval e−λ
· λ ≤ Ppoi < 1 − e−λ
(λ − ln(eλ
− 1))/(λ − ln λ) ≤ Ppoi ≤ 1.0
Poisson Ppoi simplified Def e−λ Èn(t)
k=0
λk
k!
(λ − ln
Èn(t)
k=0
λk
k!
)/λ
Interval e−λ
≤ Ppoi < 1.0 0.0 < Ppoi ≤ 1.0
Figure 2: Probability functions
uments are no guarantee for finding relevant documents,
i. e. we assume that rare terms are still relatively noisy. On
the opposite, we could lower the curve when assuming that
frequent terms are not too noisy, i. e. they are considered as
being still significantly discriminative.
The Poisson probabilities approximate the independence
probabilities for large n(t); the approximation is better for
larger λ. For n(t) < λ, the noise is zero whereas for n(t) > λ
the noise is one. This radical behaviour can be smoothened
by using a multi-dimensional Poisson distribution. Figure 1
shows a Poisson noise based on a two-dimensional Poisson:
poisson(k, λ1, λ2) := π · e−λ1
·
λk
1
k!
+ (1 − π) · e−λ2
·
λk
2
k!
The two dimensional Poisson shows a plateau between λ1 =
1000 and λ2 = 2000, we used here π = 0.5. The idea 
behind this setting is that terms that occur in less than 1000
documents are considered to be not noisy (i.e. they are 
informative), that terms between 1000 and 2000 are half noisy,
and that terms with more than 2000 are definitely noisy.
For the informativeness, we observe that the radical 
behaviour of Poisson is preserved. The plateau here is 
approximately at 1/6, and it is important to realise that this
plateau is not obtained with the multi-dimensional Poisson
noise using π = 0.5. The logarithm of the noise is 
normalised by the logarithm of a very small number, namely
0.5 · e−1000
+ 0.5 · e−2000
. That is why the informativeness
will be only close to one for very little noise, whereas for a
bit of noise, informativeness will drop to zero. This effect
can be controlled by using small values for π such that the
noise in the interval [λ1; λ2] is still very little. The setting
π = e−2000/6
leads to noise values of approximately e−2000/6
in the interval [λ1; λ2], the logarithms lead then to 1/6 for
the informativeness.
The indepence-based and frequency-based informativeness
functions do not differ as much as the noise functions do.
However, for the indepence-based probability of being 
informative, we can control the average informativeness by the
definition p := λ/N whereas the control on the 
frequencybased is limited as we address next.
For the frequency-based idf , the gradient is monotonously
decreasing and we obtain for different collections the same
distances of idf -values, i. e. the parameter N does not affect
the distance. For an illustration, consider the distance 
between the value idf(tn+1) of a term tn+1 that occurs in n+1
documents, and the value idf(tn) of a term tn that occurs in
n documents.
idf(tn+1) − idf(tn) = ln
n
n + 1
The first three values of the distance function are:
idf(t2) − idf(t1) = ln(1/(1 + 1)) = 0.69
idf(t3) − idf(t2) = ln(1/(2 + 1)) = 0.41
idf(t4) − idf(t3) = ln(1/(3 + 1)) = 0.29
For the Poisson-based informativeness, the gradient decreases
first slowly for small n(t), then rapidly near n(t) ≈ λ and
then it grows again slowly for large n(t).
In conclusion, we have seen that the Poisson-based 
definition provides more control and parameter possibilities than
233
the frequency-based definition does. Whereas more control
and parameter promises to be positive for the 
personalisation of retrieval systems, it bears at the same time the 
danger of just too many parameters. The framework presented
in this paper raises the awareness about the probabilistic
and information-theoretic meanings of the parameters. The
parallel definitions of the frequency-based probability and
the Poisson-based probability of being informative made
the underlying assumptions explicit. The frequency-based
probability can be explained by binary occurrence, constant
containment and disjointness of documents. Independence
of documents leads to Poisson, where we have to be aware
that Poisson approximates the probability of a disjunction
for a large number of events, but not for a small number.
This theoretical result explains why experimental 
investigations on Poisson (see [7]) show that a Poisson estimation
does work better for frequent (bad, noisy) terms than for
rare (good, informative) terms.
In addition to the collection-wide parameter setting, the
framework presented here allows for document-dependent
settings, as explained for the independence probability. This
is in particular interesting for heterogeneous and structured
collections, since documents are different in nature (size,
quality, root document, sub document), and therefore, 
binary occurrence and constant containment are less 
appropriate than in relatively homogeneous collections.
7. SUMMARY
The definition of the probability of being informative 
transforms the informative interpretation of the idf into a 
probabilistic interpretation, and we can use the idf -based 
probability in probabilistic retrieval approaches. We showed that
the classical definition of the noise (document frequency) in
the inverse document frequency can be explained by three
assumptions: the term within-document occurrence 
probability is binary, the document containment probability is
constant, and the document containment events are disjoint.
By explicitly and mathematically formulating the 
assumptions, we showed that the classical definition of idf does not
take into account parameters such as the different nature
(size, quality, structure, etc.) of documents in a collection,
or the different nature of terms (coverage, importance, 
position, etc.) in a document. We discussed that the absence
of those parameters is compensated by a leverage effect of
the within-document term occurrence probability and the
document containment probability.
By applying an independence rather a disjointness 
assumption for the document containment, we could 
establish a link between the noise probability (term occurrence
in a collection), information theory and Poisson. From the
frequency-based and the Poisson-based probabilities of 
being noisy, we derived the frequency-based and Poisson-based
probabilities of being informative. The frequency-based 
probability is relatively smooth whereas the Poisson probability
is radical in distinguishing between noisy or not noisy, and
informative or not informative, respectively. We showed how
to smoothen the radical behaviour of Poisson with a 
multidimensional Poisson.
The explicit and mathematical formulation of idf - and
Poisson-assumptions is the main result of this paper. Also,
the paper emphasises the duality of idf and tf , collection
space and document space, respectively. Thus, the result
applies to term occurrence and document containment in a
collection, and it applies to term occurrence and position
containment in a document. This theoretical framework is
useful for understanding and deciding the parameter 
estimation and combination in probabilistic retrieval models. The
links between indepence-based noise as document frequency,
probabilistic interpretation of idf , information theory and
Poisson described in this paper may lead to variable 
probabilistic idf and tf definitions and combinations as required
in advanced and personalised information retrieval systems.
Acknowledgment: I would like to thank Mounia Lalmas,
Gabriella Kazai and Theodora Tsikrika for their comments
on the as they said heavy pieces. My thanks also go to the
meta-reviewer who advised me to improve the presentation
to make it less formidable and more accessible for those
without a theoretic bent. This work was funded by a
research fellowship from Queen Mary University of London.
8. REFERENCES
[1] A. Aizawa. An information-theoretic perspective of
tf-idf measures. Information Processing and
Management, 39:45-65, January 2003.
[2] G. Amati and C. J. Rijsbergen. Term frequency
normalization via Pareto distributions. In 24th
BCS-IRSG European Colloquium on IR Research,
Glasgow, Scotland, 2002.
[3] R. K. Belew. Finding out about. Cambridge University
Press, 2000.
[4] A. Bookstein and D. Swanson. Probabilistic models
for automatic indexing. Journal of the American
Society for Information Science, 25:312-318, 1974.
[5] I. N. Bronstein. Taschenbuch der Mathematik. Harri
Deutsch, Thun, Frankfurt am Main, 1987.
[6] K. Church and W. Gale. Poisson mixtures. Natural
Language Engineering, 1(2):163-190, 1995.
[7] K. W. Church and W. A. Gale. Inverse document
frequency: A measure of deviations from poisson. In
Third Workshop on Very Large Corpora, ACL
Anthology, 1995.
[8] T. Lafouge and C. Michel. Links between information
construction and information gain: Entropy and
bibliometric distribution. Journal of Information
Science, 27(1):39-49, 2001.
[9] E. Margulis. N-poisson document modelling. In
Proceedings of the 15th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 177-189, 1992.
[10] S. E. Robertson and S. Walker. Some simple effective
approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of the
17th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 232-241, London, et al., 1994. Springer-Verlag.
[11] S. Wong and Y. Yao. An information-theoric measure
of term specificity. Journal of the American Society
for Information Science, 43(1):54-61, 1992.
[12] S. Wong and Y. Yao. On modeling information
retrieval with probabilistic inference. ACM
Transactions on Information Systems, 13(1):38-68,
1995.
234
Controlling Overlap in Content-Oriented XML Retrieval
Charles L. A. Clarke
School of Computer Science, University of Waterloo, Canada
claclark@plg.uwaterloo.ca
ABSTRACT
The direct application of standard ranking techniques to
retrieve individual elements from a collection of XML 
documents often produces a result set in which the top ranks are
dominated by a large number of elements taken from a small
number of highly relevant documents. This paper presents
and evaluates an algorithm that re-ranks this result set, with
the aim of minimizing redundant content while preserving
the benefits of element retrieval, including the benefit of
identifying topic-focused components contained within 
relevant documents. The test collection developed by the 
INitiative for the Evaluation of XML Retrieval (INEX) forms
the basis for the evaluation.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Storage and
Retrieval-Information Search and Retrieval
General Terms
Algorithms, Measurement, Performance, Experimentation
1. INTRODUCTION
The representation of documents in XML provides an 
opportunity for information retrieval systems to take 
advantage of document structure, returning individual document
components when appropriate, rather than complete 
documents in all circumstances. In response to a user query, an
XML information retrieval system might return a mixture
of paragraphs, sections, articles, bibliographic entries and
other components. This facility is of particular benefit when
a collection contains very long documents, such as product
manuals or books, where the user should be directed to the
most relevant portions of these documents.
<article>
<fm>
<atl>Text Compression for
Dynamic Document Databases</atl>
<au>Alistair Moffat</au>
<au>Justin Zobel</au>
<au>Neil Sharman</au>
<abs><p><b>Abstract</b> For ...</p></abs>
</fm>
<bdy>
<sec><st>INTRODUCTION</st>
<ip1>Modern document databases...</ip1>
<p>There are good reasons to compress...</p>
</sec>
<sec><st>REDUCING MEMORY REQUIREMENTS</st>...
<ss1><st>2.1 Method A</st>...
</sec>
...
</bdy>
</article>
Figure 1: A journal article encoded in XML.
Figure 1 provides an example of a journal article encoded
in XML, illustrating many of the important characteristics
of XML documents. Tags indicate the beginning and end of
each element, with elements varying widely in size, from one
word to thousands of words. Some elements, such as 
paragraphs and sections, may be reasonably presented to the user
as retrieval results, but others are not appropriate. Elements
overlap each other - articles contain sections, sections 
contain subsections, and subsections contain paragraphs. Each
of these characteristics affects the design of an XML IR 
system, and each leads to fundamental problems that must be
solved in an successful system. Most of these fundamental
problems can be solved through the careful adaptation of
standard IR techniques, but the problems caused by overlap
are unique to this area [4,11] and form the primary focus of
this paper.
The article of figure 1 may be viewed as an XML tree,
as illustrated in figure 2. Formally, a collection of XML
documents may be represented as a forest of ordered, rooted
trees, consisting of a set of nodes N and a set of directed
edges E connecting these nodes. For each node x ∈ N , the
notation x.parent refers to the parent node of x, if one exists,
and the notation x.children refers to the set of child nodes
sec
bdyfm
atl au au au
abs
p
b
st ip1
sec
st
ss1
st
article
p
Figure 2: Example XML tree.
of x. Since an element may be represented by the node at
its root, the output of an XML IR system may be viewed as
a ranked list of the top-m nodes.
The direct application of a standard relevance ranking
technique to a set of XML elements can produce a result
in which the top ranks are dominated by many structurally
related elements. A high scoring section is likely to contain
several high scoring paragraphs and to be contained in an
high scoring article. For example, many of the elements in
figure 2 would receive a high score on the keyword query
text index compression algorithms. If each of these
elements are presented to a user as an individual and 
separate result, she may waste considerable time reviewing and
rejecting redundant content.
One possible solution is to report only the highest 
scoring element along a given path in the tree, and to remove
from the lower ranks any element containing it, or contained
within it. Unfortunately, this approach destroys some of the
possible benefits of XML IR. For example, an outer element
may contain a substantial amount of information that does
not appear in an inner element, but the inner element may
be heavily focused on the query topic and provide a short
overview of the key concepts. In such cases, it is reasonable
to report elements which contain, or are contained in, higher
ranking elements. Even when an entire book is relevant, a
user may still wish to have the most important paragraphs
highlighted, to guide her reading and to save time [6].
This paper presents a method for controlling overlap. 
Starting with an initial element ranking, a re-ranking algorithm
adjusts the scores of lower ranking elements that contain, or
are contained within, higher ranking elements, reflecting the
fact that this information may now be redundant. For 
example, once an element representing a section appears in the
ranking, the scores for the paragraphs it contains and the
article that contains it are reduced. The inspiration for this
strategy comes partially from recent work on structured 
documents retrieval, where terms appearing in different fields,
such as the title and body, are given different weights [20].
Extending that approach, the re-ranking algorithm varies
weights dynamically as elements are processed.
The remainder of the paper is organized as follows: After
a discussion of background work and evaluation 
methodology, a baseline retrieval method is presented in section 4.
This baseline method represents a reasonable adaptation of
standard IR technology to XML. Section 5 then outlines a
strategy for controlling overlap, using the baseline method as
a starting point. A re-ranking algorithm implementing this
strategy is presented in section 6 and evaluated in section 7.
Section 8 discusses an extended version of the algorithm.
2. BACKGROUND
This section provides a general overview of XML 
information retrieval and discusses related work, with an emphasis
on the fundamental problems mentioned in the introduction.
Much research in the area of XML retrieval views it from a
traditional database perspective, being concerned with such
problems as the implementation of structured query 
languages [5] and the processing of joins [1]. Here, we take
a content oriented IR perceptive, focusing on XML 
documents that primarily contain natural language data and
queries that are primarily expressed in natural language.
We assume that these queries indicate only the nature of
desired content, not its structure, and that the role of the
IR system is to determine which elements best satisfy the
underlying information need. Other IR research has 
considered mixed queries, in which both content and structural
requirements are specified [2,6,14,17,23].
2.1 Term and Document Statistics
In traditional information retrieval applications the 
standard unit of retrieval is taken to be the document. 
Depending on the application, this term might be interpreted
to encompass many different objects, including web pages,
newspaper articles and email messages.
When applying standard relevance ranking techniques in
the context of XML IR, a natural approach is to treat each
element as a separate document, with term statistics 
available for each [16]. In addition, most ranking techniques
require global statistics (e.g. inverse document frequency)
computed over the collection as a whole. If we consider this
collection to include all elements that might be returned by
the system, a specific occurrence of a term may appear in
several different documents, perhaps in elements 
representing a paragraph, a subsection, a section and an article.
It is not appropriate to compute inverse document frequency
under the assumption that the term is contained in all of
these elements, since the number of elements that contain a
term depends entirely on the structural arrangement of the
documents [13,23].
2.2 Retrievable Elements
While an XML IR system might potentially retrieve any
element, many elements may not be appropriate as retrieval
results. This is usually the case when elements contain very
little text [10]. For example, a section title containing only
the query terms may receive a high score from a ranking 
algorithm, but alone it would be of limited value to a user, who
might prefer the actual section itself. Other elements may
reflect the document"s physical, rather than logical, 
structure, which may have little or no meaning to a user. An
effective XML IR system must return only those elements
that have sufficient content to be usable and are able to
stand alone as independent objects [15,18]. Standard 
document components such as paragraphs, sections, subsections,
and abstracts usually meet these requirements; titles, 
italicized phrases, and individual metadata fields often do not.
2.3 Evaluation Methodology
Over the past three years, the INitiative for the 
Evaluation of XML Retrieval (INEX) has encouraged research into
XML information retrieval technology [7,8]. INEX is an 
experimental conference series, similar to TREC, with groups
from different institutions completing one or more 
experimental tasks using their own tools and systems, and 
comparing their results at the conference itself. Over 50 groups
participated in INEX 2004, and the conference has become
as influential in the area of XML IR as TREC is in other IR
areas. The research described in this paper, as well as much
of the related work it cites, depends on the test collections
developed by INEX.
Overlap causes considerable problems with retrieval 
evaluation, and the INEX organizers and participants have 
wrestled with these problems since the beginning. While 
substantial progress has been made, these problem are still not
completely solved. Kazai et al. [11] provide a detailed 
exposition of the overlap problem in the context of INEX 
retrieval evaluation and discuss both current and proposed
evaluation metrics. Many of these metrics are applied to
evaluate the experiments reported in this paper, and they
are briefly outlined in the next section.
3. INEX 2004
Space limitations prevent the inclusion of more than a
brief summary of INEX 2004 tasks and evaluation 
methodology. For detailed information, the proceedings of the 
conference itself should be consulted [8].
3.1 Tasks
For the main experimental tasks, INEX 2004 participants
were provided with a collection of 12,107 articles taken from
the IEEE Computer Societies magazines and journals 
between 1995 and 2002. Each document is encoded in XML
using a common DTD, with the document of figures 1 and 2
providing one example.
At INEX 2004, the two main experimental tasks were both
adhoc retrieval tasks, investigating the performance of 
systems searching a static collection using previously unseen
topics. The two tasks differed in the types of topics they
used. For one task, the content-only or CO task, the
topics consist of short natural language statements with no
direct reference to the structure of the documents in the 
collection. For this task, the IR system is required to select the
elements to be returned. For the other task, the 
contentand-structure or CAS task, the topics are written in an
XML query language [22] and contain explicit references to
document structure, which the IR system must attempt to
satisfy. Since the work described in this paper is directed
at the content-only task, where the IR system receives no
guidance regarding the elements to return, the CAS task is
ignored in the remainder of our description.
In 2004, 40 new CO topics were selected by the conference
organizers from contributions provided by the conference
participants. Each topic includes a short keyword query,
which is executed over the collection by each participating
group on their own XML IR system. Each group could
submit up to three experimental runs consisting of the top
m = 1500 elements for each topic.
3.2 Relevance Assessment
Since XML IR is concerned with locating those elements
that provide complete coverage of a topic while containing as
little extraneous information as possible, simple relevant
vs. not relevant judgments are not sufficient. Instead, the
INEX organizers adopted two dimensions for relevance 
assessment: The exhaustivity dimension reflects the degree to
which an element covers the topic, and the specificity 
dimension reflects the degree to which an element is focused on the
topic. A four-point scale is used in both dimensions. Thus,
a (3,3) element is highly exhaustive and highly specific, a
(1,3) element is marginally exhaustive and highly specific,
and a (0,0) element is not relevant. Additional information
on the assessment methodology may be found in Piwowarski
and Lalmas [19], who provide a detailed rationale.
3.3 Evaluation Metrics
The principle evaluation metric used at INEX 2004 is a
version of mean average precision (MAP), adjusted by 
various quantization functions to give different weights to 
different elements, depending on their exhaustivity and specificity
values. One variant, the strict quantization function gives a
weight of 1 to (3,3) elements and a weight of 0 to all others.
This variant is essentially the familiar MAP value, with (3,3)
elements treated as relevant and all other elements treated
as not relevant. Other quantization functions are designed
to give partial credit to elements which are near misses,
due to a lack or exhaustivity and/or specificity. Both the
generalized quantization function and the specificity-oriented
generalization (sog) function credit elements according to
their degree of relevance [11], with the second function 
placing greater emphasis on specificity. This paper reports 
results of this metric using all three of these quantization 
functions. Since this metric was first introduced at INEX 2002,
it is generally referred as the inex-2002 metric.
The inex-2002 metric does not penalize overlap. In 
particular, both the generalized and sog quantization functions
give partial credit to a near miss even when a (3,3) 
element overlapping it is reported at a higher rank. To address
this problem, Kazai et al. [11] propose an XML cumulated
gain metric, which compares the cumulated gain [9] of a
ranked list to an ideal gain vector. This ideal gain vector
is constructed from the relevance judgments by 
eliminating overlap and retaining only best element along a given
path. Thus, the XCG metric rewards retrieval runs that
avoid overlap. While XCG was not used officially at INEX
2004, a version of it is likely to be used in the future.
At INEX 2003, yet another metric was introduced to 
ameliorate the perceived limitations of the inex-2002 metric.
This inex-2003 metric extends the definitions of precision
and recall to consider both the size of reported components
and the overlap between them. Two versions were created,
one that considered only component size and another that
considered both size and overlap. While the inex-2003 
metric exhibits undesirable anomalies [11], and was not used in
2004, values are reported in the evaluation section to provide
an additional instrument for investigating overlap.
4. BASELINE RETRIEVAL METHOD
This section provides an overview of baseline XML 
information retrieval method currently used in the MultiText
IR system, developed by the Information Retrieval Group at
the University of Waterloo [3]. This retrieval method results
from the adaptation and tuning of the Okapi BM25 
measure [21] to the XML information retrieval task. The 
MultiText system performed respectably at INEX 2004, placing
in the top ten under all of the quantization functions, and
placing first when the quantization function emphasized 
exhaustivity.
To support retrieval from XML and other structured 
document types, the system provides generalized queries of the
form:
rank X by Y
where X is a sub-query specifying a set of document elements
to be ranked and Y is a vector of sub-queries specifying
individual retrieval terms.
For our INEX 2004 runs, the sub-query X specified a list
of retrievable elements as those with tag names as follows:
abs app article bb bdy bm fig fm ip1
li p sec ss1 ss2 vt
This list includes bibliographic entries (bb) and figure 
captions (fig) as well as paragraphs, sections and subsections.
Prior to INEX 2004, the INEX collection and the INEX 2003
relevance judgments were manually analyzed to select these
tag names. Tag names were selected on the basis of their
frequency in the collection, the average size of their 
associated elements, and the relative number of positive relevance
judgments they received. Automating this selection process
is planned as future work.
For INEX 2004, the term vector Y was derived from the
topic by splitting phrases into individual words, eliminating
stopwords and negative terms (those starting with -), and
applying a stemmer. For example, keyword field of topic
166
+"tree edit distance" + XML -image
became the four-term query
"$tree" "$edit" "$distance" "$xml"
where the $ operator within a quoted string stems the
term that follows it.
Our implementation of Okapi BM25 is derived from the
formula of Robertson et al. [21] by setting parameters k2 = 0
and k3 = ∞. Given a term set Q, an element x is assigned
the score
 
t∈Q
w(1)
qt
(k1 + 1)xt
K + xt
(1)
where
w(1)
= log ¡
D − Dt + 0.5
Dt + 0.5 ¢
D = number of documents in the corpus
Dt = number of documents containing t
qt = frequency that t occurs in the topic
xt = frequency that t occurs in x
K = k1((1 − b) + b · lx/lavg)
lx = length of x
lavg = average document length
0.07
0.08
0.09
0.10
0.11
0.12
0.13
0.14
0.15
0 2 4 6 8 10 12 14 16
MeanAveragePrecision(inex-2002)
k1
strict
generalized
sog
Figure 3: Impact of k1 on inex-2002 mean average
precision with b = 0.75 (INEX 2003 CO topics).
Prior to INEX 2004, the INEX 2003 topics and judgments
were used to tune the b and k1 parameters, and the impact
of this tuning is discussed later in this section.
For the purposes of computing document-level statistics
(D, Dt and lavg) a document is defined to be an article.
These statistics are used for ranking all element types. 
Following the suggestion of Kamps et al. [10], the retrieval 
results are filtered to eliminate very short elements, those less
than 25 words in length.
The use of article statistics for all element types might
be questioned. This approach may be justified by 
viewing the collection as a set of articles to be searched using
standard document-oriented techniques, where only articles
may be returned. The score computed for an element is 
essentially the score it would receive if it were added to the
collection as a new document, ignoring the minor 
adjustments needed to the document-level statistics. Nonetheless,
we plan to examine this issue again in the future.
In our experience, the performance of BM25 typically 
benefits from tuning the b and k1 parameters to the 
collection, whenever training queries are available for this 
purpose. Prior to INEX 2004, we trained the MultiText system
using the INEX 2003 queries. As a starting point we used
the values b = 0.75 and k1 = 1.2, which perform well on
TREC adhoc collections and are used as default values in
our system. The results were surprising. Figure 3 shows the
result of varying k1 with b = 0.75 on the MAP values under
three quantization functions. In our experience, optimal 
values for k1 are typically in the range 0.0 to 2.0. In this case,
large values are required for good performance. Between
k1 = 1.0 and k1 = 6.0 MAP increases by over 15% under
the strict quantization. Similar improvements are seen 
under the generalized and sog quantizations. In contrast, our
default value of b = 0.75 works well under all quantization
functions (figure 4). After tuning over a wide range of 
values under several quantization functions, we selected values
of k = 10.0 and b = 0.80 for our INEX 2004 experiments,
and these values are used for the experiments reported in
section 7.
0.07
0.08
0.09
0.10
0.11
0.12
0.13
0.14
0.15
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
MeanAveragePrecision(inex-2002)
b
strict
generalized
sog
Figure 4: Impact of b on inex-2002 mean average
precision with k1 = 10 (INEX 2003 CO topics).
5. CONTROLLING OVERLAP
Starting with an element ranking generated by the 
baseline method described in the previous section, elements are
re-ranked to control overlap by iteratively adjusting the scores
of those elements containing or contained in higher ranking
elements. At a conceptual level, re-ranking proceeds as 
follows:
1. Report the highest ranking element.
2. Adjust the scores of the unreported elements.
3. Repeat steps 1 and 2 until m elements are reported.
One approach to adjusting the scores of unreported elements
in step 2 might be based on the Okapi BM25 scores of the
involved elements. For example, assume a paragraph with
score p is reported in step 1. In step 2, the section 
containing the paragraph might then have its score s lowered
by an amount α · p to reflect the reduced contribution the
paragraph should make to the section"s score.
In a related context, Robertson et al. [20] argue strongly
against the linear combination of Okapi scores in this 
fashion. That work considers the problem of assigning different
weights to different document fields, such as the title and
body associated with Web pages. A common approach to
this problem scores the title and body separately and 
generates a final score as a linear combination of the two. 
Robertson et al. discuss the theoretical flaws in this approach and
demonstrate experimentally that it can actually harm 
retrieval effectiveness. Instead, they apply the weights at the
term frequency level, with an occurrence of a query term
t in the title making a greater contribution to the score
than an occurrence in the body. In equation 1, xt becomes
α0 · yt + α1 · zt, where yt is the number of times t occurs in
the title and zt is the number of times t occurs in the body.
Translating this approach to our context, the 
contribution of terms appearing in elements is dynamically reduced
as they are reported. The next section presents and 
analysis a simple re-ranking algorithm that follows this strategy.
The algorithm is evaluated experimentally in section 7. One
limitation of the algorithm is that the contribution of terms
appearing in reported elements is reduced by the same 
factor regardless of the number of reported elements in which
it appears. In section 8 the algorithm is extended to apply
increasing weights, lowering the score, when a term appears
in more than one reported element.
6. RE-RANKING ALGORITHM
The re-ranking algorithm operates over XML trees, such
as the one appearing in figure 2. Input to the algorithm is
a list of n elements ranked according to their initial BM25
scores. During the initial ranking the XML tree is 
dynamically re-constructed to include only those nodes with 
nonzero BM25 scores, so n may be considerably less than |N |.
Output from the algorithm is a list of the top m elements,
ranked according to their adjusted scores.
An element is represented by the node x ∈ N at its root.
Associated with this node are fields storing the length of
element, term frequencies, and other information required
by the re-ranking algorithm, as follows:
x.f - term frequency vector
x.g - term frequency adjustments
x.l - element length
x.score - current Okapi BM25 score
x.reported - boolean flag, initially false
x.children - set of child nodes
x.parent - parent node, if one exists
These fields are populated during the initial ranking process,
and updated as the algorithm progresses. The vector x.f
contains term frequency information corresponding to each
term in the query. The vector x.g is initially zero and is
updated by the algorithm as elements are reported.
The score field contains the current BM25 score for the
element, which will change as the values in x.g change. The
score is computed using equation 1, with the xt value for
each term determined by a combination of the values in x.f
and x.g. Given a term t ∈ Q, let ft be the component of
x.f corresponding to t, and let gt be the component of x.g
corresponding to t, then:
xt = ft − α · gt (2)
For processing by the re-ranking algorithm, nodes are
stored in priority queues, ordered by decreasing score. Each
priority queue PQ supports three operations:
PQ.front() - returns the node with greatest score
PQ.add (x) - adds node x to the queue
PQ.remove(x) - removes node x from the queue
When implemented using standard data structures, the front
operation requires O(1) time, and the other operations 
require O(log n) time, where n is the size of the queue.
The core of the re-ranking algorithm is presented in 
figure 5. The algorithm takes as input the priority queue S
containing the initial ranking, and produces the top-m 
reranked nodes in the priority queue F. After initializing F to
be empty on line 1, the algorithm loops m times over lines 
215, transferring at least one node from S to F during each
iteration. At the start of each iteration, the unreported node
at the front of S has the greatest adjusted score, and it is
removed and added to F. The algorithm then traverses the
1 F ← ∅
2 for i ← 1 to m do
3 x ← S.front()
4 S.remove(x)
5 x.reported ← true
6 F.add(x)
7
8 foreach y ∈ x.children do
9 Down (y)
10 end do
11
12 if x is not a root node then
13 Up (x, x.parent)
14 end if
15 end do
Figure 5: Re-Ranking Algorithm - As input, the 
algorithm takes a priority queue S, containing XML
nodes ranked by their initial scores, and returns
its results in priority queue F, ranked by adjusted
scores.
1 Up(x, y) ≡
2 S.remove(y)
3 y.g ← y.g + x.f − x.g
4 recompute y.score
5 S.add(y)
6 if y is not a root node then
7 Up (x, y.parent)
8 end if
9
10 Down(x) ≡
11 if not x.reported then
12 S.remove(x)
14 x.g ← x.f
15 recompute x.score
16 if x.score > 0 then
17 F.add(x)
18 end if
19 x.reported ← true
20 foreach y ∈ x.children do
21 Down (y)
22 end do
23 end if
Figure 6: Tree traversal routines called by the 
reranking algorithm.
0.0
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.25
0.26
0.27
0.28
0.29
0.30
0.31
0.32
0.33
0.34
0.35
0.36
MeanAveragePrecision(inex-2002)
XMLCumulatedGain(XCG)
alpha
MAP (strict)
MAP (generalized)
MAP (sog)
XCG (sog2)
Figure 7: Impact of α on XCG and inex-2002 MAP
(INEX 2004 CO topics; assessment set I).
node"s ancestors (lines 8-10) and descendants (lines 12-14)
adjusting the scores of these nodes.
The tree traversal routines, Up and Down are given in 
figure 6. The Up routine removes each ancestor node from S,
adjusts its term frequency values, recomputes its score, and
adds it back into S. The adjustment of the term frequency
values (line 3) adds to y.g only the previously unreported
term occurrences in x. Re-computation of the score on line 4
uses equations 1 and 2. The Down routine performs a similar
operation on each descendant. However, since the contents
of each descendant are entirely contained in a reported 
element its final score may be computed, and it is removed
from S and added to F.
In order to determine the time complexity of the 
algorithm, first note that a node may be an argument to Down
at most once. Thereafter, the reported flag of its parent is
true. During each call to Down a node may be moved from
S to F, requiring O(log n) time. Thus, the total time for all
calls to Down is O(n log n), and we may temporarily ignore
lines 8-10 of figure 5 when considering the time complexity
of the loop over lines 2-15. During each iteration of this loop,
a node and each of its ancestors are removed from a priority
queue and then added back into a priority queue. Since a
node may have at most h ancestors, where h is the maximum
height of any tree in the collection, each of the m iterations
requires O(h log n) time. Combining these observations 
produces an overall time complexity of O((n + mh) log n).
In practice, re-ranking an INEX result set requires less
than 200ms on a three-year-old desktop PC.
7. EVALUATION
None of the metrics described in section 3.3 is a close fit
with the view of overlap advocated by this paper. 
Nonetheless, when taken together they provide insight into the 
behaviour of the re-ranking algorithm. The INEX evaluation
packages (inex_eval and inex_eval_ng) were used to 
compute values for the inex-2002 and inex-2003 metrics. Values
for the XCG metrics were computed using software supplied
by its inventors [11].
Figure 7 plots the three variants of inex-2002 MAP metric
together with the XCG metric. Values for these metrics
0.0
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
MeanAveragePrecision(inex-2003)
alpha
strict, overlap not considered
strict, overlap considered
generalized, overlap not considered
generalized, overlap considered
Figure 8: Impact of α on inex-2003 MAP (INEX
2004 CO topics; assessment set I).
are plotted for values of α between 0.0 and 1.0. Recalling
that the XCG metric is designed to penalize overlap, while
the inex-2002 metric ignores overlap, the conflict between
the metrics is obvious. The MAP values at one extreme
(α = 0.0) and the XCG value at the other extreme (α =
1.0) represent retrieval performance comparable to the best
systems at INEX 2004 [8,12].
Figure 8 plots values of the inex-2003 MAP metric for two
quantizations, with and without consideration of overlap.
Once again, conflict is apparent, with the influence of α
substantially lessened when overlap is considered.
8. EXTENDED ALGORITHM
One limitation of the re-ranking algorithm is that a single
weight α is used to adjust the scores of both the ancestors
and descendants of reported elements. An obvious extension
is to use different weights in these two cases. Furthermore,
the same weight is used regardless of the number of times
an element is contained in a reported element. For example,
a paragraph may form part of a reported section and then
form part of a reported article. Since the user may now
have seen this paragraph twice, its score should be further
lowered by increasing the value of the weight.
Motivated by these observations, the re-ranking algorithm
may be extended with a series of weights
1 = β0 ≥ β1 ≥ β2 ≥ ... ≥ βM ≥ 0.
where βj is the weight applied to a node that has been a
descendant of a reported node j times. Note that an upper
bound on M is h, the maximum height of any XML tree
in the collection. However, in practice M is likely to be
relatively small (perhaps 3 or 4).
Figure 9 presents replacements for the Up and Down 
routines of figure 6, incorporating this series of weights. One
extra field is required in each node, as follows:
x.j - down count
The value of x.j is initially set to zero in all nodes and is
incremented each time Down is called with x as its argument.
When computing the score of node, the value of x.j selects
1 Up(x, y) ≡
2 if not y.reported then
3 S.remove(y)
4 y.g ← y.g + x.f − x.g
5 recompute y.score
6 S.add(y)
8 if y is not a root node then
9 Up (x, y.parent)
10 end if
11 end if
12
13 Down(x) ≡
14 if x.j < M then
15 x.j ← x.j + 1
16 if not x.reported then
17 S.remove(x)
18 recompute x.score
19 S.add(x)
20 end if
21 foreach y ∈ x.children do
22 Down (y)
23 end do
24 end if
Figure 9: Extended tree traversal routines.
the weight to be applied to the node by adjusting the value
of xt in equation 1, as follows:
xt = βx.j · (ft − α · gt) (3)
where ft and gt are the components of x.f and x.g 
corresponding to term t.
A few additional changes are required to extend Up and
Down. The Up routine returns immediately (line 2) if its
argument has already been reported, since term frequencies
have already been adjusted in its ancestors. The Down 
routine does not report its argument, but instead recomputes
its score and adds it back into S.
A node cannot be an argument to Down more than M +1
times, which in turn implies an overall time complexity of
O((nM + mh) log n). Since M ≤ h and m ≤ n, the time
complexity is also O(nh log n).
9. CONCLUDING DISCUSSION
When generating retrieval results over an XML collection,
some overlap in the results should be tolerated, and may be
beneficial. For example, when a highly exhaustive and fairly
specific (3,2) element contains a much smaller (2,3) element,
both should be reported to the user, and retrieval algorithms
and evaluation metrics should respect this relationship. The
algorithm presented in this paper controls overlap by 
weighting the terms occurring in reported elements to reflect their
reduced importance.
Other approaches may also help to control overlap. For
example, when XML retrieval results are presented to users
it may be desirable to cluster structurally related elements
together, visually illustrating the relationships between them.
While this style of user interface may help a user cope with
overlap, the strategy presented in this paper continues to be
applicable, by determining the best elements to include in
each cluster.
At Waterloo, we continue to develop and test our ideas
for INEX 2005. In particular, we are investigating methods
for learning the α and βj weights. We are also re-evaluating
our approach to document statistics and examining 
appropriate adjustments to the k1 parameter as term weights
change [20].
10. ACKNOWLEDGMENTS
Thanks to Gabriella Kazai and Arjen de Vries for 
providing an early version of their software for computing the XCG
metric, and thanks to Phil Tilker and Stefan B¨uttcher for
their help with the experimental evaluation. In part, 
funding for this project was provided by IBM Canada through
the National Institute for Software Research.
11. REFERENCES
[1] N. Bruno, N. Koudas, and D. Srivastava. Holistic twig
joins: Optimal XML pattern matching. In Proceedings
of the 2002 ACM SIGMOD International Conference
on the Management of Data, pages 310-321, Madison,
Wisconsin, June 2002.
[2] D. Carmel, Y. S. Maarek, M. Mandelbrod, Y. Mass,
and A. Soffer. Searching XML documents via XML
fragments. In Proceedings of the 26th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
151-158, Toronto, Canada, 2003.
[3] C. L. A. Clarke and P. L. Tilker. MultiText
experiments for INEX 2004. In INEX 2004 Workshop
Proceedings, 2004. Published in LNCS 3493 [8].
[4] A. P. de Vries, G. Kazai, and M. Lalmas. Tolerance to
irrelevance: A user-effort oriented evaluation of
retrieval systems without predefined retrieval unit. In
RIAO 2004 Conference Proceedings, pages 463-473,
Avignon, France, April 2004.
[5] D. DeHaan, D. Toman, M. P. Consens, and M. T.
¨Ozsu. A comprehensive XQuery to SQL translation
using dynamic interval encoding. In Proceedings of the
2003 ACM SIGMOD International Conference on the
Management of Data, San Diego, June 2003.
[6] N. Fuhr and K. Großjohann. XIRQL: A query
language for information retrieval in XML documents.
In Proceedings of the 24th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 172-180, New Orleans,
September 2001.
[7] N. Fuhr, M. Lalmas, and S. Malik, editors. Initiative
for the Evaluation of XML Retrieval. Proceedings of
the Second Workshop (INEX 2003), Dagstuhl,
Germany, December 2003.
[8] N. Fuhr, M. Lalmas, S. Malik, and Zolt´an Szl´avik,
editors. Initiative for the Evaluation of XML
Retrieval. Proceedings of the Third Workshop (INEX
2004), Dagstuhl, Germany, December 2004. Published
as Advances in XML Information Retrieval, Lecture
Notes in Computer Science, volume 3493, Springer,
2005.
[9] K. J¨avelin and J. Kek¨al¨ainen. Cumulated gain-based
evaluation of IR techniques. ACM Transactions on
Information Systems, 20(4):422-446, 2002.
[10] J. Kamps, M. de Rijke, and B. Sigurbj¨ornsson. Length
normalization in XML retrieval. In Proceedings of the
27th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 80-87, Sheffield, UK, July 2004.
[11] G. Kazai, M. Lalmas, and A. P. de Vries. The overlap
problem in content-oriented XML retrieval evaluation.
In Proceedings of the 27th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 72-79, Sheffield, UK,
July 2004.
[12] G. Kazai, M. Lalmas, and A. P. de Vries. Reliability
tests for the XCG and inex-2002 metrics. In INEX
2004 Workshop Proceedings, 2004. Published in LNCS
3493 [8].
[13] J. Kek¨al¨ainen, M. Junkkari, P. Arvola, and T. Aalto.
TRIX 2004 - Struggling with the overlap. In INEX
2004 Workshop Proceedings, 2004. Published in LNCS
3493 [8].
[14] S. Liu, Q. Zou, and W. W. Chu. Configurable
indexing and ranking for XML information retrieval.
In Proceedings of the 27th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 88-95, Sheffield, UK,
July 2004.
[15] Y. Mass and M. Mandelbrod. Retrieving the most
relevant XML components. In INEX 2003 Workshop
Proceedings, Dagstuhl, Germany, December 2003.
[16] Y. Mass and M. Mandelbrod. Component ranking and
automatic query refinement for XML retrieval. In
INEX 2004 Workshop Proceedings, 2004. Published in
LNCS 3493 [8].
[17] P. Ogilvie and J. Callan. Hierarchical language models
for XML component retrieval. In INEX 2004
Workshop Proceedings, 2004. Published in LNCS
3493 [8].
[18] J. Pehcevski, J. A. Thom, and A. Vercoustre. Hybrid
XML retrieval re-visited. In INEX 2004 Workshop
Proceedings, 2004. Published in LNCS 3493 [8].
[19] B. Piwowarski and M. Lalmas. Providing consistent
and exhaustive relevance assessments for XML
retrieval evaluation. In Proceedings of the 13th ACM
Conference on Information and Knowledge
Management, pages 361-370, Washington, DC,
November 2004.
[20] S. Robertson, H. Zaragoza, and M. Taylor. Simple
BM25 extension to multiple weighted fields. In
Proceedings of the 13th ACM Conference on
Information and Knowledge Management, pages
42-50, Washington, DC, November 2004.
[21] S. E. Robertson, S. Walker, and M. Beaulieu. Okapi at
TREC-7: Automatic ad-hoc, filtering, VLC and
interactive track. In Proceedings of the Seventh Text
REtrieval Conference, Gaithersburg, MD, November
1998.
[22] A. Trotman and B. Sigurbj¨ornsson. NEXI, now and
next. In INEX 2004 Workshop Proceedings, 2004.
Published in LNCS 3493 [8].
[23] J. Vittaut, B. Piwowarski, and P. Gallinari. An
algebra for structured queries in bayesian networks. In
INEX 2004 Workshop Proceedings, 2004. Published in
LNCS 3493 [8].
HITS on the Web: How does it Compare?
Marc Najork
Microsoft Research
1065 La Avenida
Mountain View, CA, USA
najork@microsoft.com
Hugo Zaragoza
∗
Yahoo! Research Barcelona
Ocata 1
Barcelona 08003, Spain
hugoz@es.yahoo-inc.com
Michael Taylor
Microsoft Research
7 J J Thompson Ave
Cambridge CB3 0FB, UK
mitaylor@microsoft.com
ABSTRACT
This paper describes a large-scale evaluation of the 
effectiveness of HITS in comparison with other link-based 
ranking algorithms, when used in combination with a 
state-ofthe-art text retrieval algorithm exploiting anchor text. We
quantified their effectiveness using three common 
performance measures: the mean reciprocal rank, the mean 
average precision, and the normalized discounted cumulative
gain measurements. The evaluation is based on two large
data sets: a breadth-first search crawl of 463 million web
pages containing 17.6 billion hyperlinks and referencing 2.9
billion distinct URLs; and a set of 28,043 queries sampled
from a query log, each query having on average 2,383 
results, about 17 of which were labeled by judges. We found
that HITS outperforms PageRank, but is about as 
effective as web-page in-degree. The same holds true when any
of the link-based features are combined with the text 
retrieval algorithm. Finally, we studied the relationship 
between query specificity and the effectiveness of selected 
features, and found that link-based features perform better for
general queries, whereas BM25F performs better for specific
queries.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
Storage and Retrieval-search process, selection process
General Terms
Algorithms, Measurement, Experimentation
1. INTRODUCTION
Link graph features such as in-degree and PageRank have
been shown to significantly improve the performance of text
retrieval algorithms on the web. The HITS algorithm is also
believed to be of interest for web search; to some degree,
one may expect HITS to be more informative that other
link-based features because it is query-dependent: it tries to
measure the interest of pages with respect to a given query.
However, it remains unclear today whether there are 
practical benefits of HITS over other link graph measures. This
is even more true when we consider that modern retrieval
algorithms used on the web use a document representation
which incorporates the document"s anchor text, i.e. the text
of incoming links. This, at least to some degree, takes the
link graph into account, in a query-dependent manner.
Comparing HITS to PageRank or in-degree empirically is
no easy task. There are two main difficulties: scale and 
relevance. Scale is important because link-based features are
known to improve in quality as the document graph grows.
If we carry out a small experiment, our conclusions won"t
carry over to large graphs such as the web. However, 
computing HITS efficiently on a graph the size of a realistic web
crawl is extraordinarily difficult. Relevance is also crucial
because we cannot measure the performance of a feature in
the absence of human judgments: what is crucial is ranking
at the top of the ten or so documents that a user will peruse.
To our knowledge, this paper is the first attempt to 
evaluate HITS at a large scale and compare it to other link-based
features with respect to human evaluated judgment.
Our results confirm many of the intuitions we have about
link-based features and their relationship to text retrieval
methods exploiting anchor text. This is reassuring: in the
absence of a theoretical model capable of tying these 
measures with relevance, the only way to validate our intuitions
is to carry out realistic experiments. However, we were quite
surprised to find that HITS, a query-dependent feature, is
about as effective as web page in-degree, the most 
simpleminded query-independent link-based feature. This 
continues to be true when the link-based features are combined
with a text retrieval algorithm exploiting anchor text.
The remainder of this paper is structured as follows: 
Section 2 surveys related work. Section 3 describes the data
sets we used in our study. Section 4 reviews the 
performance measures we used. Sections 5 and 6 describe the
PageRank and HITS algorithms in more detail, and sketch
the computational infrastructure we employed to carry out
large scale experiments. Section 7 presents the results of our
evaluations, and Section 8 offers concluding remarks.
2. RELATED WORK
The idea of using hyperlink analysis for ranking web search
results arose around 1997, and manifested itself in the HITS
[16, 17] and PageRank [5, 21] algorithms. The popularity
of these two algorithms and the phenomenal success of the
Google search engine, which uses PageRank, have spawned
a large amount of subsequent research.
There are numerous attempts at improving the 
effectiveness of HITS and PageRank. Query-dependent link-based
ranking algorithms inspired by HITS include SALSA [19],
Randomized HITS [20], and PHITS [7], to name a few.
Query-independent link-based ranking algorithms inspired
by PageRank include TrafficRank [22], BlockRank [14], and
TrustRank [11], and many others.
Another line of research is concerned with analyzing the
mathematical properties of HITS and PageRank. For 
example, Borodin et al. [3] investigated various theoretical 
properties of PageRank, HITS, SALSA, and PHITS, including
their similarity and stability, while Bianchini et al. [2] 
studied the relationship between the structure of the web graph
and the distribution of PageRank scores, and Langville and
Meyer examined basic properties of PageRank such as 
existence and uniqueness of an eigenvector and convergence of
power iteration [18].
Given the attention that has been paid to improving the
effectiveness of PageRank and HITS, and the thorough 
studies of the mathematical properties of these algorithms, it is
somewhat surprising that very few evaluations of their 
effectiveness have been published. We are aware of two studies
that have attempted to formally evaluate the effectiveness of
HITS and of PageRank. Amento et al. [1] employed 
quantitative measures, but based their experiments on the result
sets of just 5 queries and the web-graph induced by topical
crawls around the result set of each query. A more recent
study by Borodin et al. [4] is based on 34 queries, result sets
of 200 pages per query obtained from Google, and a 
neighborhood graph derived by retrieving 50 in-links per result
from Google. By contrast, our study is based on over 28,000
queries and a web graph covering 2.9 billion URLs.
3. OUR DATA SETS
Our evaluation is based on two data sets: a large web
graph and a substantial set of queries with associated results,
some of which were labeled by human judges.
Our web graph is based on a web crawl that was 
conducted in a breadth-first-search fashion, and successfully
retrieved 463,685,607 HTML pages. These pages contain
17,672,011,890 hyperlinks (after eliminating duplicate 
hyperlinks embedded in the same web page), which refer to
a total of 2,897,671,002 URLs. Thus, at the end of the
crawl there were 2,433,985,395 URLs in the frontier set
of the crawler that had been discovered, but not yet 
downloaded. The mean out-degree of crawled web pages is 38.11;
the mean in-degree of discovered pages (whether crawled or
not) is 6.10. Also, it is worth pointing out that there is a
lot more variance in in-degrees than in out-degrees; some
popular pages have millions of incoming links. As we will
see, this property affects the computational cost of HITS.
Our query set was produced by sampling 28,043 queries
from the MSN Search query log, and retrieving a total of
66,846,214 result URLs for these queries (using commercial
search engine technology), or about 2,838 results per query
on average. It is important to point out that our 2.9 billion
URL web graph does not cover all these result URLs. In
fact, only 9,525,566 of the result URLs (about 14.25%) were
covered by the graph.
485,656 of the results in the query set (about 0.73% of
all results, or about 17.3 results per query) were rated by
human judges as to their relevance to the given query, and
labeled on a six-point scale (the labels being definitive,
excellent, good, fair, bad and detrimental). 
Results were selected for judgment based on their commercial
search engine placement; in other words, the subset of 
labeled results is not random, but biased towards documents
considered relevant by pre-existing ranking algorithms.
Involving a human in the evaluation process is extremely
cumbersome and expensive; however, human judgments are
crucial for the evaluation of search engines. This is so 
because no document features have been found yet that can
effectively estimate the relevance of a document to a user
query. Since content-match features are very unreliable (and
even more so link features, as we will see) we need to ask
a human to evaluate the results in order to compare the
quality of features.
Evaluating the retrieval results from document scores and
human judgments is not trivial and has been the subject of
many investigations in the IR community. A good 
performance measure should correlate with user satisfaction, 
taking into account that users will dislike having to delve deep
in the results to find relevant documents. For this reason,
standard correlation measures (such as the correlation 
coefficient between the score and the judgment of a document),
or order correlation measures (such as Kendall tau between
the score and judgment induced orders) are not adequate.
4. MEASURING PERFORMANCE
In this study, we quantify the effectiveness of various 
ranking algorithms using three measures: NDCG, MRR, and
MAP.
The normalized discounted cumulative gains (NDCG) 
measure [13] discounts the contribution of a document to the
overall score as the document"s rank increases (assuming
that the best document has the lowest rank). Such a 
measure is particularly appropriate for search engines, as studies
have shown that search engine users rarely consider anything
beyond the first few results [12]. NDCG values are 
normalized to be between 0 and 1, with 1 being the NDCG of a
perfect ranking scheme that completely agrees with the
assessment of the human judges. The discounted 
cumulative gain at a particular rank-threshold T (DCG@T) is 
defined to be
PT
j=1
1
log(1+j)

2r(j)
− 1

, where r(j) is the 
rating (0=detrimental, 1=bad, 2=fair, 3=good, 4=excellent,
and 5=definitive) at rank j. The NDCG is computed by
dividing the DCG of a ranking by the highest possible DCG
that can be obtained for that query. Finally, the NDGCs of
all queries in the query set are averaged to produce a mean
NDCG.
The reciprocal rank (RR) of the ranked result set of a
query is defined to be the reciprocal value of the rank of the
highest-ranking relevant document in the result set. The RR
at rank-threshold T is defined to be 0 if none of the 
highestranking T documents is relevant. The mean reciprocal rank
(MRR) of a query set is the average reciprocal rank of all
queries in the query set.
Given a ranked set of n results, let rel(i) be 1 if the result
at rank i is relevant and 0 otherwise. The precision P(j)
at rank j is defined to be 1
j
Pj
i=1 rel(i), i.e. the fraction
of the relevant results among the j highest-ranking results.
The average precision (AP) at rank-threshold k is defined to
be
Pk
i=1 P (i)rel(i)
Pn
i=1
rel(i)
. The mean average precision (MAP) of a
query set is the mean of the average precisions of all queries
in the query set.
The above definitions of MRR and MAP rely on the notion
of a relevant result. We investigated two definitions of 
relevance: One where all documents rated fair or better were
deemed relevant, and one were all documents rated good
or better were deemed relevant. For reasons of space, we
only report MAP and MRR values computed using the 
latter definition; using the former definition does not change
the qualitative nature of our findings. Similarly, we 
computed NDCG, MAP, and MRR values for a wide range of
rank-thresholds; we report results here at rank 10; again,
changing the rank-threshold never led us to different 
conclusions.
Recall that over 99% of documents are unlabeled. We
chose to treat all these documents as irrelevant to the query.
For some queries, however, not all relevant documents have
been judged. This introduces a bias into our evaluation:
features that bring new documents to the top of the rank
may be penalized. This will be more acute for features less
correlated to the pre-existing commercial ranking algorithms
used to select documents for judgment. On the other hand,
most queries have few perfect relevant documents (i.e. home
page or item searches) and they will most often be within
the judged set.
5. COMPUTING PAGERANK ON A LARGE
WEB GRAPH
PageRank is a query-independent measure of the 
importance of web pages, based on the notion of peer-endorsement:
A hyperlink from page A to page B is interpreted as an
endorsement of page B"s content by page A"s author. The
following recursive definition captures this notion of 
endorsement:
R(v) =
X
(u,v)∈E
R(u)
Out(u)
where R(v) is the score (importance) of page v, (u, v) is an
edge (hyperlink) from page u to page v contained in the
edge set E of the web graph, and Out(u) is the out-degree
(number of embedded hyperlinks) of page u. However, this
definition suffers from a severe shortcoming: In the 
fixedpoint of this recursive equation, only edges that are part of
a strongly-connected component receive a non-zero score. In
order to overcome this deficiency, Page et al. grant each page
a guaranteed minimum score, giving rise to the definition
of standard PageRank:
R(v) =
d
|V |
+ (1 − d)
X
(u,v)∈E
R(u)
Out(u)
where |V | is the size of the vertex set (the number of known
web pages), and d is a damping factor, typically set to be
between 0.1 and 0.2.
Assuming that scores are normalized to sum up to 1,
PageRank can be viewed as the stationary probability 
distribution of a random walk on the web graph, where at each
step of the walk, the walker with probability 1 − d moves
from its current node u to a neighboring node v, and with
probability d selects a node uniformly at random from all
nodes in the graph and jumps to it. In the limit, the random
walker is at node v with probability R(v).
One issue that has to be addressed when implementing
PageRank is how to deal with sink nodes, nodes that do
not have any outgoing links. One possibility would be to
select another node uniformly at random and transition to
it; this is equivalent to adding edges from each sink nodes
to all other nodes in the graph. We chose the alternative
approach of introducing a single phantom node. Each sink
node has an edge to the phantom node, and the phantom
node has an edge to itself.
In practice, PageRank scores can be computed using power
iteration. Since PageRank is query-independent, the 
computation can be performed off-line ahead of query time. This
property has been key to PageRank"s success, since it is a
challenging engineering problem to build a system that can
perform any non-trivial computation on the web graph at
query time.
In order to compute PageRank scores for all 2.9 billion
nodes in our web graph, we implemented a distributed 
version of PageRank. The computation consists of two distinct
phases. In the first phase, the link files produced by the web
crawler, which contain page URLs and their associated link
URLs in textual form, are partitioned among the machines
in the cluster used to compute PageRank scores, and 
converted into a more compact format along the way. 
Specifically, URLs are partitioned across the machines in the 
cluster based on a hash of the URLs" host component, and each
machine in the cluster maintains a table mapping the URL
to a 32-bit integer. The integers are drawn from a densely
packed space, so as to make suitable indices into the array
that will later hold the PageRank scores. The system then
translates our log of pages and their associated hyperlinks
into a compact representation where both page URLs and
link URLs are represented by their associated 32-bit 
integers. Hashing the host component of the URLs guarantees
that all URLs from the same host are assigned to the same
machine in our scoring cluster. Since over 80% of all 
hyperlinks on the web are relative (that is, are between two pages
on the same host), this property greatly reduces the amount
of network communication required by the second stage of
the distributed scoring computation.
The second phase performs the actual PageRank power
iteration. Both the link data and the current PageRank
vector reside on disk and are read in a streaming fashion;
while the new PageRank vector is maintained in memory.
We represent PageRank scores as 64-bit floating point 
numbers. PageRank contributions to pages assigned to remote
machines are streamed to the remote machine via a TCP
connection.
We used a three-machine cluster, each machine equipped
with 16 GB of RAM, to compute standard PageRank scores
for all 2.9 billion URLs that were contained in our web
graph. We used a damping factor of 0.15, and performed 200
power iterations. Starting at iteration 165, the L∞ norm of
the change in the PageRank vector from one iteration to the
next had stopped decreasing, indicating that we had reached
as much of a fixed point as the limitations of 64-bit floating
point arithmetic would allow.
0.07
0.08
0.09
0.10
0.11
1 10 100
Number of back-links sampled per result
NDCG@10
hits-aut-all
hits-aut-ih
hits-aut-id
0.01
0.02
0.03
0.04
1 10 100
Number of back-links sampled per result
MAP@10
hits-aut-all
hits-aut-ih
hits-aut-id
0.07
0.08
0.09
0.10
0.11
0.12
0.13
0.14
1 10 100
Number of back-links sampled per result
MRR@10
hits-aut-all
hits-aut-ih
hits-aut-id
Figure 1: Effectiveness of authority scores computed using different parameterizations of HITS.
A post-processing phase uses the final PageRank vectors
(one per machine) and the table mapping URLs to 32-bit
integers (representing indices into each PageRank vector) to
score the result URL in our query log. As mentioned above,
our web graph covered 9,525,566 of the 66,846,214 result
URLs. These URLs were annotated with their computed
PageRank score; all other URLs received a score of 0.
6. HITS
HITS, unlike PageRank, is a query-dependent ranking 
algorithm. HITS (which stands for Hypertext Induced Topic
Search) is based on the following two intuitions: First, 
hyperlinks can be viewed as topical endorsements: A hyperlink
from a page u devoted to topic T to another page v is likely
to endorse the authority of v with respect to topic T. Second,
the result set of a particular query is likely to have a certain
amount of topical coherence. Therefore, it makes sense to
perform link analysis not on the entire web graph, but rather
on just the neighborhood of pages contained in the result
set, since this neighborhood is more likely to contain 
topically relevant links. But while the set of nodes immediately
reachable from the result set is manageable (given that most
pages have only a limited number of hyperlinks embedded
into them), the set of pages immediately leading to the result
set can be enormous. For this reason, Kleinberg suggests
sampling a fixed-size random subset of the pages linking to
any high-indegree page in the result set. Moreover, 
Kleinberg suggests considering only links that cross host 
boundaries, the rationale being that links between pages on the
same host (intrinsic links) are likely to be navigational or
nepotistic and not topically relevant.
Given a web graph (V, E) with vertex set V and edge
set E ⊆ V × V , and the set of result URLs to a query
(called the root set R ⊆ V ) as input, HITS computes a
neighborhood graph consisting of a base set B ⊆ V (the
root set and some of its neighboring vertices) and some of
the edges in E induced by B. In order to formalize the
definition of the neighborhood graph, it is helpful to first
introduce a sampling operator and the concept of a 
linkselection predicate.
Given a set A, the notation Sn[A] draws n elements 
uniformly at random from A; Sn[A] = A if |A| ≤ n.
A link section predicate P takes an edge (u, v) ∈ E. In
this study, we use the following three link section predicates:
all(u, v) ⇔ true
ih(u, v) ⇔ host(u) = host(v)
id(u, v) ⇔ domain(u) = domain(v)
where host(u) denotes the host of URL u, and domain(u)
denotes the domain of URL u. So, all is true for all links,
whereas ih is true only for inter-host links, and id is true
only for inter-domain links.
The outlinked-set OP
of the root set R w.r.t. a 
linkselection predicate P is defined to be:
OP
=
[
u∈R
{v ∈ V : (u, v) ∈ E ∧ P(u, v)}
The inlinking-set IP
s of the root set R w.r.t. a link-selection
predicate P and a sampling value s is defined to be:
IP
s =
[
v∈R
Ss[{u ∈ V : (u, v) ∈ E ∧ P(u, v)}]
The base set BP
s of the root set R w.r.t. P and s is defined
to be:
BP
s = R ∪ IP
s ∪ OP
The neighborhood graph (BP
s , NP
s ) has the base set BP
s as
its vertex set and an edge set NP
s containing those edges in
E that are covered by BP
s and permitted by P:
NP
s = {(u, v) ∈ E : u ∈ BP
s ∧ v ∈ BP
s ∧ P(u, v)}
To simplify notation, we write B to denote BP
s , and N to
denote NP
s .
For each node u in the neighborhood graph, HITS 
computes two scores: an authority score A(u), estimating how
authoritative u is on the topic induced by the query, and a
hub score H(u), indicating whether u is a good reference to
many authoritative pages. This is done using the following
algorithm:
1. For all u ∈ B do H(u) :=
q
1
|B|
, A(u) :=
q
1
|B|
.
2. Repeat until H and A converge:
(a) For all v ∈ B : A (v) :=
P
(u,v)∈N H(u)
(b) For all u ∈ B : H (u) :=
P
(u,v)∈N A(v)
(c) H := H 2, A := A 2
where X 2 normalizes the vector X to unit length in 
euclidean space, i.e. the squares of its elements sum up to 1.
In practice, implementing a system that can compute HITS
within the time constraints of a major search engine (where
the peak query load is in the thousands of queries per second,
and the desired query response time is well below one 
second) is a major engineering challenge. Among other things,
the web graph cannot reasonably be stored on disk, since
.221
.106
.105
.104
.102
.095
.092
.090
.038
.036
.035
.034
.032
.032
.011
0.00
0.05
0.10
0.15
0.20
0.25
bm25f
degree-in-id
degree-in-ih
hits-aut-id-25
hits-aut-ih-100
degree-in-all
pagerank
hits-aut-all-100
hits-hub-all-100
hits-hub-ih-100
hits-hub-id-100
degree-out-all
degree-out-ih
degree-out-id
random
NDCG@10
.100
.035
.033
.033
.033
.029
.027
.027
.008
.007
.007
.006
.006
.006
.002
0.00
0.02
0.04
0.06
0.08
0.10
0.12
bm25f
hits-aut-id-9
degree-in-id
hits-aut-ih-15
degree-in-ih
degree-in-all
pagerank
hits-aut-all-100
hits-hub-all-100
hits-hub-ih-100
hits-hub-id-100
degree-out-all
degree-out-ih
degree-out-id
random
MAP@10
.273
.132
.126
.117
.114
.101
.101
.097
.032
.032
.030
.028
.027
.027
.007
0.00
0.05
0.10
0.15
0.20
0.25
0.30
bm25f
hits-aut-id-9
hits-aut-ih-15
degree-in-id
degree-in-ih
degree-in-all
hits-aut-all-100
pagerank
hits-hub-all-100
hits-hub-ih-100
hits-hub-id-100
degree-out-all
degree-out-ih
degree-out-id
random
MRR@10
Figure 2: Effectiveness of different features.
seek times of modern hard disks are too slow to retrieve the
links within the time constraints, and the graph does not fit
into the main memory of a single machine, even when using
the most aggressive compression techniques.
In order to experiment with HITS and other 
query-dependent link-based ranking algorithms that require non-regular
accesses to arbitrary nodes and edges in the web graph, we
implemented a system called the Scalable Hyperlink Store,
or SHS for short. SHS is a special-purpose database, 
distributed over an arbitrary number of machines that keeps a
highly compressed version of the web graph in memory and
allows very fast lookup of nodes and edges. On our 
hardware, it takes an average of 2 microseconds to map a URL
to a 64-bit integer handle called a UID, 15 microseconds to
look up all incoming or outgoing link UIDs associated with
a page UID, and 5 microseconds to map a UID back to a
URL (the last functionality not being required by HITS).
The RPC overhead is about 100 microseconds, but the SHS
API allows many lookups to be batched into a single RPC
request.
We implemented the HITS algorithm using the SHS 
infrastructure. We compiled three SHS databases, one 
containing all 17.6 billion links in our web graph (all), one 
containing only links between pages that are on different hosts
(ih, for inter-host), and one containing only links between
pages that are on different domains (id). We consider two
URLs to belong to different hosts if the host portions of the
URLs differ (in other words, we make no attempt to 
determine whether two distinct symbolic host names refer to
the same computer), and we consider a domain to be the
name purchased from a registrar (for example, we consider
news.bbc.co.uk and www.bbc.co.uk to be different hosts 
belonging to the same domain). Using each of these databases,
we computed HITS authority and hub scores for various 
parameterizations of the sampling operator S, sampling 
between 1 and 100 back-links of each page in the root set.
Result URLs that were not covered by our web graph 
automatically received authority and hub scores of 0, since they
were not connected to any other nodes in the neighborhood
graph and therefore did not receive any endorsements.
We performed forty-five different HITS computations, each
combining one of the three link selection predicates (all, ih,
and id) with a sampling value. For each combination, we
loaded one of the three databases into an SHS system 
running on six machines (each equipped with 16 GB of RAM),
and computed HITS authority and hub scores, one query
at a time. The longest-running combination (using the all
database and sampling 100 back-links of each root set 
vertex) required 30,456 seconds to process the entire query set,
or about 1.1 seconds per query on average.
7. EXPERIMENTAL RESULTS
For a given query Q, we need to rank the set of documents
satisfying Q (the result set of Q). Our hypothesis is that
good features should be able to rank relevant documents in
this set higher than non-relevant ones, and this should result
in an increase in each performance measure over the query
set. We are specifically interested in evaluating the 
usefulness of HITS and other link-based features. In principle, we
could do this by sorting the documents in each result set by
their feature value, and compare the resulting NDCGs. We
call this ranking with isolated features.
Let us first examine the relative performance of the 
different parameterizations of the HITS algorithm we 
examined. Recall that we computed HITS for each combination
of three link section schemes - all links (all), inter-host links
only (ih), and inter-domain links only (id) - with back-link
sampling values ranging from 1 to 100. Figure 1 shows the
impact of the number of sampled back-links on the retrieval
performance of HITS authority scores. Each graph is 
associated with one performance measure. The horizontal axis
of each graph represents the number of sampled back-links,
the vertical axis represents performance under the 
appropriate measure, and each curve depicts a link selection scheme.
The id scheme slightly outperforms ih, and both vastly 
outperform the all scheme - eliminating nepotistic links pays
off. The performance of the all scheme increases as more
back-links of each root set vertex are sampled, while the
performance of the id and ih schemes peaks at between 10
and 25 samples and then plateaus or even declines, 
depending on the performance measure.
Having compared different parameterizations of HITS, we
will now fix the number of sampled back-links at 100 and
compare the three link selection schemes against other 
isolated features: PageRank, in-degree and out-degree 
counting links of all pages, of different hosts only and of different
domains only (all, ih and id datasets respectively), and a
text retrieval algorithm exploiting anchor text: BM25F[24].
BM25F is a state-of-the art ranking function solely based on
textual content of the documents and their associated 
anchor texts. BM25F is a descendant of BM25 that combines
the different textual fields of a document, namely title, body
and anchor text. This model has been shown to be one of
the best-performing web search scoring functions over the
last few years [8, 24]. BM25F has a number of free 
parameters (2 per field, 6 in our case); we used the parameter values
described in [24].
.341
.340
.339
.337
.336
.336
.334
.311
.311
.310
.310
.310
.310
.231
0.22
0.24
0.26
0.28
0.30
0.32
0.34
0.36
degree-in-id
degree-in-ih
degree-in-all
hits-aut-ih-100
hits-aut-all-100
pagerank
hits-aut-id-10
degree-out-all
hits-hub-all-100
degree-out-ih
hits-hub-ih-100
degree-out-id
hits-hub-id-10
bm25f
NDCG@10
.152
.152
.151
.150
.150
.149
.149
.137
.136
.136
.128
.127
.127
.100
0.09
0.10
0.11
0.12
0.13
0.14
0.15
0.16
degree-in-ih
degree-in-id
degree-in-all
hits-aut-ih-100
hits-aut-all-100
hits-aut-id-10
pagerank
hits-hub-all-100
degree-out-ih
hits-hub-id-100
degree-out-all
degree-out-id
hits-hub-ih-100
bm25f
MAP@10
.398
.397
.394
.394
.392
.392
.391
.356
.356
.356
.356
.356
.355
.273
0.25
0.30
0.35
0.40
degree-in-id
degree-in-ih
degree-in-all
hits-aut-ih-100
hits-aut-all-100
pagerank
hits-aut-id-10
degree-out-all
hits-hub-all-100
degree-out-ih
hits-hub-ih-100
degree-out-id
hits-hub-id-10
bm25f
MRR@10
Figure 3: Effectiveness measures for linear combinations of link-based features with BM25F.
Figure 2 shows the NDCG, MRR, and MAP measures
of these features. Again all performance measures (and
for all rank-thresholds we explored) agree. As expected,
BM25F outperforms all link-based features by a large 
margin. The link-based features are divided into two groups,
with a noticeable performance drop between the groups.
The better-performing group consists of the features that
are based on the number and/or quality of incoming links
(in-degree, PageRank, and HITS authority scores); and the
worse-performing group consists of the features that are
based on the number and/or quality of outgoing links 
(outdegree and HITS hub scores). In the group of features based
on incoming links, features that ignore nepotistic links 
perform better than their counterparts using all links. 
Moreover, using only inter-domain (id) links seems to be marginally
better than using inter-host (ih) links.
The fact that features based on outgoing links 
underperform those based on incoming links matches our 
expectations; if anything, it is mildly surprising that outgoing links
provide a useful signal for ranking at all. On the other
hand, the fact that in-degree features outperform PageRank
under all measures is quite surprising. A possible 
explanation is that link-spammers have been targeting the published
PageRank algorithm for many years, and that this has led
to anomalies in the web graph that affect PageRank, but
not other link-based features that explore only a distance-1
neighborhood of the result set. Likewise, it is surprising that
simple query-independent features such as in-degree, which
might estimate global quality but cannot capture relevance
to a query, would outperform query-dependent features such
as HITS authority scores.
However, we cannot investigate the effect of these features
in isolation, without regard to the overall ranking function,
for several reasons. First, features based on the textual 
content of documents (as opposed to link-based features) are
the best predictors of relevance. Second, link-based features
can be strongly correlated with textual features for several
reasons, mainly the correlation between in-degree and 
numFeature Transform function
bm25f T(s) = s
pagerank T(s) = log(s + 3 · 10−12
)
degree-in-* T(s) = log(s + 3 · 10−2
)
degree-out-* T(s) = log(s + 3 · 103
)
hits-aut-* T(s) = log(s + 3 · 10−8
)
hits-hub-* T(s) = log(s + 3 · 10−1
)
Table 1: Near-optimal feature transform functions.
ber of textual anchor matches.
Therefore, one must consider the effect of link-based 
features in combination with textual features. Otherwise, we
may find a link-based feature that is very good in isolation
but is strongly correlated with textual features and results
in no overall improvement; and vice versa, we may find a
link-based feature that is weak in isolation but significantly
improves overall performance.
For this reason, we have studied the combination of the
link-based features above with BM25F. All feature 
combinations were done by considering the linear combination of two
features as a document score, using the formula score(d) =Pn
i=1 wiTi(Fi(d)), where d is a document (or 
documentquery pair, in the case of BM25F), Fi(d) (for 1 ≤ i ≤ n) is a
feature extracted from d, Ti is a transform, and wi is a free
scalar weight that needs to be tuned. We chose transform
functions that we empirically determined to be well-suited.
Table 1 shows the chosen transform functions.
This type of linear combination is appropriate if we 
assume features to be independent with respect to relevance
and an exponential model for link features, as discussed
in [8]. We tuned the weights by selecting a random 
subset of 5,000 queries from the query set, used an iterative
refinement process to find weights that maximized a given
performance measure on that training set, and used the 
remaining 23,043 queries to measure the performance of the
thus derived scoring functions.
We explored the pairwise combination of BM25F with 
every link-based scoring function. Figure 3 shows the NDCG,
MRR, and MAP measures of these feature combinations,
together with a baseline BM25F score (the right-most bar
in each graph), which was computed using the same subset
of 23,045 queries that were used as the test set for the 
feature combinations. Regardless of the performance measure
applied, we can make the following general observations:
1. Combining any of the link-based features with BM25F
results in a substantial performance improvement over
BM25F in isolation.
2. The combination of BM25F with features based on 
incoming links (PageRank, in-degree, and HITS 
authority scores) performs substantially better than the 
combination with features based on outgoing links (HITS
hub scores and out-degree).
3. The performance differences between the various 
combinations of BM25F with features based on incoming
links is comparatively small, and the relative ordering
of feature combinations is fairly stable across the 
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0 2 4 6 8 10 12 14 16 18 20 22 24
MAP@10
26 374 1640 2751 3768 4284 3944 3001 2617 1871 1367 771 1629
bm25fnorm pagerank degree-in-id hits-aut-id-100
Figure 4: Effectiveness measures for selected 
isolated features, broken down by query specificity.
ferent performance measures used. However, the 
combination of BM25F with any in-degree variant, and in
particular with id in-degree, consistently outperforms
the combination of BM25F with PageRank or HITS
authority scores, and can be computed much easier
and faster.
Finally, we investigated whether certain features are 
better for some queries than for others. Particularly, we are 
interested in the relationship between the specificity of a query
and the performance of different ranking features. The most
straightforward measure of the specificity of a query Q would
be the number of documents in a search engine"s corpus that
satisfy Q. Unfortunately, the query set available to us did
not contain this information. Therefore, we chose to 
approximate the specificity of Q by summing up the inverse
document frequencies of the individual query terms 
comprising Q. The inverse document frequency (IDF) of a term
t with respect to a corpus C is defined to be logN/doc(t),
where doc(t) is the number of documents in C containing t
and N is the total number of documents in C. By summing
up the IDFs of the query terms, we make the (flawed) 
assumption that the individual query terms are independent of
each other. However, while not perfect, this approximation
is at least directionally correct.
We broke down our query set into 13 buckets, each bucket
associated with an interval of query IDF values, and we 
computed performance metrics for all ranking functions applied
(in isolation) to the queries in each bucket. In order to
keep the graphs readable, we will not show the performance
of all the features, but rather restrict ourselves to the four
most interesting ones: PageRank, id HITS authority scores,
id in-degree, and BM25F. Figure 4 shows the MAP@10 for
all 13 query specificity buckets. Buckets on the far left of
each graph represent very general queries; buckets on the far
right represent very specific queries. The figures on the 
upper x axis of each graph show the number of queries in each
bucket (e.g. the right-most bucket contains 1,629 queries).
BM25F performs best for medium-specific queries, peaking
at the buckets representing the IDF sum interval [12,14).
By comparison, HITS peaks at the bucket representing the
IDF sum interval [4,6), and PageRank and in-degree peak at
the bucket representing the interval [6,8), i.e. more general
queries.
8. CONCLUSIONS AND FUTURE WORK
This paper describes a large-scale evaluation of the 
effectiveness of HITS in comparison with other link-based
ranking algorithms, in particular PageRank and in-degree,
when applied in isolation or in combination with a text 
retrieval algorithm exploiting anchor text (BM25F). 
Evaluation is carried out with respect to a large number of human
evaluated queries, using three different measures of 
effectiveness: NDCG, MRR, and MAP. Evaluating link-based
features in isolation, we found that web page in-degree 
outperforms PageRank, and is about as effwective as HITS 
authority scores. HITS hub scores and web page out-degree are
much less effective ranking features, but still outperform a
random ordering. A linear combination of any link-based
features with BM25F produces a significant improvement in
performance, and there is a clear difference between 
combining BM25F with a feature based on incoming links 
(indegree, PageRank, or HITS authority scores) and a feature
based on outgoing links (HITS hub scores and out-degree),
but within those two groups the precise choice of link-based
feature matters relatively little.
We believe that the measurements presented in this paper
provide a solid evaluation of the best well-known link-based
ranking schemes. There are many possible variants of these
schemes, and many other link-based ranking algorithms have
been proposed in the literature, hence we do not claim this
work to be the last word on this subject, but rather the
first step on a long road. Future work includes evaluation
of different parameterizations of PageRank and HITS. In
particular, we would like to study the impact of changes
to the PageRank damping factor on effectiveness, the 
impact of various schemes meant to counteract the effects of
link spam, and the effect of weighing hyperlinks differently
depending on whether they are nepotistic or not. Going
beyond PageRank and HITS, we would like to measure the
effectiveness of other link-based ranking algorithms, such as
SALSA. Finally, we are planning to experiment with more
complex feature combinations.
9. REFERENCES
[1] B. Amento, L. Terveen, and W. Hill. Does authority
mean quality? Predicting expert quality ratings of
web documents. In Proc. of the 23rd Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
296-303, 2000.
[2] M. Bianchini, M. Gori, and F. Scarselli. Inside
PageRank. ACM Transactions on Internet Technology,
5(1):92-128, 2005.
[3] A. Borodin, G. O. Roberts, and J. S. Rosenthal.
Finding authorities and hubs from link structures on
the World Wide Web. In Proc. of the 10th
International World Wide Web Conference, pages
415-429, 2001.
[4] A. Borodin, G. O. Roberts, J. S. Rosenthal, and
P. Tsaparas. Link analysis ranking: algorithms,
theory, and experiments. ACM Transactions on
Interet Technology, 5(1):231-297, 2005.
[5] S. Brin and L. Page. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1-7):107-117, 1998.
[6] C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. Learning
to rank using gradient descent. In Proc. of the 22nd
International Conference on Machine Learning, pages
89-96, New York, NY, USA, 2005. ACM Press.
[7] D. Cohn and H. Chang. Learning to probabilistically
identify authoritative documents. In Proc. of the 17th
International Conference on Machine Learning, pages
167-174, 2000.
[8] N. Craswell, S. Robertson, H. Zaragoza, and
M. Taylor. Relevance weighting for query independent
evidence. In Proc. of the 28th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 416-423,
2005.
[9] E. Garfield. Citation analysis as a tool in journal
evaluation. Science, 178(4060):471-479, 1972.
[10] Z. Gy¨ongyi and H. Garcia-Molina. Web spam
taxonomy. In 1st International Workshop on
Adversarial Information Retrieval on the Web, 2005.
[11] Z. Gy¨ongyi, H. Garcia-Molina, and J. Pedersen.
Combating web spam with TrustRank. In Proc. of the
30th International Conference on Very Large
Databases, pages 576-587, 2004.
[12] B. J. Jansen, A. Spink, J. Bateman, and T. Saracevic.
Real life information retrieval: a study of user queries
on the web. ACM SIGIR Forum, 32(1):5-17, 1998.
[13] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based
evaluation of IR techniques. ACM Transactions on
Information Systems, 20(4):422-446, 2002.
[14] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and
G. H. Golub. Extrapolation methods for accelerating
PageRank computations. In Proc. of the 12th
International World Wide Web Conference, pages
261-270, 2003.
[15] M. M. Kessler. Bibliographic coupling between
scientific papers. American Documentation,
14(1):10-25, 1963.
[16] J. M. Kleinberg. Authoritative sources in a
hyperlinked environment. In Proc. of the 9th Annual
ACM-SIAM Symposium on Discrete Algorithms, pages
668-677, 1998.
[17] J. M. Kleinberg. Authoritative sources in a
hyperlinked environment. Journal of the ACM,
46(5):604-632, 1999.
[18] A. N. Langville and C. D. Meyer. Deeper inside
PageRank. Internet Mathematics, 1(3):2005, 335-380.
[19] R. Lempel and S. Moran. The stochastic approach for
link-structure analysis (SALSA) and the TKC effect.
Computer Networks and ISDN Systems,
33(1-6):387-401, 2000.
[20] A. Y. Ng, A. X. Zheng, and M. I. Jordan. Stable
algorithms for link analysis. In Proc. of the 24th
Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 258-266, 2001.
[21] L. Page, S. Brin, R. Motwani, and T. Winograd. The
PageRank citation ranking: Bringing order to the
web. Technical report, Stanford Digital Library
Technologies Project, 1998.
[22] J. A. Tomlin. A new paradigm for ranking pages on
the World Wide Web. In Proc. of the 12th
International World Wide Web Conference, pages
350-355, 2003.
[23] T. Upstill, N. Craswell, and D. Hawking. Predicting
fame and fortune: Pagerank or indegree? In Proc. of
the Australasian Document Computing Symposium,
pages 31-40, 2003.
[24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and
S. Robertson. Microsoft Cambridge at TREC-13:
Web and HARD tracks. In Proc. of the 13th Text
Retrieval Conference, 2004.
Estimating the Global PageRank of Web Communities
Jason V. Davis
Dept. of Computer Sciences
University of Texas at Austin
Austin, TX 78712
jdavis@cs.utexas.edu
Inderjit S. Dhillon
Dept. of Computer Sciences
University of Texas at Austin
Austin, TX 78712
inderjit@cs.utexas.edu
ABSTRACT
Localized search engines are small-scale systems that index
a particular community on the web. They offer several 
benefits over their large-scale counterparts in that they are 
relatively inexpensive to build, and can provide more precise
and complete search capability over their relevant domains.
One disadvantage such systems have over large-scale search
engines is the lack of global PageRank values. Such 
information is needed to assess the value of pages in the localized
search domain within the context of the web as a whole.
In this paper, we present well-motivated algorithms to 
estimate the global PageRank values of a local domain. The
algorithms are all highly scalable in that, given a local 
domain of size n, they use O(n) resources that include 
computation time, bandwidth, and storage. We test our methods
across a variety of localized domains, including site-specific
domains and topic-specific domains. We demonstrate that
by crawling as few as n or 2n additional pages, our methods
can give excellent global PageRank estimates.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: 
Information Search and Retrieval; G.1.3 [Numerical Analysis]:
Numerical Linear Algebra; G.3 [Probability and 
Statistics]: Markov Processes
General Terms
PageRank, Markov Chain, Stochastic Complementation
1. INTRODUCTION
Localized search engines are small-scale search engines
that index only a single community of the web. Such 
communities can be site-specific domains, such as pages within
the cs.utexas.edu domain, or topic-related 
communitiesfor example, political websites. Compared to the web graph
crawled and indexed by large-scale search engines, the size
of such local communities is typically orders of magnitude
smaller. Consequently, the computational resources needed
to build such a search engine are also similarly lighter. By
restricting themselves to smaller, more manageable sections
of the web, localized search engines can also provide more
precise and complete search capabilities over their respective
domains.
One drawback of localized indexes is the lack of global
information needed to compute link-based rankings. The
PageRank algorithm [3], has proven to be an effective such
measure. In general, the PageRank of a given page is 
dependent on pages throughout the entire web graph. In the
context of a localized search engine, if the PageRanks are
computed using only the local subgraph, then we would 
expect the resulting PageRanks to reflect the perceived 
popularity within the local community and not of the web as a
whole. For example, consider a localized search engine that
indexes political pages with conservative views. A person
wishing to research the opinions on global warming within
the conservative political community may encounter 
numerous such opinions across various websites. If only local 
PageRank values are available, then the search results will reflect
only strongly held beliefs within the community. However, if
global PageRanks are also available, then the results can 
additionally reflect outsiders" views of the conservative 
community (those documents that liberals most often access within
the conservative community).
Thus, for many localized search engines, incorporating
global PageRanks can improve the quality of search results.
However, the number of pages a local search engine indexes
is typically orders of magnitude smaller than the number of
pages indexed by their large-scale counterparts. Localized
search engines do not have the bandwidth, storage capacity,
or computational power to crawl, download, and compute
the global PageRanks of the entire web. In this work, we
present a method of approximating the global PageRanks of
a local domain while only using resources of the same 
order as those needed to compute the PageRanks of the local
subgraph.
Our proposed method looks for a supergraph of our local
subgraph such that the local PageRanks within this 
supergraph are close to the true global PageRanks. We construct
this supergraph by iteratively crawling global pages on the
current web frontier-i.e., global pages with inlinks from
pages that have already been crawled. In order to provide
116
Research Track Paper
a good approximation to the global PageRanks, care must
be taken when choosing which pages to crawl next; in this
paper, we present a well-motivated page selection algorithm
that also performs well empirically. This algorithm is 
derived from a well-defined problem objective and has a 
running time linear in the number of local nodes.
We experiment across several types of local subgraphs,
including four topic related communities and several 
sitespecific domains. To evaluate performance, we measure the
difference between the current global PageRank estimate
and the global PageRank, as a function of the number of
pages crawled. We compare our algorithm against several
heuristics and also against a baseline algorithm that chooses
pages at random, and we show that our method outperforms
these other methods. Finally, we empirically demonstrate
that, given a local domain of size n, we can provide good
approximations to the global PageRank values by crawling
at most n or 2n additional pages.
The paper is organized as follows. Section 2 gives an
overview of localized search engines and outlines their 
advantages over global search. Section 3 provides background
on the PageRank algorithm. Section 4 formally defines our
problem, and section 5 presents our page selection criteria
and derives our algorithms. Section 6 provides 
experimental results, section 7 gives an overview of related work, and,
finally, conclusions are given in section 8.
2. LOCALIZED SEARCH ENGINES
Localized search engines index a single community of the
web, typically either a site-specific community, or a 
topicspecific community. Localized search engines enjoy three
major advantages over their large-scale counterparts: they
are relatively inexpensive to build, they can offer more 
precise search capability over their local domain, and they can
provide a more complete index.
The resources needed to build a global search engine are
enormous. A 2003 study by Lyman et al. [13] found that
the ‘surface web" (publicly available static sites) consists of
8.9 billion pages, and that the average size of these pages is
approximately 18.7 kilobytes. To download a crawl of this
size, approximately 167 terabytes of space is needed. For a
researcher who wishes to build a search engine with access
to a couple of workstations or a small server, storage of this
magnitude is simply not available. However, building a 
localized search engine over a web community of a hundred
thousand pages would only require a few gigabytes of 
storage. The computational burden required to support search
queries over a database this size is more manageable as well.
We note that, for topic-specific search engines, the relevant
community can be efficiently identified and downloaded by
using a focused crawler [21, 4].
For site-specific domains, the local domain is readily 
available on their own web server. This obviates the need for
crawling or spidering, and a complete and up-to-date 
index of the domain can thus be guaranteed. This is in 
contrast to their large-scale counterparts, which suffer from 
several shortcomings. First, crawling dynamically generated
pages-pages in the ‘hidden web"-has been the subject of
research [20] and is a non-trivial task for an external crawler.
Second, site-specific domains can enable the robots 
exclusion policy. This prohibits external search engines" crawlers
from downloading content from the domain, and an external
search engine must instead rely on outside links and anchor
text to index these restricted pages.
By restricting itself to only a specific domain of the 
internet, a localized search engine can provide more precise
search results. Consider the canonical ambiguous search
query, ‘jaguar", which can refer to either the car 
manufacturer or the animal. A scientist trying to research the 
habitat and evolutionary history of a jaguar may have better
success using a finely tuned zoology-specific search engine
than querying Google with multiple keyword searches and
wading through irrelevant results. A method to learn 
better ranking functions for retrieval was recently proposed by
Radlinski and Joachims [19] and has been applied to various
local domains, including Cornell University"s website [8].
3. PAGERANK OVERVIEW
The PageRank algorithm defines the importance of web
pages by analyzing the underlying hyperlink structure of a
web graph. The algorithm works by building a Markov chain
from the link structure of the web graph and computing its
stationary distribution. One way to compute the 
stationary distribution of a Markov chain is to find the limiting
distribution of a random walk over the chain. Thus, the
PageRank algorithm uses what is sometimes referred to as
the ‘random surfer" model. In each step of the random walk,
the ‘surfer" either follows an outlink from the current page
(i.e. the current node in the chain), or jumps to a random
page on the web.
We now precisely define the PageRank problem. Let U
be an m × m adjacency matrix for a given web graph such
that Uji = 1 if page i links to page j and Uji = 0 otherwise.
We define the PageRank matrix PU to be:
PU = αUD−1
U + (1 − α)veT
, (1)
where DU is the (unique) diagonal matrix such that UD−1
U
is column stochastic, α is a given scalar such that 0 ≤ α ≤ 1,
e is the vector of all ones, and v is a non-negative, 
L1normalized vector, sometimes called the ‘random surfer" 
vector. Note that the matrix D−1
U is well-defined only if each
column of U has at least one non-zero entry-i.e., each page
in the webgraph has at least one outlink. In the presence of
such ‘dangling nodes" that have no outlinks, one commonly
used solution, proposed by Brin et al. [3], is to replace each
zero column of U by a non-negative, L1-normalized vector.
The PageRank vector r is the dominant eigenvector of the
PageRank matrix, r = PU r. We will assume, without loss of
generality, that r has an L1-norm of one. Computationally,
r can be computed using the power method. This method
first chooses a random starting vector r(0)
, and iteratively
multiplies the current vector by the PageRank matrix PU ;
see Algorithm 1. In general, each iteration of the power
method can take O(m2
) operations when PU is a dense 
matrix. However, in practice, the number of links in a web
graph will be of the order of the number of pages. By 
exploiting the sparsity of the PageRank matrix, the work per
iteration can be reduced to O(km), where k is the average
number of links per web page. It has also been shown that
the total number of iterations needed for convergence is 
proportional to α and does not depend on the size of the web
graph [11, 7]. Finally, the total space needed is also O(km),
mainly to store the matrix U.
117
Research Track Paper
Algorithm 1: A linear time (per iteration) algorithm for
computing PageRank.
ComputePR(U)
Input: U: Adjacency matrix.
Output: r: PageRank vector.
Choose (randomly) an initial non-negative vector r(0)
such that r(0)
1 = 1.
i ← 0
repeat
i ← i + 1
ν ← αUD−1
U r(i−1)
{α is the random surfing 
probability}
r(i)
← ν + (1 − α)v {v is the random surfer vector.}
until r(i)
− r(i−1)
< δ {δ is the convergence threshold.}
r ← r(i)
4. PROBLEM DEFINITION
Given a local domain L, let G be an N × N adjacency
matrix for the entire connected component of the web that
contains L, such that Gji = 1 if page i links to page j
and Gji = 0 otherwise. Without loss of generality, we will
partition G as:
G =
L Gout
Lout Gwithin
, (2)
where L is the n × n local subgraph corresponding to links
inside the local domain, Lout is the subgraph that 
corresponds to links from the local domain pointing out to the
global domain, Gout is the subgraph containing links from
the global domain into the local domain, and Gwithin 
contains links within the global domain. We assume that when
building a localized search engine, only pages inside the 
local domain are crawled, and the links between these pages
are represented by the subgraph L. The links in Lout are
also known, as these point from crawled pages in the local
domain to uncrawled pages in the global domain.
As defined in equation (1), PG is the PageRank matrix
formed from the global graph G, and we define the global
PageRank vector of this graph to be g. Let the n-length
vector p∗
be the L1-normalized vector corresponding to the
global PageRank of the pages in the local domain L:
p∗
=
EL g
ELg 1
,
where EL = [ I | 0 ] is the restriction matrix that selects
the components from g corresponding to nodes in L. Let p
denote the PageRank vector constructed from the local 
domain subgraph L. In practice, the observed local PageRank
p and the global PageRank p∗
will be quite different. One
would expect that as the size of local matrix L approaches
the size of global matrix G, the global PageRank and the 
observed local PageRank will become more similar. Thus, one
approach to estimating the global PageRank is to crawl the
entire global domain, compute its PageRank, and extract
the PageRanks of the local domain.
Typically, however, n N , i.e., the number of global
pages is much larger than the number of local pages. 
Therefore, crawling all global pages will quickly exhaust all local
resources (computational, storage, and bandwidth) available
to create the local search engine. We instead seek a 
supergraph ˆF of our local subgraph L with size O(n). Our goal
Algorithm 2: The FindGlobalPR algorithm.
FindGlobalPR(L, Lout, T, k)
Input: L: zero-one adjacency matrix for the local 
domain, Lout: zero-one outlink matrix from L to global
subgraph as in (2), T: number of iterations, k: number of
pages to crawl per iteration.
Output: ˆp: an improved estimate of the global 
PageRank of L.
F ← L
Fout ← Lout
f ← ComputePR(F )
for (i = 1 to T)
{Determine which pages to crawl next}
pages ← SelectNodes(F , Fout, f, k)
Crawl pages, augment F and modify Fout
{Update PageRanks for new local domain}
f ← ComputePR(F )
end
{Extract PageRanks of original local domain & normalize}
ˆp ← ELf
ELf 1
is to find such a supergraph ˆF with PageRank ˆf, so that
ˆf when restricted to L is close to p∗
. Formally, we seek to
minimize
GlobalDiff( ˆf) =
EL
ˆf
EL
ˆf 1
− p∗
1
. (3)
We choose the L1 norm for measuring the error as it does
not place excessive weight on outliers (as the L2 norm does,
for example), and also because it is the most commonly used
distance measure in the literature for comparing PageRank
vectors, as well as for detecting convergence of the 
algorithm [3].
We propose a greedy framework, given in Algorithm 2,
for constructing ˆF . Initially, F is set to the local subgraph
L, and the PageRank f of this graph is computed. The 
algorithm then proceeds as follows. First, the SelectNodes
algorithm (which we discuss in the next section) is called
and it returns a set of k nodes to crawl next from the set
of nodes in the current crawl frontier, Fout. These selected
nodes are then crawled to expand the local subgraph, F , and
the PageRanks of this expanded graph are then recomputed.
These steps are repeated for each of T iterations. Finally,
the PageRank vector ˆp, which is restricted to pages within
the original local domain, is returned. Given our 
computation, bandwidth, and memory restrictions, we will assume
that the algorithm will crawl at most O(n) pages. Since the
PageRanks are computed in each iteration of the algorithm,
which is an O(n) operation, we will also assume that the
number of iterations T is a constant. Of course, the main
challenge here is in selecting which set of k nodes to crawl
next. In the next section, we formally define the problem
and give efficient algorithms.
5. NODE SELECTION
In this section, we present node selection algorithms that
operate within the greedy framework presented in the 
previous section. We first give a well-defined criteria for the
page selection problem and provide experimental evidence
that this criteria can effectively identify pages that optimize
our problem objective (3). We then present our main 
al118
Research Track Paper
gorithmic contribution of the paper, a method with linear
running time that is derived from this page selection 
criteria. Finally, we give an intuitive analysis of our algorithm in
terms of ‘leaks" and ‘flows". We show that if only the ‘flow"
is considered, then the resulting method is very similar to a
widely used page selection heuristic [6].
5.1 Formulation
For a given page j in the global domain, we define the
expanded local graph Fj:
Fj =
F s
uT
j 0
, (4)
where uj is the zero-one vector containing the outlinks from
F into page j, and s contains the inlinks from page j into
the local domain. Note that we do not allow self-links in
this framework. In practice, self-links are often removed, as
they only serve to inflate a given page"s PageRank.
Observe that the inlinks into F from node j are not known
until after node j is crawled. Therefore, we estimate this
inlink vector as the expectation over inlink counts among
the set of already crawled pages,
s =
F T
e
F T e 1
. (5)
In practice, for any given page, this estimate may not reflect
the true inlinks from that page. Furthermore, this 
expectation is sampled from the set of links within the crawled
domain, whereas a better estimate would also use links from
the global domain. However, the latter distribution is not
known to a localized search engine, and we contend that the
above estimate will, on average, be a better estimate than
the uniform distribution, for example.
Let the PageRank of F be f. We express the PageRank
f+
j of the expanded local graph Fj as
f+
j =
(1 − xj)fj
xj
, (6)
where xj is the PageRank of the candidate global node j,
and fj is the L1-normalized PageRank vector restricted to
the pages in F .
Since directly optimizing our problem goal requires 
knowing the global PageRank p∗
, we instead propose to crawl
those nodes that will have the greatest influence on the 
PageRanks of pages in the original local domain L:
influence(j) =
k∈L
|fj[k] − f[k]| (7)
= EL (fj − f) 1.
Experimentally, the influence score is a very good predictor
of our problem objective (3). For each candidate global node
j, figure 1(a) shows the objective function value Global Diff(fj)
as a function of the influence of page j. The local domain
used here is a crawl of conservative political pages (we will
provide more details about this dataset in section 6); we
observed similar results in other domains. The correlation
is quite strong, implying that the influence criteria can 
effectively identify pages that improve the global PageRank
estimate. As a baseline, figure 1(b) compares our 
objective with an alternative criteria, outlink count. The outlink
count is defined as the number of outlinks from the local
domain to page j. The correlation here is much weaker.
.00001 .0001 .001 .01
0.26
0.262
0.264
0.266
Influence
Objective
1 10 100 1000
0.266
0.264
0.262
0.26
Outlink Count
Objective
(a) (b)
Figure 1: (a) The correlation between our influence
page selection criteria (7) and the actual objective
function (3) value is quite strong. (b) This is in 
contrast to other criteria, such as outlink count, which
exhibit a much weaker correlation.
5.2 Computation
As described, for each candidate global page j, the 
influence score (7) must be computed. If fj is computed
exactly for each global page j, then the PageRank 
algorithm would need to be run for each of the O(n) such global
pages j we consider, resulting in an O(n2
) computational
cost for the node selection method. Thus, computing the
exact value of fj will lead to a quadratic algorithm, and we
must instead turn to methods of approximating this vector.
The algorithm we present works by performing one power
method iteration used by the PageRank algorithm 
(Algorithm 1). The convergence rate for the PageRank algorithm
has been shown to equal the random surfer probability α [7,
11]. Given a starting vector x(0)
, if k PageRank iterations
are performed, the current PageRank solution x(k)
satisfies:
x(k)
− x∗
1 = O(αk
x(0)
− x∗
1), (8)
where x∗
is the desired PageRank vector. Therefore, if only
one iteration is performed, choosing a good starting vector
is necessary to achieve an accurate approximation.
We partition the PageRank matrix PFj , corresponding to
the × subgraph Fj as:
PFj =
˜F ˜s
˜uT
j w
, (9)
where ˜F = αF (DF + diag(uj))−1
+ (1 − α)
e
+ 1
eT
,
˜s = αs + (1 − α)
e
+ 1
,
˜uj = α(DF + diag(uj))−1
uj + (1 − α)
e
+ 1
,
w =
1 − α
+ 1
,
and diag(uj) is the diagonal matrix with the (i, i)th
entry
equal to one if the ith
element of uj equals one, and is zero
otherwise. We have assumed here that the random surfer
vector is the uniform vector, and that L has no ‘dangling
links". These assumptions are not necessary and serve only
to simplify discussion and analysis.
A simple approach for estimating fj is the following. First,
estimate the PageRank f+
j of Fj by computing one 
PageRank iteration over the matrix PFj , using the starting 
vector ν =
f
0
. Then, estimate fj by removing the last
119
Research Track Paper
component from our estimate of f+
j (i.e., the component
corresponding to the added node j), and renormalizing.
The problem with this approach is in the starting vector.
Recall from (6) that xj is the PageRank of the added node
j. The difference between the actual PageRank f+
j of PFj
and the starting vector ν is
ν − f+
j 1 = xj + f − (1 − xj)fj 1
≥ xj + | f 1 − (1 − xj) fj 1|
= xj + |xj|
= 2xj.
Thus, by (8), after one PageRank iteration, we expect our
estimate of f+
j to still have an error of about 2αxj. In 
particular, for candidate nodes j with relatively high PageRank
xj, this method will yield more inaccurate results. We will
next present a method that eliminates this bias and runs in
O(n) time.
5.2.1 Stochastic Complementation
Since f+
j , as given in (6) is the PageRank of the matrix
PFj , we have:
fj(1 − xj)
xj
=
˜F ˜s
˜uT
j w
fj(1 − xj)
xj
=
˜F fj(1 − xj) + ˜sxj
˜uT
j fj(1 − xj) + wxj
.
Solving the above system for fj can be shown to yield
fj = ( ˜F + (1 − w)−1
˜s˜uT
j )fj. (10)
The matrix S = ˜F +(1−w)−1
˜s˜uT
j is known as the stochastic
complement of the column stochastic matrix PFj with 
respect to the sub matrix ˜F . The theory of stochastic 
complementation is well studied, and it can be shown the stochastic
complement of an irreducible matrix (such as the PageRank
matrix) is unique. Furthermore, the stochastic complement
is also irreducible and therefore has a unique stationary 
distribution as well. For an extensive study, see [15].
It can be easily shown that the sub-dominant eigenvalue
of S is at most +1
α, where is the size of F . For sufficiently
large , this value will be very close to α. This is important,
as other properties of the PageRank algorithm, notably the
algorithm"s sensitivity, are dependent on this value [11].
In this method, we estimate the length vector fj by
computing one PageRank iteration over the × stochastic
complement S, starting at the vector f:
fj ≈ Sf. (11)
This is in contrast to the simple method outlined in the 
previous section, which first iterates over the ( + 1) × ( + 1)
matrix PFj to estimate f+
j , and then removes the last 
component from the estimate and renormalizes to approximate
fj. The problem with the latter method is in the choice
of the ( + 1) length starting vector, ν. Consequently, the
PageRank estimate given by the simple method differs from
the true PageRank by at least 2αxj, where xj is the 
PageRank of page j. By using the stochastic complement, we
can establish a tight lower bound of zero for this difference.
To see this, consider the case in which a node k is added
to F to form the augmented local subgraph Fk, and that
the PageRank of this new graph is
(1 − xk)f
xk
. 
Specifically, the addition of page k does not change the PageRanks
of the pages in F , and thus fk = f. By construction of
the stochastic complement, fk = Sfk, so the approximation
given in equation (11) will yield the exact solution.
Next, we present the computational details needed to 
efficiently compute the quantity fj −f 1 over all known global
pages j. We begin by expanding the difference fj −f, where
the vector fj is estimated as in (11),
fj − f ≈ Sf − f
= αF (DF + diag(uj))−1
f + (1 − α)
e
+ 1
eT
f
+(1 − w)−1
(˜uT
j f)˜s − f. (12)
Note that the matrix (DF +diag(uj))−1
is diagonal. Letting
o[k] be the outlink count for page k in F , we can express
the kth
diagonal element as:
(DF + diag(uj))−1
[k, k] =
1
o[k]+1
if uj[k] = 1
1
o[k]
if uj[k] = 0
Noting that (o[k] + 1)−1
= o[k]−1
− (o[k](o[k] + 1))−1
and
rewriting this in matrix form yields
(DF +diag(uj))−1
= D−1
F −D−1
F (DF +diag(uj))−1
diag(uj).
(13)
We use the same identity to express
e
+ 1
=
e
−
e
( + 1)
. (14)
Recall that, by definition, we have PF = αF D−1
F +(1−α)e
.
Substituting (13) and (14) in (12) yields
fj − f ≈ (PF f − f)
−αF D−1
F (DF + diag(uj))−1
diag(uj)f
−(1 − α)
e
( + 1)
+ (1 − w)−1
(˜uT
j f)˜s
= x + y + (˜uT
j f)z, (15)
noting that by definition, f = PF f, and defining the vectors
x, y, and z to be
x = −αF D−1
F (DF + diag(uj))−1
diag(uj)f (16)
y = −(1 − α)
e
( + 1)
(17)
z = (1 − w)−1
˜s. (18)
The first term x is a sparse vector, and takes non-zero values
only for local pages k that are siblings of the global page
j. We define (i, j) ∈ F if and only if F [j, i] = 1 
(equivalently, page i links to page j) and express the value of the
component x[k ] as:
x[k ] = −α
k:(k,k )∈F ,uj [k]=1
f[k]
o[k](o[k] + 1)
, (19)
where o[k], as before, is the number of outlinks from page k
in the local domain. Note that the last two terms, y and z
are not dependent on the current global node j. Given the
function hj(f) = y + (˜uT
j f)z 1, the quantity fj − f 1
120
Research Track Paper
can be expressed as
fj − f 1 =
k
x[k] + y[k] + (˜uT
j f)z[k]
=
k:x[k]=0
y[k] + (˜uT
j f)z[k]
+
k:x[k]=0
x[k] + y[k] + (˜uT
j f)z[k]
= hj(f) −
k:x[k]=0
y[k] + (˜uT
j f)z[k]
+
k:x[k]=0
x[k] + y[k] + (˜uT
j f)z[k] .(20)
If we can compute the function hj in linear time, then we
can compute each value of fj − f 1 using an additional
amount of time that is proportional to the number of 
nonzero components in x. These optimizations are carried out
in Algorithm 3. Note that (20) computes the difference 
between all components of f and fj, whereas our node 
selection criteria, given in (7), is restricted to the components
corresponding to nodes in the original local domain L.
Let us examine Algorithm 3 in more detail. First, the
algorithm computes the outlink counts for each page in the
local domain. The algorithm then computes the quantity
˜uT
j f for each known global page j. This inner product can
be written as
(1 − α)
1
+ 1
+ α
k:(k,j)∈Fout
f[k]
o[k] + 1
,
where the second term sums over the set of local pages that
link to page j. Since the total number of edges in Fout was
assumed to have size O( ) (recall that is the number of
pages in F ), the running time of this step is also O( ).
The algorithm then computes the vectors y and z, as
given in (17) and (18), respectively. The L1NormDiff
method is called on the components of these vectors which
correspond to the pages in L, and it estimates the value of
EL(y + (˜uT
j f)z) 1 for each page j. The estimation works
as follows. First, the values of ˜uT
j f are discretized uniformly
into c values {a1, ..., ac}. The quantity EL(y + aiz) 1 is
then computed for each discretized value of ai and stored in
a table. To evaluate EL (y + az) 1 for some a ∈ [a1, ac],
the closest discretized value ai is determined, and the 
corresponding entry in the table is used. The total running time
for this method is linear in and the discretization 
parameter c (which we take to be a constant). We note that if exact
values are desired, we have also developed an algorithm that
runs in O( log ) time that is not described here.
In the main loop, we compute the vector x, as defined
in equation (16). The nested loops iterate over the set of
pages in F that are siblings of page j. Typically, the size
of this set is bounded by a constant. Finally, for each page
j, the scores vector is updated over the set of non-zero
components k of the vector x with k ∈ L. This set has
size equal to the number of local siblings of page j, and is
a subset of the total number of siblings of page j. Thus,
each iteration of the main loop takes constant time, and the
total running time of the main loop is O( ). Since we have
assumed that the size of F will not grow larger than O(n),
the total running time for the algorithm is O(n).
Algorithm 3: Node Selection via Stochastic
Complementation.
SC-Select(F , Fout, f, k)
Input: F : zero-one adjacency matrix of size 
corresponding to the current local subgraph, Fout: zero-one
outlink matrix from F to global subgraph, f: 
PageRank of F , k: number of pages to return
Output: pages: set of k pages to crawl next
{Compute outlink sums for local subgraph}
foreach (page j ∈ F )
o[j] ← k:(j,k)∈F F[j, k]
end
{Compute scalar ˜uT
j f for each global node j }
foreach (page j ∈ Fout)
g[j] ← (1 − α) 1
+1
foreach (page k : (k, j) ∈ Fout)
g[j] ← g[j] + α f[k]
o[k]+1
end
end
{Compute vectors y and z as in (17) and (18) }
y ← −(1 − α) e
( +1)
z ← (1 − w)−1
˜s
{Approximate y + g[j] ∗ z 1 for all values g[j]}
norm diffs ←L1NormDiffs(g, ELy, ELz)
foreach (page j ∈ Fout)
{Compute sparse vector x as in (19)}
x ← 0
foreach (page k : (k, j) ∈ Fout)
foreach (page k : (k, k ) ∈ F ))
x[k ] ← x[k ] − f[k]
o[k](o[k]+1)
end
end
x ← αx
scores[j] ← norm diffs[j]
foreach (k : x[k] > 0 and page k ∈ L)
scores[j] ← scores[j] − |y[k] + g[j] ∗ z[k]|
+|x[k]+y[k]+g[j]∗z[k])|
end
end
Return k pages with highest scores
5.2.2 PageRank Flows
We now present an intuitive analysis of the stochastic
complementation method by decomposing the change in 
PageRank in terms of ‘leaks" and ‘flows". This analysis is 
motivated by the decomposition given in (15). PageRank ‘flow" is
the increase in the local PageRanks originating from global
page j. The flows are represented by the non-negative vector
(˜uT
j f)z (equations (15) and (18)). The scalar ˜uT
j f can be
thought of as the total amount of PageRank flow that page
j has available to distribute. The vector z dictates how the
flow is allocated to the local domain; the flow that local
page k receives is proportional to (within a constant factor
due to the random surfer vector) the expected number of its
inlinks.
The PageRank ‘leaks" represent the decrease in PageRank
resulting from the addition of page j. The leakage can
be quantified in terms of the non-positive vectors x and
y (equations (16) and (17)). For vector x, we can see from
equation (19) that the amount of PageRank leaked by a
local page is proportional to the weighted sum of the 
Page121
Research Track Paper
Ranks of its siblings. Thus, pages that have siblings with
higher PageRanks (and low outlink counts) will experience
more leakage. The leakage caused by y is an artifact of the
random surfer vector.
We will next show that if only the ‘flow" term, (˜uT
j f)z,
is considered, then the resulting method is very similar to
a heuristic proposed by Cho et al. [6] that has been widely
used for the Crawling Through URL Ordering problem.
This heuristic is computationally cheaper, but as we will see
later, not as effective as the Stochastic Complementation
method.
Our node selection strategy chooses global nodes that
have the largest influence (equation (7)). If this influence is
approximated using only ‘flows", the optimal node j∗
is:
j∗
= argmaxj EL ˜uT
j fz 1
= argmaxj ˜uT
j f EL z 1
= argmaxj ˜uT
j f
= argmaxj α(DF + diag(uj))−1
uj + (1 − α)
e
+ 1
, f
= argmaxjfT
(DF + diag(uj))−1
uj.
The resulting page selection score can be expressed as a sum
of the PageRanks of each local page k that links to j, where
each PageRank value is normalized by o[k]+1. Interestingly,
the normalization that arises in our method differs from the
heuristic given in [6], which normalizes by o[k]. The 
algorithm PF-Select, which is omitted due to lack of space,
first computes the quantity fT
(DF +diag(uj))−1
uj for each
global page j, and then returns the pages with the k largest
scores. To see that the running time for this algorithm is
O(n), note that the computation involved in this method is
a subset of that needed for the SC-Select method 
(Algorithm 3), which was shown to have a running time of O(n).
6. EXPERIMENTS
In this section, we provide experimental evidence to 
verify the effectiveness of our algorithms. We first outline our
experimental methodology and then provide results across
a variety of local domains.
6.1 Methodology
Given the limited resources available at an academic 
institution, crawling a section of the web that is of the same
magnitude as that indexed by Google or Yahoo! is clearly
infeasible. Thus, for a given local domain, we approximate
the global graph by crawling a local neighborhood around
the domain that is several orders of magnitude larger than
the local subgraph. Even though such a graph is still orders
of magnitude smaller than the ‘true" global graph, we 
contend that, even if there exist some highly influential pages
that are very far away from our local domain, it is 
unrealistic for any local node selection algorithm to find them. Such
pages also tend to be highly unrelated to pages within the
local domain.
When explaining our node selection strategies in section
5, we made the simplifying assumption that our local graph
contained no dangling nodes. This assumption was only
made to ease our analysis. Our implementation efficiently
handles dangling links by replacing each zero column of our
adjacency matrix with the uniform vector. We evaluate the
algorithm using the two node selection strategies given in
Section 5.2, and also against the following baseline methods:
• Random: Nodes are chosen uniformly at random among
the known global nodes.
• OutlinkCount: Global nodes with the highest 
number of outlinks from the local domain are chosen.
At each iteration of the FindGlobalPR algorithm, we 
evaluate performance by computing the difference between the
current PageRank estimate of the local domain, ELf
ELf 1
, and
the global PageRank of the local domain ELg
ELg 1
. All 
PageRank calculations were performed using the uniform 
random surfer vector. Across all experiments, we set the 
random surfer parameter α, to be .85, and used a convergence
threshold of 10−6
. We evaluate the difference between the
local and global PageRank vectors using three different 
metrics: the L1 and L∞ norms, and Kendall"s tau. The L1 norm
measures the sum of the absolute value of the differences 
between the two vectors, and the L∞ norm measures the 
absolute value of the largest difference. Kendall"s tau metric is
a popular rank correlation measure used to compare 
PageRanks [2, 11]. This metric can be computed by counting
the number of pairs of pairs that agree in ranking, and 
subtracting from that the number of pairs of pairs that disagree
in ranking. The final value is then normalized by the total
number of n
2
such pairs, resulting in a [−1, 1] range, where
a negative score signifies anti-correlation among rankings,
and values near one correspond to strong rank correlation.
6.2 Results
Our experiments are based on two large web crawls and
were downloaded using the web crawler that is part of the
Nutch open source search engine project [18]. All crawls
were restricted to only ‘http" pages, and to limit the 
number of dynamically generated pages that we crawl, we 
ignored all pages with urls containing any of the characters
‘?", ‘*", ‘@", or ‘=". The first crawl, which we will refer to
as the ‘edu" dataset, was seeded by homepages of the top
100 graduate computer science departments in the USA, as
rated by the US News and World Report [16], and also by
the home pages of their respective institutions. A crawl of
depth 5 was performed, restricted to pages within the ‘.edu"
domain, resulting in a graph with approximately 4.7 million
pages and 22.9 million links. The second crawl was seeded
by the set of pages under the ‘politics" hierarchy in the dmoz
open directory project[17]. We crawled all pages up to four
links away, which yielded a graph with 4.4 million pages and
17.3 million links.
Within the ‘edu" crawl, we identified the five site-specific
domains corresponding to the websites of the top five 
graduate computer science departments, as ranked by the US
News and World Report. This yielded local domains of 
various sizes, from 10,626 (UIUC) to 59,895 (Berkeley). For each
of these site-specific domains with size n, we performed 50
iterations of the FindGlobalPR algorithm to crawl a total
of 2n additional nodes. Figure 2(a) gives the (L1) difference
from the PageRank estimate at each iteration to the global
PageRank, for the Berkeley local domain.
The performance of this dataset was representative of the
typical performance across the five computer science 
sitespecific local domains. Initially, the L1 difference between
the global and local PageRanks ranged from .0469 
(Stanford) to .149 (MIT). For the first several iterations, the
122
Research Track Paper
0 5 10 15 20 25 30 35 40 45 50
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
0.055
Number of Iterations
GlobalandLocalPageRankDifference(L1)
Stochastic Complement
PageRank Flow
Outlink Count
Random
0 10 20 30 40 50
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Number of Iterations
GlobalandLocalPageRankDifference(L1)
Stochastic Complement
PageRank Flow
Outlink Count
Random
0 5 10 15 20 25
0.16
0.18
0.2
0.22
0.24
0.26
0.28
0.3
0.32
0.34
Number of Iterations
GlobalandLocalPageRankDifference(L1)
Stochastic Complement
PageRank Flow
Outlink Count
Random
(a) www.cs.berkeley.edu (b) www.enterstageright.com (c) Politics
Figure 2: L1 difference between the estimated and true global PageRanks for (a) Berkeley"s computer science
website, (b) the site-specific domain, www.enterstageright.com, and (c) the ‘politics" topic-specific domain. The
stochastic complement method outperforms all other methods across various domains.
three link-based methods all outperform the random 
selection heuristic. After these initial iterations, the random
heuristic tended to be more competitive with (or even 
outperform, as in the Berkeley local domain) the outlink count
and PageRank flow heuristics. In all tests, the stochastic
complementation method either outperformed, or was 
competitive with, the other methods. Table 1 gives the average
difference between the final estimated global PageRanks and
the true global PageRanks for various distance measures.
Algorithm L1 L∞ Kendall
Stoch. Comp. .0384 .00154 .9257
PR Flow .0470 .00272 .8946
Outlink .0419 .00196 .9053
Random .0407 .00204 .9086
Table 1: Average final performance of various node
selection strategies for the five site-specific 
computer science local domains. Note that Kendall"s
Tau measures similarity, while the other metrics are
dissimilarity measures. Stochastic 
Complementation clearly outperforms the other methods in all
metrics.
Within the ‘politics" dataset, we also performed two 
sitespecific tests for the largest websites in the crawl: 
www.adamsmith.org, the website for the London based Adam Smith
Institute, and www.enterstageright.com, an online 
conservative journal. As with the ‘edu" local domains, we ran our
algorithm for 50 iterations, crawling a total of 2n nodes. 
Figure 2 (b) plots the results for the www.enterstageright.com
domain. In contrast to the ‘edu" local domains, the Random
and OutlinkCount methods were not competitive with 
either the SC-Select or the PF-Select methods. Among all
datasets and all node selection methods, the stochastic 
complementation method was most impressive in this dataset,
realizing a final estimate that differed only .0279 from the
global PageRank, a ten-fold improvement over the initial 
local PageRank difference of .299. For the Adam Smith local
domain, the initial difference between the local and global
PageRanks was .148, and the final estimates given by the
SC-Select, PF-Select, OutlinkCount, and Random
methods were .0208, .0193, .0222, and .0356, respectively.
Within the ‘politics" dataset, we constructed four 
topicspecific local domains. The first domain consisted of all
pages in the dmoz politics category, and also all pages within
each of these sites up to two links away. This yielded a local
domain of 90,811 pages, and the results are given in figure 2
(c). Because of the larger size of the topic-specific domains,
we ran our algorithm for only 25 iterations to crawl a total
of n nodes.
We also created topic-specific domains from three 
political sub-topics: liberalism, conservatism, and socialism. The
pages in these domains were identified by their 
corresponding dmoz categories. For each sub-topic, we set the local
domain to be all pages within three links from the 
corresponding dmoz category pages. Table 2 summarizes the
performance of these three topic-specific domains, and also
the larger political domain.
To quantify a global page j"s effect on the global 
PageRank values of pages in the local domain, we define page
j"s impact to be its PageRank value, g[j], normalized by the
fraction of its outlinks pointing to the local domain:
impact(j) =
oL[j]
o[j]
· g[j],
where, oL[j] is the number of outlinks from page j to pages
in the local domain L, and o[j] is the total number of j"s
outlinks. In terms of the random surfer model, the impact
of page j is the probability that the random surfer (1) is
currently at global page j in her random walk and (2) takes
an outlink to a local page, given that she has already decided
not to jump to a random page.
For the politics local domain, we found that many of the
pages with high impact were in fact political pages that
should have been included in the dmoz politics topic, but
were not. For example, the two most influential global pages
were the political search engine www.askhenry.com, and the
home page of the online political magazine, 
www.policyreview.com. Among non-political pages, the home page of
the journal Education Next was most influential. The
journal is freely available online and contains articles 
regarding various aspect of K-12 education in America. To provide
some anecdotal evidence for the effectiveness of our page 
selection methods, we note that the SC-Select method chose
11 pages within the www.educationnext.org domain, the
PF-Select method discovered 7 such pages, while the 
OutlinkCount and Random methods found only 6 pages each.
For the conservative political local domain, the socialist
website www.ornery.org had a very high impact score. This
123
Research Track Paper
All Politics:
Algorithm L1 L2 Kendall
Stoch. Comp. .1253 .000700 .8671
PR Flow .1446 .000710 .8518
Outlink .1470 .00225 .8642
Random .2055 .00203 .8271
Conservativism:
Algorithm L1 L2 Kendall
Stoch. Comp. .0496 .000990 .9158
PR Flow .0554 .000939 .9028
Outlink .0602 .00527 .9144
Random .1197 .00102 .8843
Liberalism:
Algorithm L1 L2 Kendall
Stoch. Comp. .0622 .001360 .8848
PR Flow .0799 .001378 .8669
Outlink .0763 .001379 .8844
Random .1127 .001899 .8372
Socialism:
Algorithm L1 L∞ Kendall
Stoch. Comp. .04318 .00439 .9604
PR Flow .0450 .004251 .9559
Outlink .04282 .00344 .9591
Random .0631 .005123 .9350
Table 2: Final performance among node selection
strategies for the four political topic-specific crawls.
Note that Kendall"s Tau measures similarity, while
the other metrics are dissimilarity measures.
was largely due to a link from the front page of this site
to an article regarding global warming published by the
National Center for Public Policy Research, a conservative
research group in Washington, DC. Not surprisingly, the
global PageRank of this article (which happens to be on the
home page of the NCCPR, www.nationalresearch.com),
was approximately .002, whereas the local PageRank of this
page was only .00158. The SC-Select method yielded a
global PageRank estimate of approximately .00182, the 
PFSelect method estimated a value of .00167, and the 
Random and OutlinkCount methods yielded values of .01522
and .00171, respectively.
7. RELATED WORK
The node selection framework we have proposed is similar
to the url ordering for crawling problem proposed by Cho
et al. in [6]. Whereas our framework seeks to minimize the
difference between the global and local PageRank, the 
objective used in [6] is to crawl the most highly (globally) ranked
pages first. They propose several node selection algorithms,
including the outlink count heuristic, as well as a variant of
our PF-Select algorithm which they refer to as the 
‘PageRank ordering metric". They found this method to be most
effective in optimizing their objective, as did a recent survey
of these methods by Baeza-Yates et al. [1]. Boldi et al. also
experiment within a similar crawling framework in [2], but
quantify their results by comparing Kendall"s rank 
correlation between the PageRanks of the current set of crawled
pages and those of the entire global graph. They found that
node selection strategies that crawled pages with the 
highest global PageRank first actually performed worse (with
respect to Kendall"s Tau correlation between the local and
global PageRanks) than basic depth first or breadth first
strategies. However, their experiments differ from our work
in that our node selection algorithms do not use (or have
access to) global PageRank values.
Many algorithmic improvements for computing exact 
PageRank values have been proposed [9, 10, 14]. If such 
algorithms are used to compute the global PageRanks of our
local domain, they would all require O(N) computation,
storage, and bandwidth, where N is the size of the global
domain. This is in contrast to our method, which 
approximates the global PageRank and scales linearly with the size
of the local domain.
Wang and Dewitt [22] propose a system where the set of
web servers that comprise the global domain communicate
with each other to compute their respective global 
PageRanks. For a given web server hosting n pages, the 
computational, bandwidth, and storage requirements are also
linear in n. One drawback of this system is that the 
number of distinct web servers that comprise the global domain
can be very large. For example, our ‘edu" dataset contains
websites from over 3,200 different universities; coordinating
such a system among a large number of sites can be very
difficult.
Gan, Chen, and Suel propose a method for estimating the
PageRank of a single page [5] which uses only constant 
bandwidth, computation, and space. Their approach relies on the
availability of a remote connectivity server that can supply
the set of inlinks to a given page, an assumption not used in
our framework. They experimentally show that a reasonable
estimate of the node"s PageRank can be obtained by visiting
at most a few hundred nodes. Using their algorithm for our
problem would require that either the entire global domain
first be downloaded or a connectivity server be used, both
of which would lead to very large web graphs.
8. CONCLUSIONS AND FUTURE WORK
The internet is growing exponentially, and in order to 
navigate such a large repository as the web, global search 
engines have established themselves as a necessity. Along with
the ubiquity of these large-scale search engines comes an 
increase in search users" expectations. By providing complete
and isolated coverage of a particular web domain, localized
search engines are an effective outlet to quickly locate 
content that could otherwise be difficult to find. In this work,
we contend that the use of global PageRank in a localized
search engine can improve performance.
To estimate the global PageRank, we have proposed an
iterative node selection framework where we select which
pages from the global frontier to crawl next. Our primary
contribution is our stochastic complementation page 
selection algorithm. This method crawls nodes that will most
significantly impact the local domain and has running time
linear in the number of nodes in the local domain. 
Experimentally, we validate these methods across a diverse set of
local domains, including seven site-specific domains and four
topic-specific domains. We conclude that by crawling an 
additional n or 2n pages, our methods find an estimate of the
global PageRanks that is up to ten times better than just
using the local PageRanks. Furthermore, we demonstrate
that our algorithm consistently outperforms other existing
heuristics.
124
Research Track Paper
Often times, topic-specific domains are discovered using
a focused web crawler which considers a page"s content in
conjunction with link anchor text to decide which pages to
crawl next [4]. Although such crawlers have proven to be
quite effective in discovering topic-related content, many 
irrelevant pages are also crawled in the process. Typically,
these pages are deleted and not indexed by the localized
search engine. These pages can of course provide valuable
information regarding the global PageRank of the local 
domain. One way to integrate these pages into our framework
is to start the FindGlobalPR algorithm with the current
subgraph F equal to the set of pages that were crawled by
the focused crawler.
The global PageRank estimation framework, along with
the node selection algorithms presented, all require O(n)
computation per iteration and bandwidth proportional to
the number of pages crawled, Tk. If the number of 
iterations T is relatively small compared to the number of pages
crawled per iteration, k, then the bottleneck of the algorithm
will be the crawling phase. However, as the number of 
iterations increases (relative to k), the bottleneck will reside in
the node selection computation. In this case, our algorithms
would benefit from constant factor optimizations. Recall
that the FindGlobalPR algorithm (Algorithm 2) requires
that the PageRanks of the current expanded local domain be
recomputed in each iteration. Recent work by Langville and
Meyer [12] gives an algorithm to quickly recompute 
PageRanks of a given webgraph if a small number of nodes are
added. This algorithm was shown to give speedup of five to
ten times on some datasets. We plan to investigate this and
other such optimizations as future work.
In this paper, we have objectively evaluated our methods
by measuring how close our global PageRank estimates are
to the actual global PageRanks. To determine the 
benefit of using global PageRanks in a localized search engine,
we suggest a user study in which users are asked to rate
the quality of search results for various search queries. For
some queries, only the local PageRanks are used in 
ranking, and for the remaining queries, local PageRanks and the
approximate global PageRanks, as computed by our 
algorithms, are used. The results of such a study can then be
analyzed to determine the added benefit of using the global
PageRanks computed by our methods, over just using the
local PageRanks.
Acknowledgements. This research was supported by NSF
grant CCF-0431257, NSF Career Award ACI-0093404, and
a grant from Sabre, Inc.
9. REFERENCES
[1] R. Baeza-Yates, M. Marin, C. Castillo, and
A. Rodriguez. Crawling a country: better strategies
than breadth-first for web page ordering. World-Wide
Web Conference, 2005.
[2] P. Boldi, M. Santini, and S. Vigna. Do your worst to
make the best: paradoxical effects in pagerank
incremental computations. Workshop on Web Graphs,
3243:168-180, 2004.
[3] S. Brin and L. Page. The anatomy of a large-scale
hypertextual web search engine. Computer Networks
and ISDN Systems, 33(1-7):107-117, 1998.
[4] S. Chakrabarti, M. van den Berg, and B. Dom.
Focused crawling: a new approach to topic-specific
web resource discovery. World-Wide Web Conference,
1999.
[5] Y. Chen, Q. Gan, and T. Suel. Local methods for
estimating pagerank values. Conference on
Information and Knowledge Management, 2004.
[6] J. Cho, H. Garcia-Molina, and L. Page. Efficient
crawling through url ordering. World-Wide Web
Conference, 1998.
[7] T. H. Haveliwala and S. D. Kamvar. The second
eigenvalue of the Google matrix. Technical report,
Stanford University, 2003.
[8] T. Joachims, F. Radlinski, L. Granka, A. Cheng,
C. Tillekeratne, and A. Patel. Learning retrieval
functions from implicit feedback.
http://www.cs.cornell.edu/People/tj/career.
[9] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and
G. H. Golub. Exploiting the block structure of the
web for computing pagerank. World-Wide Web
Conference, 2003.
[10] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and
G. H. Golub. Extrapolation methods for accelerating
pagerank computation. World-Wide Web Conference,
2003.
[11] A. N. Langville and C. D. Meyer. Deeper inside
pagerank. Internet Mathematics, 2004.
[12] A. N. Langville and C. D. Meyer. Updating the
stationary vector of an irreducible markov chain with
an eye on Google"s pagerank. SIAM Journal on
Matrix Analysis, 2005.
[13] P. Lyman, H. R. Varian, K. Swearingen, P. Charles,
N. Good, L. L. Jordan, and J. Pal. How much
information 2003? School of Information Management
and System, University of California at Berkely, 2003.
[14] F. McSherry. A uniform approach to accelerated
pagerank computation. World-Wide Web Conference,
2005.
[15] C. D. Meyer. Stochastic complementation, uncoupling
markov chains, and the theory of nearly reducible
systems. SIAM Review, 31:240-272, 1989.
[16] US News and World Report. http://www.usnews.com.
[17] Dmoz open directory project. http://www.dmoz.org.
[18] Nutch open source search engine.
http://www.nutch.org.
[19] F. Radlinski and T. Joachims. Query chains: learning
to rank from implicit feedback. ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 2005.
[20] S. Raghavan and H. Garcia-Molina. Crawling the
hidden web. In Proceedings of the Twenty-seventh
International Conference on Very Large Databases,
2001.
[21] T. Tin Tang, D. Hawking, N. Craswell, and
K. Griffiths. Focused crawling for both topical
relevance and quality of medical information.
Conference on Information and Knowledge
Management, 2005.
[22] Y. Wang and D. J. DeWitt. Computing pagerank in a
distributed internet search system. Proceedings of the
30th VLDB Conference, 2004.
125
Research Track Paper
Feature Representation for Effective Action-Item Detection
Paul N. Bennett
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213
pbennett+@cs.cmu.edu
Jaime Carbonell
Language Technologies Institute.
Carnegie Mellon University
Pittsburgh, PA 15213
jgc+@cs.cmu.edu
ABSTRACT
E-mail users face an ever-growing challenge in managing their 
inboxes due to the growing centrality of email in the workplace for
task assignment, action requests, and other roles beyond 
information dissemination. Whereas Information Retrieval and Machine
Learning techniques are gaining initial acceptance in spam filtering
and automated folder assignment, this paper reports on a new task:
automated action-item detection, in order to flag emails that require
responses, and to highlight the specific passage(s) indicating the 
request(s) for action. Unlike standard topic-driven text classification,
action-item detection requires inferring the sender"s intent, and as
such responds less well to pure bag-of-words classification. 
However, using enriched feature sets, such as n-grams (up to n=4) with
chi-squared feature selection, and contextual cues for action-item
location improve performance by up to 10% over unigrams, using
in both cases state of the art classifiers such as SVMs with 
automated model selection via embedded cross-validation.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.4 [Pattern
Recognition]: Applications
General Terms
Experimentation
1. INTRODUCTION
E-mail users are facing an increasingly difficult task of 
managing their inboxes in the face of mounting challenges that result from
rising e-mail usage. This includes prioritizing e-mails over a range
of sources from business partners to family members, filtering and
reducing junk e-mail, and quickly managing requests that demand
From: Henry Hutchins <hhutchins@innovative.company.com>
To: Sara Smith; Joe Johnson; William Woolings
Subject: meeting with prospective customers
Sent: Fri 12/10/2005 8:08 AM
Hi All,
I"d like to remind all of you that the group from GRTY will be visiting us
next Friday at 4:30 p.m. The current schedule looks like this:
+ 9:30 a.m. Informal Breakfast and Discussion in Cafeteria
+ 10:30 a.m. Company Overview
+ 11:00 a.m. Individual Meetings (Continue Over Lunch)
+ 2:00 p.m. Tour of Facilities
+ 3:00 p.m. Sales Pitch
In order to have this go off smoothly, I would like to practice the 
presentation well in advance. As a result, I will need each of your parts by
Wednesday.
Keep up the good work!
-Henry
Figure 1: An E-mail with emphasized Action-Item, an explicit
request that requires the recipient"s attention or action.
the receiver"s attention or action. Automated action-item detection
targets the third of these problems by attempting to detect which
e-mails require an action or response with information, and within
those e-mails, attempting to highlight the sentence (or other 
passage length) that directly indicates the action request.
Such a detection system can be used as one part of an e-mail
agent which would assist a user in processing important e-mails
quicker than would have been possible without the agent. We view
action-item detection as one necessary component of a successful
e-mail agent which would perform spam detection, action-item 
detection, topic classification and priority ranking, among other 
functions. The utility of such a detector can manifest as a method of
prioritizing e-mails according to task-oriented criteria other than
the standard ones of topic and sender or as a means of ensuring that
the email user hasn"t dropped the proverbial ball by forgetting to
address an action request.
Action-item detection differs from standard text classification in
two important ways. First, the user is interested both in 
detecting whether an email contains action items and in locating exactly
where these action item requests are contained within the email
body. In contrast, standard text categorization merely assigns a
topic label to each text, whether that label corresponds to an e-mail
folder or a controlled indexing vocabulary [12, 15, 22]. Second,
action-item detection attempts to recover the email sender"s intent
- whether she means to elicit response or action on the part of the
receiver; note that for this task, classifiers using only unigrams as
features do not perform optimally, as evidenced in our results 
below. Instead we find that we need more information-laden features
such as higher-order n-grams. Text categorization by topic, on the
other hand, works very well using just individual words as features
[2, 9, 13, 17]. In fact, genre-classification, which one would think
may require more than a bag-of-words approach, also works quite
well using just unigram features [14]. Topic detection and 
tracking (TDT), also works well with unigram feature sets [1, 20]. We
believe that action-item detection is one of the first clear instances
of an IR-related task where we must move beyond bag-of-words
to achieve high performance, albeit not too far, as bag-of-n-grams
seem to suffice.
We first review related work for similar text classification 
problems such as e-mail priority ranking and speech act identification.
Then we more formally define the action-item detection problem,
discuss the aspects that distinguish it from more common problems
like topic classification, and highlight the challenges in 
constructing systems that can perform well at the sentence and document
level. From there, we move to a discussion of feature 
representation and selection techniques appropriate for this problem and how
standard text classification approaches can be adapted to smoothly
move from the sentence-level detection problem to the 
documentlevel classification problem. We then conduct an empirical analysis
that helps us determine the effectiveness of our feature extraction
procedures as well as establish baselines for a number of 
classification algorithms on this task. Finally, we summarize this paper"s
contributions and consider interesting directions for future work.
2. RELATED WORK
Several other researchers have considered very similar text 
classification tasks. Cohen et al. [5] describe an ontology of speech
acts, such as Propose a Meeting, and attempt to predict when an
e-mail contains one of these speech acts. We consider action-items
to be an important specific type of speech act that falls within their
more general classification. While they provide results for 
several classification methods, their methods only make use of human
judgments at the document-level. In contrast, we consider whether
accuracy can be increased by using finer-grained human judgments
that mark the specific sentences and phrases of interest.
Corston-Oliver et al. [6] consider detecting items in e-mail to
Put on a To-Do List. This classification task is very similar to
ours except they do not consider simple factual questions to 
belong to this category. We include questions, but note that not all
questions are action-items - some are rhetorical or simply social
convention, How are you?. From a learning perspective, while
they make use of judgments at the sentence-level, they do not 
explicitly compare what if any benefits finer-grained judgments offer.
Additionally, they do not study alternative choices or approaches to
the classification task. Instead, they simply apply a standard SVM
at the sentence-level and focus primarily on a linguistic analysis of
how the sentence can be logically reformulated before adding it to
the task list. In this study, we examine several alternative 
classification methods, compare document-level and sentence-level 
approaches and analyze the machine learning issues implicit in these
problems.
Interest in a variety of learning tasks related to e-mail has been
rapidly growing in the recent literature. For example, in a forum
dedicated to e-mail learning tasks, Culotta et al. [7] presented 
methods for learning social networks from e-mail. In this work, we do
not focus on peer relationships; however, such methods could 
complement those here since peer relationships often influence word
choice when requesting an action.
3. PROBLEM DEFINITION & APPROACH
In contrast to previous work, we explicitly focus on the benefits
that finer-grained, more costly, sentence-level human judgments 
offer over coarse-grained document-level judgments. Additionally,
we consider multiple standard text classification approaches and
analyze both the quantitative and qualitative differences that arise
from taking a document-level vs. a sentence-level approach to 
classification. Finally, we focus on the representation necessary to
achieve the most competitive performance.
3.1 Problem Definition
In order to provide the most benefit to the user, a system would
not only detect the document, but it would also indicate the specific
sentences in the e-mail which contain the action-items. Therefore,
there are three basic problems:
1. Document detection: Classify a document as to whether or
not it contains an action-item.
2. Document ranking: Rank the documents such that all 
documents containing action-items occur as high as possible in
the ranking.
3. Sentence detection: Classify each sentence in a document as
to whether or not it is an action-item.
As in most Information Retrieval tasks, the weight the 
evaluation metric should give to precision and recall depends on the 
nature of the application. In situations where a user will eventually
read all received messages, ranking (e.g., via precision at recall of
1) may be most important since this will help encourage shorter 
delays in communications between users. In contrast, high-precision
detection at low recall will be of increasing importance when the
user is under severe time-pressure and therefore will likely not read
all mail. This can be the case for crisis managers during disaster
management. Finally, sentence detection plays a role in both 
timepressure situations and simply to alleviate the user"s required time
to gist the message.
3.2 Approach
As mentioned above, the labeled data can come in one of two
forms: a document-labeling provides a yes/no label for each 
document as to whether it contains an action-item; a phrase-labeling
provides only a yes label for the specific items of interest. We term
the human judgments a phrase-labeling since the user"s view of the
action-item may not correspond with actual sentence boundaries or
predicted sentence boundaries. Obviously, it is straightforward to
generate a document-labeling consistent with a phrase-labeling by
labeling a document yes if and only if it contains at least one
phrase labeled yes.
To train classifiers for this task, we can take several viewpoints
related to both the basic problems we have enumerated and the form
of the labeled data. The document-level view treats each e-mail as
a learning instance with an associated class-label. Then, the 
document can be converted to a feature-value vector and learning 
progresses as usual. Applying a document-level classifier to document
detection and ranking is straightforward. In order to apply it to
sentence detection, one must make additional steps. For example,
if the classifier predicts a document contains an action-item, then
areas of the document that contain a high-concentration of words
which the model weights heavily in favor of action-items can be
indicated. The obvious benefit of the document-level approach is
that training set collection costs are lower since the user only has
to specify whether or not an e-mail contains an action-item and not
the specific sentences.
In the sentence-level view, each e-mail is automatically segmented
into sentences, and each sentence is treated as a learning instance
with an associated class-label. Since the phrase-labeling provided
by the user may not coincide with the automatic segmentation, we
must determine what label to assign a partially overlapping 
sentence when converting it to a learning instance. Once trained, 
applying the resulting classifiers to sentence detection is now 
straightforward, but in order to apply the classifiers to document 
detection and document ranking, the individual predictions over each
sentence must be aggregated in order to make a document-level
prediction. This approach has the potential to benefit from 
morespecific labels that enable the learner to focus attention on the key
sentences instead of having to learn based on data that the majority
of the words in the e-mail provide no or little information about
class membership.
3.2.1 Features
Consider some of the phrases that might constitute part of an
action item: would like to know, let me know, as soon as
possible, have you. Each of these phrases consists of common
words that occur in many e-mails. However, when they occur in
the same sentence, they are far more indicative of an action-item.
Additionally, order can be important: consider have you versus
you have. Because of this, we posit that n-grams play a larger
role in this problem than is typical of problems like topic 
classification. Therefore, we consider all n-grams up to size 4.
When using n-grams, if we find an n-gram of size 4 in a segment
of text, we can represent the text as just one occurrence of the 
ngram or as one occurrence of the n-gram and an occurrence of each
smaller n-gram contained by it. We choose the second of these
alternatives since this will allow the algorithm itself to smoothly
back-off in terms of recall. Methods such as na¨ıve Bayes may be
hurt by such a representation because of double-counting.
Since sentence-ending punctuation can provide information, we
retain the terminating punctuation token when it is identifiable. 
Additionally, we add a beginning-of-sentence and end-of-sentence 
token in order to capture patterns that are often indicators at the 
beginning or end of a sentence. Assuming proper punctuation, these
extra tokens are unnecessary, but often e-mail lacks proper 
punctuation. In addition, for the sentence-level classifiers that use 
ngrams, we additionally code for each sentence a binary encoding
of the position of the sentence relative to the document. This 
encoding has eight associated features that represent which octile (the
first eighth, second eighth, etc.) contains the sentence.
3.2.2 Implementation Details
In order to compare the document-level to the sentence-level 
approach, we compare predictions at the document-level. We do not
address how to use a document-level classifier to make predictions
at the sentence-level.
In order to automatically segment the text of the e-mail, we use
the RASP statistical parser [4]. Since the automatically segmented
sentences may not correspond directly with the phrase-level 
boundaries, we treat any sentence that contains at least 30% of a marked
action-item segment as an action-item. When evaluating 
sentencedetection for the sentence-level system, we use these class labels
as ground truth. Since we are not evaluating multiple segmentation
approaches, this does not bias any of the methods. If multiple 
segmentation systems were under evaluation, one would need to use a
metric that matched predicted positive sentences to phrases labeled
positive. The metric would need to punish overly long true 
predictions as well as too short predictions. Our criteria for converting
to labeled instances implicitly includes both criteria. Since the 
segmentation is fixed, an overly long prediction would be predicting
yes for many no instances since presumably the extra length
corresponds to additional segmented sentences all of which do not
contain 30% of action-item. Likewise, a too short prediction must
correspond to a small sentence included in the action-item but not
constituting all of the action-item. Therefore, in order to consider
the prediction to be too short, there will be an additional 
preceding/following sentence that is an action-item where we incorrectly
predicted no.
Once a sentence-level classifier has made a prediction for each
sentence, we must combine these predictions to make both a 
document-level prediction and a document-level score. We use the
simple policy of predicting positive when any of the sentences is
predicted positive. In order to produce a document score for 
ranking, the confidence that the document contains an action-item is:
ψ(d) =
1
n(d) s∈d|π(s)=1 ψ(s) if for any s ∈ d, π(s) = 1
1
n(d)
maxs∈d ψ(s) o.w.
where s is a sentence in document d, π is the classifier"s 1/0 
prediction, ψ is the score the classifier assigns as its confidence that
π(s) = 1, and n(d) is the greater of 1 and the number of (unigram)
tokens in the document. In other words, when any sentence is 
predicted positive, the document score is the length normalized sum of
the sentence scores above threshold. When no sentence is predicted
positive, the document score is the maximum sentence score 
normalized by length. As in other text problems, we are more likely to
emit false positives for documents with more words or sentences.
Thus we include a length normalization factor.
4. EXPERIMENTAL ANALYSIS
4.1 The Data
Our corpus consists of e-mails obtained from volunteers at an
educational institution and cover subjects such as: organizing a
research workshop, arranging for job-candidate interviews, 
publishing proceedings, and talk announcements. The messages were
anonymized by replacing the names of each individual and 
institution with a pseudonym.1
After attempting to identify and eliminate
duplicate e-mails, the corpus contains 744 e-mail messages.
After identity anonymization, the corpora has three basic 
versions. Quoted material refers to the text of a previous e-mail that
an author often leaves in an e-mail message when responding to the
e-mail. Quoted material can act as noise when learning since it may
include action-items from previous messages that are no longer 
relevant. To isolate the effects of quoted material, we have three 
versions of the corpora. The raw form contains the basic messages.
The auto-stripped version contains the messages after quoted 
material has been automatically removed. The hand-stripped version
contains the messages after quoted material has been removed by
a human. Additionally, the hand-stripped version has had any xml
content and e-mail signatures removed - leaving only the essential
content of the message. The studies reported here are performed
with the hand-stripped version. This allows us to balance the 
cognitive load in terms of number of tokens that must be read in the
user-studies we report - including quoted material would 
complicate the user studies since some users might skip the material while
others read it. Additionally, ensuring all quoted material is removed
1
We have an even more highly anonymized version of the 
corpus that can be made available for some outside experimentation.
Please contact the authors for more information on obtaining this
data.
prevents tainting the cross-validation since otherwise a test item
could occur as quoted material in a training document.
4.1.1 Data Labeling
Two human annotators labeled each message as to whether or
not it contained an action-item. In addition, they identified each
segment of the e-mail which contained an action-item. A segment
is a contiguous section of text selected by the human annotators
and may span several sentences or a complete phrase contained in
a sentence. They were instructed that an action item is an explicit
request for information that requires the recipient"s attention or a
required action and told to highlight the phrases or sentences that
make up the request.
Annotator 1
No Yes
Annotator 2
No 391 26
Yes 29 298
Table 1: Agreement of Human Annotators at Document Level
Annotator One labeled 324 messages as containing action items.
Annotator Two labeled 327 messages as containing action items.
The agreement of the human annotators is shown in Tables 1 and
2. The annotators are said to agree at the document-level when
both marked the same document as containing no action-items or
both marked at least one action-item regardless of whether the text
segments were the same. At the document-level, the annotators
agreed 93% of the time. The kappa statistic [3, 5] is often used to
evaluate inter-annotator agreement:
κ =
A − R
1 − R
A is the empirical estimate of the probability of agreement. R
is the empirical estimate of the probability of random agreement
given the empirical class priors. A value close to −1 implies the
annotators agree far less often than would be expected randomly,
while a value close to 1 means they agree more often than randomly
expected.
At the document-level, the kappa statistic for inter-annotator 
agreement is 0.85. This value is both strong enough to expect the 
problem to be learnable and is comparable with results for similar tasks
[5, 6].
In order to determine the sentence-level agreement, we use each
judgment to create a sentence-corpus with labels as described in
Section 3.2.2, then consider the agreement over these sentences.
This allows us to compare agreement over no judgments. We
perform this comparison over the hand-stripped corpus since that
eliminates spurious no judgments that would come from 
including quoted material, etc. Both annotators were free to label the
subject as an action-item, but since neither did, we omit the subject
line of the message as well. This only reduces the number of no
agreements. This leaves 6301 automatically segmented sentences.
At the sentence-level, the annotators agreed 98% of the time, and
the kappa statistic for inter-annotator agreement is 0.82.
In order to produce one single set of judgments, the human 
annotators went through each annotation where there was 
disagreement and came to a consensus opinion. The annotators did not
collect statistics during this process but anecdotally reported that
the majority of disagreements were either cases of clear annotator
oversight or different interpretations of conditional statements. For
example, If you would like to keep your job, come to tomorrow"s
meeting implies a required action where If you would like to join
Annotator 1
No Yes
Annotator 2
No 5810 65
Yes 74 352
Table 2: Agreement of Human Annotators at Sentence Level
the football betting pool, come to tomorrow"s meeting does not.
The first would be an action-item in most contexts while the 
second would not. Of course, many conditional statements are not so
clearly interpretable. After reconciling the judgments there are 416
e-mails with no action-items and 328 e-mails containing 
actionitems. Of the 328 e-mails containing action-items, 259 messages
have one action-item segment; 55 messages have two action-item
segments; 11 messages have three action-item segments. Two 
messages have four action-item segments, and one message has six
action-item segments. Computing the sentence-level agreement 
using the reconciled gold standard judgments with each of the 
annotators" individual judgments gives a kappa of 0.89 for Annotator
One and a kappa of 0.92 for Annotator Two.
In terms of message characteristics, there were on average 132
content tokens in the body after stripping. For action-item 
messages, there were 115. However, by examining Figure 2 we see
the length distributions are nearly identical. As would be expected
for e-mail, it is a long-tailed distribution with about half the 
messages having more than 60 tokens in the body (this paragraph has
65 tokens).
4.2 Classifiers
For this experiment, we have selected a variety of standard text
classification algorithms. In selecting algorithms, we have chosen
algorithms that are not only known to work well but which differ
along such lines as discriminative vs. generative and lazy vs. 
eager. We have done this in order to provide both a competitive and
thorough sampling of learning methods for the task at hand. This
is important since it is easy to improve a strawman classifier by
introducing a new representation. By thoroughly sampling 
alternative classifier choices we demonstrate that representation 
improvements over bag-of-words are not due to using the information in the
bag-of-words poorly.
4.2.1 kNN
We employ a standard variant of the k-nearest neighbor 
algorithm used in text classification, kNN with s-cut score 
thresholding [19]. We use a tfidf-weighting of the terms with a 
distanceweighted vote of the neighbors to compute the score before 
thresholding it. In order to choose the value of s for thresholding, we
perform leave-one-out cross-validation over the training set. The
value of k is set to be 2( log2 N + 1) where N is the number of
training points. This rule for choosing k is theoretically motivated
by results which show such a rule converges to the optimal 
classifier as the number of training points increases [8]. In practice,
we have also found it to be a computational convenience that 
frequently leads to comparable results with numerically optimizing k
via a cross-validation procedure.
4.2.2 Na¨ıve Bayes
We use a standard multinomial na¨ıve Bayes classifier [16]. In 
using this classifier, we smoothed word and class probabilities using a
Bayesian estimate (with the word prior) and a Laplace m-estimate,
respectively.
0
20
40
60
80
100
120
140
160
0 200 400 600 800 1000 1200 1400
NumberofMessages
Number of Tokens
All Messages
Action-Item Messages
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0 200 400 600 800 1000 1200 1400
PercentageofMessages
Number of Tokens
All Messages
Action-Item Messages
Figure 2: The Histogram (left) and Distribution (right) of Message Length. A bin size of 20 words was used. Only tokens in the body
after hand-stripping were counted. After stripping, the majority of words left are usually actual message content.
Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram
F1
kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460
na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426
SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451
Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913
Accuracy
kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352
na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268
SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258
Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032
Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation
(Sn−1) in italics. The best performance for each classifier is shown in bold.
4.2.3 SVM
We have used a linear SVM with a tfidf feature representation
and L2-norm as implemented in the SVMlight package v6.01 [11].
All default settings were used.
4.2.4 Voted Perceptron
Like the SVM, the Voted Perceptron is a kernel-based 
learning method. We use the same feature representation and kernel
as we have for the SVM, a linear kernel with tfidf-weighting and
an L2-norm. The voted perceptron is an online-learning method
that keeps a history of past perceptrons used, as well as a weight
signifying how often that perceptron was correct. With each new
training example, a correct classification increases the weight on
the current perceptron and an incorrect classification updates the
perceptron. The output of the classifier uses the weights on the
perceptra to make a final voted classification. When used in an
offline-manner, multiple passes can be made through the training
data. Both the voted perceptron and the SVM give a solution from
the same hypothesis space - in this case, a linear classifier. 
Furthermore, it is well-known that the Voted Perceptron increases the
margin of the solution after each pass through the training data [10].
Since Cohen et al. [5] obtain worse results using an SVM than a
Voted Perceptron with one training iteration, they conclude that the
best solution for detecting speech acts may not lie in an area with
a large margin. Because their tasks are highly similar to ours, we
employ both classifiers to ensure we are not overlooking a 
competitive alternative classifier to the SVM for the basic bag-of-words
representation.
4.3 Performance Measures
To compare the performance of the classification methods, we
look at two standard performance measures, F1 and accuracy. The
F1 measure [18, 21] is the harmonic mean of precision and recall
where Precision = Correct Positives
Predicted Positives
and Recall = Correct Positives
Actual Positives
.
4.4 Experimental Methodology
We perform standard 10-fold cross-validation on the set of 
documents. For the sentence-level approach, all sentences in a 
document are either entirely in the training set or entirely in the test set
for each fold. For significance tests, we use a two-tailed t-test [21]
to compare the values obtained during each cross-validation fold
with a p-value of 0.05.
Feature selection was performed using the chi-squared 
statistic. Different levels of feature selection were considered for each
classifier. Each of the following number of features was tried:
10, 25, 50, 100, 250, 750, 1000, 2000, 4000. There are 
approximately 4700 unigram tokens without feature selection. In order to choose
the number of features to use for each classifier, we perform nested
cross-validation and choose the settings that yield the optimal 
document-level F1 for that classifier. For this study, only the body of
each e-mail message was used. Feature selection is always applied
to all candidate features. That is, for the n-gram representation, the
n-grams and position features are also subject to removal by the
feature selection method.
4.5 Results
The results for document-level classification are given in Table
3. The primary hypothesis we are concerned with is that n-grams
are critical for this task; if this is true, we expect to see a significant
gap in performance between the document-level classifiers that use
n-grams (denoted Document Ngram) and those using only unigram
features (denoted Document Unigram). Examining Table 3, we 
observe that this is indeed the case for every classifier except na¨ıve
Bayes. This difference in performance produced by the n-gram
representation is statistically significant for each classifier except
for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).
Na¨ıve Bayes poor performance with the n-gram representation is
not surprising since the bag-of-n-grams causes excessive 
doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is
not hurt at the sentence-level because the sparse examples provide
few chances for agglomerative effects of double counting. In either
case, when a language-modeling approach is desired, modeling the
n-grams directly would be preferable to na¨ıve Bayes. More 
importantly for the n-gram hypothesis, the n-grams lead to the best
document-level classifier performance as well.
As would be expected, the difference between the sentence-level
n-gram representation and unigram representation is small. This
is because the window of text is so small that the unigram 
representation, when done at the sentence-level, implicitly picks up
on the power of the n-grams. Further improvement would 
signify that the order of the words matter even when only 
considering a small sentence-size window. Therefore, the finer-grained
sentence-level judgments allows a unigram representation to 
succeed but only when performed in a small window - behaving as
an n-gram representation for all practical purposes.
Document Winner Sentence Winner
kNN Ngram Ngram
na¨ıve Bayes Unigram Ngram
SVM Ngram†
Ngram
Voted Perceptron Ngram†
Ngram
Table 4: Significance results for n-grams versus unigrams for
document detection using document-level and sentence-level
classifiers. When the F1 result is statistically significant, it is
shown in bold. When the accuracy result is significant, it is
shown with a †
.
F1 Winner Accuracy Winner
kNN Sentence Sentence
na¨ıve Bayes Sentence Sentence
SVM Sentence Sentence
Voted Perceptron Sentence Document
Table 5: Significance results for sentence-level classifiers vs.
document-level classifiers for the document detection problem.
When the result is statistically significant, it is shown in bold.
Further highlighting the improvement from finer-grained 
judgments and n-grams, Figure 3 graphically depicts the edge the SVM
sentence-level classifier has over the standard bag-of-words approach
with a precision-recall curve. In the high precision area of the
graph, the consistent edge of the sentence-level classifier is rather
impressive - continuing at precision 1 out to 0.1 recall. This
would mean that a tenth of the user"s action-items would be placed
at the top of their action-item sorted inbox. Additionally, the large
separation at the top right of the curves corresponds to the area
where the optimal F1 occurs for each classifier, agreeing with the
large improvement from 0.6904 to 0.7682 in F1 score. Considering
the relative unexplored nature of classification at the sentence-level,
this gives great hope for further increases in performance.
Accuracy F1
Unigram Ngram Unigram Ngram
kNN 0.9519 0.9536 0.6540 0.6686
na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676
SVM 0.9559 0.9579 0.6271 0.6672
Voted Perceptron 0.8895 0.9247 0.3744 0.5164
Table 6: Performance of the Sentence-Level Classifiers at 
Sentence Detection
Although Cohen et al. [5] observed that the Voted Perceptron
with a single training iteration outperformed SVM in a set of 
similar tasks, we see no such behavior here. This further strengthens the
evidence that an alternate classifier with the bag-of-words 
representation could not reach the same level of performance. The Voted
Perceptron classifier does improve when the number of training 
iterations are increased, but it is still lower than the SVM classifier.
Sentence detection results are presented in Table 6. With regard
to the sentence detection problem, we note that the F1 measure
gives a better feel for the remaining room for improvement in this
difficult problem. That is, unlike document detection where 
actionitem documents are fairly common, action-item sentences are very
rare. Thus, as in other text problems, the accuracy numbers are 
deceptively high sheerly because of the default accuracy attainable by
always predicting no. Although, the results here are significantly
above-random, it is unclear what level of performance is necessary
for sentence detection to be useful in and of itself and not simply
as a means to document ranking and classification.
Figure 4: Users find action-items quicker when assisted by a
classification system.
Finally, when considering a new type of classification task, one
of the most basic questions is whether an accurate classifier built
for the task can have an impact on the end-user. In order to 
demonstrate the impact this task can have on e-mail users, we conducted
a user study using an earlier less-accurate version of the sentence
classifier - where instead of using just a single sentence, a 
threesentence windowed-approach was used. There were three distinct
sets of e-mail in which users had to find action-items. These sets
were either presented in a random order (Unordered), ordered by
the classifier (Ordered), or ordered by the classifier and with the
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Precision
Recall
Action-Item Detection SVM Performance (Post Model Selection)
Document Unigram
Sentence Ngram
Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach.
center sentence in the highest confidence window highlighted 
(Order+help). In order to perform fair comparisons between 
conditions, the overall number of tokens in each message set should be
approximately equal; that is, the cognitive reading load should be
approximately the same before the classifier"s reordering. 
Additionally, users typically show practice effects by improving at the
overall task and thus performing better at later message sets. This
is typically handled by varying the ordering of the sets across users
so that the means are comparable. While omitting further detail,
we note the sets were balanced for the total number of tokens and
a latin square design was used to balance practice effects.
Figure 4 shows that at intervals of 5, 10, and 15 minutes, users
consistently found significantly more action-items when assisted
by the classifier, but were most critically aided in the first five 
minutes. Although, the classifier consistently aids the users, we did not
gain an additional end-user impact by highlighting. As mentioned
above, this might be a result of the large room for improvement that
still exists for sentence detection, but anecdotal evidence suggests
this might also be a result of how the information is presented to the
user rather than the accuracy of sentence detection. For example,
highlighting the wrong sentence near an actual action-item hurts
the user"s trust, but if a vague indicator (e.g., an arrow) points to the
approximate area the user is not aware of the near-miss. Since the
user studies used a three sentence window, we believe this played a
role as well as sentence detection accuracy.
4.6 Discussion
In contrast to problems where n-grams have yielded little 
difference, we believe their power here stems from the fact that many of
the meaningful n-grams for action-items consist of common words,
e.g., let me know. Therefore, the document-level unigram 
approach cannot gain much leverage, even when modeling their joint
probability correctly, since these words will often co-occur in the
document but not necessarily in a phrase. Additionally, action-item
detection is distinct from many text classification tasks in that a
single sentence can change the class label of the document. As a
result, good classifiers cannot rely on aggregating evidence from a
large number of weak indicators across the entire document.
Even though we discarded the header information, examining
the top-ranked features at the document-level reveals that many of
the features are names or parts of e-mail addresses that occurred in
the body and are highly associated with e-mails that tend to 
contain many or no action-items. A few examples are terms such as
org, bob, and gov. We note that these features will be 
sensitive to the particular distribution (senders/receivers) and thus the
document-level approach may produce classifiers that transfer less
readily to alternate contexts and users at different institutions. This
points out that part of the problem of going beyond bag-of-words
may be the methodology, and investigating such properties as 
learning curves and how well a model transfers may highlight 
differences in models which appear to have similar performance when
tested on the distributions they were trained on. We are currently
investigating whether the sentence-level classifiers do perform 
better over different test corpora without retraining.
5. FUTURE WORK
While applying text classifiers at the document-level is fairly
well-understood, there exists the potential for significantly 
increasing the performance of the sentence-level classifiers. Such methods
include alternate ways of combining the predictions over each 
sentence, weightings other than tfidf, which may not be appropriate
since sentences are small, better sentence segmentation, and other
types of phrasal analysis. Additionally, named entity tagging, time
expressions, etc., seem likely candidates for features that can 
further improve this task. We are currently pursuing some of these
avenues to see what additional gains these offer.
Finally, it would be interesting to investigate the best methods for
combining the document-level and sentence-level classifiers. Since
the simple bag-of-words representation at the document-level leads
to a learned model that behaves somewhat like a context-specific
prior dependent on the sender/receiver and general topic, a first
choice would be to treat it as such when combining probability
estimates with the sentence-level classifier. Such a model might
serve as a general example for other problems where bag-of-words
can establish a baseline model but richer approaches are needed to
achieve performance beyond that baseline.
6. SUMMARY AND CONCLUSIONS
The effectiveness of sentence-level detection argues that 
labeling at the sentence-level provides significant value. Further 
experiments are needed to see how this interacts with the amount of 
training data available. Sentence detection that is then agglomerated to
document-level detection works surprisingly better given low recall
than would be expected with sentence-level items. This, in turn, 
indicates that improved sentence segmentation methods could yield
further improvements in classification.
In this work, we examined how action-items can be effectively
detected in e-mails. Our empirical analysis has demonstrated that
n-grams are of key importance to making the most of 
documentlevel judgments. When finer-grained judgments are available, then
a standard bag-of-words approach using a small (sentence) window
size and automatic segmentation techniques can produce results 
almost as good as the n-gram based approaches.
Acknowledgments
This material is based upon work supported by the Defense 
Advanced Research Projects Agency (DARPA) under Contract No.
NBCHD030010. Any opinions, findings and conclusions or 
recommendations expressed in this material are those of the author(s)
and do not necessarily reflect the views of the Defense Advanced
Research Projects Agency (DARPA), or the Department of 
InteriorNational Business Center (DOI-NBC).
We would like to extend our sincerest thanks to Jill Lehman
whose efforts in data collection were essential in constructing the
corpus, and both Jill and Aaron Steinfeld for their direction of the
HCI experiments. We would also like to thank Django Wexler for
constructing and supporting the corpus labeling tools and Curtis
Huttenhower"s support of the text preprocessing package. Finally,
we gratefully acknowledge Scott Fahlman for his encouragement
and useful discussions on this topic.
7. REFERENCES
[1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. Topic detection and tracking pilot study: Final
report. In Proceedings of the DARPA Broadcast News
Transcription and Understanding Workshop, Washington,
D.C., 1998.
[2] C. Apte, F. Damerau, and S. M. Weiss. Automated learning
of decision rules for text categorization. ACM Transactions
on Information Systems, 12(3):233-251, July 1994.
[3] J. Carletta. Assessing agreement on classification tasks: The
kappa statistic. Computational Linguistics, 22(2):249-254,
1996.
[4] J. Carroll. High precision extraction of grammatical relations.
In Proceedings of the 19th International Conference on
Computational Linguistics (COLING), pages 134-140, 2002.
[5] W. W. Cohen, V. R. Carvalho, and T. M. Mitchell. Learning
to classify email into speech acts. In EMNLP-2004
(Conference on Empirical Methods in Natural Language
Processing), pages 309-316, 2004.
[6] S. Corston-Oliver, E. Ringger, M. Gamon, and R. Campbell.
Task-focused summarization of email. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop, pages
43-50, 2004.
[7] A. Culotta, R. Bekkerman, and A. McCallum. Extracting
social networks and contact information from email and the
web. In CEAS-2004 (Conference on Email and Anti-Spam),
Mountain View, CA, July 2004.
[8] L. Devroye, L. Gy¨orfi, and G. Lugosi. A Probabilistic Theory
of Pattern Recognition. Springer-Verlag, New York, NY,
1996.
[9] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.
Inductive learning algorithms and representations for text
categorization. In CIKM "98, Proceedings of the 7th ACM
Conference on Information and Knowledge Management,
pages 148-155, 1998.
[10] Y. Freund and R. Schapire. Large margin classification using
the perceptron algorithm. Machine Learning, 37(3):277-296,
1999.
[11] T. Joachims. Making large-scale svm learning practical. In
B. Sch¨olkopf, C. J. Burges, and A. J. Smola, editors,
Advances in Kernel Methods - Support Vector Learning,
pages 41-56. MIT Press, 1999.
[12] L. S. Larkey. A patent search and classification system. In
Proceedings of the Fourth ACM Conference on Digital
Libraries, pages 179 - 187, 1999.
[13] D. D. Lewis. An evaluation of phrasal and clustered
representations on a text categorization task. In SIGIR "92,
Proceedings of the 15th Annual International ACM
Conference on Research and Development in Information
Retrieval, pages 37-50, 1992.
[14] Y. Liu, J. Carbonell, and R. Jin. A pairwise ensemble
approach for accurate genre classification. In Proceedings of
the European Conference on Machine Learning (ECML),
2003.
[15] Y. Liu, R. Yan, R. Jin, and J. Carbonell. A comparison study
of kernels for multi-label text classification using category
association. In The Twenty-first International Conference on
Machine Learning (ICML), 2004.
[16] A. McCallum and K. Nigam. A comparison of event models
for naive bayes text classification. In Working Notes of AAAI
"98 (The 15th National Conference on Artificial
Intelligence), Workshop on Learning for Text Categorization,
pages 41-48, 1998. TR WS-98-05.
[17] F. Sebastiani. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1-47, March
2002.
[18] C. J. van Rijsbergen. Information Retrieval. Butterworths,
London, 1979.
[19] Y. Yang. An evaluation of statistical approaches to text
categorization. Information Retrieval, 1(1/2):67-88, 1999.
[20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald,
and X. Liu. Learning approaches to topic detection and
tracking. IEEE EXPERT, Special Issue on Applications of
Intelligent Information Retrieval, 1999.
[21] Y. Yang and X. Liu. A re-examination of text categorization
methods. In SIGIR "99, Proceedings of the 22nd Annual
International ACM Conference on Research and
Development in Information Retrieval, pages 42-49, 1999.
[22] Y. Yang, J. Zhang, J. Carbonell, and C. Jin.
Topic-conditioned novelty detection. In Proceedings of the
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, July 2002.
A Semantic Approach to Contextual Advertising
Andrei Broder Marcus Fontoura Vanja Josifovski Lance Riedel
Yahoo! Research, 2821 Mission College Blvd, Santa Clara, CA 95054
{broder, marcusf, vanjaj, riedell}@yahoo-inc.com
ABSTRACT
Contextual advertising or Context Match (CM) refers to the
placement of commercial textual advertisements within the
content of a generic web page, while Sponsored Search (SS)
advertising consists in placing ads on result pages from a web
search engine, with ads driven by the originating query. In
CM there is usually an intermediary commercial ad-network
entity in charge of optimizing the ad selection with the twin
goal of increasing revenue (shared between the publisher and
the ad-network) and improving the user experience. With
these goals in mind it is preferable to have ads relevant to
the page content, rather than generic ads.
The SS market developed quicker than the CM market,
and most textual ads are still characterized by bid phrases
representing those queries where the advertisers would like
to have their ad displayed. Hence, the first technologies
for CM have relied on previous solutions for SS, by simply
extracting one or more phrases from the given page 
content, and displaying ads corresponding to searches on these
phrases, in a purely syntactic approach. However, due to the
vagaries of phrase extraction, and the lack of context, this
approach leads to many irrelevant ads. To overcome this
problem, we propose a system for contextual ad matching
based on a combination of semantic and syntactic features.
Categories and Subject Descriptors: H.3.3 
[Information Storage and Retrieval]: Selection process
General Terms: Algorithms, Measurement, Performance,
Experimentation
1. INTRODUCTION
Web advertising supports a large swath of today"s Internet
ecosystem. The total internet advertiser spend in US alone
in 2006 is estimated at over 17 billion dollars with a growth
rate of almost 20% year over year. A large part of this
market consists of textual ads, that is, short text messages
usually marked as sponsored links or similar. The main
advertising channels used to distribute textual ads are:
1. Sponsored Search or Paid Search advertising which 
consists in placing ads on the result pages from a web
search engine, with ads driven by the originating query.
All major current web search engines (Google, Yahoo!,
and Microsoft) support such ads and act 
simultaneously as a search engine and an ad agency.
2. Contextual advertising or Context Match which refers
to the placement of commercial ads within the 
content of a generic web page. In contextual advertising
usually there is a commercial intermediary, called an
ad-network, in charge of optimizing the ad selection
with the twin goal of increasing revenue (shared 
between publisher and ad-network) and improving user
experience. Again, all major current web search 
engines (Google, Yahoo!, and Microsoft) provide such
ad-networking services but there are also many smaller
players.
The SS market developed quicker than the CM market,
and most textual ads are still characterized by bid phrases
representing those queries where the advertisers would like
to have their ad displayed. (See [5] for a brief history).
However, today, almost all of the for-profit non-transactional
web sites (that is, sites that do not sell anything directly)
rely at least in part on revenue from context match. CM
supports sites that range from individual bloggers and small
niche communities to large publishers such as major 
newspapers. Without this model, the web would be a lot smaller!
The prevalent pricing model for textual ads is that the
advertisers pay a certain amount for every click on the 
advertisement (pay-per-click or PPC). There are also other
models used: pay-per-impression, where the advertisers pay
for the number of exposures of an ad and pay-per-action
where the advertiser pays only if the ad leads to a sale or
similar transaction. For simplicity, we only deal with the
PPC model in this paper.
Given a page, rather than placing generic ads, it seems
preferable to have ads related to the content to provide a
better user experience and thus to increase the probability
of clicks. This intuition is supported by the analogy to 
conventional publishing where there are very successful 
magazines (e.g. Vogue) where a majority of the content is topical
advertising (fashion in the case of Vogue) and by user 
studies that have confirmed that increased relevance increases
the number of ad-clicks [4, 13].
Previous published approaches estimated the ad relevance
based on co-occurrence of the same words or phrases within
the ad and within the page (see [7, 8] and Section 3 for
more details). However targeting mechanisms based solely
on phrases found within the text of the page can lead to
problems: For example, a page about a famous golfer named
John Maytag might trigger an ad for Maytag 
dishwashers since Maytag is a popular brand. Another example
could be a page describing the Chevy Tahoe truck (a 
popular vehicle in US) triggering an ad about Lake Tahoe 
vacations. Polysemy is not the only culprit: there is a (maybe
apocryphal) story about a lurid news item about a headless
body found in a suitcase triggering an ad for Samsonite 
luggage! In all these examples the mismatch arises from the
fact that the ads are not appropriate for the context.
In order to solve this problem we propose a matching
mechanism that combines a semantic phase with the 
traditional keyword matching, that is, a syntactic phase. The
semantic phase classifies the page and the ads into a 
taxonomy of topics and uses the proximity of the ad and page
classes as a factor in the ad ranking formula. Hence we 
favor ads that are topically related to the page and thus avoid
the pitfalls of the purely syntactic approach. Furthermore,
by using a hierarchical taxonomy we allow for the gradual
generalization of the ad search space in the case when there
are no ads matching the precise topic of the page. For 
example if the page is about an event in curling, a rare winter
sport, and contains the words Alpine Meadows, the 
system would still rank highly ads for skiing in Alpine Meadows
as these ads belong to the class skiing which is a sibling
of the class curling and both of these classes share the
parent winter sports.
In some sense, the taxonomy classes are used to select the
set of applicable ads and the keywords are used to narrow
down the search to concepts that are of too small 
granularity to be in the taxonomy. The taxonomy contains nodes for
topics that do not change fast, for example, brands of digital
cameras, say Canon. The keywords capture the specificity
to a level that is more dynamic and granular. In the 
digital camera example this would correspond to the level of a
particular model, say Canon SD450 whose advertising life
might be just a few months. Updating the taxonomy with
new nodes or even new vocabulary each time a new model
comes to the market is prohibitively expensive when we are
dealing with millions of manufacturers.
In addition to increased click through rate (CTR) due to
increased relevance, a significant but harder to quantify 
benefit of the semantic-syntactic matching is that the resulting
page has a unified feel and improves the user experience. In
the Chevy Tahoe example above, the classifier would 
establish that the page is about cars/automotive and only those
ads will be considered. Even if there are no ads for this 
particular Chevy model, the chosen ads will still be within the
automotive domain.
To implement our approach we need to solve a challenging
problem: classify both pages and ads within a large 
taxonomy (so that the topic granularity would be small enough)
with high precision (to reduce the probability of mis-match).
We evaluated several classifiers and taxonomies and in this
paper we present results using a taxonomy with close to
6000 nodes using a variation of the Rocchio"s classifier [9].
This classifier gave the best results in both page and ad
classification, and ultimately in ad relevance.
The paper proceeds as follows. In the next section we
review the basic principles of the contextual advertising.
Section 3 overviews the related work. Section 4 describes
the taxonomy and document classifier that were used for
page and ad classification. Section 5 describes the 
semanticsyntactic method. In Section 6 we briefly discuss how to
search efficiently the ad space in order to return the top-k
ranked ads. Experimental evaluation is presented in 
Section 7. Finally, Section 8 presents the concluding remarks.
2. OVERVIEW OF CONTEXTUAL
ADVERTISING
Contextual advertising is an interplay of four players:
• The publisher is the owner of the web pages on which
the advertising is displayed. The publisher typically
aims to maximize advertising revenue while providing
a good user experience.
• The advertiser provides the supply of ads. Usually
the activity of the advertisers are organized around
campaigns which are defined by a set of ads with a 
particular temporal and thematic goal (e.g. sale of digital
cameras during the holiday season). As in traditional
advertising, the goal of the advertisers can be broadly
defined as the promotion of products or services.
• The ad network is a mediator between the advertiser
and the publisher and selects the ads that are put on
the pages. The ad-network shares the advertisement
revenue with the publisher.
• Users visit the web pages of the publisher and interact
with the ads.
Contextual advertising usually falls into the category of 
direct marketing (as opposed to brand advertising), that is
advertising whose aim is a direct response where the 
effect of an campaign is measured by the user reaction. One
of the advantages of online advertising in general and 
contextual advertising in particular is that, compared to the
traditional media, it is relatively easy to measure the user
response. Usually the desired immediate reaction is for the
user to follow the link in the ad and visit the advertiser"s
web site and, as noted, the prevalent financial model is that
the advertiser pays a certain amount for every click on the
advertisement (PPC). The revenue is shared between the
publisher and the network.
Context match advertising has grown from Sponsored Search
advertising, which consists in placing ads on the result pages
from a web search engine, with ads driven by the originating
query. In most networks, the amount paid by the advertiser
for each SS click is determined by an auction process where
the advertisers place bids on a search phrase, and their 
position in the tower of ads displayed in conjunction with the
result is determined by their bid. Thus each ad is 
annotated with one or more bid phrases. The bid phrase has no
direct bearing on the ad placement in CM. However, it is a
concise description of target ad audience as determined by
the advertiser and it has been shown to be an important
feature for successful CM ad placement [8]. In addition to
the bid phrase, an ad is also characterized by a title usually
displayed in a bold font, and an abstract or creative, which
is the few lines of text, usually less than 120 characters,
displayed on the page.
The ad-network model aligns the interests of the 
publishers, advertisers and the network. In general, clicks bring
benefits to both the publisher and the ad network by 
providing revenue, and to the advertiser by bringing traffic to
the target web site. The revenue of the network, given a
page p, can be estimated as:
R =
X
i=1..k
P(click|p, ai)price(ai, i)
where k is the number of ads displayed on page p and price(ai, i)
is the click-price of the current ad ai at position i. The
price in this model depends on the set of ads presented on
the page. Several models have been proposed to determine
the price, most of them based on generalizations of second
price auctions. However, for simplicity we ignore the pricing
model and concentrate on finding ads that will maximize the
first term of the product, that is we search for
arg max
i
P(click|p, ai)
Furthermore we assume that the probability of click for a
given ad and page is determined by its relevance score with
respect to the page, thus ignoring the positional effect of
the ad placement on the page. We assume that this is an
orthogonal factor to the relevance component and could be
easily incorporated in the model.
3. RELATED WORK
Online advertising in general and contextual advertising
in particular are emerging areas of research. The published
literature is very sparse. A study presented in [13] confirms
the intuition that ads need to be relevant to the user"s 
interest to avoid degrading the user"s experience and increase
the probability of reaction.
A recent work by Ribeiro-Neto et. al. [8] examines a 
number of strategies to match pages to ads based on extracted
keywords. The ads and pages are represented as vectors in
a vector space. The first five strategies proposed in that
work match the pages and the ads based on the cosine of
the angle between the ad vector and the page vector. To
find out the important part of the ad, the authors explore
using different ad sections (bid phrase, title, body) as a 
basis for the ad vector. The winning strategy out of the first
five requires the bid phrase to appear on the page and then
ranks all such ads by the cosine of the union of all the ad
sections and the page vectors.
While both pages and ads are mapped to the same space,
there is a discrepancy (impendence mismatch) between the
vocabulary used in the ads and in the pages. Furthermore,
since in the vector model the dimensions are determined
by the number of unique words, plain cosine similarity will
not take into account synonyms. To solve this problem,
Ribeiro-Neto et al expand the page vocabulary with terms
from other similar pages weighted based on the overall 
similarity of the origin page to the matched page, and show
improved matching precision.
In a follow-up work [7] the authors propose a method to
learn impact of individual features using genetic 
programming to produce a matching function. The function is 
represented as a tree composed of arithmetic operators and the log
function as internal nodes, and different numerical features
of the query and ad terms as leafs. The results show that
genetic programming finds matching functions that 
significantly improve the matching compared to the best method
(without page side expansion) reported in [8].
Another approach to contextual advertising is to reduce it
to the problem of sponsored search advertising by 
extracting phrases from the page and matching them with the bid
phrase of the ads. In [14] a system for phrase extraction is
described that used a variety of features to determine the
importance of page phrases for advertising purposes. The
system is trained with pages that have been hand 
annotated with important phrases. The learning algorithm takes
into account features based on tf-idf, html meta data and
query logs to detect the most important phrases. During
evaluation, each page phrase up to length 5 is considered
as potential result and evaluated against a trained classifier.
In our work we also experimented with a phrase extractor
based on the work reported in [12]. While increasing slightly
the precision, it did not change the relative performance of
the explored algorithms.
4. PAGE AND AD CLASSIFICATION
4.1 Taxonomy Choice
The semantic match of the pages and the ads is performed
by classifying both into a common taxonomy. The 
matching process requires that the taxonomy provides sufficient
differentiation between the common commercial topics. For
example, classifying all medical related pages into one node
will not result into a good classification since both sore
foot and flu pages will end up in the same node. The
ads suitable for these two concepts are, however, very 
different. To obtain sufficient resolution, we used a taxonomy of
around 6000 nodes primarily built for classifying commercial
interest queries, rather than pages or ads. This taxonomy
has been commercially built by Yahoo! US. We will explain
below how we can use the same taxonomy to classify pages
and ads as well.
Each node in our source taxonomy is represented as a 
collection of exemplary bid phrases or queries that correspond
to that node concept. Each node has on average around 100
queries. The queries placed in the taxonomy are high 
volume queries and queries of high interest to advertisers, as
indicated by an unusually high cost-per-click (CPC) price.
The taxonomy has been populated by human editors 
using keyword suggestions tools similar to the ones used by
ad networks to suggest keywords to advertisers. After 
initial seeding with a few queries, using the provided tools a
human editor can add several hundreds queries to a given
node. Nevertheless, it has been a significant effort to 
develop this 6000-nodes taxonomy and it has required several
person-years of work. A similar-in-spirit process for 
building enterprise taxonomies via queries has been presented in
[6]. However, the details and tools are completely different.
Figure 1 provides some statistics about the taxonomy used
in this work.
4.2 Classification Method
As explained, the semantic phase of the matching relies
on ads and pages being topically close. Thus we need to
classify pages into the same taxonomy used to classify ads.
In this section we overview the methods we used to build a
page and an ad classifier pair. The detailed description and
evaluation of this process is outside the scope of this paper.
Given the taxonomy of queries (or bid-phrases - we use
these terms interchangeably) described in the previous 
section, we tried three methods to build corresponding page
and ad classifiers. For the first two methods we tried to
find exemplary pages and ads for each concept as follows:
Number of Categories By Level
0
200
400
600
800
1000
1200
1400
1600
1800
2000
1 2 3 4 5 6 7 8 9
Level
NumberofCategories
Number of Children per Nodes
0
50
100
150
200
250
300
350
400
0
2
4
6
8
10
12
14
16
18
20
22
24
29
31
33
35
52
Number of Children
NumberofNodes
Queries per Node
0
500
1000
1500
2000
2500
3000
1
50
80
120
160
200
240
280
320
360
400
440
480
Number Queries (up to 500+)
NumberofNodes
Figure 1: Taxonomy statistics: categories per level; fanout for non-leaf nodes; and queries per node
We generated a page training set by running the queries in
the taxonomy over a Web search index and using the top
10 results after some filtering as documents labeled with the
query"s label. On the ad side we generated a training set
for each class by selecting the ads that have a bid phrase 
assigned to this class. Using this training sets we then trained
a hierarchical SVM [2] (one against all between every group
of siblings) and a log-regression [11] classifier. (The 
second method differs from the first in the type of secondary
filtering used. This filtering eliminates low content pages,
pages deemed unsuitable for advertising, pages that lead to
excessive class confusion, etc.)
However, we obtained the best performance by using the
third document classifier, based on the information in the
source taxonomy queries only. For each taxonomy node we
concatenated all the exemplary queries into a single 
metadocument. We then used the meta document as a centroid
for a nearest-neighbor classifier based on Rocchio"s 
framework [9] with only positive examples and no relevance 
feedback. Each centroid is defined as a sum of the tf-idf values
of each term, normalized by the number of queries in the
class
cj =
1
|Cj|
X
q∈Cj
q
q
where cj is the centroid for class Cj; q iterates over the
queries in a particular class.
The classification is based on the cosine of the angle 
between the document d and the centroid meta-documents:
Cmax = arg max
Cj ∈C
cj
cj
·
d
d
= arg max
Cj ∈C
P
i∈|F | ci
j· di
qP
i∈|F |(ci
j)2
qP
i∈|F |(di)2
where F is the set of features. The score is normalized by
the document and class length to produce comparable score.
The terms ci
and di
represent the weight of the ith feature
in the class centroid and the document respectively. These
weights are based on the standard tf-idf formula. As the
score of the max class is normalized with regard to document
length, the scores for different documents are comparable.
We conducted tests using professional editors to judge the
quality of page and ad class assignments. The tests showed
that for both ads and pages the Rocchio classifier returned
the best results, especially on the page side. This is 
probably a result of the noise induced by using a search engine
to machine generate training pages for the SVM and 
logregression classifiers. It is an area of current investigation
how to improve the classification using a noisy training set.
Based on the test results we decided to use the Rocchio"s
classifier on both the ad and the page side.
5. SEMANTIC-SYNTACTIC MATCHING
Contextual advertising systems process the content of the
page, extract features, and then search the ad space to find
the best matching ads. Given a page p and a set of ads
A = {a1 . . . as} we estimate the relative probability of click
P(click|p, a) with a score that captures the quality of the
match between the page and the ad. To find the best ads
for a page we rank the ads in A and select the top few for
display. The problem can be formally defined as matching
every page in the set of all pages P = {p1, . . . ppc} to one or
more ads in the set of ads. Each page is represented as a
set of page sections pi = {pi,1, pi,2 . . . pi,m}. The sections of
the page represent different structural parts, such as title,
metadata, body, headings, etc. In turn, each section is an
unordered bag of terms (keywords). A page is represented
by the union of the terms in each section:
pi = {pws1
1 , pws1
2 . . . pwsi
m}
where pw stands for a page word and the superscript 
indicates the section of each term. A term can be a unigram or
a phrase extracted by a phrase extractor [12].
Similarly, we represent each ad as a set of sections a =
{a1, a2, . . . al}, each section in turn being an unordered set
of terms:
ai = {aws1
1 , aws1
2 . . . awsj
l }
where aw is an ad word. The ads in our experiments have
3 sections: title, body, and bid phrase. In this work, to
produce the match score we use only the ad/page textual
information, leaving user information and other data for 
future work.
Next, each page and ad term is associated with a weight
based on the tf-idf values. The tf value is determined based
on the individual ad sections. There are several choices for
the value of idf, based on different scopes. On the ad side,
it has been shown in previous work that the set of ads of
one campaign provide good scope for the estimation of idf
that leads to improved matching results [8]. However, in this
work for simplicity we do not take into account campaigns.
To combine the impact of the term"s section and its tf-idf
score, the ad/page term weight is defined as:
tWeight(kwsi
) = weightSection(Si) · tf idf(kw)
where tWeight stands for term weight and weightSection(Si)
is the weight assigned to a page or ad section. This weight
is a fixed parameter determined by experimentation.
Each ad and page is classified into the topical taxonomy.
We define these two mappings:
Tax(pi) = {pci1, . . . pciu}
Tax(aj) = {acj1 . . . acjv}
where pc and ac are page and ad classes correspondingly.
Each assignment is associated with a weight given by the
classifier. The weights are normalized to sum to 1:
X
c∈T ax(xi)
cWeight(c) = 1
where xi is either a page or an ad, and cWeights(c) is the
class weight - normalized confidence assigned by the 
classifier. The number of classes can vary between different pages
and ads. This corresponds to the number of topics a page/ad
can be associated with and is almost always in the range 1-4.
Now we define the relevance score of an ad ai and page
pi as a convex combination of the keyword (syntactic) and
classification (semantic) score:
Score(pi, ai) = α · TaxScore(Tax(pi), Tax(ai))
+(1 − α) · KeywordScore(pi, ai)
The parameter α determines the relative weight of the 
taxonomy score and the keyword score.
To calculate the keyword score we use the vector space
model [1] where both the pages and ads are represented
in n-dimensional space - one dimension for each distinct
term. The magnitude of each dimension is determined by
the tWeight() formula. The keyword score is then defined as
the cosine of the angle between the page and the ad vectors:
KeywordScore(pi, ai)
=
P
i∈|K| tWeight(pwi)· tWeight(awi)
qP
i∈|K|(tWeight(pwi))2
qP
i∈|K|(tWeight(awi))2
where K is the set of all the keywords. The formula 
assumes independence between the words in the pages and
ads. Furthermore, it ignores the order and the proximity of
the terms in the scoring. We experimented with the impact
of phrases and proximity on the keyword score and did not
see a substantial impact of these two factors.
We now turn to the definition of the TaxScore. This
function indicates the topical match between a given ad and
a page. As opposed to the keywords that are treated as
independent dimensions, here the classes (topics) are 
organized into a hierarchy. One of the goals in the design of
the TaxScore function is to be able to generalize within the
taxonomy, that is accept topically related ads. 
Generalization can help to place ads in cases when there is no ad that
matches both the category and the keywords of the page.
The example in Figure 2 illustrates this situation. In this
example, in the absence of ski ads, a page about skiing 
containing the word Atomic could be matched to the available
snowboarding ad for the same brand.
In general we would like the match to be stronger when
both the ad and the page are classified into the same node
Figure 2: Two generalization paths
and weaker when the distance between the nodes in the 
taxonomy gets larger. There are multiple ways to specify the
distance between two taxonomy nodes. Besides the above
requirement, this function should lend itself to an efficient
search of the ad space. Given a page we have to find the
ad in a few milliseconds, as this impacts the presentation to
a waiting user. This will be further discussed in the next
section.
To capture both the generalization and efficiency 
requirements we define the TaxScore function as follows:
TaxScore(PC, AC) =
X
pc∈P C
X
ac∈AC
idist(LCA(pc, ac), ac)·cWeight(pc)·cWeight(ac)
In this function we consider every combination of page class
and ad class. For each combination we multiply the product
of the class weights with the inverse distance function 
between the least common ancestor of the two classes (LCA)
and the ad class. The inverse distance function idist(c1, c2)
takes two nodes on the same path in the class taxonomy
and returns a number in the interval [0, 1] depending on the
distance of the two class nodes. It returns 1 if the two nodes
are the same, and declines toward 0 when LCA(pc, ac) or ac
is the root of the taxonomy. The rate of decline determines
the weight of the generalization versus the other terms in
the scoring formula.
To determine the rate of decline we consider the impact
on the specificity of the match when we substitute a class
with one of its ancestors. In general the impact is topic
dependent. For example the node Hobby in our taxonomy
has tens of children, each representing a different hobby, two
examples being Sailing and Knitting. Placing an ad
about Knitting on a page about Sailing does not make
lots of sense. However, in the Winter Sports example
above, in the absence of better alternative, skiing ads could
be put on snowboarding pages as they might promote the
same venues, equipment vendors etc. Such detailed analysis
on a case by case basis is prohibitively expensive due to the
size of the taxonomy.
One option is to use the confidences of the ancestor classes
as given by the classifier. However we found these 
numbers not suitable for this purpose as the magnitude of the
confidences does not necessarily decrease when going up the
tree. Another option is to use explore-exploit methods based
on machine-learning from the click feedback as described
in [10]. However for simplicity, in this work we chose a 
simple heuristic to determine the cost of generalization from a
child to a parent. In this heuristic we look at the 
broadening of the scope when moving from a child to a parent. We
estimate the broadening by the density of ads classified in
the parent nodes vs the child node. The density is obtained
by classifying a large set of ads in the taxonomy using the
document classifier described above. Based on this, let nc
be the number of document classified into the subtree rooted
at c. Then we define:
idist(c, p) =
nc
np
where c represents the child node and p is the parent node.
This fraction can be viewed as a probability of an ad 
belonging to the parent topic being suitable for the child topic.
6. SEARCHING THE AD SPACE
In the previous section we discussed the choice of scoring
function to estimate the match between an ad and a page.
The top-k ads with the highest score are offered by the 
system for placement on the publisher"s page. The process of
score calculation and ad selection is to be done in real time
and therefore must be very efficient. As the ad collections
are in the range of hundreds of millions of entries, there is a
need for indexed access to the ads.
Inverted indexes provide scalable and low latency 
solutions for searching documents. However, these have been
traditionally used to search based on keywords. To be able
to search the ads on a combination of keywords and classes
we have mapped the classification match to term match and
adapted the scoring function to be suitable for fast 
evaluation over inverted indexes. In this section we overview the
ad indexing and the ranking function of our prototype ad
search system for matching ads and pages.
We used a standard inverted index framework where there
is one posting list for each distinct term. The ads are parsed
into terms and each term is associated with a weight based
on the section in which it appears. Weights from distinct
occurrences of a term in an ad are added together, so that
the posting lists contain one entry per term/ad combination.
The next challenge is how to index the ads so that the class
information is preserved in the index? A simple method is to
create unique meta-terms for the classes and annotate each
ad with one meta-term for each assigned class. However
this method does not allow for generalization, since only the
ads matching an exact label of the page would be selected.
Therefore we annotated each ad with one meta-term for each
ancestor of the assigned class. The weights of meta-terms
are assigned according to the value of the idist() function
defined in the previous section. On the query side, given the
keywords and the class of a page, we compose a keyword only
query by inserting one class term for each ancestor of the
classes assigned to the page.
The scoring function is adapted to the two part 
scoreone for the class meta-terms and another for the text term.
During the class score calculation, for each class path we use
only the lowest class meta-term, ignoring the others. For
example, if an ad belongs to the Skiing class and is 
annotated with both Skiing and its parent Winter Sports,
the index will contain the special class meta-terms for both
Skiing and Winter Sports (and all their ancestors) with
the weights according to the product of the classifier 
confidence and the idist function. When matching with a page
classified into Skiing, the query will contain terms for class
Skiing and for each of its ancestors. However when scoring
an ad classified into Skiing we will use the weight for the
Skiing class meta-term. Ads classified into 
Snowboarding will be scored using the weight of the Winter Sports
meta-term. To make this check efficiently we keep a sorted
list of all the class paths and, at scoring time, we search the
paths bottom up for a meta-term appearing in the ad. The
first meta-term is used for scoring, the rest are ignored.
At runtime, we evaluate the query using a variant of the
WAND algorithm [3]. This is a document-at-a-time 
algorithm [1] that uses a branch-and-bound approach to derive
efficient moves for the cursors associated to the postings
lists. It finds the next cursor to be moved based on an 
upper bound of the score for the documents at which the 
cursors are currently positioned. The algorithm keeps a heap of
current best candidates. Documents with an upper bound
smaller than the current minimum score among the 
candidate documents can be eliminated from further 
considerations, and thus the cursors can skip over them. To find the
upper bound for a document, the algorithm assumes that all
cursors that are before it will hit this document (i.e. the 
document contains all those terms represented by cursors before
or at that document). It has been shown that WAND can
be used with any function that is monotonic with respect to
the number of matching terms in the document.
Our scoring function is monotonic since the score can
never decrease when more terms are found in the document.
In the special case when we add a cursor representing an
ancestor of a class term already factored in the score, the
score simply does not change (we add 0). Given these 
properties, we use an adaptation of the WAND algorithm where
we change the calculation of the scoring function and the 
upper bound score calculation to reflect our scoring function.
The rest of the algorithm remains unchanged.
7. EXPERIMENTAL EVALUATION
7.1 Data and Methodology
We quantify the effect of the semantic-syntactic matching
using a set of 105 pages. This set of pages was selected
by a random sample of a larger set of around 20 million
pages with contextual advertising. Ads for each of these
pages have been selected from a larger pool of ads (tens of
millions) by previous experiments conducted by Yahoo! US
for other purposes. Each page-ad pair has been judged by
three or more human judges on a 1 to 3 scale:
1. Relevant The ad is semantically directly related to
the main subject of the page. For example if the page
is about the National Football League and the ad is
about tickets for NFL games, it would be scored as 1.
2. Somewhat relevant The ad is related to the 
secondary subject of the page, or is related to the main
topic of the page in a general way. In the NFL page
example, an ad about NFL branded products would
be judged as 2.
3. Irrelevant The ad is unrelated to the page. For 
example a mention of the NFL player John Maytag triggers
washing machine ads on a NFL page.
pages 105
words per page 868
judgments 2946
judg. inter-editor agreement 84%
unique ads 2680
unique ads per page 25.5
page classification precision 70%
ad classification precision 86%
Table 1: Dataset statistics
To obtain a score for a page-ad pair we average all the scores
and then round to the closest integer. We then used these
judgments to evaluate how well our methods distinguish the
positive and the negative ad assignments for each page. The
statistics of the page dataset is given in Table 1.
The original experiments that paired the pages and the
ads are loosely related to the syntactic keyword based 
matching and classification based assignment but used different
taxonomies and keyword extraction techniques. Therefore
we could not use standard pooling as an evaluation method
since we did not control the way the pairs were selected and
could not precisely establish the set of ads from which the
placed ads were selected.
Instead, in our evaluation for each page we consider only
those ads for which we have judgment. Each different method
was applied to this set and the ads were ranked by the score.
The relative effectiveness of the algorithms were judged by
comparing how well the methods separated the ads with
positive judgment from the ads with negative judgment. We
present precision on various levels of recall within this set.
As the set of ads per page is relatively small, the evaluation
reports precision that is higher than it would be with a larger
set of negative ads. However, these numbers still establish
the relative performance of the algorithms and we can use
it to evaluate performance at different score thresholds.
In addition to the precision-recall over the judged ads,
we also present Kendall"s τ rank correlation coefficient to
establish how far from the perfect ordering are the orderings
produced by our ranking algorithms. For this test we ranked
the judged ads by the scores assigned by the judges and then
compared this order to the order assigned by our algorithms.
Finally we also examined the precision at position 1, 3 and
5.
7.2 Results
Figure 3 shows the precision recall curves for the 
syntactic matching (keywords only used) vs. a syntactic-semantic
matching with the optimal value of α = 0.8 (judged by the
11-point score [1]). In this figure, we assume that the 
adpage pairs judged with 1 or 2 are positive examples and the
3s are negative examples. We also examined counting only
the pairs judged with 1 as positive examples and did not
find a significant change in the relative performance of the
tested methods. Overlaid are also results using phrases in
the keyword match. We see that these additional features
do not change the relative performance of the algorithm.
The graphs show significant impact of the class 
information, especially in the mid range of recall values. In the
low recall part of the chart the curves meet. This indicates
that when the keyword match is really strong (i.e. when
the ad is almost contained within the page) the precision
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Recall
Precision
Alpha=.9, no phrase Alpha=0, no phrase
Alpha=0, phrase Alpha=.9, phrase
Figure 3: Data Set 2: Precision vs. Recall of 
syntactic match (α = 0) vs. syntactic-semantic match
(α = 0.8)
α Kendall"s τ
α = 0 0.086
α = 0.25 0.155
α = 0.50 0.166
α = 0.75 0.158
α = 1 0.136
Table 2: Kendall"s τ for different α values
of the syntactic keyword match is comparable with that of
the semantic-syntactic match. Note however that the two
methods might produce different ads and could be used as
a complement at level of recall.
The semantic components provides largest lift in 
precision at the mid range of recall where 25% improvement is
achieved by using the class information for ad placement.
This means that when there is somewhat of a match 
between the ad and the page, the restriction to the right classes
provides a better scope for selecting the ads.
Table 2 shows the Kendall"s τ values for different values of
α. We calculated the values by ranking all the judged ads for
each page and averaging the values over all the pages. The
ads with tied judgment were given the rank of the middle
position. The results show that when we take into account
all the ad-page pairs, the optimal value of α is around 0.5.
Note that purely syntactic match (α = 0) is by far the 
weakest method.
Figure 4 shows the effect of the parameter α in the scoring.
We see that in most cases precision grows or is flat when we
increase α, except at the low level of recall where due to
small number of data points there is a bit of jitter in the
results.
Table 3 shows the precision at positions 1, 3 and 5. Again,
the purely syntactic method has clearly the lowest score by
individual positions and the total number of correctly placed
ads. The numbers are close due to the small number of the
ads considered, but there are still some noticeable trends.
For position 1 the optimal α is in the range of 0.25 to 0.75.
For positions 3 and 5 the optimum is at α = 1. This also
indicates that for those ads that have a very high keyword
score, the semantic information is somewhat less important.
If almost all the words in an ad appear in the page, this ad is
Precision Vs Alpha for Different Levels of Recall
(Data Set 2)
0.45
0.55
0.65
0.75
0.85
0.95
0 0.2 0.4 0.6 0.8 1
Alpha
Precision
80% Recall 60% Recall 40% Recall 20% Recall
Figure 4: Impact of α on precision for different levels
of recall
α #1 #3 #5 sum
α = 0 80 70 68 218
α = 0.25 89 76 73 238
α = 0.5 89 74 73 236
α = 0.75 89 78 73 240
α = 1 86 79 74 239
Table 3: Precision at position 1, 3 and 5
likely to be relevant for this page. However when there is no
such clear affinity, the class information becomes a dominant
factor.
8. CONCLUSION
Contextual advertising is the economic engine behind a
large number of non-transactional sites on the Web. Studies
have shown that one of the main success factors for 
contextual ads is their relevance to the surrounding content. All
existing commercial contextual match solutions known to us
evolved from search advertising solutions whereby a search
query is matched to the bid phrase of the ads. A natural
extension of search advertising is to extract phrases from the
page and match them to the bid phrase of the ads. However,
individual phrases and words might have multiple meanings
and/or be unrelated to the overall topic of the page leading
to miss-matched ads.
In this paper we proposed a novel way of matching 
advertisements to web pages that rely on a topical (semantic)
match as a major component of the relevance score. The
semantic match relies on the classification of pages and ads
into a 6000 nodes commercial advertising taxonomy to 
determine their topical distance. As the classification relies
on the full content of the page, it is more robust than 
individual page phrases. The semantic match is complemented
with a syntactic match and the final score is a convex 
combination of the two sub-scores with the relative weight of
each determined by a parameter α.
We evaluated the semantic-syntactic approach against a
syntactic approach over a set of pages with different 
contextual advertising. As shown in our experimental evaluation,
the optimal value of the parameter α depends on the precise
objective of optimization (precision at particular position,
precision at given recall). However in all cases the optimal
value of α is between 0.25 and 0.9 indicating significant effect
of the semantic score component. The effectiveness of the
syntactic match depends on the quality of the pages used. In
lower quality pages we are more likely to make classification
errors that will then negatively impact the matching. We
demonstrated that it is feasible to build a large scale 
classifier that has sufficient good precision for this application.
We are currently examining how to employ machine 
learning algorithms to learn the optimal value of α based on a
collection of features of the input pages.
9. REFERENCES
[1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information
Retrieval. ACM, 1999.
[2] Bernhard E. Boser, Isabelle Guyon, and Vladimir Vapnik.
A training algorithm for optimal margin classifiers. In
Computational Learning Theory, pages 144-152, 1992.
[3] A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and
J. Zien. Efficient query evaluation using a two-level
retrieval process. In CIKM "03: Proc. of the twelfth intl.
conf. on Information and knowledge management, pages
426-434, New York, NY, 2003. ACM.
[4] P. Chatterjee, D. L. Hoffman, and T. P. Novak. Modeling
the clickstream: Implications for web-based advertising
efforts. Marketing Science, 22(4):520-541, 2003.
[5] D. Fain and J. Pedersen. Sponsored search: A brief history.
In In Proc. of the Second Workshop on Sponsored Search
Auctions, 2006. Web publication, 2006.
[6] S. C. Gates, W. Teiken, and K.-Shin F. Cheng. Taxonomies
by the numbers: building high-performance taxonomies. In
CIKM "05: Proc. of the 14th ACM intl. conf. on
Information and knowledge management, pages 568-577,
New York, NY, 2005. ACM.
[7] A. Lacerda, M. Cristo, M. Andre; G., W. Fan, N. Ziviani,
and B. Ribeiro-Neto. Learning to advertise. In SIGIR "06:
Proc. of the 29th annual intl. ACM SIGIR conf., pages
549-556, New York, NY, 2006. ACM.
[8] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and E. S.
de Moura. Impedance coupling in content-targeted
advertising. In SIGIR "05: Proc. of the 28th annual intl.
ACM SIGIR conf., pages 496-503, New York, NY, 2005.
ACM.
[9] J. Rocchio. Relevance feedback in information retrieval. In
The SMART Retrieval System: Experiments in Automatic
Document Processing, pages 313-323. PrenticeHall, 1971.
[10] P. Sandeep, D. Agarwal, D. Chakrabarti, and V. Josifovski.
Bandits for taxonomies: A model-based approach. In In
Proc. of the SIAM intl. conf. on Data Mining, 2007.
[11] T. Santner and D. Duffy. The Statistical Analysis of
Discrete Data. Springer-Verlag, 1989.
[12] R. Stata, K. Bharat, and F. Maghoul. The term vector
database: fast access to indexing terms for web pages.
Computer Networks, 33(1-6):247-255, 2000.
[13] C. Wang, P. Zhang, R. Choi, and M. D. Eredita.
Understanding consumers attitude toward advertising. In
Eighth Americas conf. on Information System, pages
1143-1148, 2002.
[14] W. Yih, J. Goodman, and V. R. Carvalho. Finding
advertising keywords on web pages. In WWW "06: Proc. of
the 15th intl. conf. on World Wide Web, pages 213-222,
New York, NY, 2006. ACM.
Context Sensitive Stemming for Web Search
Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu
Yahoo! Inc.
701 First Avenue
Sunnyvale, California 94089
{fuchun, nawaaz, xinli, yumaol}@yahoo-inc.com
ABSTRACT
Traditionally, stemming has been applied to Information
Retrieval tasks by transforming words in documents to the
their root form before indexing, and applying a similar 
transformation to query terms. Although it increases recall, this
naive strategy does not work well for Web Search since it
lowers precision and requires a significant amount of 
additional computation.
In this paper, we propose a context sensitive stemming
method that addresses these two issues. Two unique 
properties make our approach feasible for Web Search. First, based
on statistical language modeling, we perform context 
sensitive analysis on the query side. We accurately predict which
of its morphological variants is useful to expand a query term
with before submitting the query to the search engine. This
dramatically reduces the number of bad expansions, which
in turn reduces the cost of additional computation and 
improves the precision at the same time. Second, our approach
performs a context sensitive document matching for those
expanded variants. This conservative strategy serves as a
safeguard against spurious stemming, and it turns out to be
very important for improving precision. Using word 
pluralization handling as an example of our stemming approach,
our experiments on a major Web search engine show that
stemming only 29% of the query traffic, we can improve
relevance as measured by average Discounted Cumulative
Gain (DCG5) by 6.1% on these queries and 1.8% over all
query traffic.
Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Storage and
Retrieval-Query formulation
General Terms
Algorithms, Experimentation
1. INTRODUCTION
Web search has now become a major tool in our daily lives
for information seeking. One of the important issues in Web
search is that user queries are often not best formulated to
get optimal results. For example, running shoe is a query
that occurs frequently in query logs. However, the query
running shoes is much more likely to give better search
results than the original query because documents matching
the intent of this query usually contain the words running
shoes.
Correctly formulating a query requires the user to 
accurately predict which word form is used in the documents
that best satisfy his or her information needs. This is 
difficult even for experienced users, and especially difficult for
non-native speakers. One traditional solution is to use 
stemming [16, 18], the process of transforming inflected or 
derived words to their root form so that a search term will
match and retrieve documents containing all forms of the
term. Thus, the word run will match running, ran,
runs, and shoe will match shoes and shoeing. 
Stemming can be done either on the terms in a document 
during indexing (and applying the same transformation to the
query terms during query processing) or by expanding the
query with the variants during query processing. Stemming
during indexing allows very little flexibility during query
processing, while stemming by query expansion allows 
handling each query differently, and hence is preferred.
Although traditional stemming increases recall by 
matching word variants [13], it can reduce precision by retrieving
too many documents that have been incorrectly matched.
When examining the results of applying stemming to a large
number of queries, one usually finds that nearly equal 
numbers of queries are helped and hurt by the technique [6]. In
addition, it reduces system performance because the search
engine has to match all the word variants. As we will show
in the experiments, this is true even if we simplify stemming
to pluralization handling, which is the process of converting
a word from its plural to singular form, or vice versa. Thus,
one needs to be very cautious when using stemming in Web
search engines.
One problem of traditional stemming is its blind 
transformation of all query terms, that is, it always performs
the same transformation for the same query word without
considering the context of the word. For example, the word
book has four forms book, books, booking, booked, and
store has four forms store, stores, storing, stored. For
the query book store, expanding both words to all of their
variants significantly increases computation cost and hurts
precision, since not all of the variants are useful for this
query. Transforming book store to match book stores
is fine, but matching book storing or booking store is
not. A weighting method that gives variant words smaller
weights alleviates the problems to a certain extent if the
weights accurately reflect the importance of the variant in
this particular query. However uniform weighting is not 
going to work and a query dependent weighting is still a 
challenging unsolved problem [20].
A second problem of traditional stemming is its blind
matching of all occurrences in documents. For the query
book store, a transformation that allows the variant stores
to be matched will cause every occurrence of stores in the
document to be treated equivalent to the query term store.
Thus, a document containing the fragment reading a book
in coffee stores will be matched, causing many wrong 
documents to be selected. Although we hope the ranking 
function can correctly handle these, with many more candidates
to rank, the risk of making mistakes increases.
To alleviate these two problems, we propose a context
sensitive stemming approach for Web search. Our solution
consists of two context sensitive analysis, one on the query
side and the other on the document side. On the query side,
we propose a statistical language modeling based approach
to predict which word variants are better forms than the
original word for search purpose and expanding the query
with only those forms. On the document side, we propose a
conservative context sensitive matching for the transformed
word variants, only matching document occurrences in the
context of other terms in the query. Our model is simple yet
effective and efficient, making it feasible to be used in real
commercial Web search engines.
We use pluralization handling as a running example for
our stemming approach. The motivation for using 
pluralization handling as an example is to show that even such
simple stemming, if handled correctly, can give significant
benefits to search relevance. As far as we know, no 
previous research has systematically investigated the usage of
pluralization in Web search. As we have to point out, the
method we propose is not limited to pluralization handling,
it is a general stemming technique, and can also be applied
to general query expansion. Experiments on general 
stemming yield additional significant improvements over 
pluralization handling for long queries, although details will not
be reported in this paper.
In the rest of the paper, we first present the related work
and distinguish our method from previous work in Section 2.
We describe the details of the context sensitive stemming
approach in Section 3. We then perform extensive 
experiments on a major Web search engine to support our claims
in Section 4, followed by discussions in Section 5. Finally,
we conclude the paper in Section 6.
2. RELATED WORK
Stemming is a long studied technology. Many stemmers
have been developed, such as the Lovins stemmer [16] and
the Porter stemmer [18]. The Porter stemmer is widely used
due to its simplicity and effectiveness in many applications.
However, the Porter stemming makes many mistakes 
because its simple rules cannot fully describe English 
morphology. Corpus analysis is used to improve Porter stemmer [26]
by creating equivalence classes for words that are 
morphologically similar and occur in similar context as measured by
expected mutual information [23]. We use a similar corpus
based approach for stemming by computing the similarity
between two words based on their distributional context 
features which can be more than just adjacent words [15], and
then only keep the morphologically similar words as 
candidates.
Using stemming in information retrieval is also a well
known technique [8, 10]. However, the effectiveness of 
stemming for English query systems was previously reported to
be rather limited. Lennon et al. [17] compared the Lovins
and Porter algorithms and found little improvement in 
retrieval performance. Later, Harman [9] compares three 
general stemming techniques in text retrieval experiments 
including pluralization handing (called S stemmer in the 
paper). They also proposed selective stemming based on query
length and term importance, but no positive results were
reported. On the other hand, Krovetz [14] performed 
comparisons over small numbers of documents (from 400 to 12k)
and showed dramatic precision improvement (up to 45%).
However, due to the limited number of tested queries (less
than 100) and the small size of the collection, the results
are hard to generalize to Web search. These mixed results,
mostly failures, led early IR researchers to deem stemming
irrelevant in general for English [4], although recent research
has shown stemming has greater benefits for retrieval in
other languages [2]. We suspect the previous failures were
mainly due to the two problems we mentioned in the 
introduction. Blind stemming, or a simple query length based
selective stemming as used in [9] is not enough. Stemming
has to be decided on case by case basis, not only at the query
level but also at the document level. As we will show, if 
handled correctly, significant improvement can be achieved.
A more general problem related to stemming is query
reformulation [3, 12] and query expansion which expands
words not only with word variants [7, 22, 24, 25]. To 
decide which expanded words to use, people often use 
pseudorelevance feedback techniquesthat send the original query to
a search engine and retrieve the top documents, extract 
relevant words from these top documents as additional query
words, and resubmit the expanded query again [21]. This
normally requires sending a query multiple times to search
engine and it is not cost effective for processing the huge
amount of queries involved in Web search. In addition,
query expansion, including query reformulation [3, 12], has
a high risk of changing the user intent (called query drift).
Since the expanded words may have different meanings, adding
them to the query could potentially change the intent of
the original query. Thus query expansion based on 
pseudorelevance and query reformulation can provide suggestions
to users for interactive refinement but can hardly be directly
used for Web search. On the other hand, stemming is much
more conservative since most of the time, stemming 
preserves the original search intent. While most work on query
expansion focuses on recall enhancement, our work focuses
on increasing both recall and precision. The increase on
recall is obvious. With quality stemming, good documents
which were not selected before stemming will be pushed up
and those low quality documents will be degraded.
On selective query expansion, Cronen-Townsend et al. [6]
proposed a method for selective query expansion based on
comparing the Kullback-Leibler divergence of the results
from the unexpanded query and the results from the 
expanded query. This is similar to the relevance feedback in
the sense that it requires multiple passes retrieval. If a word
can be expanded into several words, it requires running this
process multiple times to decide which expanded word is
useful. It is expensive to deploy this in production Web
search engines. Our method predicts the quality of 
expansion based on oﬄine information without sending the query
to a search engine.
In summary, we propose a novel approach to attack an old,
yet still important and challenging problem for Web search
- stemming. Our approach is unique in that it performs
predictive stemming on a per query basis without relevance
feedback from the Web, using the context of the variants in
documents to preserve precision. It"s simple, yet very 
efficient and effective, making real time stemming feasible for
Web search. Our results will affirm researchers that 
stemming is indeed very important to large scale information
retrieval.
3. CONTEXT SENSITIVE STEMMING
3.1 Overview
Our system has four components as illustrated in 
Figure 1: candidate generation, query segmentation and head
word detection, context sensitive query stemming and 
context sensitive document matching. Candidate generation
(component 1) is performed oﬄine and generated candidates
are stored in a dictionary. For an input query, we first 
segment query into concepts and detect the head word for each
concept (component 2). We then use statistical language
modeling to decide whether a particular variant is useful
(component 3), and finally for the expanded variants, we
perform context sensitive document matching (component
4). Below we discuss each of the components in more detail.
Component 4: context sensitive document matching
Input Query:
and head word detection
Component 2: segment Component 1: candidate generation
comparisons −> comparison
Component 3: selective word expansion
decision: comparisons −> comparison
example: hotel price comparisons
output: "hotel" "comparisons"
hotel −> hotels
Figure 1: System Components
3.2 Expansion candidate generation
One of the ways to generate candidates is using the Porter
stemmer [18]. The Porter stemmer simply uses 
morphological rules to convert a word to its base form. It has no 
knowledge of the semantic meaning of the words and sometimes
makes serious mistakes, such as executive to execution,
news to new, and paste to past. A more 
conservative way is based on using corpus analysis to improve the
Porter stemmer results [26]. The corpus analysis we do is
based on word distributional similarity [15]. The rationale
of using distributional word similarity is that true variants
tend to be used in similar contexts. In the distributional
word similarity calculation, each word is represented with a
vector of features derived from the context of the word. We
use the bigrams to the left and right of the word as its 
context features, by mining a huge Web corpus. The similarity
between two words is the cosine similarity between the two
corresponding feature vectors. The top 20 similar words to
develop is shown in the following table.
rank candidate score rank candidate score
0 develop 1 10 berts 0.119
1 developing 0.339 11 wads 0.116
2 developed 0.176 12 developer 0.107
3 incubator 0.160 13 promoting 0.100
4 develops 0.150 14 developmental 0.091
5 development 0.148 15 reengineering 0.090
6 tutoring 0.138 16 build 0.083
7 analyzing 0.128 17 construct 0.081
8 developement 0.128 18 educational 0.081
9 automation 0.126 19 institute 0.077
Table 1: Top 20 most similar candidates to word
develop. Column score is the similarity score.
To determine the stemming candidates, we apply a few
Porter stemmer [18] morphological rules to the similarity
list. After applying these rules, for the word develop,
the stemming candidates are developing, developed, 
develops, development, developement, developer, 
developmental. For the pluralization handling purpose, only the 
candidate develops is retained.
One thing we note from observing the distributionally
similar words is that they are closely related semantically.
These words might serve as candidates for general query
expansion, a topic we will investigate in the future.
3.3 Segmentation and headword identification
For long queries, it is quite important to detect the 
concepts in the query and the most important words for those
concepts. We first break a query into segments, each 
segment representing a concept which normally is a noun phrase.
For each of the noun phrases, we then detect the most 
important word which we call the head word. Segmentation
is also used in document sensitive matching (section 3.5) to
enforce proximity.
To break a query into segments, we have to define a 
criteria to measure the strength of the relation between words.
One effective method is to use mutual information as an 
indicator on whether or not to split two words [19]. We use
a log of 25M queries and collect the bigram and unigram
frequencies from it. For every incoming query, we compute
the mutual information of two adjacent words; if it passes
a predefined threshold, we do not split the query between
those two words and move on to next word. We continue
this process until the mutual information between two words
is below the threshold, then create a concept boundary here.
Table 2 shows some examples of query segmentation.
The ideal way of finding the head word of a concept is to
do syntactic parsing to determine the dependency structure
of the query. Query parsing is more difficult than sentence
[running shoe]
[best] [new york] [medical schools]
[pictures] [of] [white house]
[cookies] [in] [san francisco]
[hotel] [price comparison]
Table 2: Query segmentation: a segment is 
bracketed.
parsing since many queries are not grammatical and are very
short. Applying a parser trained on sentences from 
documents to queries will have poor performance. In our 
solution, we just use simple heuristics rules, and it works very
well in practice for English. For an English noun phrase,
the head word is typically the last nonstop word, unless the
phrase is of a particular pattern, like XYZ of/in/at/from
UVW. In such cases, the head word is typically the last
nonstop word of XYZ.
3.4 Context sensitive word expansion
After detecting which words are the most important words
to expand, we have to decide whether the expansions will
be useful.
Our statistics show that about half of the queries can be
transformed by pluralization via naive stemming. Among
this half, about 25% of the queries improve relevance when
transformed, the majority (about 50%) do not change their
top 5 results, and the remaining 25% perform worse. Thus,
it is extremely important to identify which queries should
not be stemmed for the purpose of maximizing relevance
improvement and minimizing stemming cost. In addition,
for a query with multiple words that can be transformed,
or a word with multiple variants, not all of the expansions
are useful. Taking query hotel price comparison as an 
example, we decide that hotel and price comparison are two
concepts. Head words hotel and comparison can be
expanded to hotels and comparisons. Are both 
transformations useful?
To test whether an expansion is useful, we have to know
whether the expanded query is likely to get more relevant
documents from the Web, which can be quantified by the
probability of the query occurring as a string on the Web.
The more likely a query to occur on the Web, the more
relevant documents this query is able to return. Now the
whole problem becomes how to calculate the probability of
query to occur on the Web.
Calculating the probability of string occurring in a 
corpus is a well known language modeling problem. The goal
of language modeling is to predict the probability of 
naturally occurring word sequences, s = w1w2...wN ; or more
simply, to put high probability on word sequences that 
actually occur (and low probability on word sequences that
never occur). The simplest and most successful approach to
language modeling is still based on the n-gram model. By
the chain rule of probability one can write the probability
of any word sequence as
Pr(w1w2...wN ) =
NY
i=1
Pr(wi|w1...wi−1) (1)
An n-gram model approximates this probability by 
assuming that the only words relevant to predicting Pr(wi|w1...wi−1)
are the previous n − 1 words; i.e.
Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1)
A straightforward maximum likelihood estimate of n-gram
probabilities from a corpus is given by the observed 
frequency of each of the patterns
Pr(wi|wi−n+1...wi−1) =
#(wi−n+1...wi)
#(wi−n+1...wi−1)
(2)
where #(.) denotes the number of occurrences of a specified
gram in the training corpus. Although one could attempt to
use simple n-gram models to capture long range 
dependencies in language, attempting to do so directly immediately
creates sparse data problems: Using grams of length up to
n entails estimating the probability of Wn
events, where W
is the size of the word vocabulary. This quickly overwhelms
modern computational and data resources for even modest
choices of n (beyond 3 to 6). Also, because of the heavy
tailed nature of language (i.e. Zipf"s law) one is likely to
encounter novel n-grams that were never witnessed during
training in any test corpus, and therefore some mechanism
for assigning non-zero probability to novel n-grams is a 
central and unavoidable issue in statistical language modeling.
One standard approach to smoothing probability estimates
to cope with sparse data problems (and to cope with 
potentially missing n-grams) is to use some sort of back-off
estimator.
Pr(wi|wi−n+1...wi−1)
=
8
>><
>>:
ˆPr(wi|wi−n+1...wi−1),
if #(wi−n+1...wi) > 0
β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1),
otherwise
(3)
where
ˆPr(wi|wi−n+1...wi−1) =
discount #(wi−n+1...wi)
#(wi−n+1...wi−1)
(4)
is the discounted probability and β(wi−n+1...wi−1) is a 
normalization constant
β(wi−n+1...wi−1) =
1 −
X
x∈(wi−n+1...wi−1x)
ˆPr(x|wi−n+1...wi−1)
1 −
X
x∈(wi−n+1...wi−1x)
ˆPr(x|wi−n+2...wi−1)
(5)
The discounted probability (4) can be computed with 
different smoothing techniques, including absolute smoothing,
Good-Turing smoothing, linear smoothing, and Witten-Bell
smoothing [5]. We used absolute smoothing in our 
experiments.
Since the likelihood of a string, Pr(w1w2...wN ), is a very
small number and hard to interpret, we use entropy as 
defined below to score the string.
Entropy = −
1
N
log2 Pr(w1w2...wN ) (6)
Now getting back to the example of the query hotel price
comparisons, there are four variants of this query, and the
entropy of these four candidates are shown in Table 3. We
can see that all alternatives are less likely than the input
query. It is therefore not useful to make an expansion for this
query. On the other hand, if the input query is hotel price
comparisons which is the second alternative in the table,
then there is a better alternative than the input query, and
it should therefore be expanded. To tolerate the variations
in probability estimation, we relax the selection criterion to
those query alternatives if their scores are within a certain
distance (10% in our experiments) to the best score.
Query variations Entropy
hotel price comparison 6.177
hotel price comparisons 6.597
hotels price comparison 6.937
hotels price comparisons 7.360
Table 3: Variations of query hotel price 
comparison ranked by entropy score, with the original
query in bold face.
3.5 Context sensitive document matching
Even after we know which word variants are likely to be
useful, we have to be conservative in document matching
for the expanded variants. For the query hotel price 
comparisons, we decided that word comparisons is expanded
to include comparison. However, not every occurrence of
comparison in the document is of interest. A page which
is about comparing customer service can contain all of the
words hotel price comparisons comparison. This page is not
a good page for the query.
If we accept matches of every occurrence of comparison,
it will hurt retrieval precision and this is one of the main
reasons why most stemming approaches do not work well
for information retrieval. To address this problem, we have
a proximity constraint that considers the context around
the expanded variant in the document. A variant match
is considered valid only if the variant occurs in the same
context as the original word does. The context is the left or
the right non-stop segments 1
of the original word. Taking
the same query as an example, the context of comparisons
is price. The expanded word comparison is only valid if
it is in the same context of comparisons, which is after the
word price. Thus, we should only match those occurrences
of comparison in the document if they occur after the word
price. Considering the fact that queries and documents
may not represent the intent in exactly the same way, we
relax this proximity constraint to allow variant occurrences
within a window of some fixed size. If the expanded word
comparison occurs within the context of price within
a window, it is considered valid. The smaller the window
size is, the more restrictive the matching. We use a window
size of 4, which typically captures contexts that include the
containing and adjacent noun phrases.
4. EXPERIMENTAL EVALUATION
4.1 Evaluation metrics
We will measure both relevance improvement and the
stemming cost required to achieve the relevance.
1
a context segment can not be a single stop word.
4.1.1 Relevance measurement
We use a variant of the average Discounted Cumulative
Gain (DCG), a recently popularized scheme to measure search
engine relevance [1, 11]. Given a query and a ranked list of K
documents (K is set to 5 in our experiments), the DCG(K)
score for this query is calculated as follows:
DCG(K) =
KX
k=1
gk
log2(1 + k)
. (7)
where gk is the weight for the document at rank k. Higher
degree of relevance corresponds to a higher weight. A page is
graded into one of the five scales: Perfect, Excellent, Good,
Fair, Bad, with corresponding weights. We use dcg to 
represent the average DCG(5) over a set of test queries.
4.1.2 Stemming cost
Another metric is to measure the additional cost incurred
by stemming. Given the same level of relevance 
improvement, we prefer a stemming method that has less additional
cost. We measure this by the percentage of queries that are
actually stemmed, over all the queries that could possibly
be stemmed.
4.2 Data preparation
We randomly sample 870 queries from a three month
query log, with 290 from each month. Among all these 870
queries, we remove all misspelled queries since misspelled
queries are not of interest to stemming. We also remove all
one word queries since stemming one word queries without
context has a high risk of changing query intent, especially
for short words. In the end, we have 529 correctly spelled
queries with at least 2 words.
4.3 Naive stemming for Web search
Before explaining the experiments and results in detail,
we"d like to describe the traditional way of using stemming
for Web search, referred as the naive model. This is to treat
every word variant equivalent for all possible words in the
query. The query book store will be transformed into
(book OR books)(store OR stores) when limiting stemming
to pluralization handling only, where OR is an operator that
denotes the equivalence of the left and right arguments.
4.4 Experimental setup
The baseline model is the model without stemming. We
first run the naive model to see how well it performs over
the baseline. Then we improve the naive stemming model
by document sensitive matching, referred as document 
sensitive matching model. This model makes the same stemming
as the naive model on the query side, but performs 
conservative matching on the document side using the strategy
described in section 3.5. The naive model and document
sensitive matching model stem the most queries. Out of the
529 queries, there are 408 queries that they stem, 
corresponding to 46.7% query traffic (out of a total of 870). We
then further improve the document sensitive matching model
from the query side with selective word stemming based on
statistical language modeling (section 3.4), referred as 
selective stemming model. Based on language modeling 
prediction, this model stems only a subset of the 408 queries
stemmed by the document sensitive matching model. We
experiment with unigram language model and bigram 
language model. Since we only care how much we can improve
the naive model, we will only use these 408 queries (all the
queries that are affected by the naive stemming model) in
the experiments.
To get a sense of how these models perform, we also have
an oracle model that gives the upper-bound performance a
stemmer can achieve on this data. The oracle model only
expands a word if the stemming will give better results.
To analyze the pluralization handling influence on 
different query categories, we divide queries into short queries
and long queries. Among the 408 queries stemmed by the
naive model, there are 272 short queries with 2 or 3 words,
and 136 long queries with at least 4 words.
4.5 Results
We summarize the overall results in Table 4, and present
the results on short queries and long queries separately in
Table 5. Each row in Table 4 is a stemming strategy 
described in section 4.4. The first column is the name of the
strategy. The second column is the number of queries 
affected by this strategy; this column measures the stemming
cost, and the numbers should be low for the same level of
dcg. The third column is the average dcg score over all
tested queries in this category (including the ones that were
not stemmed by the strategy). The fourth column is the
relative improvement over the baseline, and the last column
is the p-value of Wilcoxon significance test.
There are several observations about the results. We can
see the naively stemming only obtains a statistically 
insignificant improvement of 1.5%. Looking at Table 5, it gives an
improvement of 2.7% on short queries. However, it also
hurts long queries by -2.4%. Overall, the improvement is
canceled out. The reason that it improves short queries is
that most short queries only have one word that can be
stemmed. Thus, blindly pluralizing short queries is 
relatively safe. However for long queries, most queries can have
multiple words that can be pluralized. Expanding all of
them without selection will significantly hurt precision.
Document context sensitive stemming gives a significant
lift to the performance, from 2.7% to 4.2% for short queries
and from -2.4% to -1.6% for long queries, with an overall
lift from 1.5% to 2.8%. The improvement comes from the
conservative context sensitive document matching. An 
expanded word is valid only if it occurs within the context of
original query in the document. This reduces many spurious
matches. However, we still notice that for long queries, 
context sensitive stemming is not able to improve performance
because it still selects too many documents and gives the
ranking function a hard problem. While the chosen window
size of 4 works the best amongst all the choices, it still 
allows spurious matches. It is possible that the window size
needs to be chosen on a per query basis to ensure tighter
proximity constraints for different types of noun phrases.
Selective word pluralization further helps resolving the
problem faced by document context sensitive stemming. It
does not stem every word that places all the burden on the
ranking algorithm, but tries to eliminate unnecessary 
stemming in the first place. By predicting which word variants
are going to be useful, we can dramatically reduce the 
number of stemmed words, thus improving both the recall and
the precision. With the unigram language model, we can 
reduce the stemming cost by 26.7% (from 408/408 to 300/408)
and lift the overall dcg improvement from 2.8% to 3.4%. In
particular, it gives significant improvements on long queries.
The dcg gain is turned from negative to positive, from −1.6%
to 1.1%. This confirms our hypothesis that reducing 
unnecessary word expansion leads to precision improvement. For
short queries too, we observe both dcg improvement and
stemming cost reduction with the unigram language model.
The advantages of predictive word expansion with a 
language model is further boosted with a better bigram 
language model. The overall dcg gain is lifted from 3.4%
to 3.9%, and stemming cost is dramatically reduced from
408/408 to 250/408, corresponding to only 29% of query
traffic (250 out of 870) and an overall 1.8% dcg 
improvement overall all query traffic. For short queries, bigram 
language model improves the dcg gain from 4.4% to 4.7%,
and reduces stemming cost from 272/272 to 150/272. For
long queries, bigram language model improves dcg gain from
1.1% to 2.5%, and reduces stemming cost from 136/136 to
100/136. We observe that the bigram language model gives
a larger lift for long queries. This is because the uncertainty
in long queries is larger and a more powerful language model
is needed. We hypothesize that a trigram language model
would give a further lift for long queries and leave this for
future investigation.
Considering the tight upper-bound 2
on the improvement
to be gained from pluralization handling (via the oracle
model), the current performance on short queries is very 
satisfying. For short queries, the dcg gain upper-bound is 6.3%
for perfect pluralization handling, our current gain is 4.7%
with a bigram language model. For long queries, the dcg
gain upper-bound is 4.6% for perfect pluralization handling,
our current gain is 2.5% with a bigram language model. We
may gain additional benefit with a more powerful language
model for long queries. However, the difficulties of long
queries come from many other aspects including the 
proximity and the segmentation problem. These problems have
to be addressed separately. Looking at the the upper-bound
of overhead reduction for oracle stemming, 75% (308/408)
of the naive stemmings are wasteful. We currently capture
about half of them. Further reduction of the overhead 
requires sacrificing the dcg gain.
Now we can compare the stemming strategies from a 
different aspect. Instead of looking at the influence over all
queries as we described above, Table 6 summarizes the dcg
improvements over the affected queries only. We can see
that the number of affected queries decreases as the 
stemming strategy becomes more accurate (dcg improvement).
For the bigram language model, over the 250/408 stemmed
queries, the dcg improvement is 6.1%. An interesting 
observation is the average dcg decreases with a better model,
which indicates a better stemming strategy stems more 
difficult queries (low dcg queries).
5. DISCUSSIONS
5.1 Language models from query vs. from Web
As we mentioned in Section 1, we are trying to predict
the probability of a string occurring on the Web. The 
language model should describe the occurrence of the string on
the Web. However, the query log is also a good resource.
2
Note that this upperbound is for pluralization handling
only, not for general stemming. General stemming gives a
8% upperbound, which is quite substantial in terms of our
metrics.
Affected Queries dcg dcg Improvement p-value
baseline 0/408 7.102 N/A N/A
naive model 408/408 7.206 1.5% 0.22
document context sensitive model 408/408 7.302 2.8% 0.014
selective model: unigram LM 300/408 7.321 3.4% 0.001
selective model: bigram LM 250/408 7.381 3.9% 0.001
oracle model 100/408 7.519 5.9% 0.001
Table 4: Results comparison of different stemming strategies over all queries affected by naive stemming
Short Query Results
Affected Queries dcg Improvement p-value
baseline 0/272 N/A N/A
naive model 272/272 2.7% 0.48
document context sensitive model 272/272 4.2% 0.002
selective model: unigram LM 185/272 4.4% 0.001
selective model: bigram LM 150/272 4.7% 0.001
oracle model 71/272 6.3% 0.001
Long Query Results
Affected Queries dcg Improvement p-value
baseline 0/136 N/A N/A
naive model 136/136 -2.4% 0.25
document context sensitive model 136/136 -1.6% 0.27
selective model: unigram LM 115/136 1.1% 0.001
selective model: bigram LM 100/136 2.5% 0.001
oracle model 29/136 4.6% 0.001
Table 5: Results comparison of different stemming strategies overall short queries and long queries
Users reformulate a query using many different variants to
get good results.
To test the hypothesis that we can learn reliable 
transformation probabilities from the query log, we trained a 
language model from the same query top 25M queries as used
to learn segmentation, and use that for prediction. We 
observed a slight performance decrease compared to the model
trained on Web frequencies. In particular, the performance
for unigram LM was not affected, but the dcg gain for bigram
LM changed from 4.7% to 4.5% for short queries. Thus, the
query log can serve as a good approximation of the Web
frequencies.
5.2 How linguistics helps
Some linguistic knowledge is useful in stemming. For the
pluralization handling case, pluralization and de-pluralization
is not symmetric. A plural word used in a query indicates
a special intent. For example, the query new york hotels
is looking for a list of hotels in new york, not the specific
new york hotel which might be a hotel located in 
California. A simple equivalence of hotel to hotels might boost
a particular page about new york hotel to top rank. To
capture this intent, we have to make sure the document is a
general page about hotels in new york. We do this by 
requiring that the plural word hotels appears in the document.
On the other hand, converting a singular word to plural is
safer since a general purpose page normally contains 
specific information. We observed a slight overall dcg decrease,
although not statistically significant, for document context
sensitive stemming if we do not consider this asymmetric
property.
5.3 Error analysis
One type of mistakes we noticed, though rare but 
seriously hurting relevance, is the search intent change after
stemming. Generally speaking, pluralization or 
depluralization keeps the original intent. However, the intent could
change in a few cases. For one example of such a query,
job at apple, we pluralize job to jobs. This 
stemming makes the original query ambiguous. The query job
OR jobs at apple has two intents. One is the employment
opportunities at apple, and another is a person working at
Apple, Steve Jobs, who is the CEO and co-founder of the
company. Thus, the results after query stemming returns
Steve Jobs as one of the results in top 5. One solution is
performing results set based analysis to check if the intent is
changed. This is similar to relevance feedback and requires
second phase ranking.
A second type of mistakes is the entity/concept 
recognition problem, These include two kinds. One is that the
stemmed word variant now matches part of an entity or
concept. For example, query cookies in san francisco is
pluralized to cookies OR cookie in san francisco. The
results will match cookie jar in san francisco. Although
cookie still means the same thing as cookies, cookie
jar is a different concept. Another kind is the unstemmed
word matches an entity or concept because of the stemming
of the other words. For example, quote ICE is 
pluralized to quote OR quotes ICE. The original intent for this
query is searching for stock quote for ticker ICE. However,
we noticed that among the top results, one of the results
is Food quotes: Ice cream. This is matched because of
Affected Queries old dcg new dcg dcg Improvement
naive model 408/408 7.102 7.206 1.5%
document context sensitive model 408/408 7.102 7.302 2.8%
selective model: unigram LM 300/408 5.904 6.187 4.8%
selective model: bigram LM 250/408 5.551 5.891 6.1%
Table 6: Results comparison over the stemmed queries only: column old/new dcg is the dcg score over the
affected queries before/after applying stemming
the pluralized word quotes. The unchanged word ICE
matches part of the noun phrase ice cream here. To solve
this kind of problem, we have to analyze the documents and
recognize cookie jar and ice cream as concepts instead
of two independent words.
A third type of mistakes occurs in long queries. For the
query bar code reader software, two words are pluralized.
code to codes and reader to readers. In fact, bar
code reader in the original query is a strong concept and
the internal words should not be changed. This is the 
segmentation and entity and noun phrase detection problem in
queries, which we actively are attacking. For long queries,
we should correctly identify the concepts in the query, and
boost the proximity for the words within a concept.
6. CONCLUSIONS AND FUTURE WORK
We have presented a simple yet elegant way of stemming
for Web search. It improves naive stemming in two aspects:
selective word expansion on the query side and 
conservative word occurrence matching on the document side. Using
pluralization handling as an example, experiments on a 
major Web search engine data show it significantly improves
the Web relevance and reduces the stemming cost. It also
significantly improves Web click through rate (details not
reported in the paper).
For the future work, we are investigating the problems
we identified in the error analysis section. These include:
entity and noun phrase matching mistakes, and improved
segmentation.
7. REFERENCES
[1] E. Agichtein, E. Brill, and S. T. Dumais. Improving
Web Search Ranking by Incorporating User Behavior
Information. In SIGIR, 2006.
[2] E. Airio. Word Normalization and Decompounding in
Mono- and Bilingual IR. Information Retrieval,
9:249-271, 2006.
[3] P. Anick. Using Terminological Feedback for Web
Search Refinement: a Log-based Study. In SIGIR,
2003.
[4] R. Baeza-Yates and B. Ribeiro-Neto. Modern
Information Retrieval. ACM Press/Addison Wesley,
1999.
[5] S. Chen and J. Goodman. An Empirical Study of
Smoothing Techniques for Language Modeling.
Technical Report TR-10-98, Harvard University, 1998.
[6] S. Cronen-Townsend, Y. Zhou, and B. Croft. A
Framework for Selective Query Expansion. In CIKM,
2004.
[7] H. Fang and C. Zhai. Semantic Term Matching in
Axiomatic Approaches to Information Retrieval. In
SIGIR, 2006.
[8] W. B. Frakes. Term Conflation for Information
Retrieval. In C. J. Rijsbergen, editor, Research and
Development in Information Retrieval, pages 383-389.
Cambridge University Press, 1984.
[9] D. Harman. How Effective is Suffixing? JASIS,
42(1):7-15, 1991.
[10] D. Hull. Stemming Algorithms - A Case Study for
Detailed Evaluation. JASIS, 47(1):70-84, 1996.
[11] K. Jarvelin and J. Kekalainen. Cumulated Gain-Based
Evaluation Evaluation of IR Techniques. ACM TOIS,
20:422-446, 2002.
[12] R. Jones, B. Rey, O. Madani, and W. Greiner.
Generating Query Substitutions. In WWW, 2006.
[13] W. Kraaij and R. Pohlmann. Viewing Stemming as
Recall Enhancement. In SIGIR, 1996.
[14] R. Krovetz. Viewing Morphology as an Inference
Process. In SIGIR, 1993.
[15] D. Lin. Automatic Retrieval and Clustering of Similar
Words. In COLING-ACL, 1998.
[16] J. B. Lovins. Development of a Stemming Algorithm.
Mechanical Translation and Computational
Linguistics, II:22-31, 1968.
[17] M. Lennon and D. Peirce and B. Tarry and P. Willett.
An Evaluation of Some Conflation Algorithms for
Information Retrieval. Journal of Information Science,
3:177-188, 1981.
[18] M. Porter. An Algorithm for Suffix Stripping.
Program, 14(3):130-137, 1980.
[19] K. M. Risvik, T. Mikolajewski, and P. Boros. Query
Segmentation for Web Search. In WWW, 2003.
[20] S. E. Robertson. On Term Selection for Query
Expansion. Journal of Documentation, 46(4):359-364,
1990.
[21] G. Salton and C. Buckley. Improving Retrieval
Performance by Relevance Feedback. JASIS, 41(4):288
- 297, 1999.
[22] R. Sun, C.-H. Ong, and T.-S. Chua. Mining
Dependency Relations for Query Expansion in
Passage Retrieval. In SIGIR, 2006.
[23] C. Van Rijsbergen. Information Retrieval.
Butterworths, second version, 1979.
[24] B. V´elez, R. Weiss, M. A. Sheldon, and D. K. Gifford.
Fast and Effective Query Refinement. In SIGIR, 1997.
[25] J. Xu and B. Croft. Query Expansion using Local and
Global Document Analysis. In SIGIR, 1996.
[26] J. Xu and B. Croft. Corpus-based Stemming using
Cooccurrence of Word Variants. ACM TOIS, 16
(1):61-81, 1998.
Improving Web Search Ranking by Incorporating
User Behavior Information
Eugene Agichtein
Microsoft Research
eugeneag@microsoft.com
Eric Brill
Microsoft Research
brill@microsoft.com
Susan Dumais
Microsoft Research
sdumais@microsoft.com
ABSTRACT
We show that incorporating user behavior data can significantly
improve ordering of top results in real web search setting. We
examine alternatives for incorporating feedback into the ranking
process and explore the contributions of user feedback compared
to other common web search features. We report results of a large
scale evaluation over 3,000 queries and 12 million user
interactions with a popular web search engine. We show that
incorporating implicit feedback can augment other features,
improving the accuracy of a competitive web search ranking
algorithms by as much as 31% relative to the original
performance.
Categories and Subject Descriptors
H.3.3 Information Search and Retrieval - Relevance feedback,
search process; H.3.5 Online Information Services - Web-based
services.
General Terms
Algorithms, Measurement, Experimentation
1. INTRODUCTION
Millions of users interact with search engines daily. They issue
queries, follow some of the links in the results, click on ads, spend
time on pages, reformulate their queries, and perform other
actions. These interactions can serve as a valuable source of
information for tuning and improving web search result ranking
and can compliment more costly explicit judgments.
Implicit relevance feedback for ranking and personalization has
become an active area of research. Recent work by Joachims and
others exploring implicit feedback in controlled environments
have shown the value of incorporating implicit feedback into the
ranking process. Our motivation for this work is to understand
how implicit feedback can be used in a large-scale operational
environment to improve retrieval. How does it compare to and
compliment evidence from page content, anchor text, or link-based
features such as inlinks or PageRank? While it is intuitive that
user interactions with the web search engine should reveal at least
some information that could be used for ranking, estimating user
preferences in real web search settings is a challenging problem,
since real user interactions tend to be more noisy than
commonly assumed in the controlled settings of previous studies.
Our paper explores whether implicit feedback can be helpful in
realistic environments, where user feedback can be noisy (or
adversarial) and a web search engine already uses hundreds of
features and is heavily tuned. To this end, we explore different
approaches for ranking web search results using real user behavior
obtained as part of normal interactions with the web search
engine.
The specific contributions of this paper include:
• Analysis of alternatives for incorporating user behavior
into web search ranking (Section 3).
• An application of a robust implicit feedback model
derived from mining millions of user interactions with a
major web search engine (Section 4).
• A large scale evaluation over real user queries and search
results, showing significant improvements derived from
incorporating user feedback (Section 6).
We summarize our findings and discuss extensions to the current
work in Section 7, which concludes the paper.
2. BACKGROUND AND RELATED WORK
Ranking search results is a fundamental problem in information
retrieval. Most common approaches primarily focus on similarity
of query and a page, as well as the overall page quality [3,4,24].
However, with increasing popularity of search engines, implicit
feedback (i.e., the actions users take when interacting with the
search engine) can be used to improve the rankings.
Implicit relevance measures have been studied by several research
groups. An overview of implicit measures is compiled in Kelly and
Teevan [14]. This research, while developing valuable insights
into implicit relevance measures, was not applied to improve the
ranking of web search results in realistic settings.
Closely related to our work, Joachims [11] collected implicit
measures in place of explicit measures, introducing a technique
based entirely on clickthrough data to learn ranking functions. Fox
et al. [8] explored the relationship between implicit and explicit
measures in Web search, and developed Bayesian models to
correlate implicit measures and explicit relevance judgments for
both individual queries and search sessions. This work considered
a wide range of user behaviors (e.g., dwell time, scroll time,
reformulation patterns) in addition to the popular clickthrough
behavior. However, the modeling effort was aimed at predicting
explicit relevance judgments from implicit user actions and not
specifically at learning ranking functions. Other studies of user
behavior in web search include Pharo and Järvelin [19], but were
not directly applied to improve ranking.
More recently, Joachims et al. [12] presented an empirical
evaluation of interpreting clickthrough evidence. By performing
eye tracking studies and correlating predictions of their strategies
with explicit ratings, the authors showed that it is possible to
accurately interpret clickthroughs in a controlled, laboratory
setting. Unfortunately, the extent to which previous research
applies to real-world web search is unclear. At the same time,
while recent work (e.g., [26]) on using clickthrough information
for improving web search ranking is promising, it captures only
one aspect of the user interactions with web search engines.
We build on existing research to develop robust user behavior
interpretation techniques for the real web search setting. Instead of
treating each user as a reliable expert, we aggregate information
from multiple, unreliable, user search session traces, as we
describe in the next two sections.
3. INCORPORATING IMPLICIT
FEEDBACK
We consider two complementary approaches to ranking with
implicit feedback: (1) treating implicit feedback as independent
evidence for ranking results, and (2) integrating implicit feedback
features directly into the ranking algorithm. We describe the two
general ranking approaches next. The specific implicit feedback
features are described in Section 4, and the algorithms for
interpreting and incorporating implicit feedback are described in
Section 5.
3.1 Implicit Feedback as Independent
Evidence
The general approach is to re-rank the results obtained by a web
search engine according to observed clickthrough and other user
interactions for the query in previous search sessions. Each result
is assigned a score according to expected relevance/user
satisfaction based on previous interactions, resulting in some
preference ordering based on user interactions alone.
While there has been significant work on merging multiple
rankings, we adapt a simple and robust approach of ignoring the
original rankers" scores, and instead simply merge the rank orders.
The main reason for ignoring the original scores is that since the
feature spaces and learning algorithms are different, the scores are
not directly comparable, and re-normalization tends to remove the
benefit of incorporating classifier scores.
We experimented with a variety of merging functions on the
development set of queries (and using a set of interactions from a
different time period from final evaluation sets). We found that a
simple rank merging heuristic combination works well, and is
robust to variations in score values from original rankers. For a
given query q, the implicit score ISd is computed for each result d
from available user interaction features, resulting in the implicit
rank Id for each result. We compute a merged score SM(d) for d by
combining the ranks obtained from implicit feedback, Id with the
original rank of d, Od:
 
 
¡
 
 
¢
£
+
+
+
+
=
otherwise
O
dforexistsfeedbackimplicitif
OI
w
wOIdS
d
dd
I
IddM
1
1
1
1
1
1
),,,(
where the weight wI is a heuristically tuned scaling factor
representing the relative importance of the implicit feedback.
The query results are ordered in by decreasing values of SM to
produce the final ranking. One special case of this model arises
when setting wI to a very large value, effectively forcing clicked
results to be ranked higher than un-clicked results - an intuitive
and effective heuristic that we will use as a baseline. Applying
more sophisticated classifier and ranker combination algorithms
may result in additional improvements, and is a promising
direction for future work.
The approach above assumes that there are no interactions
between the underlying features producing the original web search
ranking and the implicit feedback features. We now relax this
assumption by integrating implicit feedback features directly into
the ranking process.
3.2 Ranking with Implicit Feedback Features
Modern web search engines rank results based on a large number
of features, including content-based features (i.e., how closely a
query matches the text or title or anchor text of the document), and
query-independent page quality features (e.g., PageRank of the
document or the domain). In most cases, automatic (or 
semiautomatic) methods are developed for tuning the specific ranking
function that combines these feature values.
Hence, a natural approach is to incorporate implicit feedback
features directly as features for the ranking algorithm. During
training or tuning, the ranker can be tuned as before but with
additional features. At runtime, the search engine would fetch the
implicit feedback features associated with each query-result URL
pair. This model requires a ranking algorithm to be robust to
missing values: more than 50% of queries to web search engines
are unique, with no previous implicit feedback available. We now
describe such a ranker that we used to learn over the combined
feature sets including implicit feedback.
3.3 Learning to Rank Web Search Results
A key aspect of our approach is exploiting recent advances in
machine learning, namely trainable ranking algorithms for web
search and information retrieval (e.g., [5, 11] and classical results
reviewed in [3]). In our setting, explicit human relevance
judgments (labels) are available for a set of web search queries
and results. Hence, an attractive choice to use is a supervised
machine learning technique to learn a ranking function that best
predicts relevance judgments.
RankNet is one such algorithm. It is a neural net tuning algorithm
that optimizes feature weights to best match explicitly provided
pairwise user preferences. While the specific training algorithms
used by RankNet are beyond the scope of this paper, it is
described in detail in [5] and includes extensive evaluation and
comparison with other ranking methods. An attractive feature of
RankNet is both train- and run-time efficiency - runtime ranking
can be quickly computed and can scale to the web, and training
can be done over thousands of queries and associated judged
results.
We use a 2-layer implementation of RankNet in order to model
non-linear relationships between features. Furthermore, RankNet
can learn with many (differentiable) cost functions, and hence can
automatically learn a ranking function from human-provided
labels, an attractive alternative to heuristic feature combination
techniques. Hence, we will also use RankNet as a generic ranker
to explore the contribution of implicit feedback for different
ranking alternatives.
4. IMPLICIT USER FEEDBACK MODEL
Our goal is to accurately interpret noisy user feedback obtained as
by tracing user interactions with the search engine. Interpreting
implicit feedback in real web search setting is not an easy task.
We characterize this problem in detail in [1], where we motivate
and evaluate a wide variety of models of implicit user activities.
The general approach is to represent user actions for each search
result as a vector of features, and then train a ranker on these
features to discover feature values indicative of relevant (and 
nonrelevant) search results. We first briefly summarize our features
and model, and the learning approach (Section 4.2) in order to
provide sufficient information to replicate our ranking methods
and the subsequent experiments.
4.1 Representing User Actions as Features
We model observed web search behaviors as a combination of a
``background"" component (i.e., query- and relevance-independent
noise in user behavior, including positional biases with result
interactions), and a ``relevance"" component (i.e., query-specific
behavior indicative of relevance of a result to a query). We design
our features to take advantage of aggregated user behavior. The
feature set is comprised of directly observed features (computed
directly from observations for each query), as well as 
queryspecific derived features, computed as the deviation from the
overall query-independent distribution of values for the
corresponding directly observed feature values.
The features used to represent user interactions with web search
results are summarized in Table 4.1. This information was
obtained via opt-in client-side instrumentation from users of a
major web search engine.
We include the traditional implicit feedback features such as
clickthrough counts for the results, as well as our novel derived
features such as the deviation of the observed clickthrough number
for a given query-URL pair from the expected number of clicks on
a result in the given position. We also model the browsing
behavior after a result was clicked - e.g., the average page dwell
time for a given query-URL pair, as well as its deviation from the
expected (average) dwell time. Furthermore, the feature set was
designed to provide essential information about the user
experience to make feedback interpretation robust. For example,
web search users can often determine whether a result is relevant
by looking at the result title, URL, and summary - in many cases,
looking at the original document is not necessary. To model this
aspect of user experience we include features such as overlap in
words in title and words in query (TitleOverlap) and the fraction
of words shared by the query and the result summary.
Clickthrough features
Position Position of the URL in Current ranking
ClickFrequency Number of clicks for this query, URL pair
ClickProbability Probability of a click for this query and URL
ClickDeviation Deviation from expected click probability
IsNextClicked 1 if clicked on next position, 0 otherwise
IsPreviousClicked 1 if clicked on previous position, 0 otherwise
IsClickAbove 1 if there is a click above, 0 otherwise
IsClickBelow 1 if there is click below, 0 otherwise
Browsing features
TimeOnPage Page dwell time
CumulativeTimeOnPage
Cumulative time for all subsequent pages after
search
TimeOnDomain Cumulative dwell time for this domain
TimeOnShortUrl Cumulative time on URL prefix, no parameters
IsFollowedLink 1 if followed link to result, 0 otherwise
IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise
IsRedirected 1 if initial URL same as final URL, 0 otherwise
IsPathFromSearch 1 if only followed links after query, 0 otherwise
ClicksFromSearch Number of hops to reach page from query
AverageDwellTime Average time on page for this query
DwellTimeDeviation Deviation from average dwell time on page
CumulativeDeviation Deviation from average cumulative dwell time
DomainDeviation Deviation from average dwell time on domain
Query-text features
TitleOverlap Words shared between query and title
SummaryOverlap Words shared between query and snippet
QueryURLOverlap Words shared between query and URL
QueryDomainOverlap Words shared between query and URL domain
QueryLength Number of tokens in query
QueryNextOverlap Fraction of words shared with next query
Table 4.1: Some features used to represent post-search
navigation history for a given query and search result URL.
Having described our feature set, we briefly review our general
method for deriving a user behavior model.
4.2 Deriving a User Feedback Model
To learn to interpret the observed user behavior, we correlate user
actions (i.e., the features in Table 4.1 representing the actions)
with the explicit user judgments for a set of training queries. We
find all the instances in our session logs where these queries were
submitted to the search engine, and aggregate the user behavior
features for all search sessions involving these queries.
Each observed query-URL pair is represented by the features in
Table 4.1, with values averaged over all search sessions, and
assigned one of six possible relevance labels, ranging from
Perfect to Bad, as assigned by explicit relevance judgments.
These labeled feature vectors are used as input to the RankNet
training algorithm (Section 3.3) which produces a trained user
behavior model. This approach is particularly attractive as it does
not require heuristics beyond feature engineering. The resulting
user behavior model is used to help rank web search 
resultseither directly or in combination with other features, as described
below.
5. EXPERIMENTAL SETUP
The ultimate goal of incorporating implicit feedback into ranking
is to improve the relevance of the returned web search results.
Hence, we compare the ranking methods over a large set of judged
queries with explicit relevance labels provided by human judges.
In order for the evaluation to be realistic we obtained a random
sample of queries from web search logs of a major search engine,
with associated results and traces for user actions. We describe
this dataset in detail next. Our metrics are described in Section 5.2
that we use to evaluate the ranking alternatives, listed in Section
5.3 in the experiments of Section 6.
5.1 Datasets
We compared our ranking methods over a random sample of 3,000
queries from the search engine query logs. The queries were
drawn from the logs uniformly at random by token without
replacement, resulting in a query sample representative of the
overall query distribution. On average, 30 results were explicitly
labeled by human judges using a six point scale ranging from
Perfect down to Bad. Overall, there were over 83,000 results
with explicit relevance judgments. In order to compute various
statistics, documents with label Good or better will be
considered relevant, and with lower labels to be non-relevant.
Note that the experiments were performed over the results already
highly ranked by a web search engine, which corresponds to a
typical user experience which is limited to the small number of the
highly ranked results for a typical web search query.
The user interactions were collected over a period of 8 weeks
using voluntary opt-in information. In total, over 1.2 million
unique queries were instrumented, resulting in over 12 million
individual interactions with the search engine. The data consisted
of user interactions with the web search engine (e.g., clicking on a
result link, going back to search results, etc.) performed after a
query was submitted. These actions were aggregated across users
and search sessions and converted to features in Table 4.1.
To create the training, validation, and test query sets, we created
three different random splits of 1,500 training, 500 validation, and
1000 test queries. The splits were done randomly by query, so that
there was no overlap in training, validation, and test queries.
5.2 Evaluation Metrics
We evaluate the ranking algorithms over a range of accepted
information retrieval metrics, namely Precision at K (P(K)),
Normalized Discounted Cumulative Gain (NDCG), and Mean
Average Precision (MAP). Each metric focuses on a deferent
aspect of system performance, as we describe below.
• Precision at K: As the most intuitive metric, P(K) reports the
fraction of documents ranked in the top K results that are
labeled as relevant. In our setting, we require a relevant
document to be labeled Good or higher. The position of
relevant documents within the top K is irrelevant, and hence
this metric measure overall user satisfaction with the top K
results.
• NDCG at K: NDCG is a retrieval measure devised specifically
for web search evaluation [10]. For a given query q, the ranked
results are examined from the top ranked down, and the NDCG
computed as:
 
=
+−=
K
j
jr
qq jMN
1
)(
)1log(/)12(
Where Mq is a normalization constant calculated so that a
perfect ordering would obtain NDCG of 1; and each r(j) is an
integer relevance label (0=Bad and 5=Perfect) of result
returned at position j. Note that unlabeled and Bad documents
do not contribute to the sum, but will reduce NDCG for the
query pushing down the relevant labeled documents, reducing
their contributions. NDCG is well suited to web search
evaluation, as it rewards relevant documents in the top ranked
results more heavily than those ranked lower.
• MAP: Average precision for each query is defined as the mean
of the precision at K values computed after each relevant
document was retrieved. The final MAP value is defined as the
mean of average precisions of all queries in the test set. This
metric is the most commonly used single-value summary of a
run over a set of queries.
5.3 Ranking Methods Compared
Recall that our goal is to quantify the effectiveness of implicit
behavior for real web search. One dimension is to compare the
utility of implicit feedback with other information available to a
web search engine. Specifically, we compare effectiveness of
implicit user behaviors with content-based matching, static page
quality features, and combinations of all features.
• BM25F: As a strong web search baseline we used the BM25F
scoring, which was used in one of the best performing systems
in the TREC 2004 Web track [23,27]. BM25F and its variants
have been extensively described and evaluated in IR literature,
and hence serve as a strong, reproducible baseline. The BM25F
variant we used for our experiments computes separate match
scores for each field for a result document (e.g., body text,
title, and anchor text), and incorporates query-independent 
linkbased information (e.g., PageRank, ClickDistance, and URL
depth). The scoring function and field-specific tuning is
described in detail in [23]. Note that BM25F does not directly
consider explicit or implicit feedback for tuning.
• RN: The ranking produced by a neural net ranker (RankNet,
described in Section 3.3) that learns to rank web search results
by incorporating BM25F and a large number of additional static
and dynamic features describing each search result. This system
automatically learns weights for all features (including the
BM25F score for a document) based on explicit human labels
for a large set of queries. A system incorporating an
implementation of RankNet is currently in use by a major
search engine and can be considered representative of the state
of the art in web search.
• BM25F-RerankCT: The ranking produced by incorporating
clickthrough statistics to reorder web search results ranked by
BM25F above. Clickthrough is a particularly important special
case of implicit feedback, and has been shown to correlate with
result relevance. This is a special case of the ranking method in
Section 3.1, with the weight wI set to 1000 and the ranking Id
is simply the number of clicks on the result corresponding to d.
In effect, this ranking brings to the top all returned web search
results with at least one click (and orders them in decreasing
order by number of clicks). The relative ranking of the
remainder of results is unchanged and they are inserted below
all clicked results. This method serves as our baseline implicit
feedback reranking method.
BM25F-RerankAll The ranking produced by reordering the
BM25F results using all user behavior features (Section 4).
This method learns a model of user preferences by correlating
feature values with explicit relevance labels using the RankNet
neural net algorithm (Section 4.2). At runtime, for a given
query the implicit score Ir is computed for each result r with
available user interaction features, and the implicit ranking is
produced. The merged ranking is computed as described in
Section 3.1. Based on the experiments over the development set
we fix the value of wI to 3 (the effect of the wI parameter for
this ranker turned out to be negligible).
• BM25F+All: Ranking derived by training the RankNet
(Section 3.3) learner over the features set of the BM25F score
as well as all implicit feedback features (Section 3.2). We used
the 2-layer implementation of RankNet [5] trained on the
queries and labels in the training and validation sets.
• RN+All: Ranking derived by training the 2-layer RankNet
ranking algorithm (Section 3.3) over the union of all content,
dynamic, and implicit feedback features (i.e., all of the features
described above as well as all of the new implicit feedback
features we introduced).
The ranking methods above span the range of the information used
for ranking, from not using the implicit or explicit feedback at all
(i.e., BM25F) to a modern web search engine using hundreds of
features and tuned on explicit judgments (RN). As we will show
next, incorporating user behavior into these ranking systems
dramatically improves the relevance of the returned documents.
6. EXPERIMENTAL RESULTS
Implicit feedback for web search ranking can be exploited in a
number of ways. We compare alternative methods of exploiting
implicit feedback, both by re-ranking the top results (i.e., the
BM25F-RerankCT and BM25F-RerankAll methods that reorder
BM25F results), as well as by integrating the implicit features
directly into the ranking process (i.e., the RN+ALL and
BM25F+All methods which learn to rank results over the implicit
feedback and other features). We compare our methods over strong
baselines (BM25F and RN) over the NDCG, Precision at K, and
MAP measures defined in Section 5.2. The results were averaged
over three random splits of the overall dataset. Each split
contained 1500 training, 500 validation, and 1000 test queries, all
query sets disjoint. We first present the results over all 1000 test
queries (i.e., including queries for which there are no implicit
measures so we use the original web rankings). We then drill
down to examine the effects on reranking for the attempted
queries in more detail, analyzing where implicit feedback proved
most beneficial.
We first experimented with different methods of re-ranking the
output of the BM25F search results. Figures 6.1 and 6.2 report
NDCG and Precision for BM25F, as well as for the strategies
reranking results with user feedback (Section 3.1). Incorporating
all user feedback (either in reranking framework or as features to
the learner directly) results in significant improvements (using
two-tailed t-test with p=0.01) over both the original BM25F
ranking as well as over reranking with clickthrough alone. The
improvement is consistent across the top 10 results and largest for
the top result: NDCG at 1 for BM25F+All is 0.622 compared to
0.518 of the original results, and precision at 1 similarly increases
from 0.5 to 0.63. Based on these results we will use the direct
feature combination (i.e., BM25F+All) ranker for subsequent
comparisons involving implicit feedback.
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
1 2 3 4 5 6 7 8 9 10K
NDCG
BM25
BM25-Rerank-CT
BM25-Rerank-All
BM25+All
Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT,
BM25F-Rerank-All, and BM25F+All for varying K
0.35
0.4
0.45
0.5
0.55
0.6
0.65
1 3 5 10
K
Precision
BM25
BM25-Rerank-CT
BM25-Rerank-All
BM25+All
Figure 6.2: Precision at K for BM25F, BM25F-RerankCT,
BM25F-Rerank-All, and BM25F+All for varying K
Interestingly, using clickthrough alone, while giving significant
benefit over the original BM25F ranking, is not as effective as
considering the full set of features in Table 4.1. While we analyze
user behavior (and most effective component features) in a
separate paper [1], it is worthwhile to give a concrete example of
the kind of noise inherent in real user feedback in web search
setting.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 5
Result position
Relativeclickfrequency
PTR=2
PTR=3
PTR=5
Figure 6.3: Relative clickthrough frequency for queries with
varying Position of Top Relevant result (PTR).
If users considered only the relevance of a result to their query,
they would click on the topmost relevant results. Unfortunately, as
Joachims and others have shown, presentation also influences
which results users click on quite dramatically. Users often click
on results above the relevant one presumably because the short
summaries do not provide enough information to make accurate
relevance assessments and they have learned that on average 
topranked items are relevant. Figure 6.3 shows relative clickthrough
frequencies for queries with known relevant items at positions
other than the first position; the position of the top relevant result
(PTR) ranges from 2-10 in the figure. For example, for queries
with first relevant result at position 5 (PTR=5), there are more
clicks on the non-relevant results in higher ranked positions than
on the first relevant result at position 5. As we will see, learning
over a richer behavior feature set, results in substantial accuracy
improvement over clickthrough alone.
We now consider incorporating user behavior into a much richer
feature set, RN (Section 5.3) used by a major web search engine.
RN incorporates BM25F, link-based features, and hundreds of
other features. Figure 6.4 reports NDCG at K and Figure 6.5
reports Precision at K. Interestingly, while the original RN
rankings are significantly more accurate than BM25F alone,
incorporating implicit feedback features (BM25F+All) results in
ranking that significantly outperforms the original RN rankings. In
other words, implicit feedback incorporates sufficient information
to replace the hundreds of other features available to the RankNet
learner trained on the RN feature set.
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
0.7
1 2 3 4 5 6 7 8 9 10K
NDCG
RN
RN+All
BM25
BM25+All
Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and
RN+All for varying K
Furthermore, enriching the RN features with implicit feedback set
exhibits significant gain on all measures, allowing RN+All to
outperform all other methods. This demonstrates the
complementary nature of implicit feedback with other features
available to a state of the art web search engine.
0.4
0.45
0.5
0.55
0.6
0.65
1 3 5 10
K
Precision
RN
RN+All
BM25
BM25+All
Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and
RN+All for varying K
We summarize the performance of the different ranking methods
in Table 6.1. We report the Mean Average Precision (MAP) score
for each system. While not intuitive to interpret, MAP allows
quantitative comparison on a single metric. The gains marked with
* are significant at p=0.01 level using two tailed t-test.
MAP Gain P(1) Gain
BM25F 0.184 - 
0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073*
BM25F-RerankImplicit 0.218 0.003 0.605 0.028*
BM25F+Implicit 0.222 0.004 0.620 0.015*
RN 0.215 - 
0.597RN+All 0.248 0.033* 0.629 0.032*
Table 6.1: Mean Average Precision (MAP) for all strategies.
So far we reported results averaged across all queries in the test
set. Unfortunately, less than half had sufficient interactions to
attempt reranking. Out of the 1000 queries in test, between 46%
and 49%, depending on the train-test split, had sufficient
interaction information to make predictions (i.e., there was at least
1 search session in which at least 1 result URL was clicked on by
the user). This is not surprising: web search is heavy-tailed, and
there are many unique queries. We now consider the performance
on the queries for which user interactions were available. Figure
6.6 reports NDCG for the subset of the test queries with the
implicit feedback features. The gains at top 1 are dramatic. The
NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31%
relative gain), achieving performance comparable to RN+All
operating over a much richer feature set.
0.6
0.65
0.7
0.75
0.8
1 3 5 10K
NDCG
RN RN+All
BM25 BM25+All
Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and
RN+All on test queries with user interactions
Similarly, gains on precision at top 1 are substantial (Figure 6.7),
and are likely to be apparent to web search users. When implicit
feedback is available, the BM25F+All system returns relevant
document at top 1 almost 70% of the time, compared 53% of the
time when implicit feedback is not considered by the original
BM25F system.
0.45
0.5
0.55
0.6
0.65
0.7
1 3 5 10K
Precision
RN
RN+All
BM25
BM25+All
Figure 6.7: Precision at K NDCG at K for BM25F,
BM25F+All, RN, and RN+All on test queries with user
interactions
We summarize the results on the MAP measure for attempted
queries in Table 6.2. MAP improvements are both substantial and
significant, with improvements over the BM25F ranker most
pronounced.
Method MAP Gain P(1) Gain
RN 0.269 0.632
RN+All 0.321 0.051 (19%) 0.693 0.061(10%)
BM25F 0.236 0.525
BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%)
Table 6.2: Mean Average Precision (MAP) on attempted
queries for best performing methods
We now analyze the cases where implicit feedback was shown
most helpful. Figure 6.8 reports the MAP improvements over the
baseline BM25F run for each query with MAP under 0.6. Note
that most of the improvement is for poorly performing queries
(i.e., MAP < 0.1). Interestingly, incorporating user behavior
information degrades accuracy for queries with high original MAP
score. One possible explanation is that these easy queries tend
to be navigational (i.e., having a single, highly-ranked most
appropriate answer), and user interactions with lower-ranked
results may indicate divergent information needs that are better
served by the less popular results (with correspondingly poor
overall relevance ratings).
0
50
100
150
200
250
300
350
0.1 0.2 0.3 0.4 0.5 0.6
-0.4
-0.35
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
Frequency Average Gain
Figure 6.8: Gain of BM25F+All over original BM25F ranking
To summarize our experimental results, incorporating implicit
feedback in real web search setting resulted in significant
improvements over the original rankings, using both BM25F and
RN baselines. Our rich set of implicit features, such as time on
page and deviations from the average behavior, provides
advantages over using clickthrough alone as an indicator of
interest. Furthermore, incorporating implicit feedback features
directly into the learned ranking function is more effective than
using implicit feedback for reranking. The improvements observed
over large test sets of queries (1,000 total, between 466 and 495
with implicit feedback available) are both substantial and
statistically significant.
7. CONCLUSIONS AND FUTURE WORK
In this paper we explored the utility of incorporating noisy implicit
feedback obtained in a real web search setting to improve web
search ranking. We performed a large-scale evaluation over 3,000
queries and more than 12 million user interactions with a major
search engine, establishing the utility of incorporating noisy
implicit feedback to improve web search relevance.
We compared two alternatives of incorporating implicit feedback
into the search process, namely reranking with implicit feedback
and incorporating implicit feedback features directly into the
trained ranking function. Our experiments showed significant
improvement over methods that do not consider implicit feedback.
The gains are particularly dramatic for the top K=1 result in the
final ranking, with precision improvements as high as 31%, and
the gains are substantial for all values of K. Our experiments
showed that implicit user feedback can further improve web
search performance, when incorporated directly with popular
content- and link-based features.
Interestingly, implicit feedback is particularly valuable for queries
with poor original ranking of results (e.g., MAP lower than 0.1).
One promising direction for future work is to apply recent research
on automatically predicting query difficulty, and only attempt to
incorporate implicit feedback for the difficult queries. As
another research direction we are exploring methods for extending
our predictions to the previously unseen queries (e.g., query
clustering), which should further improve the web search
experience of users.
ACKNOWLEDGMENTS
We thank Chris Burges and Matt Richardson for an
implementation of RankNet for our experiments. We also thank
Robert Ragno for his valuable suggestions and many discussions.
8. REFERENCES
[1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning
User Interaction Models for Predicting Web Search Result
Preferences. In Proceedings of the ACM Conference on
Research and Development on Information Retrieval (SIGIR),
2006
[2] J. Allan, HARD Track Overview in TREC 2003, High
Accuracy Retrieval from Documents, 2003
[3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information
Retrieval, Addison-Wesley, 1999.
[4] S. Brin and L. Page, The Anatomy of a Large-scale
Hypertextual Web Search Engine, in Proceedings of WWW,
1997
[5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, G. Hullender, Learning to Rank using Gradient
Descent, in Proceedings of the International Conference on
Machine Learning, 2005
[6] D.M. Chickering, The WinMine Toolkit, Microsoft Technical
Report MSR-TR-2002-103, 2002
[7] M. Claypool, D. Brown, P. Lee and M. Waseda. Inferring
user interest. IEEE Internet Computing. 2001
[8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T.
White. Evaluating implicit measures to improve the search
experience. In ACM Transactions on Information Systems,
2005
[9] J. Goecks and J. Shavlick. Learning users" interests by
unobtrusively observing their normal behavior. In
Proceedings of the IJCAI Workshop on Machine Learning for
Information Filtering. 1999.
[10] K Jarvelin and J. Kekalainen. IR evaluation methods for
retrieving highly relevant documents. In Proceedings of the
ACM Conference on Research and Development on
Information Retrieval (SIGIR), 2000
[11] T. Joachims, Optimizing Search Engines Using Clickthrough
Data. In Proceedings of the ACM Conference on Knowledge
Discovery and Datamining (SIGKDD), 2002
[12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay,
Accurately Interpreting Clickthrough Data as Implicit
Feedback, Proceedings of the ACM Conference on Research
and Development on Information Retrieval (SIGIR), 2005
[13] T. Joachims, Making Large-Scale SVM Learning Practical.
Advances in Kernel Methods, in Support Vector Learning,
MIT Press, 1999
[14] D. Kelly and J. Teevan, Implicit feedback for inferring user
preference: A bibliography. In SIGIR Forum, 2003
[15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and
J. Riedl. GroupLens: Applying collaborative filtering to
usenet news. In Communications of ACM, 1997.
[16] M. Morita, and Y. Shinoda, Information filtering based on
user behavior analysis and best match text retrieval.
Proceedings of the ACM Conference on Research and
Development on Information Retrieval (SIGIR), 1994
[17] D. Oard and J. Kim. Implicit feedback for recommender
systems. In Proceedings of the AAAI Workshop on
Recommender Systems. 1998
[18] D. Oard and J. Kim. Modeling information content using
observable behavior. In Proceedings of the 64th Annual
Meeting of the American Society for Information Science and
Technology. 2001
[19] N. Pharo, N. and K. Järvelin. The SST method: a tool for
analyzing web information search processes. In Information
Processing & Management, 2004
[20] P. Pirolli, The Use of Proximal Information Scent to Forage
for Distal Content on the World Wide Web. In Working with
Technology in Mind: Brunswikian. Resources for Cognitive
Science and Engineering, Oxford University Press, 2004
[21] F. Radlinski and T. Joachims, Query Chains: Learning to
Rank from Implicit Feedback. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Mining
(SIGKDD), 2005.
[22] F. Radlinski and T. Joachims, Evaluating the Robustness of
Learning from Implicit Feedback, in Proceedings of the ICML
Workshop on Learning in Web Search, 2005
[23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25
extension to multiple weighted fields, in Proceedings of the
Conference on Information and Knowledge Management
(CIKM), 2004
[24] G. Salton & M. McGill. Introduction to modern information
retrieval. McGraw-Hill, 1983
[25] E.M. Voorhees, D. Harman, Overview of TREC, 2001
[26] G.R. Xue, H.J. Zeng, Z. Chen, Y. Yu, W.Y. Ma, W.S. Xi,
and W.G. Fan, Optimizing web search using web 
clickthrough data, in Proceedings of the Conference on
Information and Knowledge Management (CIKM), 2004
[27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S.
Robertson. Microsoft Cambridge at TREC 13: Web and Hard
Tracks. In Proceedings of TREC 2004
AdaRank: A Boosting Algorithm for Information Retrieval
Jun Xu
Microsoft Research Asia
No. 49 Zhichun Road, Haidian Distinct
Beijing, China 100080
junxu@microsoft.com
Hang Li
Microsoft Research Asia
No. 49 Zhichun Road, Haidian Distinct
Beijing, China 100080
hangli@microsoft.com
ABSTRACT
In this paper we address the issue of learning to rank for document
retrieval. In the task, a model is automatically created with some
training data and then is utilized for ranking of documents. The
goodness of a model is usually evaluated with performance 
measures such as MAP (Mean Average Precision) and NDCG 
(Normalized Discounted Cumulative Gain). Ideally a learning 
algorithm would train a ranking model that could directly optimize the
performance measures with respect to the training data. Existing
methods, however, are only able to train ranking models by 
minimizing loss functions loosely related to the performance measures.
For example, Ranking SVM and RankBoost train ranking 
models by minimizing classification errors on instance pairs. To deal
with the problem, we propose a novel learning algorithm within
the framework of boosting, which can minimize a loss function
directly defined on the performance measures. Our algorithm, 
referred to as AdaRank, repeatedly constructs ‘weak rankers" on the
basis of re-weighted training data and finally linearly combines the
weak rankers for making ranking predictions. We prove that the
training process of AdaRank is exactly that of enhancing the 
performance measure used. Experimental results on four benchmark
datasets show that AdaRank significantly outperforms the baseline
methods of BM25, Ranking SVM, and RankBoost.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models
General Terms
Algorithms, Experimentation, Theory
1. INTRODUCTION
Recently ‘learning to rank" has gained increasing attention in
both the fields of information retrieval and machine learning. When
applied to document retrieval, learning to rank becomes a task as
follows. In training, a ranking model is constructed with data 
consisting of queries, their corresponding retrieved documents, and 
relevance levels given by humans. In ranking, given a new query, the
corresponding retrieved documents are sorted by using the trained
ranking model. In document retrieval, usually ranking results are
evaluated in terms of performance measures such as MAP (Mean
Average Precision) [1] and NDCG (Normalized Discounted 
Cumulative Gain) [15]. Ideally, the ranking function is created so that the
accuracy of ranking in terms of one of the measures with respect to
the training data is maximized.
Several methods for learning to rank have been developed and
applied to document retrieval. For example, Herbrich et al. [13]
propose a learning algorithm for ranking on the basis of Support
Vector Machines, called Ranking SVM. Freund et al. [8] take a
similar approach and perform the learning by using boosting, 
referred to as RankBoost. All the existing methods used for 
document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss
functions loosely related to the IR performance measures, not loss
functions directly based on the measures. For example, Ranking
SVM and RankBoost train ranking models by minimizing 
classification errors on instance pairs.
In this paper, we aim to develop a new learning algorithm that
can directly optimize any performance measure used in document
retrieval. Inspired by the work of AdaBoost for classification [9],
we propose to develop a boosting algorithm for information 
retrieval, referred to as AdaRank. AdaRank utilizes a linear 
combination of ‘weak rankers" as its model. In learning, it repeats the
process of re-weighting the training sample, creating a weak ranker,
and calculating a weight for the ranker.
We show that AdaRank algorithm can iteratively optimize an 
exponential loss function based on any of IR performance measures.
A lower bound of the performance on training data is given, which
indicates that the ranking accuracy in terms of the performance
measure can be continuously improved during the training process.
AdaRank offers several advantages: ease in implementation, 
theoretical soundness, efficiency in training, and high accuracy in ranking.
Experimental results indicate that AdaRank can outperform the 
baseline methods of BM25, Ranking SVM, and RankBoost, on four
benchmark datasets including OHSUMED, WSJ, AP, and .Gov.
Tuning ranking models using certain training data and a 
performance measure is a common practice in IR [1]. As the number of
features in the ranking model gets larger and the amount of 
training data gets larger, the tuning becomes harder. From the viewpoint
of IR, AdaRank can be viewed as a machine learning method for
ranking model tuning.
Recently, direct optimization of performance measures in 
learning has become a hot research topic. Several methods for 
classification [17] and ranking [5, 19] have been proposed. AdaRank can
be viewed as a machine learning method for direct optimization of
performance measures, based on a different approach.
The rest of the paper is organized as follows. After a summary
of related work in Section 2, we describe the proposed AdaRank
algorithm in details in Section 3. Experimental results and 
discussions are given in Section 4. Section 5 concludes this paper and
gives future work.
2. RELATED WORK
2.1 Information Retrieval
The key problem for document retrieval is ranking, specifically,
how to create the ranking model (function) that can sort documents
based on their relevance to the given query. It is a common practice
in IR to tune the parameters of a ranking model using some labeled
data and one performance measure [1]. For example, the 
state-ofthe-art methods of BM25 [24] and LMIR (Language Models for
Information Retrieval) [18, 22] all have parameters to tune. As
the ranking models become more sophisticated (more features are
used) and more labeled data become available, how to tune or train
ranking models turns out to be a challenging issue.
Recently methods of ‘learning to rank" have been applied to
ranking model construction and some promising results have been
obtained. For example, Joachims [16] applies Ranking SVM to
document retrieval. He utilizes click-through data to deduce 
training data for the model creation. Cao et al. [4] adapt Ranking
SVM to document retrieval by modifying the Hinge Loss function
to better meet the requirements of IR. Specifically, they introduce
a Hinge Loss function that heavily penalizes errors on the tops of
ranking lists and errors from queries with fewer retrieved 
documents. Burges et al. [3] employ Relative Entropy as a loss function
and Gradient Descent as an algorithm to train a Neural Network
model for ranking in document retrieval. The method is referred to
as ‘RankNet".
2.2 Machine Learning
There are three topics in machine learning which are related to
our current work. They are ‘learning to rank", boosting, and direct
optimization of performance measures.
Learning to rank is to automatically create a ranking function
that assigns scores to instances and then rank the instances by 
using the scores. Several approaches have been proposed to tackle
the problem. One major approach to learning to rank is that of
transforming it into binary classification on instance pairs. This
‘pair-wise" approach fits well with information retrieval and thus is
widely used in IR. Typical methods of the approach include 
Ranking SVM [13], RankBoost [8], and RankNet [3]. For other 
approaches to learning to rank, refer to [2, 11, 31].
In the pair-wise approach to ranking, the learning task is 
formalized as a problem of classifying instance pairs into two categories
(correctly ranked and incorrectly ranked). Actually, it is known
that reducing classification errors on instance pairs is equivalent to
maximizing a lower bound of MAP [16]. In that sense, the 
existing methods of Ranking SVM, RankBoost, and RankNet are only
able to minimize loss functions that are loosely related to the IR
performance measures.
Boosting is a general technique for improving the accuracies of
machine learning algorithms. The basic idea of boosting is to 
repeatedly construct ‘weak learners" by re-weighting training data
and form an ensemble of weak learners such that the total 
performance of the ensemble is ‘boosted". Freund and Schapire have
proposed the first well-known boosting algorithm called AdaBoost
(Adaptive Boosting) [9], which is designed for binary 
classification (0-1 prediction). Later, Schapire & Singer have introduced a
generalized version of AdaBoost in which weak learners can give
confidence scores in their predictions rather than make 0-1 
decisions [26]. Extensions have been made to deal with the problems
of multi-class classification [10, 26], regression [7], and ranking
[8]. In fact, AdaBoost is an algorithm that ingeniously constructs
a linear model by minimizing the ‘exponential loss function" with
respect to the training data [26]. Our work in this paper can be
viewed as a boosting method developed for ranking, particularly
for ranking in IR.
Recently, a number of authors have proposed conducting direct
optimization of multivariate performance measures in learning. For
instance, Joachims [17] presents an SVM method to directly 
optimize nonlinear multivariate performance measures like the F1 
measure for classification. Cossock & Zhang [5] find a way to 
approximately optimize the ranking performance measure DCG [15].
Metzler et al. [19] also propose a method of directly maximizing
rank-based metrics for ranking on the basis of manifold learning.
AdaRank is also one that tries to directly optimize multivariate 
performance measures, but is based on a different approach. AdaRank
is unique in that it employs an exponential loss function based on
IR performance measures and a boosting technique.
3. OUR METHOD: ADARANK
3.1 General Framework
We first describe the general framework of learning to rank for
document retrieval. In retrieval (testing), given a query the system
returns a ranking list of documents in descending order of the 
relevance scores. The relevance scores are calculated with a ranking
function (model). In learning (training), a number of queries and
their corresponding retrieved documents are given. Furthermore,
the relevance levels of the documents with respect to the queries are
also provided. The relevance levels are represented as ranks (i.e.,
categories in a total order). The objective of learning is to construct
a ranking function which achieves the best results in ranking of the
training data in the sense of minimization of a loss function. Ideally
the loss function is defined on the basis of the performance measure
used in testing.
Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes
the number of ranks. There exists a total order between the ranks
r r −1 · · · r1, where ‘ " denotes a preference relationship.
In training, a set of queries Q = {q1, q2, · · · , qm} is given. Each
query qi is associated with a list of retrieved documents di = {di1, di2,
· · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi)
denotes the sizes of lists di and yi, dij denotes the jth
document in
di, and yij ∈ Y denotes the rank of document di j. A feature 
vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair
(qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Thus, the training set
can be represented as S = {(qi, di, yi)}m
i=1.
The objective of learning is to create a ranking function f : X →
, such that for each query the elements in its corresponding 
document list can be assigned relevance scores using the function and
then be ranked according to the scores. Specifically, we create a
permutation of integers π(qi, di, f) for query qi, the 
corresponding list of documents di, and the ranking function f. Let di =
{di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)},
then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · ,
n(qi)} to itself. We use π( j) to denote the position of item j (i.e.,
di j). The learning process turns out to be that of minimizing the
loss function which represents the disagreement between the 
permutation π(qi, di, f) and the list of ranks yi, for all of the queries.
Table 1: Notations and explanations.
Notations Explanations
qi ∈ Q ith
query
di = {di1, di2, · · · , di,n(qi)} List of documents for qi
yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi
yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi
S = {(qi, di, yi)}m
i=1 Training set
xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j)
f(xij) ∈ Ranking model
π(qi, di, f) Permutation for qi, di, and f
ht(xi j) ∈ tth
weak ranker
E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function
In the paper, we define the rank model as a linear combination of
weak rankers: f(x) = T
t=1 αtht(x), where ht(x) is a weak ranker, αt
is its weight, and T is the number of weak rankers.
In information retrieval, query-based performance measures are
used to evaluate the ‘goodness" of a ranking function. By query
based measure, we mean a measure defined over a ranking list
of documents with respect to a query. These measures include
MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take
ALL), and Precision@n [1, 15]. We utilize a general function
E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance 
measures. The first argument of E is the permutation π created using
the ranking function f on di. The second argument is the list of
ranks yi given by humans. E measures the agreement between π
and yi. Table 1 gives a summary of notations described above.
Next, as examples of performance measures, we present the 
definitions of MAP and NDCG. Given a query qi, the corresponding
list of ranks yi, and a permutation πi on di, average precision for qi
is defined as:
AvgPi =
n(qi)
j=1 Pi( j) · yij
n(qi)
j=1 yij
, (1)
where yij takes on 1 and 0 as values, representing being relevant or
irrelevant and Pi( j) is defined as precision at the position of dij:
Pi( j) =
k:πi(k)≤πi(j) yik
πi(j)
, (2)
where πi( j) denotes the position of di j.
Given a query qi, the list of ranks yi, and a permutation πi on di,
NDCG at position m for qi is defined as:
Ni = ni ·
j:πi(j)≤m
2yi j − 1
log(1 + πi( j))
, (3)
where yij takes on ranks as values and ni is a normalization 
constant. ni is chosen so that a perfect ranking π∗
i "s NDCG score at
position m is 1.
3.2 Algorithm
Inspired by the AdaBoost algorithm for classification, we have
devised a novel algorithm which can optimize a loss function based
on the IR performance measures. The algorithm is referred to as
‘AdaRank" and is shown in Figure 1.
AdaRank takes a training set S = {(qi, di, yi)}m
i=1 as input and
takes the performance measure function E and the number of 
iterations T as parameters. AdaRank runs T rounds and at each round it
creates a weak ranker ht(t = 1, · · · , T). Finally, it outputs a ranking
model f by linearly combining the weak rankers.
At each round, AdaRank maintains a distribution of weights over
the queries in the training data. We denote the distribution of weights
Input: S = {(qi, di, yi)}m
i=1, and parameters E and T
Initialize P1(i) = 1/m.
For t = 1, · · · , T
• Create weak ranker ht with weighted distribution Pt on 
training data S .
• Choose αt
αt =
1
2
· ln
m
i=1 Pt(i){1 + E(π(qi, di, ht), yi)}
m
i=1 Pt(i){1 − E(π(qi, di, ht), yi)}
.
• Create ft
ft(x) =
t
k=1
αkhk(x).
• Update Pt+1
Pt+1(i) =
exp{−E(π(qi, di, ft), yi)}
m
j=1 exp{−E(π(qj, dj, ft), yj)}
.
End For
Output ranking model: f(x) = fT (x).
Figure 1: The AdaRank algorithm.
at round t as Pt and the weight on the ith
training query qi at round
t as Pt(i). Initially, AdaRank sets equal weights to the queries. At
each round, it increases the weights of those queries that are not
ranked well by ft, the model created so far. As a result, the learning
at the next round will be focused on the creation of a weak ranker
that can work on the ranking of those ‘hard" queries.
At each round, a weak ranker ht is constructed based on training
data with weight distribution Pt. The goodness of a weak ranker is
measured by the performance measure E weighted by Pt:
m
i=1
Pt(i)E(π(qi, di, ht), yi).
Several methods for weak ranker construction can be considered.
For example, a weak ranker can be created by using a subset of
queries (together with their document list and label list) sampled
according to the distribution Pt. In this paper, we use single features
as weak rankers, as will be explained in Section 3.6.
Once a weak ranker ht is built, AdaRank chooses a weight αt > 0
for the weak ranker. Intuitively, αt measures the importance of ht.
A ranking model ft is created at each round by linearly 
combining the weak rankers constructed so far h1, · · · , ht with weights
α1, · · · , αt. ft is then used for updating the distribution Pt+1.
3.3 Theoretical Analysis
The existing learning algorithms for ranking attempt to minimize
a loss function based on instance pairs (document pairs). In 
contrast, AdaRank tries to optimize a loss function based on queries.
Furthermore, the loss function in AdaRank is defined on the basis
of general IR performance measures. The measures can be MAP,
NDCG, WTA, MRR, or any other measures whose range is within
[−1, +1]. We next explain why this is the case.
Ideally we want to maximize the ranking accuracy in terms of a
performance measure on the training data:
max
f∈F
m
i=1
E(π(qi, di, f), yi), (4)
where F is the set of possible ranking functions. This is equivalent
to minimizing the loss on the training data
min
f∈F
m
i=1
(1 − E(π(qi, di, f), yi)). (5)
It is difficult to directly optimize the loss, because E is a 
noncontinuous function and thus may be difficult to handle. We instead
attempt to minimize an upper bound of the loss in (5)
min
f∈F
m
i=1
exp{−E(π(qi, di, f), yi)}, (6)
because e−x
≥ 1 − x holds for any x ∈ . We consider the use of a
linear combination of weak rankers as our ranking model:
f(x) =
T
t=1
αtht(x). (7)
The minimization in (6) then turns out to be
min
ht∈H,αt∈ +
L(ht, αt) =
m
i=1
exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8)
where H is the set of possible weak rankers, αt is a positive weight,
and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Several ways of computing
coefficients αt and weak rankers ht may be considered. Following
the idea of AdaBoost, in AdaRank we take the approach of ‘forward
stage-wise additive modeling" [12] and get the algorithm in Figure
1. It can be proved that there exists a lower bound on the ranking
accuracy for AdaRank on training data, as presented in Theorem 1.
T 1. The following bound holds on the ranking 
accuracy of the AdaRank algorithm on training data:
1
m
m
i=1
E(π(qi, di, fT ), yi) ≥ 1 −
T
t=1
e−δt
min 1 − ϕ(t)2,
where ϕ(t) = m
i=1 Pt(i)E(π(qi, di, ht), yi), δt
min = mini=1,··· ,m δt
i, and
δt
i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi)
−αtE(π(qi, di, ht), yi),
for all i = 1, 2, · · · , m and t = 1, 2, · · · , T.
A proof of the theorem can be found in appendix. The theorem
implies that the ranking accuracy in terms of the performance 
measure can be continuously improved, as long as e−δt
min 1 − ϕ(t)2 < 1
holds.
3.4 Advantages
AdaRank is a simple yet powerful method. More importantly, it
is a method that can be justified from the theoretical viewpoint, as
discussed above. In addition AdaRank has several other advantages
when compared with the existing learning to rank methods such as
Ranking SVM, RankBoost, and RankNet.
First, AdaRank can incorporate any performance measure, 
provided that the measure is query based and in the range of [−1, +1].
Notice that the major IR measures meet this requirement. In 
contrast the existing methods only minimize loss functions that are
loosely related to the IR measures [16].
Second, the learning process of AdaRank is more efficient than
those of the existing learning algorithms. The time complexity of
AdaRank is of order O((k+T)·m·n log n), where k denotes the 
number of features, T the number of rounds, m the number of queries
in training data, and n is the maximum number of documents for
queries in training data. The time complexity of RankBoost, for
example, is of order O(T · m · n2
) [8].
Third, AdaRank employs a more reasonable framework for 
performing the ranking task than the existing methods. Specifically in
AdaRank the instances correspond to queries, while in the existing
methods the instances correspond to document pairs. As a result,
AdaRank does not have the following shortcomings that plague the
existing methods. (a) The existing methods have to make a strong
assumption that the document pairs from the same query are 
independently distributed. In reality, this is clearly not the case and this
problem does not exist for AdaRank. (b) Ranking the most relevant
documents on the tops of document lists is crucial for document 
retrieval. The existing methods cannot focus on the training on the
tops, as indicated in [4]. Several methods for rectifying the problem
have been proposed (e.g., [4]), however, they do not seem to 
fundamentally solve the problem. In contrast, AdaRank can naturally
focus on training on the tops of document lists, because the 
performance measures used favor rankings for which relevant documents
are on the tops. (c) In the existing methods, the numbers of 
document pairs vary from query to query, resulting in creating models
biased toward queries with more document pairs, as pointed out in
[4]. AdaRank does not have this drawback, because it treats queries
rather than document pairs as basic units in learning.
3.5 Differences from AdaBoost
AdaRank is a boosting algorithm. In that sense, it is similar to
AdaBoost, but it also has several striking differences from AdaBoost.
First, the types of instances are different. AdaRank makes use of
queries and their corresponding document lists as instances. The 
labels in training data are lists of ranks (relevance levels). AdaBoost
makes use of feature vectors as instances. The labels in training
data are simply +1 and −1.
Second, the performance measures are different. In AdaRank,
the performance measure is a generic measure, defined on the 
document list and the rank list of a query. In AdaBoost the 
corresponding performance measure is a specific measure for binary 
classification, also referred to as ‘margin" [25].
Third, the ways of updating weights are also different. In 
AdaBoost, the distribution of weights on training instances is 
calculated according to the current distribution and the performance of
the current weak learner. In AdaRank, in contrast, it is calculated
according to the performance of the ranking model created so far,
as shown in Figure 1. Note that AdaBoost can also adopt the weight
updating method used in AdaRank. For AdaBoost they are 
equivalent (cf., [12] page 305). However, this is not true for AdaRank.
3.6 Construction of Weak Ranker
We consider an efficient implementation for weak ranker 
construction, which is also used in our experiments. In the 
implementation, as weak ranker we choose the feature that has the optimal
weighted performance among all of the features:
max
k
m
i=1
Pt(i)E(π(qi, di, xk), yi).
Creating weak rankers in this way, the learning process turns out
to be that of repeatedly selecting features and linearly combining
the selected features. Note that features which are not selected in
the training phase will have a weight of zero.
4. EXPERIMENTAL RESULTS
We conducted experiments to test the performances of AdaRank
using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov.
Table 2: Features used in the experiments on OHSUMED,
WSJ, and AP datasets. C(w, d) represents frequency of word
w in document d; C represents the entire collection; n denotes
number of terms in query; | · | denotes the size function; and
id f(·) denotes inverse document frequency.
1 wi∈q d ln(c(wi, d) + 1) 2 wi∈q d ln( |C|
c(wi,C)
+ 1)
3 wi∈q d ln(id f(wi)) 4 wi∈q d ln(c(wi,d)
|d|
+ 1)
5 wi∈q d ln(c(wi,d)
|d|
· id f(wi) + 1) 6 wi∈q d ln(c(wi,d)·|C|
|d|·c(wi,C)
+ 1)
7 ln(BM25 score)
0.2
0.3
0.4
0.5
0.6
MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10
BM25
Ranking SVM
RarnkBoost
AdaRank.MAP
AdaRank.NDCG
Figure 2: Ranking accuracies on OHSUMED data.
4.1 Experiment Setting
Ranking SVM [13, 16] and RankBoost [8] were selected as 
baselines in the experiments, because they are the state-of-the-art 
learning to rank methods. Furthermore, BM25 [24] was used as a 
baseline, representing the state-of-the-arts IR method (we actually used
the tool Lemur1
).
For AdaRank, the parameter T was determined automatically
during each experiment. Specifically, when there is no 
improvement in ranking accuracy in terms of the performance measure, the
iteration stops (and T is determined). As the measure E, MAP and
NDCG@5 were utilized. The results for AdaRank using MAP and
NDCG@5 as measures in training are represented as AdaRank.MAP
and AdaRank.NDCG, respectively.
4.2 Experiment with OHSUMED Data
In this experiment, we made use of the OHSUMED dataset [14]
to test the performances of AdaRank. The OHSUMED dataset 
consists of 348,566 documents and 106 queries. There are in total
16,140 query-document pairs upon which relevance judgments are
made. The relevance judgments are either ‘d" (definitely relevant),
‘p" (possibly relevant), or ‘n"(not relevant). The data have been
used in many experiments in IR, for example [4, 29].
As features, we adopted those used in document retrieval [4].
Table 2 shows the features. For example, tf (term frequency), idf
(inverse document frequency), dl (document length), and 
combinations of them are defined as features. BM25 score itself is also a
feature. Stop words were removed and stemming was conducted in
the data.
We randomly divided queries into four even subsets and 
conducted 4-fold cross-validation experiments. We tuned the 
parameters for BM25 during one of the trials and applied them to the other
trials. The results reported in Figure 2 are those averaged over four
trials. In MAP calculation, we define the rank ‘d" as relevant and
1
http://www.lemurproject.com
Table 3: Statistics on WSJ and AP datasets.
Dataset # queries # retrieved docs # docs per query
AP 116 24,727 213.16
WSJ 126 40,230 319.29
0.40
0.45
0.50
0.55
0.60
MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10
BM25
Ranking SVM
RankBoost
AdaRank.MAP
AdaRank.NDCG
Figure 3: Ranking accuracies on WSJ dataset.
the other two ranks as irrelevant. From Figure 2, we see that both
AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking
SVM, and RankBoost in terms of all measures. We conducted 
significant tests (t-test) on the improvements of AdaRank.MAP over
BM25, Ranking SVM, and RankBoost in terms of MAP. The 
results indicate that all the improvements are statistically significant
(p-value < 0.05). We also conducted t-test on the improvements
of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost
in terms of NDCG@5. The improvements are also statistically
significant.
4.3 Experiment with WSJ and AP Data
In this experiment, we made use of the WSJ and AP datasets
from the TREC ad-hoc retrieval track, to test the performances of
AdaRank. WSJ contains 74,520 articles of Wall Street Journals
from 1990 to 1992, and AP contains 158,240 articles of 
Associated Press in 1988 and 1990. 200 queries are selected from the
TREC topics (No.101 ∼ No.300). Each query has a number of 
documents associated and they are labeled as ‘relevant" or ‘irrelevant"
(to the query). Following the practice in [28], the queries that have
less than 10 relevant documents were discarded. Table 3 shows the
statistics on the two datasets.
In the same way as in section 4.2, we adopted the features listed
in Table 2 for ranking. We also conducted 4-fold cross-validation
experiments. The results reported in Figure 3 and 4 are those 
averaged over four trials on WSJ and AP datasets, respectively. From
Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG
outperform BM25, Ranking SVM, and RankBoost in terms of all
measures on both WSJ and AP. We conducted t-tests on the 
improvements of AdaRank.MAP and AdaRank.NDCG over BM25,
Ranking SVM, and RankBoost on WSJ and AP. The results 
indicate that all the improvements in terms of MAP are statistically 
significant (p-value < 0.05). However only some of the improvements
in terms of NDCG@5 are statistically significant, although overall
the improvements on NDCG scores are quite high (1-2 points).
4.4 Experiment with .Gov Data
In this experiment, we further made use of the TREC .Gov data
to test the performance of AdaRank for the task of web retrieval.
The corpus is a crawl from the .gov domain in early 2002, and
has been used at TREC Web Track since 2002. There are a total
0.40
0.45
0.50
0.55
MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10
BM25
Ranking SVM
RankBoost
AdaRank.MAP
AdaRank.NDCG
Figure 4: Ranking accuracies on AP dataset.
0.1
0.2
0.3
0.4
0.5
0.6
0.7
MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10
BM25
Ranking SVM
RankBoost
AdaRank.MAP
AdaRank.NDCG
Figure 5: Ranking accuracies on .Gov dataset.
Table 4: Features used in the experiments on .Gov dataset.
1 BM25 [24] 2 MSRA1000 [27]
3 PageRank [21] 4 HostRank [30]
5 Relevance Propagation [23] (10 features)
of 1,053,110 web pages with 11,164,829 hyperlinks in the data.
The 50 queries in the topic distillation task in the Web Track of
TREC 2003 [6] were used. The ground truths for the queries are
provided by the TREC committee with binary judgment: relevant
or irrelevant. The number of relevant pages vary from query to
query (from 1 to 86).
We extracted 14 features from each query-document pair. 
Table 4 gives a list of the features. They are the outputs of some
well-known algorithms (systems). These features are different from
those in Table 2, because the task is different.
Again, we conducted 4-fold cross-validation experiments. The
results averaged over four trials are reported in Figure 5. From the
results, we can see that AdaRank.MAP and AdaRank.NDCG 
outperform all the baselines in terms of all measures. We conducted 
ttests on the improvements of AdaRank.MAP and AdaRank.NDCG
over BM25, Ranking SVM, and RankBoost. Some of the 
improvements are not statistically significant. This is because we have only
50 queries used in the experiments, and the number of queries is
too small.
4.5 Discussions
We investigated the reasons that AdaRank outperforms the 
baseline methods, using the results of the OHSUMED dataset as examples.
First, we examined the reason that AdaRank has higher 
performances than Ranking SVM and RankBoost. Specifically we 
com0.58
0.60
0.62
0.64
0.66
0.68
d-n d-p p-n
accuracy
pair type
Ranking SVM
RankBoost
AdaRank.MAP
AdaRank.NDCG
Figure 6: Accuracy on ranking document pairs with
OHSUMED dataset.
0
2
4
6
8
10
12
numberofqueries
number of document pairs per query
Figure 7: Distribution of queries with different number of 
document pairs in training data of trial 1.
pared the error rates between different rank pairs made by 
Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the
test data. The results averaged over four trials in the 4-fold cross
validation are shown in Figure 6. We use ‘d-n" to stand for the pairs
between ‘definitely relevant" and ‘not relevant", ‘d-p" the pairs 
between ‘definitely relevant" and ‘partially relevant", and ‘p-n" the
pairs between ‘partially relevant" and ‘not relevant". From 
Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make
fewer errors for ‘d-n" and ‘d-p", which are related to the tops of
rankings and are important. This is because AdaRank.MAP and
AdaRank.NDCG can naturally focus upon the training on the tops
by optimizing MAP and NDCG@5, respectively.
We also made statistics on the number of document pairs per
query in the training data (for trial 1). The queries are clustered into
different groups based on the the number of their associated 
document pairs. Figure 7 shows the distribution of the query groups. In
the figure, for example, ‘0-1k" is the group of queries whose 
number of document pairs are between 0 and 999. We can see that the
numbers of document pairs really vary from query to query. Next
we evaluated the accuracies of AdaRank.MAP and RankBoost in
terms of MAP for each of the query group. The results are reported
in Figure 8. We found that the average MAP of AdaRank.MAP
over the groups is two points higher than RankBoost. Furthermore,
it is interesting to see that AdaRank.MAP performs particularly
better than RankBoost for queries with small numbers of document
pairs (e.g., ‘0-1k", ‘1k-2k", and ‘2k-3k"). The results indicate that
AdaRank.MAP can effectively avoid creating a model biased 
towards queries with more document pairs. For AdaRank.NDCG,
similar results can be observed.
0.2
0.3
0.4
0.5
MAP
query group
RankBoost
AdaRank.MAP
Figure 8: Differences in MAP for different query groups.
0.30
0.31
0.32
0.33
0.34
trial 1 trial 2 trial 3 trial 4
MAP
AdaRank.MAP
AdaRank.NDCG
Figure 9: MAP on training set when model is trained with MAP
or NDCG@5.
We further conducted an experiment to see whether AdaRank has
the ability to improve the ranking accuracy in terms of a measure
by using the measure in training. Specifically, we trained ranking
models using AdaRank.MAP and AdaRank.NDCG and evaluated
their accuracies on the training dataset in terms of both MAP and
NDCG@5. The experiment was conducted for each trial. Figure
9 and Figure 10 show the results in terms of MAP and NDCG@5,
respectively. We can see that, AdaRank.MAP trained with MAP
performs better in terms of MAP while AdaRank.NDCG trained
with NDCG@5 performs better in terms of NDCG@5. The results
indicate that AdaRank can indeed enhance ranking performance in
terms of a measure by using the measure in training.
Finally, we tried to verify the correctness of Theorem 1. That is,
the ranking accuracy in terms of the performance measure can be
continuously improved, as long as e−δt
min 1 − ϕ(t)2 < 1 holds. As
an example, Figure 11 shows the learning curve of AdaRank.MAP
in terms of MAP during the training phase in one trial of the cross
validation. From the figure, we can see that the ranking accuracy
of AdaRank.MAP steadily improves, as the training goes on, until
it reaches to the peak. The result agrees well with Theorem 1.
5. CONCLUSION AND FUTURE WORK
In this paper we have proposed a novel algorithm for learning
ranking models in document retrieval, referred to as AdaRank. In
contrast to existing methods, AdaRank optimizes a loss function
that is directly defined on the performance measures. It employs
a boosting technique in ranking model learning. AdaRank offers
several advantages: ease of implementation, theoretical soundness,
efficiency in training, and high accuracy in ranking. Experimental
results based on four benchmark datasets show that AdaRank can
significantly outperform the baseline methods of BM25, Ranking
SVM, and RankBoost.
0.49
0.50
0.51
0.52
0.53
trial 1 trial 2 trial 3 trial 4
NDCG@5
AdaRank.MAP
AdaRank.NDCG
Figure 10: NDCG@5 on training set when model is trained
with MAP or NDCG@5.
0.29
0.30
0.31
0.32
0 50 100 150 200 250 300 350
MAP
number of rounds
Figure 11: Learning curve of AdaRank.
Future work includes theoretical analysis on the generalization
error and other properties of the AdaRank algorithm, and further
empirical evaluations of the algorithm including comparisons with
other algorithms that can directly optimize performance measures.
6. ACKNOWLEDGMENTS
We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin
Gao, Robert Schapire, and Andrew Arnold for their valuable 
comments and suggestions to this paper.
7. REFERENCES
[1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information
Retrieval. Addison Wesley, May 1999.
[2] C. Burges, R. Ragno, and Q. Le. Learning to rank with
nonsmooth cost functions. In Advances in Neural
Information Processing Systems 18, pages 395-402. MIT
Press, Cambridge, MA, 2006.
[3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank using
gradient descent. In ICML 22, pages 89-96, 2005.
[4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon.
Adapting ranking SVM to document retrieval. In SIGIR 29,
pages 186-193, 2006.
[5] D. Cossock and T. Zhang. Subset ranking using regression.
In COLT, pages 605-619, 2006.
[6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.
Overview of the TREC 2003 web track. In TREC, pages
78-92, 2003.
[7] N. Duffy and D. Helmbold. Boosting methods for regression.
Mach. Learn., 47(2-3):153-200, 2002.
[8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. An
efficient boosting algorithm for combining preferences.
Journal of Machine Learning Research, 4:933-969, 2003.
[9] Y. Freund and R. E. Schapire. A decision-theoretic
generalization of on-line learning and an application to
boosting. J. Comput. Syst. Sci., 55(1):119-139, 1997.
[10] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic
regression: A statistical view of boosting. The Annals of
Statistics, 28(2):337-374, 2000.
[11] G. Fung, R. Rosales, and B. Krishnapuram. Learning
rankings via convex hull separation. In Advances in Neural
Information Processing Systems 18, pages 395-402. MIT
Press, Cambridge, MA, 2006.
[12] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of
Statistical Learning. Springer, August 2001.
[13] R. Herbrich, T. Graepel, and K. Obermayer. Large Margin
rank boundaries for ordinal regression. MIT Press,
Cambridge, MA, 2000.
[14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.
Ohsumed: an interactive retrieval evaluation and new large
test collection for research. In SIGIR, pages 192-201, 1994.
[15] K. Jarvelin and J. Kekalainen. IR evaluation methods for
retrieving highly relevant documents. In SIGIR 23, pages
41-48, 2000.
[16] T. Joachims. Optimizing search engines using clickthrough
data. In SIGKDD 8, pages 133-142, 2002.
[17] T. Joachims. A support vector method for multivariate
performance measures. In ICML 22, pages 377-384, 2005.
[18] J. Lafferty and C. Zhai. Document language models, query
models, and risk minimization for information retrieval. In
SIGIR 24, pages 111-119, 2001.
[19] D. A. Metzler, W. B. Croft, and A. McCallum. Direct
maximization of rank-based metrics for information
retrieval. Technical report, CIIR, 2005.
[20] R. Nallapati. Discriminative models for information retrieval.
In SIGIR 27, pages 64-71, 2004.
[21] L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web.
Technical report, Stanford Digital Library Technologies
Project, 1998.
[22] J. M. Ponte and W. B. Croft. A language modeling approach
to information retrieval. In SIGIR 21, pages 275-281, 1998.
[23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma. A
study of relevance propagation for web search. In SIGIR 28,
pages 408-415, 2005.
[24] S. E. Robertson and D. A. Hull. The TREC-9 filtering track
final report. In TREC, pages 25-40, 2000.
[25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee. Boosting
the margin: A new explanation for the effectiveness of voting
methods. In ICML 14, pages 322-330, 1997.
[26] R. E. Schapire and Y. Singer. Improved boosting algorithms
using confidence-rated predictions. Mach. Learn.,
37(3):297-336, 1999.
[27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng,
J. Zhang, G. Xue, and W.-Y. Ma. Microsoft Research Asia at
web track and terabyte track of TREC 2004. In TREC, 2004.
[28] A. Trotman. Learning to rank. Inf. Retr., 8(3):359-381, 2005.
[29] J. Xu, Y. Cao, H. Li, and Y. Huang. Cost-sensitive learning
of SVM for ranking. In ECML, pages 833-840, 2006.
[30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu, and Z. Chen.
Exploiting the hierarchical structure for link analysis. In
SIGIR 28, pages 186-193, 2005.
[31] H. Yu. SVM selective sampling for ranking with application
to data retrieval. In SIGKDD 11, pages 354-363, 2005.
APPENDIX
Here we give the proof of Theorem 1.
P. Set ZT = m
i=1 exp {−E(π(qi, di, fT ), yi)} and φ(t) = 1
2
(1 +
ϕ(t)). According to the definition of αt, we know that eαt = φ(t)
1−φ(t)
.
ZT =
m
i=1
exp {−E(π(qi, di, fT−1 + αT hT ), yi)}
=
m
i=1
exp −E(π(qi, di, fT−1), yi) − αT E(π(qi, di, hT ), yi) − δT
i
≤
m
i=1
exp {−E(π(qi, di, fT−1), yi)} exp {−αT E(π(qi, di, hT ), yi)} e−δT
min
= e−δT
min ZT−1
m
i=1
exp {−E(π(qi, di, fT−1), yi)}
ZT−1
exp{−αT E(π(qi, di, hT ), yi)}
= e−δT
min ZT−1
m
i=1
PT (i) exp{−αT E(π(qi, di, hT ), yi)}.
Moreover, if E(π(qi, di, hT ), yi) ∈ [−1, +1] then,
ZT ≤ e−δT
minZT−1
m
i=1
PT (i)
1+E(π(qi, di, hT ), yi)
2
e−αT
+
1−E(π(qi, di, hT ), yi)
2
eαT
= e−δT
min ZT−1

φ(T)
1 − φ(T)
φ(T)
+ (1 − φ(T))
φ(T)
1 − φ(T)


= ZT−1e−δT
min 4φ(T)(1 − φ(T))
≤ ZT−2
T
t=T−1
e−δt
min 4φ(t)(1 − φ(t))
≤ Z1
T
t=2
e−δt
min 4φ(t)(1 − φ(t))
= m
m
i=1
1
m
exp{−E(π(qi, di, α1h1), yi)}
T
t=2
e−δt
min 4φ(t)(1 − φ(t))
= m
m
i=1
1
m
exp{−α1E(π(qi, di, h1), yi) − δ1
i }
T
t=2
e−δt
min 4φ(t)(1 − φ(t))
≤ me−δ1
min
m
i=1
1
m
exp{−α1E(π(qi, di, h1), yi)}
T
t=2
e−δt
min 4φ(t)(1 − φ(t))
≤ m e−δ1
min 4φ(1)(1 − φ(1))
T
t=2
e−δt
min 4φ(t)(1 − φ(t))
= m
T
t=1
e−δt
min 1 − ϕ(t)2.
∴
1
m
m
i=1
E(π(qi, di, fT ), yi) ≥
1
m
m
i=1
{1 − exp(−E(π(qi, di, fT ), yi))}
≥ 1 −
T
t=1
e−δt
min 1 − ϕ(t)2.
HITS Hits 
TRECExploring IR Evaluation Results with Network Analysis
Stefano Mizzaro
Dept. of Mathematics and Computer Science
University of Udine
Via delle Scienze, 206 - 33100 Udine, Italy
mizzaro@dimi.uniud.it
Stephen Robertson
Microsoft Research
7 JJ Thomson Avenue
Cambridge CB3 0FB, UK
ser@microsoft.com
ABSTRACT
We propose a novel method of analysing data gathered from
TREC or similar information retrieval evaluation 
experiments. We define two normalized versions of average 
precision, that we use to construct a weighted bipartite graph
of TREC systems and topics. We analyze the meaning of
well known - and somewhat generalized - indicators from
social network analysis on the Systems-Topics graph. We
apply this method to an analysis of TREC 8 data; among
the results, we find that authority measures systems 
performance, that hubness of topics reveals that some topics are
better than others at distinguishing more or less effective
systems, that with current measures a system that wants to
be effective in TREC needs to be effective on easy topics,
and that by using different effectiveness measures this is no
longer the case.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval
General Terms
Measurement, Experimentation
1. INTRODUCTION
Evaluation is a primary concern in the Information 
Retrieval (IR) field. TREC (Text REtrieval Conference) [12,
15] is an annual benchmarking exercise that has become a
de facto standard in IR evaluation: before the actual 
conference, TREC provides to participants a collection of 
documents and a set of topics (representations of information
needs). Participants use their systems to retrieve, and 
submit to TREC, a list of documents for each topic. After the
lists have been submitted and pooled, the TREC organizers
employ human assessors to provide relevance judgements on
the pooled set. This defines a set of relevant documents for
each topic. System effectiveness is then measured by well 
established metrics (Mean Average Precision being the most
used). Other conferences such as NTCIR, INEX, CLEF 
provide comparable data.
Network analysis is a discipline that studies features and
properties of (usually large) networks, or graphs. Of 
particular importance is Social Network Analysis [16], that studies
networks made up by links among humans (friendship, 
acquaintance, co-authorship, bibliographic citation, etc.).
Network analysis and IR fruitfully meet in Web Search
Engine implementation, as is already described in textbooks
[3,6]. Current search engines use link analysis techniques to
help rank the retrieved documents. Some indicators (and
the corresponding algorithms that compute them) have been
found useful in this respect, and are nowadays well known:
Inlinks (the number of links to a Web page), PageRank [7],
and HITS (Hyperlink-Induced Topic Search) [5]. Several
extensions to these algorithms have been and are being 
proposed. These indicators and algorithms might be quite 
general in nature, and can be used for applications which are
very different from search result ranking. One example is
using HITS for stemming, as described by Agosti et al. [1].
In this paper, we propose and demonstrate a method
for constructing a network, specifically a weighted complete
bidirectional directed bipartite graph, on a set of TREC 
topics and participating systems. Links represent effectiveness
measurements on system-topic pairs. We then apply 
analysis methods originally developed for search applications to
the resulting network. This reveals phenomena previously
hidden in TREC data. In passing, we also provide a small
generalization to Kleinberg"s HITS algorithm, as well as to
Inlinks and PageRank.
The paper is organized as follows: Sect. 2 gives some 
motivations for the work. Sect. 3 presents the basic ideas of
normalizing average precision and of constructing a 
systemstopics graph, whose properties are analyzed in Sect. 4; Sect. 5
presents some experiments on TREC 8 data; Sect. 6 
discusses some issues and Sect. 7 closes the paper.
2. MOTIVATIONS
We are interested in the following hypotheses:
1. Some systems are more effective than others;
t1 · · · tn MAP
s1 AP(s1, t1) · · · AP(s1, tn) MAP(s1)
...
...
...
sm AP(sm, t1) · · · AP(sm, tn) MAP(sm)
AAP AAP(t1) · · · AAP(tn)
(a)
t1 t2 · · · MAP
s1 0.5 0.4 · · · 0.6
s2 0.4 · · · · · · 0.3
...
...
...
...
AAP 0.6 0.3 · · ·
(b)
Table 1: AP, MAP and AAP
2. Some topics are easier than others;
3. Some systems are better than others at distinguishing
easy and difficult topics;
4. Some topics are better than others at distinguishing
more or less effective systems.
The first of these hypotheses needs no further justification
- every reported significant difference between any two 
systems supports it. There is now also quite a lot of evidence
for the second, centered on the TREC Robust Track [14].
Our primary interest is in the third and fourth. The third
might be regarded as being of purely academic interest; 
however, the fourth has the potential for being of major 
practical importance in evaluation studies. If we could identify
a relatively small number of topics which were really good
at distinguishing effective and ineffective systems, we could
save considerable effort in evaluating systems.
One possible direction from this point would be to attempt
direct identification of such small sets of topics. However, in
the present paper, we seek instead to explore the 
relationships suggested by the hypotheses, between what different
topics tell us about systems and what different systems tell
us about topics. We seek methods of building and analysing
a matrix of system-topic normalised performances, with a
view to giving insight into the issue and confirming or 
refuting the third and fourth hypotheses. It turns out that
the obvious symmetry implied by the above formulation of
the hypotheses is a property worth investigating, and the
investigation does indeed give us valuable insights.
3. THE IDEA
3.1 1st step: average precision table
From TREC results, one can produce an Average 
Precision (AP) table (see Tab. 1a): each AP(si, tj) value 
measures the AP of system si on topic tj.
Besides AP values, the table shows Mean Average 
Precision (MAP) values i.e., the mean of the AP values for a
single system over all topics, and what we call Average 
Average Precision (AAP) values i.e., the average of the AP
values for a single topic over all systems:
MAP(si) =
1
n
nX
j=1
AP(si, tj), (1)
AAP(tj) =
1
m
mX
i=1
AP(si, tj). (2)
MAPs are indicators of systems performance: higher MAP
means good system. AAP are indicators of the performance
on a topic: higher AAP means easy topic - a topic on which
all or most systems have good performance.
3.2 Critique of pure AP
MAP is a standard, well known, and widely used IR 
effectiveness measure. Single AP values are used too (e.g.,
in AP histograms). Topic difficulty is often discussed (e.g.,
in TREC Robust track [14]), although AAP values are not
used and, to the best of our knowledge, have never been
proposed (the median, not the average, of AP on a topic
is used to produce TREC AP histograms [11]). However,
the AP values in Tab. 1 present two limitations, which are
symmetric in some respect:
• Problem 1. They are not reliable to compare the
effectiveness of a system on different topics, relative
to the other systems. If, for example, AP(s1, t1) >
AP(s1, t2), can we infer that s1 is a good system (i.e.,
has a good performance) on t1 and a bad system on
t2? The answer is no: t1 might be an easy topic (with
high AAP) and t2 a difficult one (low AAP). See an
example in Tab. 1b: s1 is outperformed (on average)
by the other systems on t1, and it outperforms the
other systems on t2.
• Problem 2. Conversely, if, for example, AP(s1, t1) >
AP(s2, t1), can we infer that t1 is considered easier
by s1 than by s2? No, we cannot: s1 might be a good
system (with high MAP) and s2 a bad one (low MAP);
see an example in Tab. 1b.
These two problems are a sort of breakdown of the well
known high influence of topics on IR evaluation; again, our
formulation makes explicit the topics / systems symmetry.
3.3 2nd step: normalizations
To avoid these two problems, we can normalize the AP
table in two ways. The first normalization removes the 
influence of the single topic ease on system performance. Each
AP(si, tj) value in the table depends on both system 
goodness and topic ease (the value will increase if a system is
good and/or the topic is easy). By subtracting from each
AP(si, tj) the AAP(tj) value, we obtain normalized AP
values (APA(si, tj), Normalized AP according to AAP):
APA(si, tj) = AP(si, tj) − AAP(tj), (3)
that depend on system performance only (the value will 
increase only if system performance is good). See Tab. 2a.
The second normalization removes the influence of the 
single system effectiveness on topic ease: by subtracting from
each AP(si, tj) the MAP(si) value, we obtain normalized
AP values (APM(si, tj), Normalized AP according to MAP):
APM(si, tj) = AP(si, tj) − MAP(si), (4)
that depend on topic ease only (the value will increase only
if the topic is easy, i.e., all systems perform well on that
topic). See Tab. 2b.
In other words, APA avoids Problem 1: APA(s, t) values
measure the performance of system s on topic t normalized
t1 · · · tn MAP
s1 APA(s1, t1) · · · APA(s1, tn) MAP(s1)
...
...
...
sm APA(sm, t1) · · · APA(sm, tn) MAP(sm)
0 · · · 0 0
(a)
t1 · · · tn
s1 APM(s1, t1) · · · APM(s1, tn) 0
...
...
...
sm APM(sm, t1) · · · APM(sm, tn) 0
AAP AAP(t1) · · · AAP(tn) 0
(b)
t1 t2 · · · MAP
s1 −0.1 0.1 · · · . . .
s2 0.2 · · · · · · . . .
...
...
...
0 0 · · ·
t1 t2 · · ·
s1 −0.1 −0.2 · · · 0
s2 0.1 · · · · · · 0
...
...
...
AAP . . . . . . · · ·
(c) (d)
Table 2: Normalizations: APA and MAP: normalized
AP (APA) and MAP (MAP) (a); normalized AP (APM)
and AAP (AAP) (b); a numeric example (c) and (d)
according to the ease of the topic (easy topics will not have
higher APA values). Now, if, for example, APA(s1, t2) >
APA(s1, t1), we can infer that s1 is a good system on t2 and
a bad system on t1 (see Tab. 2c). Vice versa, APM avoids
Problem 2: APM(s, t) values measure the ease of topic t
according to system s, normalized according to goodness
of the system (good systems will not lead to higher APM
values). If, for example, APM(s2, t1) > APM(s1, t1), we
can infer that t1 is considered easier by s2 than by s1 (see
Tab. 2d).
On the basis of Tables 2a and 2b, we can also define two
new measures of system effectiveness and topic ease, i.e., a
Normalized MAP (MAP), obtained by averaging the APA
values on one row in Tab. 2a, and a Normalized AAP (AAP),
obtained by averaging the APM values on one column in
Tab. 2b:
MAP(si) =
1
n
nX
j=1
APA(si, tj) (5)
AAP(tj) =
1
m
mX
i=1
APM(si, tj). (6)
Thus, overall system performance can be measured, 
besides by means of MAP, also by means of MAP. Moreover,
MAP is equivalent to MAP, as can be immediately proved
by using Eqs. (5), (3), and (1):
MAP(si) =
1
n
nX
j=1
(AP(si, tj) − AAP(tj)) =
= MAP(si) −
1
n
nX
j=1
AAP(tj)
(and 1
n
Pn
j=1 AAP(tj) is the same for all systems). And,
conversely, overall topic ease can be measured, besides by
t1 · · · tn
s1
... APM
sm
t1 · · · tn
s1
... APA
sm
s1 · · · sm t1 · · · tn
s1
... 0 APM 0
sm
t1
... APA
T
0 0
tn
MAP AAP 0
Figure 1: Construction of the adjacency matrix.
APA
T
is the transpose of APA.
means of AAP, also by means of AAP, and this is equivalent
(the proof is analogous, and relies on Eqs. (6), (4), and (2)).
The two Tables 2a and 2b are interesting per se, and can
be analyzed in several different ways. In the following we
propose an analysis based on network analysis techniques,
mainly Kleinberg"s HITS algorithm. There is a little further
discussion of these normalizations in Sect. 6.
3.4 3rd step: Systems-Topics Graph
The two tables 2a and 2b can be merged into a single one
with the procedure shown in Fig. 1. The obtained matrix
can be interpreted as the adjacency matrix of a complete
weighted bipartite graph, that we call Systems-Topics graph.
Arcs and weights in the graph can be interpreted as follows:
• (weight on) arc s → t: how much the system s thinks
that the topic t is easy - assuming that a system has
no knowledge of the other systems (or in other words,
how easy we might think the topic is, knowing only
the results for this one system). This corresponds to
APM values, i.e., to normalized topic ease (Fig. 2a).
• (weight on) arc s ← t: how much the topic t thinks
that the system s is good - assuming that a topic has
no knowledge of the other topics (or in other words,
how good we might think the system is, knowing only
the results for this one topic). This corresponds to
APA (normalized system effectiveness, Fig. 2b).
Figs. 2c and 2d show the Systems-Topics complete weighted
bipartite graph, on a toy example with 4 systems and 2 
topics; the graph is split in two parts to have an understandable
graphical representation: arcs in Fig. 2c are labeled with
APM values; arcs in Fig. 2d are labeled with APA values.
4. ANALYSIS OF THE GRAPH
4.1 Weighted Inlinks, Outlinks, PageRank
The sum of weighted outlinks, i.e., the sum of the weights
on the outgoing arcs from each node, is always zero:
• The outlinks on each node corresponding to a system
s (Fig. 2c) is the sum of all the corresponding APM
values on one row of the matrix in Tab. 2b.
• The outlinks on each node corresponding to a topic
t (Fig. 2d) is the sum of all the corresponding APA
(a) (b)
(c) (d)
Figure 2: The relationships between systems and
topics (a) and (b); and the Systems-Topics graph for
a toy example (c) and (d). Dashed arcs correspond
to negative values.
h
(a)
s1
...
sm
t1
...
tn
=
s1 · · · sm t1 · · · tn
s1
... 0 APM
(APA)
sm
t1
... APA
T
0
tn (APM
T
)
·
a
(h)
s1
...
sm
t1
...
tn
Figure 3: Hub and Authority computation
values on one row of the transpose of the matrix in
Tab. 2a.
The average1
of weighted inlinks is:
• MAP for each node corresponding to a system s; this
corresponds to the average of all the corresponding
APA values on one column of the APA
T
part of the
adjacency matrix (see Fig. 1).
• AAP for each node corresponding to a topic t; this
corresponds to the average of all the corresponding
APM values on one column of the APM part of the
adjacency matrix (see Fig. 1).
Therefore, weighted inlinks measure either system 
effectiveness or topic ease; weighted outlinks are not meaningful. We
could also apply the PageRank algorithm to the network;
the meaning of the PageRank of a node is not quite so 
obvious as Inlinks and Outlinks, but it also seems a sensible
measure of either system effectiveness or topic ease: if a 
system is effective, it will have several incoming links with high
1
Usually, the sum of the weights on the incoming arcs to
each node is used in place of the average; since the graph is
complete, it makes no difference.
weights (APA); if a topic is easy it will have high weights
(APM) on the incoming links too. We will see experimental
confirmation in the following.
4.2 Hubs and Authorities
Let us now turn to more sophisticated indicators. 
Kleinberg"s HITS algorithm defines, for a directed graph, two
indicators: hubness and authority; we reiterate here some of
the basic details of the HITS algorithm in order to 
emphasize both the nature of our generalization and the 
interpretation of the HITS concepts in this context. Usually, hubness
and authority are defined as h(x) =
P
x→y a(y) and a(x) =
P
y→x h(y), and described intuitively as a good hub links
many good authorities; a good authority is linked from many
good hubs. As it is well known, an equivalent formulation
in linear algebra terms is (see also Fig. 3):
h = Aa and a = AT
h (7)
(where h is the hubness vector, with the hub values for all
the nodes; a is the authority vector; A is the adjacency 
matrix of the graph; and AT
its transpose). Usually, A 
contains 0s and 1s only, corresponding to presence and absence
of unweighted directed arcs, but Eq. (7) can be immediately
generalized to (in fact, it is already valid for) A containing
any real value, i.e., to weighted graphs.
Therefore we can have a generalized version (or rather
a generalized interpretation, since the formulation is still
the original one) of hubness and authority for all nodes in
a graph. An intuitive formulation of this generalized HITS
is still available, although slightly more complex: a good
hub links, by means of arcs having high weights, many good
authorities; a good authority is linked, by means of arcs 
having high weights, from many good hubs. Since arc weights
can be, in general, negative, hub and authority values can be
negative, and one could speak of unhubness and unauthority;
the intuitive formulation could be completed by adding that
a good hub links good unauthorities by means of links with
highly negative weights; a good authority is linked by good
unhubs by means of links with highly negative weights.
And, also, a good unhub links positively good 
unauthorities and negatively good authorities; a good unauthority
is linked positively from good unhubs and negatively from
good hubs.
Let us now apply generalized HITS to our Systems-Topics
graph. We compute a(s), h(s), a(t), and h(t). Intuitively,
we expect that a(s) is somehow similar to Inlinks, so it
should be a measure of either systems effectiveness or topic
ease. Similarly, hubness should be more similar to Outlinks,
thus less meaningful, although the interplay between hub
and authority might lead to the discovery of something 
different. Let us start by remarking that authority of topics
and hubness of systems depend only on each other; similarly
hubness of topics and authority of systems depend only on
each other: see Figs. 2c, 2d and 3.
Thus the two graphs in Figs. 2c and 2d can be analyzed
independently. In fact the entire HITS analysis could be
done in one direction only, with just APM(s, t) values or
alternatively with just APA(s, t). As discussed below, 
probably most interest resides in the hubness of topics and the
authority of systems, so the latter makes sense. However, in
this paper, we pursue both analyses together, because the
symmetry itself is interesting.
Considering Fig. 2c we can state that:
• Authority a(t) of a topic node t increases when:
- if h(si) > 0, APM(si, t) increases
(or if APM(si, t) > 0, h(si) increases);
- if h(si) < 0, APM(si, t) decreases
(or if APM(si, t) < 0, h(si) decreases).
• Hubness h(s) of a system node s increases when:
- if a(tj) > 0, APM(s, tj) increases
(or, if APM(s, tj) > 0, a(tj) increases);
- if a(tj) < 0, APM(s, tj) decreases
(or, if APM(s, tj) < 0, a(tj) decreases).
We can summarize this as: a(t) is high if APM(s, t) is high
for those systems with high h(s); h(s) is high if APM(s, t)
is high for those topics with high a(t). Intuitively, authority
a(t) of a topic measures topic ease; hubness h(s) of a system
measures system"s capability to recognize easy topics. A
system with high unhubness (negative hubness) would tend
to regard easy topics as hard and hard ones as easy.
The situation for Fig. 2d, i.e., for a(s) and h(t), is 
analogous. Authority a(s) of a system node s measures system
effectiveness: it increases with the weight on the arc (i.e.,
APA(s, tj)) and the hubness of the incoming topic nodes tj.
Hubness h(t) of a topic node t measures topic capability to
recognize effective systems: if h(t) > 0, it increases further
if APA(s, tj) increases; if h(t) < 0, it increases if APA(s, tj)
decreases.
Intuitively, we can state that A system has a higher 
authority if it is more effective on topics with high hubness;
and A topic has a higher hubness if it is easier for those
systems which are more effective in general. Conversely for
system hubness and topic authority: A topic has a higher
authority if it is easier on systems with high hubness; and
A system has a higher hubness if it is more effective for
those topics which are easier in general.
Therefore, for each system we have two indicators: 
authority (a(s)), measuring system effectiveness, and hubness
(h(s)), measuring system capability to estimate topic ease.
And for each topic, we have two indicators: authority (a(t)),
measuring topic ease, and hubness (h(t)), measuring topic
capability to estimate systems effectiveness. We can define
them formally as
a(s) =
X
t
h(t) · APA(s, t), h(t) =
X
s
a(s) · APA(s, t),
a(t) =
X
s
h(s) · APM(s, t), h(s) =
X
t
a(t) · APM(s, t).
We observe that the hubness of topics may be of particular
interest for evaluation studies. It may be that we can 
evaluate the effectiveness of systems efficiently by using relatively
few high-hubness topics.
5. EXPERIMENTS
We now turn to discuss if these indicators are meaningful
and useful in practice, and how they correlate with standard
measures used in TREC. We have built the Systems-Topics
graph for TREC 8 data (featuring 1282
systems - actually,
2
Actually, TREC 8 data features 129 systems; due to
some bug in our scripts, we did not include one system
(8manexT3D1N0), but the results should not be affected.
0
0.1
0.2
0.3
-1 -0.5 0 0.5 1
NAPM
NAPA
AP
Figure 4: Distributions of AP, APA, and APM values
in TREC 8 data
MAP In PR H A
MAP 1 1.0 1.0 .80 .99
Inlinks 1 1.0 .80 .99
PageRank 1 .80 .99
Hub 1 .87
(a)
AAP In PR H A
AAP 1 1.0 1.0 .92 1.0
Inlinks 1 1.0 .92 1.0
PageRank 1 .92 1.0
Hub 1 .93
(b)
Table 3: Correlations between network analysis
measures and MAP (a) and AAP (b)
runs - on 50 topics). This section illustrates the results 
obtained mining these data according to the method presented
in previous sections.
Fig. 4 shows the distributions of AP, APA, and APM:
whereas AP is very skewed, both APA and APM are much
more symmetric (as it should be, since they are constructed
by subtracting the mean). Tables 3a and 3b show the 
Pearson"s correlation values between Inlinks, PageRank, Hub,
Authority and, respectively, MAP or AAP (Outlinks 
values are not shown since they are always zero, as seen in
Sect. 4). As expected, Inlinks and PageRank have a perfect
correlation with MAP and AAP. Authority has a very high
correlation too with MAP and AAP; Hub assumes slightly
lower values.
Let us analyze the correlations more in detail. The 
correlations chart in Figs. 5a and 5b demonstrate the high 
correlation between Authority and MAP or AAP. Hubness
presents interesting phenomena: both Fig. 5c (correlation
with MAP) and Fig. 5d (correlation with AAP) show that
correlation is not exact, but neither is it random. This, given
the meaning of hubness (capability in estimating topic ease
and system effectiveness), means two things: (i) more 
effective systems are better at estimating topic ease; and (ii)
easier topics are better at estimating system effectiveness.
Whereas the first statement is fine (there is nothing against
it), the second is a bit worrying. It means that system 
effectiveness in TREC is affected more by easy topics than by
difficult topics, which is rather undesirable for quite obvious
reasons: a system capable of performing well on a difficult
topic, i.e., on a topic on which the other systems perform
badly, would be an important result for IR effectiveness; 
con-8E-5
-6E-5
-4E-5
-2E-5
0E+0
2E-5
4E-5
6E-5
0.0 0.1 0.2 0.3 0.4 0.5
-3E-1
-2E-1
-1E-1
0E+0
1E-1
2E-1
3E-1
4E-1
5E-1
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
(a) (b)
0E+0
2E-2
4E-2
6E-2
8E-2
1E-1
1E-1
1E-1
0.0 0.1 0.2 0.3 0.4 0.5
0E+0
1E-5
2E-5
3E-5
4E-5
5E-5
6E-5
7E-5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
(c) (d)
Figure 5: Correlations: MAP (x axis) and authority (y axis) of systems (a); AAP and authority of topics
(b); MAP and hub of systems (c) and AAP and hub of topics (d)
versely, a system capable of performing well on easy topics
is just a confirmation of the state of the art. Indeed, the 
correlation between hubness and AAP (statement (i) above) is
higher than the correlation between hubness and MAP 
(corresponding to statement (ii)): 0.92 vs. 0.80. However, this
phenomenon is quite strong. This is also confirmed by the
work being done on the TREC Robust Track [14].
In this respect, it is interesting to see what happens if we
use a different measure from MAP (and AAP). The GMAP
(Geometric MAP) metric is defined as the geometric mean of
AP values, or equivalently as the arithmetic mean of the 
logarithms of AP values [8]. GMAP has the property of giving
more weight to the low end of the AP scale (i.e., to low AP
values), and this seems reasonable, since, intuitively, a 
performance increase in MAP values from 0.01 to 0.02 should
be more important than an increase from 0.81 to 0.82. To
use GMAP in place of MAP and AAP, we only need to take
the logarithms of initial AP values, i.e., those in Tab. 1a
(zero values are modified into ε = 0.00001). We then repeat
the same normalization process (with GMAP and GAAP
- Geometric AAP - replacing MAP and AAP): whereas
authority values still perfectly correlate with GMAP (0.99)
and GAAP (1.00), the correlation with hubness largely 
disappears (values are −0.16 and −0.09 - slightly negative but
not enough to concern us).
This is yet another confirmation that TREC effectiveness
as measured by MAP depends mainly on easy topics; GMAP
appears to be a more balanced measure. Note that, 
perhaps surprisingly, GMAP is indeed fairly well balanced, not
biased in the opposite direction - that is, it does not 
overemphasize the difficult topics.
In Sect. 6.3 below, we discuss another transformation, 
replacing the log function used in GMAP with logit. This has
a similar effect: the correlation of mean logitAP and average
logitAP with hubness are now small positive numbers (0.23
and 0.15 respectively), still comfortably away from the high
correlations with regular MAP and AAP, i.e., not presenting
the problematic phenomenon (ii) above (over-dependency on
easy topics).
We also observe that hub values are positive, whereas 
authority assumes, as predicted, both positive and negative
values. An intuitive justification is that negative hubness
would indicate a node which disagrees with the other nodes,
e.g., a system which does better on difficult topics, or a
topic on which bad systems do better; such systems and
topics would be quite strange, and probably do not appear
in TREC. Finally, although one might think that topics with
several relevant documents are more important and difficult,
this is not the case: there is no correlation between hub (or
any other indicator) and the number of documents relevant
to a topic.
6. DISCUSSION
6.1 Related work
There has been considerable interest in recent years in
questions of statistical significance of effectiveness 
comparisons between systems (e.g. [2, 9]), and related questions of
how many topics might be needed to establish differences
(e.g. [13]). We regard some results of the present study as
in some way complementary to this work, in that we make a
step towards answering the question Which topics are best
for establishing differences?.
The results on evaluation without relevance judgements
such as [10] show that, to some extent, good systems agree
on which are the good documents. We have not addressed
the question of individual documents in the present analysis,
but this effect is certainly analogous to our results.
6.2 Are normalizations necessary?
At this point it is also worthwhile to analyze what would
happen without the MAP- and AAP-normalizations defined
in Sect. 3.3. Indeed, the process of graph construction
(Sect. 3.4) is still valid: both the APM and APA matrices
are replaced by the AP one, and then everything goes on as
above. Therefore, one might think that the normalizations
are unuseful in this setting.
This is not the case. From the theoretical point of view,
the AP-only graph does not present the interesting 
properties above discussed: since the AP-only graph is 
symmetrical (the weight on each incoming link is equal to the weight
on the corresponding outgoing link), Inlinks and Outlinks
assume the same values. There is symmetry also in 
computing hub and authority, that assume the same value for each
node since the weights on the incoming and outgoing arcs
are the same. This could be stated in more precise and 
formal terms, but one might still wonder if on the overall graph
there are some sort of counterbalancing effects. It is 
therefore easier to look at experimental data, which confirm that
the normalizations are needed: the correlations between AP,
Inlinks, Outlinks, Hub, and/or Authority are all very close
to one (none of them is below 0.98).
6.3 Are these normalizations sufficient?
It might be argued that (in the case of APA, for example)
the amount we have subtracted from each AP value is 
topicdependent, therefore the range of the resulting APA value
is also topic-dependent (e.g. the maximum is 1 − AAP(tj)
and the minimum is − AAP(tj)). This suggests that the
cross-topic comparisons of these values suggested in Sect. 3.3
may not be reliable. A similar issue arises for APM and
comparisons across systems.
One possible way to overcome this would be to use an
unconstrained measure whose range is the full real line. Note
that in applying the method to GMAP by using log AP, we
avoid the problem with the lower limit but retain it for the
upper limit. One way to achieve an unconstrainted range
would be to use the logit function rather than the log [4,8].
We have also run this variant (as already reported in
Sect. 5 above), and it appears to provide very similar 
results to the GMAP results already given. This is not 
surprising, since in practice the two functions are very similar
over most of the operative range. The normalizations thus
seem reliable.
6.4 On AAT
and AT
A
It is well known that h and a vectors are the principal
left eigenvectors of AAT
and AT
A, respectively (this can
be easily derived from Eqs. (7)), and that, in the case of 
citation graphs, AAT
and AT
A represent, respectively, 
bibliographic coupling and co-citations. What is the meaning,
if any, of AAT
and AT
A in our Systems-Topics graph? It
is easy to derive that:
AAT
[i, j] =
8
><
>:
0

if i ∈ S ∧ j ∈ T
or i ∈ T ∧ j ∈ S
P
k A[i, k] · A[j, k] otherwise
AT
A[i, j] =
8
><
>:
0

if i ∈ S ∧ j ∈ T
or i ∈ T ∧ j ∈ S
P
k A[k, i] · A[k, j] otherwise
(where S is the set of indices corresponding to systems and T
the set of indices corresponding to topics). Thus AAT
and
AT
A are block diagonal matrices, with two blocks each, one
relative to systems and one relative to topics:
(a) if i, j ∈ S, then AAT
[i, j] =
P
k∈T APM(i, k)·APM(j, k)
measures how much the two systems i and j agree in
estimating topics ease (APM): high values mean that
the two systems agree on topics ease.
(b) if i, j ∈ T, then AAT
[i, j] =
P
k∈S APA(k, i)·APA(k, j)
measures how much the two topics i and j agree in 
estimating systems effectiveness (APA): high values mean
that the two topics agree on systems effectiveness (and
that TREC results would not change by leaving out one
of the two topics).
(c) if i, j ∈ S, then AT
A[i, j] =
P
k∈T APA(i, k) · APA(j, k)
measures how much agreement on the effectiveness of
two systems i and j there is over all topics: high 
values mean that many topics quite agree on the two 
systems effectiveness; low values single out systems that
are somehow controversial, and that need several topics
to have a correct effectiveness assessment.
(d) if i, j ∈ T, then AT
A[i, j] =
P
k∈S APM(k, i)·APM(k, j)
measures how much agreement on the ease of the two
topics i and j there is over all systems: high values mean
that many systems quite agree on the two topics ease.
Therefore, these matrices are meaningful and somehow
interesting. For instance, the submatrix (b) corresponds to
a weighted undirected complete graph, whose nodes are the
topics and whose arc weights are a measure of how much
two topics agree on systems effectiveness. Two topics that
are very close on this graph give the same information, and
therefore one of them could be discarded without changes in
TREC results. It would be interesting to cluster the topics
on this graph. Furthermore, the matrix/graph (a) could be
useful in TREC pool formation: systems that do not agree
on topic ease would probably find different relevant 
documents, and should therefore be complementary in pool 
formation. Note that no notion of single documents is involved
in the above analysis.
6.5 Insights
As indicated, the primary contribution of this paper has
been a method of analysis. However, in the course of 
applying this method to one set of TREC results, we have
achieved some insights relating to the hypotheses formulated
in Sect. 2:
• We confirm Hypothesis 2 above, that some topics are
easier than others.
• Differences in the hubness of systems reveal that some
systems are better than others at distinguishing easy
and difficult topics; thus we have some confirmation of
Hypothesis 3.
• There are some relatively idiosyncratic systems which
do badly on some topics generally considered easy but
well on some hard topics. However, on the whole, the
more effective systems are better at distinguishing easy
and difficult topics. This is to be expected: a really
bad system will do badly on everything, while even a
good system may have difficulty with some topics.
• Differences in the hubness of topics reveal that some
topics are better than others at distinguising more or
less effective systems; thus we have some confirmation
of Hypothesis 4.
• If we use MAP as the measure of effectiveness, it is
also true that the easiest topics are better at 
distinguishing more or less effective systems. As argued in
Sect. 5, this is an undesirable property. GMAP is more
balanced.
Clearly these ideas need to be tested on other data sets.
However, they reveal that the method of analysis proposed
in this paper can provide valuable information.
6.6 Selecting topics
The confirmation of Hypothesis 4 leads, as indicated, to
the idea that we could do reliable system evaluation on a
much smaller set of topics, provided we could select such an
appropriate set. This selection may not be straightforward,
however. It is possible that simply selecting the high 
hubness topics will achieve this end; however, it is also possible
that there are significant interactions between topics which
would render such a simple rule ineffective. This 
investigation would therefore require serious experimentation. For
this reason we have not attempted in this paper to point to
the specific high hubness topics as being good for evaluation.
This is left for future work.
7. CONCLUSIONS AND FUTURE
DEVELOPMENTS
The contribution of this paper is threefold:
• we propose a novel way of normalizing AP values;
• we propose a novel method to analyse TREC data;
• the method applied on TREC data does indeed reveal
some hidden properties.
More particularly, we propose Average Average Precision
(AAP), a measure of topic ease, and a novel way of 
normalizing the average precision measure in TREC, on the basis
of both MAP (Mean Average Precision) and AAP. The
normalized measures (APM and APA) are used to build a
bipartite weighted Systems-Topics graph, that is then 
analyzed by means of network analysis indicators widely known
in the (social) network analysis field, but somewhat 
generalised. We note that no such approach to TREC data
analysis has been proposed so far. The analysis shows that,
with current measures, a system that wants to be effective
in TREC needs to be effective on easy topics. Also, it is 
suggested that a cluster analysis on topic similarity can lead to
relying on a lower number of topics.
Our method of analysis, as described in this paper, can
be applied only a posteriori, i.e., once we have all the 
topics and all the systems available. Adding (removing) a new
system / topic would mean re-computing hubness and 
authority indicators. Moreover, we are not explicitly proposing
a change to current TREC methodology, although this could
be a by-product of these - and further - analyses.
This is an initial work, and further analyses could be 
performed. For instance, other effectiveness metrics could be
used, in place of AP. Other centrality indicators, widely
used in social network analysis, could be computed, although
probably with similar results to PageRank. It would be 
interesting to compute the higher-order eigenvectors of AT
A
and AAT
. The same kind of analysis could be performed at
the document level, measuring document ease. Hopefully,
further analyses of the graph defined in this paper, 
according to the approach described, can be insightful for a better
understanding of TREC or similar data.
Acknowledgments
We would like to thank Nick Craswell for insightful 
discussions and the anonymous referees for useful remarks. Part
of this research has been carried on while the first author
was visiting Microsoft Research Cambridge, whose financial
support is acknowledged.
8. REFERENCES
[1] M. Agosti, M. Bacchin, N. Ferro, and M. Melucci.
Improving the automatic retrieval of text documents.
In Proceedings of the 3rd CLEF Workshop, volume
2785 of LNCS, pages 279-290, 2003.
[2] C. Buckley and E. Voorhees. Evaluating evaluation
measure stability. In 23rd SIGIR, pages 33-40, 2000.
[3] S. Chakrabarti. Mining the Web. Morgan Kaufmann,
2003.
[4] G. V. Cormack and T. R. Lynam. Statistical precision
of information retrieval evaluation. In 29th SIGIR,
pages 533-540, 2006.
[5] J. Kleinberg. Authoritative sources in a hyperlinked
environment. J. of the ACM, 46(5):604-632, 1999.
[6] M. Levene. An Introduction to Search Engines and
Web Navigation. Addison Wesley, 2006.
[7] L. Page, S. Brin, R. Motwani, and T. Winograd. The
PageRank Citation Ranking: Bringing Order to the
Web, 1998.
http://dbpubs.stanford.edu:8090/pub/1999-66.
[8] S. Robertson. On GMAP - and other transformations.
In 13th CIKM, pages 78-83, 2006.
[9] M. Sanderson and J. Zobel. Information retrieval
system evaluation: effort, sensitivity, and reliability. In
28th SIGIR, pages 162-169, 2005.
http://doi.acm.org/10.1145/1076034.1076064.
[10] I. Soboroff, C. Nicholas, and P. Cahan. Ranking
retrieval systems without relevance judgments. In 24th
SIGIR, pages 66-73, 2001.
[11] TREC Common Evaluation Measures, 2005.
http://trec.nist.gov/pubs/trec14/appendices/
CE.MEASURES05.pdf (Last visit: Jan. 2007).
[12] Text REtrieval Conference (TREC).
http://trec.nist.gov/ (Last visit: Jan. 2007).
[13] E. Voorhees and C. Buckley. The effect of topic set
size on retrieval experiment error. In 25th SIGIR,
pages 316-323, 2002.
[14] E. M. Voorhees. Overview of the TREC 2005 Robust
Retrieval Track. In TREC 2005 Proceedings, 2005.
[15] E. M. Voorhees and D. K. Harman. 
TRECExperiment and Evaluation in Information Retrieval.
MIT Press, 2005.
[16] S. Wasserman and K. Faust. Social Network Analysis.
Cambridge University Press, Cambridge, UK, 1994.
DiffusionRank: A Possible Penicillin for Web Spamming
Haixuan Yang, Irwin King, and Michael R. Lyu
Dept. of Computer Science and Engineering
The Chinese University of Hong Kong
Shatin, NT, Hong Kong
{hxyang,king,lyu}@cse.cuhk.edu.hk
ABSTRACT
While the PageRank algorithm has proven to be very 
effective for ranking Web pages, the rank scores of Web pages
can be manipulated. To handle the manipulation problem
and to cast a new insight on the Web structure, we propose
a ranking algorithm called DiffusionRank. DiffusionRank is
motivated by the heat diffusion phenomena, which can be
connected to Web ranking because the activities flow on the
Web can be imagined as heat flow, the link from a page to
another can be treated as the pipe of an air-conditioner, and
heat flow can embody the structure of the underlying Web
graph. Theoretically we show that DiffusionRank can serve
as a generalization of PageRank when the heat diffusion 
coefficient γ tends to infinity. In such a case 1/γ = 0, 
DiffusionRank (PageRank) has low ability of anti-manipulation.
When γ = 0, DiffusionRank obtains the highest ability of
anti-manipulation, but in such a case, the web structure is
completely ignored. Consequently, γ is an interesting factor
that can control the balance between the ability of 
preserving the original Web and the ability of reducing the effect
of manipulation. It is found empirically that, when γ = 1,
DiffusionRank has a Penicillin-like effect on the link 
manipulation. Moreover, DiffusionRank can be employed to
find group-to-group relations on the Web, to divide the Web
graph into several parts, and to find link communities. 
Experimental results show that the DiffusionRank algorithm
achieves the above mentioned advantages as expected.
Categories and Subject Descriptors: H.3.3 
[Information Systems]: Information Search and Retrieval; G2.2
[Discrete Mathematics]: Graph Theory
General Terms: Algorithms.
1. INTRODUCTION
While the PageRank algorithm [13] has proven to be very
effective for ranking Web pages, inaccurate PageRank 
results are induced because of web page manipulations by 
people for commercial interests. The manipulation problem is
also called the Web spam, which refers to hyperlinked pages
on the World Wide Web that are created with the intention
of misleading search engines [7]. It is reported that 
approximately 70% of all pages in the .biz domain and about 35%
of the pages in the .us domain belong to the spam category
[12]. The reason for the increasing amount of Web spam is
explained in [12]: some web site operators try to influence
the positioning of their pages within search results because
of the large fraction of web traffic originating from searches
and the high potential monetary value of this traffic.
From the viewpoint of the Web site operators who want
to increase the ranking value of a particular page for search
engines, Keyword Stuffing and Link Stuffing are being used
widely [7, 12]. From the viewpoint of the search engine 
managers, the Web spam is very harmful to the users" evaluations
and thus their preference to choosing search engines because
people believe that a good search engine should not return
irrelevant or low-quality results. There are two methods 
being employed to combat the Web spam problem. Machine
learning methods are employed to handle the keyword 
stuffing. To successfully apply machine learning methods, we
need to dig out some useful textual features for Web pages,
to mark part of the Web pages as either spam or non-spam,
then to apply supervised learning techniques to mark other
pages. For example, see [5, 12]. Link analysis methods are
also employed to handle the link stuffing problem. One 
example is the TrustRank [7], a link-based method, in which
the link structure is utilized so that human labelled trusted
pages can propagate their trust scores trough their links.
This paper focuses on the link-based method.
The rest of the materials are organized as follows. In the
next section, we give a brief literature review on various
related ranking techniques. We establish the Heat Diffusion
Model (HDM) on various cases in Section 3, and propose
DiffusionRank in Section 4. In Section 5, we describe the
data sets that we worked on and the experimental results.
Finally, we draw conclusions in Section 6.
2. LITERATURE REVIEW
The importance of a Web page is determined by either
the textual content of pages or the hyperlink structure or
both. As in previous work [7, 13], we focus on ranking
methods solely determined by hyperlink structure of the
Web graph. All the mentioned ranking algorithms are 
established on a graph. For our convenience, we first give some
notations. Denote a static graph by G = (V, E), where V =
{v1, v2, . . . , vn}, E = {(vi, vj) | there is an edge from vi to
vj}. Ii and di denote the in-degree and the out-degree of
page i respectively.
2.1 PageRank
The importance of a Web page is an inherently subjective
matter, which depends on the reader"s interests, knowledge
and attitudes [13]. However, the average importance of all
readers can be considered as an objective matter. PageRank
tries to find such average importance based on the Web link
structure, which is considered to contain a large amount of
statistical data. The Web is modelled by a directed graph G
in the PageRank algorithms, and the rank or importance
xi for page vi ∈ V is defined recursively in terms of pages
which point to it: xi = (j,i)∈E aijxj, where aij is assumed
to be 1/dj if there is a link from j to i, and 0 otherwise. Or
in matrix terms, x = Ax. When the concept of random
jump is introduced, the matrix form is changed to
x = [(1 − α)g1T
+ αA]x, (1)
where α is the probability of following the actual link from a
page, (1 − α) is the probability of taking a random jump,
and g is a stochastic vector, i.e., 1T
g = 1. Typically, α =
0.85, and g = 1
n
1 is one of the standard settings, where 1 is
the vector of all ones [6, 13].
2.2 TrustRank
TrustRank [7] is composed of two parts. The first part
is the seed selection algorithm, in which the inverse 
PageRank was proposed to help an expert of determining a good
node. The second part is to utilize the biased PageRank,
in which the stochastic distribution g is set to be shared by
all the trusted pages found in the first part. Moreover, the
initial input of x is also set to be g. The justification for
the inverse PageRank and the solid experiments support its
advantage in combating the Web spam. Although there are
many variations of PageRank, e.g., a family of link-based
ranking algorithms in [2], TrustRank is especially chosen for
comparisons for three reasonss: (1) it is designed for 
combatting spamming; (2) its fixed parameters make a 
comparison easy; and (3) it has a strong theoretical relations with
PageRank and DiffusionRank.
2.3 Manifold Ranking
In [17], the idea of ranking on the data manifolds was 
proposed. The data points represented as vectors in Euclidean
space are considered to be drawn from a manifold. From
the data points on such a manifold, an undirected weighted
graph is created, then the weight matrix is given by the
Gaussian Kernel smoothing. While the manifold ranking
algorithm achieves an impressive result on ranking images,
the biased vector g and the parameter k in the general 
personalized PageRank in [17] are unknown in the Web graph
setting; therefore we do not include it in the comparisons.
2.4 Heat Diffusion
Heat diffusion is a physical phenomena. In a medium,
heat always flow from position with high temperature to
position with low temperature. Heat kernel is used to 
describe the amount of heat that one point receives from 
another point. Recently, the idea of heat kernel on a manifold
is borrowed in applications such as dimension reduction [3]
and classification [9, 10, 14]. In these work, the input data
is considered to lie in a special structure.
All the above topics are related to our work. The readers
can find that our model is a generalization of PageRank in
order to resist Web manipulation, that we inherit the first
part of TrustRank, that we borrow the concept of ranking on
the manifold to introduce our model, and that heat diffusion
is a main scheme in this paper.
3. HEAT DIFFUSION MODEL
Heat diffusion provides us with another perspective about
how we can view the Web and also a way to calculate 
ranking values. In this paper, the Web pages are considered to
be drawn from an unknown manifold, and the link structure
forms a directed graph, which is considered as an 
approximation to the unknown manifold. The heat kernel established
on the Web graph is considered as the representation of the
relationship between Web pages. The temperature 
distribution after a fixed time period, induced by a special initial
temperature distribution, is considered as the rank scores on
the Web pages. Before establishing the proposed models, we
first show our motivations.
3.1 Motivations
There are two points to explain that PageRank is 
susceptible to web spam.
• Over-democratic. There is a belief behind 
PageRank-all pages are born equal. This can be seen from
the equal voting ability of one page: the sum of each
column is equal to one. This equal voting ability of all
pages gives the chance for a Web site operator to 
increase a manipulated page by creating a large number
of new pages pointing to this page since all the newly
created pages can obtain an equal voting right.
• Input-independent. For any given non-zero initial
input, the iteration will converge to the same stable
distribution corresponding to the maximum eigenvalue
1 of the transition matrix. This input-independent
property makes it impossible to set a special initial 
input (larger values for trusted pages and less values even
negative values for spam pages) to avoid web spam.
The input-independent feature of PageRank can be further
explained as follows. P = [(1 − α)g1T
+ αA] is a positive
stochastic matrix if g is set to be a positive stochastic vector
(the uniform distribution is one of such settings), and so the
largest eigenvalue is 1 and no other eigenvalue whose 
absolute value is equal to 1, which is guaranteed by the Perron
Theorem [11]. Let y be the eigenvector corresponding to 1,
then we have Py = y. Let {xk} be the sequence generated
from the iterations xk+1 = Pxk, and x0 is the initial input.
If {xk} converges to x, then xk+1 = Pxk implies that x
must satisfy Px = x. Since the only maximum eigenvalue
is 1, we have x = cy where c is a constant, and if both x
and y are normalized by their sums, then c = 1. The above
discussions show that PageRank is independent of the initial
input x0.
In our opinion, g and α are objective parameters 
determined by the users" behaviors and preferences. A, α and
g are the true web structure. While A is obtained by a
crawler and the setting α = 0.85 is accepted by the people,
we think that g should be determined by a user behavior
investigation, something like [1]. Without any prior 
knowledge, g has to be set as g = 1
n
1.
TrustRank model does not follow the true web structure
by setting a biased g, but the effects of combatting 
spamming are achieved in [7]; PageRank is on the contrary in
some ways. We expect a ranking algorithm that has an 
effect of anti-manipulation as TrustRank while respecting the
true web structure as PageRank.
We observe that the heat diffusion model is a natural way
to avoid the over-democratic and input-independent feature
of PageRank. Since heat always flows from a position with
higher temperatures to one with lower temperatures, points
are not equal as some points are born with high 
temperatures while others are born with low temperatures. On the
other hand, different initial temperature distributions will
give rise to different temperature distributions after a fixed
time period. Based on these considerations, we propose the
novel DiffusionRank. This ranking algorithm is also 
motivated by the viewpoint for the Web structure. We view
all the Web pages as points drawn from a highly complex
geometric structure, like a manifold in a high dimensional
space. On a manifold, heat can flow from one point to 
another through the underlying geometric structure in a given
time period. Different geometric structures determine 
different heat diffusion behaviors, and conversely the diffusion
behavior can reflect the geometric structure. More 
specifically, on the manifold, the heat flows from one point to
another point, and in a given time period, if one point x
receives a large amount of heat from another point y, we
can say x and y are well connected, and thus x and y have
a high similarity in the sense of a high mutual connection.
We note that on a point with unit mass, the temperature
and the heat of this point are equivalent, and these two terms
are interchangeable in this paper. In the following, we first
show the HDM on a manifold, which is the origin of HDM,
but cannot be employed to the World Wide Web directly,
and so is considered as the ideal case. To connect the ideal
case and the practical case, we then establish HDM on a
graph as an intermediate case. To model the real world
problem, we further build HDM on a random graph as a
practical case. Finally we demonstrate the DiffusionRank
which is derived from the HDM on a random graph.
3.2 Heat Flow On a Known Manifold
If the underlying manifold is known, the heat flow 
throughout a geometric manifold with initial conditions can be 
described by the following second order differential equation:
∂f(x,t)
∂t
− ∆f(x, t) = 0, where f(x, t) is the heat at location x
at time t, and ∆f is the Laplace-Beltrami operator on a 
function f. The heat diffusion kernel Kt(x, y) is a special 
solution to the heat equation with a special initial condition-a
unit heat source at position y when there is no heat in other
positions. Based on this, the heat kernel Kt(x, y) describes
the heat distribution at time t diffusing from the initial unit
heat source at position y, and thus describes the 
connectivity (which is considered as a kind of similarity) between x
and y. However, it is very difficult to represent the World
Wide Web as a regular geometry with a known dimension;
even the underlying is known, it is very difficult to find the
heat kernel Kt(x, y), which involves solving the heat 
equation with the delta function as the initial condition. This
motivates us to investigate the heat flow on a graph. The
graph is considered as an approximation to the underlying
manifold, and so the heat flow on the graph is considered as
an approximation to the heat flow on the manifold.
3.3 On an Undirected Graph
On an undirected graph G, the edge (vi, vj) is considered
as a pipe that connects nodes vi and vj. The value fi(t)
describes the heat at node vi at time t, beginning from an
initial distribution of heat given by fi(0) at time zero. f(t)
(f(0)) denotes the vector consisting of fi(t) (fi(0)).
We construct our model as follows. Suppose, at time t,
each node i receives M(i, j, t, ∆t) amount of heat from its
neighbor j during a period of ∆t. The heat M(i, j, t, ∆t)
should be proportional to the time period ∆t and the heat
difference fj(t) − fi(t). Moreover, the heat flows from node
j to node i through the pipe that connects nodes i and j.
Based on this consideration, we assume that M(i, j, t, ∆t) =
γ(fj(t) − fi(t))∆t. As a result, the heat difference at node
i between time t + ∆t and time t will be equal to the sum
of the heat that it receives from all its neighbors. This is
formulated as
fi(t + ∆t) − fi(t) =
j:(j,i)∈E
γ(fj(t) − fi(t))∆t, (2)
where E is the set of edges. To find a closed form solution
to Eq. (2), we express it in a matrix form: (f(t + ∆t) −
f(t))/∆t = γHf(t), where d(v) denotes the degree of the
node v. In the limit ∆t → 0, it becomes d
dt
f(t) = γHf(t).
Solving it, we obtain f(t) = eγtH
f(0), especially we have
f(1) = eγH
f(0), Hij =



−d(vj), j = i,
1, (vj, vi) ∈ E,
0, otherwise,
(3)
where eγH
is defined as eγH
= I+γH+ γ2
2!
H2
+ γ3
3!
H3
+· · · .
3.4 On a Directed Graph
The above heat diffusion model must be modified to fit the
situation where the links between Web pages are directed.
On one Web page, when the page-maker creates a link (a, b)
to another page b, he actually forces the energy flow, for
example, people"s click-through activities, to that page, and
so there is added energy imposed on the link. As a result,
heat flows in a one-way manner, only from a to b, not from
b to a. Based on such consideration, we modified the heat
diffusion model on an undirected graph as follows.
On a directed graph G, the pipe (vi, vj) is forced by added
energy such that heat flows only from vi to vj. Suppose, at
time t, each node vi receives RH = RH(i, j, t, ∆t) amount of
heat from vj during a period of ∆t. We have three 
assumptions: (1) RH should be proportional to the time period ∆t;
(2) RH should be proportional to the the heat at node vj;
and (3) RH is zero if there is no link from vj to vi. As a
result, vi will receive j:(vj ,vi)∈E σjfj(t)∆t amount of heat
from all its neighbors that points to it.
On the other hand, node vi diffuses DH(i, t, ∆t) amount
of heat to its subsequent nodes. We assume that: (1) The
heat DH(i, t, ∆t) should be proportional to the time period
∆t. (2) The heat DH(i, t, ∆t) should be proportional to the
the heat at node vi. (3) Each node has the same ability of
diffusing heat. This fits the intuition that a Web surfer only
has one choice to find the next page that he wants to browse.
(4) The heat DH(i, t, ∆t) should be uniformly distributed
to its subsequent nodes. The real situation is more complex
than what we assume, but we have to make this simple 
assumption in order to make our model concise. As a result,
node vi will diffuse γfi(t)∆t/di amount of heat to any of its
subsequent nodes, and each of its subsequent node should
receive γfi(t)∆t/di amount of heat. Therefore σj = γ/dj.
To sum up, the heat difference at node vi between time
t+∆t and time t will be equal to the sum of the heat that it
receives, deducted by what it diffuses. This is formulated as
fi(t + ∆t) − fi(t) = −γfi(t)∆t + j:(vj ,vi)∈E γ/djfj(t)∆t.
Similarly, we obtain
f(1) = eγH
f(0), Hij =



−1, j = i,
1/dj, (vj, vi) ∈ E,
0, otherwise.
(4)
3.5 On a Random Directed Graph
For real world applications, we have to consider random
edges. This can be seen in two viewpoints. The first one
is that in Eq. (1), the Web graph is actually modelled as
a random graph, there is an edge from node vi to node vj
with a probability of (1 − α)gj (see the item (1 − α)g1T
),
and that the Web graph is predicted by a random graph
[15, 16]. The second one is that the Web structure is a
random graph in essence if we consider the content similarity
between two pages, though this is not done in this paper.
For these reasons, the model would become more flexible if
we extend it to random graphs. The definition of a random
graph is given below.
Definition 1. A random graph RG = (V, P = (pij)) is
defined as a graph with a vertex set V in which the edges are
chosen independently, and for 1 ≤ i, j ≤ |V | the probability
of (vi, vj) being an edge is exactly pij.
The original definition of random graphs in [4], is changed
slightly to consider the situation of directed graphs. Note
that every static graph can be considered as a special 
random graph in the sense that pij can only be 0 or 1.
On a random graph RG = (V, P), where P = (pij) is
the probability of the edge (vi, vj) exists. In such a random
graph, the expected heat difference at node i between time
t + ∆t and time t will be equal to the sum of the expected
heat that it receives from all its antecedents, deducted by
the expected heat that it diffuses.
Since the probability of the link (vj, vi) is pji, the 
expected heat flow from node j to node i should be multiplied
by pji, and so we have fi(t + ∆t) − fi(t) = −γ fi(t) ∆t +
j:(vj ,vi)∈E γpjifj(t)∆t/RD+
(vj), where RD+
(vi) is the 
expected out-degree of node vi, it is defined as k pik. 
Similarly we have
f(1) = eγR
f(0), Rij =



−1, j = i;
pji/RD+
(vj), j = i.
(5)
When the graph is large, a direct computation of eγR
is
time-consuming, and we adopt its discrete approximation:
f(1) = (I +
γ
N
R)N
f(0). (6)
The matrix (I+ γ
N
R)N
in Eq. (6) and matrix eγR
in Eq. (5)
are called Discrete Diffusion Kernel and the Continuous
Diffusion Kernel respectively. Based on the Heat Diffusion
Models and their solutions, DiffusionRank can be 
established on undirected graphs, directed graphs, and random
graphs. In the next section, we mainly focus on 
DiffusionRank in the random graph setting.
4. DIFFUSIONRANK
For a random graph, the matrix (I + γ
N
R)N
or eγR
can
measure the similarity relationship between nodes. Let fi(0)=
1, fj(0) = 0 if j = i, then the vector f(0) represent the unit
heat at node vi while all other nodes has zero heat. For such
f(0) in a random graph, we can find the heat distribution
at time 1 by using Eq. (5) or Eq. (6). The heat 
distribution is exactly the i−th row of the matrix of (I + γ
N
R)N
or
eγR
. So the ith-row jth-column element hij in the matrix
(I + γ∆tR)N
or eγR
means the amount of heat that vi can
receive from vj from time 0 to 1. Thus the value hij can be
used to measure the similarity from vj to vi. For a static
graph, similarly the matrix (I + γ
N
H)N
or eγH
can measure
the similarity relationship between nodes.
The intuition behind is that the amount h(i, j) of heat
that a page vi receives from a unit heat in a page vj in a
unit time embodies the extent of the link connections from
page vj to page vi. Roughly speaking, when there are more
uncrossed paths from vj to vi, vi will receive more heat from
vj; when the path length from vj to vi is shorter, vi will
receive more heat from vj; and when the pipe connecting
vj and vi is wide, the heat will flow quickly. The final heat
that vi receives will depend on various paths from vj to vi,
their length, and the width of the pipes.
Algorithm 1 DiffusionRank Function
Input: The transition matrix A; the inverse transition 
matrix U; the decay factor αI for the inverse PageRank; the
decay factor αB for PageRank; number of iterations MI for
the inverse PageRank; the number of trusted pages L; the
thermal conductivity coefficient γ.
Output: DiffusionRank score vector h.
1: s = 1
2: for i = 1 TO MI do
3: s = αI · U · s + (1 − αI ) · 1
n
· 1
4: end for
5: Sort s in a decreasing order: π = Rank({1, . . . , n}, s)
6: d = 0, Count = 0, i = 0
7: while Count ≤ L do
8: if π(i) is evaluated as a trusted page then
9: d(π(i)) = 1, Count + +
10: end if
11: i + +
12: end while
13: d = d/|d|
14: h = d
15: Find the iteration number MB according to λ
16: for i = 1 TO MB do
17: h = (1 − γ
MB
)h + γ
MB
(αB · A · h + (1 − αB) · 1
n
· 1)
18: end for
19: RETURN h
4.1 Algorithm
For the ranking task, we adopt the heat kernel on a 
random graph. Formally the DiffusionRank is described in 
Algorithm 1, in which,the element Uij in the inverse transition
matrix U is defined to be 1/Ij if there is a link from i to j,
and 0 otherwise. This trusted pages selection procedure by
inverse PageRank is completely borrowed from TrustRank
[7] except for a fix number of the size of the trusted set.
Although the inverse PageRank is not perfect in its 
ability of determining the maximum coverage, it is appealing
because of its polynomial execution time and its 
reasonable intuition-we actually inverse the original link when
we try to build the seed set from those pages that point
to many pages that in turn point to many pages and so
on. In the algorithm, the underlying random graph is set as
P = αB · A + (1 − αB) · 1
n
· 1n×n, which is induced by the
Web graph. As a result, R = −I + P.
In fact, the more general setting for DiffusionRank is P =
αB ·A+(1−αB)· 1
n
·g·1T
. By such a setting, DiffusionRank
is a generalization of TrustRank when γ tends to infinity
and when g is set in the same way as TrustRank. However,
the second part of TrustRank is not adopted by us. In our
model, g should be the true teleportation determined by
the user"s browse habits, popularity distribution over all the
Web pages, and so on; P should be the true model of the
random nature of the World Wide Web. Setting g according
to the trusted pages will not be consistent with the basic idea
of Heat Diffusion on a random graph. We simply set g = 1
only because we cannot find it without any priori knowledge.
Remark. In a social network interpretation, 
DiffusionRank first recognizes a group of trusted people, who may
not be highly ranked, but they know many other people.
The initially trusted people are endowed with the power to
decide who can be further trusted, but cannot decide the
final voting results, and so they are not dictators.
4.2 Advantages
Next we show the four advantages for DiffusionRank.
4.2.1 Two closed forms
First, its solutions have two forms, both of which are
closed form. One takes the discrete form, and has the 
advantage of fast computing while the other takes the continuous
form, and has the advantage of being easily analyzed in 
theoretical aspects. The theoretical advantage has been shown
in the proof of theorem in the next section.
(a) Group to Group Relations (b) An undirected graph
Figure 1: Two graphs
4.2.2 Group-group relations
Second, it can be naturally employed to detect the 
groupgroup relation. For example, let G2 and G1 denote two
groups, containing pages (j1, j2, . . . , js) and (i1, i2, . . . , it),
respectively. Then u,v hiu,jv is the total amounts of heat
that G1 receives from G2, where hiu,jv is the iu−th row
jv−th column element of the heat kernel. More specifically,
we need to first set f(0) for such an application as follows.
In f(0) = (f1(0), f2(0), . . . , fn(0))T
, if i ∈ {j1, j2, . . . , js},
then fi(0) = 1, and 0 otherwise. Then we employ Eq. (5)
to calculate f(1) = (f1(1), f2(1), . . . , fn(1))T
, finally we sum
those fj(1) where j ∈ {i1, i2, . . . , it}. Fig. 1 (a) shows the
results generated by the DiffusionRank. We consider five
groups-five departments in our Engineering Faculty: CSE,
MAE, EE, IE, and SE. γ is set to be 1, the numbers in
Fig. 1 (a) are the amount of heat that they diffuse to each
other. These results are normalized by the total number of
each group, and the edges are ignored if the values are less
than 0.000001. The group-to-group relations are therefore
detected, for example, we can see that the most strong 
overall tie is from EE to IE. While it is a natural application
for DiffusionRank because of the easy interpretation by the
amount heat from one group to another group, it is difficult
to apply other ranking techniques to such an application
because they lack such a physical meaning.
4.2.3 Graph cut
Third, it can be used to partition the Web graph into
several parts. A quick example is shown below. The graph
in Fig. 1 (b) is an undirected graph, and so we employ the
Eq. (3). If we know that node 1 belongs to one 
community and that node 12 belongs to another community, then
we can put one unit positive heat source on node 1 and
one unit negative heat source on node 12. After time 1, if
we set γ = 0.5, the heat distribution is [0.25, 0.16, 0.17,
0.16, 0.15, 0.09, 0.01, -0.04, -0.18 -0.21, -0.21, -0.34], and if
we set γ = 1, it will be [0.17, 0.16, 0.17, 0.16, 0.16, 0.12,
0.02, -0.07, -0.18, -0.22, -0.24, -0.24]. In both settings, we
can easily divide the graph into two parts: {1, 2, 3, 4, 5, 6, 7}
with positive temperatures and {8, 9, 10, 11, 12} with 
negative temperatures. For directed graphs and random graphs,
similarly we can cut them by employing corresponding heat
solution.
4.2.4 Anti-manipulation
Fourth, it can be used to combat manipulation. Let G2
contain trusted Web pages (j1, j2, . . . , js), then for each page
i, v hi,jv is the heat that page i receives from G2, and can
be computed by the discrete approximation of Eq. (4) in
the case of a static graph or Eq. (6) in the case of a random
graph, in which f(0) is set to be a special initial heat 
distribution so that the trusted Web pages have unit heat while
all the others have zero heat. In doing so, manipulated Web
page will get a lower rank unless it has strong in-links from
the trusted Web pages directly or indirectly. The situation
is quite different for PageRank because PageRank is 
inputindependent as we have shown in Section 3.1. Based on the
fact that the connection from a trusted page to a bad page
should be weak-less uncross paths, longer distance and 
narrower pipe, we can say DiffusionRank can resist web spam if
we can select trusted pages. It is fortunate that the trusted
pages selection method in [7]-the first part of TrustRank can
help us to fulfill this task. For such an application of 
DiffusionRank, the computation complexity for Discrete 
Diffusion Kernel is the same as that for PageRank in cases of
both a static graph and a random graph. This can be seen
in Eq. (6), by which we need N iterations and for each 
iteration we need a multiplication operation between a matrix
and a vector, while in Eq. (1) we also need a multiplication
operation between a matrix and a vector for each iteration.
4.3 The Physical Meaning of γ
γ plays an important role in the anti-manipulation effect
of DiffusionRank. γ is the thermal conductivity-the heat
diffusion coefficient. If it has a high value, heat will 
diffuse very quickly. Conversely, if it is small, heat will diffuse
slowly. In the extreme case, if it is infinitely large, then heat
will diffuse from one node to other nodes immediately, and
this is exactly the case corresponding to PageRank. Next,
we will interpret it mathematically.
Theorem 1. When γ tends to infinity and f(0) is not the
zero vector, eγR
f(0) is proportional to the stable distribution
produced by PageRank.
Let g = 1
n
1. By the Perron Theorem [11], we have shown
that 1 is the largest eigenvalue of P = [(1 − α)g1T
+ αA],
and that no other eigenvalue whose absolute value is equal
to 1. Let x be the stable distribution, and so Px = x. x is
the eigenvector corresponding to the eigenvalue 1. Assume
the n − 1 other eigenvalues of P are |λ2| < 1, . . . , |λn| < 1,
we can find an invertible matrix S = ( x S1 ) such that
S−1
PS =





1 ∗ ∗ ∗
0 λ2 ∗ ∗
0 0
... ∗
0 0 0 λn





. (7)
Since eγR
= eγ(−I+P)
=
S−1





1 ∗ ∗ ∗
0 eγ(λ2−1)
∗ ∗
0 0
... ∗
0 0 0 eγ(λn−1)





S, (8)
all eigenvalues of the matrix eγR
are 1, eγ(λ2−1)
, . . . , eγ(λn−1)
.
When γ → ∞, they become 1, 0, . . . , 0, which means that 1 is
the only nonzero eigenvalue of eγR
when γ → ∞. We can see
that when γ → ∞, eγR
eγR
f(0) = eγR
f(0), and so eγR
f(0)
is an eigenvector of eγR
when γ → ∞. On the other hand,
eγR
x = (I+γR+γ2
2!
R2
+γ3
3!
R3
+. . .)x = Ix+γRx+γ2
2!
R2
x+
γ3
3!
R3
x + . . . = x since Rx = (−I + P)x = −x + x = 0,
and hence x is the eigenvector of eγR
for any γ. Therefore
both x and eγR
f(0) are the eigenvectors corresponding the
unique eigenvalue 1 of eγR
when γ → ∞, and consequently
x = ceγR
f(0).
By this theorem, we see that DiffusionRank is a 
generalization of PageRank. When γ = 0, the ranking value is
most robust to manipulation since no heat is diffused and
the system is unchangeable, but the Web structure is 
completely ignored since eγR
f(0) = e0R
f(0) = If(0) = f(0);
when γ = ∞, DiffusionRank becomes PageRank, it can be
manipulated easily. We expect an appropriate setting of
γ that can balance both. For this, we have no theoretical
result, but in practice we find that γ = 1 works well in 
Section 5. Next we discuss how to determine the number of
iterations if we employ the discrete heat kernel.
4.4 The Number of Iterations
While we enjoy the advantage of the concise form of the
exponential heat kernel, it is better for us to calculate 
DiffusionRank by employing Eq. (6) in an iterative way. Then
the problem about determining N-the number of iterations
arises:
For a given threshold , find N such that ||((I + γ
N
R)N
−
eγR
)f(0)|| < for any f(0) whose sum is one.
Since it is difficult to solve this problem, we propose a
heuristic motivated by the following observations. When
R = −I+P, by Eq. (7), we have (I+ γ
N
R)N
= (I+ γ
N
(−I+
P))N
=
S−1





1 ∗ ∗ ∗
0 (1 + γ(λ2−1)
N
)N
∗ ∗
0 0
... ∗
0 0 0 (1 + γ(λn−1)
N
)N





S. (9)
Comparing Eq. (8) and Eq. (9), we observe that the 
eigenvalues of (I + γ
N
R)N
− eγR
are (1 + γ(λn−1)
N
)N
− eγ(λn−1)
.
We propose a heuristic method to determine N so that the
difference between the eigenvalues are less than a threshold
for only positive λs.
We also observe that if γ = 1, λ < 1, then |(1+ γ(λ−1)
N
)N
−
eγ(λ−1)
| < 0.005 if N ≥ 100, and |(1+ γ(λ−1)
N
)N
−eγ(λ−1)
| <
0.01 if N ≥ 30. So we can set N = 30, or N = 100, or others
according to different accuracy requirements. In this paper,
we use the relatively accurate setting N = 100 to make the
real eigenvalues in (I + γ
N
R)N
− eγR
less than 0.005.
5. EXPERIMENTS
In this section, we show the experimental data, the 
methodology, the setting, and the results.
5.1 Data Preparation
Our input data consist of a toy graph, a middle-size 
realworld graph, and a large-size real-world graph. The toy
graph is shown in Fig. 2 (a). The graph below it shows node
1 is being manipulated by adding new nodes A, B, C, . . .
such that they all point to node 1, and node 1 points to
them all. The data of two real Web graph were obtained
from the domain in our institute in October, 2004. The
total number of pages found are 18,542 in the middle-size
graph, and 607,170 in the large-size graph respectively. The
middle-size graph is a subgraph of the large-size graph, and
they were obtained by the same crawler: one is recorded
by the crawler in its earlier time, and the other is obtained
when the crawler stopped.
5.2 Methodology
The algorithms we run include PageRank, TrustRank and
DiffusionRank. All the rank values are multiplied by the
number of nodes so that the sum of the rank values is equal
to the number of nodes. By this normalization, we can 
compare the results on graphs with different sizes since the 
average rank value is one for any graph after such normalization.
We will need value difference and pairwise order difference as
comparison measures. Their definitions are listed as follows.
Value Difference. The value difference between A =
{Ai}n
i=1 and B = {Bi}n
i=1 is measured as n
i=1 |Ai − Bi|.
Pairwise Order Difference. The order difference between
A and B is measured as the number of significant order
differences between A and B. The pair (A[i], A[j]) and
(B[i], B[j]) is considered as a significant order difference if
one of the following cases happens: both A[i] > [ <]A[j]+0.1
and B[i] ≤ [ ≥]A[j]; both A[i] ≤ [ ≥]A[j] and B[i] > [ <
]A[j] + 0.1.
A
1
B
C
...
2
5
6 3
4
1
2 5
6 3 4
0 1 2 3 4 5 6
0
2
4
6
8
10
12
Gamma
ValueDifference
Trust set={1}
Trust set={2}
Trust set={3}
Trust set={4}
Trust set={5}
Trust set={6}
(a) (b)
Figure 2: (a) The toy graph consisting of six nodes,
and node 1 is being manipulated by adding new
nodes A, B, C, . . . (b) The approximation tendency to
PageRank by DiffusionRank
5.3 Experimental Set-up
The experiments on the middle-size graph and the 
largesize graphs are conducted on the workstation, whose 
hardware model is Nix Dual Intel Xeon 2.2GHz with 1GB RAM
and a Linux Kernel 2.4.18-27smp (RedHat7.3). In 
calculating DiffusionRank, we employ Eq. (6) and the discrete
approximation of Eq. (4) for such graphs. The related tasks
are implemented using C language. While in the toy graph,
we employ the continuous diffusion kernel in Eq. (4) and
Eq. (5), and implement related tasks using Matlab.
For nodes that have zero out-degree (dangling nodes), we
employ the method in the modified PageRank algorithm [8],
in which dangling nodes of are considered to have random
links uniformly to each node. We set α = αI = αB = 0.85 in
all algorithms. We also set g to be the uniform distribution
in both PageRank and DiffusionRank. For DiffusionRank,
we set γ = 1. According to the discussions in Section 4.3 and
Section 4.4, we set the iteration number to be MB = 100 in
DiffusionRank, and for accuracy consideration, the iteration
number in all the algorithms is set to be 100.
5.4 Approximation of PageRank
We show that when γ tends to infinity, the value 
differences between DiffusionRank and PageRank tend to zero.
Fig. 2 (b) shows the approximation property of 
DiffusionRank, as proved in Theorem 1, on the toy graph. The 
horizontal axis of Fig. 2 (b) marks the γ value, and vertical axis
corresponds to the value difference between DiffusionRank
and PageRank. All the possible trusted sets with L = 1
are considered. For L > 1, the results should be the linear
combination of some of these curves because of the 
linearity of the solutions to heat equations. On other graphs, the
situations are similar.
5.5 Results of Anti-manipulation
In this section, we show how the rank values change as the
intensity of manipulation increases. We measure the 
intensity of manipulation by the number of newly added points
that point to the manipulated point. The horizontal axes
of Fig. 3 stand for the numbers of newly added points, and
vertical axes show the corresponding rank values of the 
manipulated nodes. To be clear, we consider all six situations.
Every node in Fig. 2 (a) is manipulated respectively, and its
0 50 100
0
10
20
30
40
50
RankoftheManipulatdNode−1
DiffusionRank−Trust 4
PageRank
TrustRanl−Trust 4
0 50 100
0
10
20
30
40
50
RankoftheManipulatdNode−2
DiffusionRank−Trust 4
PageRank
TrustRanl−Trust 4
0 50 100
0
10
20
30
40
50
RankoftheManipulatdNode−3
DiffusionRank−Trust 4
PageRank
TrustRanl−Trust 4
0 50 100
0
10
20
30
40
50
Number of New Added Nodes
RankoftheManipulatdNode−4
DiffusionRank−Trust 3
PageRank
TrustRanl−Trust 3
0 50 100
0
10
20
30
40
50
Number of New Added Nodes
RankoftheManipulatdNode−5
DiffusionRank−Trust 4
PageRank
TrustRanl−Trust 4
0 50 100
0
10
20
30
40
50
Number of New Added Nodes
RankoftheManipulatdNode−6
DiffusionRank−Trust 4
PageRank
TrustRanl−Trust 4
Figure 3: The rank values of the manipulated nodes
on the toy graph
200040006000800010000
0
1000
2000
3000
4000
5000
6000
7000
8000
Number of New Added Points
RankoftheManipulatdNode
PageRank
DiffusionRank−uniform
DiffusionRank0
DiffusionRank1
DiffusionRank2
DiffusionRank3
TrustRank0
TrustRank1
TrustRank2
TrustRank3
2000 4000 6000 8000 10000
0
20
40
60
80
100
120
140
160
180
Number of New Added Points
RankoftheManipulatdNode
PageRank
DiffusionRank
TrustRank
DiffusionRank−uniform
(a) (b)
Figure 4: (a) The rank values of the manipulated
nodes on the middle-size graph; (b) The rank values
of the manipulated nodes on the large-size graph
corresponding values for PageRank, TrustRank (TR), 
DiffusionRank (DR) are shown in the one of six sub-figures in
Fig. 3. The vertical axes show which node is being 
manipulated. In each sub-figure, the trusted sets are 
computed below. Since the inverse PageRank yields the results
[1.26, 0.85, 1.31, 1.36, 0.51, 0.71]. Let L = 1. If the 
manipulated node is not 4, then the trusted set is {4}, and 
otherwise {3}. We observe that in all the cases, rank values
of the manipulated node for DiffusionRank grow slowest as
the number of the newly added nodes increases. On the
middle-size graph and the large-size graph, this conclusion
is also true, see Fig. 4. Note that, in Fig. 4 (a), we choose
four trusted sets (L = 1), on which we test DiffusionRank
and TrustRank, the results are denoted by DiffusionRanki
and TrustRanki (i = 0, 1, 2, 3 denotes the four trusted set);
in Fig. 4 (b), we choose one trusted set (L = 1). Moreover,
in both Fig. 4 (a) and Fig. 4 (b), we show the results for
DiffusionRank when we have no trusted set, and we trust
all the pages before some of them are manipulated.
We also test the order difference between the ranking 
order A before the page is manipulated and the ranking order
PA after the page is manipulated. Because after 
manipulation, the number of pages changes, we only compare the
common part of A and PA. This experiment is used to test
the stability of all these algorithms. The less the order 
difference, the stabler the algorithm, in the sense that only a
smaller part of the order relations is affected by the 
manipulation. Figure 5 (a) shows that the order difference values
change when we add new nodes that point to the 
manipulated node. We give several γ settings. We find that when
γ = 1, the least order difference is achieved by 
DiffusionRank. It is interesting to point out that as γ increases, the
order difference will increase first; after reaching a maximum
value, it will decrease, and finally it tends to the PageRank
results. We show this tendency in Fig. 5 (b), in which we
choose three different settings-the number of manipulated
nodes are 2,000, 5,000, and 10,000 respectively. From these
figures, we can see that when γ < 2, the values are less than
those for PageRank, and that when γ > 20, the difference
between PageRank and DiffusionRank is very small. 
After these investigations, we find that in all the graphs we
tested, DiffusionRank (when γ = 1) is most robust to 
manipulation both in value difference and order difference. The
trust set selection algorithm proposed in [7] is effective for
both TrustRank and DiffusionRank.
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
0
0.5
1
1.5
2
2.5
3
x 10
5
Number of New Added Points
PairwiseOrderDifference
PageRank
DiffusionRank−Gamma=1
DiffusionRank−Gamma=2
DiffusionRank−Gamma=3
DiffusionRank−Gamma=4
DiffusionRank−Gamma=5
DiffusionRank−Gamma=15
TrustRank
0 5 10 15 20
0
0.5
1
1.5
2
2.5
x 10
5
Gamma
PairwiseOrderDifference
DiffusionRank: when added 2000 nodes
DiffusionRank: when added 5000 nodes
DiffusionRank: when added 10000 nodes
PageRank
(a) (b)
Figure 5: (a) Pairwise order difference on the
middle-size graph, the least it is, the more stable
the algorithm; (b) The tendency of varying γ
6. CONCLUSIONS
We conclude that DiffusionRank is a generalization of
PageRank, which is interesting in that the heat diffusion 
coefficient γ can balance the extent that we want to model the
original Web graph and the extent that we want to reduce
the effect of link manipulations. The experimental results
show that we can actually achieve such a balance by 
setting γ = 1, although the best setting including varying γi
is still under further investigation. This anti-manipulation
feature enables DiffusionRank to be a candidate as a 
penicillin for Web spamming. Moreover, DiffusionRank can be
employed to find group-group relations and to partition Web
graph into small communities. All these advantages can be
achieved in the same computational complexity as 
PageRank. For the special application of anti-manipulation, 
DiffusionRank performs best both in reduction effects and in
its stability among all the three algorithms.
7. ACKNOWLEDGMENTS
We thank Patrick Lau, Zhenjiang Lin and Zenglin Xu
for their help. This work is fully supported by two grants
from the Research Grants Council of the Hong Kong Special
administrative Region, China (Project No. CUHK4205/04E
and Project No. CUHK4235/04E).
8. REFERENCES
[1] E. Agichtein, E. Brill, and S. T. Dumais. Improving web search
ranking by incorporating user behavior information. In E. N.
Efthimiadis, S. T. Dumais, D. Hawking, and K. J¨arvelin,
editors, Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 19-26, 2006.
[2] R. A. Baeza-Yates, P. Boldi, and C. Castillo. Generalizing
pagerank: damping functions for link-based ranking
algorithms. In E. N. Efthimiadis, S. T. Dumais, D. Hawking,
and K. J¨arvelin, editors, Proceedings of the 29th Annual
International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR), pages
308-315, 2006.
[3] M. Belkin and P. Niyogi. Laplacian eigenmaps for
dimensionality reduction and data representation. Neural
Computation, 15(6):1373-1396, Jun 2003.
[4] B. Bollob´as. Random Graphs. Academic Press Inc. (London),
1985.
[5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank using
gradient descent. In Proceedings of the 22nd international
conference on Machine learning (ICML), pages 89-96, 2005.
[6] N. Eiron, K. S. McCurley, and J. A. Tomlin. Ranking the web
frontier. In Proceeding of the 13th World Wide Web
Conference (WWW), pages 309-318, 2004.
[7] Z. Gy¨ongyi, H. Garcia-Molina, and J. Pedersen. Combating
web spam with trustrank. In M. A. Nascimento, M. T. ¨Ozsu,
D. Kossmann, R. J. Miller, J. A. Blakeley, and K. B. Schiefer,
editors, Proceedings of the Thirtieth International Conference
on Very Large Data Bases (VLDB), pages 576-587, 2004.
[8] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and G. H.
Golub. Exploiting the block structure of the web for computing
pagerank. Technical report, Stanford University, 2003.
[9] R. I. Kondor and J. D. Lafferty. Diffusion kernels on graphs
and other discrete input spaces. In C. Sammut and A. G.
Hoffmann, editors, Proceedings of the Nineteenth
International Conference on Machine Learning (ICML),
pages 315-322, 2002.
[10] J. Lafferty and G. Lebanon. Diffusion kernels on statistical
manifolds. Journal of Machine Learning Research, 6:129-163,
Jan 2005.
[11] C. R. MacCluer. The many proofs and applications of perron"s
theorem. SIAM Review, 42(3):487-498, 2000.
[12] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly. Detecting
spam web pages through content analysis. In Proceedings of
the 15th international conference on World Wide Web
(WWW), pages 83-92, 2006.
[13] L. Page, S. Brin, R. Motwani, and T. Winograd. The pagerank
citation ranking: Bringing order to the web. Technical Report
Paper SIDL-WP-1999-0120 (version of 11/11/1999), Stanford
Digital Library Technologies Project, 1999.
[14] H. Yang, I. King, and M. R. Lyu. NHDC and PHDC:
Non-propagating and propagating heat diffusion classifiers. In
Proceedings of the 12th International Conference on Neural
Information Processing (ICONIP), pages 394-399, 2005.
[15] H. Yang, I. King, and M. R. Lyu. Predictive ranking: a novel
page ranking approach by estimating the web structure. In
Proceedings of the 14th international conference on World
Wide Web (WWW) - Special interest tracks and posters,
pages 944-945, 2005.
[16] H. Yang, I. King, and M. R. Lyu. Predictive random graph
ranking on the web. In Proceedings of the IEEE World
Congress on Computational Intelligence (WCCI), pages
3491-3498, 2006.
[17] D. Zhou, J. Weston, A. Gretton, O. Bousquet, and
B. Sch¨olkopf. Ranking on data manifolds. In S. Thrun, L. Saul,
and B. Sch¨olkopf, editors, Advances in Neural Information
Processing Systems 16 (NIPS 2003), 2004.
Location based Indexing Scheme for DAYS
Debopam Acharya and Vijay Kumar
1
Computer Science and Informatics
University of Missouri-Kansas City
Kansas City, MO 64110
dargc(kumarv)@umkc.edu
ABSTRACT
Data dissemination through wireless channels for broadcasting
information to consumers is becoming quite common. Many
dissemination schemes have been proposed but most of them push
data to wireless channels for general consumption. Push based
broadcast [1] is essentially asymmetric, i.e., the volume of data
being higher from the server to the users than from the users back
to the server. Push based scheme requires some indexing which
indicates when the data will be broadcast and its position in the
broadcast. Access latency and tuning time are the two main
parameters which may be used to evaluate an indexing scheme.
Two of the important indexing schemes proposed earlier were tree
based and the exponential indexing schemes. None of these
schemes were able to address the requirements of location
dependent data (LDD) which is highly desirable feature of data
dissemination. In this paper, we discuss the broadcast of LDD in
our project DAta in Your Space (DAYS), and propose a scheme
for indexing LDD. We argue that this scheme, when applied to
LDD, significantly improves performance in terms of tuning time
over the above mentioned schemes. We prove our argument with
the help of simulation results.
Categories and Subject Descriptors
H.3.1 [Information Systems]: Information Storage and Retrieval
- content analysis and indexing; H.3.3 [Information Systems]:
Information Storage and Retrieval - information search and
retrieval.
General Terms
Algorithms, Performance, Experimentation
1. INTRODUCTION
Wireless data dissemination is an economical and efficient
way to make desired data available to a large number of mobile or
static users. The mode of data transfer is essentially asymmetric,
that is, the capacity of the transfer of data (downstream
communication) from the server to the client (mobile user) is
significantly larger than the client or mobile user to the server
(upstream communication). The effectiveness of a data
dissemination system is judged by its ability to provide user the
required data at anywhere and at anytime. One of the best ways to
accomplish this is through the dissemination of highly
personalized Location Based Services (LBS) which allows users
to access personalized location dependent data. An example
would be someone using their mobile device to search for a
vegetarian restaurant. The LBS application would interact with
other location technology components or use the mobile user's
input to determine the user's location and download the
information about the restaurants in proximity to the user by
tuning into the wireless channel which is disseminating LDD.
We see a limited deployment of LBS by some service
providers. But there are every indications that with time some of
the complex technical problems such as uniform location
framework, calculating and tracking locations in all types of
places, positioning in various environments, innovative location
applications, etc., will be resolved and LBS will become a
common facility and will help to improve market productivity and
customer comfort. In our project called DAYS, we use wireless
data broadcast mechanism to push LDD to users and mobile users
monitor and tune the channel to find and download the required
data. A simple broadcast, however, is likely to cause significant
performance degradation in the energy constrained mobile devices
and a common solution to this problem is the use of efficient air
indexing. The indexing approach stores control information which
tells the user about the data location in the broadcast and how and
when he could access it. A mobile user, thus, has some free time
to go into the doze mode which conserves valuable power. It also
allows the user to personalize his own mobile device by
selectively tuning to the information of his choice.
Access efficiency and energy conservation are the two issues
which are significant for data broadcast systems. Access efficiency
refers to the latency experienced when a request is initiated till the
response is received. Energy conservation [7, 10] refers to the
efficient use of the limited energy of the mobile device in
accessing broadcast data. Two parameters that affect these are the
tuning time and the access latency. Tuning time refers to the time
during which the mobile unit (MU) remains in active state to tune
the channel and download its required data. It can also be defined
as the number of buckets tuned by the mobile device in active
state to get its required data. Access latency may be defined as the
time elapsed since a request has been issued till the response has
been received.
1
This research was supported by a grant from NSF IIS-0209170.
Several indexing schemes have been proposed in the past and
the prominent among them are the tree based and the exponential
indexing schemes [17]. The main disadvantages of the tree based
schemes are that they are based on centralized tree structures. To
start a search, the MU has to wait until it reaches the root of the
next broadcast tree. This significantly affects the tuning time of
the mobile unit. The exponential schemes facilitate index
replication by sharing links in different search trees. For
broadcasts with large number of pages, the exponential scheme
has been shown to perform similarly as the tree based schemes in
terms of access latency. Also, the average length of broadcast
increases due to the index replication and this may cause
significant increase in the access latency. None of the above
indexing schemes is equally effective in broadcasting location
dependent data. In addition to providing low latency, they lack
properties which are used to address LDD issues. We propose an
indexing scheme in DAYS which takes care of some these
problems. We show with simulation results that our scheme
outperforms some of the earlier indexing schemes for
broadcasting LDD in terms of tuning time.
The rest of the paper is presented as follows. In section 2, we
discuss previous work related to indexing of broadcast data.
Section 3 describes our DAYS architecture. Location dependent
data, its generation and subsequent broadcast is presented in
section 4. Section 5 discusses our indexing scheme in detail.
Simulation of our scheme and its performance evaluation is
presented in section 6. Section 7 concludes the paper and
mentions future related work.
2. PREVIOUS WORK
Several disk-based indexing techniques have been used for air
indexing. Imielinski et al. [5, 6] applied the B+ index tree, where
the leaf nodes store the arrival times of the data items. The
distributed indexing method was proposed to efficiently replicate
and distribute the index tree in a broadcast. Specifically, the index
tree is divided into a replicated part and a non replicated part.
Each broadcast consists of the replicated part and the 
nonreplicated part that indexes the data items immediately following
it. As such, each node in the non-replicated part appears only once
in a broadcast and, hence, reduces the replication cost and access
latency while achieving a good tuning time. Chen et al. [2] and
Shivakumar et al. [8] considered unbalanced tree structures to
optimize energy consumption for non-uniform data access. These
structures minimize the average index search cost by reducing the
number of index searches for hot data at the expense of spending
more on cold data. Tan and Yu discussed data and index
organization under skewed broadcast Hashing and signature
methods have also been suggested for wireless broadcast that
supports equality queries [9]. A flexible indexing method was
proposed in [5]. The flexible index first sorts the data items in
ascending (or descending) order of the search key values and then
divides them into p segments. The first bucket in each data
segment contains a control index, which is a binary index
mapping a given key value to the segment containing that key,
and a local index, which is an m-entry index mapping a given key
value to the buckets within the current segment. By tuning the
parameters of p and m, mobile clients can achieve either a good
tuning time or good access latency. Another indexing technique
proposed is the exponential indexing scheme [17]. In this scheme,
a parameterized index, called the exponential index is used to
optimize the access latency or the tuning time. It facilitates index
replication by linking different search trees. All of the above
mentioned schemes have been applied to data which are non
related to each other. These non related data may be clustered or
non clustered. However, none of them has specifically addressed
the requirements of LDD. Location dependent data are data which
are associated with a location. Presently there are several
applications that deal with LDD [13, 16]. Almost all of them
depict LDD with the help of hierarchical structures [3, 4]. This is
based on the containment property of location dependent data.
The Containment property helps determining relative position of
an object by defining or identifying locations that contains those
objects. The subordinate locations are hierarchically related to
each other. Thus, Containment property limits the range of
availability or operation of a service. We use this containment
property in our indexing scheme to index LDD.
3. DAYS ARCHITECTURE
DAYS has been conceptualized to disseminate topical and 
nontopical data to users in a local broadcast space and to accept
queries from individual users globally. Topical data, for example,
weather information, traffic information, stock information, etc.,
constantly changes over time. Non topical data such as hotel,
restaurant, real estate prices, etc., do not change so often. Thus,
we envision the presence of two types of data distribution: In the
first case, server pushes data to local users through wireless
channels. The other case deals with the server sending results of
user queries through downlink wireless channels. Technically, we
see the presence of two types of queues in the pull based data
access. One is a heavily loaded queue containing globally
uploaded queries. The other is a comparatively lightly loaded
queue consisting of locally uploaded queries. The DAYS
architecture [12] as shown in figure 1 consists of a Data Server,
Broadcast Scheduler, DAYS Coordinator, Network of LEO
satellites for global data delivery and a Local broadcast space.
Data is pushed into the local broadcast space so that users may
tune into the wireless channels to access the data. The local
broadcast space consists of a broadcast tower, mobile units and a
network of data staging machines called the surrogates. Data
staging in surrogates has been earlier investigated as a successful
technique [12, 15] to cache users' related data. We believe that
data staging can be used to drastically reduce the latency time for
both the local broadcast data as well as global responses. Query
request in the surrogates may subsequently be used to generate the
popularity patterns which ultimately decide the broadcast
schedule [12].
18
Popularity
Feedback from
Surrogates for
Broadcast
Scheduler
Local Broadcast Space
Broadcast Tower
SurrogateMU
MU
MU
MU
Data ServerBroadcast schedulerDAYS Coordinator
Local downlink
channel
Global downlink
channel
Pull request queue
Global request queue
Local request queue Location based index
Starbucks
Plaza
Kansas
City
Figure 1. DAYS Architecture Figure 2. Location Structure ofStarbucks, Plaza
4. LOCATION DEPENDENT DATA (LDD)
We argue that incorporating location information in wireless data
broadcast can significantly decrease the access latency. This
property becomes highly useful for mobile unit which has limited
storage and processing capability. There are a variety of
applications to obtain information about traffic, restaurant and
hotel booking, fast food, gas stations, post office, grocery stores,
etc. If these applications are coupled with location information,
then the search will be fast and highly cost effective. An important
property of the locations is Containment which helps to determine
the relative location of an object with respect to its parent that
contains the object. Thus, Containment limits the range of
availability of a data. We use this property in our indexing
scheme. The database contains the broadcast contents which are
converted into LDD [14] by associating them with respective
locations so that it can be broadcasted in a clustered manner. The
clustering of LDD helps the user to locate information efficiently
and supports containment property. We present an example to
justify our proposition.
Example: Suppose a user issues query Starbucks Coffee in
Plaza please. to access information about the Plaza branch of
Starbucks Coffee in Kansas City. In the case of location
independent set up the system will list all Starbucks coffee shops
in Kansas City area. It is obvious that such responses will
increase access latency and are not desirable. These can be
managed efficiently if the server has location dependent data, i.e.,
a mapping between a Starbucks coffee shop data and its physical
location. Also, for a query including range of locations of
Starbucks, a single query requesting locations for the entire
region of Kansas City, as shown in Figure 2, will suffice. This
will save enormous amount of bandwidth by decreasing the
number of messages and at the same time will be helpful in
preventing the scalability bottleneck in highly populated area.
4.1 Mapping Function for LDD
The example justifies the need for a mapping function to process
location dependent queries. This will be especially important for
pull based queries across the globe for which the reply could be
composed for different parts of the world. The mapping function
is necessary to construct the broadcast schedule.
We define Global Property Set (GPS) [11], Information Content
(IC) set, and Location Hierarchy (LH) set where IC ⊆ GPS and
LH ⊆ GPS to develop a mapping function. LH = {l1, l2, l3…,lk}
where li represent locations in the location tree and IC = {ic1, ic2,
ic3,…,icn} where ici represent information type. For example, if
we have traffic, weather, and stock information are in broadcast
then IC = {ictraffic, icweather, and icstock}. The mapping scheme must
be able to identify and select an IC member and a LH node for (a)
correct association, (b) granularity match, (c) and termination
condition. For example, weather ∈ IC could be associated with a
country or a state or a city or a town of LH. The granularity match
between the weather and a LH node is as per user requirement.
Thus, with a coarse granularity weather information is associated
with a country to get country"s weather and with town in a finer
granularity. If a town is the finest granularity, then it defines the
terminal condition for association between IC and LH for weather.
This means that a user cannot get weather information about
subdivision of a town. In reality weather of a subdivision does
not make any sense.
We develop a simple heuristic mapping approach scheme based
on user requirement. Let IC = {m1, m2,m3 .,..., mk}, where mi
represent its element and let LH = {n1, n2, n3, ..., nl}, where ni
represents LH"s member. We define GPS for IC (GPSIC) ⊆ GPS
and for LH (GPSLH) ⊆ GPS as GPSIC = {P1, P2,…, Pn}, where
P1, P2, P3,…, Pn are properties of its members and GPSLH = {Q1,
Q2,…, Qm} where Q1, Q2,…, Qm are properties of its members.
The properties of a particular member of IC are a subset of
GPSIC. It is generally true that (property set (mi∈ IC) ∪ property
set (mj∈ IC)) ≠ ∅, however, there may be cases where the
intersection is not null. For example, stock ∈ IC and movie ∈ IC
rating do not have any property in common. We assume that any
two or more members of IC have at least one common
geographical property (i.e. location) because DAYS broadcasts
information about those categories, which are closely tied with a
location. For example, stock of a company is related to a country,
weather is related to a city or state, etc.
We define the property subset of mi∈ IC as PSm
i
∀ mi ∈ IC and
PSm
i
= {P1, P2, ..., Pr} where r ≤ n. ∀ Pr {Pr ∈ PSm
i
→ Pr∈
GPSIC} which implies that ∀ i, PSm
i
⊆ GPSIC. The geographical
properties of this set are indicative of whether mi ∈ IC can be
mapped to only a single granularity level (i.e. a single location) in
LH or a multiple granularity levels (i.e. more than one nodes in
19
the hierarchy) in LH. How many and which granularity levels
should a mi map to, depends upon the level at which the service
provider wants to provide information about the mi in question.
Similarly we define a property subset of LH members as PSn
j
∀ nj
∈ LH which can be written as PSn
j
={Q1, Q2, Q3, …, Qs} where s ≤
m. In addition, ∀ Qs {Qs∈ PSn
j
→ Qs∈ GPSLH} which implies that
∀j, PSn
j
⊆ GPSLH.
The process of mapping from IC to LH is then identifying for
some mx∈ IC one or more ny∈ LH such that PSmx ∩ PSnv ≠ φ.
This means that when mx maps to ny and all children of ny if mx
can map to multiple granularity levels or mx maps only to ny if mx
can map to a single granularity level.
We assume that new members can join and old member can leave
IC or LH any time. The deletion of members from the IC space is
simple but addition of members to the IC space is more restrictive.
If we want to add a new member to the IC space, then we first
define a property set for the new member: PSmnew_m ={P1, P2, P3,
…, Pt} and add it to the IC only if the condition:∀ Pw {Pw∈
PSpnew_m → Pw∈ GPSIC} is satisfied. This scheme has an
additional benefit of allowing the information service providers to
have a control over what kind of information they wish to provide
to the users. We present the following example to illustrate the
mapping concept.
IC = {Traffic, Stock, Restaurant, Weather, Important history
dates, Road conditions}
LH = {Country, State, City, Zip-code, Major-roads}
GPSIC = {Surface-mobility, Roads, High, Low, Italian-food,
StateName, Temp, CityName, Seat-availability, Zip, Traffic-jams,
Stock-price, CountryName, MajorRoadName, Wars, Discoveries,
World}
GPSLH = {Country, CountrySize, StateName, CityName, Zip,
MajorRoadName}
Ps(ICStock) = {Stock-price, CountryName, High, Low}
Ps(ICTraffic) = {Surface-mobility, Roads, High, Low, Traffic-jams,
CityName}
Ps(ICImportant dates in history) = {World, Wars, Discoveries}
Ps(ICRoad conditions) = {Precipitation, StateName, CityName}
Ps(ICRestaurant) = {Italian-food, Zip code}
Ps(ICWeather) = {StateName, CityName, Precipitation,
Temperature}
PS(LHCountry) = {CountryName, CountrySize}
PS(LHState = {StateName, State size},
PS(LHCity) ={CityName, City size}
PS(LHZipcode) = {ZipCodeNum }
PS(LHMajor roads) = {MajorRoadName}
Now, only PS(ICStock) ∩ PSCountry ≠φ. In addition, PS(ICStock)
indicated that Stock can map to only a single location Country.
When we consider the member Traffic of IC space, only
PS(ICTraffic) ∩ PScity ≠ φ. As PS(ICTraffic) indicates that Traffic can
map to only a single location, it maps only to City and none of its
children. Now unlike Stock, mapping of Traffic with Major roads,
which is a child of City, is meaningful. However service providers
have right to control the granularity levels at which they want to
provide information about a member of IC space.
PS(ICRoad conditions) ∩ PSState ≠φ and PS(ICRoad conditions) ∩ PSCity≠φ.
So Road conditions maps to State as well as City. As PS(ICRoad
conditions) indicates that Road conditions can map to multiple
granularity levels, Road conditions will also map to Zip Code and
Major roads, which are the children of State and City. Similarly,
Restaurant maps only to Zip code, and Weather maps to State,
City and their children, Major Roads and Zip Code.
5. LOCATION BASED INDEXING SCHEME
This section discusses our location based indexing scheme
(LBIS). The scheme is designed to conform to the LDD broadcast
in our project DAYS. As discussed earlier, we use the
containment property of LDD in the indexing scheme. This
significantly limits the search of our required data to a particular
portion of broadcast. Thus, we argue that the scheme provides
bounded tuning time.
We describe the architecture of our indexing scheme. Our scheme
contains separate data buckets and index buckets. The index
buckets are of two types. The first type is called the Major index.
The Major index provides information about the types of data
broadcasted. For example, if we intend to broadcast information
like Entertainment, Weather, Traffic etc., then the major index
points to either these major types of information and/or their main
subtypes of information, the number of main subtypes varying
from one information to another. This strictly limits number of
accesses to a Major index. The Major index never points to the
original data. It points to the sub indexes called the Minor index.
The minor indexes are the indexes which actually points to the
original data. We called these minor index pointers as Location
Pointers as they points to the data which are associated with a
location. Thus, our search for a data includes accessing of a major
index and some minor indexes, the number of minor index
varying depending on the type of information.
Thus, our indexing scheme takes into account the hierarchical
nature of the LDD, the Containment property, and requires our
broadcast schedule to be clustered based on data type and
location. The structure of the location hierarchy requires the use
of different types of index at different levels. The structure and
positions of index strictly depend on the location hierarchy as
described in our mapping scheme earlier. We illustrate the
implementation of our scheme with an example. The rules for
framing the index are mentioned subsequently.
20
A1
Entertainment
Resturant
Movie
A2
A3
A4
R1
R2
R3
R4
R5
R6
R7
R8
Weather
KC
SL
JC
SF
Entertainment
R1 R2 R3 R4 R5 R6 R7 R8 KC SL JC SF
(A, R, NEXT = 8)
3, R5
4, R7
Type (S, L)
ER
W
E
EM
(1, 4)
(5, 4)
(1, 4), (9, 4)
(9, 4)
Type (S, L)
W
E
EM
ER
(1, 4)
(5, 8)
(5, 4)
(9, 4)
Type (S, L)
E
EM
ER
W
(1, 8)
(1, 4)
(5, 4)
(9, 4)
A1 A2 A3 A4
Movie Resturant Weather
1 2 3 4 5 6 7 8 9 10 11 12
Major index Major index Major index
Minor index
Major index Minor index
Figure 3. Location Mapped Information for Broadcast Figure 4. Data coupled with Location based Index
Example: Let us suppose that our broadcast content contains
ICentertainment and ICweather which is represented as shown in Fig. 3.
Ai represents Areas of City and Ri represents roads in a certain
area. The leaves of Weather structure represent four cities. The
index structure is given in Fig. 4 which shows the position of
major and minor index and data in the broadcast schedule.
We propose the following rules for the creation of the air indexed
broadcast schedule:
• The major index and the minor index are created.
• The major index contains the position and range of different
types of data items (Weather and Entertainment, Figure 3)
and their categories. The sub categories of Entertainment,
Movie and Restaurant, are also in the index. Thus, the major
index contains Entertainment (E), Entertainment-Movie
(EM), Entertainment-Restaurant (ER), and Weather (W). The
tuple (S, L) represents the starting position (S) of the data
item and L represents the range of the item in terms of
number of data buckets.
• The minor index contains the variables A, R and a pointer
Next. In our example (Figure 3), road R represents the first
node of area A. The minor index is used to point to actual
data buckets present at the lowest levels of the hierarchy. In
contrast, the major index points to a broader range of
locations and so it contains information about main and sub
categories of data.
• Index information is not incorporated in the data buckets.
Index buckets are separate containing only the control
information.
• The number of major index buckets m=#(IC), IC = {ic1, ic2,
ic3,…,icn} where ici represent information type and #
represents the cardinality of the Information Content set IC.
In this example, IC= {icMovie, icWeather, icRestaurant} and so
#(IC) =3. Hence, the number of major index buckets is 3.
• Mechanism to resolve the query is present in the java based
coordinator in MU. For example, if a query Q is presented as
Q (Entertainment, Movie, Road_1), then the resultant search
will be for the EM information in the major index. We say,
Q EM.
Our proposed index works as follows: Let us suppose that an MU
issues a query which is represented by Java Coordinator present in
the MU as Restaurant information on Road 7. This is resolved
by the coordinator as Q ER. This means one has to search for
ER unit of index in the major index. Let us suppose that the MU
logs into the channel at R2. The first index it receives is a minor
index after R2. In this index, value of Next variable = 4, which
means that the next major index is present after bucket 4. The MU
may go into doze mode. It becomes active after bucket 4 and
receives the major index. It searches for ER information which is
the first entry in this index. It is now certain that the MU will get
the position of the data bucket in the adjoining minor index. The
second unit in the minor index depicts the position of the required
data R7. It tells that the data bucket is the first bucket in Area 4.
The MU goes into doze mode again and becomes active after
bucket 6. It gets the required data in the next bucket. We present
the algorithm for searching the location based Index.
Algorithm 1 Location based Index Search in DAYS
1. Scan broadcast for the next index bucket, found=false
2. While (not found) do
3. if bucket is Major Index then
4. Find the Type & Tuple (S, L)
5. if S is greater than 1, go into doze mode for S seconds
6. end if
7. Wake up at the Sth
bucket and observe the Minor Index
8. end if
9. if bucket is Minor Index then
10. if TypeRequested not equal to Typefound and (A,R)Request not
equal to (A,R)found then
11. Go into doze mode till NEXT & repeat from step 3
12. end if
13. else find entry in Minor Index which points to data
14. Compute time of arrival T of data bucket
15. Go into doze mode till T
16. Wake up at T and access data, found = true
17. end else
18. end if
19. end While
21
6. PERFORMANCE EVALUATION
Conservation of energy is the main concern when we try to access
data from wireless broadcast. An efficient scheme should allow
the mobile device to access its required data by staying active for
a minimum amount of time. This would save considerable amount
of energy. Since items are distributed based on types and are
mapped to suitable locations, we argue that our broadcast deals
with clustered data types. The mobile unit has to access a larger
major index and a relatively much smaller minor index to get
information about the time of arrival of data. This is in contrast to
the exponential scheme where the indexes are of equal sizes. The
example discussed and Algorithm 1 reveals that to access any
data, we need to access the major index only once followed by
one or more accesses to the minor index. The number of minor
index access depends on the number of internal locations. As the
number of internal locations vary for item to item (for example,
Weather is generally associated with a City whereas traffic is
granulated up to major and minor roads of a city), we argue that
the structure of the location mapped information may be
visualized as a forest which is a collection of general trees, the
number of general trees depending on the types of information
broadcasted and depth of a tree depending on the granularity of
the location information associated with the information.
For our experiments, we assume the forest as a collection of
balanced M-ary trees. We further assume the M-ary trees to be
full by assuming the presence of dummy nodes in different levels
of a tree.
Thus, if the number of data items is d and the height of the tree is
m, then
n= (m*d-1)/(m-1) where n is the number of vertices in the tree and
i= (d-1)/(m-1) where i is the number of internal vertices.
Tuning time for a data item involves 1 unit of time required to
access the major index plus time required to access the data items
present in the leaves of the tree.
Thus, tuning time with d data items is t = logmd+1
We can say that tuning time is bounded by O(logmd).
We compare our scheme with the distributed indexing and
exponential scheme. We assume a flat broadcast and number of
pages varying from 5000 to 25000. The various simulation
parameters are shown in Table 1.
Figure 5-8 shows the relative tuning times of three indexing
algorithms, ie, the LBIS, exponential scheme and the distributed
tree scheme. Figure 5 shows the result for number of internal
location nodes = 3. We can see that LBIS significantly
outperforms both the other schemes. The tuning time in LBIS
ranges from approx 6.8 to 8. This large tuning time is due to the
fact that after reaching the lowest minor index, the MU may have
to access few buckets sequentially to get the required data bucket.
We can see that the tuning time tends to become stable as the
length of broadcast increases. In figure 6 we consider m= 4. Here
we can see that the exponential and the distributed perform almost
similarly, though the former seems to perform slightly better as
the broadcast length increases. A very interesting pattern is visible
in figure 7. For smaller broadcast size, the LBIS seems to have
larger tuning time than the other two schemes. But as the length of
broadcast increases, it is clearly visible the LBIS outperforms the
other two schemes. The Distributed tree indexing shows similar
behavior like the LBIS. The tuning time in LBIS remains low
because the algorithm allows the MU to skip some intermediate
Minor Indexes. This allows the MU to move into lower levels
directly after coming into active mode, thus saving valuable
energy. This action is not possible in the distributed tree indexing
and hence we can observe that its tuning time is more than the
LBIS scheme, although it performs better than the exponential
scheme. Figure 8, in contrast, shows us that the tuning time in
LBIS, though less than the other two schemes, tends to increase
sharply as the broadcast length becomes greater than the 15000
pages. This may be attributed both due to increase in time
required to scan the intermediate Minor Indexes and the length of
the broadcast. But we can observe that the slope of the LBIS
curve is significantly less than the other two curves.
Table 1 Simulation Parameters
P Definition Values
N Number of data Items 5000 - 25000
m Number of internal location nodes 3, 4, 5, 6
B Capacity of bucket without index (for
exponential index)
10,64,128,256
i Index base for exponential index 2,4,6,8
k Index size for distributed tree 8 bytes
The simulation results establish some facts about our
location based indexing scheme. The scheme performs
better than the other two schemes in terms of tuning time in
most of the cases. As the length of the broadcast increases, after a
certain point, though the tuning time increases as a result of
factors which we have described before, the scheme always
performs better than the other two schemes. Due to the prescribed
limit of the number of pages in the paper, we are unable to show
more results. But these omitted results show similar trend as the
results depicted in figure 5-8.
7. CONCLUSION AND FUTURE WORK
In this paper we have presented a scheme for mapping of wireless
broadcast data with their locations. We have presented an example
to show how the hierarchical structure of the location tree maps
with the data to create LDD. We have presented a scheme called
LBIS to index this LDD. We have used the containment property
of LDD in the scheme that limits the search to a narrow range of
data in the broadcast, thus saving valuable energy in the device.
The mapping of data with locations and the indexing scheme will
be used in our DAYS project to create the push based
architecture. The LBIS has been compared with two other
prominent indexing schemes, i.e., the distributed tree indexing
scheme and the exponential indexing scheme. We showed in our
simulations that the LBIS scheme has the lowest tuning time for
broadcasts having large number of pages, thus saving valuable
battery power in the MU.
22
In the future work we try to incorporate pull based architecture in
our DAYS project. Data from the server is available for access by
the global users. This may be done by putting a request to the
source server. The query in this case is a global query. It is
transferred from the user"s source server to the destination server
through the use of LEO satellites. We intend to use our LDD
scheme and data staging architecture in the pull based architecture.
We will show that the LDD scheme together with the data staging
architecture significantly improves the latency for global as well as
local query.
8. REFERENCES
[1] Acharya, S., Alonso, R. Franklin, M and Zdonik S. Broadcast
disk: Data management for asymmetric communications
environments. In Proceedings of ACM SIGMOD Conference
on Management of Data, pages 199-210, San Jose, CA, May
1995.
[2] Chen, M.S.,Wu, K.L. and Yu, P. S. Optimizing index
allocation for sequential data broadcasting in wireless mobile
computing. IEEE Transactions on Knowledge and Data
Engineering (TKDE), 15(1):161-173, January/February 2003.
Figure 5. Broadcast Size (# buckets)
Dist tree
Expo
LBIS
Figure 6. Broadcast Size (# buckets)
Dist tree
Expo
LBIS
Figure 7. Broadcast Size (# buckets)
Dist tree
Expo
LBIS
Figure 8. Broadcast Size (# buckets)
Dist tree
Expo
LBIS
Averagetuningtime
Averagetuningtime
Averagetuningtime
Averagetuningtime
23
[3] Hu, Q. L., Lee, D. L. and Lee, W.C. Performance evaluation
of a wireless hierarchical data dissemination system. In
Proceedings of the 5th
Annual ACM International Conference
on Mobile Computing and Networking (MobiCom"99), pages
163-173, Seattle, WA, August 1999.
[4] Hu, Q. L. Lee, W.C. and Lee, D. L. Power conservative
multi-attribute queries on data broadcast. In Proceedings of
the 16th International Conference on Data Engineering
(ICDE"00), pages 157-166, San Diego, CA, February 2000.
[5] Imielinski, T., Viswanathan, S. and Badrinath. B. R. Power
efficient filtering of data on air. In Proceedings of the 4th
International Conference on Extending Database Technology
(EDBT"94), pages 245-258, Cambridge, UK, March 1994.
[6] Imielinski, T., Viswanathan, S. and Badrinath. B. R. Data on
air - Organization and access. IEEE Transactions on
Knowledge and Data Engineering (TKDE), 9(3):353-372,
May/June 1997.
[7] Shih, E., Bahl, P. and Sinclair, M. J. Wake on wireless: An
event driven energy saving strategy for battery operated
devices. In Proceedings of the 8th Annual ACM International
Conference on Mobile Computing and Networking
(MobiCom"02), pages 160-171, Atlanta, GA, September
2002.
[8] Shivakumar N. and Venkatasubramanian, S. Energy-efficient
indexing for information dissemination in wireless systems.
ACM/Baltzer Journal of Mobile Networks and Applications
(MONET), 1(4):433-446, December 1996.
[9] Tan K. L. and Yu, J. X. Energy efficient filtering of non
uniform broadcast. In Proceedings of the 16th International
Conference on Distributed Computing Systems (ICDCS"96),
pages 520-527, Hong Kong, May 1996.
[10] Viredaz, M. A., Brakmo, L. S. and Hamburgen, W. R. Energy
management on handheld devices. ACM Queue, 1(7):44-52,
October 2003.
[11] Garg, N. Kumar, V., & Dunham, M.H. Information Mapping
and Indexing in DAYS, 6th International Workshop on
Mobility in Databases and Distributed Systems, in
conjunction with the 14th International Conference on
Database and Expert Systems Applications September 1-5,
Prague, Czech Republic, 2003.
[12] Acharya D., Kumar, V., & Dunham, M.H. InfoSpace: Hybrid
and Adaptive Public Data Dissemination System for
Ubiquitous Computing. Accepted for publication in the
special issue of Pervasive Computing. Wiley Journal for
Wireless Communications and Mobile Computing, 2004.
[13] Acharya D., Kumar, V., & Prabhu, N. Discovering and using
Web Services in M-Commerce, Proceedings for 5th VLDB
Workshop on Technologies for E-Services, Toronto,
Canada,2004.
[14] Acharya D., Kumar, V. Indexing Location Dependent Data in
broadcast environment. Accepted for publication, JDIM
special issue on Distributed Data Management, 2005.
[15] Flinn, J., Sinnamohideen, S., & Satyanarayan, M. Data
Staging on Untrusted Surrogates, Intel Research, Pittsburg,
Unpublished Report, 2003.
[16] Seydim, A.Y., Dunham, M.H. & Kumar, V. Location
dependent query processing, Proceedings of the 2nd ACM
international workshop on Data engineering for wireless and
mobile access, p.47-53, Santa Barbara, California, USA,
2001.
[17] Xu, J., Lee, W.C., Tang., X. Exponential Index: A
Parameterized Distributed Indexing Scheme for Data on Air.
In Proceedings of the 2nd ACM/USENIX International
Conference on Mobile Systems, Applications, and Services
(MobiSys'04), Boston, MA, June 2004.
24
Automatic Extraction of Titles from General Documents
using Machine Learning
Yunhua Hu1
Computer Science Department
Xi"an Jiaotong University
No 28, Xianning West Road
Xi'an, China, 710049
yunhuahu@mail.xjtu.edu.cn
Hang Li, Yunbo Cao
Microsoft Research Asia
5F Sigma Center,
No. 49 Zhichun Road, Haidian,
Beijing, China, 100080
{hangli,yucao}@microsoft.com
Qinghua Zheng
Computer Science Department
Xi"an Jiaotong University
No 28, Xianning West Road
Xi'an, China, 710049
qhzheng@mail.xjtu.edu.cn
Dmitriy Meyerzon
Microsoft Corporation
One Microsoft Way
Redmond, WA,
USA, 98052
dmitriym@microsoft.com
ABSTRACT
In this paper, we propose a machine learning approach to title
extraction from general documents. By general documents, we
mean documents that can belong to any one of a number of
specific genres, including presentations, book chapters, technical
papers, brochures, reports, and letters. Previously, methods have
been proposed mainly for title extraction from research papers. It
has not been clear whether it could be possible to conduct
automatic title extraction from general documents. As a case study,
we consider extraction from Office including Word and
PowerPoint. In our approach, we annotate titles in sample
documents (for Word and PowerPoint respectively) and take them
as training data, train machine learning models, and perform title
extraction using the trained models. Our method is unique in that
we mainly utilize formatting information such as font size as
features in the models. It turns out that the use of formatting
information can lead to quite accurate extraction from general
documents. Precision and recall for title extraction from Word is
0.810 and 0.837 respectively, and precision and recall for title
extraction from PowerPoint is 0.875 and 0.895 respectively in an
experiment on intranet data. Other important new findings in this
work include that we can train models in one domain and apply
them to another domain, and more surprisingly we can even train
models in one language and apply them to another language.
Moreover, we can significantly improve search ranking results in
document retrieval by using the extracted titles.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval - Search Process; H.4.1 [Information Systems
Applications]: Office Automation - Word processing; D.2.8
[Software Engineering]: Metrics - complexity measures,
performance measures
General Terms
Algorithms, Experimentation, Performance.
1. INTRODUCTION
Metadata of documents is useful for many kinds of document
processing such as search, browsing, and filtering. Ideally,
metadata is defined by the authors of documents and is then used
by various systems. However, people seldom define document
metadata by themselves, even when they have convenient
metadata definition tools [26]. Thus, how to automatically extract
metadata from the bodies of documents turns out to be an
important research issue.
Methods for performing the task have been proposed. However,
the focus was mainly on extraction from research papers. For
instance, Han et al. [10] proposed a machine learning based
method to conduct extraction from research papers. They
formalized the problem as that of classification and employed
Support Vector Machines as the classifier. They mainly used
linguistic features in the model.1
In this paper, we consider metadata extraction from general
documents. By general documents, we mean documents that may
belong to any one of a number of specific genres. General
documents are more widely available in digital libraries, intranets
and the internet, and thus investigation on extraction from them is
sorely needed. Research papers usually have well-formed styles
and noticeable characteristics. In contrast, the styles of general
documents can vary greatly. It has not been clarified whether a
machine learning based approach can work well for this task.
There are many types of metadata: title, author, date of creation,
etc. As a case study, we consider title extraction in this paper.
General documents can be in many different file formats:
Microsoft Office, PDF (PS), etc. As a case study, we consider
extraction from Office including Word and PowerPoint.
We take a machine learning approach. We annotate titles in
sample documents (for Word and PowerPoint respectively) and
take them as training data to train several types of models, and
perform title extraction using any one type of the trained models.
In the models, we mainly utilize formatting information such as
font size as features. We employ the following models: Maximum
Entropy Model, Perceptron with Uneven Margins, Maximum
Entropy Markov Model, and Voted Perceptron.
In this paper, we also investigate the following three problems,
which did not seem to have been examined previously.
(1) Comparison between models: among the models above, which
model performs best for title extraction;
(2) Generality of model: whether it is possible to train a model on
one domain and apply it to another domain, and whether it is
possible to train a model in one language and apply it to another
language;
(3) Usefulness of extracted titles: whether extracted titles can
improve document processing such as search.
Experimental results indicate that our approach works well for
title extraction from general documents. Our method can
significantly outperform the baselines: one that always uses the
first lines as titles and the other that always uses the lines in the
largest font sizes as titles. Precision and recall for title extraction
from Word are 0.810 and 0.837 respectively, and precision and
recall for title extraction from PowerPoint are 0.875 and 0.895
respectively. It turns out that the use of format features is the key
to successful title extraction.
(1) We have observed that Perceptron based models perform
better in terms of extraction accuracies. (2) We have empirically
verified that the models trained with our approach are generic in
the sense that they can be trained on one domain and applied to
another, and they can be trained in one language and applied to
another. (3) We have found that using the extracted titles we can
significantly improve precision of document retrieval (by 10%).
We conclude that we can indeed conduct reliable title extraction
from general documents and use the extracted results to improve
real applications.
The rest of the paper is organized as follows. In section 2, we
introduce related work, and in section 3, we explain the
motivation and problem setting of our work. In section 4, we
describe our method of title extraction, and in section 5, we
describe our method of document retrieval using extracted titles.
Section 6 gives our experimental results. We make concluding
remarks in section 7.
2. RELATED WORK
2.1 Document Metadata Extraction
Methods have been proposed for performing automatic metadata
extraction from documents; however, the main focus was on
extraction from research papers.
The proposed methods fall into two categories: the rule based
approach and the machine learning based approach.
Giuffrida et al. [9], for instance, developed a rule-based system for
automatically extracting metadata from research papers in
Postscript. They used rules like titles are usually located on the
upper portions of the first pages and they are usually in the largest
font sizes. Liddy et al. [14] and Yilmazel el al. [23] performed
metadata extraction from educational materials using rule-based
natural language processing technologies. Mao et al. [16] also
conducted automatic metadata extraction from research papers
using rules on formatting information.
The rule-based approach can achieve high performance. However,
it also has disadvantages. It is less adaptive and robust when
compared with the machine learning approach.
Han et al. [10], for instance, conducted metadata extraction with
the machine learning approach. They viewed the problem as that
of classifying the lines in a document into the categories of
metadata and proposed using Support Vector Machines as the
classifier. They mainly used linguistic information as features.
They reported high extraction accuracy from research papers in
terms of precision and recall.
2.2 Information Extraction
Metadata extraction can be viewed as an application of
information extraction, in which given a sequence of instances, we
identify a subsequence that represents information in which we
are interested. Hidden Markov Model [6], Maximum Entropy
Model [1, 4], Maximum Entropy Markov Model [17], Support
Vector Machines [3], Conditional Random Field [12], and Voted
Perceptron [2] are widely used information extraction models.
Information extraction has been applied, for instance, to 
part-ofspeech tagging [20], named entity recognition [25] and table
extraction [19].
2.3 Search Using Title Information
Title information is useful for document retrieval.
In the system Citeseer, for instance, Giles et al. managed to
extract titles from research papers and make use of the extracted
titles in metadata search of papers [8].
In web search, the title fields (i.e., file properties) and anchor texts
of web pages (HTML documents) can be viewed as ‘titles" of the
pages [5]. Many search engines seem to utilize them for web page
retrieval [7, 11, 18, 22]. Zhang et al., found that web pages with
well-defined metadata are more easily retrieved than those without
well-defined metadata [24].
To the best of our knowledge, no research has been conducted on
using extracted titles from general documents (e.g., Office
documents) for search of the documents.
146
3. MOTIVATION AND PROBLEM
SETTING
We consider the issue of automatically extracting titles from
general documents.
By general documents, we mean documents that belong to one of
any number of specific genres. The documents can be
presentations, books, book chapters, technical papers, brochures,
reports, memos, specifications, letters, announcements, or resumes.
General documents are more widely available in digital libraries,
intranets, and internet, and thus investigation on title extraction
from them is sorely needed.
Figure 1 shows an estimate on distributions of file formats on
intranet and internet [15]. Office and PDF are the main file
formats on the intranet. Even on the internet, the documents in the
formats are still not negligible, given its extremely large size. In
this paper, without loss of generality, we take Office documents as
an example.
Figure 1. Distributions of file formats in internet and intranet.
For Office documents, users can define titles as file properties
using a feature provided by Office. We found in an experiment,
however, that users seldom use the feature and thus titles in file
properties are usually very inaccurate. That is to say, titles in file
properties are usually inconsistent with the ‘true" titles in the file
bodies that are created by the authors and are visible to readers.
We collected 6,000 Word and 6,000 PowerPoint documents from
an intranet and the internet and examined how many titles in the
file properties are correct. We found that surprisingly the accuracy
was only 0.265 (cf., Section 6.3 for details). A number of reasons
can be considered. For example, if one creates a new file by
copying an old file, then the file property of the new file will also
be copied from the old file.
In another experiment, we found that Google uses the titles in file
properties of Office documents in search and browsing, but the
titles are not very accurate. We created 50 queries to search Word
and PowerPoint documents and examined the top 15 results of
each query returned by Google. We found that nearly all the titles
presented in the search results were from the file properties of the
documents. However, only 0.272 of them were correct.
Actually, ‘true" titles usually exist at the beginnings of the bodies
of documents. If we can accurately extract the titles from the
bodies of documents, then we can exploit reliable title information
in document processing. This is exactly the problem we address in
this paper.
More specifically, given a Word document, we are to extract the
title from the top region of the first page. Given a PowerPoint
document, we are to extract the title from the first slide. A title
sometimes consists of a main title and one or two subtitles. We
only consider extraction of the main title.
As baselines for title extraction, we use that of always using the
first lines as titles and that of always using the lines with largest
font sizes as titles.
Figure 2. Title extraction from Word document.
Figure 3. Title extraction from PowerPoint document.
Next, we define a ‘specification" for human judgments in title data
annotation. The annotated data will be used in training and testing
of the title extraction methods.
Summary of the specification: The title of a document should be
identified on the basis of common sense, if there is no difficulty in
the identification. However, there are many cases in which the
identification is not easy. There are some rules defined in the
specification that guide identification for such cases. The rules
include a title is usually in consecutive lines in the same format,
a document can have no title, titles in images are not
considered, a title should not contain words like ‘draft",
147
‘whitepaper", etc, if it is difficult to determine which is the title,
select the one in the largest font size, and if it is still difficult to
determine which is the title, select the first candidate. (The
specification covers all the cases we have encountered in data
annotation.)
Figures 2 and 3 show examples of Office documents from which
we conduct title extraction. In Figure 2, ‘Differences in Win32
API Implementations among Windows Operating Systems" is the
title of the Word document. ‘Microsoft Windows" on the top of
this page is a picture and thus is ignored. In Figure 3, ‘Building
Competitive Advantages through an Agile Infrastructure" is the
title of the PowerPoint document.
We have developed a tool for annotation of titles by human
annotators. Figure 4 shows a snapshot of the tool.
Figure 4. Title annotation tool.
4. TITLE EXTRACTION METHOD
4.1 Outline
Title extraction based on machine learning consists of training and
extraction. The same pre-processing step occurs before training
and extraction.
During pre-processing, from the top region of the first page of a
Word document or the first slide of a PowerPoint document a
number of units for processing are extracted. If a line (lines are
separated by ‘return" symbols) only has a single format, then the
line will become a unit. If a line has several parts and each of
them has its own format, then each part will become a unit. Each
unit will be treated as an instance in learning. A unit contains not
only content information (linguistic information) but also
formatting information. The input to pre-processing is a document
and the output of pre-processing is a sequence of units (instances).
Figure 5 shows the units obtained from the document in Figure 2.
Figure 5. Example of units.
In learning, the input is sequences of units where each sequence
corresponds to a document. We take labeled units (labeled as
title_begin, title_end, or other) in the sequences as training data
and construct models for identifying whether a unit is title_begin
title_end, or other. We employ four types of models: Perceptron,
Maximum Entropy (ME), Perceptron Markov Model (PMM), and
Maximum Entropy Markov Model (MEMM).
In extraction, the input is a sequence of units from one document.
We employ one type of model to identify whether a unit is
title_begin, title_end, or other. We then extract units from the unit
labeled with ‘title_begin" to the unit labeled with ‘title_end". The
result is the extracted title of the document.
The unique characteristic of our approach is that we mainly utilize
formatting information for title extraction. Our assumption is that
although general documents vary in styles, their formats have
certain patterns and we can learn and utilize the patterns for title
extraction. This is in contrast to the work by Han et al., in which
only linguistic features are used for extraction from research
papers.
4.2 Models
The four models actually can be considered in the same metadata
extraction framework. That is why we apply them together to our
current problem.
Each input is a sequence of instances kxxx L21 together with a
sequence of labels kyyy L21 . ix and iy represents an instance
and its label, respectively ( ki ,,2,1 L= ). Recall that an instance
here represents a unit. A label represents title_begin, title_end, or
other. Here, k is the number of units in a document.
In learning, we train a model which can be generally denoted as a
conditional probability distribution )|( 11 kk XXYYP LL where
iX and iY denote random variables taking instance ix and label
iy as values, respectively ( ki ,,2,1 L= ).
Learning Tool
Extraction Tool
21121
2222122221
1121111211
nknnknn
kk
kk
yyyxxx
yyyxxx
yyyxxx
LL
LL
LL
LL
→
→
→
)|(maxarg 11 mkmmkm xxyyP LL
)|( 11 kk XXYYP LL
Conditional
Distribution
mkmm xxx L21
Figure 6. Metadata extraction model.
We can make assumptions about the general model in order to
make it simple enough for training.
148
For example, we can assume that kYY ,,1 L are independent of
each other given kXX ,,1 L . Thus, we have
)|()|(
)|(
11
11
kk
kk
XYPXYP
XXYYP
L
LL
=
In this way, we decompose the model into a number of classifiers.
We train the classifiers locally using the labeled data. As the
classifier, we employ the Perceptron or Maximum Entropy model.
We can also assume that the first order Markov property holds for
kYY ,,1 L given kXX ,,1 L . Thus, we have
)|()|(
)|(
111
11
kkk
kk
XYYPXYP
XXYYP
−= L
LL
Again, we obtain a number of classifiers. However, the classifiers
are conditioned on the previous label. When we employ the
Percepton or Maximum Entropy model as a classifier, the models
become a Percepton Markov Model or Maximum Entropy Markov
Model, respectively. That is to say, the two models are more
precise.
In extraction, given a new sequence of instances, we resort to one
of the constructed models to assign a sequence of labels to the
sequence of instances, i.e., perform extraction.
For Perceptron and ME, we assign labels locally and combine the
results globally later using heuristics. Specifically, we first
identify the most likely title_begin. Then we find the most likely
title_end within three units after the title_begin. Finally, we
extract as a title the units between the title_begin and the title_end.
For PMM and MEMM, we employ the Viterbi algorithm to find
the globally optimal label sequence.
In this paper, for Perceptron, we actually employ an improved
variant of it, called Perceptron with Uneven Margin [13]. This
version of Perceptron can work well especially when the number
of positive instances and the number of negative instances differ
greatly, which is exactly the case in our problem.
We also employ an improved version of Perceptron Markov
Model in which the Perceptron model is the so-called Voted
Perceptron [2]. In addition, in training, the parameters of the
model are updated globally rather than locally.
4.3 Features
There are two types of features: format features and linguistic
features. We mainly use the former. The features are used for both
the title-begin and the title-end classifiers.
4.3.1 Format Features
Font Size: There are four binary features that represent the
normalized font size of the unit (recall that a unit has only one
type of font).
If the font size of the unit is the largest in the document, then the
first feature will be 1, otherwise 0. If the font size is the smallest
in the document, then the fourth feature will be 1, otherwise 0. If
the font size is above the average font size and not the largest in
the document, then the second feature will be 1, otherwise 0. If the
font size is below the average font size and not the smallest, the
third feature will be 1, otherwise 0.
It is necessary to conduct normalization on font sizes. For
example, in one document the largest font size might be ‘12pt",
while in another the smallest one might be ‘18pt".
Boldface: This binary feature represents whether or not the
current unit is in boldface.
Alignment: There are four binary features that respectively
represent the location of the current unit: ‘left", ‘center", ‘right",
and ‘unknown alignment".
The following format features with respect to ‘context" play an
important role in title extraction.
Empty Neighboring Unit: There are two binary features that
represent, respectively, whether or not the previous unit and the
current unit are blank lines.
Font Size Change: There are two binary features that represent,
respectively, whether or not the font size of the previous unit and
the font size of the next unit differ from that of the current unit.
Alignment Change: There are two binary features that represent,
respectively, whether or not the alignment of the previous unit and
the alignment of the next unit differ from that of the current one.
Same Paragraph: There are two binary features that represent,
respectively, whether or not the previous unit and the next unit are
in the same paragraph as the current unit.
4.3.2 Linguistic Features
The linguistic features are based on key words.
Positive Word: This binary feature represents whether or not the
current unit begins with one of the positive words. The positive
words include ‘title:", ‘subject:", ‘subject line:" For example, in
some documents the lines of titles and authors have the same
formats. However, if lines begin with one of the positive words,
then it is likely that they are title lines.
Negative Word: This binary feature represents whether or not the
current unit begins with one of the negative words. The negative
words include ‘To", ‘By", ‘created by", ‘updated by", etc.
There are more negative words than positive words. The above
linguistic features are language dependent.
Word Count: A title should not be too long. We heuristically
create four intervals: [1, 2], [3, 6], [7, 9] and [9, ∞) and define one
feature for each interval. If the number of words in a title falls into
an interval, then the corresponding feature will be 1; otherwise 0.
Ending Character: This feature represents whether the unit ends
with ‘:", ‘-", or other special characters. A title usually does not
end with such a character.
5. DOCUMENT RETRIEVAL METHOD
We describe our method of document retrieval using extracted
titles.
Typically, in information retrieval a document is split into a
number of fields including body, title, and anchor text. A ranking
function in search can use different weights for different fields of
149
the document. Also, titles are typically assigned high weights,
indicating that they are important for document retrieval. As
explained previously, our experiment has shown that a significant
number of documents actually have incorrect titles in the file
properties, and thus in addition of using them we use the extracted
titles as one more field of the document. By doing this, we attempt
to improve the overall precision.
In this paper, we employ a modification of BM25 that allows field
weighting [21]. As fields, we make use of body, title, extracted
title and anchor. First, for each term in the query we count the
term frequency in each field of the document; each field
frequency is then weighted according to the corresponding weight
parameter:
∑=
f
tfft tfwwtf
Similarly, we compute the document length as a weighted sum of
lengths of each field. Average document length in the corpus
becomes the average of all weighted document lengths.
∑=
f
ff dlwwdl
In our experiments we used 75.0,8.11 == bk . Weight for content
was 1.0, title was 10.0, anchor was 10.0, and extracted title was
5.0.
6. EXPERIMENTAL RESULTS
6.1 Data Sets and Evaluation Measures
We used two data sets in our experiments.
First, we downloaded and randomly selected 5,000 Word
documents and 5,000 PowerPoint documents from an intranet of
Microsoft. We call it MS hereafter.
Second, we downloaded and randomly selected 500 Word and 500
PowerPoint documents from the DotGov and DotCom domains on
the internet, respectively.
Figure 7 shows the distributions of the genres of the documents.
We see that the documents are indeed ‘general documents" as we
define them.
Figure 7. Distributions of document genres.
Third, a data set in Chinese was also downloaded from the internet.
It includes 500 Word documents and 500 PowerPoint documents
in Chinese.
We manually labeled the titles of all the documents, on the basis
of our specification.
Not all the documents in the two data sets have titles. Table 1
shows the percentages of the documents having titles. We see that
DotCom and DotGov have more PowerPoint documents with titles
than MS. This might be because PowerPoint documents published
on the internet are more formal than those on the intranet.
Table 1. The portion of documents with titles
Domain
Type
MS DotCom DotGov
Word 75.7% 77.8% 75.6%
PowerPoint 82.1% 93.4% 96.4%
In our experiments, we conducted evaluations on title extraction in
terms of precision, recall, and F-measure. The evaluation
measures are defined as follows:
Precision: P = A / ( A + B )
Recall: R = A / ( A + C )
F-measure: F1 = 2PR / ( P + R )
Here, A, B, C, and D are numbers of documents as those defined
in Table 2.
Table 2. Contingence table with regard to title extraction
Is title Is not title
Extracted A B
Not extracted C D
6.2 Baselines
We test the accuracies of the two baselines described in section
4.2. They are denoted as ‘largest font size" and ‘first line"
respectively.
6.3 Accuracy of Titles in File Properties
We investigate how many titles in the file properties of the
documents are reliable. We view the titles annotated by humans as
true titles and test how many titles in the file properties can
approximately match with the true titles. We use Edit Distance to
conduct the approximate match. (Approximate match is only used
in this evaluation). This is because sometimes human annotated
titles can be slightly different from the titles in file properties on
the surface, e.g., contain extra spaces).
Given string A and string B:
if ( (D == 0) or ( D / ( La + Lb ) < θ ) ) then string A = string B
D: Edit Distance between string A and string B
La: length of string A
Lb: length of string B
θ: 0.1
∑ ×
++−
+
=
t
t
n
N
wtf
avwdl
wdl
bbk
kwtf
FBM )log(
))1((
)1(
25
1
1
150
Table 3. Accuracies of titles in file properties
File Type Domain Precision Recall F1
MS 0.299 0.311 0.305
DotCom 0.210 0.214 0.212Word
DotGov 0.182 0.177 0.180
MS 0.229 0.245 0.237
DotCom 0.185 0.186 0.186PowerPoint
DotGov 0.180 0.182 0.181
6.4 Comparison with Baselines
We conducted title extraction from the first data set (Word and
PowerPoint in MS). As the model, we used Perceptron.
We conduct 4-fold cross validation. Thus, all the results reported
here are those averaged over 4 trials. Tables 4 and 5 show the
results. We see that Perceptron significantly outperforms the
baselines. In the evaluation, we use exact matching between the
true titles annotated by humans and the extracted titles.
Table 4. Accuracies of title extraction with Word
Precision Recall F1
Model Perceptron 0.810 0.837 0.823
Largest font size 0.700 0.758 0.727
Baselines
First line 0.707 0.767 0.736
Table 5. Accuracies of title extraction with PowerPoint
Precision Recall F1
Model Perceptron 0.875 0. 895 0.885
Largest font size 0.844 0.887 0.865
Baselines
First line 0.639 0.671 0.655
We see that the machine learning approach can achieve good
performance in title extraction. For Word documents both
precision and recall of the approach are 8 percent higher than
those of the baselines. For PowerPoint both precision and recall of
the approach are 2 percent higher than those of the baselines.
We conduct significance tests. The results are shown in Table 6.
Here, ‘Largest" denotes the baseline of using the largest font size,
‘First" denotes the baseline of using the first line. The results
indicate that the improvements of machine learning over baselines
are statistically significant (in the sense p-value < 0.05)
Table 6. Sign test results
Documents Type Sign test between p-value
Perceptron vs. Largest 3.59e-26
Word
Perceptron vs. First 7.12e-10
Perceptron vs. Largest 0.010
PowerPoint
Perceptron vs. First 5.13e-40
We see, from the results, that the two baselines can work well for
title extraction, suggesting that font size and position information
are most useful features for title extraction. However, it is also
obvious that using only these two features is not enough. There
are cases in which all the lines have the same font size (i.e., the
largest font size), or cases in which the lines with the largest font
size only contain general descriptions like ‘Confidential", ‘White
paper", etc. For those cases, the ‘largest font size" method cannot
work well. For similar reasons, the ‘first line" method alone
cannot work well, either. With the combination of different
features (evidence in title judgment), Perceptron can outperform
Largest and First.
We investigate the performance of solely using linguistic features.
We found that it does not work well. It seems that the format
features play important roles and the linguistic features are
supplements..
Figure 8. An example Word document.
Figure 9. An example PowerPoint document.
We conducted an error analysis on the results of Perceptron. We
found that the errors fell into three categories. (1) About one third
of the errors were related to ‘hard cases". In these documents, the
layouts of the first pages were difficult to understand, even for
humans. Figure 8 and 9 shows examples. (2) Nearly one fourth of
the errors were from the documents which do not have true titles
but only contain bullets. Since we conduct extraction from the top
regions, it is difficult to get rid of these errors with the current
approach. (3). Confusions between main titles and subtitles were
another type of error. Since we only labeled the main titles as
titles, the extractions of both titles were considered incorrect. This
type of error does little harm to document processing like search,
however.
6.5 Comparison between Models
To compare the performance of different machine learning models,
we conducted another experiment. Again, we perform 4-fold cross
151
validation on the first data set (MS). Table 7, 8 shows the results
of all the four models.
It turns out that Perceptron and PMM perform the best, followed
by MEMM, and ME performs the worst. In general, the
Markovian models perform better than or as well as their classifier
counterparts. This seems to be because the Markovian models are
trained globally, while the classifiers are trained locally. The
Perceptron based models perform better than the ME based
counterparts. This seems to be because the Perceptron based
models are created to make better classifications, while ME
models are constructed for better prediction.
Table 7. Comparison between different learning models for
title extraction with Word
Model Precision Recall F1
Perceptron 0.810 0.837 0.823
MEMM 0.797 0.824 0.810
PMM 0.827 0.823 0.825
ME 0.801 0.621 0.699
Table 8. Comparison between different learning models for
title extraction with PowerPoint
Model Precision Recall F1
Perceptron 0.875 0. 895 0. 885
MEMM 0.841 0.861 0.851
PMM 0.873 0.896 0.885
ME 0.753 0.766 0.759
6.6 Domain Adaptation
We apply the model trained with the first data set (MS) to the
second data set (DotCom and DotGov). Tables 9-12 show the
results.
Table 9. Accuracies of title extraction with Word in DotGov
Precision Recall F1
Model Perceptron 0.716 0.759 0.737
Largest font size 0.549 0.619 0.582Baselines
First line 0.462 0.521 0.490
Table 10. Accuracies of title extraction with PowerPoint in
DotGov
Precision Recall F1
Model Perceptron 0.900 0.906 0.903
Largest font size 0.871 0.888 0.879Baselines
First line 0.554 0.564 0.559
Table 11. Accuracies of title extraction with Word in DotCom
Precisio
n
Recall F1
Model Perceptron 0.832 0.880 0.855
Largest font size 0.676 0.753 0.712Baselines
First line 0.577 0.643 0.608
Table 12. Performance of PowerPoint document title
extraction in DotCom
Precisio
n
Recall F1
Model Perceptron 0.910 0.903 0.907
Largest font size 0.864 0.886 0.875Baselines
First line 0.570 0.585 0.577
From the results, we see that the models can be adapted to
different domains well. There is almost no drop in accuracy. The
results indicate that the patterns of title formats exist across
different domains, and it is possible to construct a domain
independent model by mainly using formatting information.
6.7 Language Adaptation
We apply the model trained with the data in English (MS) to the
data set in Chinese.
Tables 13-14 show the results.
Table 13. Accuracies of title extraction with Word in Chinese
Precision Recall F1
Model Perceptron 0.817 0.805 0.811
Largest font size 0.722 0.755 0.738Baselines
First line 0.743 0.777 0.760
Table 14. Accuracies of title extraction with PowerPoint in
Chinese
Precision Recall F1
Model Perceptron 0.766 0.812 0.789
Largest font size 0.753 0.813 0.782Baselines
First line 0.627 0.676 0.650
We see that the models can be adapted to a different language.
There are only small drops in accuracy. Obviously, the linguistic
features do not work for Chinese, but the effect of not using them
is negligible. The results indicate that the patterns of title formats
exist across different languages.
From the domain adaptation and language adaptation results, we
conclude that the use of formatting information is the key to a
successful extraction from general documents.
6.8 Search with Extracted Titles
We performed experiments on using title extraction for document
retrieval. As a baseline, we employed BM25 without using
extracted titles. The ranking mechanism was as described in
Section 5. The weights were heuristically set. We did not conduct
optimization on the weights.
The evaluation was conducted on a corpus of 1.3 M documents
crawled from the intranet of Microsoft using 100 evaluation
queries obtained from this intranet"s search engine query logs. 50
queries were from the most popular set, while 50 queries other
were chosen randomly. Users were asked to provide judgments of
the degree of document relevance from a scale of 1to 5 (1
meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent).
152
Figure 10 shows the results. In the chart two sets of precision
results were obtained by either considering good or excellent
documents as relevant (left 3 bars with relevance threshold 0.5), or
by considering only excellent documents as relevant (right 3 bars
with relevance threshold 1.0)
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
P@10 P@5 Reciprocal P@10 P@5 Reciprocal
0.5 1
BM25 Anchor, Title, Body
BM25 Anchor, Title, Body, ExtractedTitle
Name All
RelevanceThreshold Data
Description
Figure 10. Search ranking results.
Figure 10 shows different document retrieval results with different
ranking functions in terms of precision @10, precision @5 and
reciprocal rank:
• Blue bar - BM25 including the fields body, title (file
property), and anchor text.
• Purple bar - BM25 including the fields body, title (file
property), anchor text, and extracted title.
With the additional field of extracted title included in BM25 the
precision @10 increased from 0.132 to 0.145, or by ~10%. Thus,
it is safe to say that the use of extracted title can indeed improve
the precision of document retrieval.
7. CONCLUSION
In this paper, we have investigated the problem of automatically
extracting titles from general documents. We have tried using a
machine learning approach to address the problem.
Previous work showed that the machine learning approach can
work well for metadata extraction from research papers. In this
paper, we showed that the approach can work for extraction from
general documents as well. Our experimental results indicated that
the machine learning approach can work significantly better than
the baselines in title extraction from Office documents. Previous
work on metadata extraction mainly used linguistic features in
documents, while we mainly used formatting information. It
appeared that using formatting information is a key for
successfully conducting title extraction from general documents.
We tried different machine learning models including Perceptron,
Maximum Entropy, Maximum Entropy Markov Model, and Voted
Perceptron. We found that the performance of the Perceptorn
models was the best. We applied models constructed in one
domain to another domain and applied models trained in one
language to another language. We found that the accuracies did
not drop substantially across different domains and across
different languages, indicating that the models were generic. We
also attempted to use the extracted titles in document retrieval. We
observed a significant improvement in document ranking
performance for search when using extracted title information. All
the above investigations were not conducted in previous work, and
through our investigations we verified the generality and the
significance of the title extraction approach.
8. ACKNOWLEDGEMENTS
We thank Chunyu Wei and Bojuan Zhao for their work on data
annotation. We acknowledge Jinzhu Li for his assistance in
conducting the experiments. We thank Ming Zhou, John Chen,
Jun Xu, and the anonymous reviewers of JCDL"05 for their
valuable comments on this paper.
9. REFERENCES
[1] Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22:39-71, 1996.
[2] Collins, M. Discriminative training methods for hidden
markov models: theory and experiments with perceptron
algorithms. In Proceedings of Conference on Empirical
Methods in Natural Language Processing, 1-8, 2002.
[3] Cortes, C. and Vapnik, V. Support-vector networks. Machine
Learning, 20:273-297, 1995.
[4] Chieu, H. L. and Ng, H. T. A maximum entropy approach to
information extraction from semi-structured and free text. In
Proceedings of the Eighteenth National Conference on
Artificial Intelligence, 768-791, 2002.
[5] Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia
newsblaster: multilingual news summarization on the Web.
In Proceedings of Human Language Technology conference /
North American chapter of the Association for
Computational Linguistics annual meeting, 1-4, 2004.
[6] Ghahramani, Z. and Jordan, M. I. Factorial hidden markov
models. Machine Learning, 29:245-273, 1997.
[7] Gheel, J. and Anderson, T. Data and metadata for finding and
reminding, In Proceedings of the 1999 International
Conference on Information Visualization, 446-451,1999.
[8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H.,
Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a
niche search engine for e-Business. In Proceedings of the
26th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, 
413414, 2003.
[9] Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based
metadata extraction from PostScript files. In Proceedings of
the Fifth ACM Conference on Digital Libraries, 77-84, 2000.
[10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and
Fox, E. A. Automatic document metadata extraction using
support vector machines. In Proceedings of the Third
ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48,
2003.
[11] Kobayashi, M., and Takeda, K. Information retrieval on the
Web. ACM Computing Surveys, 32:144-173, 2000.
[12] Lafferty, J., McCallum, A., and Pereira, F. Conditional
random fields: probabilistic models for segmenting and
153
labeling sequence data. In Proceedings of the Eighteenth
International Conference on Machine Learning, 282-289,
2001.
[13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and
Kandola, J. S. The perceptron algorithm with uneven margins.
In Proceedings of the Nineteenth International Conference
on Machine Learning, 379-386, 2002.
[14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S.,
Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N.,
and Silverstein, J. Automatic Metadata generation &
evaluation. In Proceedings of the 25th Annual International
ACM SIGIR Conference on Research and Development in
Information Retrieval, 401-402, 2002.
[15] Littlefield, A. Effective enterprise information retrieval
across new content formats. In Proceedings of the Seventh
Search Engine Conference,
http://www.infonortics.com/searchengines/sh02/02prog.html,
2002.
[16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature
generation system for automated metadata extraction in
preservation of digital materials. In Proceedings of the First
International Workshop on Document Image Analysis for
Libraries, 225-232, 2004.
[17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy
markov models for information extraction and segmentation.
In Proceedings of the Seventeenth International Conference
on Machine Learning, 591-598, 2000.
[18] Murphy, L. D. Digital document metadata in organizations:
roles, analytical approaches, and future research directions.
In Proceedings of the Thirty-First Annual Hawaii
International Conference on System Sciences, 267-276, 1998.
[19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B. Table
extraction using conditional random fields. In Proceedings of
the 26th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, 
235242, 2003.
[20] Ratnaparkhi, A. Unsupervised statistical models for
prepositional phrase attachment. In Proceedings of the
Seventeenth International Conference on Computational
Linguistics. 1079-1085, 1998.
[21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25
extension to multiple weighted fields, In Proceedings of
ACM Thirteenth Conference on Information and Knowledge
Management, 42-49, 2004.
[22] Yi, J. and Sundaresan, N. Metadata based Web mining for
relevance, In Proceedings of the 2000 International
Symposium on Database Engineering & Applications, 
113121, 2000.
[23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract:
An NLP system to automatically assign metadata. In
Proceedings of the 2004 Joint ACM/IEEE Conference on
Digital Libraries, 241-242, 2004.
[24] Zhang, J. and Dimitroff, A. Internet search engines' response
to metadata Dublin Core implementation. Journal of
Information Science, 30:310-320, 2004.
[25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using
named entities: focused named entity recognition using
machine learning. In Proceedings of the 27th Annual
International ACM SIGIR Conference on Research and
Development in Information Retrieval, 281-288, 2004.
[26] http://dublincore.org/groups/corporate/Seattle/
154
Performance Prediction Using Spatial Autocorrelation
Fernando Diaz
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
fdiaz@cs.umass.edu
ABSTRACT
Evaluation of information retrieval systems is one of the core
tasks in information retrieval. Problems include the 
inability to exhaustively label all documents for a topic, 
nongeneralizability from a small number of topics, and 
incorporating the variability of retrieval systems. Previous work
addresses the evaluation of systems, the ranking of queries
by difficulty, and the ranking of individual retrievals by 
performance. Approaches exist for the case of few and even no
relevance judgments. Our focus is on zero-judgment 
performance prediction of individual retrievals.
One common shortcoming of previous techniques is the 
assumption of uncorrelated document scores and judgments.
If documents are embedded in a high-dimensional space (as
they often are), we can apply techniques from spatial data
analysis to detect correlations between document scores.
We find that the low correlation between scores of 
topically close documents often implies a poor retrieval 
performance. When compared to a state of the art baseline,
we demonstrate that the spatial analysis of retrieval scores
provides significantly better prediction performance. These
new predictors can also be incorporated with classic 
predictors to improve performance further. We also describe
the first large-scale experiment to evaluate zero-judgment
performance prediction for a massive number of retrieval
systems over a variety of collections in several languages.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
models; H.3.4 [Systems and Software]: Performance 
evaluation (efficiency and effectiveness)
General Terms
Performance, Design, Reliability, Experimentation
1. INTRODUCTION
In information retrieval, a user poses a query to a system.
The system retrieves n documents each receiving a 
realvalued score indicating the predicted degree of relevance.
If we randomly select pairs of documents from this set, we
expect some pairs to share the same topic and other pairs to
not share the same topic. Take two topically-related 
documents from the set and call them a and b. If the scores of a
and b are very different, we may be concerned about the 
performance of our system. That is, if a and b are both on the
topic of the query, we would like them both to receive a high
score; if a and b are not on the topic of the query, we would
like them both to receive a low score. We might become more
worried as we find more differences between scores of related
documents. We would be more comfortable with a retrieval
where scores are consistent between related documents.
Our paper studies the quantification of this inconsistency
in a retrieval from a spatial perspective. Spatial analysis is
appropriate since many retrieval models embed documents
in some vector space. If documents are embedded in a space,
proximity correlates with topical relationships. Score 
consistency can be measured by the spatial version of 
autocorrelation known as the Moran coefficient or IM [5, 10]. In
this paper, we demonstrate a strong correlation between IM
and retrieval performance.
The discussion up to this point is reminiscent of the 
cluster hypothesis. The cluster hypothesis states: closely-related
documents tend to be relevant to the same request [12]. As we
shall see, a retrieval function"s spatial autocorrelation 
measures the degree to which closely-related documents receive
similar scores. Because of this, we interpret 
autocorrelation as measuring the degree to which a retrieval function
satisfies the clustering hypothesis. If this connection is 
reasonable, in Section 6, we present evidence that failure to
satisfy the cluster hypothesis correlates strongly with poor
performance.
In this work, we provide the following contributions,
1. A general, robust method for predicting the 
performance of retrievals with zero relevance judgments 
(Section 3).
2. A theoretical treatment of the similarities and 
motivations behind several state-of-the-art performance 
prediction techniques (Section 4).
3. The first large-scale experiments of zero-judgment, 
single run performance prediction (Sections 5 and 6).
2. PROBLEM DEFINITION
Given a query, an information retrieval system produces
a ranking of documents in the collection encoded as a set
of scores associated with documents. We refer to the set
of scores for a particular query-system combination as a 
retrieval. We would like to predict the performance of this
retrieval with respect to some evaluation measure (eg, mean
average precision). In this paper, we present results for
ranking retrievals from arbitrary systems. We would like
this ranking to approximate the ranking of retrievals by the
evaluation measure. This is different from ranking queries
by the average performance on each query. It is also 
different from ranking systems by the average performance on a
set of queries.
Scores are often only computed for the top n documents
from the collection. We place these scores in the length
n vector, y, where yi refers to the score of the ith-ranked
document. We adjust scores to have zero mean and unit
variance. We use this method because of its simplicity and
its success in previous work [15].
3. SPATIAL CORRELATION
In information retrieval, we often assume that the 
representations of documents exist in some high-dimensional 
vector space. For example, given a vocabulary, V, this vector
space may be an arbitrary |V|-dimensional space with cosine
inner-product or a multinomial simplex with a 
distributionbased distance measure. An embedding space is often 
selected to respect topical proximity; if two documents are
near, they are more likely to share a topic.
Because of the prevalence and success of spatial models
of information retrieval, we believe that the application of
spatial data analysis techniques are appropriate. Whereas
in information retrieval, we are concerned with the score at
a point in a space, in spatial data analysis, we are concerned
with the value of a function at a point or location in a space.
We use the term function here to mean a mapping from a
location to a real value. For example, we might be interested
in the prevalence of a disease in the neighborhood of some
city. The function would map the location of a neighborhood
to an infection rate.
If we want to quantify the spatial dependencies of a 
function, we would employ a measure referred to as the spatial
autocorrelation [5, 10]. High spatial autocorrelation suggests
that knowing the value of a function at location a will tell
us a great deal about the value at a neighboring location
b. There is a high spatial autocorrelation for a function
representing the temperature of a location since knowing
the temperature at a location a will tell us a lot about the
temperature at a neighboring location b. Low spatial 
autocorrelation suggests that knowing the value of a function
at location a tells us little about the value at a neighboring
location b. There is low spatial autocorrelation in a function
measuring the outcome of a coin toss at a and b.
In this section, we will begin by describing what we mean
by spatial proximity for documents and then define a 
measure of spatial autocorrelation. We conclude by extending
this model to include information from multiple retrievals
from multiple systems for a single query.
3.1 Spatial Representation of Documents
Our work does not focus on improving a specific similarity
measure or defining a novel vector space. Instead, we choose
an inner product known to be effective at detecting 
interdocument topical relationships. Specifically, we adopt tf.idf
document vectors,
˜di = di log
„
(n + 0.5) − ci
0.5 + ci
«
(1)
where d is a vector of term frequencies, c is the length-|V|
document frequency vector. We use this weighting scheme
due to its success for topical link detection in the context
of Topic Detection and Tracking (TDT) evaluations [6]. 
Assuming vectors are scaled by their L2 norm, we use the inner
product, ˜di, ˜dj , to define similarity.
Given documents and some similarity measure, we can
construct a matrix which encodes the similarity between
pairs of documents. Recall that we are given the top n
documents retrieved in y. We can compute an n × n 
similarity matrix, W. An element of this matrix, Wij represents
the similarity between documents ranked i and j. In 
practice, we only include the affinities for a document"s k-nearest
neighbors. In all of our experiments, we have fixed k to 5.
We leave exploration of parameter sensitivity to future work.
We also row normalize the matrix so that
Pn
j=1 Wij = 1 for
all i.
3.2 Spatial Autocorrelation of a Retrieval
Recall that we are interested in measuring the similarity
between the scores of spatially-close documents. One such
suitable measure is the Moran coefficient of spatial 
autocorrelation. Assuming the function y over n locations, this is
defined as
˜IM =
n
eTWe
P
i,j Wijyiyj
P
i y2
i
=
n
eTWe
yT
Wy
yTy
(2)
where eT
We =
P
ij Wij.
We would like to compare autocorrelation values for 
different retrievals. Unfortunately, the bound for Equation 2
is not consistent for different W and y. Therefore, we use
the Cauchy-Schwartz inequality to establish a bound,
˜IM ≤
n
eTWe
s
yTWTWy
yTy
And we define the normalized spatial autocorrelation as
IM =
yT
Wy
p
yTy × yTWTWy
Notice that if we let ˜y = Wy, then we can write this formula
as,
IM =
yT
˜y
y 2 ˜y 2
(3)
which can be interpreted as the correlation between the 
original retrieval scores and a set of retrieval scores diffused
in the space.
We present some examples of autocorrelations of functions
on a grid in Figure 1.
3.3 Correlation with Other Retrievals
Sometimes we are interested in the performance of a single
retrieval but have access to scores from multiple systems for
(a) IM = 0.006 (b) IM = 0.241 (c) IM = 0.487
Figure 1: The Moran coefficient, IM for a several
binary functions on a grid. The Moran coefficient
is a local measure of function consistency. From the
perspective of information retrieval, each of these
grid spaces would represent a document and 
documents would be organized so that they lay next to
topically-related documents. Binary retrieval scores
would define a pattern on this grid. Notice that,
as the Moran coefficient increases, neighboring cells
tend to have similar values.
the same query. In this situation, we can use combined
information from these scores to construct a surrogate for
a high-quality ranking [17]. We can treat the correlation
between the retrieval we are interested in and the combined
scores as a predictor of performance.
Assume that we are given m score functions, yi, for the
same n documents. We will represent the mean of these
vectors as yµ =
Pm
i=1 yi. We use the mean vector as an 
approximation to relevance. Since we use zero mean and unit
variance normalization, work in metasearch suggests that
this assumption is justified [15]. Because yµ represents a
very good retrieval, we hypothesize that a strong similarity
between yµ and y will correlate positively with system 
performance. We use Pearson"s product-moment correlation to
measure the similarity between these vectors,
ρ(y, yµ) =
yT
yµ
y 2 yµ 2
(4)
We will comment on the similarity between Equation 3 and
4 in Section 7.
Of course, we can combine ρ(y, ˜y) and ρ(y, yµ) if we 
assume that they capture different factors in the prediction.
One way to accomplish this is to combine these predictors
as independent variables in a linear regression. An 
alternative means of combination is suggested by the mathematical
form of our predictors. Since ˜y encodes the spatial 
dependencies in y and yµ encodes the spatial properties of the
multiple runs, we can compute a third correlation between
these two vectors,
ρ(˜y, yµ) =
˜yT
yµ
˜y 2 yµ 2
(5)
We can interpret Equation 5 as measuring the correlation
between a high quality ranking (yµ) and a spatially smoothed
version of the retrieval (˜y).
4. RELATIONSHIP WITH OTHER
PREDICTORS
One way to predict the effectiveness of a retrieval is to
look at the shared vocabulary of the top n retrieved 
documents. If we computed the most frequent content words
in this set, we would hope that they would be consistent
with our topic. In fact, we might believe that a bad 
retrieval would include documents on many disparate topics,
resulting in an overlap of terminological noise. The Clarity
of a query attempts to quantify exactly this [7]. Specifically,
Clarity measures the similarity of the words most frequently
used in retrieved documents to those most frequently used
in the whole corpus. The conjecture is that a good retrieval
will use language distinct from general text; the overlapping
language in a bad retrieval will tend to be more similar to
general text. Mathematically, we can compute a 
representation of the language used in the initial retrieval as a weighted
combination of document language models,
P(w|θQ) =
nX
i=1
P(w|θi)
P(Q|θi)
Z
(6)
where θi is the language model of the ith-ranked 
document, P(Q|θi) is the query likelihood score of the ith-ranked
document and Z =
Pn
i=1 P(Q|θi) is a normalization 
constant. The similarity between the multinomial P(w|θQ)
and a model of general text can be computed using the
Kullback-Leibler divergence, DV
KL(θQ θC ). Here, the 
distribution P(w|θC ) is our model of general text which can be
computed using term frequencies in the corpus. In Figure
2a, we present Clarity as measuring the distance between the
weighted center of mass of the retrieval (labeled y) and the
unweighted center of mass of the collection (labeled O).
Clarity reaches a minimum when a retrieval assigns every
document the same score.
Let"s again assume we have a set of n documents retrieved
for our query. Another way to quantify the dispersion of a
set of documents is to look at how clustered they are. We
may hypothesize that a good retrieval will return a single,
tight cluster. A poorly performing retrieval will return a
loosely related set of documents covering many topics. One
proposed method of quantifying this dispersion is to 
measure the distance from a random document a to it"s nearest
neighbor, b. A retrieval which is tightly clustered will, on
average, have a low distance between a and b; a retrieval
which is less tightly-closed will, on average have high 
distances between a and b. This average corresponds to using
the Cox-Lewis statistic to measure the randomness of the
top n documents retrieved from a system [18]. In Figure
2a, this is roughly equivalent to measuring the area of the
set n. Notice that we are throwing away information about
the retrieval function y. Therefore the Cox-Lewis statistic
is highly dependent on selecting the top n documents.1
Remember that we have n documents and a set of scores.
Let"s assume that we have access to the system which 
provided the original scores and that we can also request scores
for new documents. This suggests a third method for 
predicting performance. Take some document, a, from the 
retrieved set and arbitrarily add or remove words at random
to create a new document ˜a. Now, we can ask our system
to score ˜a with respect to our query. If, on average over
the n documents, the scores of a and ˜a tend to be very 
different, we might suspect that the system is failing on this
query. So, an alternative approach is to measure the 
simi1
The authors have suggested coupling the query with the
distance measure [18]. The information introduced by the
query, though, is retrieval-independent so that, if two 
retrievals return the same set of documents, the approximate
Cox-Lewis statistic will be the same regardless of the 
retrieval scores.
yOy
(a) Global Divergence
µ(y)˜y
y
(b) Score Perturbation
µ(y)
y
(c) Multirun Averaging
Figure 2: Representation of several performance predictors on a grid. In Figure 2a, we depict predictors
which measure the divergence between the center of mass of a retrieval and the center of the embedding
space. In Figure 2b, we depict predictors which compare the original retrieval, y, to a perturbed version of
the retrieval, ˜y. Our approach uses a particular type of perturbation based on score diffusion. Finally, in
Figure 2c, we depict prediction when given retrievals from several other systems on the same query. Here,
we can consider the fusion of these retrieval as a surrogate for relevance.
larity between the retrieval and a perturbed version of that
retrieval [18, 19]. This can be accomplished by either 
perturbing the documents or queries. The similarity between
the two retrievals can be measured using some correlation
measure. This is depicted in Figure 2b. The upper grid
represents the original retrieval, y, while the lower grid 
represents the function after having been perturbed, ˜y. The
nature of the perturbation process requires additional 
scorings or retrievals. Our predictor does not require access to
the original scoring function or additional retrievals. So, 
although our method is similar to other perturbation methods
in spirit, it can be applied in situations when the retrieval
system is inaccessible or costly to access.
Finally, assume that we have, in addition to the retrieval
we want to evaluate, m retrievals from a variety of 
different systems. In this case, we might take a document a,
compare its rank in the retrieval to its average rank in the
m retrievals. If we believe that the m retrievals provide a
satisfactory approximation to relevance, then a very large
difference in rank would suggest that our retrieval is 
misranking a. If this difference is large on average over all
n documents, then we might predict that the retrieval is
bad. If, on the other hand, the retrieval is very consistent
with the m retrievals, then we might predict that the 
retrieval is good. The similarity between the retrieval and
the combined retrieval may be computed using some 
correlation measure. This is depicted in Figure 2c. In previous
work, the Kullback-Leibler divergence between the 
normalized scores of the retrieval and the normalized scores of the
combined retrieval provides the similarity [1].
5. EXPERIMENTS
Our experiments focus on testing the predictive power of
each of our predictors: ρ(y, ˜y), ρ(y, yµ), and ρ(˜y, yµ). As
stated in Section 2, we are interested in predicting the 
performance of the retrieval generated by an arbitrary system.
Our methodology is consistent with previous research in that
we predict the relative performance of a retrieval by 
comparing a ranking based on our predictor to a ranking based on
average precision.
We present results for two sets of experiments. The first
set of experiments presents detailed comparisons of our 
predictors to previously-proposed predictors using identical data
sets. Our second set of experiments demonstrates the 
generalizability of our approach to arbitrary retrieval methods,
corpus types, and corpus languages.
5.1 Detailed Experiments
In these experiments, we will predict the performance of
language modeling scores using our autocorrelation 
predictor, ρ(y, ˜y); we do not consider ρ(y, yµ) or ρ(˜y, yµ) 
because, in these detailed experiments, we focus on ranking
the retrievals from a single system. We use retrievals, values
for baseline predictors, and evaluation measures reported in
previous work [19].
5.1.1 Topics and Collections
These performance prediction experiments use language
model retrievals performed for queries associated with 
collections in the TREC corpora. Using TREC collections 
allows us to confidently associate an average precision with a
retrieval. In these experiments, we use the following topic
collections: TREC 4 ad-hoc, TREC 5 ad-hoc, Robust 2004,
Terabyte 2004, and Terabyte 2005.
5.1.2 Baselines
We provide two baselines. Our first baseline is the 
classic Clarity predictor presented in Equation 6. Clarity is
designed to be used with language modeling systems. Our
second baseline is Zhou and Croft"s ranking robustness
predictor. This predictor corrupts the top k documents
from retrieval and re-computes the language model scores
for these corrupted documents. The value of the predictor
is the Spearman rank correlation between the original 
ranking and the corrupted ranking. In our tables, we will label
results for Clarity using DV
KL and the ranking robustness
predictor using P.
5.2 Generalizability Experiments
Our predictors do not require a particular baseline 
retrieval system; the predictors can be computed for an 
arbitrary retrieval, regardless of how scores were generated. We
believe that that is one of the most attractive aspects of our
algorithm. Therefore, in a second set of experiments, we
demonstrate the ability of our techniques to generalize to a
variety of collections, topics, and retrieval systems.
5.2.1 Topics and Collections
We gathered a diverse set of collections from all possible
TREC corpora. We cast a wide net in order to locate 
collections where our predictors might fail. Our hypothesis is that
documents with high topical similarity should have 
correlated scores. Therefore, we avoided collections where scores
were unlikely to be correlated (eg, question-answering) or
were likely to be negatively correlated (eg, novelty). 
Nevertheless, our collections include corpora where correlations
are weakly justified (eg, non-English corpora) or not 
justified at all (eg, expert search). We use the ad-hoc tracks from
TREC3-8, TREC Robust 2003-2005, TREC Terabyte 
20042005, TREC4-5 Spanish, TREC5-6 Chinese, and TREC 
Enterprise Expert Search 2005. In all cases, we use only the
automatic runs for ad-hoc tracks submitted to NIST.
For all English and Spanish corpora, we construct the 
matrix W according to the process described in Section 3.1. For
Chinese corpora, we use na¨ıve character-based tf.idf vectors.
For entities, entries in W are proportional to the number of
documents in which two entities cooccur.
5.2.2 Baselines
In our detailed experiments, we used the Clarity measure
as a baseline. Since we are predicting the performance of
retrievals which are not based on language modeling, we
use a version of Clarity referred to as ranked-list Clarity
[7]. Ranked-list clarity converts document ranks to P(Q|θi)
values. This conversion begins by replacing all of the scores
in y with the respective ranks. Our estimation of P(Q|θi)
from the ranks, then is,
P(Q|θi) =
(
2(c+1−yi)
c(c+1)
if yi ≤ c
0 otherwise
(7)
where c is a cutoff parameter. As suggested by the authors,
we fix the algorithm parameters c and λ2 so that c = 60
and λ2 = 0.10. We use Equation 6 to estimate P(w|θQ) and
DV
KL(θQ θC ) to compute the value of the predictor. We
will refer to this predictor as DV
KL, superscripted by V to
indicate that the Kullback-Leibler divergence is with respect
to the term embedding space.
When information from multiple runs on the same query is
available, we use Aslam and Pavlu"s document-space 
multinomial divergence as a baseline [1]. This rank-based method
first normalizes the scores in a retrieval as an n-dimensional
multinomial. As with ranked-list Clarity, we begin by 
replacing all of the scores in y with their respective ranks.
Then, we adjust the elements of y in the following way,
ˆyi =
1
2n
0
@1 +
nX
k=yi
1
k
1
A (8)
In our multirun experiments, we only use the top 75 
documents from each retrieval (n = 75); this is within the range
of parameter values suggested by the authors. However, we
admit not tuning this parameter for either our system or the
baseline. The predictor is the divergence between the 
candidate distribution, y, and the mean distribution, yµ . With
the uniform linear combination of these m retrievals 
represented as yµ, we can compute the divergence as Dn
KL(ˆy ˆyµ)
where we use the superscript n to indicate that the 
summation is over the set of n documents. This baseline was
developed in the context of predicting query difficulty but
we adopt it as a reasonable baseline for predicting retrieval
performance.
5.2.3 Parameter Settings
When given multiple retrievals, we use documents in the
union of the top k = 75 documents from each of the m 
retrievals for that query. If the size of this union is ˜n, then
yµ and each yi is of length ˜n. In some cases, a system
did not score a document in the union. Since we are 
making a Gaussian assumption about our scores, we can sample
scores for these unseen documents from the negative tail
of the distribution. Specifically, we sample from the part
of the distribution lower than the minimum value of in the
normalized retrieval. This introduces randomness into our
algorithm but we believe it is more appropriate than 
assigning an arbitrary fixed value.
We optimized the linear regression using the square root
of each predictor. We found that this substantially improved
fits for all predictors, including the baselines. We considered
linear combinations of pairs of predictors (labeled by the
components) and all predictors (labeled as β).
5.3 Evaluation
Given a set of retrievals, potentially from a combination
of queries and systems, we measure the correlation of the
rank ordering of this set by the predictor and by the 
performance metric. In order to ensure comparability with 
previous results, we present Kendall"s τ correlation between the
predictor"s ranking and ranking based on average precision
of the retrieval. Unless explicitly noted, all correlations are
significant with p < 0.05.
Predictors can sometimes perform better when linearly
combined [9, 11]. Although previous work has presented
the coefficient of determination (R2
) to measure the quality
of the regression, this measure cannot be reliably used when
comparing slight improvements from combining predictors.
Therefore, we adopt the adjusted coefficient of 
determination which penalizes models with more variables. The 
adjusted R2
allows us to evaluate the improvement in 
prediction achieved by adding a parameter but loses the statistical
interpretation of R2
. We will use Kendall"s τ to evaluate the
magnitude of the correlation and the adjusted R2
to 
evaluate the combination of variables.
6. RESULTS
We present results for our detailed experiments comparing
the prediction of language model scores in Table 1. Although
the Clarity measure is theoretically designed for language
model scores, it consistently underperforms our system-agnostic
predictor. Ranking robustness was presented as an 
improvement to Clarity for web collections (represented in our 
experiments by the terabyte04 and terabyte05 collections), 
shifting the τ correlation from 0.139 to 0.150 for terabyte04 and
0.171 to 0.208 for terabyte05. However, these improvements
are slight compared to the performance of autocorrelation
on these collections. Our predictor achieves a τ correlation
of 0.454 for terabyte04 and 0.383 for terabyte05. Though
not always the strongest, autocorrelation achieves 
correlations competitive with baseline predictors. When 
examining the performance of linear combinations of predictors, we
note that in every case, autocorrelation factors as a 
necessary component of a strong predictor. We also note that the
adjusted R2
for individual baselines are always significantly
improved by incorporating autocorrelation.
We present our generalizability results in Table 2. We
begin by examining the situation in column (a) where we
are presented with a single retrieval and no information
from additional retrievals. For every collection except one,
we achieve significantly better correlations than ranked-list
Clarity. Surprisingly, we achieve relatively strong 
correlations for Spanish and Chinese collections despite our na¨ıve
processing. We do not have a ranked-list clarity correlation
for ent05 because entity modeling is itself an open research
question. However, our autocorrelation measure does not
achieve high correlations perhaps because relevance for 
entity retrieval does not propagate according to the 
cooccurrence links we use.
As noted above, the poor Clarity performance on web
data is consistent with our findings in the detailed 
experiments. Clarity also notably underperforms for several news
corpora (trec5, trec7, and robust04). On the other hand, 
autocorrelation seems robust to the changes between different
corpora.
Next, we turn to the introduction of information from
multiple retrievals. We compare the correlations between
those predictors which do not use this information in column
(a) and those which do in column (b). For every collection,
the predictors in column (b) outperform the predictors in
column (a), indicating that the information from additional
runs can be critical to making good predictions.
Inspecting the predictors in column (b), we only draw
weak conclusions. Our new predictors tend to perform 
better on news corpora. And between our new predictors, the
hybrid ρ(˜y, yµ) predictor tends to perform better. Recall
that our ρ(˜y, yµ) measure incorporates both spatial and
multiple retrieval information. Therefore, we believe that
the improvement in correlation is the result of 
incorporating information from spatial behavior.
In column (c), we can investigate the utility of 
incorporating spatial information with information from 
multiple retrievals. Notice that in the cases where 
autocorrelation, ρ(y, ˜y), alone performs well (trec3, trec5-spanish, and
trec6-chinese), it is substantially improved by 
incorporating multiple-retrieval information from ρ(y, yµ) in the 
linear regression, β. In the cases where ρ(y, yµ) performs well,
incorporating autocorrelation rarely results in a significant
improvement in performance. In fact, in every case where
our predictor outperforms the baseline, it includes 
information from multiple runs.
7. DISCUSSION
The most important result from our experiments involves
prediction when no information is available from multiple
runs (Tables 1 and 2a). This situation arises often in system
design. For example, a system may need to, at retrieval
time, assess its performance before deciding to conduct more
intensive processing such as pseudo-relevance feedback or
interaction. Assuming the presence of multiple retrievals is
unrealistic in this case.
We believe that autocorrelation is, like multiple-retrieval
algorithms, approximating a good ranking; in this case by
diffusing scores. Why is ˜y a reasonable surrogate? We know
that diffusion of scores on the web graph and language model
graphs improves performance [14, 16]. Therefore, if score
diffusion tends to, in general, improve performance, then
diffused scores will, in general, provide a good surrogate for
relevance. Our results demonstrate that this approximation
is not as powerful as information from multiple retrievals.
Nevertheless, in situations where this information is lacking,
autocorrelation provides substantial information.
The success of autocorrelation as a predictor may also
have roots in the clustering hypothesis. Recall that we
regard autocorrelation as the degree to which a retrieval
satisfies the clustering hypothesis. Our experiments, then,
demonstrate that a failure to respect the clustering 
hypothesis correlates with poor performance. Why might systems
fail to conform to the cluster hypothesis? Query-based 
information retrieval systems often score documents 
independently. The score of document a may be computed by 
examining query term or phrase matches, the document length,
and perhaps global collection statistics. Once computed,
a system rarely compares the score of a to the score of a
topically-related document b. With some exceptions, the
correlation of document scores has largely been ignored.
We should make it clear that we have selected tasks where
topical autocorrelation is appropriate. There are certainly
cases where there is no reason to believe that retrieval scores
will have topical autocorrelation. For example, ranked lists
which incorporate document novelty should not exhibit 
spatial autocorrelation; if anything autocorrelation should be
negative for this task. Similarly, answer candidates in a
question-answering task may or may not exhibit 
autocorrelation; in this case, the semantics of links is questionable
too. It is important before applying this measure to confirm
that, given the semantics for some link between two retrieved
items, we should expect a correlation between scores.
8. RELATED WORK
In this section we draw more general comparisons to other
work in performance prediction and spatial data analysis.
There is a growing body of work which attempts to predict
the performance of individual retrievals [7, 3, 11, 9, 19]. We
have attempted to place our work in the context of much of
this work in Section 4. However, a complete comparison is
beyond the scope of this paper. We note, though, that our
experiments cover a larger and more diverse set of retrievals,
collections, and topics than previously examined.
Much previous work-particularly in the context of 
TRECfocuses on predicting the performance of systems. Here,
each system generates k retrievals. The task is, given these
retrievals, to predict the ranking of systems according to
some performance measure. Several papers attempt to 
address this task under the constraint of few judgments [2, 4].
Some work even attempts to use zero judgments by 
leveraging multiple retrievals for the same query [17]. Our task
differs because we focus on ranking retrievals independent
of the generating system. The task here is not to test the
hypothesis system A is superior to system B but to test
the hypothesis retrieval A is superior to retrieval B.
Autocorrelation manifests itself in many classification tasks.
Neville and Jensen define relational autocorrelation for 
relational learning problems and demonstrate that many 
classification tasks manifest autocorrelation [13]. Temporal 
autocorrelation of initial retrievals has also been used to predict
performance [9]. However, temporal autocorrelation is 
performed by projecting the retrieval function into the temporal
embedding space. In our work, we focus on the behavior of
the function over the relationships between documents.
τ adjusted R2
DV
KL P ρ(y, ˜y) DV
KL P ρ(y, ˜y) DV
KL, P DV
KL, ρ(y, ˜y) Pρ(y, ˜y) β
trec4 0.353 0.548 0.513 0.168 0.363 0.422 0.466 0.420 0.557 0.553
trec5 0.311 0.329 0.357 0.116 0.190 0.236 0.238 0.244 0.266 0.269
robust04 0.418 0.398 0.373 0.256 0.304 0.278 0.403 0.373 0.402 0.442
terabyte04 0.139 0.150 0.454 0.059 0.045 0.292 0.076 0.293 0.289 0.284
terabyte05 0.171 0.208 0.383 0.022 0.072 0.193 0.120 0.225 0.218 0.257
Table 1: Comparison to Robustness and Clarity measures for language model scores. Evaluation replicates
experiments from [19]. We present correlations between the classic Clarity measure (DV
KL), the ranking
robustness measure (P), and autocorrelation (ρ(y, ˜y)) each with mean average precision in terms of Kendall"s
τ. The adjusted coefficient of determination is presented to measure the effectiveness of combining predictors.
Measures in bold represent the strongest correlation for that test/collection pair.
multiple run
(a) (b) (c)
τ τ adjusted R2
DKL ρ(y, ˜y) Dn
KL ρ(y, yµ) ρ(˜y, yµ) Dn
KL ρ(y, ˜y) ρ(y, yµ) ρ(˜y, yµ) β
trec3 0.201 0.461 0.461 0.439 0.456 0.444 0.395 0.394 0.386 0.498
trec4 0.252 0.396 0.455 0.482 0.489 0.379 0.263 0.429 0.482 0.483
trec5 0.016 0.277 0.433 0.459 0.393 0.280 0.157 0.375 0.323 0.386
trec6 0.230 0.227 0.352 0.428 0.418 0.203 0.089 0.323 0.325 0.325
trec7 0.083 0.326 0.341 0.430 0.483 0.264 0.182 0.363 0.442 0.400
trec8 0.235 0.396 0.454 0.508 0.567 0.402 0.272 0.490 0.580 0.523
robust03 0.302 0.354 0.377 0.385 0.447 0.269 0.206 0.274 0.392 0.303
robust04 0.183 0.308 0.301 0.384 0.453 0.200 0.182 0.301 0.393 0.335
robust05 0.224 0.249 0.371 0.377 0.404 0.341 0.108 0.313 0.328 0.336
terabyte04 0.043 0.245 0.544 0.420 0.392 0.516 0.105 0.357 0.343 0.365
terabyte05 0.068 0.306 0.480 0.434 0.390 0.491 0.168 0.384 0.309 0.403
trec4-spanish 0.307 0.388 0.488 0.398 0.395 0.423 0.299 0.282 0.299 0.388
trec5-spanish 0.220 0.458 0.446 0.484 0.475 0.411 0.398 0.428 0.437 0.529
trec5-chinese 0.092 0.199 0.367 0.379 0.384 0.379 0.199 0.273 0.276 0.310
trec6-chinese 0.144 0.276 0.265 0.353 0.376 0.115 0.128 0.188 0.223 0.199
ent05 - 0.181 0.324 0.305 0.282 0.211 0.043 0.158 0.155 0.179
Table 2: Large scale prediction experiments. We predict the ranking of large sets of retrievals for various
collections and retrieval systems. Kendall"s τ correlations are computed between the predicted ranking and
a ranking based on the retrieval"s average precision. In column (a), we have predictors which do not use
information from other retrievals for the same query. In columns (b) and (c) we present performance for
predictors which incorporate information from multiple retrievals. The adjusted coefficient of determination
is computed to determine effectiveness of combining predictors. Measures in bold represent the strongest
correlation for that test/collection pair.
Finally, regularization-based re-ranking processes are also
closely-related to our work [8]. These techniques seek to
maximize the agreement between scores of related 
documents by solving a constrained optimization problem. The
maximization of consistency is equivalent to maximizing the
Moran autocorrelation. Therefore, we believe that our work
provides explanation for why regularization-based re-ranking
works.
9. CONCLUSION
We have presented a new method for predicting the 
performance of a retrieval ranking without any relevance 
judgments. We consider two cases. First, when making 
predictions in the absence of retrievals from other systems, our
predictors demonstrate robust, strong correlations with 
average precision. This performance, combined with a simple
implementation, makes our predictors, in particular, very 
attractive. We have demonstrated this improvement for many,
diverse settings. To our knowledge, this is the first large
scale examination of zero-judgment, single-retrieval 
performance prediction. Second, when provided retrievals from
other systems, our extended methods demonstrate 
competitive performance with state of the art baselines. Our 
experiments also demonstrate the limits of the usefulness of our
predictors when information from multiple runs is provided.
Our results suggest two conclusions. First, our results
could affect retrieval algorithm design. Retrieval algorithms
designed to consider spatial autocorrelation will conform to
the cluster hypothesis and improve performance. Second,
our results could affect the design of minimal test collection
algorithms. Much of the recent work in ranking systems
sometimes ignores correlations between document labels and
scores. We believe that these two directions could be 
rewarding given the theoretical and experimental evidence in this
paper.
10. ACKNOWLEDGMENTS
This work was supported in part by the Center for 
Intelligent Information Retrieval and in part by the Defense
Advanced Research Projects Agency (DARPA) under 
contract number HR0011-06-C-0023. Any opinions, findings
and conclusions or recommendations expressed in this 
material are the author"s and do not necessarily reflect those
of the sponsor. We thank Yun Zhou and Desislava Petkova
for providing data and Andre Gauthier for technical 
assistance.
11. REFERENCES
[1] J. Aslam and V. Pavlu. Query hardness estimation using
jensen-shannon divergence among multiple scoring
functions. In ECIR 2007: Proceedings of the 29th European
Conference on Information Retrieval, 2007.
[2] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method
for system evaluation using incomplete judgments. In
S. Dumais, E. N. Efthimiadis, D. Hawking, and K. Jarvelin,
editors, Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 541-548. ACM Press, August
2006.
[3] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What
makes a query difficult? In SIGIR "06: Proceedings of the
29th annual international ACM SIGIR conference on
Research and development in information retrieval, pages
390-397, New York, NY, USA, 2006. ACM Press.
[4] B. Carterette, J. Allan, and R. Sitaraman. Minimal test
collections for retrieval evaluation. In SIGIR "06:
Proceedings of the 29th annual international ACM SIGIR
conference on Research and development in information
retrieval, pages 268-275, New York, NY, USA, 2006. ACM
Press.
[5] A. D. Cliff and J. K. Ord. Spatial Autocorrelation. Pion
Ltd., 1973.
[6] M. Connell, A. Feng, G. Kumaran, H. Raghavan, C. Shah,
and J. Allan. Umass at tdt 2004. Technical Report CIIR
Technical Report IR - 357, Department of Computer
Science, University of Massachusetts, 2004.
[7] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Precision
prediction based on ranked list coherence. Inf. Retr.,
9(6):723-755, 2006.
[8] F. Diaz. Regularizing ad-hoc retrieval scores. In CIKM "05:
Proceedings of the 14th ACM international conference on
Information and knowledge management, pages 672-679,
New York, NY, USA, 2005. ACM Press.
[9] F. Diaz and R. Jones. Using temporal profiles of queries for
precision prediction. In SIGIR "04: Proceedings of the 27th
annual international ACM SIGIR conference on Research
and development in information retrieval, pages 18-24,
New York, NY, USA, 2004. ACM Press.
[10] D. A. Griffith. Spatial Autocorrelation and Spatial
Filtering. Springer Verlag, 2003.
[11] B. He and I. Ounis. Inferring Query Performance Using
Pre-retrieval Predictors. In The Eleventh Symposium on
String Processing and Information Retrieval (SPIRE),
2004.
[12] N. Jardine and C. J. V. Rijsbergen. The use of hierarchic
clustering in information retrieval. Information Storage and
Retrieval, 7:217-240, 1971.
[13] D. Jensen and J. Neville. Linkage and autocorrelation cause
feature selection bias in relational learning. In ICML "02:
Proceedings of the Nineteenth International Conference on
Machine Learning, pages 259-266, San Francisco, CA,
USA, 2002. Morgan Kaufmann Publishers Inc.
[14] O. Kurland and L. Lee. Corpus structure, language models,
and ad-hoc information retrieval. In SIGIR "04:
Proceedings of the 27th annual international conference on
Research and development in information retrieval, pages
194-201, New York, NY, USA, 2004. ACM Press.
[15] M. Montague and J. A. Aslam. Relevance score
normalization for metasearch. In CIKM "01: Proceedings of
the tenth international conference on Information and
knowledge management, pages 427-433, New York, NY,
USA, 2001. ACM Press.
[16] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma. A
study of relevance propagation for web search. In SIGIR
"05: Proceedings of the 28th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 408-415, New York, NY, USA,
2005. ACM Press.
[17] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval
systems without relevance judgments. In SIGIR "01:
Proceedings of the 24th annual international ACM SIGIR
conference on Research and development in information
retrieval, pages 66-73, New York, NY, USA, 2001. ACM
Press.
[18] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. Wood. On
ranking the effectiveness of searches. In SIGIR "06:
Proceedings of the 29th annual international ACM SIGIR
conference on Research and development in information
retrieval, pages 398-404, New York, NY, USA, 2006. ACM
Press.
[19] Y. Zhou and W. B. Croft. Ranking robustness: a novel
framework to predict query performance. In CIKM "06:
Proceedings of the 15th ACM international conference on
Information and knowledge management, pages 567-574,
New York, NY, USA, 2006. ACM Press.
An Outranking Approach for Rank Aggregation in
Information Retrieval
Mohamed Farah
Lamsade, Paris Dauphine University
Place du Mal de Lattre de Tassigny
75775 Paris Cedex 16, France
farah@lamsade.dauphine.fr
Daniel Vanderpooten
Lamsade, Paris Dauphine University
Place du Mal de Lattre de Tassigny
75775 Paris Cedex 16, France
vdp@lamsade.dauphine.fr
ABSTRACT
Research in Information Retrieval usually shows performance
improvement when many sources of evidence are combined
to produce a ranking of documents (e.g., texts, pictures,
sounds, etc.). In this paper, we focus on the rank aggregation
problem, also called data fusion problem, where rankings of
documents, searched into the same collection and provided
by multiple methods, are combined in order to produce a
new ranking. In this context, we propose a rank aggregation
method within a multiple criteria framework using 
aggregation mechanisms based on decision rules identifying positive
and negative reasons for judging whether a document should
get a better rank than another. We show that the proposed
method deals well with the Information Retrieval distinctive
features. Experimental results are reported showing that
the suggested method performs better than the well-known
CombSUM and CombMNZ operators.
Categories and Subject Descriptors: H.3.3 
[Information Systems]: Information Search and Retrieval - 
Retrieval models.
General Terms: Algorithms, Measurement, 
Experimentation, Performance, Theory.
1. INTRODUCTION
A wide range of current Information Retrieval (IR) 
approaches are based on various search models (Boolean, 
Vector Space, Probabilistic, Language, etc. [2]) in order to 
retrieve relevant documents in response to a user request. The
result lists produced by these approaches depend on the 
exact definition of the relevance concept.
Rank aggregation approaches, also called data fusion 
approaches, consist in combining these result lists in order
to produce a new and hopefully better ranking. Such 
approaches give rise to metasearch engines in the Web context.
We consider, in the following, cases where only ranks are
available and no other additional information is provided
such as the relevance scores. This corresponds indeed to the
reality, where only ordinal information is available.
Data fusion is also relevant in other contexts, such as when
the user writes several queries of his/her information need
(e.g., a boolean query and a natural language query) [4], or
when many document surrogates are available [16].
Several studies argued that rank aggregation has the 
potential of combining effectively all the various sources of 
evidence considered in various input methods. For instance,
experiments carried out in [16], [30], [4] and [19] showed that
documents which appear in the lists of the majority of the
input methods are more likely to be relevant. Moreover, Lee
[19] and Vogt and Cottrell [31] found that various retrieval
approaches often return very different irrelevant documents,
but many of the same relevant documents. Bartell et al.
[3] also found that rank aggregation methods improve the
performances w.r.t. those of the input methods, even when
some of them have weak individual performances. These
methods also tend to smooth out biases of the input 
methods according to Montague and Aslam [22]. Data fusion has
recently been proved to improve performances for both the
ad-hoc retrieval and categorization tasks within the TREC
genomics track in 2005 [1].
The rank aggregation problem was addressed in various
fields such as i) in social choice theory which studies 
voting algorithms which specify winners of elections or winners
of competitions in tournaments [29], ii) in statistics when
studying correlation between rankings, iii) in distributed
databases when results from different databases must be
combined [12], and iv) in collaborative filtering [23].
Most current rank aggregation methods consider each 
input ranking as a permutation over the same set of items.
They also give rigid interpretation to the exact ranking of
the items. Both of these assumptions are rather not valid in
the IR context, as will be shown in the following sections.
The remaining of the paper is organized as follows. We
first review current rank aggregation methods in Section 2.
Then we outline the specificities of the data fusion problem
in the IR context (Section 3). In Section 4, we present a
new aggregation method which is proven to best fit the IR
context. Experimental results are presented in Section 5 and
conclusions are provided in a final section.
2. RELATED WORK
As pointed out by Riker [25], we can distinguish two 
families of rank aggregation methods: positional methods which
assign scores to items to be ranked according to the ranks
they receive and majoritarian methods which are based on
pairwise comparisons of items to be ranked. These two 
families of methods find their roots in the pioneering works of
Borda [5] and Condorcet [7], respectively, in the social choice
literature.
2.1 Preliminaries
We first introduce some basic notations to present the
rank aggregation methods in a uniform way. Let D =
{d1, d2, . . . , dnd } be a set of nd documents. A list or a 
ranking j is an ordering defined on Dj ⊆ D (j = 1, . . . , n).
Thus, di j di means di ‘is ranked better than" di in j.
When Dj = D, j is said to be a full list. Otherwise, it
is a partial list. If di belongs to Dj, rj
i denotes the rank
or position of di in j. We assume that the best answer
(document) is assigned the position 1 and the worst one is
assigned the position |Dj|. Let D be the set of all 
permutations on D or all subsets of D. A profile is a n-tuple
of rankings PR = ( 1, 2, . . . , n). Restricting PR to the
rankings containing document di defines PRi. We also call
the number of rankings which contain document di the rank
hits of di [19].
The rank aggregation or data fusion problem consists of
finding a ranking function or mechanism Ψ (also called a 
social welfare function in the social choice theory terminology)
defined by:
Ψ :
n
D → D
PR = ( 1, 2, . . . , n) → σ = Ψ(PR)
where σ is called a consensus ranking.
2.2 Positional Methods
2.2.1 Borda Count
This method [5] first assigns a score n
j=1 rj
i to each 
document di. Documents are then ranked by increasing order
of this score, breaking ties, if any, arbitrarily.
2.2.2 Linear Combination Methods
This family of methods basically combine scores of 
documents. When used for the rank aggregation problem, ranks
are assumed to be scores or performances to be combined
using aggregation operators such as the weighted sum or
some variation of it [3, 31, 17, 28].
For instance, Callan et al. [6] used the inference 
networks model [30] to combine rankings. Fox and Shaw [15]
proposed several combination strategies which are 
CombSUM, CombMIN, CombMAX, CombANZ and CombMNZ.
The first three operators correspond to the sum, min and
max operators, respectively. CombANZ and CombMNZ 
respectively divides and multiplies the CombSUM score by
the rank hits. It is shown in [19] that the CombSUM and
CombMNZ operators perform better than the others. 
Metasearch engines such as SavvySearch and MetaCrawler use
the CombSUM strategy to fuse rankings.
2.2.3 Footrule Optimal Aggregation
In this method, a consensus ranking minimizes the 
Spearman footrule distance from the input rankings [21]. 
Formally, given two full lists j and j , this distance is given
by F( j, j ) = nd
i=1 |rj
i − rj
i |. It extends to several lists
as follows. Given a profile PR and a consensus ranking
σ, the Spearman footrule distance of σ to PR is given by
F(σ, PR) = n
j=1 F(σ, j).
Cook and Kress [8] proposed a similar method which 
consists in optimizing the distance D( j, j ) = 1
2
nd
i,i =1 |rj
i,i −
rj
i,i |, where rj
i,i = rj
i −rj
i . This formulation has the 
advantage that it considers the intensity of preferences.
2.2.4 Probabilistic Methods
This kind of methods assume that the performance of the
input methods on a number of training queries is indicative
of their future performance. During the training process,
probabilities of relevance are calculated. For subsequent
queries, documents are ranked based on these probabilities.
For instance, in [20], each input ranking j is divided into a
number of segments, and the conditional probability of 
relevance (R) of each document di depending on the segment
k it occurs in, is computed, i.e. prob(R|di, k, j). For 
subsequent queries, the score of each document di is given by
n
j=1
prob(R|di,k, j )
k
. Le Calve and Savoy [18] suggest using
a logistic regression approach for combining scores. Training
data is needed to infer the model parameters.
2.3 Majoritarian Methods
2.3.1 Condorcet Procedure
The original Condorcet rule [7] specifies that a winner of
the election is any item that beats or ties with every other
item in a pairwise contest. Formally, let C(diσdi ) = { j∈
PR : di j di } be the coalition of rankings that are 
concordant with establishing diσdi , i.e. with the proposition
di ‘should be ranked better than" di in the final ranking σ.
di beats or ties with di iff |C(diσdi )| ≥ |C(di σdi)|.
The repetitive application of the Condorcet algorithm can
produce a ranking of items in a natural way: select the 
Condorcet winner, remove it from the lists, and repeat the 
previous two steps until there are no more documents to rank.
Since there is not always Condorcet winners, variations of
the Condorcet procedure have been developed within the
multiple criteria decision aid theory, with methods such as
ELECTRE [26].
2.3.2 Kemeny Optimal Aggregation
As in section 2.2.3, a consensus ranking minimizes a 
geometric distance from the input rankings, where the Kendall
tau distance is used instead of the Spearman footrule 
distance. Formally, given two full lists j and j , the Kendall
tau distance is given by K( j, j ) = |{(di, di ) : i < i , rj
i <
rj
i , rj
i > rj
i }|, i.e. the number of pairwise disagreements 
between the two lists. It is easy to show that the consensus
ranking corresponds to the geometric median of the input
rankings and that the Kemeny optimal aggregation problem
corresponds to the minimum feedback edge set problem.
2.3.3 Markov Chain Methods
Markov chains (MCs) have been used by Dwork et al. [11]
as a ‘natural" method to obtain a consensus ranking where
states correspond to the documents to be ranked and the
transition probabilities vary depending on the interpretation
of the transition event. In the same reference, the authors
proposed four specific MCs and experimental testing had
shown that the following MC is the best performing one
(see also [24]):
• MC4: move from the current state di to the next state
di by first choosing a document di uniformly from D.
If for the majority of the rankings, we have rj
i ≤ rj
i ,
then move to di , else stay in di.
The consensus ranking corresponds to the stationary 
distribution of MC4.
3. SPECIFICITIES OF THE RANK 
AGGREGATION PROBLEM IN THE IR CONTEXT
3.1 Limited Significance of the Rankings
The exact positions of documents in one input ranking
have limited significance and should not be overemphasized.
For instance, having three relevant documents in the first
three positions, any perturbation of these three items will
have the same value. Indeed, in the IR context, the complete
order provided by an input method may hide ties. In this
case, we call such rankings semi orders. This was outlined in
[13] as the problem of aggregation with ties. It is therefore
important to build the consensus ranking based on robust
information:
• Documents with near positions in j are more likely
to have similar interest or relevance. Thus a slight
perturbation of the initial ranking is meaningless.
• Assuming that document di is better ranked than 
document di in a ranking j, di is more likely to be 
definitively more relevant than di in j when the number
of intermediate positions between di and di increases.
3.2 Partial Lists
In real world applications, such as metasearch engines,
rankings provided by the input methods are often partial
lists. This was outlined in [14] as the problem of having to
merge top-k results from various input lists. For instance,
in the experiments carried out by Dwork et al. [11], authors
found that among the top 100 best documents of 7 input
search engines, 67% of the documents were present in only
one search engine, whereas less than two documents were
present in all the search engines.
Rank aggregation of partial lists raises four major 
difficulties which we state hereafter, proposing for each of them
various working assumptions:
1. Partial lists can have various lengths, which can favour
long lists. We thus consider the following two working
hypotheses:
H1
k : We only consider the top k best documents from
each input ranking.
H1
all: We consider all the documents from each input
ranking.
2. Since there are different documents in the input 
rankings, we must decide which documents should be kept
in the consensus ranking. Two working hypotheses are
therefore considered:
H2
k : We only consider documents which are present in
at least k input rankings (k > 1).
H2
all: We consider all the documents which are ranked
in at least one input ranking.
Hereafter, we call documents which will be retained
in the consensus ranking, candidate documents, and
documents that will be excluded from the consensus
ranking, excluded documents. We also call a candidate
document which is missing in one or more rankings, a
missing document.
3. Some candidate documents are missing documents in
some input rankings. Main reasons for a missing 
document are that it was not indexed or it was indexed
but deemed irrelevant ; usually this information is not
available. We consider the following two working 
hypotheses:
H3
yes: Each missing document in each j is assigned
a position.
H3
no: No assumption is made, that is each missing 
document is considered neither better nor worse than any
other document.
4. When assumption H2
k holds, each input ranking may
contain documents which will not be considered in the
consensus ranking. Regarding the positions of the 
candidate documents, we can consider the following 
working hypotheses:
H4
init: The initial positions of candidate documents
are kept in each input ranking.
H4
new: Candidate documents receive new positions in
each input ranking, after discarding excluded ones.
In the IR context, rank aggregation methods need to 
decide more or less explicitly which assumptions to retain
w.r.t. the above-mentioned difficulties.
4. OUTRANKING APPROACH FOR RANK
AGGREGATION
4.1 Presentation
Positional methods consider implicitly that the positions
of the documents in the input rankings are scores giving thus
a cardinal meaning to an ordinal information. This 
constitutes a strong assumption that is questionable, especially
when the input rankings have different lengths. Moreover,
for positional methods, assumptions H3
and H4
, which are
often arbitrary, have a strong impact on the results. For
instance, let us consider an input ranking of 500 documents
out of 1000 candidate documents. Whether we assign to
each of the missing documents the position 1, 501, 750 or
1000 -corresponding to variations of H3
yes- will give rise to
very contrasted results, especially regarding the top of the
consensus ranking.
Majoritarian methods do not suffer from the 
above-mentioned drawbacks of the positional methods since they build
consensus rankings exploiting only ordinal information 
contained in the input rankings. Nevertheless, they suppose
that such rankings are complete orders, ignoring that they
may hide ties. Therefore, majoritarian methods base 
consensus rankings on illusory discriminant information rather
than less discriminant but more robust information.
Trying to overcome the limits of current rank aggregation
methods, we found that outranking approaches, which were
initially used for multiple criteria aggregation problems [26],
can also be used for the rank aggregation purpose, where
each ranking plays the role of a criterion. Therefore, in
order to decide whether a document di should be ranked
better than di in the consensus ranking σ, the two following
conditions should be met:
• a concordance condition which ensures that a 
majority of the input rankings are concordant with diσdi
(majority principle).
• a discordance condition which ensures that none of the
discordant input rankings strongly refutes dσd 
(respect of minorities principle).
Formally, the concordance coalition with diσdi is
Csp (diσdi ) = { j∈ PR : rj
i ≤ rj
i − sp}
where sp is a preference threshold which is the variation
of document positions -whether it is absolute or relative to
the ranking length- which draws the boundaries between an
indifference and a preference situation between documents.
The discordance coalition with diσdi is
Dsv (diσdi ) = { j∈ PR : rj
i ≥ rj
i + sv}
where sv is a veto threshold which is the variation of 
document positions -whether it is absolute or relative to the
ranking length- which draws the boundaries between a weak
and a strong opposition to diσdi .
Depending on the exact definition of the preceding 
concordance and discordance coalitions leading to the definition
of some decision rules, several outranking relations can be
defined. They can be more or less demanding depending on
i) the values of the thresholds sp and sv, ii) the importance
or minimal size cmin required for the concordance coalition,
and iii) the importance or maximum size dmax of the 
discordance coalition.
A generic outranking relation can thus be defined as 
follows:
diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin
AND |Dsv (diσdi )| ≤ dmax
This expression defines a family of nested outranking 
relations since S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) when
cmin ≥ cmin and/or dmax ≤ dmax and/or sp ≥ sp and/or
sv ≤ sv. This expression also generalizes the majority rule
which corresponds to the particular relation S(0,∞, n
2
,n). It
also satisfies important properties of rank aggregation 
methods, called neutrality, Pareto-optimality, Condorcet 
property and Extended Condorcet property, in the social choice
literature [29].
Outranking relations are not necessarily transitive and do
not necessarily correspond to rankings since directed cycles
may exist. Therefore, we need specific procedures in order to
derive a consensus ranking. We propose the following 
procedure which finds its roots in [27]. It consists in partitioning
the set of documents into r ranked classes.
Each class Ch contains documents with the same relevance
and results from the application of all relations (if possible)
to the set of documents remaining after previous classes are
computed. Documents within the same equivalence class are
ranked arbitrarily.
Formally, let
• R be the set of candidate documents for a query,
• S1
, S2
, . . . be a family of nested outranking relations,
• Fk(di, E) = |{di ∈ E : diSk
di }| be the number of
documents in E(E ⊆ R) that could be considered
‘worse" than di according to relation Sk
,
• fk(di, E) = |{di ∈ E : di Sk
di}| be the number of
documents in E that could be considered ‘better" than
di according to Sk
,
• sk(di, E) = Fk(di, E) − fk(di, E) be the qualification
of di in E according to Sk
.
Each class Ch results from a distillation process. It 
corresponds to the last distillate of a series of sets E0 ⊇ E1 ⊇ . . .
where E0 = R \ (C1 ∪ . . . ∪ Ch−1) and Ek is a reduced 
subset of Ek−1 resulting from the application of the following
procedure:
1. compute for each di ∈ Ek−1 its qualification according
to Sk
, i.e. sk(di, Ek−1),
2. define smax = maxdi∈Ek−1 {sk(di, Ek−1)}, then
3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax}
When one outranking relation is used, the distillation 
process stops after the first application of the previous 
procedure, i.e., Ch corresponds to distillate E1. When different
outranking relations are used, the distillation process stops
when all the pre-defined outranking relations have been used
or when |Ek| = 1.
4.2 Illustrative Example
This section illustrates the concepts and procedures of
section 4.1. Let us consider a set of candidate documents
R = {d1, d2, d3, d4, d5}. The following table gives a profile
PR of different rankings of the documents of R: PR = ( 1
, 2, 3, 4).
Table 1: Rankings of documents
rj
i 1 2 3 4
d1 1 3 1 5
d2 2 1 3 3
d3 3 2 2 1
d4 4 4 5 2
d5 5 5 4 4
Let us suppose that the preference and veto thresholds
are set to values 1 and 4 respectively, and that the 
concordance and discordance thresholds are set to values 2 and 1
respectively. The following tables give the concordance, 
discordance and outranking matrices. Each entry csp (di, di )
(dsv (di, di )) in the concordance (discordance) matrix gives
the number of rankings that are concordant (discordant)
with diσdi , i.e. csp (di, di ) = |Csp (diσdi )| and dsv (di, di ) =
|Dsv (diσdi )|.
Table 2: Computation of the outranking relation
d1 d2 d3 d4 d5
d1 - 2 2 3 3
d2 2 - 2 3 4
d3 2 2 - 4 4
d4 1 1 0 - 3
d5 1 0 0 
1Concordance Matrix
d1 d2 d3 d4 d5
d1 - 0 1 0 0
d2 0 - 0 0 0
d3 0 0 - 0 0
d4 1 0 0 - 0
d5 1 1 0 
0Discordance Matrix
d1 d2 d3 d4 d5
d1 - 1 1 1 1
d2 1 - 1 1 1
d3 1 1 - 1 1
d4 0 0 0 - 1
d5 0 0 0 
0Outranking Matrix (S1)
For instance, the concordance coalition for the assertion
d1σd4 is C1(d1σd4) = { 1, 2, 3} and the discordance
coalition for the same assertion is D4(d1σd4) = ∅. 
Therefore, c1(d1, d4) = 3, d4(d1, d4) = 0 and d1S1
d4 holds.
Notice that Fk(di, R) (fk(di, R)) is given by summing the
values of the ith
row (column) of the outranking matrix. The
consensus ranking is obtained as follows: to get the first class
C1, we compute the qualifications of all the documents of
E0 = R with respect to S1
. They are respectively 2, 2, 2, -2
and -4. Therefore smax equals 2 and C1 = E1 = {d1, d2, d3}.
Observe that, if we had used a second outranking relation
S2(⊇ S1), these three documents could have been 
possibly discriminated. At this stage, we remove documents of
C1 from the outranking matrix and compute the next class
C2: we compute the new qualifications of the documents of
E0 = R \ C1 = {d4, d5}. They are respectively 1 and -1. So
C3 = E1 = {d4}. The last document d5 is the only 
document of the last class C3. Thus, the consensus ranking is
{d1, d2, d3} → {d4} → {d5}.
5. EXPERIMENTS AND RESULTS
5.1 Test Setting
To facilitate empirical investigation of the proposed 
methodology, we developed a prototype metasearch engine that
implements a version of our outranking approach for rank
aggregation. In this paper, we apply our approach to the
Topic Distillation (TD) task of TREC-2004 Web track [10].
In this task, there are 75 topics where only a short 
description of each is given. For each query, we retained the 
rankings of the 10 best runs of the TD task which are provided
by TREC-2004 participating teams. The performances of
these runs are reported in table 3.
Table 3: Performances of the 10 best runs of the TD
task of TREC-2004
Run Id MAP P@10 S@1 S@5 S@10
uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3%
MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0%
MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0%
humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7%
THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7%
UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0%
ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7%
SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0%
MU04web1 11.5% 19.9% 33.3% 64.0% 76.0%
MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0%
Average 14.7% 21.2% 34.9% 66.8% 78.94%
For each query, each run provides a ranking of about 1000
documents. The number of documents retrieved by all these
runs ranges from 543 to 5769. Their average (median) 
number is 3340 (3386). It is worth noting that we found similar
distributions of the documents among the rankings as in
[11].
For evaluation, we used the ‘trec eval" standard tool which
is used by the TREC community to calculate the standard
measures of system effectiveness which are Mean Average
Precision (MAP) and Success@n (S@n) for n=1, 5 and 10.
Our approach effectiveness is compared against some high
performing official results from TREC-2004 as well as against
some standard rank aggregation algorithms. In the 
experiments, significance testing is mainly based on the t-student
statistic which is computed on the basis of the MAP values of
the compared runs. In the tables of the following section,
statistically significant differences are marked with an 
asterisk. Values between brackets of the first column of each
table, indicate the parameter value of the corresponding run.
5.2 Results
We carried out several series of runs in order to i) study
performance variations of the outranking approach when
tuning the parameters and working assumptions, ii) 
compare performances of the outranking approach vs standard
rank aggregation strategies , and iii) check whether rank
aggregation performs better than the best input rankings.
We set our basic run mcm with the following parameters.
We considered that each input ranking is a complete 
order (sp = 0) and that an input ranking strongly refutes
diσdi when the difference of both document positions is
large enough (sv = 75%). Preference and veto thresholds
are computed proportionally to the number of documents 
retained in each input ranking. They consequently may vary
from one ranking to another. In addition, to accept the
assertion diσdi , we supposed that the majority of the 
rankings must be concordant (cmin = 50%) and that every input
ranking can impose its veto (dmax = 0). Concordance and
discordance thresholds are computed for each tuple (di, di )
as the percentage of the input rankings of PRi ∩PRi . Thus,
our choice of parameters leads to the definition of the 
outranking relation S(0,75%,50%,0).
To test the run mcm, we had chosen the following 
assumptions. We retained the top 100 best documents from each
input ranking (H1
100), only considered documents which are
present in at least half of the input rankings (H2
5 ) and 
assumed H3
no and H4
new. In these conditions, the number of
successful documents was about 100 on average, and the
computation time per query was less than one second.
Obviously, modifying the working assumptions should have
deeper impact on the performances than tuning our model
parameters. This was validated by preliminary experiments.
Thus, we hereafter begin by studying performance variation
when different sets of assumptions are considered. 
Afterwards, we study the impact of tuning parameters. Finally,
we compare our model performances w.r.t. the input 
rankings as well as some standard data fusion algorithms.
5.2.1 Impact of the Working Assumptions
Table 4 summarizes the performance variation of the 
outranking approach under different working hypotheses. In
Table 4: Impact of the working assumptions
Run Id MAP S@1 S@5 S@10
mcm 18.47% 41.33% 81.33% 86.67%
mcm22 (H3
yes) 17.72% (-4.06%) 34.67% 81.33% 86.67%
mcm23 (H4
init) 18.26% (-1.14%) 41.33% 81.33% 86.67%
mcm24 (H1
all) 20.67% (+11.91%*) 38.66% 80.00% 86.66%
mcm25 (H2
all) 21.68% (+17.38%*) 40.00% 78.66% 89.33%
this table, we first show that run mcm22, in which missing
documents are all put in the same last position of each input
ranking, leads to performance drop w.r.t. run mcm. 
Moreover, S@1 moves from 41.33% to 34.67% (-16.11%). This
shows that several relevant documents which were initially
put at the first position of the consensus ranking in mcm, lose
this first position but remain ranked in the top 5 documents
since S@5 did not change. We also conclude that documents
which have rather good positions in some input rankings are
more likely to be relevant, even though they are missing in
some other rankings. Consequently, when they are missing
in some rankings, assigning worse ranks to these documents
is harmful for performance.
Also, from Table 4, we found that the performances of
runs mcm and mcm23 are similar. Therefore, the outranking
approach is not sensitive to keeping the initial positions of
candidate documents or recomputing them by discarding
excluded ones.
From the same Table 4, performance of the outranking
approach increases significantly for runs mcm24 and mcm25.
Therefore, whether we consider all the documents which are
present in half of the rankings (mcm24) or we consider all
the documents which are ranked in the first 100 positions in
one or more rankings (mcm25), increases performances. This
result was predictable since in both cases we have more 
detailed information on the relative importance of documents.
Tables 5 and 6 confirm this evidence. Table 5, where 
values between brackets of the first column give the number
of documents which are retained from each input ranking,
shows that selecting more documents from each input 
ranking leads to performance increase. It is worth mentioning
that selecting more than 600 documents from each input
ranking does not improve performance.
Table 5: Impact of the number of retained 
documents
Run Id MAP S@1 S@5 S@10
mcm (100) 18.47% 41.33% 81.33% 86.67%
mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00%
mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00%
mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00%
mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67%
mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66%
Table 6 reports runs corresponding to variations of H2
k .
Values between brackets are rank hits. For instance, in
the run mcm32, only documents which are present in 3 or
more input rankings, were considered successful. This 
table shows that performance is significantly better when rare
documents are considered, whereas it decreases significantly
when these documents are discarded. Therefore, we 
conclude that many of the relevant documents are retrieved by
a rather small set of IR models.
Table 6: Performance considering different rank hits
Run Id MAP S@1 S@5 S@10
mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33%
mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33%
mcm (5) 18.47% 41.33% 81.33% 86.67%
mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33%
mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83%
mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70%
For both runs mcm24 and mcm25, the number of successful
documents was about 1000 and therefore, the computation
time per query increased and became around 5 seconds.
5.2.2 Impact of the Variation of the Parameters
Table 7 shows performance variation of the outranking 
approach when different preference thresholds are considered.
We found performance improvement up to threshold values
of about 5%, then there is a decrease in the performance
which becomes significant for threshold values greater than
10%. Moreover, S@1 improves from 41.33% to 46.67% when
preference threshold changes from 0 to 5%. We can thus
conclude that the input rankings are semi orders rather than
complete orders.
Table 8 shows the evolution of the performance measures
w.r.t. the concordance threshold. We can conclude that in
order to put document di before di in the consensus ranking,
Table 7: Impact of the variation of the preference
threshold from 0 to 12.5%
Run Id MAP S@1 S@5 S@10
mcm (0%) 18.47% 41.33% 81.33% 86.67%
mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67%
mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67%
mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67%
mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67%
mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67%
mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67%
at least half of the input rankings of PRi ∩ PRi should be
concordant. Performance drops significantly for very low
and very high values of the concordance threshold. In fact,
for such values, the concordance condition is either fulfilled
rather always by too many document pairs or not fulfilled at
all, respectively. Therefore, the outranking relation becomes
either too weak or too strong respectively.
Table 8: Impact of the variation of cmin
Run Id MAP S@1 S@5 S@10
mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33%
mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67%
mcm (50%) 18.47% 41.33% 81.33% 86.67%
mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67%
mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67%
mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33%
In the experiments, varying the veto threshold as well as
the discordance threshold within reasonable intervals does
not have significant impact on performance measures. In
fact, runs with different veto thresholds (sv ∈ [50%; 100%])
had similar performances even though there is a slight 
advantage for runs with high threshold values which means
that it is better not to allow the input rankings to put their
veto easily. Also, tuning the discordance threshold was 
carried out for values 50% and 75% of the veto threshold. For
these runs we did not get any noticeable performance 
variation, although for low discordance thresholds (dmax < 20%),
performance slightly decreased.
5.2.3 Impact of the Variation of the Number of Input
Rankings
To study performance evolution when different sets of 
input rankings are considered, we carried three more runs
where 2, 4, and 6 of the best performing sets of the 
input rankings are considered. Results reported in Table 9
are seemingly counter-intuitive and also do not support 
previous findings regarding rank aggregation research [3]. 
Nevertheless, this result shows that low performing rankings
bring more noise than information to the establishment of
the consensus ranking. Therefore, when they are considered,
performance decreases.
Table 9: Performance considering different best 
performing sets of input rankings
Run Id MAP S@1 S@5 S@10
mcm (10) 18.47% 41.33% 81.33% 86.67%
mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33%
mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00%
mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00%
5.2.4 Comparison of the Performance of Different
Rank Aggregation Methods
In this set of runs, we compare the outranking approach
with some standard rank aggregation methods which were
proven to have acceptable performance in previous studies:
we considered two positional methods which are the 
CombSUM and the CombMNZ strategies. We also examined
the performance of one majoritarian method which is the
Markov chain method (MC4). For the comparisons, we 
considered a specific outranking relation S∗
= S(5%,50%,50%,30%)
which results in good overall performances when tuning all
the parameters.
The first row of Table 10 gives performances of the rank
aggregation methods w.r.t. a basic assumption set A1 =
(H1
100, H2
5 , H4
new): we only consider the 100 first documents
from each ranking, then retain documents present in 5 or
more rankings and update ranks of successful documents.
For positional methods, we place missing documents at the
queue of the ranking (H3
yes) whereas for our method as well
as for MC4, we retained hypothesis H3
no. The three 
following rows of Table 10 report performances when changing
one element from the basic assumption set: the second row
corresponds to the assumption set A2 = (H1
1000, H2
5 , H4
new),
i.e. changing the number of retained documents from 100
to 1000. The third row corresponds to the assumption set
A3 = (H1
100, H2
all, H4
new), i.e. considering the documents
present in at least one ranking. The fourth row corresponds
to the assumption set A4 = (H1
100, H2
5 , H4
init), i.e. keeping
the original ranks of successful documents.
The fifth row of Table 10, labeled A5, gives performance
when all the 225 queries of the Web track of TREC-2004 are
considered. Obviously, performance level cannot be 
compared with previous lines since the additional queries are
different from the TD queries and correspond to other tasks
(Home Page and Named Page tasks [10]) of TREC-2004
Web track. This set of runs aims to show whether relative
performance of the various methods is task-dependent.
The last row of Table 10, labeled A6, reports performance
of the various methods considering the TD task of 
TREC2002 instead of TREC-2004: we fused the results of input
rankings of the 10 best official runs for each of the 50 TD
queries [9] considering the set of assumptions A1 of the first
row. This aims to show whether relative performance of the
various methods changes from year to year.
Values between brackets of Table 10 are variations of 
performance of each rank aggregation method w.r.t. 
performance of the outranking approach.
Table 10: Performance (MAP) of different rank 
aggregation methods under 3 different test collections
mcm combSUM combMNZ markov
A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%)
A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%)
A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*)
A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%)
A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%)
A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%)
From the analysis of table 10 the following can be 
established:
• for all the runs, considering all the documents in each
input ranking (A2) significantly improves performance
(MAP increases by 11.62% on average). This is 
predictable since some initially unreported relevant 
documents would receive better positions in the consensus
ranking.
• for all the runs, considering documents even those 
present in only one input ranking (A3) significantly 
improves performance. For mcm, combSUM and combMNZ,
performance improvement is more important (MAP 
increases by 20.27% on average) than for the markov run
(MAP increases by 3.86%).
• preserving the initial positions of documents (A4) or
recomputing them (A1) does not have a noticeable 
influence on performance for both positional and 
majoritarian methods.
• considering all the queries of the Web track of 
TREC2004 (A5) as well as the TD queries of the Web track
of TREC-2002 (A6) does not alter the relative 
performance of the different data fusion methods.
• considering the TD queries of the Web track of 
TREC2002, performances of all the data fusion methods are
lower than that of the best performing input ranking
for which the MAP value equals 18.58%. This is because
most of the fused input rankings have very low 
performances compared to the best one, which brings more
noise to the consensus ranking.
• performances of the data fusion methods mcm and markov
are significantly better than that of the best input
ranking uogWebCAU150. This remains true for runs
combSUM and combMNZ only under assumptions H1
all or
H2
all. This shows that majoritarian methods are less
sensitive to assumptions than positional methods.
• outranking approach always performs significantly 
better than positional methods combSUM and combMNZ. It
has also better performances than the Markov chain
method, especially under assumption H2
all where 
difference of performances becomes significant.
6. CONCLUSIONS
In this paper, we address the rank aggregation problem
where different, but not disjoint, lists of documents are to
be fused. We noticed that the input rankings can hide ties,
so they should not be considered as complete orders. Only
robust information should be used from each input ranking.
Current rank aggregation methods, and especially 
positional methods (e.g. combSUM [15]), are not initially 
designed to work with such rankings. They should be adapted
by considering specific working assumptions.
We propose a new outranking method for rank 
aggregation which is well adapted to the IR context. Indeed, it
ranks two documents w.r.t. the intensity of their positions
difference in each input ranking and also considering the
number of the input rankings that are concordant and 
discordant in favor of a specific document. There is also no
need to make specific assumptions on the positions of the
missing documents. This is an important feature since the
absence of a document from a ranking should not be 
necessarily interpreted negatively.
Experimental results show that the outranking method
significantly out-performs popular classical positional data
fusion methods like combSUM and combMNZ strategies. It
also out-performs a good performing majoritarian methods
which is the Markov chain method. These results are tested
against different test collections and queries. From the 
experiments, we can also conclude that in order to improve the
performances, we should fuse result lists of well performing
IR models, and that majoritarian data fusion methods 
perform better than positional methods.
The proposed method can have a real impact on Web
metasearch performances since only ranks are available from
most primary search engines, whereas most of the current
approaches need scores to merge result lists into one single
list.
Further work involves investigating whether the 
outranking approach performs well in various other contexts, e.g.
using the document scores or some combination of 
document ranks and scores.
Acknowledgments
The authors would like to thank Jacques Savoy for his 
valuable comments on a preliminary version of this paper.
7. REFERENCES
[1] A. Aronson, D. Demner-Fushman, S. Humphrey,
J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe,
and W. Wilbur. Fusion of knowledge-intensive and
statistical approaches for retrieving and annotating
textual genomics documents. In Proceedings
TREC"2005. NIST Publication, 2005.
[2] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern
Information Retrieval. ACM Press , 1999.
[3] B. T. Bartell, G. W. Cottrell, and R. K. Belew.
Automatic combination of multiple ranked retrieval
systems. In Proceedings ACM-SIGIR"94, pages
173-181. Springer-Verlag, 1994.
[4] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw.
Combining evidence of multiple query representations
for information retrieval. IPM, 31(3):431-448, 1995.
[5] J. Borda. M´emoire sur les ´elections au scrutin.
Histoire de l"Acad´emie des Sciences, 1781.
[6] J. P. Callan, Z. Lu, and W. B. Croft. Searching
distributed collections with inference networks. In
Proceedings ACM-SIGIR"95, pages 21-28, 1995.
[7] M. Condorcet. Essai sur l"application de l"analyse `a la
probabilit´e des d´ecisions rendues `a la pluralit´e des
voix. Imprimerie Royale, Paris, 1785.
[8] W. D. Cook and M. Kress. Ordinal ranking with
intensity of preference. Management Science,
31(1):26-32, 1985.
[9] N. Craswell and D. Hawking. Overview of the
TREC-2002 Web Track. In Proceedings TREC"2002.
NIST Publication, 2002.
[10] N. Craswell and D. Hawking. Overview of the
TREC-2004 Web Track. In Proceedings of
TREC"2004. NIST Publication, 2004.
[11] C. Dwork, S. R. Kumar, M. Naor, and D. Sivakumar.
Rank aggregation methods for the Web. In
Proceedings WWW"2001, pages 613-622, 2001.
[12] R. Fagin. Combining fuzzy information from multiple
systems. JCSS, 58(1):83-99, 1999.
[13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar, and
E. Vee. Comparing and aggregating rankings with
ties. In PODS, pages 47-58, 2004.
[14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing
top k lists. SIAM J. on Discrete Mathematics,
17(1):134-160, 2003.
[15] E. A. Fox and J. A. Shaw. Combination of multiple
searches. In Proceedings of TREC"3. NIST
Publication, 1994.
[16] J. Katzer, M. McGill, J. Tessier, W. Frakes, and
P. DasGupta. A study of the overlap among document
representations. Information Technology: Research
and Development, 1(4):261-274, 1982.
[17] L. S. Larkey, M. E. Connell, and J. Callan. Collection
selection and results merging with topically organized
U.S. patents and TREC data. In Proceedings
ACM-CIKM"2000, pages 282-289. ACM Press, 2000.
[18] A. Le Calv´e and J. Savoy. Database merging strategy
based on logistic regression. IPM, 36(3):341-359, 2000.
[19] J. H. Lee. Analyses of multiple evidence combination.
In Proceedings ACM-SIGIR"97, pages 267-276, 1997.
[20] D. Lillis, F. Toolan, R. Collier, and J. Dunnion.
Probfuse: a probabilistic approach to data fusion. In
Proceedings ACM-SIGIR"2006, pages 139-146. ACM
Press, 2006.
[21] J. I. Marden. Analyzing and Modeling Rank Data.
Number 64 in Monographs on Statistics and Applied
Probability. Chapman & Hall, 1995.
[22] M. Montague and J. A. Aslam. Metasearch
consistency. In Proceedings ACM-SIGIR"2001, pages
386-387. ACM Press, 2001.
[23] D. M. Pennock and E. Horvitz. Analysis of the
axiomatic foundations of collaborative filtering. In
Workshop on AI for Electronic Commerce at the 16th
National Conference on Artificial Intelligence, 1999.
[24] M. E. Renda and U. Straccia. Web metasearch: rank
vs. score based rank aggregation methods. In
Proceedings ACM-SAC"2003, pages 841-846. ACM
Press, 2003.
[25] W. H. Riker. Liberalism against populism. Waveland
Press, 1982.
[26] B. Roy. The outranking approach and the foundations
of ELECTRE methods. Theory and Decision,
31:49-73, 1991.
[27] B. Roy and J. Hugonnard. Ranking of suburban line
extension projects on the Paris metro system by a
multicriteria method. Transportation Research,
16A(4):301-312, 1982.
[28] L. Si and J. Callan. Using sampled data and regression
to merge search engine results. In Proceedings
ACM-SIGIR"2002, pages 19-26. ACM Press, 2002.
[29] M. Truchon. An extension of the Condorcet criterion
and Kemeny orders. Cahier 9813, Centre de Recherche
en Economie et Finance Appliqu´ees, Oct. 1998.
[30] H. Turtle and W. B. Croft. Inference networks for
document retrieval. In Proceedings of ACM-SIGIR"90,
pages 1-24. ACM Press, 1990.
[31] C. C. Vogt and G. W. Cottrell. Fusion via a linear
combination of scores. Information Retrieval,
1(3):151-173, 1999.
A Time Machine for Text Search
Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum
Max-Planck Institute for Informatics
Saarbr¨ucken, Germany
{kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de
ABSTRACT
Text search over temporally versioned document collections
such as web archives has received little attention as a 
research problem. As a consequence, there is no scalable and
principled solution to search such a collection as of a 
specified time t. In this work, we address this shortcoming and
propose an efficient solution for time-travel text search by
extending the inverted file index to make it ready for 
temporal search. We introduce approximate temporal coalescing
as a tunable method to reduce the index size without 
significantly affecting the quality of results. In order to further
improve the performance of time-travel queries, we 
introduce two principled techniques to trade off index size for
its performance. These techniques can be formulated as 
optimization problems that can be solved to near-optimality.
Finally, our approach is evaluated in a comprehensive 
series of experiments on two large-scale real-world datasets.
Results unequivocally show that our methods make it 
possible to build an efficient time machine scalable to large
versioned text collections.
Categories and Subject Descriptors
H.3.1 [Content Analysis and Indexing]: Indexing 
methods; H.3.3 [Information Search and Retrieval]: 
Retrieval models, Search process
General Terms
Algorithms, Experimentation, Performance
1. INTRODUCTION
In this work we address time-travel text search over 
temporally versioned document collections. Given a keyword
query q and a time t our goal is to identify and rank 
relevant documents as if the collection was in its state as of
time t.
An increasing number of such versioned document 
collections is available today including web archives, 
collaborative authoring environments like Wikis, or timestamped
information feeds. Text search on these collections, 
however, is mostly time-ignorant: while the searched collection
changes over time, often only the most recent version of
a documents is indexed, or, versions are indexed 
independently and treated as separate documents. Even worse, for
some collections, in particular web archives like the 
Internet Archive [18], a comprehensive text-search functionality
is often completely missing.
Time-travel text search, as we develop it in this paper,
is a crucial tool to explore these collections and to unfold
their full potential as the following example demonstrates.
For a documentary about a past political scandal, a 
journalist needs to research early opinions and statements made
by the involved politicians. Sending an appropriate query
to a major web search-engine, the majority of returned 
results contains only recent coverage, since many of the early
web pages have disappeared and are only preserved in web
archives. If the query could be enriched with a time point,
say August 20th 2003 as the day after the scandal got 
revealed, and be issued against a web archive, only pages that
existed specifically at that time could be retrieved thus better
satisfying the journalist"s information need.
Document collections like the Web or Wikipedia [32], as
we target them here, are already large if only a single 
snapshot is considered. Looking at their evolutionary history, we
are faced with even larger data volumes. As a consequence,
na¨ıve approaches to time-travel text search fail, and viable
approaches must scale-up well to such large data volumes.
This paper presents an efficient solution to time-travel
text search by making the following key contributions:
1. The popular well-studied inverted file index [35] is 
transparently extended to enable time-travel text search.
2. Temporal coalescing is introduced to avoid an 
indexsize explosion while keeping results highly accurate.
3. We develop two sublist materialization techniques to
improve index performance that allow trading off space
vs. performance.
4. In a comprehensive experimental evaluation our 
approach is evaluated on the English Wikipedia and parts
of the Internet Archive as two large-scale real-world
datasets with versioned documents.
The remainder of this paper is organized as follows. The
presented work is put in context with related work in 
Section 2. We delineate our model of a temporally versioned
document collection in Section 3. We present our time-travel
inverted index in Section 4. Building on it, temporal 
coalescing is described in Section 5. In Section 6 we describe
principled techniques to improve index performance, before
presenting the results of our experimental evaluation in 
Section 7.
2. RELATED WORK
We can classify the related work mainly into the following
two categories: (i) methods that deal explicitly with 
collections of versioned documents or temporal databases, and
(ii) methods for reducing the index size by exploiting either
the document-content overlap or by pruning portions of the
index. We briefly review work under these categories here.
To the best of our knowledge, there is very little prior work
dealing with historical search over temporally versioned 
documents. Anick and Flynn [3], while pioneering this research,
describe a help-desk system that supports historical queries.
Access costs are optimized for accesses to the most recent
versions and increase as one moves farther into the past.
Burrows and Hisgen [10], in a patent description, delineate
a method for indexing range-based values and mention its
potential use for searching based on dates associated with
documents. Recent work by Nørv˚ag and Nybø [25] and
their earlier proposals concentrate on the relatively simpler
problem of supporting text-containment queries only and
neglect the relevance scoring of results. Stack [29] reports
practical experiences made when adapting the open source
search-engine Nutch to search web archives. This 
adaptation, however, does not provide the intended time-travel
text search functionality. In contrast, research in temporal
databases has produced several index structures tailored for
time-evolving databases; a comprehensive overview of the
state-of-art is available in [28]. Unlike the inverted file 
index, their applicability to text search is not well understood.
Moving on to the second category of related work, Broder
et al. [8] describe a technique that exploits large content
overlaps between documents to achieve a reduction in index
size. Their technique makes strong assumptions about the
structure of document overlaps rendering it inapplicable to
our context. More recent approaches by Hersovici et al. [17]
and Zhang and Suel [34] exploit arbitrary content overlaps
between documents to reduce index size. None of the 
approaches, however, considers time explicitly or provides the
desired time-travel text search functionality. Static 
indexpruning techniques [11, 12] aim to reduce the effective index
size, by removing portions of the index that are expected
to have low impact on the query result. They also do not
consider temporal aspects of documents, and thus are 
technically quite different from our proposal despite having a
shared goal of index-size reduction. It should be noted that
index-pruning techniques can be adapted to work along with
the temporal text index we propose here.
3. MODEL
In the present work, we deal with a temporally versioned
document collection D that is modeled as described in the
following. Each document d ∈ D is a sequence of its versions
d = dt1
, dt2
, . . . .
Each version dti
has an associated timestamp ti reflecting
when the version was created. Each version is a vector of
searchable terms or features. Any modification to a 
document version results in the insertion of a new version with
corresponding timestamp. We employ a discrete definition
of time, so that timestamps are non-negative integers. The
deletion of a document at time ti, i.e., its disappearance
from the current state of the collection, is modeled as the
insertion of a special tombstone version ⊥. The validity
time-interval val(dti
) of a version dti
is [ti, ti+1), if a newer
version with associated timestamp ti+1 exists, and [ti, now)
otherwise where now points to the greatest possible value of
a timestamp (i.e., ∀t : t < now).
Putting all this together, we define the state Dt
of the
collection at time t (i.e., the set of versions valid at t that
are not deletions) as
Dt
=
[
d∈D
{dti
∈ d | t ∈ val(dti
) ∧ dti
= ⊥} .
As mentioned earlier, we want to enrich a keyword query
q with a timestamp t, so that q be evaluated over Dt
, i.e.,
the state of the collection at time t. The enriched time-travel
query is written as q t
for brevity.
As a retrieval model in this work we adopt Okapi BM25 [27],
but note that the proposed techniques are not dependent on
this choice and are applicable to other retrieval models like
tf-idf [4] or language models [26] as well. For our considered
setting, we slightly adapt Okapi BM25 as
w(q t
, dti
) =
X
v∈q
wtf (v, dti
) · widf (v, t) .
In the above formula, the relevance w(q t
, dti
) of a 
document version dti
to the time-travel query q t
is defined.
We reiterate that q t
is evaluated over Dt
so that only the
version dti
valid at time t is considered. The first factor
wtf (v, dti
) in the summation, further referred to as the 
tfscore is defined as
wtf (v, dti
) =
(k1 + 1) · tf(v, dti
)
k1 · ((1 − b) + b · dl(d ti )
avdl(ti)
) + tf(v, dti )
.
It considers the plain term frequency tf(v, dti
) of term v
in version dti
normalizing it, taking into account both the
length dl(dti
) of the version and the average document length
avdl(ti) in the collection at time ti. The length-normalization
parameter b and the tf-saturation parameter k1 are inherited
from the original Okapi BM25 and are commonly set to 
values 1.2 and 0.75 respectively. The second factor widf (v, t),
which we refer to as the idf-score in the remainder, conveys
the inverse document frequency of term v in the collection
at time t and is defined as
widf (v, t) = log
N(t) − df(v, t) + 0.5
df(v, t) + 0.5
where N(t) = |Dt
| is the collection size at time t and df(v, t)
gives the number of documents in the collection that contain
the term v at time t. While the idf-score depends on the
whole corpus as of the query time t, the tf-score is specific
to each version.
4. TIME-TRAVELINVERTEDFILEINDEX
The inverted file index is a standard technique for text
indexing, deployed in many systems. In this section, we
briefly review this technique and present our extensions to
the inverted file index that make it ready for time-travel text
search.
4.1 Inverted File Index
An inverted file index consists of a vocabulary, commonly
organized as a B+-Tree, that maps each term to its 
idfscore and inverted list. The index list Lv belonging to term
v contains postings of the form
( d, p )
where d is a document-identifier and p is the so-called 
payload. The payload p contains information about the term
frequency of v in d, but may also include positional 
information about where the term appears in the document.
The sort-order of index lists depends on which queries are
to be supported efficiently. For Boolean queries it is 
favorable to sort index lists in document-order. 
Frequencyorder and impact-order sorted index lists are beneficial for
ranked queries and enable optimized query processing that
stops early after having identified the k most relevant 
documents [1, 2, 9, 15, 31]. A variety of compression techniques,
such as encoding document identifiers more compactly, have
been proposed [33, 35] to reduce the size of index lists. For
an excellent recent survey about inverted file indexes we 
refer to [35].
4.2 Time-Travel Inverted File Index
In order to prepare an inverted file index for time travel
we extend both inverted lists and the vocabulary structure
by explicitly incorporating temporal information. The main
idea for inverted lists is that we include a validity 
timeinterval [tb, te) in postings to denote when the payload 
information was valid. The postings in our time-travel inverted
file index are thus of the form
( d, p, [tb, te) )
where d and p are defined as in the standard inverted file
index above and [tb, te) is the validity time-interval.
As a concrete example, in our implementation, for a 
version dti
having the Okapi BM25 tf-score wtf (v, dti
) for term
v, the index list Lv contains the posting
( d, wtf (v, dti
), [ti, ti+1) ) .
Similarly, the extended vocabulary structure maintains
for each term a time-series of idf-scores organized as a 
B+Tree. Unlike the tf-score, the idf-score of every term could
vary with every change in the corpus. Therefore, we take
a simplified approach to idf-score maintenance, by 
computing idf-scores for all terms in the corpus at specific (possibly
periodic) times.
4.3 Query Processing
During processing of a time-travel query q t
, for each query
term the corresponding idf-score valid at time t is retrieved
from the extended vocabulary. Then, index lists are 
sequentially read from disk, thereby accumulating the information
contained in the postings. We transparently extend the 
sequential reading, which is - to the best of our 
knowledgecommon to all query processing techniques on inverted file
indexes, thus making them suitable for time-travel 
queryprocessing. To this end, sequential reading is extended by
skipping all postings whose validity time-interval does not
contain t (i.e., t ∈ [tb, te)). Whether a posting can be
skipped can only be decided after the posting has been 
transferred from disk into memory and therefore still incurs 
significant I/O cost. As a remedy, we propose index organization
techniques in Section 6 that aim to reduce the I/O overhead
significantly.
We note that our proposed extension of the inverted file
index makes no assumptions about the sort-order of 
index lists. As a consequence, existing query-processing 
techniques and most optimizations (e.g., compression techniques)
remain equally applicable.
5. TEMPORAL COALESCING
If we employ the time-travel inverted index, as described
in the previous section, to a versioned document collection,
we obtain one posting per term per document version. For
frequent terms and large highly-dynamic collections, this
time
score
non-coalesced
coalesced
Figure 1: Approximate Temporal Coalescing
leads to extremely long index lists with very poor 
queryprocessing performance.
The approximate temporal coalescing technique that we
propose in this section counters this blowup in index-list
size. It builds on the observation that most changes in a
versioned document collection are minor, leaving large parts
of the document untouched. As a consequence, the payload
of many postings belonging to temporally adjacent versions
will differ only slightly or not at all. Approximate temporal
coalescing reduces the number of postings in an index list by
merging such a sequence of postings that have almost equal
payloads, while keeping the maximal error bounded. This
idea is illustrated in Figure 1, which plots non-coalesced and
coalesced scores of postings belonging to a single document.
Approximate temporal coalescing is greatly effective given
such fluctuating payloads and reduces the number of 
postings from 9 to 3 in the example. The notion of temporal
coalescing was originally introduced in temporal database
research by B¨ohlen et al. [6], where the simpler problem of
coalescing only equal information was considered.
We next formally state the problem dealt with in 
approximate temporal coalescing, and discuss the computation of
optimal and approximate solutions. Note that the technique
is applied to each index list separately, so that the following
explanations assume a fixed term v and index list Lv.
As an input we are given a sequence of temporally 
adjacent postings
I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) .
Each sequence represents a contiguous time period during
which the term was present in a single document d. If a term
disappears from d but reappears later, we obtain multiple
input sequences that are dealt with separately. We seek to
generate the minimal length output sequence of postings
O = ( d, pj, [tj, tj+1) ), . . . , ( d, pm−1, [tm−1, tm)) ) ,
that adheres to the following constraints: First, O and I
must cover the same time-range, i.e., ti = tj and tn = tm.
Second, when coalescing a subsequence of postings of the 
input into a single posting of the output, we want the 
approximation error to be below a threshold . In other words, if
(d, pi, [ti, ti+1)) and (d, pj, [tj, tj+1)) are postings of I and
O respectively, then the following must hold for a chosen
error function and a threshold :
tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ .
In this paper, as an error function we employ the relative
error between payloads (i.e., tf-scores) of a document in I
and O, defined as:
errrel(pi, pj) = |pi − pj| / |pi| .
Finding an optimal output sequence of postings can be
cast into finding a piecewise-constant representation for the
points (ti, pi) that uses a minimal number of segments while
retaining the above approximation guarantee. Similar 
problems occur in time-series segmentation [21, 30] and histogram
construction [19, 20]. Typically dynamic programming is
applied to obtain an optimal solution in O(n2
m∗
) [20, 30]
time with m∗
being the number of segments in an optimal
sequence. In our setting, as a key difference, only a 
guarantee on the local error is retained - in contrast to a guarantee
on the global error in the aforementioned settings. 
Exploiting this fact, an optimal solution is computable by means of
induction [24] in O(n2
) time. Details of the optimal 
algorithm are omitted here but can be found in the 
accompanying technical report [5].
The quadratic complexity of the optimal algorithm makes
it inappropriate for the large datasets encountered in this
work. As an alternative, we introduce a linear-time 
approximate algorithm that is based on the sliding-window 
algorithm given in [21]. This algorithm produces nearly-optimal
output sequences that retain the bound on the relative error,
but possibly require a few additional segments more than an
optimal solution.
Algorithm 1 Temporal Coalescing (Approximate)
1: I = ( d, pi, [ti, ti+1) ), . . . O =
2: pmin = pi pmax = pi p = pi tb = ti te = ti+1
3: for ( d, pj, [tj, tj+1) ) ∈ I do
4: pmin = min( pmin, pj ) pmax = max( pmax, pj )
5: p = optrep(pmin, pmax)
6: if errrel(pmin, p ) ≤ ∧ errrel(pmax, p ) ≤ then
7: pmin = pmin pmax = pmax p = p te = tj+1
8: else
9: O = O ∪ ( d, p, [tb, te) )
10: pmin = pj pmax = pj p = pj tb = tj te = tj+1
11: end if
12: end for
13: O = O ∪ ( d, p, [tb, te) )
Algorithm 1 makes one pass over the input sequence I.
While doing so, it coalesces sequences of postings having
maximal length. The optimal representative for a sequence
of postings depends only on their minimal and maximal 
payload (pmin and pmax) and can be looked up using optrep in
O(1) (see [16] for details). When reading the next 
posting, the algorithm tries to add it to the current sequence of
postings. It computes the hypothetical new representative
p and checks whether it would retain the approximation
guarantee. If this test fails, a coalesced posting bearing the
old representative is added to the output sequence O and,
following that, the bookkeeping is reinitialized. The time
complexity of the algorithm is in O(n).
Note that, since we make no assumptions about the sort
order of index lists, temporal-coalescing algorithms have an
additional preprocessing cost in O(|Lv| log |Lv|) for sorting
the index list and chopping it up into subsequences for each
document.
6. SUBLIST MATERIALIZATION
Efficiency of processing a query q t
on our time-travel 
inverted index is influenced adversely by the wasted I/O due
to read but skipped postings. Temporal coalescing 
implicitly addresses this problem by reducing the overall index list
size, but still a significant overhead remains. In this section,
we tackle this problem by proposing the idea of materializing
sublists each of which corresponds to a contiguous 
subinterval of time spanned by the full index. Each of these sublists
contains all coalesced postings that overlap with the 
corresponding time interval of the sublist. Note that all those
postings whose validity time-interval spans across the 
temporal boundaries of several sublists are replicated in each of
the spanned sublists. Thus, in order to process the query q t
time
t1 t2 t3 t4 t5 t6 t7 t8 t9 t10
d1
d2
d3
document
1 2 3 4
5 6 7
8 9 10
Figure 2: Sublist Materialization
it is sufficient to scan any materialized sublist whose 
timeinterval contains t.
We illustrate the idea of sublist materialization using an
example shown in Figure 2. The index list Lv visualized in
the figure contains a total of 10 postings from three 
documents d1, d2, and d3. For ease of description, we have
numbered boundaries of validity time-intervals, in increasing
time-order, as t1, . . . , t10 and numbered the postings 
themselves as 1, . . . , 10. Now, consider the processing of a query
q t
with t ∈ [t1, t2) using this inverted list. Although only
three postings (postings 1, 5 and 8) are valid at time t, the
whole inverted list has to be read in the worst case. Suppose
that we split the time axis of the list at time t2, forming two
sublists with postings {1, 5, 8} and {2, 3, 4, 5, 6, 7, 8, 9,
10} respectively. Then, we can process the above query with
optimal cost by reading only those postings that existed at
this t.
At a first glance, it may seem counterintuitive to reduce
index size in the first step (using temporal coalescing), and
then to increase it again using the sublist materialization
techniques presented in this section. However, we reiterate
that our main objective is to improve the efficiency of 
processing queries, not to reduce the index size alone. The use
of temporal coalescing improves the performance by 
reducing the index size, while the sublist materialization improves
performance by judiciously replicating entries. Further, the
two techniques, can be applied separately and are 
independent. If applied in conjunction, though, there is a synergetic
effect - sublists that are materialized from a temporally 
coalesced index are generally smaller.
We employ the notation Lv : [ti, tj) to refer to the 
materialized sublist for the time interval [ti, tj), that is formally
defined as,
Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti} .
To aid the presentation in the rest of the paper, we first
provide some definitions. Let T = t1 . . . tn be the sorted
sequence of all unique time-interval boundaries of an 
inverted list Lv. Then we define
E = { [ti, ti+1) | 1 ≤ i < n}
to be the set of elementary time intervals. We refer to the
set of time intervals for which sublists are materialized as
M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n } ,
and demand
∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m ,
i.e., the time intervals in M must completely cover the
time interval [t1, tn), so that time-travel queries q t
for all
t ∈ [t1, tn) can be processed. We also assume that 
intervals in M are disjoint. We can make this assumption 
without ruling out any optimal solution with regard to space
or performance defined below. The space required for the
materialization of sublists in a set M is defined as
S( M ) =
X
m∈M
|Lv : m| ,
i.e., the total length of all lists in M. Given a set M, we let
π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk)
denote the time interval that is used to process queries q t
with t ∈ [ti, ti+1). The performance of processing queries
q t
for t ∈ [ti, ti+1) inversely depends on its processing cost
PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| ,
which is assumed to be proportional to the length of the
list Lv : π( [ti, ti+1) ). Thus, in order to optimize the 
performance of processing queries we minimize their processing
costs.
6.1 Performance/Space-Optimal Approaches
One strategy to eliminate the problem of skipped 
postings is to eagerly materialize sublists for all elementary time
intervals, i.e., to choose M = E. In doing so, for every
query q t
only postings valid at time t are read and thus the
best possible performance is achieved. Therefore, we will
refer to this approach as Popt in the remainder. The initial
approach described above that keeps only the full list Lv
and thus picks M = { [t1, tn) } is referred to as Sopt in the
remainder. This approach requires minimal space, since it
keeps each posting exactly once.
Popt and Sopt are extremes: the former provides the best
possible performance but is not space-efficient, the latter
requires minimal space but does not provide good 
performance. The two approaches presented in the rest of this
section allow mutually trading off space and performance
and can thus be thought of as means to explore the 
configuration spectrum between the Popt and the Sopt approach.
6.2 Performance-Guarantee Approach
The Popt approach clearly wastes a lot of space 
materializing many nearly-identical sublists. In the example 
illustrated in Figure 2 materialized sublists for [t1, t2) and
[t2, t3) differ only by one posting. If the sublist for [t1, t3)
was materialized instead, one could save significant space
while incurring only an overhead of one skipped posting for
all t ∈ [t1, t3). The technique presented next is driven by
the idea that significant space savings over Popt are 
achievable, if an upper-bounded loss on the performance can be
tolerated, or to put it differently, if a performance 
guarantee relative to the optimum is to be retained. In detail,
the technique, which we refer to as PG (Performance 
Guarantee) in the remainder, finds a set M that has minimal
required space, but guarantees for any elementary time 
interval [ti, ti+1) (and thus for any query q t
with t ∈ [ti, ti+1))
that performance is worse than optimal by at most a factor
of γ ≥ 1. Formally, this problem can be stated as
argmin
M
S( M ) s.t.
∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| .
An optimal solution to the problem can be computed by
means of induction using the recurrence
C( [t1, tk+1) ) =
min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condition} ,
where C( [t1, tj) ) is the optimal cost (i.e., the space 
required) for the prefix subproblem
{ [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) }
and condition stands for
∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1)
⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)|
.
Intuitively, the recurrence states that an optimal solution
for [t1, tk+1) be combined from an optimal solution to a
prefix subproblem C( [t1, tj) ) and a time interval [tj, tk+1)
that can be materialized without violating the performance
guarantee.
Pseudocode of the algorithm is omitted for space reasons,
but can be found in the accompanying technical report [5].
The time complexity of the algorithm is in O(n2
) - for each
prefix subproblem the above recurrence must be evaluated,
which is possible in linear time if list sizes |L : [ti, tj)| are
precomputed. The space complexity is in O(n2
) - the cost
of keeping the precomputed sublist lengths and memoizing
optimal solutions to prefix subproblems.
6.3 Space-Bound Approach
So far we considered the problem of materializing sublists
that give a guarantee on performance while requiring 
minimal space. In many situations, though, the storage space is
at a premium and the aim would be to materialize a set of
sublists that optimizes expected performance while not 
exceeding a given space limit. The technique presented next,
which is named SB, tackles this very problem. The space
restriction is modeled by means of a user-specified 
parameter κ ≥ 1 that limits the maximum allowed blowup in index
size from the space-optimal solution provided by Sopt. The
SB technique seeks to find a set M that adheres to this
space limit but minimizes the expected processing cost (and
thus optimizes the expected performance). In the definition
of the expected processing cost, P( [ti, ti+1) ) denotes the
probability of a query time-point being in [ti, ti+1). 
Formally, this space-bound sublist-materialization problem can
be stated as
argmin
M
X
[ti, ti+1) ∈ E
P( [ti, ti+1) ) · PC( [ti, ti+1) ) s.t.
X
m∈M
|Lv : m| ≤ κ |Lv| .
The problem can be solved by using dynamic 
programming over an increasing number of time intervals: At each
time interval in E the algorithms decides whether to start a
new materialization time-interval, using the known best 
materialization decision from the previous time intervals, and
keeping track of the required space consumption for 
materialization. A detailed description of the algorithm is omitted
here, but can be found in the accompanying technical 
report [5]. Unfortunately, the algorithm has time complexity
in O(n3
|Lv|) and its space complexity is in O(n2
|Lv|), which
is not practical for large data sets.
We obtain an approximate solution to the problem 
using simulated annealing [22, 23]. Simulated annealing takes
a fixed number R of rounds to explore the solution space.
In each round a random successor of the current solution
is looked at. If the successor does not adhere to the space
limit, it is always rejected (i.e., the current solution is kept).
A successor adhering to the space limit is always accepted if
it achieves lower expected processing cost than the current
solution. If it achieves higher expected processing cost, it is
randomly accepted with probability e−∆/r
where ∆ is the
increase in expected processing cost and R ≥ r ≥ 1 denotes
the number of remaining rounds. In addition, throughout
all rounds, the method keeps track of the best solution seen
so far. The solution space for the problem at hand can be
efficiently explored. As we argued above, we solely have
to look at sets M that completely cover the time interval
[t1, tn) and do not contain overlapping time intervals. We
represent such a set M as an array of n boolean variables
b1 . . . bn that convey the boundaries of time intervals in the
set. Note that b1 and bn are always set to true. Initially,
all n − 2 intermediate variables assume false, which 
corresponds to the set M = { [t1, tn) }. A random successor
can now be easily generated by switching the value of one
of the n − 2 intermediate variables. The time complexity of
the method is in O(n2
) - the expected processing cost must
be computed in each round. Its space complexity is in O(n)
- for keeping the n boolean variables.
As a side remark note that for κ = 1.0 the SB method
does not necessarily produce the solution that is obtained
from Sopt, but may produce a solution that requires the
same amount of space while achieving better expected 
performance.
7. EXPERIMENTAL EVALUATION
We conducted a comprehensive series of experiments on
two real-world datasets to evaluate the techniques proposed
in this paper.
7.1 Setup and Datasets
The techniques described in this paper were implemented
in a prototype system using Java JDK 1.5. All 
experiments described below were run on a single SUN V40z 
machine having four AMD Opteron CPUs, 16GB RAM, a large
network-attached RAID-5 disk array, and running Microsoft
Windows Server 2003. All data and indexes are kept in an
Oracle 10g database that runs on the same machine. For
our experiments we used two different datasets.
The English Wikipedia revision history (referred to as
WIKI in the remainder) is available for free download as
a single XML file. This large dataset, totaling 0.7 TBytes,
contains the full editing history of the English Wikipedia
from January 2001 to December 2005 (the time of our 
download). We indexed all encyclopedia articles excluding 
versions that were marked as the result of a minor edit (e.g.,
the correction of spelling errors etc.). This yielded a total of
892,255 documents with 13,976,915 versions having a mean
(µ) of 15.67 versions per document at standard deviation (σ)
of 59.18. We built a time-travel query workload using the
query log temporarily made available recently by AOL 
Research as follows - we first extracted the 300 most frequent
keyword queries that yielded a result click on a Wikipedia 
article (for e.g., french revolution, hurricane season 2005,
da vinci code etc.). The thus extracted queries contained
a total of 422 distinct terms. For each extracted query, we
randomly picked a time point for each month covered by
the dataset. This resulted in a total of 18, 000 (= 300 × 60)
time-travel queries.
The second dataset used in our experiments was based
on a subset of the European Archive [13], containing weekly
crawls of 11 .gov.uk websites throughout the years 2004 and
2005 amounting close to 2 TBytes of raw data. We filtered
out documents not belonging to MIME-types text/plain
and text/html, to obtain a dataset that totals 0.4 TBytes
and which we refer to as UKGOV in rest of the paper. This
included a total of 502,617 documents with 8,687,108 
versions (µ = 17.28 and σ = 13.79). We built a corresponding
query workload as mentioned before, this time choosing 
keyword queries that led to a site in the .gov.uk domain (e.g.,
minimum wage, inheritance tax , citizenship ceremony
dates etc.), and randomly sampling a time point for every
month within the two year period spanned by the dataset.
Thus, we obtained a total of 7,200 (= 300 × 24) time-travel
queries for the UKGOV dataset. In total 522 terms appear
in the extracted queries.
The collection statistics (i.e., N and avdl) and term 
statistics (i.e., DF) were computed at monthly granularity for
both datasets.
7.2 Impact of Temporal Coalescing
Our first set of experiments is aimed at evaluating the 
approximate temporal coalescing technique, described in 
Section 5, in terms of index-size reduction and its effect on the
result quality. For both the WIKI and UKGOV datasets, we
compare temporally coalesced indexes for different values of
the error threshold computed using Algorithm 1 with the
non-coalesced index as a baseline.
WIKI UKGOV
# Postings Ratio # Postings Ratio
- 8,647,996,223 100.00% 7,888,560,482 100.00%
0.00 7,769,776,831 89.84% 2,926,731,708 37.10%
0.01 1,616,014,825 18.69% 744,438,831 9.44%
0.05 556,204,068 6.43% 259,947,199 3.30%
0.10 379,962,802 4.39% 187,387,342 2.38%
0.25 252,581,230 2.92% 158,107,198 2.00%
0.50 203,269,464 2.35% 155,434,617 1.97%
Table 1: Index sizes for non-coalesced index (-) and
coalesced indexes for different values of
Table 1 summarizes the index sizes measured as the total
number of postings. As these results demonstrate, 
approximate temporal coalescing is highly effective in reducing 
index size. Even a small threshold value, e.g. = 0.01, has a
considerable effect by reducing the index size almost by an
order of magnitude. Note that on the UKGOV dataset, even
accurate coalescing ( = 0) manages to reduce the index size
to less than 38% of the original size. Index size continues to
reduce on both datasets, as we increase the value of .
How does the reduction in index size affect the query 
results? In order to evaluate this aspect, we compared the
top-k results computed using a coalesced index against the
ground-truth result obtained from the original index, for 
different cutoff levels k. Let Gk and Ck be the top-k documents
from the ground-truth result and from the coalesced index
respectively. We used the following two measures for 
comparison: (i) Relative Recall at cutoff level k (RR@k), that
measures the overlap between Gk and Ck, which ranges in
[0, 1] and is defined as
RR@k = |Gk ∩ Ck|/k .
(ii) Kendall"s τ (see [7, 14] for a detailed definition) at 
cutoff level k (KT@k), measuring the agreement between two
results in the relative order of items in Gk ∩ Ck, with value
1 (or -1) indicating total agreement (or disagreement).
Figure 3 plots, for cutoff levels 10 and 100, the mean of
RR@k and KT@k along with 5% and 95% percentiles, for
different values of the threshold starting from 0.01. Note
that for = 0, results coincide with those obtained by the
original index, and hence are omitted from the graph.
It is reassuring to see from these results that approximate
temporal coalescing induces minimal disruption to the query
results, since RR@k and KT@k are within reasonable limits.
For = 0.01, the smallest value of in our experiments,
RR@100 for WIKI is 0.98 indicating that the results are
-1
-0.5
0
0.5
1
ε
0.01 0.05 0.10 0.25 0.50
Relative Recall @ 10 (WIKI)
Kendall's τ @ 10 (WIKI)
Relative Recall @ 10 (UKGOV)
Kendall's τ @ 10 (UKGOV)
(a) @10
-1
-0.5
0
0.5
1
ε
0.01 0.05 0.10 0.25 0.50
Relative Recall @ 100 (WIKI)
Kendall's τ @ 100 (WIKI)
Relative Recall @ 100 (UKGOV)
Kendall's τ @ 100 (UKGOV)
(b) @100
Figure 3: Relative recall and Kendall"s τ observed on coalesced indexes for different values of
almost indistinguishable from those obtained through the
original index. Even the relative order of these common
results is quite high, as the mean KT@100 is close to 0.95.
For the extreme value of = 0.5, which results in an index
size of just 2.35% of the original, the RR@100 and KT@100
are about 0.8 and 0.6 respectively. On the relatively less
dynamic UKGOV dataset (as can be seen from the σ values
above), results were even better, with high values of RR and
KT seen throughout the spectrum of values for both cutoff
values.
7.3 Sublist Materialization
We now turn our attention towards evaluating the 
sublist materialization techniques introduced in Section 6. For
both datasets, we started with the coalesced index produced
by a moderate threshold setting of = 0.10. In order to
reduce the computational effort, boundaries of elementary
time intervals were rounded to day granularity before 
computing the sublist materializations. However, note that the
postings in the materialized sublists still retain their 
original timestamps. For a comparative evaluation of the four
approaches - Popt, Sopt, PG, and SB - we measure space
and performance as follows. The required space S(M), as
defined earlier, is equal to the total number of postings in
the materialized sublists. To assess performance we 
compute the expected processing cost (EPC) for all terms in
the respective query workload assuming a uniform 
probability distribution among query time-points. We report the
mean EPC, as well as the 5%- and 95%-percentile. In other
words, the mean EPC reflects the expected length of the 
index list (in terms of index postings) that needs to be scanned
for a random time point and a random term from the query
workload.
The Sopt and Popt approaches are, by their definition,
parameter-free. For the PG approach, we varied its 
parameter γ, which limits the maximal performance degradation,
between 1.0 and 3.0. Analogously, for the SB approach
the parameter κ, as an upper-bound on the allowed space
blowup, was varied between 1.0 and 3.0. Solutions for the
SB approach were obtained running simulated annealing for
R = 50, 000 rounds.
Table 2 lists the obtained space and performance figures.
Note that EPC values are smaller on WIKI than on 
UKGOV, since terms in the query workload employed for WIKI
are relatively rarer in the corpus. Based on the depicted
results, we make the following key observations. i) As 
expected, Popt achieves optimal performance at the cost of an
enormous space consumption. Sopt, to the contrary, while
consuming an optimal amount of space, provides only poor
expected processing cost. The PG and SB methods, for
different values of their respective parameter, produce 
solutions whose space and performance lie in between the 
extremes that Popt and Sopt represent. ii) For the PG method
we see that for an acceptable performance degradation of
only 10% (i.e., γ = 1.10) the required space drops by more
than one order of magnitude in comparison to Popt on both
datasets. iii) The SB approach achieves close-to-optimal
performance on both datasets, if allowed to consume at most
three times the optimal amount of space (i.e., κ = 3.0),
which on our datasets still corresponds to a space reduction
over Popt by more than one order of magnitude.
We also measured wall-clock times on a sample of the
queries with results indicating improvements in execution
time by up to a factor of 12.
8. CONCLUSIONS
In this work we have developed an efficient solution for
time-travel text search over temporally versioned document
collections. Experiments on two real-world datasets showed
that a combination of the proposed techniques can reduce
index size by up to an order of magnitude while achieving
nearly optimal performance and highly accurate results.
The present work opens up many interesting questions
for future research, e.g.: How can we even further improve
performance by applying (and possibly extending) encoding,
compression, and skipping techniques [35]?. How can we
extend the approach for queries q [tb, te]
specifying a time
interval instead of a time point? How can the described
time-travel text search functionality enable or speed up text
mining along the time axis (e.g., tracking sentiment changes
in customer opinions)?
9. ACKNOWLEDGMENTS
We are grateful to the anonymous reviewers for their 
valuable comments - in particular to the reviewer who pointed
out the opportunity for algorithmic improvements in 
Section 5 and Section 6.2.
10. REFERENCES
[1] V. N. Anh and A. Moffat. Pruned Query Evaluation
Using Pre-Computed Impacts. In SIGIR, 2006.
[2] V. N. Anh and A. Moffat. Pruning Strategies for
Mixed-Mode Querying. In CIKM, 2006.
WIKI UKGOV
S(M) EPC S(M) EPC
5% Mean 95% 5% Mean 95%
Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86
Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85
PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99
PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00
PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89
PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48
PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31
PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95
PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94
SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58
SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33
SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38
SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44
SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02
SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28
SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 511,470,192 42.15 16,772.65 72,307.43
Table 2: Required space and expected processing cost (in # postings) observed on coalesced indexes ( = 0.10)
[3] P. G. Anick and R. A. Flynn. Versioning a Full-Text
Information Retrieval System. In SIGIR, 1992.
[4] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern
Information Retrieval. Addison-Wesley, 1999.
[5] K. Berberich, S. Bedathur, T. Neumann, and
G. Weikum. A Time Machine for Text search.
Technical Report MPI-I-2007-5-002, Max-Planck
Institute for Informatics, 2007.
[6] M. H. B¨ohlen, R. T. Snodgrass, and M. D. Soo.
Coalescing in Temporal Databases. In VLDB, 1996.
[7] P. Boldi, M. Santini, and S. Vigna. Do Your Worst to
Make the Best: Paradoxical Effects in PageRank
Incremental Computations. In WAW, 2004.
[8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici,
R. Lempel, J. McPherson, R. Qi, and E. J. Shekita.
Indexing Shared Content in Information Retrieval
Systems. In EDBT, 2006.
[9] C. Buckley and A. F. Lewit. Optimization of Inverted
Vector Searches. In SIGIR, 1985.
[10] M. Burrows and A. L. Hisgen. Method and Apparatus
for Generating and Searching Range-Based Index of
Word Locations. U.S. Patent 5,915,251, 1999.
[11] S. B¨uttcher and C. L. A. Clarke. A Document-Centric
Approach to Static Index Pruning in Text Retrieval
Systems. In CIKM, 2006.
[12] D. Carmel, D. Cohen, R. Fagin, E. Farchi,
M. Herscovici, Y. S. Maarek, and A. Soffer. Static
Index Pruning for Information Retrieval Systems. In
SIGIR, 2001.
[13] http://www.europarchive.org.
[14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing
Top k Lists. SIAM J. Discrete Math., 17(1):134-160,
2003.
[15] R. Fagin, A. Lotem, and M. Naor. Optimal
Aggregation Algorithms for Middleware. J. Comput.
Syst. Sci., 66(4):614-656, 2003.
[16] S. Guha, K. Shim, and J. Woo. REHIST: Relative
Error Histogram Construction Algorithms. In VLDB,
2004.
[17] M. Hersovici, R. Lempel, and S. Yogev. Efficient
Indexing of Versioned Document Sequences. In ECIR,
2007.
[18] http://www.archive.org.
[19] Y. E. Ioannidis and V. Poosala. Balancing Histogram
Optimality and Practicality for Query Result Size
Estimation. In SIGMOD, 1995.
[20] H. V. Jagadish, N. Koudas, S. Muthukrishnan,
V. Poosala, K. C. Sevcik, and T. Suel. Optimal
Histograms with Quality Guarantees. In VLDB, 1998.
[21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani. An
Online Algorithm for Segmenting Time Series. In
ICDM, 2001.
[22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi.
Optimization by Simulated Annealing. Science,
220(4598):671-680, 1983.
[23] J. Kleinberg and E. Tardos. Algorithm Design.
Addison-Wesley, 2005.
[24] U. Manber. Introduction to Algorithms: A Creative
Approach. Addison-Wesley, 1989.
[25] K. Nørv˚ag and A. O. N. Nybø. DyST: Dynamic and
Scalable Temporal Text Indexing. In TIME, 2006.
[26] J. M. Ponte and W. B. Croft. A Language Modeling
Approach to Information Retrieval. In SIGIR, 1998.
[27] S. E. Robertson and S. Walker. Okapi/Keenbow at
TREC-8. In TREC, 1999.
[28] B. Salzberg and V. J. Tsotras. Comparison of Access
Methods for Time-Evolving Data. ACM Comput.
Surv., 31(2):158-221, 1999.
[29] M. Stack. Full Text Search of Web Archive
Collections. In IWAW, 2006.
[30] E. Terzi and P. Tsaparas. Efficient Algorithms for
Sequence Segmentation. In SIAM-DM, 2006.
[31] M. Theobald, G. Weikum, and R. Schenkel. Top-k
Query Evaluation with Probabilistic Guarantees. In
VLDB, 2004.
[32] http://www.wikipedia.org.
[33] I. H. Witten, A. Moffat, and T. C. Bell. Managing
Gigabytes: Compressing and Indexing Documents and
Images. Morgan Kaufmann publishers Inc., 1999.
[34] J. Zhang and T. Suel. Efficient Search in Large
Textual Collections with Redundancy. In WWW,
2007.
[35] J. Zobel and A. Moffat. Inverted Files for Text Search
Engines. ACM Comput. Surv., 38(2):6, 2006.
Robustness of Adaptive Filtering Methods
In a Cross-benchmark Evaluation
Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel
School of Computer Science, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
ABSTRACT
This paper reports a cross-benchmark evaluation of regularized
logistic regression (LR) and incremental Rocchio for adaptive
filtering. Using four corpora from the Topic Detection and
Tracking (TDT) forum and the Text Retrieval Conferences
(TREC) we evaluated these methods with non-stationary topics
at various granularity levels, and measured performance with
different utility settings. We found that LR performs strongly
and robustly in optimizing T11SU (a TREC utility function)
while Rocchio is better for optimizing Ctrk (the TDT tracking
cost), a high-recall oriented objective function. Using systematic
cross-corpus parameter optimization with both methods, we
obtained the best results ever reported on TDT5, TREC10 and
TREC11. Relevance feedback on a small portion (0.05~0.2%)
of the TDT5 test documents yielded significant performance
improvements, measuring up to a 54% reduction in Ctrk and a
20.9% increase in T11SU (with β=0.1), compared to the results
of the top-performing system in TDT2004 without relevance
feedback information.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
filtering, Relevance feedback, Retrieval models, Selection
process; I.5.2 [Design Methodology]: Classifier design and
evaluation
General Terms
Algorithms, Measurement, Performance, Experimentation
1. INTRODUCTION
Adaptive filtering (AF) has been a challenging research topic in
information retrieval. The task is for the system to make an
online topic membership decision (yes or no) for every
document, as soon as it arrives, with respect to each pre-defined
topic of interest. Starting from 1997 in the Topic Detection and
Tracking (TDT) area and 1998 in the Text Retrieval
Conferences (TREC), benchmark evaluations have been
conducted by NIST under the following
conditions[6][7][8][3][4]:
• A very small number (1 to 4) of positive training examples
was provided for each topic at the starting point.
• Relevance feedback was available but only for the 
systemaccepted documents (with a yes decision) in the TREC
evaluations for AF.
• Relevance feedback (RF) was not allowed in the TDT
evaluations for AF (or topic tracking in the TDT
terminology) until 2004.
• TDT2004 was the first time that TREC and TDT metrics
were jointly used in evaluating AF methods on the same
benchmark (the TDT5 corpus) where non-stationary topics
dominate.
The above conditions attempt to mimic realistic situations where
an AF system would be used. That is, the user would be willing
to provide a few positive examples for each topic of interest at
the start, and might or might not be able to provide additional
labeling on a small portion of incoming documents through
relevance feedback. Furthermore, topics of interest might
change over time, with new topics appearing and growing, and
old topics shrinking and diminishing. These conditions make
adaptive filtering a difficult task in statistical learning (online
classification), for the following reasons:
1) it is difficult to learn accurate models for prediction based
on extremely sparse training data;
2) it is not obvious how to correct the sampling bias (i.e.,
relevance feedback on system-accepted documents only)
during the adaptation process;
3) it is not well understood how to effectively tune parameters
in AF methods using cross-corpus validation where the
validation and evaluation topics do not overlap, and the
documents may be from different sources or different
epochs.
None of these problems is addressed in the literature of
statistical learning for batch classification where all the training
data are given at once. The first two problems have been
studied in the adaptive filtering literature, including topic profile
adaptation using incremental Rocchio, Gaussian-Exponential
density models, logistic regression in a Bayesian framework,
etc., and threshold optimization strategies using probabilistic
calibration or local fitting techniques [1][2][9][10][11][12][13].
Although these works provide valuable insights for
understanding the problems and possible solutions, it is difficult
to draw conclusions regarding the effectiveness and robustness
of current methods because the third problem has not been
thoroughly investigated. Addressing the third issue is the main
focus in this paper.
We argue that robustness is an important measure for evaluating
and comparing AF methods. By robust we mean consistent
and strong performance across benchmark corpora with a
systematic method for parameter tuning across multiple corpora.
Most AF methods have pre-specified parameters that may
influence the performance significantly and that must be
determined before the test process starts. Available training
examples, on the other hand, are often insufficient for tuning the
parameters. In TDT5, for example, there is only one labeled
training example per topic at the start; parameter optimization
on such training data is doomed to be ineffective.
This leaves only one option (assuming tuning on the test set is
not an alternative), that is, choosing an external corpus as the
validation set. Notice that the validation-set topics often do not
overlap with the test-set topics, thus the parameter optimization
is performed under the tough condition that the validation data
and the test data may be quite different from each other. Now
the important question is: which methods (if any) are robust
under the condition of using cross-corpus validation to tune
parameters? Current literature does not offer an answer because
no thorough investigation on the robustness of AF methods has
been reported.
In this paper we address the above question by conducting a
cross-benchmark evaluation with two effective approaches in
AF: incremental Rocchio and regularized logistic regression
(LR). Rocchio-style classifiers have been popular in AF, with
good performance in benchmark evaluations (TREC and TDT)
if appropriate parameters were used and if combined with an
effective threshold calibration strategy [2][4][7][8][9][11][13].
Logistic regression is a classical method in statistical learning,
and one of the best in batch-mode text categorization [15][14]. It
was recently evaluated in adaptive filtering and was found to
have relatively strong performance (Section 5.1). Furthermore, a
recent paper [13] reported that the joint use of Rocchio and LR
in a Bayesian framework outperformed the results of using each
method alone on the TREC11 corpus. Stimulated by those
findings, we decided to include Rocchio and LR in our 
crossbenchmark evaluation for robustness testing. Specifically, we
focus on how much the performance of these methods depends
on parameter tuning, what the most influential parameters are in
these methods, how difficult (or how easy) to optimize these
influential parameters using cross-corpus validation, how strong
these methods perform on multiple benchmarks with the
systematic tuning of parameters on other corpora, and how
efficient these methods are in running AF on large benchmark
corpora.
The organization of the paper is as follows: Section 2 introduces
the four benchmark corpora (TREC10 and TREC11, TDT3 and
TDT5) used in this study. Section 3 analyzes the differences
among the TREC and TDT metrics (utilities and tracking cost)
and the potential implications of those differences. Section 4
outlines the Rocchio and LR approaches to AF, respectively.
Section 5 reports the experiments and results. Section 6
concludes the main findings in this study.
2. BENCHMARK CORPORA
We used four benchmark corpora in our study. Table 1 shows
the statistics about these data sets.
TREC10 was the evaluation benchmark for adaptive filtering in
TREC 2001, consisting of roughly 806,791 Reuters news stories
from August 1996 to August 1997 with 84 topic labels (subject
categories)[7]. The first two weeks (August 20th
to 31st
, 1996) of
documents is the training set, and the remaining 11 & ½ months
(from September 1st
, 1996 to August 19th
, 1997) is the test set.
TREC11 was the evaluation benchmark for adaptive filtering in
TREC 2002, consisting of the same set of documents as those in
TREC10 but with a slightly different splitting point for the
training and test sets. The TREC11 topics (50) are quite
different from those in TREC10; they are queries for retrieval
with relevance judgments by NIST assessors [8].
TDT3 was the evaluation benchmark in the TDT2001 dry run1
.
The tracking part of the corpus consists of 71,388 news stories
from multiple sources in English and Mandarin (AP, NYT,
CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America
and PRI the World) in the period of October to December 1998.
Machine-translated versions of the non-English stories (Xinhua,
Zaobao and VOA Mandarin) are provided as well. The splitting
point for training-test sets is different for each topic in TDT.
TDT5 was the evaluation benchmark in TDT2004 [4]. The
tracking part of the corpus consists of 407,459 news stories in
the period of April to September, 2003 from 15 news agents or
broadcast sources in English, Arabic and Mandarin, with
machine-translated versions of the non-English stories. We only
used the English versions of those documents in our
experiments for this paper.
The TDT topics differ from TREC topics both conceptually
and statistically. Instead of generic, ever-lasting subject
categories (as those in TREC), TDT topics are defined at a finer
level of granularity, for events that happen at certain times and
locations, and that are born and die, typically associated
with a bursty distribution over chronologically ordered news
stories. The average size of TDT topics (events) is two orders
of magnitude smaller than that of the TREC10 topics. Figure 1
compares the document densities of a TREC topic (Civil
Wars) and two TDT topics (Gunshot and APEC Summit
Meeting, respectively) over a 3-month time period, where the
area under each curve is normalized to one.
The granularity differences among topics and the corresponding
non-stationary distributions make the cross-benchmark
evaluation interesting. For example, algorithms favoring large
and stable topics may not work well for short-lasting and 
nonstationary topics, and vice versa. Cross-benchmark evaluations
allow us to test this hypothesis and possibly identify the
weaknesses in current approaches to adaptive filtering in
tracking the drifting trends of topics.
1
http://www.ldc.upenn.edu/Projects/TDT2001/topics.html
Table 1: Statistics of benchmark corpora for adaptive filtering evaluations
N(tr) is the number of the initial training documents; N(ts) is the number of the test documents;
n+ is the number of positive examples of a predefined topic; * is an average over all the topics.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
Week
P(topic|week)
Gunshot (TDT5)
APEC Summit Meeting (TDT3)
Civil War(TREC10)
Figure 1: The temporal nature of topics
3. METRICS
To make our results comparable to the literature, we decided to
use both TREC-conventional and TDT-conventional metrics in
our evaluation.
3.1 TREC11 metrics
Let A, B, C and D be, respectively, the numbers of true
positives, false alarms, misses and true negatives for a specific
topic, and DCBAN +++= be the total number of test
documents. The TREC-conventional metrics are defined as:
Precision )/( BAA += , Recall )/( CAA +=
)(2
)21(
CABA
A
F
+++
+
=
β
β
β
( )
η
ηηβ
ηβ
−
−+−
=
1
),/()(max
11 ,
CABA
SUT
where parameters β and η were set to 0.5 and -0.5 respectively
in TREC10 (2001) and TREC11 (2002). For evaluating the
performance of a system, the performance scores are computed
for individual topics first and then averaged over topics 
(macroaveraging).
3.2 TDT metrics
The TDT-conventional metric for topic tracking is defined as:
famisstrk PTPwPTPwTC ))(1()()( 21 −+=
where P(T) is the percentage of documents on topic T, missP is
the miss rate by the system on that topic, faP is the false alarm
rate, and 1w and 2w are the costs (pre-specified constants) for a
miss and a false alarm, respectively. The TDT benchmark
evaluations (since 1997) have used the settings
of 11 =w , 1.02 =w and 02.0)( =TP for all topics. For evaluating
the performance of a system, Ctrk is computed for each topic
first and then the resulting scores are averaged for a single
measure (the topic-weighted Ctrk).
To make the intuition behind this measure transparent, we
substitute the terms in the definition of Ctrk as follows:
N
CA
TP
+
=)( ,
N
DB
TP
+
=− )(1 ,
CA
C
Pmiss
+
= ,
DB
B
Pfa
+
= ,
)(
1
)(
21
21
BwCw
N
DB
B
N
DB
w
CA
C
N
CA
wTCtrk
+⋅=
+
⋅
+
⋅+
+
⋅
+
⋅=
Clearly, trkC is the average cost per error on topic T, with 1w
and 2w controlling the penalty ratio for misses vs. false alarms.
In addition to trkC , TDT2004 also employed 1.011 =βSUT as a
utility metric. To distinguish this from the 5.011 =βSUT in
TREC11, we call former TDT5SU in the rest of this paper.
Corpus #Topics N(tr) N(ts) Avg
n+ (tr)
Avg
n+ (ts)
Max
n+ (ts)
Min
n+ (ts)
#Topics per
doc (ts)
TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57
TREC11 50 80.664 726,419 3 378.0 597 198 1.12
TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06
TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01
3.3 The correlations and the differences
From an optimization point of view, TDT5SU and T11SU are
both utility functions while Ctrk is a cost function. Our objective
is to maximize the former or to minimize the latter on test
documents. The differences and correlations among these
objective functions can be analyzed through the shared counts
of A, B, C and D in their definitions. For example, both
TDT5SU and T11SU are positively correlated to the values of A
and D, and negatively correlated to the values of B and C; the
only difference between them is in their penalty ratios for
misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU.
The Ctrk function, on the other hand, is positively correlated to
the values of C and B, and negatively correlated to the values of
A and D; hence, it is negatively correlated to T11SU and
TDT5SU.
More importantly, there is a subtle and major difference
between Ctrk and the utility functions: T11SU and TDT5SU.
That is, Ctrk has a very different penalty ratio for misses vs.
false alarms: it favors recall-oriented systems to an extreme. At
first glance, one would think that the penalty ratio in Ctrk is
10:1 since 11 =w and 1.02 =w . However, this is not true if
02.0)( =TP is an inaccurate estimate of the on-topic documents
on average for the test corpus. Using TDT3 as an example, the
true percentage is:
002.0
37770
3.79
)( ≈=
+
=
N
n
TP
where N is the average size of the test sets in TDT3, and n+ is
the average number of positive examples per topic in the test
sets. Using 02.0)(ˆ =TP as an (inaccurate) estimate of 0.002
enlarges the intended penalty ratio of 10:1 to 100:1, roughly
speaking. To wit:
)1.010(
1
1.010
)3.7937770(37770
3.79
1011.0101
1011.0101
))(1(2)(1
)02.01(202.01)(
BC
NN
B
N
C
B
N
C
DB
B
N
CA
N
C
faPTPwmissPTPw
faPwmissPwTtrkC
×+×=×+×≈
−
×−×+××=
+
+
×−×+××=
×−⋅+××=
−×+××=
⎟
⎠
⎞
⎜
⎝
⎛
⎟
⎠
⎞
⎜
⎝
⎛
ρρ
where 10
002.0
02.0
)(
)(ˆ
===
TP
TP
ρ is the factor of enlargement in the
estimation of P(T) compared to the truth. Comparing the above
result to formula 2, we can see the actual penalty ratio for
misses vs. false alarms was 100:1 in the evaluations on TDT3
using Ctrk. Similarly, we can compute the enlargement factor
for TDT5 using the statistics in Table 1 as follows:
3.58
991,207/3.71
02.0
)(
)(ˆ
===
TP
TP
ρ
which means the actual penalty ratio for misses vs. false alarms
in the evaluation on TDT5 using Ctrk was approximately 583:1.
The implications of the above analysis are rather significant:
• Ctrk defined in the same formula does not necessarily
mean the same objective function in evaluation; instead,
the optimization criterion depends on the test corpus.
• Systems optimized for Ctrk would not optimize TDT5SU
(and T11SU) because the former favors high-recall
oriented to an extreme while the latter does not.
• Parameters tuned on one corpus (e.g., TDT3) might not
work for an evaluation on another corpus (say, TDT5)
unless we account for the previously-unknown subtle
dependency of Ctrk on data.
• Results in Ctrk in the past years of TDT evaluations may
not be directly comparable to each other because the
evaluation collections changed most years and hence the
penalty ratio in Ctrk varied.
Although these problems with Ctrk were not originally
anticipated, it offered an opportunity to examine the ability of
systems in trading off precision for extreme recall. This was a
challenging part of the TDT2004 evaluation for AF.
Comparing the metrics in TDT and TREC from a utility or cost
optimization point of view is important for understanding the
evaluation results of adaptive filtering methods. This is the first
time this issue is explicitly analyzed, to our knowledge.
4. METHODS
4.1 Incremental Rocchio for AF
We employed a common version of Rocchio-style classifiers
which computes a prototype vector per topic (T) as follows:
|)(|
'
|)(|
)()(
)(')(
TD
d
TD
d
TqTp
TDdTDd
−
∈
+
∈ ∑∑ −+
−+=
rr
rr
rr
γβα
The first term on the RHS is the weighted vector representation
of topic description whose elements are terms weights. The
second term is the weighted centroid of the set )(TD+ of
positive training examples, each of which is a vector of 
withindocument term weights. The third term is the weighted centroid
of the set )(TD− of negative training examples which are the
nearest neighbors of the positive centroid. The three terms are
given pre-specified weights of βα, and γ , controlling the
relative influence of these components in the prototype.
The prototype of a topic is updated each time the system makes
a yes decision on a new document for that topic. If relevance
feedback is available (as is the case in TREC adaptive filtering),
the new document is added to the pool of
either )(TD+ or )(TD− , and the prototype is recomputed
accordingly; if relevance feedback is not available (as is the case
in TDT event tracking), the system"s prediction (yes) is
treated as the truth, and the new document is added to )(TD+ for
updating the prototype. Both cases are part of our experiments
in this paper (and part of the TDT 2004 evaluations for AF). To
distinguish the two, we call the first case simply Rocchio and
the second case PRF Rocchio where PRF stands for 
pseudorelevance feedback.
The predictions on a new document are made by computing the
cosine similarity between each topic prototype and the
document vector, and then comparing the resulting scores
against a threshold:
⎩
⎨
⎧
−
+
=−
)(
)(
))),((cos(
no
yes
dTpsign new θ
rr
Threshold calibration in incremental Rocchio is a challenging
research topic. Multiple approaches have been developed. The
simplest is to use a universal threshold for all topics, tuned on a
validation set and fixed during the testing phase. More elaborate
methods include probabilistic threshold calibration which
converts the non-probabilistic similarity scores to probabilities
(i.e., )|( dTP
r
) for utility optimization [9][13], and margin-based
local regression for risk reduction [11].
It is beyond the scope of this paper to compare all the different
ways to adapt Rocchio-style methods for AF. Instead, our focus
here is to investigate the robustness of Rocchio-style methods in
terms of how much their performance depends on elaborate
system tuning, and how difficult (or how easy) it is to get good
performance through cross-corpus parameter optimization.
Hence, we decided to use a relatively simple version of Rocchio
as the baseline, i.e., with a universal threshold tuned on a
validation corpus and fixed for all topics in the testing phase.
This simple version of Rocchio has been commonly used in the
past TDT benchmark evaluations for topic tracking, and had
strong performance in the TDT2004 evaluations for adaptive
filtering with and without relevance feedback (Section 5.1).
Results of more complex variants of Rocchio are also discussed
when relevant.
4.2 Logistic Regression for AF
Logistic regression (LR) estimates the posterior probability of a
topic given a document using a sigmoid function
)1/(1),|1( xw
ewxyP
rrrr ⋅−
+==
where x
r
is the document vector whose elements are term
weights, w
r
is the vector of regression coefficients, and
}1,1{ −+∈y is the output variable corresponding to yes or
no with respect to a particular topic. Given a training set of
labeled documents { }),(,),,( 11 nn yxyxD
r
L
r
= , the
standard regression problem is defined as to find the maximum
likelihood estimates of the regression coefficients (the model
parameters):
{ } { }
{ }))exp(1(1logminarg
)|(logmaxarg)|(maxarg
ii xwyn
i
w
wDP
w
wDP
w
mlw
rr
r
r
r
r
r
r
⋅−+∑ ==
==
This is a convex optimization problem which can be solved
using a standard conjugate gradient algorithm in O(INF) time
for training per topic, where I is the average number of
iterations needed for convergence, and N and F are the number
of training documents and number of features respectively [14].
Once the regression coefficients are optimized on the training
data, the filtering prediction on each incoming document is
made as:
( )
⎩
⎨
⎧
−
+
=−
)(
)(
),|(
no
yes
wxyPsign optnew θ
rr
Note that w
r
is constantly updated whenever a new relevance
judgment is available in the testing phase of AF, while the
optimal threshold optθ is constant, depending only on the 
predefined utility (or cost) function for evaluation. If T11SU is the
metric, for example, with the penalty ratio of 2:1 for misses and
false alarms (Section 3.1), the optimal threshold for LR
is 33.0)12/(1 =+ for all topics.
We modified the standard (above) version of LR to allow more
flexible optimization criteria as follows:
⎭
⎬
⎫
⎩
⎨
⎧
−++= ∑=
⋅− 2
1
)1log()(minarg μλ
rrr rr
r
weysw
n
i
xwy
i
w
map
ii
where )( iys is taken to be α , β and γ for query, positive
and negative documents respectively, which are similar to those
in Rocchio, giving different weights to the three kinds of
training examples: topic descriptions (queries), on-topic
documents and off-topic documents. The second term in the
objective function is for regularization, equivalent to adding a
Gaussian prior to the regression coefficients with mean μ
r
and
covariance variance matrix Ι⋅λ2/1 , where Ι is the identity
matrix. Tuning λ (≥0) is theoretically justified for reducing
model complexity (the effective degree of freedom) and
avoiding over-fitting on training data [5]. How to find an
effective μ
r
is an open issue for research, depending on the
user"s belief about the parameter space and the optimal range.
The solution of the modified objective function is called the
Maximum A Posteriori (MAP) estimate, which reduces to the
maximum likelihood solution for standard LR if 0=λ .
5. EVALUATIONS
We report our empirical findings in four parts: the TDT2004
official evaluation results, the cross-corpus parameter
optimization results, and the results corresponding to the
amounts of relevance feedback.
5.1 TDT2004 benchmark results
The TDT2004 evaluations for adaptive filtering were conducted
by NIST in November 2004. Multiple research teams
participated and multiple runs from each team were allowed.
Ctrk and TDT5SU were used as the metrics. Figure 2 and Figure
3 show the results; the best run from each team was selected
with respect to Ctrk or TDT5SU, respectively. Our Rocchio
(with adaptive profiles but fixed universal threshold for all
topics) had the best result in Ctrk, and our logistic regression
had the best result in TDT5SU. All the parameters of our runs
were tuned on the TDT3 corpus. Results for other sites are also
listed anonymously for comparison.
Ctrk
Ours 0.0324
Site2 0.0467
Site3 0.1366
Site4 0.2438
Metric = Ctrk (the lower the better)
0.0324 0.0467
0.1366
0.2438
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Ours Site2 Site3 Site4
Figure 2: TDT2004 results in Ctrk of systems using true
relevance feedback. (Ours is the Rocchio method.) We
also put the 1st
and 3rd
quartiles as sticks for each site.2
T11SU
Ours 0.7328
Site3 0.7281
Site2 0.6672
Site4 0.382
Metric = TDT5SU (the higher the better)
0.7328 0.7281
0.6672
0.382
0
0.2
0.4
0.6
0.8
1
Ours Site3 Site2 Site4
Figure 3:TDT2004 results in TDT5SU of systems using true
relevance feedback. (Ours is LR
with 0=μ
r
and 005.0=λ ).
CTrk
Ours 0.0707
Site2 0.1545
Site5 0.5669
Site4 0.6507
Site6 0.8973
Primary Topic Traking Results in TDT2004
0.0707
0.8973
0.6507
0.1545
0.5669
0
0.2
0.4
0.6
0.8
1
1.2
Ours Site2 Site5 Site4 Site6
Ctrk
Figure 4:TDT2004 results in Ctrk of systems without using
true relevance feedback. (Ours is PRF Rocchio.)
Adaptive filtering without using true relevance feedback was
also a part of the evaluations. In this case, systems had only one
labeled training example per topic during the entire training and
testing processes, although unlabeled test documents could be
used as soon as predictions on them were made. Such a setting
has been conventional for the Topic Tracking task in TDT until
2004. Figure 4 shows the summarized official submissions from
each team. Our PRF Rocchio (with a fixed threshold for all the
topics) had the best performance.
2
We use quartiles rather than standard deviations since the
former is more resistant to outliers.
5.2 Cross-corpus parameter optimization
How much the strong performance of our systems depends on
parameter tuning is an important question.
Both Rocchio and LR have parameters that must be 
prespecified before the AF process. The shared parameters include
the sample weightsα , β and γ , the sample size of the negative
training documents (i.e., )(TD− ), the term-weighting scheme,
and the maximal number of non-zero elements in each
document vector. The method-specific parameters include the
decision threshold in Rocchio, and μ
r
, λ and MI (the maximum
number of iterations in training) in LR. Given that we only have
one labeled example per topic in the TDT5 training sets, it is
impossible to effectively optimize these parameters on the
training data, and we had to choose an external corpus for
validation. Among the choices of TREC10, TREC11 and TDT3,
we chose TDT3 (c.f. Section 2) because it is most similar to
TDT5 in terms of the nature of the topics (Section 2). We
optimized the parameters of our systems on TDT3, and fixed
those parameters in the runs on TDT5 for our submissions to
TDT2004. We also tested our methods on TREC10 and
TREC11 for further analysis. Since exhaustive testing of all
possible parameter settings is computationally intractable, we
followed a step-wise forward chaining procedure instead: we
pre-specified an order of the parameters in a method (Rocchio
or LR), and then tuned one parameter at the time while fixing
the settings of the remaining parameters. We repeated this
procedure for several passes as time allowed.
0.05
0.26
0.67
0.69
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3
Threshold
TDT5SU
TDT3 TDT5 TREC10 TREC11
Figure 5: Performance curves of adaptive Rocchio
Figure 5 compares the performance curves in TDT5SU for
Rocchio on TDT3, TDT5, TREC10 and TREC11 when the
decision threshold varied. These curves peak at different
locations: the TDT3-optimal is closest to the TDT5-optimal
while the TREC10-optimal and TREC1-optimal are quite far
away from the TDT5-optimal. If we were using TREC10 or
TREC11 instead of TDT3 as the validation corpus for TDT5, or
if the TDT3 corpus were not available, we would have difficulty
in obtaining strong performance for Rocchio in TDT2004. The
difficulty comes from the ad-hoc (non-probabilistic) scores
generated by the Rocchio method: the distribution of the scores
depends on the corpus, making cross-corpus threshold
optimization a tricky problem.
Logistic regression has less difficulty with respect to threshold
tuning because it produces probabilistic scores of )|1Pr( xy =
upon which the optimal threshold can be directly computed if
probability estimation is accurate. Given the penalty ratio for
misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and
583:1 in Ctrk (Section 3.3), the corresponding optimal
thresholds (t) are 0.33, 0.091 and 0.0017 respectively.
Although the theoretical threshold could be inaccurate, it still
suggests the range of near-optimal settings. With these threshold
settings in our experiments for LR, we focused on the 
crosscorpus validation of the Bayesian prior parameters, that is, μ
r
and λ. Table 2 summarizes the results 3
. We measured the
performance of the runs on TREC10 and TREC11 using T11SU,
and the performance of the runs on TDT3 and TDT5 using
TDT5SU. For comparison we also include the best results of
Rocchio-based methods on these corpora, which are our own
results of Rocchio on TDT3 and TDT5, and the best results
reported by NIST for TREC10 and TREC11. From this set of
results, we see that LR significantly outperformed Rocchio on
all the corpora, even in the runs of standard LR without any
tuning, i.e. λ=0. This empirical finding is consistent with a
previous report [13] for LR on TREC11 although our results of
LR (0.585~0.608 in T11SU) are stronger than the results (0.49
for standard LR and 0.54 for LR using Rocchio prototype as the
prior) in that report. More importantly, our cross-benchmark
evaluation gives strong evidence for the robustness of LR. The
robustness, we believe, comes from the probabilistic nature of
the system-generated scores. That is, compared to the ad-hoc
scores in Rocchio, the normalized posterior probabilities make
the threshold optimization in LR a much easier problem.
Moreover, logistic regression is known to converge towards the
Bayes classifier asymptotically while Rocchio classifiers"
parameters do not.
Another interesting observation in these results is that the
performance of LR did not improve when using a Rocchio
prototype as the mean in the prior; instead, the performance
decreased in some cases. This observation does not support the
previous report by [13], but we are not surprised because we are
not convinced that Rocchio prototypes are more accurate than
LR models for topics in the early stage of the AF process, and
we believe that using a Rocchio prototype as the mean in the
Gaussian prior would introduce undesirable bias to LR. We also
believe that variance reduction (in the testing phase) should be
controlled by the choice of λ (but not μ
r
), for which we
conducted the experiments as shown in Figure 6.
Table 2: Results of LR with different Bayesian priors
Corpus TDT3 TDT5 TREC10 TREC11
LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715
LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747
LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698
Best Rocchio 0.6628 0.6917 0.4964
0.475
3
The LR results (0.77~0.78) on TDT5 in this table are better
than our TDT2004 official result (0.73) because parameter
optimization has been improved afterwards.
4
The TREC10-best result (0.496 by Oracle) is only available in
T10U which is not directly comparable to the scores in
T11SU, just indicative.
*: μ
r
was set to the Rocchio prototype
0
0.2
0.4
0.6
0.8
0.000 0.001 0.005 0.050 0.500
Lambda
Performance
Ctrk on TDT3 TDT5SU on TDT3
TDT5SU on TDT5 T11SU on TREC11
Figure 6: LR with varying lambda.
The performance of LR is summarized with respect to λ tuning
on the corpora of TREC10, TREC11 and TDT3. The
performance on each corpus was measured using the
corresponding metrics, that is, T11SU for the runs on TREC10
and TREC11, and TDT5SU and Ctrk for the runs on TDT3,. In
the case of maximizing the utilities, the safe interval for λ is
between 0 and 0.01, meaning that the performance of
regularized LR is stable, the same as or improved slightly over
the performance of standard LR. In the case of minimizing Ctrk,
the safe range for λ is between 0 and 0.1, and setting λ between
0.005 and 0.05 yielded relatively large improvements over the
performance of standard LR because training a model for
extremely high recall is statistically more tricky, and hence
more regularization is needed. In either case, tuning λ is
relatively safe, and easy to do successfully by cross-corpus
tuning.
Another influential choice in our experiment settings is term
weighting: we examined the choices of binary, TF and TF-IDF
(the ltc version) schemes. We found TF-IDF most effective
for both Rocchio and LR, and used this setting in all our
experiments.
5.3 Percentages of labeled data
How much relevance feedback (RF) would be needed during the
AF process is a meaningful question in real-world applications.
To answer it, we evaluated Rocchio and LR on TDT with the
following settings:
• Basic Rocchio, no adaptation at all
• PRF Rocchio, updating topic profiles without using true
relevance feedback;
• Adaptive Rocchio, updating topic profiles using relevance
feedback on system-accepted documents plus 10
documents randomly sampled from the pool of 
systemrejected documents;
• LR with 0
rr
=μ , 01.0=λ and threshold = 0.004;
• All the parameters in Rocchio tuned on TDT3.
Table 3 summarizes the results in Ctrk: Adaptive Rocchio with
relevance feedback on 0.6% of the test documents reduced the
tracking cost by 54% over the result of the PRF Rocchio, the
best system in the TDT2004 evaluation for topic tracking
without relevance feedback information. Incremental LR, on the
other hand, was weaker but still impressive. Recall that Ctrk is
an extremely high-recall oriented metric, causing frequent
updating of profiles and hence an efficiency problem in LR. For
this reason we set a higher threshold (0.004) instead of the
theoretically optimal threshold (0.0017) in LR to avoid an
untolerable computation cost. The computation time in
machine-hours was 0.33 for the run of adaptive Rocchio and 14
for the run of LR on TDT5 when optimizing Ctrk. Table 4
summarizes the results in TDT5SU; adaptive LR was the winner
in this case, with relevance feedback on 0.05% of the test
documents improving the utility by 20.9% over the results of
PRF Rocchio.
Table 3: AF methods on TDT5 (Performance in Ctrk)
Base Roc PRF Roc Adp Roc LR
% of RF 0% 0% 0.6% 0.2%
Ctrk 0.076 0.0707 0.0324 0.0382
±% +7% (baseline) -54% -46%
Table 4: AF methods on TDT5 (Performance in TDT5SU)
Base Roc PRF Roc Adp Roc LR(λ=.01)
% of RF 0% 0% 0.04% 0.05%
TDT5SU 0.57 0.6452 0.69 0.78
±% -11.7% (baseline) +6.9% +20.9%
Evidently, both Rocchio and LR are highly effective in adaptive
filtering, in terms of using of a small amount of labeled data to
significantly improve the model accuracy in statistical learning,
which is the main goal of AF.
5.4 Summary of Adaptation Process
After we decided the parameter settings using validation, we
perform the adaptive filtering in the following steps for each
topic: 1) Train the LR/Rocchio model using the provided
positive training examples and 30 randomly sampled negative
examples; 2) For each document in the test corpus: we first
make a prediction about relevance, and then get relevance
feedback for those (predicted) positive documents. 3) Model and
IDF statistics will be incrementally updated if we obtain its true
relevance feedback.
6. CONCLUDING REMARKS
We presented a cross-benchmark evaluation of incremental
Rocchio and incremental LR in adaptive filtering, focusing on
their robustness in terms of performance consistency with
respect to cross-corpus parameter optimization. Our main
conclusions from this study are the following:
• Parameter optimization in AF is an open challenge but has
not been thoroughly studied in the past.
• Robustness in cross-corpus parameter tuning is important
for evaluation and method comparison.
• We found LR more robust than Rocchio; it had the best
results (in T11SU) ever reported on TDT5, TREC10 and
TREC11 without extensive tuning.
• We found Rocchio performs strongly when a good
validation corpus is available, and a preferred choice when
optimizing Ctrk is the objective, favoring recall over
precision to an extreme.
For future research we want to study explicit modeling of the
temporal trends in topic distributions and content drifting.
Acknowledgments
This material is based upon work supported in parts by the
National Science Foundation (NSF) under grant IIS-0434035,
by the DoD under award 114008-N66001992891808 and by the
Defense Advanced Research Project Agency (DARPA) under
Contract No. NBCHD030010. Any opinions, findings and
conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of
the sponsors.
7. REFERENCES
[1] J. Allan. Incremental relevance feedback for information
filtering. In SIGIR-96, 1996.
[2] J. Callan. Learning while filtering documents. In SIGIR-98,
224-231, 1998.
[3] J. Fiscus and G. Duddington. Topic detection and tracking
overview. In Topic detection and tracking: event-based
information organization, 17-31, 2002.
[4] J. Fiscus and B. Wheatley. Overview of the TDT 2004
Evaluation and Results. In TDT-04, 2004.
[5] T. Hastie, R. Tibshirani and J. Friedman. Elements of
Statistical Learning. Springer, 2001.
[6] S. Robertson and D. Hull. The TREC-9 filtering track final
report. In TREC-9, 2000.
[7] S. Robertson and I. Soboroff. The TREC-10 filtering track
final report. In TREC-10, 2001.
[8] S. Robertson and I. Soboroff. The TREC 2002 filtering
track report. In TREC-11, 2002.
[9] S. Robertson and S. Walker. Microsoft Cambridge at
TREC-9. In TREC-9, 2000.
[10] R. Schapire, Y. Singer and A. Singhal. Boosting and
Rocchio applied to text filtering. In SIGIR-98, 215-223,
1998.
[11] Y. Yang and B. Kisiel. Margin-based local regression for
adaptive filtering. In CIKM-03, 2003.
[12] Y. Zhang and J. Callan. Maximum likelihood estimation
for filtering thresholds. In SIGIR-01, 2001.
[13] Y. Zhang. Using Bayesian priors to combine classifiers for
adaptive filtering. In SIGIR-04, 2004.
[14] J. Zhang and Y. Yang. Robustness of regularized linear
classification methods in text categorization. In SIGIR-03:
190-197, 2003.
[15] T. Zhang, F. J. Oles. Text Categorization Based on
Regularized Linear Classification Methods. Inf. Retr. 4(1):
5-31 (2001).
Combining Content and Link for Classification
using Matrix Factorization
Shenghuo Zhu Kai Yu Yun Chi Yihong Gong
{zsh,kyu,ychi,ygong}@sv.nec-labs.com
NEC Laboratories America, Inc.
10080 North Wolfe Road SW3-350
Cupertino, CA 95014, USA
ABSTRACT
The world wide web contains rich textual contents that are 
interconnected via complex hyperlinks. This huge database violates the 
assumption held by most of conventional statistical methods that each
web page is considered as an independent and identical sample. It
is thus difficult to apply traditional mining or learning methods for
solving web mining problems, e.g., web page classification, by 
exploiting both the content and the link structure. The research in this
direction has recently received considerable attention but are still in
an early stage. Though a few methods exploit both the link 
structure or the content information, some of them combine the only
authority information with the content information, and the others
first decompose the link structure into hub and authority features,
then apply them as additional document features. Being practically
attractive for its great simplicity, this paper aims to design an 
algorithm that exploits both the content and linkage information, by 
carrying out a joint factorization on both the linkage adjacency matrix
and the document-term matrix, and derives a new representation
for web pages in a low-dimensional factor space, without explicitly
separating them as content, hub or authority factors. Further 
analysis can be performed based on the compact representation of web
pages. In the experiments, the proposed method is compared with
state-of-the-art methods and demonstrates an excellent accuracy in
hypertext classification on the WebKB and Cora benchmarks.
Categories and Subject Descriptors: H.3.3 [Information 
Systems]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
1. INTRODUCTION
With the advance of the World Wide Web, more and more 
hypertext documents become available on the Web. Some examples of
such data include organizational and personal web pages (e.g, the
WebKB benchmark data set, which contains university web pages),
research papers (e.g., data in CiteSeer), online news articles, and
customer-generated media (e.g., blogs). Comparing to data in 
traditional information management, in addition to content, these data
on the Web also contain links: e.g., hyperlinks from a student"s
homepage pointing to the homepage of her advisor, paper citations,
sources of a news article, comments of one blogger on posts from
another blogger, and so on. Performing information management
tasks on such structured data raises many new research challenges.
In the following discussion, we use the task of web page 
classification as an illustrating example, while the techniques we develop
in later sections are applicable equally well to many other tasks in
information retrieval and data mining.
For the classification problem of web pages, a simple approach
is to treat web pages as independent documents. The advantage
of this approach is that many off-the-shelf classification tools can
be directly applied to the problem. However, this approach 
relies only on the content of web pages and ignores the structure of
links among them. Link structures provide invaluable information
about properties of the documents as well as relationships among
them. For example, in the WebKB dataset, the link structure 
provides additional insights about the relationship among documents
(e.g., links often pointing from a student to her advisor or from
a faculty member to his projects). Since some links among these
documents imply the inter-dependence among the documents, the
usual i.i.d. (independent and identical distributed) assumption of
documents does not hold any more. From this point of view, the
traditional classification methods that ignore the link structure may
not be suitable.
On the other hand, a few studies, for example [25], rely solely on
link structures. It is however a very rare case that content 
information can be ignorable. For example, in the Cora dataset, the content
of a research article abstract largely determines the category of the
article.
To improve the performance of web page classification, 
therefore, both link structure and content information should be taken
into consideration. To achieve this goal, a simple approach is to
convert one type of information to the other. For example, in spam
blog classification, Kolari et al. [13] concatenate outlink features
with the content features of the blog. In document classification,
Kurland and Lee [14] convert content similarity among documents
into weights of links. However, link and content information have
different properties. For example, a link is an actual piece of 
evidence that represents an asymmetric relationship whereas the 
content similarity is usually defined conceptually for every pair of 
documents in a symmetric way. Therefore, directly converting one type
of information to the other usually degrades the quality of 
information. On the other hand, there exist some studies, as we will discuss
in detail in related work, that consider link information and content
information separately and then combine them. We argue that such
an approach ignores the inherent consistency between link and 
content information and therefore fails to combine the two seamlessly.
Some work, such as [3], incorporates link information using 
cocitation similarity, but this may not fully capture the global link
structure. In Figure 1, for example, web pages v6 and v7 co-cite
web page v8, implying that v6 and v7 are similar to each other.
In turns, v4 and v5 should be similar to each other, since v4 and
v5 cite similar web pages v6 and v7, respectively. But using 
cocitation similarity, the similarity between v4 and v5 is zero without
considering other information.
v1
v2
v3
v4
v5
v6
v7
v8
Figure 1: An example of link structure
In this paper, we propose a simple technique for analyzing
inter-connected documents, such as web pages, using factor 
analysis[18]. In the proposed technique, both content information and
link structures are seamlessly combined through a single set of 
latent factors. Our model contains two components. The first 
component captures the content information. This component has a form
similar to that of the latent topics in the Latent Semantic Indexing
(LSI) [8] in traditional information retrieval. That is, documents
are decomposed into latent topics/factors, which in turn are 
represented as term vectors. The second component captures the 
information contained in the underlying link structure, such as links
from homepages of students to those of faculty members. A 
factor can be loosely considered as a type of documents (e.g., those
homepages belonging to students). It is worth noting that we do
not explicitly define the semantic of a factor a priori. Instead, 
similar to LSI, the factors are learned from the data. Traditional factor
analysis models the variables associated with entities through the
factors. However, in analysis of link structures, we need to model
the relationship of two ends of links, i.e., edges between vertex
pairs. Therefore, the model should involve factors of both vertices
of the edge. This is a key difference between traditional factor
analysis and our model. In our model, we connect two 
components through a set of shared factors, that is, the latent factors in the
second component (for contents) are tied to the factors in the first
component (for links). By doing this, we search for a unified set
of latent factors that best explains both content and link structures
simultaneously and seamlessly.
In the formulation, we perform factor analysis based on matrix
factorization: solution to the first component is based on 
factorizing the term-document matrix derived from content features; 
solution to the second component is based on factorizing the adjacency
matrix derived from links. Because the two factorizations share
a common base, the discovered bases (latent factors) explain both
content information and link structures, and are then used in further
information management tasks such as classification.
This paper is organized as follows. Section 2 reviews related
work. Section 3 presents the proposed approach to analyze the web
page based on the combined information of links and content. 
Section 4 extends the basic framework and a few variants for fine tune.
Section 5 shows the experiment results. Section 6 discusses the
details of this approach and Section 7 concludes.
2. RELATED WORK
In the content analysis part, our approach is closely related to
Latent Semantic Indexing (LSI) [8]. LSI maps documents into a
lower dimensional latent space. The latent space implicitly 
captures a large portion of information of documents, therefore it is
called the latent semantic space. The similarity between documents
could be defined by the dot products of the corresponding vectors
of documents in the latent space. Analysis tasks, such as 
classification, could be performed on the latent space. The commonly
used singular value decomposition (SVD) method ensures that the
data points in the latent space can optimally reconstruct the original
documents. Though our approach also uses latent space to 
represent web pages (documents), we consider the link structure as well
as the content of web pages.
In the link analysis approach, the framework of hubs and 
authorities (HITS) [12] puts web page into two categories, hubs and
authorities. Using recursive notion, a hub is a web page with many
outgoing links to authorities, while an authority is a web page with
many incoming links from hubs. Instead of using two categories,
PageRank [17] uses a single category for the recursive notion, an
authority is a web page with many incoming links from authorities.
He et al. [9] propose a clustering algorithm for web document 
clustering. The algorithm incorporates link structure and the co-citation
patterns. In the algorithm, all links are treated as undirected edge of
the link graph. The content information is only used for weighing
the links by the textual similarity of both ends of the links. Zhang
et al. [23] uses the undirected graph regularization framework for
document classification. Achlioptas et al[2] decompose the web
into hub and authority attributes then combine them with content.
Zhou et al. [25] and [24] propose a directed graph regularization
framework for semi-supervised learning. The framework combines
the hub and authority information of web pages. But it is difficult
to combine the content information into that framework. Our 
approach consider the content and the directed linkage between topics
of source and destination web pages in one step, which implies the
topic combines the information of web page as authorities and as
hubs in a single set of factors.
Cohn and Hofmann [6] construct the latent space from both 
content and link information, using content analysis based on 
probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5]. The
major difference between the approach of [6] (PLSI+PHITS) and
our approach is in the part of link analysis. In PLSI+PHITS, the
link is constructed with the linkage from the topic of the source
web page to the destination web page. In the model, the outgoing
links of the destination web page have no effect on the source web
page. In other words, the overall link structure is not utilized in
PHITS. In our approach, the link is constructed with the linkage
between the factor of the source web page and the factor of the 
destination web page, instead of the destination web page itself. The
factor of the destination web page contains information of its 
outgoing links. In turn, such information is passed to the factor of the
source web page. As the result of matrix factorization, the factor
forms a factor graph, a miniature of the original graph, preserving
the major structure of the original graph.
Taskar et al. [19] propose relational Markov networks (RMNs)
for entity classification, by describing a conditional distribution of
entity classes given entity attributes and relationships. The model
was applied to web page classification, where web pages are 
entities and hyperlinks are treated as relationships. RMNs apply 
conditional random fields to define a set of potential functions on cliques
of random variables, where the link structure provides hints to form
the cliques. However the model does not give an off-the-shelf 
solution, because the success highly depends on the arts of designing
the potential functions. On the other hand, the inference for RMNs
is intractable and requires belief propagation.
The following are some work on combining documents and
links, but the methods are loosely related to our approach. The
experiments of [21] show that using terms from the linked 
document improves the classification accuracy. Chakrabarti et al.[3] use
co-citation information in their classification model. Joachims et
al.[11] combine text kernels and co-citation kernels for 
classification. Oh et al [16] use the Naive Bayesian frame to combine link
information with content.
3. OUR APPROACH
In this section we will first introduce a novel matrix 
factorization method, which is more suitable than conventional matrix 
factorization methods for link analysis. Then we will introduce our
approach that jointly factorizes the document-term matrix and link
matrix and obtains compact and highly indicative factors for 
representing documents or web pages.
3.1 Link Matrix Factorization
Suppose we have a directed graph G = (V, E), where the vertex
set V = {vi}n
i=1 represents the web pages and the edge set E 
represents the hyperlinks between web pages. Let A = {asd} denotes
the n×n adjacency matrix of G, which is also called the link matrix
in this paper. For a pair of vertices, vs and vd, let asd = 1 when
there is an edge from vs to vd, and asd = 0, otherwise. Note that
A is an asymmetric matrix, because hyperlinks are directed.
Most machine learning algorithms assume a feature-vector 
representation of instances. For web page classification, however, the
link graph does not readily give such a vector representation for
web pages. If one directly uses each row or column of A for the job,
she will suffer a very high computational cost because the 
dimensionality equals to the number of web pages. On the other hand, it
will produces a poor classification accuracy (see our experiments
in Section 5), because A is extremely sparse1
.
The idea of link matrix factorization is to derive a high-quality
feature representation Z of web pages based on analyzing the link
matrix A, where Z is an n × l matrix, with each row being the 
ldimensional feature vector of a web page. The new representation
of web pages captures the principal factors of the link structure and
makes further processing more efficient.
One may use a method similar to LSI, to apply the well-known
principal component analysis (PCA) for deriving Z from A. The
corresponding optimization problem 2
is
min
Z,U
A − ZU 2
F + γ U 2
F (1)
where γ is a small positive number, U is an l ×n matrix, and · F
is the Frobenius norm. The optimization aims to approximate A by
ZU , a product of two low-rank matrices, with a regularization on
U. In the end, the i-th row vector of Z can be thought as the hub
feature vector of vertex vi, and the row vector of U can be thought
as the authority features. A link generation model proposed in [2] is
similar to the PCA approach. Since A is a nonnegative matrix here,
one can also consider to put nonnegative constraints on U and Z,
which produces an algorithm similar to PLSA [10] and NMF [20].
1
Due to the sparsity of A, links from two similar pages may not
share any common target pages, which makes them to appear 
dissimilar. However the two pages may be indirectly linked to many
common pages via their neighbors.
2
Another equivalent form is minZ,U A − ZU 2
F , s. t. U U =
I. The solution Z is identical subject to a scaling factor.
However, despite its popularity in matrix analysis, PCA (or other
similar methods like PLSA) is restrictive for link matrix 
factorization. The major problem is that, PCA ignores the fact that the rows
and columns of A are indexed by exactly the same set of objects
(i.e., web pages). The approximating matrix ˜A = ZU shows no
evidence that links are within the same set of objects. To see the
drawback, let"s consider a link transitivity situation vi → vs → vj,
where page i is linked to page s which itself is linked to page j.
Since ˜A = ZU treats A as links from web pages {vi} to a 
different set of objects, let it be denoted by {oi}, ˜A = ZU actually
splits an linked object os from vs and breaks down the link path
into two parts vi → os and vs → oj. This is obviously a miss
interpretation to the original link path.
To overcome the problem of PCA, in this paper we suggest to
use a different factorization:
min
Z,U
A − ZUZ 2
F + γ U 2
F (2)
where U is an l × l full matrix. Note that U is not symmetric, thus
ZUZ produces an asymmetric matrix, which is the case of A.
Again, each row vector of Z corresponds to a feature vector of a
web pages. The new approximating form ˜A = ZUZ puts a clear
meaning that the links are between the same set of objects, 
represented by features Z. The factor model actually maps each vertex,
vi, into a vector zi = {zi,k; 1 ≤ k ≤ l} in the Rl
space. We call
the Rl
space the factor space. Then, {zi} encodes the information
of incoming and outgoing connectivity of vertices {vi}. The 
factor loadings, U, explain how these observed connections happened
based on {zi}. Once we have the vector zi, we can use many 
traditional classification methods (such as SVMs) or clustering tools
(such as K-Means) to perform the analysis.
Illustration Based on a Synthetic Problem
To further illustrate the advantages of the proposed link matrix 
factorization Eq. (2), let us consider the graph in Figure 1. Given
v1
v2
v3
v4
v5
v6
v7
v8
Figure 2: Summarize Figure 1 with a factor graph
these observations, we can summarize the graph by grouping as
factor graph depicted in Figure 2. In the next we preform the two
factorization methods Eq. (2) and Eq. (1) on this link matrix. A
good low-rank representation should reveal the structure of the 
factor graph.
First we try PCA-like decomposition, solving Eq. (1) and
obtaining
Z = U =
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
1. 1. 0 0 0
0 0 −.6 −.7 .1
0 0 .0 .6 −.0
0 0 .8 −.4 .3
0 0 .2 −.2 −.9
.7 .7 0 0 0
.7 .7 0 0 0
0 0 0 0 0
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
0 0 0 0 0
.5 −.5 0 0 0
.5 −.5 0 0 0
0 0 −0.6 −.7 .1
0 0 .0 .6 −.0
0 0 .8 −.4 .3
0 0 .2 −.2 −.9
.7 .7 0 0 0
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
We can see that the row vectors of v6 and v7 are the same in Z,
indicating that v6 and v7 have the same hub attributes. The row
vectors of v2 and v3 are the same in U, indicating that v2 and
v3 have the same authority attributes. It is not clear to see the
similarity between v4 and v5, because their inlinks (and outlinks)
are different.
Then, we factorize A by ZUZ via solving Eq. (2), and obtain
the results
Z = U =
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
−.8 −.5 .3 −.1 −.0
−.0 .4 .6 −.1 −.4
−.0 .4 .6 −.1 −.4
.3 −.2 .3 −.4 .3
.3 −.2 .3 −.4 .3
−.4 .5 .0 −.2 .6
−.4 .5 .0 −.2 .6
−.1 .1 −.4 −.8 −.4
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
2
6
6
6
6
6
6
6
6
4
−.1 −.2 −.4 .6 .7
.2 −.5 −.5 −.5 .0
.1 .1 .4 −.4 .3
.1 −.2 −.0 .3 −.1
−.3 .3 −.5 −.4 −.2
3
7
7
7
7
7
7
7
7
5
The resultant Z is very consistent with the clustering structure
of vertices: the row vectors of v2 and v3 are the same, those
of v4 and v5 are the same, those of v6 and v7 are the same.
Even interestingly, if we add constraints to ensure Z and U be
nonnegative, we have
Z = U =
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
1. 0 0 0 0
0 .9 0 0 0
0 .9 0 0 0
0 0 .7 0 0
0 0 .7 0 0
0 0 0 .9 0
0 0 0 .9 0
0 0 0 0 1.
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
2
6
6
6
6
6
6
6
6
4
0 1. 0 0 0
0 0 .7 0 0
0 0 0 .7 0
0 0 0 0 1.
0 0 0 0 0
3
7
7
7
7
7
7
7
7
5
which clearly tells the assignment of vertices to clusters from Z
and the links of factor graph from U. When the interpretability is
not critical in some tasks, for example, classification, we found that
it achieves better accuracies without the nonnegative constraints.
Given our above analysis, it is clear that the factorization ZUZ
is more expressive than ZU in representing the link matrix A.
3.2 Content Matrix Factorization
Now let us consider the content information on the vertices. To
combine the link information and content information, we want to
use the same latent space to approximate the content as the latent
space for the links. Using the bag-of-words approach, we denote
the content of web pages by an n×m matrix C, each of whose rows
represents a document, each column represents a keyword, where
m is the number of keywords. Like the latent semantic indexing
(LSI) [8], the l-dimensional latent space for words is denoted by an
m × l matrix V . Therefore, we use ZV to approximate matrix
C,
min
V,Z
C − ZV 2
F + β V 2
F , (3)
where β is a small positive number, β V 2
F serves as a 
regularization term to improve the robustness.
3.3 Joint Link-Content Matrix Factorization
There are many ways to employ both the content and link 
information for web page classification. Our idea in this paper is not to
simply combine them, but rather to fuse them into a single, 
consistent, and compact feature representation. To achieve this goal, we
solve the following problem,
min
U,V,Z
n
J (U, V, Z)
def
= A − ZUZ 2
F +
α C − ZV 2
F + γ U 2
F + β V 2
F
o
.
(4)
Eq. (4) is the joined matrix factorization of A and C with 
regularization. The new representation Z is ensured to capture both the
structures of the link matrix A and the content matrix C. Once
we find the optimal Z, we can apply the traditional classification
or clustering methods on vectorial data Z. The relationship among
these matrices can be depicted as Figure 3.
A Y C
U Z V
Figure 3: Relationship among the matrices. Node Y is the 
target of classification.
Eq. (4) can be solved using gradient methods, such as the 
conjugate gradient method and quasi-Newton methods. Then main 
computation of gradient methods is evaluating the object function J
and its gradients against variables,
∂J
∂U
=

Z ZUZ Z − Z AZ

+ γU,
∂J
∂V
=α

V Z Z − C Z

+ βV,
∂J
∂Z
=

ZU Z ZU + ZUZ ZU − A ZU − AZU

+ α

ZV V − CV

.
Because of the sparsity of A, the computational complexity of
multiplication of A and Z is O(µAl), where µA is the number of
nonzero entries in A. Similarly, the computational complexity of
C Z and CV is O(µC l), where µC is the number of nonzero
entries in C. The computational complexity of the rest 
multiplications in the gradient computation is O(nl2
). Therefore, the total
computational complexity in one iteration is O(µAl + µC l + nl2
).
The number of links and the number of words in a web page are
relatively small comparing to the number of web pages, and are 
almost constant as the number of web pages/documents increases, i.e.
µA = O(n) and µC = O(n). Therefore, theoretically the 
computation time is almost linear to the number of web pages/documents,
n.
4. SUPERVISED MATRIX 
FACTORIZATION
Consider a web page classification problem. We can solve
Eq. (4) to obtain Z as Section 3, then use a traditional classifier
to perform classification. However, this approach does not take
data labels into account in the first step. Believing that using data
labels improves the accuracy by obtaining a better Z for the 
classification, we consider to use the data labels to guide the matrix
factorization, called supervised matrix factorization [22]. Because
some data used in the matrix factorization have no label 
information, the supervised matrix factorization falls into the category of
semi-supervised learning.
Let C be the set of classes. For simplicity, we first consider 
binary class problem, i.e. C = {−1, 1}. Assume we know the 
labels {yi} for vertices in T ⊂ V. We want to find a hypothesis
h : V → R, such that we assign vi to 1 when h(vi) ≥ 0, −1 
otherwise. We assume a transform from the latent space to R is linear,
i.e.
h(vi) = w φ(vi) + b = w zi + b, (5)
School course dept. faculty other project staff student total
Cornell 44 1 34 581 18 21 128 827
Texas 36 1 46 561 20 2 148 814
Washington 77 1 30 907 18 10 123 1166
Wisconsin 85 0 38 894 25 12 156 1210
Table 1: Dataset of WebKB
where w and b are parameters to estimate. Here, w is the norm
of the decision boundary. Similar to Support Vector Machines
(SVMs) [7], we can use the hinge loss to measure the loss,
X
i:vi∈T
[1 − yih(vi)]+ ,
where [x]+ is x if x ≥ 0, 0 if x < 0. However, the hinge loss
is not smooth at the hinge point, which makes it difficult to apply
gradient methods on the problem. To overcome the difficulty, we
use a smoothed version of hinge loss for each data point,
g(yih(vi)), (6)
where
g(x) =
8
><
>:
0 when x ≥ 2,
1 − x when x ≤ 0,
1
4
(x − 2)2
when 0 < x < 2.
We reduce a multiclass problem into multiple binary ones. One
simple scheme of reduction is the one-against-rest coding scheme.
In the one-against-rest scheme, we assign a label vector for each
class label. The element of a label vector is 1 if the data point 
belongs the corresponding class, −1, if the data point does not belong
the corresponding class, 0, if the data point is not labeled. Let Y be
the label matrix, each column of which is a label vector. Therefore,
Y is a matrix of n × c, where c is the number of classes, |C|. Then
the values of Eq. (5) form a matrix
H = ZW + 1b , (7)
where 1 is a vector of size n, whose elements are all one, W is a
c × l parameter matrix, and b is a parameter vector of size c. The
total loss is proportional to the sum of Eq. (6) over all labeled data
points and the classes,
LY (W, b, Z) = λ
X
i:vi∈T ,j∈C
g(YijHij),
where λ is the parameter to scale the term.
To derive a robust solution, we also use Tikhonov regularization
for W,
ΩW (W) =
ν
2
W 2
F ,
where ν is the parameter to scale the term.
Then the supervised matrix factorization problem becomes
min
U,V,Z,W,b
Js(U, V, Z, W, b) (8)
where
Js(U, V, Z, W, b) = J (U, V, Z) + LY (W, b, Z) + ΩW (W).
We can also use gradient methods to solve the problem of Eq. (8).
The gradients are
∂Js
∂U
=
∂J
∂U
,
∂Js
∂V
=
∂J
∂V
,
∂Js
∂Z
=
∂J
∂Z
+ λGW,
∂Js
∂W
=λG Z + νW,
∂Js
∂b
=λG 1,
where G is an n×c matrix, whose ik-th element is Yikg (YikHik),
and
g (x) =
8
><
>:
0 when x ≥ 2,
−1 when x ≤ 0,
1
2
(x − 2) when 0 < x < 2.
Once we obtain w, b, and Z, we can apply h on the vertices with
unknown class labels, or apply traditional classification algorithms
on Z to get the classification results.
5. EXPERIMENTS
5.1 Data Description
In this section, we perform classification on two datasets, to
demonstrate the our approach. The two datasets are the WebKB
data set[1] and the Cora data set [15]. The WebKB data set 
consists of about 6000 web pages from computer science departments
of four schools (Cornell, Texas, Washington, and Wisconsin). The
web pages are classified into seven categories. The numbers of
pages in each category are shown in Table 1. The Cora data set
consists of the abstracts and references of about 34,000 computer
science research papers. We use part of them to categorize into
one of subfields of data structure (DS), hardware and architecture
(HA), machine learning (ML), and programing language (PL). We
remove those articles without reference to other articles in the set.
The number of papers and the number of subfields in each area are
shown in Table 2.
area # of papers # of subfields
Data structure (DS) 751 9
Hardware and architecture (HA) 400 7
Machine learning (ML) 1617 7
Programing language (PL) 1575 9
Table 2: Dataset of Cora
5.2 Methods
The task of the experiments is to classify the data based on their
content information and/or link structure. We use the following
methods:
65
70
75
80
85
90
95
100
WisconsinWashingtonTexasCornell
accuracy(%)
dataset
SVM on content
SVM on link
SVM on link-content
Directed graph reg.
PLSI+PHITS
link-content MF
link-content sup. MF
method Cornell Texas Washington Wisconsin
SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80
SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00
SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.70 84.00 ± 0.90
Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45
PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87
link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60
link-content sup. MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30
Table 3: Classification accuracy (mean ± std-err %) on WebKB data set
• SVM on content We apply support vector machines (SVM)
on the content of documents. The features are the 
bag-ofwords and all word are stemmed. This method ignores link
structure in the data. Linear SVM is used. The 
regularization parameter of SVM is selected using the cross-validation
method. The implementation of SVM used in the 
experiments is libSVM[4].
• SVM on links We treat links as the features of each 
document, i.e. the i-th feature is link-to-pagei. We apply SVM on
link features. This method uses link information, but not the
link structure.
• SVM on link-content We combine the features of the above
two methods. We use different weights for these two set
of features. The weights are also selected using 
crossvalidation.
• Directed graph regularization This method is described in
[25] and [24]. This method is solely based on link structure.
• PLSI+PHITS This method is described in [6]. This method
combines text content information and link structure for
analysis. The PHITS algorithm is in spirit similar to Eq.1,
with an additional nonnegative constraint. It models the 
outgoing and in-coming structures separately.
• Link-content MF This is our approach of matrix 
factorization described in Section 3. We use 50 latent factors for Z.
After we compute Z, we train a linear SVM using Z as the
feature vectors, then apply SVM on testing portion of Z to
obtain the final result, because of the multiclass output.
• Link-content sup. MF This method is our approach of the
supervised matrix factorization in Section 4. We use 50 latent
factors for Z. After we compute Z, we train a linear SVM
on the training portion of Z, then apply SVM on testing 
portion of Z to obtain the final result, because of the multiclass
output.
We randomly split data into five folds and repeat the experiment
for five times, for each time we use one fold for test, four other
folds for training. During the training process, we use the 
crossvalidation to select all model parameters. We measure the results
by the classification accuracy, i.e., the percentage of the number
of correct classified documents in the entire data set. The results
are shown as the average classification accuracies and it standard
deviation over the five repeats.
5.3 Results
The average classification accuracies for the WebKB data set are
shown in Table 3. For this task, the accuracies of SVM on links
are worse than that of SVM on content. But the directed graph
regularization, which is also based on link alone, achieves a much
higher accuracy. This implies that the link structure plays an 
important role in the classification of this dataset, but individual links
in a web page give little information. The combination of link and
content using SVM achieves similar accuracy as that of SVM on
content alone, which confirms individual links in a web page give
little information. Since our approach consider the link structure
as well as the content information, our two methods give results
a highest accuracies among these approaches. The difference 
between the results of our two methods is not significant. However in
the experiments below, we show the difference between them.
The classification accuracies for the Cora data set are shown in
Table 4. In this experiment, the accuracies of SVM on the 
combination of links and content are higher than either SVM on content
or SVM on links. This indicates both content and links are 
infor45
50
55
60
65
70
75
80
PLMLHADS
accuracy(%)
dataset
SVM on content
SVM on link
SVM on link-content
Directed graph reg.
PLSI+PHITS
link-content MF
link-content sup. MF
method DS HA ML PL
SVM on content 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70
SVM on links 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70
SVM on link-content 63.70 ± 1.50 70.50 ± 2.20 70.56 ± 0.80 62.35 ± 1.00
Directed graph regularization 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84
PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68
link-content MF 61.00 ± 0.70 74.20 ± 1.20 77.50 ± 0.80 62.50 ± 0.80
link-content sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32
Table 4: Classification accuracy (mean ± std-err %) on Cora data set
mative for classifying the articles into subfields. The method of
directed graph regularization does not perform as good as SVM on
link-content, which confirms the importance of the article content
in this task. Though our method of link-content matrix 
factorization perform slightly better than other methods, our method of 
linkcontent supervised matrix factorization outperform significantly.
5.4 The Number of Factors
As we discussed in Section 3, the computational complexity of
each iteration for solving the optimization problem is quadratic to
the number of factors. We perform experiments to study how the
number of factors affects the accuracy of predication. We use 
different numbers of factors for the Cornell data of WebKB data set
and the machine learning (ML) data of Cora data set. The result
shown in Figure 4(a) and 4(b). The figures show that the accuracy
88
89
90
91
92
93
94
95
0 10 20 30 40 50
accuracy(%)
number of factors
link-content sup. MF
link-content MF
(a) Cornell data
62
64
66
68
70
72
74
76
78
80
0 10 20 30 40 50
accuracy(%)
number of factors
link-content sup. MF
link-content MF
(b) ML data
Figure 4: Accuracy vs number of factors
increases as the number of factors increases. It is a different 
concept from choosing the optimal number of clusters in clustering
application. It is how much information to represent in the latent
variables. We have considered the regularization over the factors,
which avoids the overfit problem for a large number of factors. To
choose of the number of factors, we need to consider the trade-off
between the accuracy and the computation time, which is quadratic
to the number of factors.
The difference between the method of matrix factorization and
that of supervised one decreases as the number of factors increases.
This indicates that the usefulness of supervised matrix factorization
at lower number of factors.
6. DISCUSSIONS
The loss functions LA in Eq. (2) and LC in Eq. (3) use squared
loss due to computationally convenience. Actually, squared loss
does not precisely describe the underlying noise model, because
the weights of adjacency matrix can only take nonnegative 
values, in our case, zero or one only, and the components of 
content matrix C can only take nonnegative integers. Therefore, we
can apply other types of loss, such as hinge loss or smoothed
hinge loss, e.g. LA(U, Z) = µh(A, ZUZ ), where h(A, B) =P
i,j [1 − AijBij]+ .
In our paper, we mainly discuss the application of classification.
A entry of matrix Z means the relationship of a web page and a
factor. The values of the entries are the weights of linear model,
instead of the probabilities of web pages belonging to latent 
topics. Therefore, we allow the components take any possible real 
values. When we come to the clustering application, we can use this
model to find Z, then apply K-means to partition the web pages
into clusters. Actually, we can use the idea of nonnegative matrix
factorization for clustering [20] to directly cluster web pages. As
the example with nonnegative constraints shown in Section 3, we
represent each cluster by a latent topic, i.e. the dimensionality of
the latent space is set to the number of clusters we want. Then the
problem of Eq. (4) becomes
min
U,V,Z
J (U, V, Z), s.t.Z ≥ 0. (9)
Solving Eq. (9), we can obtain more interpretable results, which
could be used for clustering.
7. CONCLUSIONS
In this paper, we study the problem of how to combine the 
information of content and links for web page analysis, mainly on 
classification application. We propose a simple approach using factors to
model the text content and link structure of web pages/documents.
The directed links are generated from the linear combination of
linkage of between source and destination factors. By sharing 
factors between text content and link structure, it is easy to combine
both the content information and link structure. Our experiments
show our approach is effective for classification. We also discuss
an extension for clustering application.
Acknowledgment
We would like to thank Dr. Dengyong Zhou for sharing his code
of his algorithm. Also, thanks to the reviewers for constructive
comments.
8. REFERENCES
[1] CMU world wide knowledge base (WebKB) project.
Available at http://www.cs.cmu.edu/∼WebKB/.
[2] D. Achlioptas, A. Fiat, A. R. Karlin, and F. McSherry. Web
search via hub synthesis. In IEEE Symposium on
Foundations of Computer Science, pages 500-509, 2001.
[3] S. Chakrabarti, B. E. Dom, and P. Indyk. Enhanced hypertext
categorization using hyperlinks. In L. M. Haas and
A. Tiwary, editors, Proceedings of SIGMOD-98, ACM
International Conference on Management of Data, pages
307-318, Seattle, US, 1998. ACM Press, New York, US.
[4] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support
vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/∼cjlin/libsvm.
[5] D. Cohn and H. Chang. Learning to probabilistically identify
authoritative documents. Proc. ICML 2000. pp.167-174.,
2000.
[6] D. Cohn and T. Hofmann. The missing link - a probabilistic
model of document content and hypertext connectivity. In
T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances
in Neural Information Processing Systems 13, pages
430-436. MIT Press, 2001.
[7] C. Cortes and V. Vapnik. Support-vector networks. Machine
Learning, 20:273, 1995.
[8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. Indexing by latent semantic
analysis. Journal of the American Society of Information
Science, 41(6):391-407, 1990.
[9] X. He, H. Zha, C. Ding, and H. Simon. Web document
clustering using hyperlink structures. Computational
Statistics and Data Analysis, 41(1):19-45, 2002.
[10] T. Hofmann. Probabilistic latent semantic indexing. In
Proceedings of the Twenty-Second Annual International
SIGIR Conference, 1999.
[11] T. Joachims, N. Cristianini, and J. Shawe-Taylor. Composite
kernels for hypertext categorisation. In C. Brodley and
A. Danyluk, editors, Proceedings of ICML-01, 18th
International Conference on Machine Learning, pages
250-257, Williams College, US, 2001. Morgan Kaufmann
Publishers, San Francisco, US.
[12] J. M. Kleinberg. Authoritative sources in a hyperlinked
environment. J. ACM, 48:604-632, 1999.
[13] P. Kolari, T. Finin, and A. Joshi. SVMs for the Blogosphere:
Blog Identification and Splog Detection. In AAAI Spring
Symposium on Computational Approaches to Analysing
Weblogs, March 2006.
[14] O. Kurland and L. Lee. Pagerank without hyperlinks:
structural re-ranking using links induced by language
models. In SIGIR "05: Proceedings of the 28th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 306-313, New
York, NY, USA, 2005. ACM Press.
[15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.
Automating the contruction of internet portals with machine
learning. Information Retrieval Journal, 3(127-163), 2000.
[16] H.-J. Oh, S. H. Myaeng, and M.-H. Lee. A practical
hypertext catergorization method using links and
incrementally available class information. In SIGIR "00:
Proceedings of the 23rd annual international ACM SIGIR
conference on Research and development in information
retrieval, pages 264-271, New York, NY, USA, 2000. ACM
Press.
[17] L. Page, S. Brin, R. Motowani, and T. Winograd. PageRank
citation ranking: bring order to the web. Stanford Digital
Library working paper 1997-0072, 1997.
[18] C. Spearman. General Intelligence, objectively determined
and measured. The American Journal of Psychology,
15(2):201-292, Apr 1904.
[19] B. Taskar, P. Abbeel, and D. Koller. Discriminative
probabilistic models for relational data. In Proceedings of
18th International UAI Conference, 2002.
[20] W. Xu, X. Liu, and Y. Gong. Document clustering based on
non-negative matrix factorization. In SIGIR "03:
Proceedings of the 26th annual international ACM SIGIR
conference on Research and development in informaion
retrieval, pages 267-273. ACM Press, 2003.
[21] Y. Yang, S. Slattery, and R. Ghani. A study of approaches to
hypertext categorization. Journal of Intelligent Information
Systems, 18(2-3):219-241, 2002.
[22] K. Yu, S. Yu, and V. Tresp. Multi-label informed latent
semantic indexing. In SIGIR "05: Proceedings of the 28th
annual international ACM SIGIR conference on Research
and development in information retrieval, pages 258-265,
New York, NY, USA, 2005. ACM Press.
[23] T. Zhang, A. Popescul, and B. Dom. Linear prediction
models with graph regularization for web-page
categorization. In KDD "06: Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery
and data mining, pages 821-826, New York, NY, USA,
2006. ACM Press.
[24] D. Zhou, J. Huang, and B. Sch¨olkopf. Learning from labeled
and unlabeled data on a directed graph. In Proceedings of the
22nd International Conference on Machine Learning, Bonn,
Germany, 2005.
[25] D. Zhou, B. Sch¨olkopf, and T. Hofmann. Semi-supervised
learning on directed graphs. Proc. Neural Info. Processing
Systems, 2004.
Implicit User Modeling for Personalized Search
Xuehua Shen, Bin Tan, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
ABSTRACT
Information retrieval systems (e.g., web search engines) are 
critical for overcoming information overload. A major deficiency of
existing retrieval systems is that they generally lack user 
modeling and are not adaptive to individual users, resulting in inherently
non-optimal retrieval performance. For example, a tourist and a
programmer may use the same word java to search for different
information, but the current search systems would return the same
results. In this paper, we study how to infer a user"s interest from
the user"s search context and use the inferred implicit user model
for personalized search . We present a decision theoretic framework
and develop techniques for implicit user modeling in information
retrieval. We develop an intelligent client-side web search agent
(UCAIR) that can perform eager implicit feedback, e.g., query 
expansion based on previous queries and immediate result reranking
based on clickthrough information. Experiments on web search
show that our search agent can improve search accuracy over the
popular Google search engine.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval models, 
Relevance feedback, Search Process
General Terms
Algorithms
1. INTRODUCTION
Although many information retrieval systems (e.g., web search
engines and digital library systems) have been successfully deployed,
the current retrieval systems are far from optimal. A major 
deficiency of existing retrieval systems is that they generally lack user
modeling and are not adaptive to individual users [17]. This 
inherent non-optimality is seen clearly in the following two cases:
(1) Different users may use exactly the same query (e.g., Java) to
search for different information (e.g., the Java island in Indonesia or
the Java programming language), but existing IR systems return the
same results for these users. Without considering the actual user, it
is impossible to know which sense Java refers to in a query. (2)
A user"s information needs may change over time. The same user
may use Java sometimes to mean the Java island in Indonesia
and some other times to mean the programming language. 
Without recognizing the search context, it would be again impossible to
recognize the correct sense.
In order to optimize retrieval accuracy, we clearly need to model
the user appropriately and personalize search according to each 
individual user. The major goal of user modeling for information
retrieval is to accurately model a user"s information need, which is,
unfortunately, a very difficult task. Indeed, it is even hard for a user
to precisely describe what his/her information need is.
What information is available for a system to infer a user"s 
information need? Obviously, the user"s query provides the most direct
evidence. Indeed, most existing retrieval systems rely solely on
the query to model a user"s information need. However, since a
query is often extremely short, the user model constructed based
on a keyword query is inevitably impoverished . An effective way
to improve user modeling in information retrieval is to ask the user
to explicitly specify which documents are relevant (i.e., useful for
satisfying his/her information need), and then to improve user 
modeling based on such examples of relevant documents. This is called
relevance feedback, which has been proved to be quite effective for
improving retrieval accuracy [19, 20]. Unfortunately, in real world
applications, users are usually reluctant to make the extra effort to
provide relevant examples for feedback [11].
It is thus very interesting to study how to infer a user"s 
information need based on any implicit feedback information, which
naturally exists through user interactions and thus does not require
any extra user effort. Indeed, several previous studies have shown
that implicit user modeling can improve retrieval accuracy. In [3],
a web browser (Curious Browser) is developed to record a user"s
explicit relevance ratings of web pages (relevance feedback) and
browsing behavior when viewing a page, such as dwelling time,
mouse click, mouse movement and scrolling (implicit feedback).
It is shown that the dwelling time on a page, amount of scrolling
on a page and the combination of time and scrolling have a strong
correlation with explicit relevance ratings, which suggests that 
implicit feedback may be helpful for inferring user information need.
In [10], user clickthrough data is collected as training data to learn
a retrieval function, which is used to produce a customized ranking
of search results that suits a group of users" preferences. In [25],
the clickthrough data collected over a long time period is exploited
through query expansion to improve retrieval accuracy.
824
While a user may have general long term interests and 
preferences for information, often he/she is searching for documents to
satisfy an ad-hoc information need, which only lasts for a short
period of time; once the information need is satisfied, the user
would generally no longer be interested in such information. For
example, a user may be looking for information about used cars
in order to buy one, but once the user has bought a car, he/she is
generally no longer interested in such information. In such cases,
implicit feedback information collected over a long period of time
is unlikely to be very useful, but the immediate search context and
feedback information, such as which of the search results for the
current information need are viewed, can be expected to be much
more useful. Consider the query Java again. Any of the 
following immediate feedback information about the user could 
potentially help determine the intended meaning of Java in the query:
(1) The previous query submitted by the user is hashtable (as 
opposed to, e.g., travel Indonesia). (2) In the search results, the user
viewed a page where words such as programming, software,
and applet occur many times.
To the best of our knowledge, how to exploit such immediate
and short-term search context to improve search has so far not been
well addressed in the previous work. In this paper, we study how to
construct and update a user model based on the immediate search
context and implicit feedback information and use the model to 
improve the accuracy of ad-hoc retrieval. In order to maximally 
benefit the user of a retrieval system through implicit user modeling,
we propose to perform eager implicit feedback. That is, as soon
as we observe any new piece of evidence from the user, we would
update the system"s belief about the user"s information need and
respond with improved retrieval results based on the updated user
model. We present a decision-theoretic framework for optimizing
interactive information retrieval based on eager user model 
updating, in which the system responds to every action of the user by
choosing a system action to optimize a utility function. In a 
traditional retrieval paradigm, the retrieval problem is to match a query
with documents and rank documents according to their relevance
values. As a result, the retrieval process is a simple independent
cycle of query and result display. In the proposed new retrieval
paradigm, the user"s search context plays an important role and the
inferred implicit user model is exploited immediately to benefit the
user. The new retrieval paradigm is thus fundamentally different
from the traditional paradigm, and is inherently more general.
We further propose specific techniques to capture and exploit two
types of implicit feedback information: (1) identifying related 
immediately preceding query and using the query and the 
corresponding search results to select appropriate terms to expand the current
query, and (2) exploiting the viewed document summaries to 
immediately rerank any documents that have not yet been seen by the
user. Using these techniques, we develop a client-side web search
agent UCAIR (User-Centered Adaptive Information Retrieval) on
top of a popular search engine (Google). Experiments on web
search show that our search agent can improve search accuracy over
Google. Since the implicit information we exploit already naturally
exists through user interactions, the user does not need to make any
extra effort. Thus the developed search agent can improve existing
web search performance without additional effort from the user.
The remaining sections are organized as follows. In Section 2,
we discuss the related work. In Section 3, we present a 
decisiontheoretic interactive retrieval framework for implicit user modeling.
In Section 4, we present the design and implementation of an 
intelligent client-side web search agent (UCAIR) that performs eager
implicit feedback. In Section 5, we report our experiment results
using the search agent. Section 6 concludes our work.
2. RELATED WORK
Implicit user modeling for personalized search has been 
studied in previous work, but our work differs from all previous work
in several aspects: (1) We emphasize the exploitation of 
immediate search context such as the related immediately preceding query
and the viewed documents in the same session, while most previous
work relies on long-term collection of implicit feedback 
information [25]. (2) We perform eager feedback and bring the benefit of
implicit user modeling as soon as any new implicit feedback 
information is available, while the previous work mostly exploits 
longterm implicit feedback [10]. (3) We propose a retrieval framework
to integrate implicit user modeling with the interactive retrieval 
process, while the previous work either studies implicit user modeling
separately from retrieval [3] or only studies specific retrieval 
models for exploiting implicit feedback to better match a query with
documents [23, 27, 22]. (4) We develop and evaluate a 
personalized Web search agent with online user studies, while most existing
work evaluates algorithms offline without real user interactions.
Currently some search engines provide rudimentary 
personalization, such as Google Personalized web search [6], which allows
users to explicitly describe their interests by selecting from 
predefined topics, so that those results that match their interests are
brought to the top, and My Yahoo! search [16], which gives users
the option to save web sites they like and block those they 
dislike. In contrast, UCAIR personalizes web search through implicit
user modeling without any additional user efforts. Furthermore, the
personalization of UCAIR is provided on the client side. There are
two remarkable advantages on this. First, the user does not need to
worry about the privacy infringement, which is a big concern for
personalized search [26]. Second, both the computation of 
personalization and the storage of the user profile are done at the client
side so that the server load is reduced dramatically [9].
There have been many works studying user query logs [1] or
query dynamics [13]. UCAIR makes direct use of a user"s query
history to benefit the same user immediately in the same search
session. UCAIR first judges whether two neighboring queries 
belong to the same information session and if so, it selects terms from
the previous query to perform query expansion.
Our query expansion approach is similar to automatic query 
expansion [28, 15, 5], but instead of using pseudo feedback to expand
the query, we use user"s implicit feedback information to expand
the current query. These two techniques may be combined.
3. OPTIMIZATION IN INTERACTIVE IR
In interactive IR, a user interacts with the retrieval system through
an action dialogue, in which the system responds to each user 
action with some system action. For example, the user"s action may
be submitting a query and the system"s response may be returning
a list of 10 document summaries. In general, the space of user 
actions and system responses and their granularities would depend on
the interface of a particular retrieval system.
In principle, every action of the user can potentially provide new
evidence to help the system better infer the user"s information need.
Thus in order to respond optimally, the system should use all the
evidence collected so far about the user when choosing a response.
When viewed in this way, most existing search engines are clearly
non-optimal. For example, if a user has viewed some documents on
the first page of search results, when the user clicks on the Next
link to fetch more results, an existing retrieval system would still
return the next page of results retrieved based on the original query
without considering the new evidence that a particular result has
been viewed by the user.
825
We propose to optimize retrieval performance by adapting 
system responses based on every action that a user has taken, and cast
the optimization problem as a decision task. Specifically, at any
time, the system would attempt to do two tasks: (1) User model
updating: Monitor any useful evidence from the user regarding
his/her information need and update the user model as soon as such
evidence is available; (2) Improving search results: Rerank 
immediately all the documents that the user has not yet seen, as soon
as the user model is updated. We emphasize eager updating and
reranking, which makes our work quite different from any existing
work. Below we present a formal decision theoretic framework for
optimizing retrieval performance through implicit user modeling in
interactive information retrieval.
3.1 A decision-theoretic framework
Let A be the set of all user actions and R(a) be the set of all
possible system responses to a user action a ∈ A. At any time, let
At = (a1, ..., at) be the observed sequence of user actions so far
(up to time point t) and Rt−1 = (r1, ..., rt−1) be the responses that
the system has made responding to the user actions. The system"s
goal is to choose an optimal response rt ∈ R(at) for the current
user action at.
Let M be the space of all possible user models. We further 
define a loss function L(a, r, m) ∈ , where a ∈ A is a user action,
r ∈ R(a) is a system response, and m ∈ M is a user model.
L(a, r, m) encodes our decision preferences and assesses the 
optimality of responding with r when the current user model is m
and the current user action is a. According to Bayesian decision
theory, the optimal decision at time t is to choose a response that
minimizes the Bayes risk, i.e.,
r∗
t = argmin
r∈R(at) M
L(at, r, mt)P(mt|U, D, At, Rt−1)dmt (1)
where P(mt|U, D, At, Rt−1) is the posterior probability of the
user model mt given all the observations about the user U we have
made up to time t.
To simplify the computation of Equation 1, let us assume that the
posterior probability mass P(mt|U, D, At, Rt−1) is mostly 
concentrated on the mode m∗
t = argmaxmt P(mt|U, D, At, Rt−1).
We can then approximate the integral with the value of the loss
function at m∗
t . That is,
r∗
t ≈ argminr∈R(at)L(at, r, m∗
t ) (2)
where m∗
t = argmaxmt P(mt|U, D, At, Rt−1).
Leaving aside how to define and estimate these probabilistic 
models and the loss function, we can see that such a decision-theoretic
formulation suggests that, in order to choose the optimal response
to at, the system should perform two tasks: (1) compute the 
current user model and obtain m∗
t based on all the useful 
information. (2) choose a response rt to minimize the loss function value
L(at, rt, m∗
t ). When at does not affect our belief about m∗
t , the
first step can be omitted and we may reuse m∗
t−1 for m∗
t .
Note that our framework is quite general since we can 
potentially model any kind of user actions and system responses. In most
cases, as we may expect, the system"s response is some ranking of
documents, i.e., for most actions a, R(a) consists of all the 
possible rankings of the unseen documents, and the decision problem
boils down to choosing the best ranking of unseen documents based
on the most current user model. When a is the action of submitting
a keyword query, such a response is exactly what a current retrieval
system would do. However, we can easily imagine that a more 
intelligent web search engine would respond to a user"s clicking of
the Next link (to fetch more unseen results) with a more 
optimized ranking of documents based on any viewed documents in
the current page of results. In fact, according to our eager updating
strategy, we may even allow a system to respond to a user"s clicking
of browser"s Back button after viewing a document in the same
way, so that the user can maximally benefit from implicit feedback.
These are precisely what our UCAIR system does.
3.2 User models
A user model m ∈ M represents what we know about the user
U, so in principle, it can contain any information about the user
that we wish to model. We now discuss two important components
in a user model.
The first component is a component model of the user"s 
information need. Presumably, the most important factor affecting the 
optimality of the system"s response is how well the response addresses
the user"s information need. Indeed, at any time, we may assume
that the system has some belief about what the user is interested
in, which we model through a term vector x = (x1, ..., x|V |),
where V = {w1, ..., w|V |} is the set of all terms (i.e., vocabulary)
and xi is the weight of term wi. Such a term vector is commonly
used in information retrieval to represent both queries and 
documents. For example, the vector-space model, assumes that both
the query and the documents are represented as term vectors and
the score of a document with respect to a query is computed based
on the similarity between the query vector and the document 
vector [21]. In a language modeling approach, we may also regard
the query unigram language model [12, 29] or the relevance model
[14] as a term vector representation of the user"s information need.
Intuitively, x would assign high weights to terms that characterize
the topics which the user is interested in.
The second component we may include in our user model is the
documents that the user has already viewed. Obviously, even if a
document is relevant, if the user has already seen the document, it
would not be useful to present the same document again. We thus
introduce another variable S ⊂ D (D is the whole set of documents
in the collection) to denote the subset of documents in the search
results that the user has already seen/viewed.
In general, at time t, we may represent a user model as mt =
(S, x, At, Rt−1), where S is the seen documents, x is the system"s
understanding of the user"s information need, and (At, Rt−1)
represents the user"s interaction history. Note that an even more
general user model may also include other factors such as the user"s
reading level and occupation.
If we assume that the uncertainty of a user model mt is solely
due to the uncertainty of x, the computation of our current estimate
of user model m∗
t will mainly involve computing our best estimate
of x. That is, the system would choose a response according to
r∗
t = argminr∈R(at)L(at, r, S, x∗
, At, Rt−1) (3)
where x∗
= argmaxx P(x|U, D, At, Rt−1). This is the 
decision mechanism implemented in the UCAIR system to be described
later. In this system, we avoided specifying the probabilistic model
P(x|U, D, At, Rt−1) by computing x∗
directly with some existing
feedback method.
3.3 Loss functions
The exact definition of loss function L depends on the responses,
thus it is inevitably application-specific. We now briefly discuss
some possibilities when the response is to rank all the unseen 
documents and present the top k of them. Let r = (d1, ..., dk) be the
top k documents, S be the set of seen documents by the user, and
x∗
be the system"s best guess of the user"s information need. We
826
may simply define the loss associated with r as the negative sum
of the probability that each of the di is relevant, i.e., L(a, r, m) =
− k
i=1 P(relevant|di, m). Clearly, in order to minimize this
loss function, the optimal response r would contain the k 
documents with the highest probability of relevance, which is intuitively
reasonable.
One deficiency of this top-k loss function is that it is not 
sensitive to the internal order of the selected top k documents, so 
switching the ranking order of a non-relevant document and a relevant one
would not affect the loss, which is unreasonable. To model 
ranking, we can introduce a factor of the user model - the probability
of each of the k documents being viewed by the user, P(view|di),
and define the following ranking loss function:
L(a, r, m) = −
k
i=1
P(view|di)P(relevant|di, m)
Since in general, if di is ranked above dj (i.e., i < j), P(view|di) >
P(view|dj), this loss function would favor a decision to rank 
relevant documents above non-relevant ones, as otherwise, we could
always switch di with dj to reduce the loss value. Thus the 
system should simply perform a regular retrieval and rank documents
according to the probability of relevance [18].
Depending on the user"s retrieval preferences, there can be many
other possibilities. For example, if the user does not want to see
redundant documents, the loss function should include some 
redundancy measure on r based on the already seen documents S.
Of course, when the response is not to choose a ranked list of
documents, we would need a different loss function. We discuss
one such example that is relevant to the search agent that we 
implement. When a user enters a query qt (current action), our search
agent relies on some existing search engine to actually carry out
search. In such a case, even though the search agent does not have
control of the retrieval algorithm, it can still attempt to optimize the
search results through refining the query sent to the search engine
and/or reranking the results obtained from the search engine. The
loss functions for reranking are already discussed above; we now
take a look at the loss functions for query refinement.
Let f be the retrieval function of the search engine that our agent
uses so that f(q) would give us the search results using query q.
Given that the current action of the user is entering a query qt (i.e.,
at = qt), our response would be f(q) for some q. Since we have
no choice of f, our decision is to choose a good q. Formally,
r∗
t = argminrt L(a, rt, m)
= argminf(q)L(a, f(q), m)
= f(argminqL(qt, f(q), m))
which shows that our goal is to find q∗
= argminqL(qt, f(q), m),
i.e., an optimal query that would give us the best f(q). A different
choice of loss function L(qt, f(q), m) would lead to a different
query refinement strategy. In UCAIR, we heuristically compute q∗
by expanding qt with terms extracted from rt−1 whenever qt−1 and
qt have high similarity. Note that rt−1 and qt−1 are contained in
m as part of the user"s interaction history.
3.4 Implicit user modeling
Implicit user modeling is captured in our framework through
the computation of x∗
= argmaxx P(x|U, D, At, Rt−1), i.e., the
system"s current belief of what the user"s information need is. Here
again there may be many possibilities, leading to different 
algorithms for implicit user modeling. We now discuss a few of them.
First, when two consecutive queries are related, the previous
query can be exploited to enrich the current query and provide more
search context to help disambiguation. For this purpose, instead of
performing query expansion as we did in the previous section, we
could also compute an updated x∗
based on the previous query and
retrieval results. The computed new user model can then be used to
rank the documents with a standard information retrieval model.
Second, we can also infer a user"s interest based on the 
summaries of the viewed documents. When a user is presented with a
list of summaries of top ranked documents, if the user chooses to
skip the first n documents and to view the (n+1)-th document, we
may infer that the user is not interested in the displayed summaries
for the first n documents, but is attracted by the displayed summary
of the (n + 1)-th document. We can thus use these summaries as
negative and positive examples to learn a more accurate user model
x∗
. Here many standard relevance feedback techniques can be 
exploited [19, 20]. Note that we should use the displayed summaries,
as opposed to the actual contents of those documents, since it is
possible that the displayed summary of the viewed document is
relevant, but the document content is actually not. Similarly, a 
displayed summary may mislead a user to skip a relevant document.
Inferring user models based on such displayed information, rather
than the actual content of a document is an important difference
between UCAIR and some other similar systems.
In UCAIR, both of these strategies for inferring an implicit user
model are implemented.
4. UCAIR: A PERSONALIZED
SEARCH AGENT
4.1 Design
In this section, we present a client-side web search agent called
UCAIR, in which we implement some of the methods discussed
in the previous section for performing personalized search through
implicit user modeling. UCAIR is a web browser plug-in 1
that
acts as a proxy for web search engines. Currently, it is only 
implemented for Internet Explorer and Google, but it is a matter of
engineering to make it run on other web browsers and interact with
other search engines.
The issue of privacy is a primary obstacle for deploying any real
world applications involving serious user modeling, such as 
personalized search. For this reason, UCAIR is strictly running as
a client-side search agent, as opposed to a server-side application.
This way, the captured user information always resides on the 
computer that the user is using, thus the user does not need to release
any information to the outside. Client-side personalization also 
allows the system to easily observe a lot of user information that may
not be easily available to a server. Furthermore, performing 
personalized search on the client-side is more scalable than on the 
serverside, since the overhead of computation and storage is distributed
among clients.
As shown in Figure 1, the UCAIR toolbar has 3 major 
components: (1) The (implicit) user modeling module captures a user"s
search context and history information, including the submitted
queries and any clicked search results and infers search session
boundaries. (2) The query modification module selectively 
improves the query formulation according to the current user model.
(3) The result re-ranking module immediately re-ranks any unseen
search results whenever the user model is updated.
In UCAIR, we consider four basic user actions: (1) submitting a
keyword query; (2) viewing a document; (3) clicking the Back
button; (4) clicking the Next link on a result page. For each
of these four actions, the system responds with, respectively, (1)
1
UCAIR is available at: http://sifaka.cs.uiuc.edu/ir/ucair/download.html
827
Search
Engine
(e.g.,
Google)
Search History Log
(e.g.,past queries,
clicked results)
Query
Modification
Result
Re-Ranking
User
Modeling
Result Buffer
UCAIR
Userquery
results
clickthrough…
Figure 1: UCAIR architecture
generating a ranked list of results by sending a possibly expanded
query to a search engine; (2) updating the information need model
x; (3) reranking the unseen results on the current result page based
on the current model x; and (4) reranking the unseen pages and
generating the next page of results based on the current model x.
Behind these responses, there are three basic tasks: (1) Decide
whether the previous query is related to the current query and if so
expand the current query with useful terms from the previous query
or the results of the previous query. (2) Update the information
need model x based on a newly clicked document summary. (3)
Rerank a set of unseen documents based on the current model x.
Below we describe our algorithms for each of them.
4.2 Session boundary detection and query 
expansion
To effectively exploit previous queries and their corresponding
clickthrough information, UCAIR needs to judge whether two 
adjacent queries belong to the same search session (i.e., detect 
session boundaries). Existing work on session boundary detection is
mostly in the context of web log analysis (e.g., [8]), and uses 
statistical information rather than textual features. Since our 
clientside agent does not have access to server query logs, we make 
session boundary decisions based on textual similarity between two
queries. Because related queries do not necessarily share the same
words (e.g., java island and travel Indonesia), it is insufficient
to use only query text. Therefore we use the search results of the
two queries to help decide whether they are topically related. For
example, for the above queries java island and travel 
Indonesia", the words java, bali, island, indonesia and travel
may occur frequently in both queries" search results, yielding a high
similarity score.
We only use the titles and summaries of the search results to 
calculate the similarity since they are available in the retrieved search
result page and fetching the full text of every result page would 
significantly slow down the process. To compensate for the terseness
of titles and summaries, we retrieve more results than a user would
normally view for the purpose of detecting session boundaries 
(typically 50 results).
The similarity between the previous query q and the current
query q is computed as follows. Let {s1, s2, . . . , sn } and
{s1, s2, . . . , sn} be the result sets for the two queries. We use
the pivoted normalization TF-IDF weighting formula [24] to 
compute a term weight vector si for each result si. We define the 
average result savg to be the centroid of all the result vectors, i.e.,
(s1 + s2 + . . . + sn)/n. The cosine similarity between the two
average results is calculated as
s avg · savg/ s
2
avg · s2
avg
If the similarity value exceeds a predefined threshold, the two queries
will be considered to be in the same information session.
If the previous query and the current query are found to belong
to the same search session, UCAIR would attempt to expand the
current query with terms from the previous query and its search
results. Specifically, for each term in the previous query or the
corresponding search results, if its frequency in the results of the
current query is greater than a preset threshold (e.g. 5 results out
of 50), the term would be added to the current query to form an
expanded query. In this case, UCAIR would send this expanded
query rather than the original one to the search engine and return
the results corresponding to the expanded query. Currently, UCAIR
only uses the immediate preceding query for query expansion; in
principle, we could exploit all related past queries.
4.3 Information need model updating
Suppose at time t, we have observed that the user has viewed
k documents whose summaries are s1, ..., sk. We update our user
model by computing a new information need vector with a standard
feedback method in information retrieval (i.e., Rocchio [19]). 
According to the vector space retrieval model, each clicked summary
si can be represented by a term weight vector si with each term
weighted by a TF-IDF weighting formula [21]. Rocchio computes
the centroid vector of all the summaries and interpolates it with the
original query vector to obtain an updated term vector. That is,
x = αq + (1 − α)
1
k
k
i=1
si
where q is the query vector, k is the number of summaries the user
clicks immediately following the current query and α is a parameter
that controls the influence of the clicked summaries on the inferred
information need model. In our experiments, α is set to 0.5. Note
that we update the information need model whenever the user views
a document.
4.4 Result reranking
In general, we want to rerank all the unseen results as soon as the
user model is updated. Currently, UCAIR implements reranking in
two cases, corresponding to the user clicking the Back button
and Next link in the Internet Explorer. In both cases, the current
(updated) user model would be used to rerank the unseen results so
that the user would see improved search results immediately.
To rerank any unseen document summaries, UCAIR uses the
standard vector space retrieval model and scores each summary
based on the similarity of the result and the current user information
need vector x [21]. Since implicit feedback is not completely 
reliable, we bring up only a small number (e.g. 5) of highest reranked
results to be followed by any originally high ranked results.
828
Google result (user query = java map) UCAIR result (user query =java map)
previous query = travel Indonesia previous query = hashtable
expanded user query = java map Indonesia expanded user query = java map class
1 Java map projections of the world ... Lonely Planet - Indonesia Map Map (Java 2 Platform SE v1.4.2)
www.btinternet.com/ se16/js/mapproj.htm www.lonelyplanet.com/mapshells/... java.sun.com/j2se/1.4.2/docs/...
2 Java map projections of the world ... INDONESIA TOURISM : CENTRAL JAVA - MAP Java 2 Platform SE v1.3.1: Interface Map
www.btinternet.com/ se16/js/oldmapproj.htm www.indonesia-tourism.com/... java.sun.com/j2se/1.3/docs/api/java/...
3 Java Map INDONESIA TOURISM : WEST JAVA - MAP An Introduction to Java Map Collection Classes
java.sun.com/developer/... www.indonesia-tourism.com/ ... www.oracle.com/technology/...
4 Java Technology Concept Map IndoStreets - Java Map An Introduction to Java Map Collection Classes
java.sun.com/developer/onlineTraining/... www.indostreets.com/maps/java/ www.theserverside.com/news/...
5 Science@NASA Home Indonesia Regions and Islands Maps, Bali, Java, ... Koders - Mappings.java
science.nasa.gov/Realtime/... www.maps2anywhere.com/Maps/... www.koders.com/java/
6 An Introduction to Java Map Collection Classes Indonesia City Street Map,... Hibernate simplifies inheritance mapping
www.oracle.com/technology/... www.maps2anywhere.com/Maps/... www.ibm.com/developerworks/java/...
7 Lonely Planet - Java Map Maps Of Indonesia tmap 30.map Class Hierarchy
www.lonelyplanet.com/mapshells/ www.embassyworld.com/maps/... tmap.pmel.noaa.gov/...
8 ONJava.com: Java API Map Maps of Indonesia by Peter Loud Class Scope
www.onjava.com/pub/a/onjava/api map/ users.powernet.co.uk/... jalbum.net/api/se/datadosen/util/Scope.html
9 GTA San Andreas : Sam Maps of Indonesia by Peter Loud Class PrintSafeHashMap
www.gtasanandreas.net/sam/ users.powernet.co.uk/mkmarina/indonesia/ jalbum.net/api/se/datadosen/...
10 INDONESIA TOURISM : WEST JAVA - MAP indonesiaphoto.com Java Pro - Union and Vertical Mapping of Classes
www.indonesia-tourism.com/... www.indonesiaphoto.com/... www.fawcette.com/javapro/...
Table 1: Sample results of query expansion
5. EVALUATION OF UCAIR
We now present some results on evaluating the two major UCAIR
functions: selective query expansion and result reranking based on
user clickthrough data.
5.1 Sample results
The query expansion strategy implemented in UCAIR is 
intentionally conservative to avoid misinterpretation of implicit user 
models. In practice, whenever it chooses to expand the query, the 
expansion usually makes sense. In Table 1, we show how UCAIR can
successfully distinguish two different search contexts for the query
java map, corresponding to two different previous queries (i.e.,
travel Indonesia vs. hashtable). Due to implicit user modeling,
UCAIR intelligently figures out to add Indonesia and class,
respectively, to the user"s query java map, which would 
otherwise be ambiguous as shown in the original results from Google
on March 21, 2005. UCAIR"s results are much more accurate than
Google"s results and reflect personalization in search.
The eager implicit feedback component is designed to 
immediately respond to a user"s activity such as viewing a document. In
Figure 2, we show how UCAIR can successfully disambiguate an
ambiguous query jaguar by exploiting a viewed document 
summary. In this case, the initial retrieval results using jaguar (shown
on the left side) contain two results about the Jaguar cars followed
by two results about the Jaguar software. However, after the user
views the web page content of the second result (about Jaguar
car) and returns to the search result page by clicking Back 
button, UCAIR automatically nominates two new search results about
Jaguar cars (shown on the right side), while the original two results
about Jaguar software are pushed down on the list (unseen from the
picture).
5.2 Quantitative evaluation
To further evaluate UCAIR quantitatively, we conduct a user
study on the effectiveness of the eager implicit feedback 
component. It is a challenge to quantitatively evaluate the potential 
performance improvement of our proposed model and UCAIR over
Google in an unbiased way [7]. Here, we design a user study,
in which participants would do normal web search and judge a
randomly and anonymously mixed set of results from Google and
UCAIR at the end of the search session; participants do not know
whether a result comes from Google or UCAIR.
We recruited 6 graduate students for this user study, who have
different backgrounds (3 computer science, 2 biology, and 1 
chem<top>
<num> Number: 716
<title> Spammer arrest sue
<desc> Description: Have any spammers
been arrested or sued for sending unsolicited
e-mail?
<narr> Narrative: Instances of arrests,
prosecutions, convictions, and punishments
of spammers, and lawsuits against them are
relevant. Documents which describe laws to
limit spam without giving details of lawsuits
or criminal trials are not relevant.
</top>
Figure 3: An example of TREC query topic, expressed in a
form which might be given to a human assistant or librarian
istry). We use query topics from TREC 2
2004 Terabyte track [2]
and TREC 2003 Web track [4] topic distillation task in the way to
be described below.
An example topic from TREC 2004 Terabyte track appears in
Figure 3. The title is a short phrase and may be used as a query
to the retrieval system. The description field provides a slightly
longer statement of the topic requirement, usually expressed as a
single complete sentence or question. Finally the narrative supplies
additional information necessary to fully specify the requirement,
expressed in the form of a short paragraph.
Initially, each participant would browse 50 topics either from
Terabyte track or Web track and pick 5 or 7 most interesting topics.
For each picked topic, the participant would essentially do the 
normal web search using UCAIR to find many relevant web pages by
using the title of the query topic as the initial keyword query. 
During this process, the participant may view the search results and
possibly click on some interesting ones to view the web pages, just
as in a normal web search. There is no requirement or restriction
on how many queries the participant must submit or when the 
participant should stop the search for one topic. When the participant
plans to change the search topic, he/she will simply press a button
2
Text REtrieval Conference: http://trec.nist.gov/
829
Figure 2: Screen shots for result reranking
to evaluate the search results before actually switching to the next
topic.
At the time of evaluation, 30 top ranked results from Google and
UCAIR (some are overlapping) are randomly mixed together so
that the participant would not know whether a result comes from
Google or UCAIR. The participant would then judge the relevance
of these results. We measure precision at top n (n = 5, 10, 20, 30)
documents of Google and UCAIR. We also evaluate precisions at
different recall levels.
Altogether, 368 documents judged as relevant from Google search
results and 429 documents judged as relevant from UCAIR by 
participants. Scatter plots of precision at top 10 and top 20 documents
are shown in Figure 4 and Figure 5 respectively (The scatter plot
of precision at top 30 documents is very similar to precision at top
20 documents). Each point of the scatter plots represents the 
precisions of Google and UCAIR on one query topic.
Table 2 shows the average precision at top n documents among
32 topics. From Figure 4, Figure 5 and Table 2, we see that the
search results from UCAIR are consistently better than those from
Google by all the measures. Moreover, the performance 
improvement is more dramatic for precision at top 20 documents than that
at precision at top 10 documents. One explanation for this is that
the more interaction the user has with the system, the more 
clickthrough data UCAIR can be expected to collect. Thus the retrieval
system can build more precise implicit user models, which lead to
better retrieval accuracy.
Ranking Method prec@5 prec@10 prec@20 prec@30
Google 0.538 0.472 0.377 0.308
UCAIR 0.581 0.556 0.453 0.375
Improvement 8.0% 17.8% 20.2% 21.8%
Table 2: Table of average precision at top n documents for 32
query topics
The plot in Figure 6 shows the precision-recall curves for UCAIR
and Google, where it is clearly seen that the performance of UCAIR
0 0.2 0.4 0.6 0.8 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
UCAIR prec@10
Googleprec@10
Scatterplot of Precision at Top 10 Documents
Figure 4: Precision at top 10 documents of UCAIR and Google
is consistently and considerably better than that of Google at all
levels of recall.
6. CONCLUSIONS
In this paper, we studied how to exploit implicit user modeling to
intelligently personalize information retrieval and improve search
accuracy. Unlike most previous work, we emphasize the use of 
immediate search context and implicit feedback information as well
as eager updating of search results to maximally benefit a user. We
presented a decision-theoretic framework for optimizing 
interactive information retrieval based on eager user model updating, in
which the system responds to every action of the user by 
choosing a system action to optimize a utility function. We further 
propose specific techniques to capture and exploit two types of implicit
feedback information: (1) identifying related immediately 
preceding query and using the query and the corresponding search results
to select appropriate terms to expand the current query, and (2)
exploiting the viewed document summaries to immediately rerank
any documents that have not yet been seen by the user. Using these
techniques, we develop a client-side web search agent (UCAIR)
on top of a popular search engine (Google). Experiments on web
search show that our search agent can improve search accuracy over
830
0 0.2 0.4 0.6 0.8 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
UCAIR prec@20
Googleprec@20
Scatterplot of Precision at Top 20 documents
Figure 5: Precision at top 20 documents of UCAIR and Google
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
recall
precision
Precision−Recall curves
Google Result
UCAIR Result
Figure 6: Precision at top 20 result of UCAIR and Google
Google. Since the implicit information we exploit already naturally
exists through user interactions, the user does not need to make any
extra effort. The developed search agent thus can improve 
existing web search performance without any additional effort from the
user.
7. ACKNOWLEDGEMENT
We thank the six participants of our evaluation experiments. This
work was supported in part by the National Science Foundation
grants IIS-0347933 and IIS-0428472.
8. REFERENCES
[1] S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman,
and O. Frieder. Hourly analysis of a very large topically
categorized web query log. In Proceedings of SIGIR 2004,
pages 321-328, 2004.
[2] C. Clarke, N. Craswell, and I. Soboroff. Overview of the
TREC 2004 terabyte track. In Proceedings of TREC 2004,
2004.
[3] M. Claypool, P. Le, M. Waseda, and D. Brown. Implicit
interest indicators. In Proceedings of Intelligent User
Interfaces 2001, pages 33-40, 2001.
[4] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.
Overview of the TREC 2003 web track. In Proceedings of
TREC 2003, 2003.
[5] W. B. Croft, S. Cronen-Townsend, and V. Larvrenko.
Relevance feedback and personalization: A language
modeling perspective. In Proeedings of Second DELOS
Workshop: Personalisation and Recommender Systems in
Digital Libraries, 2001.
[6] Google Personalized. http://labs.google.com/personalized.
[7] D. Hawking, N. Craswell, P. B. Thistlewaite, and D. Harman.
Results and challenges in web search evaluation. Computer
Networks, 31(11-16):1321-1330, 1999.
[8] X. Huang, F. Peng, A. An, and D. Schuurmans. Dynamic
web log session identification with statistical language
models. Journal of the American Society for Information
Science and Technology, 55(14):1290-1303, 2004.
[9] G. Jeh and J. Widom. Scaling personalized web search. In
Proceedings of WWW 2003, pages 271-279, 2003.
[10] T. Joachims. Optimizing search engines using clickthrough
data. In Proceedings of SIGKDD 2002, pages 133-142,
2002.
[11] D. Kelly and J. Teevan. Implicit feedback for inferring user
preference: A bibliography. SIGIR Forum, 37(2):18-28,
2003.
[12] J. Lafferty and C. Zhai. Document language models, query
models, and risk minimization for information retrieval. In
Proceedings of SIGIR"01, pages 111-119, 2001.
[13] T. Lau and E. Horvitz. Patterns of search: Analyzing and
modeling web query refinement. In Proceedings of the
Seventh International Conference on User Modeling (UM),
pages 145 -152, 1999.
[14] V. Lavrenko and B. Croft. Relevance-based language
models. In Proceedings of SIGIR"01, pages 120-127, 2001.
[15] M. Mitra, A. Singhal, and C. Buckley. Improving automatic
query expansion. In Proceedings of SIGIR 1998, pages
206-214, 1998.
[16] My Yahoo! http://mysearch.yahoo.com.
[17] G. Nunberg. As google goes, so goes the nation. New York
Times, May 2003.
[18] S. E. Robertson. The probability ranking principle in ı˚.
Journal of Documentation, 33(4):294-304, 1977.
[19] J. J. Rocchio. Relevance feedback in information retrieval. In
The SMART Retrieval System: Experiments in Automatic
Document Processing, pages 313-323. Prentice-Hall Inc.,
1971.
[20] G. Salton and C. Buckley. Improving retrieval performance
by retrieval feedback. Journal of the American Society for
Information Science, 41(4):288-297, 1990.
[21] G. Salton and M. J. McGill. Introduction to Modern
Information Retrieval. McGraw-Hill, 1983.
[22] X. Shen, B. Tan, and C. Zhai. Context-sensitive information
retrieval using implicit feedback. In Proceedings of SIGIR
2005, pages 43-50, 2005.
[23] X. Shen and C. Zhai. Exploiting query history for document
ranking in interactive information retrieval (Poster). In
Proceedings of SIGIR 2003, pages 377-378, 2003.
[24] A. Singhal. Modern information retrieval: A brief overview.
Bulletin of the IEEE Computer Society Technical Committee
on Data Engineering, 24(4):35-43, 2001.
[25] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web
search based on user profile constructed without any effort
from users. In Proceedings of WWW 2004, pages 675-684,
2004.
[26] E. Volokh. Personalization and privacy. Communications of
the ACM, 43(8):84-88, 2000.
[27] R. W. White, J. M. Jose, C. J. van Rijsbergen, and
I. Ruthven. A simulated study of implicit feedback models.
In Proceedings of ECIR 2004, pages 311-326, 2004.
[28] J. Xu and W. B. Croft. Query expansion using local and
global document analysis. In Proceedings of SIGIR 1996,
pages 4-11, 1996.
[29] C. Zhai and J. Lafferty. Model-based feedback in KL
divergence retrieval model. In Proceedings of the CIKM
2001, pages 403-410, 2001.
831
Ranking Web Objects from Multiple Communities
Le Chen
∗
Le.Chen@idiap.ch
Lei Zhang
leizhang@
microsoft.com
Feng Jing
fengjing@
microsoft.com
Ke-Feng Deng
kefengdeng@hotmail.com
Wei-Ying Ma
wyma@microsoft.com
Microsoft Research Asia
5F, Sigma Center, No. 49, Zhichun Road
Haidian District, Beijing, 100080, P R China
ABSTRACT
Vertical search is a promising direction as it leverages 
domainspecific knowledge and can provide more precise information
for users. In this paper, we study the Web object-ranking
problem, one of the key issues in building a vertical search
engine. More specifically, we focus on this problem in cases
when objects lack relationships between different Web 
communities, and take high-quality photo search as the test bed
for this investigation. We proposed two score fusion methods
that can automatically integrate as many Web communities
(Web forums) with rating information as possible. The 
proposed fusion methods leverage the hidden links discovered
by a duplicate photo detection algorithm, and aims at 
minimizing score differences of duplicate photos in different 
forums. Both intermediate results and user studies show the
proposed fusion methods are practical and efficient solutions
to Web object ranking in cases we have described. Though
the experiments were conducted on high-quality photo 
ranking, the proposed algorithms are also applicable to other
ranking problems, such as movie ranking and music 
ranking.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: 
Information Search and Retrieval; G.2.2 [Discrete 
Mathematics]: Graph Theory; H.3.5 [Information Storage and 
Retrieval]: Online Information Services - Web-based services
General Terms
Algorithms, Experimentation
1. INTRODUCTION
Despite numerous refinements and optimizations, general
purpose search engines still fail to find relevant results for
many queries. As a new trend, vertical search has shown
promise because it can leverage domain-specific knowledge
and is more effective in connecting users with the 
information they want. There are many vertical search engines,
including some for paper search (e.g. Libra [21], Citeseer
[7] and Google Scholar [4]), product search (e.g. Froogle
[5]), movie search [6], image search [1, 8], video search [6],
local search [2], as well as news search [3]. We believe the
vertical search engine trend will continue to grow.
Essentially, building vertical search engines includes data
crawling, information extraction, object identification and
integration, and object-level Web information retrieval (or
Web object ranking) [20], among which ranking is one of the
most important factors. This is because it deals with the
core problem of how to combine and rank objects coming
from multiple communities.
Although object-level ranking has been well studied in
building vertical search engines, there are still some kinds
of vertical domains in which objects cannot be effectively
ranked. For example, algorithms that evolved from 
PageRank [22], PopRank [21] and LinkFusion [27] were proposed
to rank objects coming from multiple communities, but can
only work on well-defined graphs of heterogeneous data.
Well-defined means that like objects (e.g. authors in 
paper search) can be identified in multiple communities (e.g.
conferences). This allows heterogeneous objects to be well
linked to form a graph through leveraging all the 
relationships (e.g. cited-by, authored-by and published-by) among
the multiple communities.
However, this assumption does not always stand for some
domains. High-quality photo search, movie search and news
search are exceptions. For example, a photograph forum
377
website usually includes three kinds of objects: photos, 
authors and reviewers. Yet different photo forums seem to
lack any relationships, as there are no cited-by relationships.
This makes it difficult to judge whether two authors cited
are the same author, or two photos are indeed identical 
photos. Consequently, although each photo has a rating score
in a forum, it is non-trivial to rank photos coming from 
different photo forums. Similar problems also exist in movie
search and news search. Although two movie titles can be
identified as the same one by title and director in different
movie discussion groups, it is non-trivial to combine 
rating scores from different discussion groups and rank movies
effectively. We call such non-trivial object relationship in
which identification is difficult, incomplete relationships.
Other related work includes rank aggregation for the Web
[13, 14], and learning algorithm for rank, such as RankBoost
[15], RankSVM [17, 19], and RankNet [12]. We will contrast
differences of these methods with the proposed methods 
after we have described the problem and our methods.
We will specifically focus on Web object-ranking 
problem in cases that lack object relationships or have with 
incomplete object relationships, and take high-quality photo
search as the test bed for this investigation. In the following,
we will introduce rationale for building high-quality photo
search.
1.1 High-Quality Photo Search
In the past ten years, the Internet has grown to become
an incredible resource, allowing users to easily access a huge
number of images. However, compared to the more than 1
billion images indexed by commercial search engines, actual
queries submitted to image search engines are relatively 
minor, and occupy only 8-10 percent of total image and text
queries submitted to commercial search engines [24]. This
is partially because user requirements for image search are
far less than those for general text search. On the other
hand, current commercial search engines still cannot well
meet various user requirements, because there is no 
effective and practical solution to understand image content.
To better understand user needs in image search, we 
conducted a query log analysis based on a commercial search
engine. The result shows that more than 20% of image
search queries are related to nature and places and daily
life categories. Users apparently are interested in enjoying
high-quality photos or searching for beautiful images of 
locations or other kinds. However, such user needs are not
well supported by current image search engines because of
the difficulty of the quality assessment problem.
Ideally, the most critical part of a search engine - the
ranking function - can be simplified as consisting of two
key factors: relevance and quality. For the relevance 
factor, search in current commercial image search engines 
provide most returned images that are quite relevant to queries,
except for some ambiguity. However, as to quality factor,
there is still no way to give an optimal rank to an image.
Though content-based image quality assessment has been
investigated over many years [23, 25, 26], it is still far from
ready to provide a realistic quality measure in the immediate
future.
Seemingly, it really looks pessimistic to build an image
search engine that can fulfill the potentially large 
requirement of enjoying high-quality photos. Various proliferating
Web communities, however, notices us that people today
have created and shared a lot of high-quality photos on the
Web on virtually any topics, which provide a rich source for
building a better image search engine.
In general, photos from various photo forums are of higher
quality than personal photos, and are also much more 
appealing to public users than personal photos. In addition,
photos uploaded to photo forums generally require rich 
metadata about title, camera setting, category and description to
be provide by photographers. These metadata are actually
the most precise descriptions for photos and undoubtedly
can be indexed to help search engines find relevant results.
More important, there are volunteer users in Web 
communities actively providing valuable ratings for these photos.
The rating information is generally of great value in solving
the photo quality ranking problem.
Motivated by such observations, we have been attempting
to build a vertical photo search engine by extracting rich
metadata and integrating information form various photo
Web forums. In this paper, we specifically focus on how to
rank photos from multiple Web forums.
Intuitively, the rating scores from different photo forums
can be empirically normalized based on the number of 
photos and the number of users in each forum. However, such
a straightforward approach usually requires large manual
effort in both tedious parameter tuning and subjective 
results evaluation, which makes it impractical when there are
tens or hundreds of photo forums to combine. To address
this problem, we seek to build relationships/links between
different photo forums. That is, we first adopt an efficient
algorithm to find duplicate photos which can be considered
as hidden links connecting multiple forums. We then 
formulate the ranking challenge as an optimization problem,
which eventually results in an optimal ranking function.
1.2 Main Contributions and Organization.
The main contributions of this paper are:
1. We have proposed and built a vertical image search 
engine by leveraging rich metadata from various photo
forum Web sites to meet user requirements of searching
for and enjoying high-quality photos, which is 
impossible in traditional image search engines.
2. We have proposed two kinds of Web object-ranking
algorithms for photos with incomplete relationships,
which can automatically and efficiently integrate as
many as possible Web communities with rating 
information and achieves an equal qualitative result 
compared with the manually tuned fusion scheme.
The rest of this paper is organized as follows. In Section
2, we present in detail the proposed solutions to the 
ranking problem, including how to find hidden links between
different forums, normalize rating scores, obtain the 
optimal ranking function, and contrast our methods with some
other related research. In Section 3, we describe the 
experimental setting and experiments and user studies conducted
to evaluate our algorithm. Our conclusion and a discussion
of future work is in Section 4.
It is worth noting that although we treat vertical photo
search as the test bed in this paper, the proposed ranking
algorithm can also be applied to rank other content that
includes video clips, poems, short stories, drawings, 
sculptures, music, and so on.
378
2. ALGORITHM
2.1 Overview
The difficulty of integrating multiple Web forums is in
their different rating systems, where there are generally two
kinds of freedom. The first kind of freedom is the rating
interval or rating scale including the minimal and maximal
ratings for each Web object. For example, some forums use
a 5-point rating scale whereas other forums use 3-point or
10-point rating scales. It seems easy to fix this freedom, but
detailed analysis of the data and experiments show that it
is a non-trivial problem.
The second kind of freedom is the varying rating criteria
found in different Web forums. That is, the same score does
not mean the same quality in different forums. Intuitively, if
we can detect same photographers or same photographs, we
can build relationships between any two photo forums and
therefore can standardize the rating criterion by score 
normalization and transformation. Fortunately, we find that
quite a number of duplicate photographs exist in various
Web photo forums. This fact is reasonable when 
considering that photographers sometimes submit a photo to more
than one forum to obtain critiques or in hopes of widespread
publicity. In this work, we adopt an efficient duplicate photo
detection algorithm [10] to find these photos.
The proposed methods below are based on the following
considerations. Faced with the need to overcome a ranking
problem, a standardized rating criterion rather than a 
reasonable rating criterion is needed. Therefore, we can take
a large scale forum as the reference forum, and align other
forums by taking into account duplicate Web objects 
(duplicate photos in this work). Ideally, the scores of duplicate
photos should be equal even though they are in different
forums. Yet we can deem that scores in different 
forumsexcept for the reference forum - can vary in a parametric
space. This can be determined by minimizing the objective
function defined by the sum of squares of the score 
differences. By formulating the ranking problem as an 
optimization problem that attempts to make the scores of duplicate
photos in non-reference forums as close as possible to those
in the reference forum, we can effectively solve the ranking
problem.
For convenience, the following notations are employed.
Ski and ¯Ski denote the total score and mean score of ith Web
object (photo) in the kth Web site, respectively. The total
score refers to the sum of the various rating scores (e.g., 
novelty rating and aesthetic rating), and the mean score refers
to the mean of the various rating scores. Suppose there are
a total of K Web sites. We further use
{Skl
i |i = 1, ..., Ikl; k, l = 1, ..., K; k = l}
to denote the set of scores for Web objects (photos) in kth
Web forums that are duplicate with the lth Web forums,
where Ikl is the total number of duplicate Web objects 
between these two Web sites. In general, score fusion can be
seen as the procedure of finding K transforms
ψk(Ski) = eSki, k = 1, ..., K
such that eSki can be used to rank Web objects from different
Web sites. The objective function described in the above
Figure 1: Web community integration. Each Web
community forms a subgraph, and all communities
are linked together by some hidden links (dashed
lines).
paragraph can then be formulated as
min
{ψk|k=2,...,K}
KX
k=2
Ik1X
i=1
¯wk
i

S1k
i − ψk(Sk1
i )
2
(1)
where we use k = 1 as the reference forum and thus ψ1(S1i) =
S1i. ¯wk
i (≥ 0) is the weight coefficient that can be set 
heuristically according to the numbers of voters (reviewers or 
commenters) in both the reference forum and the non-reference
forum. The more reviewers, the more popular the photo is
and the larger the corresponding weight ¯wk
i should be. In
this work, we do not inspect the problem of how to choose ¯wk
i
and simply set them to one. But we believe the proper use
of ¯wk
i , which leverages more information, can significantly
improve the results.
Figure 1 illustrates the aforementioned idea. The Web
Community 1 is the reference community. The dashed lines
are links indicating that the two linked Web objects are 
actually the same. The proposed algorithm will try to find the
best ψk(k = 2, ..., K), which has certain parametric forms
according to certain models. So as to minimize the cost
function defined in Eq. 1, the summation is taken on all the
red dashed lines.
We will first discuss the score normalization methods in
Section 2.2, which serves as the basis for the following work.
Before we describe the proposed ranking algorithms, we first
introduce a manually tuned method in Section 2.3, which is
laborious and even impractical when the number of 
communities become large. In Section 2.4, we will briefly explain
how to precisely find duplicate photos between Web forums.
Then we will describe the two proposed methods: Linear 
fusion and Non-linear fusion, and a performance measure for
result evaluation in Section 2.5. Finally, in Section 2.6 we
will discuss the relationship of the proposed methods with
some other related work.
2.2 Score Normalization
Since different Web (photo) forums on the Web usually
have different rating criteria, it is necessary to normalize
them before applying different kinds of fusion methods. In
addition, as there are many kinds of ratings, such as 
ratings for novelty, ratings for aesthetics etc, it is reasonable
to choose a common one - total score or average 
scorethat can always be extracted in any Web forum or 
calculated by corresponding ratings. This allows the 
normaliza379
tion method on the total score or average score to be viewed
as an impartial rating method between different Web 
forums.
It is straightforward to normalize average scores by 
linearly transforming them to a fixed interval. We call this
kind of score as Scaled Mean Score. The difficulty, however,
of using this normalization method is that, if there are only
a few users rating an object, say a photo in a photo forum,
the average score for the object is likely to be spammed or
skewed.
Total score can avoid such drawbacks that contain more
information such as a Web object"s quality and popularity.
The problem is thus how to normalize total scores in 
different Web forums. The simplest way may be normalization
by the maximal and minimal scores. The drawback of this
normalization method is it is non robust, or in other words,
it is sensitive to outliers.
To make the normalization insensitive to unusual data,
we propose the Mode-90% Percentile normalization method.
Here, the mode score represents the total score that has been
assigned to more photos than any other total score. And The
high percentile score (e.g.,90%) represents the total score for
which the high percentile of images have a lower total score.
This normalization method utilizes the mode and 90% 
percentile as two reference points to align two rating systems,
which makes the distributions of total scores in different 
forums more consistent. The underlying assumption, for 
example in different photo forums, is that even the qualities of
top photos in different forums may vary greatly and be less
dependent on the forum quality, the distribution of photos
of middle-level quality (from mode to 90% percentile) should
be almost of the same quality up to the freedom which 
reflects the rating criterion (strictness) of Web forums. 
Photos of this middle-level in a Web forum usually occupy more
than 70 % of total photos in that forum.
We will give more detailed analysis of the scores in Section
3.2.
2.3 Manual Fusion
The Web movie forum, IMDB [16], proposed to use a
Bayesian-ranking function to normalize rating scores within
one community. Motivated by this ranking function, we 
propose this manual fusion method: For the kth Web site, we
use the following formula
eSki = αk ·
„
nk · ¯Ski
nk + n∗
k
+
n∗
k · S∗
k
nk + n∗
k
«
(2)
to rank photos, where nk is the number of votes and n∗
k,
S∗
k and αk are three parameters. This ranking function first
takes a balance between the original mean score ¯Ski and a
reference score S∗
k to get a weighted mean score which may
be more reliable than ¯Ski. Then the weighted mean score is
scaled by αk to get the final score fSki.
For n Web communities, there are then about 3n 
parameters in {(αk, n∗
k, S∗
k)|k = 1, ..., n} to tune. Though this
method can achieves pretty good results after careful and
thorough manual tuning on these parameters, when n 
becomes increasingly large, say there are tens or hundreds of
Web communities crawled and indexed, this method will 
become more and more laborious and will eventually become
impractical. It is therefore desirable to find an effective 
fusion method whose parameters can be automatically 
determined.
2.4 Duplicate Photo Detection
We use Dedup [10], an efficient and effective duplicate 
image detection algorithm, to find duplicate photos between
any two photo forums. This algorithm uses hash function
to map a high dimensional feature to a 32 bits hash code
(see below for how to construct the hash code). Its 
computational complexity to find all the duplicate images among
n images is about O(n log n). The low-level visual feature
for each photo is extracted on k × k regular grids. Based
on all features extracted from the image database, a PCA
model is built. The visual features are then transformed to
a relatively low-dimensional and zero mean PCA space, or
29 dimensions in our system. Then the hash code for each
photo is built as follows: each dimension is transformed to
one, if the value in this dimension is greater than 0, and 0
otherwise. Photos in the same bucket are deemed potential
duplicates and are further filtered by a threshold in terms
of Euclidean similarity in the visual feature space.
Figure 2 illustrates the hashing procedure, where visual
features - mean gray values - are extracted on both 6 × 6
and 7×7 grids. The 85-dimensional features are transformed
to a 32-dimensional vector, and the hash code is generated
according to the signs.
Figure 2: Hashing procedure for duplicate photo
dectection
2.5 Score Fusion
In this section, we will present two solutions on score 
fusion based on different parametric form assumptions of ψk
in Eq. 1.
2.5.1 Linear Fusion by Duplicate Photos
Intuitively, the most straightforward way to factor out the
uncertainties caused by the different criterion is to scale, 
rel380
ative to a given center, the total scores of each unreferenced
Web photo forum with respect to the reference forum. More
strictly, we assume ψk has the following form
ψk(Ski) = αkSki + tk, k = 2, ..., K (3)
ψ1(S1i) = S1i (4)
which means that the scores of k(= 1)th forum should be
scaled by αk relative to the center tk
1−αk
as shown in Figure
3.
Then, if we substitute above ψk to Eq. 1, we get the
following objective function,
min
{αk,tk|k=2,...,K}
KX
k=2
Ik1X
i=1
¯wk
i
h
S1k
i − αkSk1
i − tk
i2
. (5)
By solving the following set of functions,
(
∂f
∂αk
= = 0
∂f
∂tk
= 0
, k = 1, ..., K
where f is the objective function defined in Eq. 5, we get
the closed form solution as:
„
αk
tk
«
= A−1
k Lk (6)
where
Ak =
„ P
i ¯wi(Sk1
i )2 P
i ¯wiSk1
iP
i ¯wiSk1
i
P
i ¯wi
«
(7)
Lk =
„ P
i ¯wiS1k
i Sk1
iP
i ¯wiS1k
i
«
(8)
and k = 2, ..., K.
This is a linear fusion method. It enjoys simplicity and
excellent performance in the following experiments.
Figure 3: Linear Fusion method
2.5.2 Nonlinear Fusion by Duplicate Photos
Sometimes we want a method which can adjust scores on
intervals with two endpoints unchanged. As illustrated in
Figure 4, the method can tune scores between [C0, C1] while
leaving scores C0 and C1 unchanged. This kind of fusion
method is then much finer than the linear ones and 
contains many more parameters to tune and expect to further
improve the results.
Here, we propose a nonlinear fusion solution to satisfy
such constraints. First, we introduce a transform:
ηc0,c1,α(x) =
( 
x−c0
c1−c0
α
(c1 − c0) + c0, if x ∈ (c0, c1]
x otherwise
where α > 0. This transform satisfies that for x ∈ [c0, c1],
ηc0,c1,α(x) ∈ [c0, c1] with ηc0,c1,α(c0) = c0 and ηc0,c1,α(c1) =
c1. Then we can utilize this nonlinear transform to adjust
the scores in certain interval, say (M, T],
ψk(Ski) = ηM,T,α(Ski) . (9)
Figure 4: Nonlinear Fusion method. We intent to
finely adjust the shape of the curves in each segment.
Even there is no closed-form solution for the following
optimization problem,
min
{αk|k∈[2,K]}
KX
k=2
Ik1X
i=1
¯wk
i
h
S1k
i − ηM,T,α(Ski)
i2
it is not hard to get the numeric one. Under the same 
assumptions made in Section 2.2, we can use this method to
adjust scores of the middle-level (from the mode point to
the 90 % percentile).
This more complicated non-linear fusion method is 
expected to achieve better results than the linear one. 
However, difficulties in evaluating the rank results block us from
tuning these parameters extensively. The current 
experiments in Section 3.5 do not reveal any advantages over the
simple linear model.
2.5.3 Performance Measure of the Fusion Results
Since our objective function is to make the scores of the
same Web objects (e.g. duplicate photos) between a 
nonreference forum and the reference forum as close as possible,
it is natural to investigate how close they become to each
other and how the scores of the same Web objects change
between the two non-reference forums before and after score
fusion.
Taken Figure 1 as an example, the proposed algorithms
minimize the score differences of the same Web objects in
two Web forums: the reference forum (the Web Community
1) and a non-reference forum, which corresponds to 
minimizing the objective function on the red dashed (hidden)
links. After the optimization, we must ask what happens to
the score differences of the same Web objects in two 
nonreference forums? Or, in other words, whether the scores
of two objects linked by the green dashed (hidden) links
become more consistent?
We therefore define the following performance 
measureδ measure - to quantify the changes for scores of the same
Web objects in different Web forums as
δkl = Sim(Slk
, Skl
) − Sim(Slk
∗ , Skl
∗ ) (10)
381
where Skl
= (Skl
1 , ..., Skl
Ikl
)T
, Skl
∗ = (eSkl
1 , ..., eSkl
Ikl
)T
and
Sim(a, b) =
a · b
||a||||b||
.
δkl > 0 means after score fusion, scores on the same Web
objects between kth and lth Web forum become more 
consistent, which is what we expect. On the contrary, if δkl < 0,
those scores become more inconsistent.
Although we cannot rely on this measure to evaluate our
final fusion results as ranking photos by their popularity and
qualities is such a subjective process that every person can
have its own results, it can help us understand the 
intermediate ranking results and provide insights into the final
performances of different ranking methods.
2.6 Contrasts with Other Related Work
We have already mentioned the differences of the proposed
methods with the traditional methods, such as PageRank
[22], PopRank [21], and LinkFusion [27] algorithms in 
Section 1. Here, we discuss some other related works.
The current problem can also be viewed as a rank 
aggregation one [13, 14] as we deal with the problem of how to
combine several rank lists. However, there are 
fundamental differences between them. First of all, unlike the Web
pages, which can be easily and accurately detected as the
same pages, detecting the same photos in different Web 
forums is a non-trivial work, and can only be implemented by
some delicate algorithms while with certain precision and
recall. Second, the numbers of the duplicate photos from
different Web forums are small relative to the whole photo
sets (see Table 1). In another words, the top K rank lists
of different Web forums are almost disjointed for a given
query. Under this condition, both the algorithms proposed
in [13] and their measurements - Kendall tau distance or
Spearman footrule distance - will degenerate to some 
trivial cases.
Another category of rank fusion (aggregation) methods is
based on machine learning algorithms, such as RankSVM
[17, 19], RankBoost [15], and RankNet [12]. All of these
methods entail some labelled datasets to train a model. In
current settings, it is difficult or even impossible to get these
datasets labelled as to their level of professionalism or 
popularity, since the photos are too vague and subjective to rank.
Instead, the problem here is how to combine several ordered
sub lists to form a total order list.
3. EXPERIMENTS
In this section, we carry out our research on high-quality
photo search. We first briefly introduce the newly proposed
vertical image search engine - EnjoyPhoto in section 3.1.
Then we focus on how to rank photos from different Web
forums. In order to do so, we first normalize the scores
(ratings) for photos from different multiple Web forums in
section 3.2. Then we try to find duplicate photos in section
3.3. Some intermediate results are discussed using δ measure
in section 3.4. Finally a set of user studies is carried out
carefully to justify our proposed method in section 3.5.
3.1 EnjoyPhoto: high-quality Photo Search
Engine
In order to meet user requirement of enjoying high-quality
photos, we propose and build a high-quality photo search 
engine - EnjoyPhoto, which accounts for the following three
key issues: 1. how to crawl and index photos, 2. how to
determine the qualities of each photo and 3. how to 
display the search results in order to make the search process
enjoyable. For a given text based query, this system ranks
the photos based on certain combination of relevance of the
photo to this query (Issue 1) and the quality of the photo
(Issue 2), and finally displays them in an enjoyable manner
(Issue 3).
As for Issue 3, we devise the interface of the system 
deliberately in order to smooth the users" process of enjoying
high-quality photos. Techniques, such as Fisheye and slides
show, are utilized in current system. Figure 5 shows the
interface. We will not talk more about this issue as it is not
an emphasis of this paper.
Figure 5: EnjoyPhoto: an enjoyable high-quality
photo search engine, where 26,477 records are 
returned for the query fall in about 0.421 seconds
As for Issue 1, we extracted from a commercial search 
engine a subset of photos coming from various photo forums
all over the world, and explicitly parsed the Web pages 
containing these photos. The number of photos in the data 
collection is about 2.5 million. After the parsing, each photo
was associated with its title, category, description, camera
setting, EXIF data 1
(when available for digital images), 
location (when available in some photo forums), and many
kinds of ratings. All these metadata are generally precise
descriptions or annotations for the image content, which are
then indexed by general text-based search technologies [9,
18, 11]. In current system, the ranking function was 
specifically tuned to emphasize title, categorization, and rating
information.
Issue 2 is essentially dealt with in the following sections
which derive the quality of photos by analyzing ratings 
provided by various Web photo forums. Here we chose six photo
forums to study the ranking problem and denote them as
Web-A, Web-B, Web-C, Web-D, Web-E and Web-F.
3.2 Photo Score Normalization
Detailed analysis of different score normalization 
methods are analyzed in this section. In this analysis, the zero
1
Digital cameras save JPEG (.jpg) files with EXIF 
(Exchangeable Image File) data. Camera settings and scene
information are recorded by the camera into the image file.
www.digicamhelp.com/what-is-exif/
382
0 2 4 6 8 10
0
1000
2000
3000
4000
Normalized Score
TotalNumber
(a) Web-A
0 2 4 6 8 10
0
0.5
1
1.5
2
2.5
3
x 10
4
Normalized Score
TotalNumber
(b) Web-B
0 2 4 6 8 10
0
0.5
1
1.5
2
x 10
5
Normalized Score
TotalNumber
(c) Web-C
0 2 4 6 8 10
0
2
4
6
8
10
x 10
4
Normalized Score
TotalNumber
(d) Web-D
0 2 4 6 8 10
0
2000
4000
6000
8000
10000
12000
14000
Normalized Score
TotalNumber
(e) Web-E
0 2 4 6 8 10
0
1
2
3
4
5
6
x 10
4
Normalized Score
TotalNumber
(f) Web-F
Figure 6: Distributions of mean scores normalized
to [0, 10]
scores that usually occupy about than 30% of the total 
number of photos for some Web forums are not currently taken
into account. How to utilize these photos is left for future
explorations.
In Figure 6, we list the distributions of the mean score,
which is transformed to a fixed interval [0, 10]. The 
distributions of the average scores of these Web forums look quite
different. Distributions in Figure 6(a), 6(b), and 6(e) looks
like Gaussian distributions, while those in Figure 6(d) and
6(f) are dominated by the top score. The reason of these
eccentric distributions for Web-D and Web-F lies in their
coarse rating systems. In fact, Web-D and Web-F use 2 or
3 point rating scales whereas other Web forums use 7 or 14
point rating scales. Therefore, it will be problematic if we
directly use these averaged scores. Furthermore the average
score is very likely to be spammed, if there are only a few
users rating a photo.
Figure 7 shows the total score normalization method by
maximal and minimal scores, which is one of our base line
system. All the total scores of a given Web forum are 
normalized to [0, 100] according to the maximal score and 
minimal score of corresponding Web forum. We notice that total
score distribution of Web-A in Figure 7(a) has two larger
tails than all the others. To show the shape of the 
distributions more clearly, we only show the distributions on [0, 25]
in Figure 7(b),7(c),7(d),7(e), and 7(f).
Figure 8 shows the Mode-90% Percentile normalization
method, where the modes of the six distributions are 
normalized to 5 and the 90% percentile to 8. We can see that
this normalization method makes the distributions of total
scores in different forums more consistent. The two proposed
algorithms are all based on these normalization methods.
3.3 Duplicate photo detection
Targeting at computational efficiency, the Dedup 
algorithm may lose some recall rate, but can achieve a high
precision rate. We also focus on finding precise hidden links
rather than all hidden links. Figure 9 shows some duplicate
detection examples. The results are shown in Table 1 and
verify that large numbers of duplicate photos exist in any
two Web forums even with the strict condition for Dedup
where we chose first 29 bits as the hash code. Since there
are only a few parameters to estimate in the proposed fusion
methods, the numbers of duplicate photos shown Table 1 are
0 20 40 60 80 100
0
100
200
300
400
500
600
Normalized Score
TotalNumber
(a) Web-A
0 5 10 15 20 25
0
1
2
3
4
5
x 10
4
Normalized Score
TotalNumber
(b) Web-B
0 5 10 15 20 25
0
1
2
3
4
5
x 10
5
Normalized Score
TotalNumber
(c) Web-C
0 5 10 15 20 25
0
0.5
1
1.5
2
2.5
x 10
4
Normalized Score
TotalNumber
(d) Web-D
0 5 10 15 20 25
0
2000
4000
6000
8000
10000
Normalized Score
TotalNumber
(e) Web-E
0 5 10 15 20 25
0
0.5
1
1.5
2
2.5
3
x 10
4
Normalized Score
TotalNumber
(f) Web-F
Figure 7: Maxmin Normalization
0 5 10 15
0
200
400
600
800
1000
1200
1400
Normalized Score
TotalNumber
(a) Web-A
0 5 10 15
0
1
2
3
4
5
x 10
4
Normalized Score
TotalNumber
(b) Web-B
0 5 10 15
0
2
4
6
8
10
12
14
x 10
4
Normalized Score
TotalNumber
(c) Web-C
0 5 10 15
0
0.5
1
1.5
2
2.5
x 10
4
Normalized Score
TotalNumber
(d) Web-D
0 5 10 15
0
2000
4000
6000
8000
10000
12000
Normalized Score
TotalNumber
(e) Web-E
0 5 10 15
0
2000
4000
6000
8000
10000
Normalized Score
TotalNumber
(f) Web-F
Figure 8: Mode-90% Percentile Normalization
sufficient to determine these parameters. The last table 
column lists the total number of photos in the corresponding
Web forums.
3.4 δ Measure
The parameters of the proposed linear and nonlinear 
algorithms are calculated using the duplicate data shown in
Table 1, where the Web-C is chosen as the reference Web
forum since it shares the most duplicate photos with other
forums.
Table 2 and 3 show the δ measure on the linear model and
nonlinear model. As δkl is symmetric and δkk = 0, we only
show the upper triangular part. The NaN values in both
tables lie in that no duplicate photos have been detected by
the Dedup algorithm as reported in Table 1.
The linear model guarantees that the δ measures related
Table 1: Number of duplicate photos between each
pair of Web forums
A B C D E F Scale
A 0 316 1,386 178 302 0 130k
B 316 0 14,708 909 8,023 348 675k
C 1,386 14,708 0 1,508 19,271 1,083 1,003k
D 178 909 1,508 0 1,084 21 155k
E 302 8,023 19,271 1,084 0 98 448k
F 0 348 1,083 21 98 0 122k
383
Figure 9: Some results of duplicate photo detection
Table 2: δ measure on the linear model.
Web-B Web-C Web-D Web-E Web-F
Web-A 0.0659 0.0911 0.0956 0.0928 NaN
Web-B - 0.0672 0.0578 0.0791 0.4618
Web-C - - 0.0105 0.0070 0.2220
Web-D - - - 0.0566 0.0232
Web-E - - - - 0.6525
to the reference community should be no less than 0 
theoretically. It is indeed the case (see the underlined numbers
in Table 2). But this model can not guarantee that the δ
measures on the non-reference communities can also be no
less than 0, as the normalization steps are based on 
duplicate photos between the reference community and a 
nonreference community. Results shows that all the numbers in
the δ measure are greater than 0 (see all the non-underlined
numbers in Table 2), which indicates that it is probable that
this model will give optimal results.
On the contrary, the nonlinear model does not guarantee
that δ measures related to the reference community should
be no less than 0, as not all duplicate photos between the
two Web forums can be used when optimizing this model.
In fact, the duplicate photos that lie in different intervals
will not be used in this model. It is these specific duplicate
photos that make the δ measure negative. As a result, there
are both negative and positive items in Table 3, but overall
the number of positive ones are greater than negative ones
(9:5), that indicates the model may be better than the 
normalization only method (see next subsection) which has an
all-zero δ measure, and worse than the linear model.
3.5 User Study
Because it is hard to find an objective criterion to evaluate
Table 3: δ measure on the nonlinear model.
Web-B Web-C Web-D Web-E Web-F
Web-A 0.0559 0.0054 -0.0185 -0.0054 NaN
Web-B - -0.0162 -0.0345 -0.0301 0.0466
Web-C - - 0.0136 0.0071 0.1264
Web-D - - - 0.0032 0.0143
Web-E - - - - 0.214
which ranking function is better, we chose to employ user
studies for subjective evaluations. Ten subjects were invited
to participate in the user study. They were recruited from
nearby universities. As search engines of both text search
and image search are familiar to university students, there
was no prerequisite criterion for choosing students.
We conducted user studies using Internet Explorer 6.0 on
Windows XP with 17-inch LCD monitors set at 1,280 pixels
by 1,024 pixels in 32-bit color. Data was recorded with
server logs and paper-based surveys after each task.
Figure 10: User study interface
We specifically device an interface for user study as shown
in Figure 10. For each pair of fusion methods, participants
were encouraged to try any query they wished. For those
without specific ideas, two combo boxes (category list and
query list) were listed on the bottom panel, where the top
1,000 image search queries from a commercial search engine
were provided. After a participant submitted a query, the
system randomly selected the left or right frame to display
each of the two ranking results. The participant were then
required to judge which ranking result was better of the two
ranking results, or whether the two ranking results were of
equal quality, and submit the judgment by choosing the 
corresponding radio button and clicking the Submit button.
For example, in Figure 10, query sunset is submitted to
the system. Then, 79,092 photos were returned and ranked
by the Minmax fusion method in the left frame and linear
fusion method in the right frame. A participant then 
compares the two ranking results (without knowing the ranking
methods) and submits his/her feedback by choosing answers
in the Your option.
Table 4: Results of user study
Norm.Only Manually Linear
Linear 29:13:10 
14:22:15Nonlinear 29:15:9 12:27:12 6:4:45
Table 4 shows the experimental results, where Linear
denotes the linear fusion method, Nonlinear denotes the
non linear fusion method, Norm. Only means Maxmin
normalization method, Manually means the manually tuned
method. The three numbers in each item, say 29:13:10,
mean that 29 judgments prefer the linear fusion results, 10
384
judgments prefer the normalization only method, and 13
judgments consider these two methods as equivalent.
We conduct the ANOVA analysis, and obtain the 
following conclusions:
1. Both the linear and nonlinear methods are significantly
better than the Norm. Only method with respective
P-values 0.00165(< 0.05) and 0.00073(<< 0.05). This
result is consistent with the δ-measure evaluation 
result. The Norm. Only method assumes that the top
10% photos in different forums are of the same 
quality. However, this assumption does not stand in 
general. For example, a top 10% photo in a top tier photo
forum is generally of higher quality than a top 10%
photo in a second-tier photo forum. This is similar
to that, those top 10% students in a top-tier 
university and those in a second-tier university are generally
of different quality. Both linear and nonlinear fusion
methods acknowledge the existence of such differences
and aim at quantizing the differences. Therefore, they
perform better than the Norm. Only method.
2. The linear fusion method is significantly better than
the nonlinear one with P-value 1.195 × 10−10
. This
result is rather surprising as this more complicated
ranking method is expected to tune the ranking more
finely than the linear one. The main reason for this
result may be that it is difficult to find the best 
intervals where the nonlinear tuning should be carried out
and yet simply the middle part of the Mode-90% 
Percentile Normalization method was chosen. The 
timeconsuming and subjective evaluation methods - user
studies - blocked us extensively tuning these 
parameters.
3. The proposed linear and nonlinear methods perform
almost the same with or slightly better than the 
manually tuned method. Given that the linear/nonlinear
fusion methods are fully automatic approaches, they
are considered practical and efficient solutions when
more communities (e.g. dozens of communities) need
to be integrated.
4. CONCLUSIONS AND FUTURE WORK
In this paper, we studied the Web object-ranking 
problem in the cases of lacking object relationships where 
traditional ranking algorithms are no longer valid, and took
high-quality photo search as the test bed for this 
investigation. We have built a vertical high-quality photo search
engine, and proposed score fusion methods which can 
automatically integrate as many data sources (Web forums) as
possible. The proposed fusion methods leverage the hidden
links discovered by duplicate photo detection algorithm, and
minimize score differences of duplicate photos in different
forums. Both the intermediate results and the user 
studies show that the proposed fusion methods are a practical
and efficient solution to Web object ranking in the 
aforesaid relationships. Though the experiments were conducted
on high-quality photo ranking, the proposed algorithms are
also applicable to other kinds of Web objects including video
clips, poems, short stories, music, drawings, sculptures, and
so on.
Current system is far from being perfect. In order to make
this system more effective, more delicate analysis for the
vertical domain (e.g., Web photo forums) are needed. The
following points, for example, may improve the searching
results and will be our future work: 1. more subtle 
analysis and then utilization of different kinds of ratings (e.g.,
novelty ratings, aesthetic ratings); 2. differentiating various
communities who may have different interests and 
preferences or even distinct culture understandings; 3. 
incorporating more useful information, including photographers" and
reviewers" information, to model the photos in a 
heterogeneous data space instead of the current homogeneous one.
We will further utilize collaborative filtering to recommend
relevant high-quality photos to browsers.
One open problem is whether we can find an objective and
efficient criterion for evaluating the ranking results, instead
of employing subjective and inefficient user studies, which
blocked us from trying more ranking algorithms and tuning
parameters in one algorithm.
5. ACKNOWLEDGMENTS
We thank Bin Wang and Zhi Wei Li for providing Dedup
codes to detect duplicate photos; Zhen Li for helping us
design the interface of EnjoyPhoto; Ming Jing Li, Longbin
Chen, Changhu Wang, Yuanhao Chen, and Li Zhuang etc.
for useful discussions. Special thanks go to Dwight Daniels
for helping us revise the language of this paper.
6. REFERENCES
[1] Google image search. http://images.google.com.
[2] Google local search. http://local.google.com/.
[3] Google news search. http://news.google.com.
[4] Google paper search. http://Scholar.google.com.
[5] Google product search. http://froogle.google.com.
[6] Google video search. http://video.google.com.
[7] Scientific literature digital library.
http://citeseer.ist.psu.edu.
[8] Yahoo image search. http://images.yahoo.com.
[9] R. Baeza-Yates and B. Ribeiro-Neto. Modern
Information Retrieval. New York: ACM Press;
Harlow, England: Addison-Wesley, 1999.
[10] W. Bin, L. Zhiwei, L. Ming Jing, and M. Wei-Ying.
Large-scale duplicate detection for web image search.
In Proceedings of the International Conference on
Multimedia and Expo, page 353, 2006.
[11] S. Brin and L. Page. The anatomy of a large-scale
hypertextual web search engine. In Computer
Networks, volume 30, pages 107-117, 1998.
[12] C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. Learning
to rank using gradient descent. In Proceedings of the
22nd international conference on Machine learning,
pages 89 - 96, 2005.
[13] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar.
Rank aggregation methods for the web. In Proceedings
10th International Conference on World Wide Web,
pages 613 - 622, Hong-Kong, 2001.
[14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing
top k lists. SIAM Journal on Discrete Mathematics,
17(1):134 - 160, 2003.
[15] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An
efficient boosting algorithm for combining preferences.
385
Journal of Machine Learning Research,
4(1):933-969(37), 2004.
[16] IMDB. Formula for calculating the top rated 250 titles
in imdb. http://www.imdb.com/chart/top.
[17] T. Joachims. Optimizing search engines using
clickthrough data. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 133 - 142, 2002.
[18] J. M. Kleinberg. Authoritative sources in a
hyperlinked environment. Journal of the ACM,
46(5):604-632, 1999.
[19] R. Nallapati. Discriminative models for information
retrieval. In Proceedings of the 25th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 64 - 71,
2004.
[20] Z. Nie, Y. Ma, J.-R. Wen, and W.-Y. Ma. Object-level
web information retrieval. In Technical Report of
Microsoft Research, volume MSR-TR-2005-11, 2005.
[21] Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma.
Object-level ranking: Bringing order to web objects.
In Proceedings of the 14th international conference on
World Wide Web, pages 567 - 574, Chiba, Japan,
2005.
[22] L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web.
In Technical report, Stanford Digital Libraries, 1998.
[23] A. Savakis, S. Etz, and A. Loui. Evaluation of image
appeal in consumer photography. In SPIE Human
Vision and Electronic Imaging, pages 111-120, 2000.
[24] D. Sullivan. Hitwise search engine ratings. Search
Engine Watch Articles, http://searchenginewatch.
com/reports/article.php/3099931, August 23, 2005.
[25] S. Susstrunk and S. Winkler. Color image quality on
the internet. In IS&T/SPIE Electronic Imaging 2004:
Internet Imaging V, volume 5304, pages 118-131,
2004.
[26] H. Tong, M. Li, Z. H.J., J. He, and Z. C.S.
Classification of digital photos taken by photographers
or home users. In Pacific-Rim Conference on
Multimedia (PCM), pages 198-205, 2004.
[27] W. Xi, B. Zhang, Z. Chen, Y. Lu, S. Yan, W.-Y. Ma,
and E. A. Fox. Link fusion: a unified link analysis
framework for multi-type interrelated data objects. In
Proceedings of the 13th international conference on
World Wide Web, pages 319 - 327, 2004.
386
A New Approach for Evaluating Query Expansion:
Query-Document Term Mismatch
Tonya Custis
Thomson Corporation
610 Opperman Drive
St. Paul, MN
tonya.custis@thomson.com
Khalid Al-Kofahi
Thomson Corporation
610 Opperman Drive
St. Paul, MN
khalid.al-kofahi@thomson.com
ABSTRACT
The effectiveness of information retrieval (IR) systems is 
influenced by the degree of term overlap between user queries
and relevant documents. Query-document term mismatch,
whether partial or total, is a fact that must be dealt with by
IR systems. Query Expansion (QE) is one method for 
dealing with term mismatch. IR systems implementing query
expansion are typically evaluated by executing each query
twice, with and without query expansion, and then 
comparing the two result sets. While this measures an overall
change in performance, it does not directly measure the 
effectiveness of IR systems in overcoming the inherent issue of
term mismatch between the query and relevant documents,
nor does it provide any insight into how such systems would
behave in the presence of query-document term mismatch.
In this paper, we propose a new approach for evaluating
query expansion techniques. The proposed approach is 
attractive because it provides an estimate of system 
performance under varying degrees of query-document term 
mismatch, it makes use of readily available test collections, and
it does not require any additional relevance judgments or
any form of manual processing.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval
General Terms
Measurement, Experimentation
1. INTRODUCTION
In our domain,1
and unlike web search, it is very 
important for attorneys to find all documents (e.g., cases) that
are relevant to an issue. Missing relevant documents may
have non-trivial consequences on the outcome of a court 
proceeding. Attorneys are especially concerned about missing
relevant documents when researching a legal topic that is
new to them, as they may not be aware of all language 
variations in such topics. Therefore, it is important to develop
information retrieval systems that are robust with respect to
language variations or term mismatch between queries and
relevant documents. During our work on developing such
systems, we concluded that current evaluation methods are
not sufficient for this purpose.
{Whooping cough, pertussis}, {heart attack, myocardial
infarction}, {car wash, automobile cleaning}, {attorney, 
legal counsel, lawyer} are all examples of things that share
the same meaning. Often, the terms chosen by users in their
queries are different than those appearing in the documents
relevant to their information needs. This query-document
term mismatch arises from two sources: (1) the synonymy
found in natural language, both at the term and the phrasal
level, and (2) the degree to which the user is an expert at
searching and/or has expert knowledge in the domain of the
collection being searched.
IR evaluations are comparative in nature (cf. TREC).
Generally, IR evaluations show how System A did in 
relation to System B on the same test collection based on various
precision- and recall-based metrics. Similarly, IR systems
with QE capabilities are typically evaluated by executing
each search twice, once with and once without query 
expansion, and then comparing the two result sets. While this
approach shows which system may have performed better
overall with respect to a particular test collection, it does
not directly or systematically measure the effectiveness of
IR systems in overcoming query-document term mismatch.
If the goal of QE is to increase search performance by 
mitigating the effects of query-document term mismatch, then
the degree to which a system does so should be measurable
in evaluation. An effective evaluation method should 
measure the performance of IR systems under varying degrees of
query-document term mismatch, not just in terms of overall
performance on a collection relative to another system.
1
Thomson Corporation builds information based solutions
to the professional markets including legal, financial, health
care, scientific, and tax and accounting.
In order to measure that a particular IR system is able
to overcome query-document term mismatch by retrieving
documents that are relevant to a user"s query, but that do
not necessarily contain the query terms themselves, we 
systematically introduce term mismatch into the test collection
by removing query terms from known relevant documents.
Because we are purposely inducing term mismatch between
the queries and known relevant documents in our test 
collections, the proposed evaluation framework is able to measure
the effectiveness of QE in a way that testing on the whole
collection is not. If a QE search method finds a document
that is known to be relevant but that is nonetheless missing
query terms, it shows that QE technique is indeed robust
with respect to query-document term mismatch.
2. RELATED WORK
Accounting for term mismatch between the terms in user
queries and the documents relevant to users" information
needs has been a fundamental issue in IR research for 
almost 40 years [38, 37, 47]. Query expansion (QE) is one
technique used in IR to improve search performance by 
increasing the likelihood of term overlap (either explicitly or
implicitly) between queries and documents that are relevant
to users" information needs. Explicit query expansion 
occurs at run-time, based on the initial search results, as is
the case with relevance feedback and pseudo relevance 
feedback [34, 37]. Implicit query expansion can be based on
statistical properties of the document collection, or it may
rely on external knowledge sources such as a thesaurus or an
ontology [32, 17, 26, 50, 51, 2]. Regardless of method, QE
algorithms that are capable of retrieving relevant documents
despite partial or total term mismatch between queries and
relevant documents should increase the recall of IR systems
(by retrieving documents that would have previously been
missed) as well as their precision (by retrieving more 
relevant documents).
In practice, QE tends to improve the average overall 
retrieval performance, doing so by improving performance on
some queries while making it worse on others. QE 
techniques are judged as effective in the case that they help
more than they hurt overall on a particular collection [47,
45, 41, 27]. Often, the expansion terms added to a query
in the query expansion phase end up hurting the overall 
retrieval performance because they introduce semantic noise,
causing the meaning of the query to drift. As such, much
work has been done with respect to different strategies for
choosing semantically relevant QE terms to include in order
to avoid query drift [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5].
The evaluation of IR systems has received much attention
in the research community, both in terms of developing test
collections for the evaluation of different systems [11, 12, 13,
43] and in terms of the utility of evaluation metrics such as
recall, precision, mean average precision, precision at rank,
Bpref, etc. [7, 8, 44, 14]. In addition, there have been 
comparative evaluations of different QE techniques on various
test collections [47, 45, 41].
In addition, the IR research community has given 
attention to differences between the performance of individual
queries. Research efforts have been made to predict which
queries will be improved by QE and then selectively 
applying it only to those queries [1, 5, 27, 29, 15, 48], to achieve
optimal overall performance. In addition, related work on
predicting query difficulty, or which queries are likely to 
perform poorly, has been done [1, 4, 5, 9]. There is general
interest in the research community to improve the 
robustness of IR systems by improving retrieval performance on
difficult queries, as is evidenced by the Robust track in the
TREC competitions and new evaluation measures such as
GMAP. GMAP (geometric mean average precision) gives
more weight to the lower end of the average precision (as
opposed to MAP), thereby emphasizing the degree to which
difficult or poorly performing queries contribute to the score
[33].
However, no attention is given to evaluating the 
robustness of IR systems implementing QE with respect to 
querydocument term mismatch in quantifiable terms. By 
purposely inducing mismatch between the terms in queries and
relevant documents, our evaluation framework allows us a
controlled manner in which to degrade the quality of the
queries with respect to their relevant documents, and then
to measure the both the degree of (induced) difficulty of the
query and the degree to which QE improves the retrieval
performance of the degraded query.
The work most similar to our own in the literature consists
of work in which document collections or queries are altered
in a systematic way to measure differences query 
performance. [42] introduces into the document collection 
pseudowords that are ambiguous with respect to word sense, in 
order to measure the degree to which word sense 
disambiguation is useful in IR. [6] experiments with altering the 
document collection by adding semantically related expansion
terms to documents at indexing time. In cross-language IR,
[28] explores different query expansion techniques while 
purposely degrading their translation resources, in what amounts
to expanding a query with only a controlled percentage of
its translation terms. Although similar in introducing a 
controlled amount of variance into their test collections, these
works differ from the work being presented in this paper
in that the work being presented here explicitly and 
systematically measures query effectiveness in the presence of
query-document term mismatch.
3. METHODOLOGY
In order to accurately measure IR system performance in
the presence of query-term mismatch, we need to be able
to adjust the degree of term mismatch in a test corpus in
a principled manner. Our approach is to introduce 
querydocument term mismatch into a corpus in a controlled 
manner and then measure the performance of IR systems as
the degree of term mismatch changes. We systematically
remove query terms from known relevant documents, 
creating alternate versions of a test collection that differ only in
how many or which query terms have been removed from
the documents relevant to a particular query. Introducing
query-document term mismatch into the test collection in
this manner allows us to manipulate the degree of term 
mismatch between relevant documents and queries in a 
controlled manner.
This removal process affects only the relevant documents
in the search collection. The queries themselves remain 
unaltered. Query terms are removed from documents one by
one, so the differences in IR system performance can be 
measured with respect to missing terms. In the most extreme
case (i.e., when the length of the query is less than or equal
to the number of query terms removed from the relevant
documents), there will be no term overlap between a query
and its relevant documents. Notice that, for a given query,
only relevant documents are modified. Non-relevant 
documents are left unchanged, even in the case that they contain
query terms.
Although, on the surface, we are changing the 
distribution of terms between the relevant and non-relevant 
documents sets by removing query terms from the relevant 
documents, doing so does not change the conceptual relevancy
of these documents. Systematically removing query terms
from known relevant documents introduces a controlled amount
of query-document term mismatch by which we can 
evaluate the degree to which particular QE techniques are able
to retrieve conceptually relevant documents, despite a lack
of actual term overlap. Removing a query term from 
relevant documents simply masks the presence of that query
term in those documents. It does not in any way change the
conceptual relevancy of the documents.
The evaluation framework presented in this paper consists
of three elements: a test collection, C; a strategy for selecting
which query terms to remove from the relevant documents in
that collection, S; and a metric by which to compare 
performance of the IR systems, m. The test collection, C, consists
of a document collection, queries, and relevance judgments.
The strategy, S, determines the order and manner in which
query terms are removed from the relevant documents in C.
This evaluation framework is not metric-specific; any metric
(MAP, P@10, recall, etc.) can be used to measure IR system
performance.
Although test collections are difficult to come by, it should
be noted that this evaluation framework can be used on
any available test collection. In fact, using this framework
stretches the value of existing test collections in that one 
collection becomes several when query terms are removed from
relevant documents, thereby increasing the amount of 
information that can be gained from evaluating on a particular
collection.
In other evaluations of QE effectiveness, the controlled
variable is simply whether or not queries have been 
expanded or not, compared in terms of some metric. In 
contrast, the controlled variable in this framework is the query
term that has been removed from the documents relevant to
that query, as determined by the removal strategy, S. Query
terms are removed one by one, in a manner and order 
determined by S, so that collections differ only with respect
to the one term that has been removed (or masked) in the
documents relevant to that query. It is in this way that we
can explicitly measure the degree to which an IR system
overcomes query-document term mismatch.
The choice of a query term removal strategy is relatively
flexible; the only restriction in choosing a strategy S is that
query terms must be removed one at a time. Two 
decisions must be made when choosing a removal strategy S.
The first is the order in which S removes terms from the
relevant documents. Possible orders for removal could be
based on metrics such as IDF or the global probability of a
term in a document collection. Based on the purpose of the
evaluation and the retrieval algorithm being used, it might
make more sense to choose a removal order for S based on
query term IDF or perhaps based on a measure of query
term probability in the document collection.
Once an order for removal has been decided, a manner for
term removal/masking must be decided. It must be 
determined if S will remove the terms individually (i.e., remove
just one different term each time) or additively (i.e., remove
one term first, then that term in addition to another, and so
on). The incremental additive removal of query terms from
relevant documents allows the evaluation to show the 
degree to which IR system performance degrades as more and
more query terms are missing, thereby increasing the degree
of query-document term mismatch. Removing terms 
individually allows for a clear comparison of the contribution of
QE in the absence of each individual query term.
4. EXPERIMENTAL SET-UP
4.1 IR Systems
We used the proposed evaluation framework to evaluate
four IR systems on two test collections. Of the four 
systems used in the evaluation, two implement query 
expansion techniques: Okapi (with pseudo-feedback for QE), and
a proprietary concept search engine (we"ll call it TCS, for
Thomson Concept Search). TCS is a language modeling
based retrieval engine that utilizes a subject-appropriate 
external corpus (i.e., legal or news) as a knowledge source.
This external knowledge source is a corpus separate from,
but thematically related to, the document collection to be
searched. Translation probabilities for QE [2] are calculated
from these large external corpora.
Okapi (without feedback) and a language model query
likelihood (QL) model (implemented using Indri) are 
included as keyword-only baselines. Okapi without feedback
is intended as an analogous baseline for Okapi with 
feedback, and the QL model is intended as an appropriate 
baseline for TCS, as they both implement language-modeling
based retrieval algorithms. We choose these as baselines 
because they are dependent only on the words appearing in
the queries and have no QE capabilities. As a result, we 
expect that when query terms are removed from relevant 
documents, the performance of these systems should degrade
more dramatically than their counterparts that implement
QE.
The Okapi and QL model results were obtained using the
Lemur Toolkit.2
Okapi was run with the parameters k1=1.2,
b=0.75, and k3=7. When run with feedback, the feedback
parameters used in Okapi were set at 10 documents and 25
terms. The QL model used Jelinek-Mercer smoothing, with
λ = 0.6.
4.2 Test Collections
We evaluated the performance of the four IR systems 
outlined above on two different test collections. The two test
collections used were the TREC AP89 collection (TIPSTER
disk 1) and the FSupp Collection.
The FSupp Collection is a proprietary collection of 11,953
case law documents for which we have 44 queries (ranging
from four to twenty-two words after stop word removal) with
full relevance judgments.3
The average length of documents
in the FSupp Collection is 3444 words.
2
www.lemurproject.org
3
Each of the 11,953 documents was evaluated by domain
experts with respect to each of the 44 queries.
The TREC AP89 test collection contains 84,678 
documents, averaging 252 words in length. In our evaluation, we
used both the title and the description fields of topics 
151200 as queries, so we have two sets of results for the AP89
Collection. After stop word removal, the title queries range
from two to eleven words and the description queries range
from four to twenty-six terms.
4.3 Query Term Removal Strategy
In our experiments, we chose to sequentially and 
additively remove query terms from highest-to-lowest inverse
document frequency (IDF) with respect to the entire 
document collection. Terms with high IDF values tend to 
influence document ranking more than those with lower IDF
values. Additionally, high IDF terms tend to be 
domainspecific terms that are less likely to be known to non-expert
user, hence we start by removing these first.
For the FSupp Collection, queries were evaluated 
incrementally with one, two, three, five, and seven terms 
removed from their corresponding relevant documents. The
longer description queries from TREC topics 151-200 were
likewise evaluated on the AP89 Collection with one, two,
three, five, and seven query terms removed from their 
relevant documents. For the shorter TREC title queries, we
removed one, two, three, and five terms from the relevant
documents.
4.4 Metrics
In this implementation of the evaluation framework, we
chose three metrics by which to compare IR system 
performance: mean average precision (MAP), precision at 10
documents (P10), and recall at 1000 documents. Although
these are the metrics we chose to demonstrate this 
framework, any appropriate IR metrics could be used within the
framework.
5. RESULTS
5.1 FSupp Collection
Figures 1, 2, and 3 show the performance (in terms of
MAP, P10 and Recall, respectively) for the four search 
engines on the FSupp Collection. As expected, the 
performance of the keyword-only IR systems, QL and Okapi, drops
quickly as query terms are removed from the relevant 
documents in the collection. The performance of Okapi with
feedback (Okapi FB) is somewhat surprising in that on the
original collection (i.e., prior to query term removal), its 
performance is worse than that of Okapi without feedback on
all three measures.
TCS outperforms the QL keyword baseline on every 
measure except for MAP on the original collection (i.e., prior
to removing any query terms). Because TCS employs 
implicit query expansion using an external domain specific
knowledge base, it is less sensitive to term removal (i.e.,
mismatch) than the Okapi FB, which relies on terms from
the top-ranked documents retrieved by an initial 
keywordonly search. Because overall search engine performance is
frequently measured in terms of MAP, and because other
evaluations of QE often only consider performance on the
entire collection (i.e., they do not consider term mismatch),
the QE implemented in TCS would be considered (in 
an0 1 2 3 4 5 6 7
Number of Query Terms Removed from Relevant Documents
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
MeanAveragePrecision(MAP)
Okapi FB
Okapi
TCS
QL
FSupp: Mean Average Precision with Query Terms Removed
Figure 1: The performance of the four retrieval 
systems on the FSupp collection in terms of Mean 
Average Precision (MAP) and as a function of the 
number of query terms removed (the horizontal axis).
0 1 2 3 4 5 6 7
Number of Query Terms Removed from Relevant Documents
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
Precisionat10Documents(P10)
Okapi FB
Okapi
TCS
QL
FSupp: P10 with Query Terms Removed
Figure 2: The performance of the four retrieval 
systems on the FSupp collection in terms of Precision
at 10 and as a function of the number of query terms
removed (the horizontal axis).
other evaluation) to hurt performance on the FSupp 
Collection. However, when we look at the comparison of TCS to
QL when query terms are removed from the relevant 
documents, we can see that the QE in TCS is indeed contributing
positively to the search.
5.2 The AP89 Collection: using the
description queries
Figures 4, 5, and 6 show the performance of the four IR
systems on the AP89 Collection, using the TREC topic 
descriptions as queries. The most interesting difference 
between the performance on the FSupp Collection and the
AP89 collection is the reversal of Okapi FB and TCS. On
FSupp, TCS outperformed the other engines consistently
(see Figures 1, 2, and 3); on the AP89 Collection, Okapi
FB is clearly the best performer (see Figures 4, 5, and 6).
This is all the more interesting, based on the fact that QE in
Okapi FB takes place after the first search iteration, which
0 1 2 3 4 5 6 7
Number of Query Terms Removed from Relevant Documents
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Okapi FB
Okapi
TCS
Indri
FSupp: Recall at 1000 documents with Query Terms Removed
Figure 3: The Recall (at 1000) of the four retrieval
systems on the FSupp collection as a function of
the number of query terms removed (the horizontal
axis).
0 1 2 3 4 5 6 7
Number of Query Terms Removed from Relevant Documents
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
MeanAveragePrecision(MAP)
Okapi FB
Okapi
TCS
QL
AP89: Mean Average Precision with Query Terms Removed (description queries)
Figure 4: MAP of the four IR systems on the AP89
Collection, using TREC description queries. MAP
is measured as a function of the number of query
terms removed.
0 1 2 3 4 5 6 7
Number of Query Terms Removed from Relevant Documents
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Precisionat10Documents(P10)
Okapi FB
Okapi
TCS
QL
AP89: P10 with Query Terms Removed (description queries)
Figure 5: Precision at 10 of the four IR systems
on the AP89 Collection, using TREC description
queries. P at 10 is measured as a function of the
number of query terms removed.
0 1 2 3 4 5 6 7
Number of Query Terms Removed from Relevant Documents
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Okapi FB
Okapi
TCS
QL
AP89: Recall at 1000 documents with Query Terms Removed (description queries)
Figure 6: Recall (at 1000) of the four IR systems
on the AP89 Collection, using TREC description
queries, and as a function of the number of query
terms removed.
we would expect to be handicapped when query terms are
removed.
Looking at P10 in Figure 5, we can see that TCS and
Okapi FB score similarly on P10, starting at the point where
one query term is removed from relevant documents. At two
query terms removed, TCS starts outperforming Okapi FB.
If modeling this in terms of expert versus non-expert users,
we could conclude that TCS might be a better search engine
for non-experts to use on the AP89 Collection, while Okapi
FB would be best for an expert searcher.
It is interesting to note that on each metric for the AP89
description queries, TCS performs more poorly than all the
other systems on the original collection, but quickly 
surpasses the baseline systems and approaches Okapi FB"s 
performance as terms are removed. This is again a case where
the performance of a system on the entire collection is not
necessarily indicative of how it handles query-document term
mismatch.
5.3 The AP89 Collection: using the title queries
Figures 7, 8, and 9 show the performance of the four IR
systems on the AP89 Collection, using the TREC topic titles
as queries. As with the AP89 description queries, Okapi
FB is again the best performer of the four systems in the
evaluation. As before, the performance of the Okapi and
QL systems, the non-QE baseline systems, sharply degrades
as query terms are removed. On the shorter queries, TCS
seems to have a harder time catching up to the performance
of Okapi FB as terms are removed.
Perhaps the most interesting result from our evaluation
is that although the keyword-only baselines performed 
consistently and as expected on both collections with respect
to query term removal from relevant documents, the 
performances of the engines implementing QE techniques differed
dramatically between collections.
0 1 2 3 4 5
Number of Query Terms Removed from Relevant Documents
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
MeanAveragePrecision(MAP)
Okapi FB
Okapi
TCS
QL
AP89: Mean Average Precision with Query Terms Removed (title queries)
Figure 7: MAP of the four IR systems on the AP89
Collection, using TREC title queries and as a 
function of the number of query terms removed.
0 1 2 3 4 5
Number of Query Terms Removed from Relevant Documents
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Precisionat10Documents(P10)
Okapi FB
Okapi
TCS
QL
AP89: P10 with Query Terms Removed (title queries)
Figure 8: Precision at 10 of the four IR systems on
the AP89 Collection, using TREC title queries, and
as a function of the number of query terms removed.
0 1 2 3 4 5
Number of Query Terms Removed from Relevant Documents
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Okapi FB
Okapi
TCS
QL
AP89: Recall at 1000 documents with Query Terms Removed (title queries)
Figure 9: Recall (at 1000) of the four IR systems on
the AP89 Collection, using TREC title queries and
as a function of the number of query terms removed.
6. DISCUSSION
The intuition behind this evaluation framework is to 
measure the degree to which various QE techniques overcome
term mismatch between queries and relevant documents. In
general, it is easy to evaluate the overall performance of
different techniques for QE in comparison to each other or
against a non-QE variant on any complete test collection.
Such an approach does tell us which systems perform better
on a complete test collection, but it does not measure the
ability of a particular QE technique to retrieve relevant 
documents despite partial or complete term mismatch between
queries and relevant documents.
A systematic evaluation of IR systems as outlined in this
paper is useful not only with respect to measuring the 
general success or failure of particular QE techniques in the
presence of query-document term mismatch, but it also 
provides insight into how a particular IR system will perform
when used by expert versus non-expert users on a 
particular collection. The less a user knows about the domain of
the document collection on which they are searching, the
more prevalent query-document term mismatch is likely to
be. This distinction is especially relevant in the case that
the test collection is domain-specific (i.e., medical or legal, as
opposed to a more general domain, such as news), where the
distinction between experts and non-experts may be more
marked. For example, a non-expert in the medical domain
might search for whooping cough, but relevant documents
might instead contain the medical term pertussis.
Since query terms are masked only the in relevant 
documents, this evaluation framework is actually biased against
retrieving relevant documents. This is because non-relevant
documents may also contain query terms, which can cause
a retrieval system to rank such documents higher than it
would have before terms were masked in relevant documents.
Still, we think this is a more realistic scenario than removing
terms from all documents regardless of relevance.
The degree to which a QE technique is well-suited to a
particular collection can be evaluated in terms of its ability
to still find the relevant documents, even when they are 
missing query terms, despite the bias of this approach against 
relevant documents. However, given that Okapi FB and TCS
outperformed each other on two different collection sets, 
further investigation into the degree of compatibility between
QE expansion approach and target collection is probably
warranted. Furthermore, the investigation of other term 
removal strategies could provide insight into the behavior of
different QE techniques and their overall impact on the user
experience.
As mentioned earlier, our choice of the term removal 
strategy was motivated by (1) our desire to see the highest 
impact on system performance as terms are removed and (2)
because high IDF terms, in our domain context, are more
likely to be domain specific, which allows us to better 
understand the performance of an IR system as experienced
by expert and non-expert users.
Although not attempted in our experiments, another 
application of this evaluation framework would be to remove
query terms individually, rather than incrementally, to 
analyze which terms (or possibly which types of terms) are
being helped most by a QE technique on a particular test
collection. This could lead to insight as to when QE should
and should not be applied.
This evaluation framework allows us to see how IR 
systems perform in the presence of query-document term 
mismatch. In other evaluations, the performance of a system is
measured only on the entire collection, in which the degree
of query-term document mismatch is not known. By 
systematically introducing this mismatch, we can see that even
if an IR system is not the best performer on the entire 
collection, its performance may nonetheless be more robust to
query-document term mismatch than other systems. Such
robustness makes a system more user-friendly, especially to
non-expert users.
This paper presents a novel framework for IR system 
evaluation, the applications of which are numerous. The results
presented in this paper are not by any means meant to be
exhaustive or entirely representative of the ways in which
this evaluation could be applied. To be sure, there is much
future work that could be done using this framework.
In addition to looking at average performance of IR 
systems, the results of individual queries could be examined and
compared more closely, perhaps giving more insight into the
classification and prediction of difficult queries, or perhaps
showing which QE techniques improve (or degrade) 
individual query performance under differing degrees of 
querydocument term mismatch. Indeed, this framework would
also benefit from further testing on a larger collection.
7. CONCLUSION
The proposed evaluation framework allows us to measure
the degree to which different IR systems overcome (or don"t
overcome) term mismatch between queries and relevant 
documents. Evaluations of IR systems employing QE performed
only on the entire collection do not take into account that
the purpose of QE is to mitigate the effects of term mismatch
in retrieval. By systematically removing query terms from
relevant documents, we can measure the degree to which
QE contributes to a search by showing the difference 
between the performances of a QE system and its 
keywordonly baseline when query terms have been removed from
known relevant documents. Further, we can model the 
behavior of expert versus non-expert users by manipulating
the amount of query-document term mismatch introduced
into the collection.
The evaluation framework proposed in this paper is 
attractive for several reasons. Most importantly, it provides
a controlled manner in which to measure the performance
of QE with respect to query-document term mismatch. In
addition, this framework takes advantage and stretches the
amount of information we can get from existing test 
collections. Further, this evaluation framework is not 
metricspecific: information in terms of any metric (MAP, P@10,
etc.) can be gained from evaluating an IR system this way.
It should also be noted that this framework is 
generalizable to any IR system, in that it evaluates how well IR
systems evaluate users" information needs as represented by
their queries. An IR system that is easy to use should be
good at retrieving documents that are relevant to users" 
information needs, even if the queries provided by the users do
not contain the same keywords as the relevant documents.
8. REFERENCES
[1] Amati, G., C. Carpineto, and G. Romano. Query
difficulty, robustness and selective application of query
expansion. In Proceedings of the 25th European
Conference on Information Retrieval (ECIR 2004),
pp. 127-137.
[2] Berger, A. and J.D. Lafferty. 1999. Information
retrieval as statistical translation. In Research and
Development in Information Retrieval, pages 222-229.
[3] Billerbeck, B., F. Scholer, H. E. Williams, and J.
Zobel. 2003. Query expansion using associated queries.
In Proceedings of CIKM 2003, pp. 2-9.
[4] Billerbeck, B., and J. Zobel. 2003. When Query
Expansion Fails. In Proceedings of SIGIR 2003, pp.
387-388.
[5] Billerbeck, B. and J. Zobel. 2004. Questioning Query
Expansion: An Examination of Behaviour and
Parameters. In Proceedings of the 15th Australasian
Database Conference (ADC2004), pp. 69-76.
[6] Billerbeck, B. and J. Zobel. 2005. Document
Expansion versus Query Expansion for ad-hoc
Retrieval. In Proceedings of the 10th Australasian
Document Computing Symposium.
[7] Buckley, C. and E.M. Voorhees. 2000. Evaluating
Evaluation Measure Stability. In Proceedings of SIGIR
2000, pp. 33-40.
[8] Buckley, C. and E.M. Voorhees. 2004. Retrieval
evaluation with incomplete information. In
Proceedings of SIGIR 2004, pp. 25-32.
[9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg. 2006.
What Makes A Query Difficult? In Proceedings of
SIGIR 2006, pp. 390-397.
[10] Carpineto, C., R. Mori and G. Romano. 1998.
Informative Term Selection for Automatic Query
Expansion. In The 7th Text REtrieval Conference,
pp.363:369.
[11] Carterette, B. and J. Allan. 2005. Incremental Test
Collections. In Proceedings of CIKM 2005, pp.
680-687.
[12] Carterette, B., J. Allan, and R. Sitaraman. 2006.
Minimal Test Collections for Retrieval Evaluation. In
Proceedings of SIGIR 2006, pp. 268-275.
[13] Cormack, G.V., C. R. Palmer, and C.L. Clarke. 1998.
Efficient Construction of Large Test Collections. In
Proceedings of SIGIR 1998, pp. 282-289.
[14] Cormack, G. and T.R. Lynam. 2006. Statistical
Precision of Information Retrieval Evaluation. In
Proceedings of SIGIR 2006, pp. 533-540.
[15] Cronen-Townsend, S., Y. Zhou, and W.B. Croft. 2004.
A Language Modeling Framework for Selective Query
Expansion, CIIR Technical Report.
[16] Efthimiadis, E.N. Query Expansion. 1996. In Martha
E. Williams (ed.), Annual Review of Information
Systems and Technology (ARIST), v31, pp 121- 187.
[17] Evans, D.A. and Lefferts, R.G. 1995. CLARIT-TREC
Experiments. Information Processing & Management.
31(3): 385-295.
[18] Fang, H. and C.X. Zhai. 2006. Semantic Term
Matching in Axiomatic Approaches to Information
Retrieval. In Proceedings of SIGIR 2006, pp. 115-122.
[19] Gao, J., J. Nie, G. Wu and G. Cao. 2004. Dependence
language model for information retrieval. In
Proceedings of SIGIR 2004, pp. 170-177.
[20] Harman, D.K. 1992. Relevance feedback revisited. In
Proceedings of ACM SIGIR 1992, pp. 1-10.
[21] Harman, D.K., ed. 1993. The First Text REtrieval
Conference (TREC-1): 1992.
[22] Harman, D.K., ed. 1994. The Second Text REtrieval
Conference (TREC-2): 1993.
[23] Harman, D.K., ed. 1995. The Third Text REtrieval
Conference (TREC-3): 1994.
[24] Harman, D.K., 1998. Towards Interactive Query
Expansion. In Proceedings of SIGIR 1998, pp. 321-331.
[25] Hofmann, T. 1999. Probabilistic latent semantic
indexing. In Proceedings of SIGIR 1999, pp 50-57.
[26] Jing, Y. and W.B. Croft. 1994. The Association
Thesaurus for Information Retrieval. In Proceedings of
RIAO 1994, pp. 146-160
[27] Lu, X.A. and R.B. Keefer. Query expansion/reduction
and its impact on retrieval effectiveness. In: D.K.
Harman, ed. The Third Text REtrieval Conference
(TREC-3). Gaithersburg, MD: National Institute of
Standards and Technology, 1995,231-239.
[28] McNamee, P. and J. Mayfield. 2002. Comparing
Cross-Language Query Expansion Techniques by
Degrading Translation Resources. In Proceedings of
SIGIR 2002, pp. 159-166.
[29] Mitra, M., A. Singhal, and C. Buckley. 1998.
Improving Automatic Query Expansion. In
Proceedings of SIGIR 1998, pp. 206-214.
[30] Peat, H. J. and P. Willett. 1991. The limitations of
term co-occurrence data for query expansion in
document retrieval systems. Journal of the American
Society for Information Science, 42(5): 378-383.
[31] Ponte, J.M. and W.B. Croft. 1998. A language
modeling approach to information retrieval. In
Proceedings of SIGIR 1998, pp.275-281.
[32] Qiu Y., and Frei H. 1993. Concept based query
expansion. In Proceedings of SIGIR 1993, pp. 160-169.
[33] Robertson, S. 2006. On GMAP - and other
transformations. In Proceedings of CIKM 2006, pp.
78-83.
[34] Robertson, S.E. and K. Sparck Jones. 1976. Relevance
Weighting of Search Terms. Journal of the American
Society for Information Science, 27(3): 129-146.
[35] Robertson, S.E., S. Walker, S. Jones, M.M.
Hancock-Beaulieu, and M. Gatford. 1994. Okapi at
TREC-2. In D.K. Harman (ed). 1994. The Second Text
REtrieval Conference (TREC-2): 1993, pp. 21-34.
[36] Robertson, S.E., S. Walker, S. Jones, M.M.
Hancock-Beaulieu, and M. Gatford. 1995. Okapi at
TREC-3. In D.K. Harman (ed). 1995. The Third Text
REtrieval Conference (TREC-2): 1993, pp. 109-126
[37] Rocchio, J.J. 1971. Relevance feedback in information
retrieval. In G. Salton (Ed.), The SMART Retrieval
System. Prentice-Hall, Inc., Englewood Cliffs, NJ, pp.
313-323.
[38] Salton, G. 1968. Automatic Information Organization
and Retrieval. McGraw-Hill.
[39] Salton, G. 1971. The SMART Retrieval System:
Experiments in Automatic Document Processing.
Englewood Cliffs NJ; Prentice-Hall.
[40] Salton,G. 1980. Automatic term class construction
using relevance-a summary of work in automatic
pseudoclassification. Information Processing &
Management. 16(1): 1-15.
[41] Salton, G., and C. Buckley. 1988. On the Use of
Spreading Activation Methods in Automatic
Information Retrieval. In Proceedings of SIGIR 1998,
pp. 147-160.
[42] Sanderson, M. 1994. Word sense disambiguation and
information retrieval. In Proceedings of SIGIR 1994,
pp. 161-175.
[43] Sanderson, M. and H. Joho. 2004. Forming test
collections with no system pooling. In Proceedings of
SIGIR 2004, pp. 186-193.
[44] Sanderson, M. and Zobel, J. 2005. Information
Retrieval System Evaluation: Effort, Sensitivity, and
Reliability. In Proceedings of SIGIR 2005, pp. 162-169.
[45] Smeaton, A.F. and C.J. Van Rijsbergen. 1983. The
Retrieval Effects of Query Expansion on a Feedback
Document Retrieval System. Computer Journal.
26(3):239-246.
[46] Song, F. and W.B. Croft. 1999. A general language
model for information retrieval. In Proceedings of the
Eighth International Conference on Information and
Knowledge Management, pages 316-321.
[47] Sparck Jones, K. 1971. Automatic Keyword
Classification for Information Retrieval. London:
Butterworths.
[48] Terra, E. and C. L. Clarke. 2004. Scoring missing
terms in information retrieval tasks. In Proceedings of
CIKM 2004, pp. 50-58.
[49] Turtle, Howard. 1994. Natural Language vs. Boolean
Query Evaluation: A Comparison of Retrieval
Performance. In Proceedings of SIGIR 1994, pp.
212-220.
[50] Voorhees, E.M. 1994a. On Expanding Query Vectors
with Lexically Related Words. In Harman, D. K., ed.
Text REtrieval Conference (TREC-1): 1992.
[51] Voorhees, E.M. 1994b. Query Expansion Using
Lexical-Semantic Relations. In Proceedings of SIGIR
1994, pp. 61-69.
Distance Measures for MPEG-7-based Retrieval
Horst Eidenberger
Vienna University of Technology, Institute of Software Technology and Interactive Systems
Favoritenstrasse 9-11 - A-1040 Vienna, Austria
Tel. + 43-1-58801-18853
eidenberger@ims.tuwien.ac.at
ABSTRACT
In visual information retrieval the careful choice of suitable
proximity measures is a crucial success factor. The evaluation
presented in this paper aims at showing that the distance measures
suggested by the MPEG-7 group for the visual descriptors can be
beaten by general-purpose measures. Eight visual MPEG-7
descriptors were selected and 38 distance measures implemented.
Three media collections were created and assessed, performance
indicators developed and more than 22500 tests performed.
Additionally, a quantisation model was developed to be able to
use predicate-based distance measures on continuous data as well.
The evaluation shows that the distance measures recommended in
the MPEG-7-standard are among the best but that other measures
perform even better.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval - Information filtering, Query formulation,
Retrieval models.
General Terms
Algorithms, Measurement, Experimentation, Performance, Theory.
1. INTRODUCTION
The MPEG-7 standard defines - among others - a set of
descriptors for visual media. Each descriptor consists of a feature
extraction mechanism, a description (in binary and XML format)
and guidelines that define how to apply the descriptor on different
kinds of media (e.g. on temporal media). The MPEG-7 descriptors
have been carefully designed to meet - partially 
complementaryrequirements of different application domains: archival, browsing,
retrieval, etc. [9]. In the following, we will exclusively deal with
the visual MPEG-7 descriptors in the context of media retrieval.
The visual MPEG-7 descriptors fall in five groups: colour,
texture, shape, motion and others (e.g. face description) and sum
up to 16 basic descriptors. For retrieval applications, a rule for
each descriptor is mandatory that defines how to measure the
similarity of two descriptions. Common rules are distance
functions, like the Euclidean distance and the Mahalanobis
distance. Unfortunately, the MPEG-7 standard does not include
distance measures in the normative part, because it was not
designed to be (and should not exclusively understood to be)
retrieval-specific. However, the MPEG-7 authors give
recommendations, which distance measure to use on a particular
descriptor. These recommendations are based on accurate
knowledge of the descriptors' behaviour and the description
structures.
In the present study a large number of successful distance
measures from different areas (statistics, psychology, medicine,
social and economic sciences, etc.) were implemented and applied
on MPEG-7 data vectors to verify whether or not the
recommended MPEG-7 distance measures are really the best for
any reasonable class of media objects. From the MPEG-7 tests
and the recommendations it does not become clear, how many and
which distance measures have been tested on the visual
descriptors and the MPEG-7 test datasets. The hypothesis is that
analytically derived distance measures may be good in general but
only a quantitative analysis is capable to identify the best distance
measure for a specific feature extraction method.
The paper is organised as follows. Section 2 gives a minimum of
background information on the MPEG-7 descriptors and distance
measurement in visual information retrieval (VIR, see [3], [16]).
Section 3 gives an overview over the implemented distance
measures. Section 4 describes the test setup, including the test
data and the implemented evaluation methods. Finally, Section 5
presents the results per descriptor and over all descriptors.
2. BACKGROUND
2.1 MPEG-7: visual descriptors
The visual part of the MPEG-7 standard defines several
descriptors. Not all of them are really descriptors in the sense that
they extract properties from visual media. Some of them are just
structures for descriptor aggregation or localisation. The basic
descriptors are Color Layout, Color Structure, Dominant Color,
Scalable Color, Edge Histogram, Homogeneous Texture, Texture
Browsing, Region-based Shape, Contour-based Shape, Camera
Motion, Parametric Motion and Motion Activity.
Other descriptors are based on low-level descriptors or semantic
information: Group-of-Frames/Group-of-Pictures Color (based on
Scalable Color), Shape 3D (based on 3D mesh information),
Motion Trajectory (based on object segmentation) and Face
Recognition (based on face extraction).
Descriptors for spatiotemporal aggregation and localisation are:
Spatial 2D Coordinates, Grid Layout, Region Locator (spatial),
Time Series, Temporal Interpolation (temporal) and
SpatioTemporal Locator (combined). Finally, other structures
exist for colour spaces, colour quantisation and multiple 2D views
of 3D objects.
These additional structures allow combining the basic descriptors
in multiple ways and on different levels. But they do not change
the characteristics of the extracted information. Consequently,
structures for aggregation and localisation were not considered in
the work described in this paper.
2.2 Similarity measurement on visual data
Generally, similarity measurement on visual information aims at
imitating human visual similarity perception. Unfortunately,
human perception is much more complex than any of the existing
similarity models (it includes perception, recognition and
subjectivity).
The common approach in visual information retrieval is
measuring dis-similarity as distance. Both, query object and
candidate object are represented by their corresponding feature
vectors. The distance between these objects is measured by
computing the distance between the two vectors. Consequently,
the process is independent of the employed querying paradigm
(e.g. query by example). The query object may be natural (e.g. a
real object) or artificial (e.g. properties of a group of objects).
Goal of the measurement process is to express a relationship
between the two objects by their distance. Iteration for multiple
candidates allows then to define a partial order over the
candidates and to address those in a (to be defined)
neighbourhood being similar to the query object. At this point, it
has to be mentioned that in a multi-descriptor 
environmentespecially in MPEG-7 - we are only half way towards a statement
on similarity. If multiple descriptors are used (e.g. a descriptor
scheme), a rule has to be defined how to combine all distances to
a global value for each object. Still, distance measurement is the
most important first step in similarity measurement.
Obviously, the main task of good distance measures is to
reorganise descriptor space in a way that media objects with the
highest similarity are nearest to the query object. If distance is
defined minimal, the query object is always in the origin of
distance space and similar candidates should form clusters around
the origin that are as large as possible. Consequently, many well
known distance measures are based on geometric assumptions of
descriptor space (e.g. Euclidean distance is based on the metric
axioms). Unfortunately, these measures do not fit ideally with
human similarity perception (e.g. due to human subjectivity). To
overcome this shortage, researchers from different areas have
developed alternative models that are mostly predicate-based
(descriptors are assumed to contain just binary elements, e.g.
Tversky's Feature Contrast Model [17]) and fit better with human
perception. In the following distance measures of both groups of
approaches will be considered.
3. DISTANCE MEASURES
The distance measures used in this work have been collected from
various areas (Subsection 3.1). Because they work on differently
quantised data, Subsection 3.2 sketches a model for unification on
the basis of quantitative descriptions. Finally, Subsection 3.3
introduces the distance measures as well as their origin and the
idea they implement.
3.1 Sources
Distance measurement is used in many research areas such as
psychology, sociology (e.g. comparing test results), medicine (e.g.
comparing parameters of test persons), economics (e.g. comparing
balance sheet ratios), etc. Naturally, the character of data available
in these areas differs significantly. Essentially, there are two
extreme cases of data vectors (and distance measures): 
predicatebased (all vector elements are binary, e.g. {0, 1}) and quantitative
(all vector elements are continuous, e.g. [0, 1]).
Predicates express the existence of properties and represent 
highlevel information while quantitative values can be used to measure
and mostly represent low-level information. Predicates are often
employed in psychology, sociology and other human-related
sciences and most predicate-based distance measures were
therefore developed in these areas. Descriptions in visual
information retrieval are nearly ever (if they do not integrate
semantic information) quantitative. Consequently, mostly
quantitative distance measures are used in visual information
retrieval.
The goal of this work is to compare the MPEG-7 distance
measures with the most powerful distance measures developed in
other areas. Since MPEG-7 descriptions are purely quantitative
but some of the most sophisticated distance measures are defined
exclusively on predicates, a model is mandatory that allows the
application of predicate-based distance measures on quantitative
data. The model developed for this purpose is presented in the
next section.
3.2 Quantisation model
The goal of the quantisation model is to redefine the set operators
that are usually used in predicate-based distance measures on
continuous data. The first in visual information retrieval to follow
this approach were Santini and Jain, who tried to apply Tversky's
Feature Contrast Model [17] to content-based image retrieval
[12], [13]. They interpreted continuous data as fuzzy predicates
and used fuzzy set operators. Unfortunately, their model suffered
from several shortcomings they described in [12], [13] (for
example, the quantitative model worked only for one specific
version of the original predicate-based measure).
The main idea of the presented quantisation model is that set
operators are replaced by statistical functions. In [5] the authors
could show that this interpretation of set operators is reasonable.
The model offers a solution for the descriptors considered in the
evaluation. It is not specific to one distance measure, but can be
applied to any predicate-based measure. Below, it will be shown
that the model does not only work for predicate data but for
quantitative data as well. Each measure implementing the model
can be used as a substitute for the original predicate-based measure.
Generally, binary properties of two objects (e.g. media objects)
can exist in both objects (denoted as a), in just one (b, c) or in
none of them (d). The operator needed for these relationships are
UNION, MINUS and NOT. In the quantisation model they are
replaced as follows (see [5] for further details).
131
∑




≤
+
−
+
==∩=
k
jkikjkik
kkji
else
xx
Mif
xx
ssXXa
0
22, 1ε
( )
( )
∑
∑
∑




≤
++
−==¬∩¬=


 ≤−−−
==−=


 ≤−−−
==−=
k
jkikjkik
kkji
k
ikjkikjk
kkij
k
jkikjkik
kkji
else
xx
if
xx
MssXXd
else
xxMifxx
ssXXc
else
xxMifxx
ssXXb
0
22,
0
,
0
,
1
2
2
ε
ε
ε
with:
( ) [ ]
( )
{ }0\
.
0
1
.
0
1
,
2
2
1
minmax
maxmin
+
∈
−
=




≥





−
=
=




≥





−
=
−=
∈=
∑ ∑
∑ ∑
Rp
ki
x
where
else
pif
p
M
ki
x
where
else
pif
p
M
xxM
xxxwithxX
i k
ik
i k
ik
ikiki
µ
σ
σ
σ
ε
µ
µ
µ
ε
a selects properties that are present in both data vectors (Xi, Xj
representing media objects), b and c select properties that are
present in just one of them and d selects properties that are present
in neither of the two data vectors. Every property is selected by
the extent to which it is present (a and d: mean, b and c:
difference) and only if the amount to which it is present exceeds a
certain threshold (depending on the mean and standard deviation
over all elements of descriptor space).
The implementation of these operators is based on one assumption.
It is assumed that vector elements measure on interval scale. That
means, each element expresses that the measured property is
"more or less" present ("0": not at all, "M": fully present). This is
true for most visual descriptors and all MPEG-7 descriptors. A
natural origin as it is assumed here ("0") is not needed.
Introducing p (called discriminance-defining parameter) for the
thresholds 21 ,εε has the positive consequence that a, b, c, d can
then be controlled through a single parameter. p is an additional
criterion for the behaviour of a distance measure and determines
the thresholds used in the operators. It expresses how accurate
data items are present (quantisation) and consequently, how
accurate they should be investigated. p can be set by the user or
automatically. Interesting are the limits:
1. Mp →⇒∞→ 21 ,εε
In this case, all elements (=properties) are assumed to be
continuous (high quantisation). In consequence, all properties of a
descriptor are used by the operators. Then, the distance measure is
not discriminant for properties.
2. 0,0 21 →⇒→ εεp
In this case, all properties are assumed to be predicates. In
consequence, only binary elements (=predicates) are used by the
operators (1-bit quantisation). The distance measure is then highly
discriminant for properties.
Between these limits, a distance measure that uses the
quantisation model is - depending on p - more or less
discriminant for properties. This means, it selects a subset of all
available description vector elements for distance measurement.
For both predicate data and quantitative data it can be shown that
the quantisation model is reasonable. If description vectors consist
of binary elements only, p should be used as follows (for example,
p can easily be set automatically):
( )σµεε ,min..,0,0 21 ==⇒→ pgep
In this case, a, b, c, d measure like the set operators they replace.
For example, Table 1 shows their behaviour for two 
onedimensional feature vectors Xi and Xj. As can be seen, the
statistical measures work like set operators. Actually, the
quantisation model works accurate on predicate data for any p≠∞.
To show that the model is reasonable for quantitative data the
following fact is used. It is easy to show that for predicate data
some quantitative distance measures degenerate to 
predicatebased measures. For example, the L1
metric (Manhattan metric)
degenerates to the Hamming distance (from [9], without weights):
distanceHammingcbxxL
k
jkik =+≡−= ∑1
If it can be shown that the quantisation model is able to
reconstruct the quantitative measure from the degenerated
predicate-based measure, the model is obviously able to extend
predicate-based measures to the quantitative domain. This is easy
to illustrate. For purely quantitative feature vectors, p should be
used as follows (again, p can easily be set automatically):
1, 21 =⇒∞→ εεp
Then, a and d become continuous functions:
∑
∑
+
−==⇒≡≤
+
+
==⇒≡≤
+
−
k
jkik
kk
jkik
k
jkik
kk
jkik
xx
MswheresdtrueM
xx
xx
swheresatrueM
xx
M
22
22
b and c can be made continuous for the following expressions:
( )
( )
∑
∑
∑
−==+⇒


 ≥−−
==⇒
≥−≡≤−−


 ≥−−
==⇒
≥−≡≤−−
k
jkikkk
k
ikjkikjk
kk
ikjkikjk
k
jkikjkik
kk
jkikjkik
xxswherescb
else
xxifxx
swheresc
xxMxxM
else
xxifxx
swheresb
xxMxxM
0
0
0
0
0
0
Table 1. Quantisation model on predicate vectors.
Xi Xj a b c d
(1) (1) 1 0 0 0
(1) (0) 0 1 0 0
(0) (1) 0 0 1 0
(0) (0) 0 0 0 1
132
∑
∑
−==−
−==−
k
ikjkkk
k
jkikkk
xxswheresbc
xxswherescb
This means, for sufficiently high p every predicate-based distance
measure that is either not using b and c or just as b+c, b-c or c-b,
can be transformed into a continuous quantitative distance
measure. For example, the Hamming distance (again, without
weights):
1
Lxxxxswherescb
k
jkik
k
jkikkk =−=−==+ ∑∑
The quantisation model successfully reconstructs the L1
metric
and no distance measure-specific modification has to be made to
the model. This demonstrates that the model is reasonable. In the
following it will be used to extend successful predicate-based
distance measures on the quantitative domain.
The major advantages of the quantisation model are: (1) it is
application domain independent, (2) the implementation is
straightforward, (3) the model is easy to use and finally, (4) the
new parameter p allows to control the similarity measurement
process in a new way (discriminance on property level).
3.3 Implemented measures
For the evaluation described in this work next to predicate-based
(based on the quantisation model) and quantitative measures, the
distance measures recommended in the MPEG-7 standard were
implemented (all together 38 different distance measures).
Table 2 summarises those predicate-based measures that
performed best in the evaluation (in sum 20 predicate-based
measures were investigated). For these measures, K is the number
of predicates in the data vectors Xi and Xj. In P1, the sum is used
for Tversky's f() (as Tversky himself does in [17]) and α, β are
weights for element b and c. In [5] the author's investigated
Tversky's Feature Contrast Model and found α=1, β=0 to be the
optimum parameters.
Some of the predicate-based measures are very simple (e.g. P2,
P4) but have been heavily exploited in psychological research.
Pattern difference (P6) - a very powerful measure - is used in the
statistics package SPSS for cluster analysis. P7 is a correlation
coefficient for predicates developed by Pearson.
Table 3 shows the best quantitative distance measures that were
used. Q1 and Q2 are metric-based and were implemented as
representatives for the entire group of Minkowski distances. The
wi are weights. In Q5, ii σµ , are mean and standard deviation
for the elements of descriptor Xi. In Q6, m is
2
M
(=0.5). Q3, the
Canberra metric, is a normalised form of Q1. Similarly, Q4,
Clark's divergence coefficient is a normalised version of Q2. Q6 is
a further-developed correlation coefficient that is invariant against
sign changes. This measure is used even though its particular
properties are of minor importance for this application domain.
Finally, Q8 is a measure that takes the differences between
adjacent vector elements into account. This makes it structurally
different from all other measures.
Obviously, one important distance measure is missing. The
Mahalanobis distance was not considered, because different
descriptors would require different covariance matrices and for
some descriptors it is simply impossible to define a covariance
matrix. If the identity matrix was used in this case, the
Mahalanobis distance would degenerate to a Minkowski distance.
Additionally, the recommended MPEG-7 distances were
implemented with the following parameters: In the distance
measure of the Color Layout descriptor all weights were set to "1"
(as in all other implemented measures). In the distance measure of
the Dominant Color descriptor the following parameters were
used: 20,1,3.0,7.0 21 ==== dTww α (as recommended). In the
Homogeneous Texture descriptor's distance all ( )kα were set to
"1" and matching was done rotation- and scale-invariant.
Important! Some of the measures presented in this section are
distance measures while others are similarity measures. For the
tests, it is important to notice, that all similarity measures were
inverted to distance measures.
4. TEST SETUP
Subsection 4.1 describes the descriptors (including parameters)
and the collections (including ground truth information) that were
used in the evaluation. Subsection 4.2 discusses the evaluation
method that was implemented and Subsection 4.3 sketches the test
environment used for the evaluation process.
4.1 Test data
For the evaluation eight MPEG-7 descriptors were used. All
colour descriptors: Color Layout, Color Structure, Dominant
Color, Scalable Color, all texture descriptors: Edge Histogram,
Homogeneous Texture, Texture Browsing and one shape
descriptor: Region-based Shape. Texture Browsing was used even
though the MPEG-7 standard suggests that it is not suitable for
retrieval. The other basic shape descriptor, Contour-based Shape,
was not used, because it produces structurally different
descriptions that cannot be transformed to data vectors with
elements measuring on interval-scales. The motion descriptors
were not used, because they integrate the temporal dimension of
visual media and would only be comparable, if the basic colour,
texture and shape descriptors would be aggregated over time. This
was not done. Finally, no high-level descriptors were used
(Localisation, Face Recognition, etc., see Subsection 2.1),
because - to the author's opinion - the behaviour of the basic
descriptors on elementary media objects should be evaluated
before conclusions on aggregated structures can be drawn.
Table 2. Predicate-based distance measures.
No. Measure Comment
P1 cba .. βα −− Feature Contrast Model,
Tversky 1977 [17]
P2 a No. of co-occurrences
P3 cb + Hamming distance
P4
K
a Russel 1940 [14]
P5
cb
a
+
Kulczvnski 1927 [14]
P6
2
K
bc Pattern difference [14]
P7
( )( )( )( )dcdbcaba
bcad
++++
− Pearson 1926 [11]
133
The Texture Browsing descriptions had to be transformed from
five bins to an eight bin representation in order that all elements
of the descriptor measure on an interval scale. A Manhattan metric
was used to measure proximity (see [6] for details).
Descriptor extraction was performed using the MPEG-7 reference
implementation. In the extraction process each descriptor was
applied on the entire content of each media object and the
following extraction parameters were used. Colour in Color
Structure was quantised to 32 bins. For Dominant Color colour
space was set to YCrCb, 5-bit default quantisation was used and
the default value for spatial coherency was used. Homogeneous
Texture was quantised to 32 components. Scalable Color values
were quantised to sizeof(int)-3 bits and 64 bins were used. Finally,
Texture Browsing was used with five components.
These descriptors were applied on three media collections with
image content: the Brodatz dataset (112 images, 512x512 pixel), a
subset of the Corel dataset (260 images, 460x300 pixel, portrait
and landscape) and a dataset with coats-of-arms images (426
images, 200x200 pixel). Figure 1 shows examples from the three
collections.
Designing appropriate test sets for a visual evaluation is a highly
difficult task (for example, see the TREC video 2002 report [15]).
Of course, for identifying the best distance measure for a
descriptor, it should be tested on an infinite number of media
objects. But this is not the aim of this study. It is just evaluated if
- for likely image collections - better proximity measures than
those suggested by the MPEG-7 group can be found. Collections
of this relatively small size were used in the evaluation, because
the applied evaluation methods are above a certain minimum size
invariant against collection size and for smaller collections it is
easier to define a high-quality ground truth. Still, the average ratio
of ground truth size to collection size is at least 1:7. Especially, no
collection from the MPEG-7 dataset was used in the evaluation
because the evaluations should show, how well the descriptors
and the recommended distance measures perform on "unknown"
material.
When the descriptor extraction was finished, the resulting XML
descriptions were transformed into a data matrix with 798 lines
(media objects) and 314 columns (descriptor elements). To be
usable with distance measures that do not integrate domain
knowledge, the elements of this data matrix were normalised to
[0, 1].
For the distance evaluation - next to the normalised data 
matrixhuman similarity judgement is needed. In this work, the ground
truth is built of twelve groups of similar images (four for each
dataset). Group membership was rated by humans based on
semantic criterions. Table 4 summarises the twelve groups and the
underlying descriptions. It has to be noticed, that some of these
groups (especially 5, 7 and 10) are much harder to find with 
lowlevel descriptors than others.
4.2 Evaluation method
Usually, retrieval evaluation is performed based on a ground truth
with recall and precision (see, for example, [3], [16]). In 
multidescriptor environments this leads to a problem, because the
resulting recall and precision values are strongly influenced by the
method used to merge the distance values for one media object.
Even though it is nearly impossible to say, how big the influence
of a single distance measure was on the resulting recall and
precision values, this problem has been almost ignored so far.
In Subsection 2.2 it was stated that the major task of a distance
measure is to bring the relevant media objects as close to the
origin (where the query object lies) as possible. Even in a 
multidescriptor environment it is then simple to identify the similar
objects in a large distance space. Consequently, it was decided to
Table 3. Quantitative distance measures.
No. Measure Comment No. Measure Comment
Q1
∑ −
k
jkiki xxw
City block
distance (L1
)
Q2
( )∑ −
k
jkiki xxw
2
Euclidean
distance (L2
)
Q3
∑ +
−
k jkik
jkik
xx
xx Canberra metric,
Lance, Williams
1967 [8]
Q4 ( )
∑ +
−
k jkik
jkik
xx
xx
K
2
1
Divergence
coefficient,
Clark 1952 [1]
Q5 ( )( )
( ) ( )∑ ∑
∑
−−
−−
k k
jjkiik
k
jjkiik
xx
xx
22
µµ
µµ Correlation
coefficient
Q6






−+





−−






+−−
∑∑∑
∑ ∑∑
k
ik
k
jkik
k
ik
k k
jk
k
ikjkik
xmKmxxmKmx
xxmKmxx
2..2 2222
Cohen 1969 [2]
Q7
∑ ∑
∑
k k
jkik
k
jkik
xx
xx
22
Angular distance,
Gower 1967 [7]
Q8
( ) ( )( )∑
−
++ −−−
1
2
11
K
k
jkjkikik xxxx
Meehl Index
[10]
Table 4. Ground truth information.
Coll. No Images Description
1 19 Regular, chequered patterns
2 38 Dark white noise
3 33 Moon-like surfaces
Brodatz
4 35 Water-like surfaces
5 73 Humans in nature (difficult)
6 17 Images with snow (mountains, skiing)
7 76 Animals in nature (difficult)
Corel
8 27 Large coloured flowers
9 12 Bavarian communal arms
10 10 All Bavarian arms (difficult)
11 18 Dark objects / light unsegmented shield
Arms
12 14 Major charges on blue or red shield
134
use indicators measuring the distribution in distance space of
candidates similar to the query object for this evaluation instead
of recall and precision. Identifying clusters of similar objects
(based on the given ground truth) is relatively easy, because the
resulting distance space for one descriptor and any distance
measure is always one-dimensional. Clusters are found by
searching from the origin of distance space to the first similar
object, grouping all following similar objects in the cluster,
breaking off the cluster with the first un-similar object and so
forth.
For the evaluation two indicators were defined. The first measures
the average distance of all cluster means to the origin:
distanceavgclustersno
sizecluster
distanceclustersno
i i
sizecluster
j
ij
d
i
_._
_
_
_
∑
∑
=µ
where distanceij is the distance value of the j-th element in the i-th
cluster,
∑
∑ ∑
= CLUSTERS
i
i
CLUSTERS
i
sizecluster
j
ij
sizecluster
distance
distanceavg
i
_
_
_
, no_clusters is the
number of found clusters and cluster_sizei is the size of the i-th
cluster. The resulting indicator is normalised by the distribution
characteristics of the distance measure (avg_distance).
Additionally, the standard deviation is used. In the evaluation
process this measure turned out to produce valuable results and to
be relatively robust against parameter p of the quantisation model.
In Subsection 3.2 we noted that p affects the discriminance of a
predicate-based distance measure: The smaller p is set the larger
are the resulting clusters because the quantisation model is then
more discriminant against properties and less elements of the data
matrix are used. This causes a side-effect that is measured by the
second indicator: more and more un-similar objects come out with
exactly the same distance value as similar objects (a problem that
does not exist for large p's) and become indiscernible from similar
objects. Consequently, they are (false) cluster members. This
phenomenon (conceptually similar to the "false negatives"
indicator) was named "cluster pollution" and the indicator
measures the average cluster pollution over all clusters:
clustersno
doublesno
cp
clustersno
i
sizecluster
j
ij
i
_
_
_ _
∑ ∑
=
where no_doublesij is the number of indiscernible un-similar
objects associated with the j-th element of cluster i.
Remark: Even though there is a certain influence, it could be
proven in [5] that no significant correlation exists between
parameter p of the quantisation model and cluster pollution.
4.3 Test environment
As pointed out above, to generate the descriptors, the MPEG-7
reference implementation in version 5.6 was used (provided by
TU Munich). Image processing was done with Adobe Photoshop
and normalisation and all evaluations were done with Perl. The
querying process was performed in the following steps: (1)
random selection of a ground truth group, (2) random selection of
a query object from this group, (3) distance comparison for all
other objects in the dataset, (4) clustering of the resulting distance
space based on the ground truth and finally, (5) evaluation.
For each combination of dataset and distance measure 250 queries
were issued and evaluations were aggregated over all datasets and
descriptors. The next section shows the - partially 
surprisingresults.
5. RESULTS
In the results presented below the first indicator from Subsection
4.2 was used to evaluate distance measures. In a first step
parameter p had to be set in a way that all measures are equally
discriminant. Distance measurement is fair if the following
condition holds true for any predicate-based measure dP and any
continuous measure dC:
( ) ( )CP dcppdcp ≈,
Then, it is guaranteed that predicate-based measures do not create
larger clusters (with a higher number of similar objects) for the
price of higher cluster pollution. In more than 1000 test queries
the optimum value was found to be p=1.
Results are organised as follows: Subsection 5.1 summarises the
Figure 1. Test datasets. Left: Brodatz dataset, middle: Corel dataset, right: coats-of-arms dataset.
135
best distance measures per descriptor, Section 5.2 shows the best
overall distance measures and Section 5.3 points out other
interesting results (for example, distance measures that work
particularly good on specific ground truth groups).
5.1 Best measure per descriptor
Figure 2 shows the evaluation results for the first indicator. For
each descriptor the best measure and the performance of the
MPEG-7 recommendation are shown. The results are aggregated
over the tested datasets.
On first sight, it becomes clear that the MPEG-7
recommendations are mostly relatively good but never the best.
For Color Layout the difference between MP7 and the best
measure, the Meehl index (Q8), is just 4% and the MPEG-7
measure has a smaller standard deviation. The reason why the
Meehl index is better may be that this descriptors generates
descriptions with elements that have very similar variance.
Statistical analysis confirmed that (see [6]).
For Color Structure, Edge Histogram, Homogeneous Texture,
Region-based Shape and Scalable Color by far the best measure is
pattern difference (P6). Psychological research on human visual
perception has revealed that in many situation differences between
the query object and a candidate weigh much stronger than
common properties. The pattern difference measure implements
this insight in the most consequent way. In the author's opinion,
the reason why pattern difference performs so extremely well on
many descriptors is due to this fact. Additional advantages of
pattern difference are that it usually has a very low variance 
andbecause it is a predicate-based measure - its discriminance (and
cluster structure) can be tuned with parameter p.
The best measure for Dominant Color turned out to be Clark's
Divergence coefficient (Q4). This is a similar measure to pattern
difference on the continuous domain. The Texture Browsing
descriptor is a special problem. In the MPEG-7 standard it is
recommended to use it exclusively for browsing. After testing it
for retrieval on various distance measures the author supports this
opinion. It is very difficult to find a good distance measure for
Texture Browsing. The proposed Manhattan metric, for example,
performs very bad. The best measure is predicate-based (P7). It
works on common properties (a, d) but produces clusters with
very high cluster pollution. For this descriptor the second
indicator is up to eight times higher than for predicate-based
measures on other descriptors.
5.2 Best overall measures
Figure 3 summarises the results over all descriptors and media
collections. The diagram should give an indication on the general
potential of the investigated distance measures for visual
information retrieval.
It can be seen that the best overall measure is a predicate-based
one. The top performance of pattern difference (P6) proves that
the quantisation model is a reasonable method to extend
predicate-based distance measures on the continuous domain. The
second best group of measures are the MPEG-7
recommendations, which have a slightly higher mean but a lower
standard deviation than pattern difference. The third best measure
is the Meehl index (Q8), a measure developed for psychological
applications but because of its characteristic properties 
tailormade for certain (homogeneous) descriptors.
Minkowski metrics are also among the best measures: the average
mean and variance of the Manhattan metric (Q1) and the
Euclidean metric (Q2) are in the range of Q8. Of course, these
measures do not perform particularly well for any of the
descriptors. Remarkably for a predicate-based measure, Tversky's
Feature Contrast Model (P1) is also in the group of very good
measures (even though it is not among the best) that ends with
Q5, the correlation coefficient. The other measures either have a
significantly higher mean or a very large standard deviation.
5.3 Other interesting results
Distance measures that perform in average worse than others may
in certain situations (e.g. on specific content) still perform better.
For Color Layout, for example, Q7 is a very good measure on
colour photos. It performs as good as Q8 and has a lower standard
deviation. For artificial images the pattern difference and the
Hamming distance produce comparable results as well.
If colour information is available in media objects, pattern
difference performs well on Dominant Color (just 20% worse Q4)
and in case of difficult ground truth (group 5, 7, 10) the Meehl
index is as strong as P6.
0,000
0,001
0,002
0,003
0,004
0,005
0,006
0,007
0,008
Q8
MP7
P6
MP7
Q4
MP7
P6
MP7
P6
MP7
P6
MP7
P6
MP7
P7
Q2
Color
Layout
Color
Structure
Dominant
Color
Edge
Histogram
Homog.
Texture
Region
Shape
Scalable
Color
Texture
Browsing
Figure 2. Results per measure and descriptor. The horizontal axis shows the best measure and the performance of the MPEG-7
recommendation for each descriptor. The vertical axis shows the values for the first indicator (smaller value = better cluster structure).
Shades have the following meaning: black=µ-σ (good cases), black + dark grey=µ (average) and black + dark grey + light grey=µ+σ (bad).
136
6. CONCLUSION
The evaluation presented in this paper aims at testing the
recommended distance measures and finding better ones for the
basic visual MPEG-7 descriptors. Eight descriptors were selected,
38 distance measures were implemented, media collections were
created and assessed, performance indicators were defined and
more than 22500 tests were performed. To be able to use
predicate-based distance measures next to quantitative measures a
quantisation model was defined that allows the application of
predicate-based measures on continuous data.
In the evaluation the best overall distance measures for visual
content - as extracted by the visual MPEG-7 descriptors - turned
out to be the pattern difference measure and the Meehl index (for
homogeneous descriptions). Since these two measures perform
significantly better than the MPEG-7 recommendations they
should be further tested on large collections of image and video
content (e.g. from [15]).
The choice of the right distance function for similarity
measurement depends on the descriptor, the queried media
collection and the semantic level of the user's idea of similarity.
This work offers suitable distance measures for various situations.
In consequence, the distance measures identified as the best will
be implemented in the open MPEG-7 based visual information
retrieval framework VizIR [4].
ACKNOWLEDGEMENTS
The author would like to thank Christian Breiteneder for his
valuable comments and suggestions for improvement. The work
presented in this paper is part of the VizIR project funded by the
Austrian Scientific Research Fund FWF under grant no. P16111.
REFERENCES
[1] Clark, P.S. An extension of the coefficient of divergence for
use with multiple characters. Copeia, 2 (1952), 61-64.
[2] Cohen, J. A profile similarity coefficient invariant over
variable reflection. Psychological Bulletin, 71 (1969), 
281284.
[3] Del Bimbo, A. Visual information retrieval. Morgan
Kaufmann Publishers, San Francisco CA, 1999.
[4] Eidenberger, H., and Breiteneder, C. A framework for visual
information retrieval. In Proceedings Visual Information
Systems Conference (HSinChu Taiwan, March 2002), LNCS
2314, Springer Verlag, 105-116.
[5] Eidenberger, H., and Breiteneder, C. Visual similarity
measurement with the Feature Contrast Model. In
Proceedings SPIE Storage and Retrieval for Media Databases
Conference (Santa Clara CA, January 2003), SPIE Vol.
5021, 64-76.
[6] Eidenberger, H., How good are the visual MPEG-7 features?
In Proceedings SPIE Visual Communications and Image
Processing Conference (Lugano Switzerland, July 2003),
SPIE Vol. 5150, 476-488.
[7] Gower, J.G. Multivariate analysis and multidimensional
geometry. The Statistician, 17 (1967),13-25.
[8] Lance, G.N., and Williams, W.T. Mixed data classificatory
programs. Agglomerative Systems Australian Comp. Journal,
9 (1967), 373-380.
[9] Manjunath, B.S., Ohm, J.R., Vasudevan, V.V., and Yamada,
A. Color and texture descriptors. In Special Issue on 
MPEG7. IEEE Transactions on Circuits and Systems for Video
Technology, 11/6 (June 2001), 703-715.
[10] Meehl, P. E. The problem is epistemology, not statistics:
Replace significance tests by confidence intervals and
quantify accuracy of risky numerical predictions. In Harlow,
L.L., Mulaik, S.A., and Steiger, J.H. (Eds.). What if there
were no significance tests? Erlbaum, Mahwah NJ, 393-425.
[11] Pearson, K. On the coefficients of racial likeness. Biometrica,
18 (1926), 105-117.
[12] Santini, S., and Jain, R. Similarity is a geometer. Multimedia
Tools and Application, 5/3 (1997), 277-306.
[13] Santini, S., and Jain, R. Similarity measures. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
21/9 (September 1999), 871-883.
[14] Sint, P.P. Similarity structures and similarity measures.
Austrian Academy of Sciences Press, Vienna Austria, 1975
(in German).
[15] Smeaton, A.F., and Over, P. The TREC-2002 video track
report. NIST Special Publication SP 500-251 (March 2003),
available from: http://trec.nist.gov/pubs/trec11/papers/
VIDEO.OVER.pdf (last visited: 2003-07-29)
[16] Smeulders, A.W.M., Worring, M., Santini, S., Gupta, A., and
Jain, R. Content-based image retrieval at the end of the early
years. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 22/12 (December 2000), 1349-1380.
[17] Tversky, A. Features of similarity. Psychological Review,
84/4 (July 1977), 327-351.
0,000
0,002
0,004
0,006
0,008
0,010
0,012
0,014
0,016
0,018
0,020
P6
MP7
Q8
Q1
Q4
Q2
P2
P4
Q6
Q3
Q7
P1
Q5
P3
P5
P7
Figure 3. Overall results (ordered by the first indicator). The vertical axis shows the values for the first indicator (smaller value = better
cluster structure). Shades have the following meaning: black=µ-σ, black + dark grey=µ and black + dark grey + light grey=µ+σ.
137
Handling Locations in Search Engine Queries
Bruno Martins, Mário J. Silva, Sérgio Freitas and Ana Paula Afonso
Faculdade de Ciências da Universidade de Lisboa
1749-016 Lisboa, Portugal
{bmartins,mjs,sfreitas,apa}@xldb.di.fc.ul.pt
ABSTRACT
This paper proposes simple techniques for handling place 
references in search engine queries, an important aspect of geographical
information retrieval. We address not only the detection, but also
the disambiguation of place references, by matching them 
explicitly with concepts at an ontology. Moreover, when a query does not
reference any locations, we propose to use information from 
documents matching the query, exploiting geographic scopes previously
assigned to these documents. Evaluation experiments, using topics
from CLEF campaigns and logs from real search engine queries,
show the effectiveness of the proposed approaches.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval
1. INTRODUCTION
Search engine queries are often associated with geographical 
locations, either explicitly (i.e. a location reference is given as part of
the query) or implicitly (i.e. the location reference is not present in
the query string, but the query clearly has a local intent [17]). One
of the concerns of geographical information retrieval (GIR) lies in
appropriately handling such queries, bringing better targeted search
results and improving user satisfaction.
Nowadays, GIR is getting increasing attention. Systems that 
access resources on the basis of geographic context are starting to
appear, both in the academic and commercial domains [4, 7]. 
Accurately and effectively detecting location references in search 
engine queries is a crucial aspect of these systems, as they are 
generally based on interpreting geographical terms differently from the
others. Detecting locations in queries is also important for 
generalpropose search engines, as this information can be used to improve
ranking algorithms. Queries with a local intent are best answered
with localized pages, while queries without any geographical 
references are best answered with broad pages [5].
Text mining methods have been successfully used in GIR to 
detect and disambiguate geographical references in text [9], or even to
infer geographic scopes for documents [1, 13]. However, this body
of research has been focused on processing Web pages and full-text
documents. Search engine queries are more difficult to handle, in
the sense that they are very short and with implicit and subjective
user intents. Moreover, the data is also noisier and more versatile
in form, and we have to deal with misspellings, multilingualism
and acronyms. How to automatically understand what the user 
intended, given a search query, without putting the burden in the user
himself, remains an open text mining problem.
Key challenges in handling locations over search engine queries
include their detection and disambiguation, the ranking of possible
candidates, the detection of false positives (i.e not all contained 
location names refer to geographical locations), and the detection of
implied locations by the context of the query (i.e. when the query
does not explicitly contain a place reference but it is nonetheless 
geographical). Simple named entity recognition (NER) algorithms,
based on dictionary look-ups for geographical names, may 
introduce high false positives for queries whose location names do not
constitute place references. For example the query Denzel 
Washington contains the place name Washington, but the query is not
geographical. Queries can also be geographic without containing
any explicit reference to locations at the dictionary. In these cases,
place name extraction and disambiguation does not give any results,
and we need to access other sources of information.
This paper proposes simple and yet effective techniques for 
handling place references over queries. Each query is split into a triple
< what,relation,where >, where what specifies the non-geographic
aspect of the information need, where specifies the geographic 
areas of interest, and relation specifies a spatial relationship 
connecting what and where. When this is not possible, i.e. the query does
not contain any place references, we try using information from
documents matching the query, exploiting geographic scopes 
previously assigned to these documents.
Disambiguating place references is one of the most important
aspects. We use a search procedure that combines textual patterns
with geographical names defined at an ontology, and we use 
heuristics to disambiguate the discovered references (e.g. more important
places are preferred). Disambiguation results in having the where
term, from the triple above, associated with the most likely 
corresponding concepts from the ontology. When we cannot detect
any locations, we attempt to use geographical scopes previously
inferred for the documents at the top search results. By doing this,
we assume that the most frequent geographical scope in the results
should correspond to the geographical context implicit in the query.
Experiments with CLEF topics [4] and sample queries from a
Web search engine show that the proposed methods are accurate,
and may have applications in improving search results.
The rest of this paper is organized as follows. We first formalize
the problem and describe related work to our research. Next, we
describe our approach for handling place names in queries, starting
with the general approach for disambiguating place references over
textual strings, then presenting the method for splitting a query into
a < what,relation,where > triple, and finally discussing the 
technique for exploiting geographic scopes previously assigned to 
documents in the result set. Section 4 presents evaluation results. 
Finally, we give some conclusions and directions for future research.
2. CONCEPTS AND RELATED WORK
Search engine performance depends on the ability to capture the
most likely meaning of a query as intended by the user [16]. 
Previous studies showed that a significant portion of the queries 
submitted to search engines are geographic [8, 14]. A recent enhancement
to search engine technology is the addition of geographic 
reasoning, combining geographic information systems and information
retrieval in order to build search engines that find information 
associated with given locations. The ability to recognize and reason
about the geographical terminology, given in the text documents
and user queries, is a crucial aspect of these geographical 
information retrieval (GIR) systems [4, 7].
Extracting and distinguishing different types of entities in text is
usually referred to as Named Entity Recognition (NER). For at least
a decade, this has been an important text mining task, and a key 
feature of the Message Understanding Conferences (MUC) [3]. NER
has been successfully automated with near-human performance,
but the specific problem of recognizing geographical references
presents additional challenges [9]. When handling named 
entities with a high level of detail, ambiguity problems arise more 
frequently. Ambiguity in geographical references is bi-directional [15].
The same name can be used for more than one location (referent
ambiguity), and the same location can have more than one name
(reference ambiguity). The former has another twist, since the same
name can be used for locations as well as for other class of 
entities, such as persons or company names (referent class ambiguity).
Besides the recognition of geographical expressions, GIR also 
requires that the recognized expressions be classified and grounded
to unique identifiers [11]. Grounding the recognized expressions
(e.g. associating them to coordinates or concepts at an ontology)
assures that they can be used in more advanced GIR tasks.
Previous works have addressed the tagging and grounding of
locations in Web pages, as well as the assignment of geographic
scopes to these documents [1, 7, 13]. This is a complementary 
aspect to the techniques described in this paper, since if we have the
Web pages tagged with location information, a search engine can
conveniently return pages with a geographical scope related to the
scope of the query. The task of handling geographical references
over documents is however considerably different from that of 
handling geographical references over queries. In our case, queries are
usually short and often do not constitute proper sentences. Text
mining techniques that make use of context information are 
difficult to apply for high accuracy.
Previous studies have also addressed the use of text mining and
automated classification techniques over search engine queries [16,
10]. However, most of these works did not consider place 
references or geographical categories. Again, these previously proposed
methods are difficult to apply to the geographic domain.
Gravano et. al. studied the classification of Web queries into two
types, namely local and global [5]. They defined a query as local if
its best matches on a Web search engine are likely to be local pages,
such as houses for sale. A number of classification algorithms
have been evaluated using search engine queries. However, their
experimental results showed that only a rather low precision and
recall could be achieved. The problem addressed in this paper is
also slightly different, since we are trying not only to detect local
queries but also to disambiguate the local of interest.
Wang et. al. proposed to go further than detecting local queries,
by also disambiguating the implicit local of interest [17]. The 
proposed approach works for both queries containing place references
and queries not containing them, by looking for dominant 
geographic references over query logs and text from search results.
In comparison, we propose simpler techniques based on matching
names from a geographic ontology. Our approach looks for spatial
relationships at the query string, and it also associates the place 
references to ontology concepts. In the case of queries not containing
explicit place references, we use geographical scopes previously
assigned to the documents, whereas Wang et. al. proposed to 
extract locations from the text of the top search results.
There are nowadays many geocoding, reverse-geocoding, and
mapping services on the Web that can be easily integrated with
other applications. Geocoding is the process of locating points on
the surface of the Earth from alphanumeric addressing data. Taking
a string with an address, a geocoder queries a geographical 
information system and returns interpolated coordinate values for the
given location. Instead of computing coordinates for a given place
reference, the technique described in this paper aims at assigning
references to the corresponding ontology concepts. However, if
each concept at the ontology contains associated coordinate 
information, the approach described here could also be used to build a
geocoding service. Most of such existing services are commercial
in nature, and there are no technical publications describing them.
A number of commercial search services have also started to 
support location-based searches. Google Local, for instance, initially
required the user to specify a location qualifier separately from the
search query. More recently, it added location look-up 
capabilities that extract locations from query strings. For example, in a
search for Pizza Seattle, Google Local returns local results for
pizza near Seattle, WA. However, the intrinsics of their solution
are not published, and their approach also does not handle 
locationimplicit queries. Moreover, Google Local does not take spatial 
relations into account.
In sum, there are already some studies on tagging geographical
references, but Web queries pose additional challenges which have
not been addressed. In this paper, we explain the proposed 
solutions for the identified problems.
3. HANDLINGQUERIESIN GIR SYSTEMS
Most GIR queries can be parsed to < what,relation,where >
triple, where the what term is used to specify the general 
nongeographical aspect of the information need, the where term is used
to specify the geographical areas of interest, and the relation term
is used to specify a spatial relationship connecting what and where.
While the what term can assume any form, in order to reflect any
information need, the relation and where terms should be part of a
controlled vocabulary. In particular, the relation term should refer
to a well-known geographical relation that the underlying GIR 
system can interpret (e.g. near or contained at), and the where
term should be disambiguated into a set of unique identifiers, 
corresponding to concepts at the ontology.
Different systems can use alternative schemes to take input queries
from the users. Three general strategies can be identified, and GIR
systems often support more than one of the following schemes:
Figure 1: Strategies for processing queries in Geographical Information Retrieval systems.
1. Input to the system is a textual query string. This is the 
hardest case, since we need to separate the query into the three
different components, and then we need to disambiguate the
where term into a set of unique identifiers.
2. Input to the system is provided in two separate strings, one
concerning the what term, and the other concerning the where.
The relation term can be either fixed (e.g. always assume the
near relation), specified together with the where string,
or provided separately from the users from a set of 
possible choices. Although there is no need for separating query
string into the different components, we still need to 
disambiguate the where term into a set of unique identifiers.
3. Input to the system is provided through a query string 
together with an unambiguous description of the geographical
area of interest (e.g. a sketch in a map, spatial coordinates
or a selection from a set of possible choices). No 
disambiguation is required, and therefore the techniques described
in this paper do not have to be applied.
The first two schemes depend on place name disambiguation.
Figure 1 illustrates how we propose to handle geographic queries
in these first two schemes. A common component is the algorithm
for disambiguating place references into corresponding ontology
concepts, which is described next.
3.1 From Place Names to Ontology Concepts
A required task in handling GIR queries consists of associating
a string containing a geographical reference with the set of 
corresponding concepts at the geographic ontology. We propose to do
this according to the pseudo-code listed at Algorithm 1.
The algorithm considers the cases where a second (or even more
than one) location is given to qualify a first (e.g. Paris, France).
It makes recursive calls to match each location, and relies on 
hierarchical part-of relations to detect if two locations share a common
hierarchy path. One of the provided locations should be more 
general and the other more specific, in the sense that there must exist
a part-of relationship among the associated concepts at the 
ontology (either direct or transitive). The most specific location is a
sub-region of the most general, and the algorithm returns the most
specific one (i.e. for Paris, France the algorithm returns the 
ontology concept associated with Paris, the capital city of France).
We also consider the cases where a geographical type expression
is used to qualify a given name (e.g. city of Lisbon or state
of New York). For instance the name Lisbon can correspond
to many different concepts at a geographical ontology, and type
Algorithm 1 Matching a place name with ontology concepts
Require: O = A geographic ontology
Require: GN = A string with the geographic name to be matched
1: L = An empty list
2: INDEX = The position in GN for the first occurrence of a comma,
semi-colon or bracket character
3: if INDEX is defined then
4: GN1 = The substring of GN from position 0 to INDEX
5: GN2 = The substring of GN from INDEX +1 to length(GN)
6: L1 = Algorithm1(O,GN1)
7: L2 = Algorithm1(O,GN2)
8: for each C1 in L1 do
9: for each C2 in L2 do
10: if C1 is an ancestor of C2 at O then
11: L = The list L after adding element C2
12: else if C1 is a descendant of C2 at O then
13: L = The list L after adding element C1
14: end if
15: end for
16: end for
17: else
18: GN = The string GN after removing case and diacritics
19: if GN contains a geographic type qualifier then
20: T = The substring of GN containing the type qualifier
21: GN = The substring of GN with the type qualifier removed
22: L = The list of concepts from O with name GN and type T
23: else
24: L = The list of concepts from O with name GN
25: end if
26: end if
27: return The list L
qualifiers can provide useful information for disambiguation. The
considered type qualifiers should also described at the ontologies
(e.g. each geographic concept should be associated to a type that is
also defined at the ontology, such as country, district or city).
Ideally, the geographical reference provided by the user should
be disambiguated into a single ontology concept. However, this is
not always possible, since the user may not provide all the required
information (i.e. a type expression or a second qualifying location).
The output is therefore a list with the possible concepts being 
referred to by the user. In a final step, we propose to sort this list,
so that if a single concept is required as output, we can use the one
that is ranked higher. The sorting procedure reflects the likelihood
of each concept being indeed the one referred to. We propose to
rank concepts according to the following heuristics:
1. The geographical type expression associated with the 
ontology concept. For the same name, a country is more likely to
be referenced than a city, and in turn a city more likely to be
referenced than a street.
2. Number of ancestors at the ontology. Top places at the 
ontology tend to be more general, and are therefore more likely
to be referenced in search engine queries.
3. Population count. Highly populated places are better known,
and therefore more likely to be referenced in queries.
4. Population counts from direct ancestors at the ontology. 
Subregions of highly populated places are better known, and also
more likely to be referenced in search engine queries.
5. Occurrence frequency over Web documents (e.g. Google
counts) for the geographical names. Places names that occur
more frequently over Web documents are also more likely to
be referenced in search engine queries.
6. Number of descendants at the ontology. Places that have
more sub-regions tend to be more general, and are therefore
more likely to be mentioned in search engine queries.
7. String size for the geographical names. Short names are more
likely to be mentioned in search engine queries.
Algorithm 1, plus the ranking procedure, can already handle GIR
queries where the where term is given separately from the what and
relation terms. However, if the query is given in a single string, we
require the identification of the associated < what,relation,where >
triple, before disambiguating the where term into the corresponding
ontology concepts. This is described in the following Section.
3.2 Handling Single Query Strings
Algorithm 2 provides the mechanism for separating a query string
into a < what,relation,where > triple. It uses Algorithm 1 to find
the where term, disambiguating it into a set of ontology concepts.
The algorithm starts by tokenizing the query string into 
individual words, also taking care of removing case and diacritics. We
have a simple tokenizer that uses the space character as a word 
delimiter, but we could also have a tokenization approach similar to
the proposal of Wang et. al. which relies on Web occurrence 
statistics to avoid breaking collocations [17]. In the future, we plan on
testing if this different tokenization scheme can improve results.
Next, the algorithm tests different possible splits of the query,
building the what, relation and where terms through 
concatenations of the individual tokens. The relation term is matched against
a list of possible values (e.g. near, at, around, or south
of), corresponding to the operators that are supported by the GIR
system. Note that is also the responsibility of the underlying GIR
system to interpret the actual meaning of the different spatial 
relations. Algorithm 1 is used to check whether a where term 
constitutes a geographical reference or not. We also check if the last
word in the what term belongs to a list of exceptions, containing for
instance first names of people in different languages. This ensures
that a query like Denzel Washington is appropriately handled.
If the algorithm succeeds in finding valid relation and where
terms, then the corresponding triple is returned. Otherwise, we 
return a triple with the what term equaling the query string, and the
relation and where terms set as empty. If the entire query string
constitutes a geographical reference, we return a triple with the
what term set to empty, the where term equaling the query string,
and the relation term set the DEFINITION (i.e. these queries
should be answered with information about the given place 
references). The algorithm also handles query strings where more
than one geographical reference is provided, using and or an
equivalent preposition, together with a recursive call to Algorithm
2. A query like Diamond trade in Angola and South Africa is
Algorithm 2 Get < what,relation,where > from a query string
Require: O = A geographical ontology
Require: Q = A non-empty string with the query
1: Q = The string Q after removing case and diacritics
2: TOKENS[0..N] = An array of strings with the individual words of Q
3: N = The size of the TOKENS array
4: for INDEX = 0 to N do
5: if INDEX = 0 then
6: WHAT = Concatenation of TOKENS[0..INDEX −1]
7: LASTWHAT = TOKENS[INDEX −1]
8: else
9: WHAT = An empty string
10: LASTWHAT = An empty string
11: end if
12: WHERE = Concatenation of TOKENS[INDEX..N]
13: RELATION = An empty string
14: for INDEX2 = INDEX to N −1 do
15: RELATION2 = Concatenation of TOKENS[INDEX..INDEX2]
16: if RELATION2 is a valid geographical relation then
17: WHERE = Concatenation of S[INDEX2 +1..N]
18: RELATION = RELATION2;
19: end if
20: end for
21: if RELATION = empty AND LASTWHAT in an exception then
22: TESTGEO = FALSE
23: else
24: TESTGEO = TRUE
25: end if
26: if TESTGEO AND Algorithm1(WHERE) <> EMPTY then
27: if WHERE ends with AND SURROUNDINGS then
28: RELATION = The string NEAR;
29: WHERE = The substring of WHERE with AND 
SURROUNDINGS removed
30: end if
31: if WHAT ends with AND or similar) then
32: < WHAT,RELATION,WHERE2 >= Algorithm2(WHAT)
33: WHERE = Concatenation of WHERE with WHERE2
34: end if
35: if RELATION = An empty string then
36: if WHAT = An empty string then
37: RELATION = The string DEFINITION
38: else
39: RELATION = The string CONTAINED-AT
40: end if
41: end if
42: else
43: WHAT = The string Q
44: WHERE = An empty string
45: RELATION = An empty string
46: end if
47: end for
48: return < WHAT,RELATION,WHERE >
therefore appropriately handled. Finally, if the geographical 
reference in the query is complemented with an expression similar to
and its surroundings, the spatial relation (which is assumed to be
CONTAINED-AT if none is provided) is changed to NEAR.
3.3 From Search Results to Query Locality
The procedures given so far are appropriate for handling queries
where a place reference is explicitly mentioned. However, the fact
that a query can be associated with a geographical context may
not be directly observable in the query itself, but rather from the
results returned. For instance, queries like recommended hotels
for SIGIR 2006 or SeaFair 2006 lodging can be seen to refer to
the city of Seattle. Although they do not contain an explicit place
reference, we expect results to be about hotels in Seattle.
In the cases where a query does not contain place references, we
start by assuming that the top results from a search engine represent
the most popular and correct context and usage for the query. We
Topic What Relation Where TGN concepts ML concepts
Vegetable Exporters of Europe Vegetable Exporters CONTAINED-AT Europe 1 1
Trade Unions in Europe Trade Unions CONTAINED-AT Europe 1 1
Roman cities in the UK and Germany Roman cities CONTAINED-AT UK and Germany 6 2
Cathedrals in Europe Cathedrals CONTAINED-AT Europe 1 1
Car bombings near Madrid Car bombings NEAR Madrid 14 2
Volcanos around Quito Volcanos NEAR Quito 4 1
Cities within 100km of Frankfurt Cities NEAR Frankfurt 3 1
Russian troops in south(ern) Caucasus Russian troops in south(ern) CONTAINED-AT Caucasus 2 1
Cities near active volcanoes (This topic could not be appropriately handled - the relation and where terms are returned empty)
Japanese rice imports (This topic could not be appropriately handled - the relation and where terms are returned empty)
Table 1: Example topics from the GeoCLEF evaluation campaigns and the corresponding < what,relation,where > triples.
then propose to use the distributional characteristics of 
geographical scopes previously assigned to the documents corresponding to
these top results. In a previous work, we presented a text mining
approach for assigning documents with corresponding 
geographical scopes, defined at an ontology, that worked as an offline 
preprocessing stage in a GIR system [13]. This pre-processing step is a
fundamental stage of GIR, and it is reasonable to assume that this
kind of information would be available on any system. Similarly to
Wang et. al., we could also attempt to process the results on-line,
in order to detect place references in the documents [17]. However,
a GIR system already requires the offline stage.
For the top N documents given at the results, we check the 
geographic scopes that were assigned to them. If a significant portion
of the results are assigned to the same scope, than the query can be
seen to be related to the corresponding geographic concept. This
assumption could even be relaxed, for instance by checking if the
documents belong to scopes that are hierarchically related.
4. EVALUATION EXPERIMENTS
We used three different ontologies in evaluation experiments,
namely the Getty thesaurus of geographic names (TGN) [6] and
two specific resources developed at our group, here referred to as
the PT and ML ontologies [2]. TGN and ML include global 
geographical information in multiple languages (although TGN is 
considerably larger), while the PT ontology focuses on the Portuguese
territory with a high detail. Place types are also different across
ontologies, as for instance PT includes street names and postal 
addresses, whereas ML only goes to the level of cities. The reader
should refer to [2, 6] for a complete description of these resources.
Our initial experiments used Portuguese and English topics from
the GeoCLEF 2005 and 2006 evaluation campaigns. Topics in 
GeoCLEF correspond to query strings that can be used as input to a GIR
system [4]. ImageCLEF 2006 also included topics specifying place
references, and participants were encouraged to run their GIR 
systems on them. Our experiments also considered this dataset. For
each topic, we measured if Algorithm 2 was able to find the 
corresponding < what,relation,where > triple. The ontologies used
in this experiment were the TGN and ML, as topics were given in
multiple languages and covered the whole globe.
Dataset Number of Correct Triples Time per Query
Queries ML TGN ML TGN
GeoCLEF05 EN 25 19 20
GeoCLEF05 PT 25 20 18 288.1 334.5
GeoCLEF06 EN 32 28 19 msec msec
GeoCLEF06 PT 25 23 11
ImgCLEF06 EN 24 16 18
Table 2: Summary of results over CLEF topics.
Table 1 illustrates some of the topics, and Table 2 summarizes
the obtained results. The tables show that the proposed technique
adequately handles most of these queries. A manual inspection of
the ontology concepts that were returned for each case also revealed
that the where term was being correctly disambiguated. Note that
the TGN ontology indeed added some ambiguity, as for instance
names like Madrid can correspond to many different places around
the globe. It should also be noted that some of the considered 
topics are very hard for an automated system to handle. Some of them
were ambiguous (e.g. in Japanese rice imports, the query can
be said to refer either rice imports in Japan or imports of Japanese
rice), and others contained no direct geographical references (e.g.
cities near active volcanoes). Besides these very hard cases, we
also missed some topics due to their usage of place adjectives and
specific regions that are not defined at the ontologies (e.g. 
environmental concerns around the Scottish Trossachs).
In a second experiment, we used a sample of around 100,000
real search engine queries. The objective was to see if a 
significant number of these queries were geographical in nature, also
checking if the algorithm did not produce many mistakes by 
classifying a query as geographical when that was not the case. The
Portuguese ontology was used in this experiment, and queries were
taken from the logs of a Portuguese Web search engine available
at www.tumba.pt. Table 3 summarizes the obtained results. Many
queries were indeed geographical (around 3.4%, although 
previous studies reported values above 14% [8]). A manual inspection
showed that the algorithm did not produce many false positives,
and the geographical queries were indeed correctly split into correct
< what,relation,where > triple. The few mistakes we encountered
were related to place names that were more frequently used in other
contexts (e.g. in Teófilo Braga we have the problem that Braga
is a Portuguese district, and Teófilo Braga was a well known 
Portuguese writer and politician). The addition of more names to the
exception list can provide a workaround for most of these cases.
Value
Num. Queries 110,916
Num. Queries without Geographical References 107,159 (96.6%)
Num. Queries with Geographical References 3,757 (3.4%)
Table 3: Results from an experiment with search engine logs.
We also tested the procedure for detecting queries that are 
implicitly geographical with a small sample of queries from the logs.
For instance, for the query Estádio do Dragão (e.g. home stadium
for a soccer team from Porto), the correct geographical context can
be discovered from the analysis of the results (more than 75% of
the top 20 results are assigned with the scope Porto). For future
work, we plan on using a larger collection of queries to evaluate
this aspect. Besides queries from the search engine logs, we also
plan on using the names of well-known buildings, monuments and
other landmarks, as they have a strong geographical connotation.
Finally, we also made a comparative experiment with 2 popular
geocoders, Maporama and Microsoft"s Mappoint. The objective
was to compare Algorithm 1 with other approaches, in terms of 
being able to correctly disambiguate a string with a place reference.
Civil Parishes from Lisbon Maporama Mappoint Ours
Coded refs. (out of 53) 9 (16.9%) 30 (56,6%) 15 (28.3%)
Avg. Time per ref. (msec) 506.23 1235.87 143.43
Civil Parishes from Porto Maporama Mappoint Ours
Coded refs. (out of 15) 0 (0%) 2 (13,3%) 5 (33.3%)
Avg. Time per ref. (msec) 514.45 991.88 132.14
Table 4: Results from a comparison with geocoding services.
The Portuguese ontology was used in this experiment, taking as 
input the names of civil parishes from the Portuguese municipalities
of Lisbon and Porto, and checking if the systems were able to 
disambiguate the full name (e.g. Campo Grande, Lisboa or Foz
do Douro, Porto) into the correct geocode. We specifically 
measured whether our approach was better at unambiguously returning
geocodes given the place reference (i.e. return the single correct
code), and providing results rapidly. Table 4 shows the obtained
results, and the accuracy of our method seems comparable to the
commercial geocoders. Note that for Maporama and Mappoint, the
times given at Table 4 include fetching results from the Web, but we
have no direct way of accessing the geocoding algorithms (in both
cases, fetching static content from the Web servers takes around
125 milliseconds). Although our approach cannot unambiguously
return the correct geocode in most cases (only 20 out of a total of
68 cases), it nonetheless returns results that a human user can 
disambiguate (e.g. for Madalena, Lisboa we return both a street and
a civil parish), as opposed to the other systems that often did not
produce results. Moreover, if we consider the top geocode 
according to the ranking procedure described in Section 3.1, or if we use
a type qualifier in the name (e.g. civil parish of Campo Grande,
Lisboa), our algorithm always returns the correct geocode.
5. CONCLUSIONS
This paper presented simple approaches for handling place 
references in search engine queries. This is a hard text mining problem,
as queries are often ambiguous or underspecify information needs.
However, our initial experiments indicate that for many queries, the
referenced places can be determined effectively. Unlike the 
techniques proposed by Wang et. al. [17], we mainly focused on 
recognizing spatial relations and associating place names to ontology
concepts. The proposed techniques were employed in the prototype
system that we used for participating in GeoCLEF 2006. In queries
where a geographical reference is not explicitly mentioned, we 
propose to use the results for the query, exploiting geographic scopes
previously assigned to these documents. In the future, we plan on
doing a careful evaluation of this last approach. Another idea that
we would like to test involves the integration of a spelling 
correction mechanism [12] into Algorithm 1, so that incorrectly spelled
place references can be matched to ontology concepts.
The proposed techniques for handling geographic queries can
have many applications in improving GIR systems or even general
purpose search engines. After place references are appropriately
disambiguated into ontology concepts, a GIR system can use them
to retrieve relevant results, through the use of appropriate index
structures (e.g. indexing the spatial coordinates associated with 
ontology concepts) and provided that the documents are also assigned
to scopes corresponding to ontology concepts. A different GIR
strategy can involve query expansion, by taking the where terms
from the query and using the ontology to add names from 
neighboring locations. In a general purpose search engine, and if a local
query is detected, we can forward users to a GIR system, which
should be better suited for properly handling the query. The regular
Google search interface already does this, by presenting a link to
Google Local when it detects a geographical query.
6. REFERENCES
[1] E. Amitay, N. Har"El, R. Sivan, and A. Soffer. Web-a-Where:
Geotagging Web content. In Proceedings of SIGIR-04, the
27th Conference on research and development in information
retrieval, 2004.
[2] M. Chaves, M. J. Silva, and B. Martins. A Geographic
Knowledge Base for Semantic Web Applications. In
Proceedings of SBBD-05, the 20th Brazilian Symposium on
Databases, 2005.
[3] N. A. Chinchor. Overview of MUC-7/MET-2. In
Proceedings of MUC-7, the 7th Message Understanding
Conference, 1998.
[4] F. Gey, R. Larson, M. Sanderson, H. Joho, and P. Clough.
GeoCLEF: the CLEF 2005 cross-language geographic
information retrieval track. In Working Notes for the CLEF
2005 Workshop, 2005.
[5] L. Gravano, V. Hatzivassiloglou, and R. Lichtenstein.
Categorizing Web queries according to geographical locality.
In Proceedings of CIKM-03, the 12th Conference on
Information and knowledge management, 2003.
[6] P. Harpring. Proper words in proper places: The thesaurus of
geographic names. MDA Information, 3, 1997.
[7] C. Jones, R. Purves, A. Ruas, M. Sanderson, M. Sester,
M. van Kreveld, and R. Weibel. Spatial information retrieval
and geographical ontologies: An overview of the SPIRIT
project. In Proceedings of SIGIR-02, the 25th Conference on
Research and Development in Information Retrieval, 2002.
[8] J. Kohler. Analyzing search engine queries for the use of
geographic terms, 2003. (MSc Thesis).
[9] A. Kornai and B. Sundheim, editors. Proceedings of the
NAACL-HLT Workshop on the Analysis of Geographic
References, 2003.
[10] Y. Li, Z. Zheng, and H. Dai. KDD CUP-2005 report: Facing
a great challenge. SIGKDD Explorations, 7, 2006.
[11] D. Manov, A. Kiryakov, B. Popov, K. Bontcheva,
D. Maynard, and H. Cunningham. Experiments with
geographic knowledge for information extraction. In
Proceedings of the NAACL-HLT Workshop on the Analysis of
Geographic References, 2003.
[12] B. Martins and M. J. Silva. Spelling correction for search
engine queries. In Proceedings of EsTAL-04, España for
Natural Language Processing, 2004.
[13] B. Martins and M. J. Silva. A graph-ranking algorithm for
geo-referencing documents. In Proceedings of ICDM-05, the
5th IEEE International Conference on Data Mining, 2005.
[14] L. Souza, C. J. Davis, K. Borges, T. Delboni, and
A. Laender. The role of gazetteers in geographic knowledge
discovery on the web. In Proceedings of LA-Web-05, the 3rd
Latin American Web Congress, 2005.
[15] E. Tjong, K. Sang, and F. D. Meulder. Introduction to the
CoNLL-2003 shared task: Language-Independent Named
Entity Recognition. In Proceedings of CoNLL-2003, the 7th
Conference on Natural Language Learning, 2003.
[16] D. Vogel, S. Bickel, P. Haider, R. Schimpfky, P. Siemen,
S. Bridges, and T. Scheffer. Classifying search engine
queries using the Web as background knowledge. SIGKDD
Explorations Newsletter, 7(2):117-122, 2005.
[17] L. Wang, C. Wang, X. Xie, J. Forman, Y. Lu, W.-Y. Ma, and
Y. Li. Detecting dominant locations from search queries. In
Proceedings of SIGIR-05, the 28th Conference on Research
and development in information retrieval, 2005.
Machine Learning for Information Architecture in a Large
Governmental Website ∗
Miles Efron
School of Information & Library Science
CB#3360, 100 Manning Hall
University of North Carolina
Chapel Hill, NC 27599-3360
efrom@ils.unc.edu
Jonathan Elsas
School of Information & Library Science
CB#3360, 100 Manning Hall
University of North Carolina
Chapel Hill, NC 27599-3360
jelsas@email.unc.edu
Gary Marchionini
School of Information & Library Science
CB#3360, 100 Manning Hall
University of North Carolina
Chapel Hill, NC 27599-3360
march@ils.unc.edu
Junliang Zhang
School of Information & Library Science
CB#3360, 100 Manning Hall
University of North Carolina
Chapel Hill, NC 27599-3360
junliang@email.unc.edu
ABSTRACT
This paper describes ongoing research into the application
of machine learning techniques for improving access to 
governmental information in complex digital libraries. Under
the auspices of the GovStat Project, our goal is to identify a
small number of semantically valid concepts that adequately
spans the intellectual domain of a collection. The goal of this
discovery is twofold. First we desire a practical aid for 
information architects. Second, automatically derived 
documentconcept relationships are a necessary precondition for 
realworld deployment of many dynamic interfaces. The current
study compares concept learning strategies based on three
document representations: keywords, titles, and full-text. In
statistical and user-based studies, human-created keywords
provide significant improvements in concept learning over
both title-only and full-text representations.
Categories and Subject Descriptors
H.3.7 [Information Storage and Retrieval]: Digital 
Libraries-Systems Issues, User Issues; H.3.3 [Information
Storage and Retrieval]: Information Search and 
RetrievalClustering
General Terms
Design, Experimentation
1. INTRODUCTION
The GovStat Project is a joint effort of the University
of North Carolina Interaction Design Lab and the 
University of Maryland Human-Computer Interaction Lab1
. 
Citing end-user difficulty in finding governmental information
(especially statistical data) online, the project seeks to 
create an integrated model of user access to US government
statistical information that is rooted in realistic data 
models and innovative user interfaces. To enable such models
and interfaces, we propose a data-driven approach, based
on data mining and machine learning techniques. In 
particular, our work analyzes a particular digital library-the
website of the Bureau of Labor Statistics2
(BLS)-in efforts
to discover a small number of linguistically meaningful 
concepts, or bins, that collectively summarize the semantic
domain of the site.
The project goal is to classify the site"s web content 
according to these inferred concepts as an initial step towards
data filtering via active user interfaces (cf. [13]). Many
digital libraries already make use of content classification,
both explicitly and implicitly; they divide their resources
manually by topical relation; they organize content into 
hierarchically oriented file systems. The goal of the present
1
http://www.ils.unc.edu/govstat
2
http://www.bls.gov
151
research is to develop another means of browsing the content
of these collections. By analyzing the distribution of terms
across documents, our goal is to supplement the agency"s
pre-existing information structures. Statistical learning 
technologies are appealing in this context insofar as they stand
to define a data-driven-as opposed to an 
agency-drivennavigational structure for a site.
Our approach combines supervised and unsupervised 
learning techniques. A pure document clustering [12] approach
to such a large, diverse collection as BLS led to poor results
in early tests [6]. But strictly supervised techniques [5] are
inappropriate, too. Although BLS designers have defined
high-level subject headings for their collections, as we 
discuss in Section 2, this scheme is less than optimal. Thus we
hope to learn an additional set of concepts by letting the
data speak for themselves.
The remainder of this paper describes the details of our
concept discovery efforts and subsequent evaluation. In 
Section 2 we describe the previously existing, human-created
conceptual structure of the BLS website. This section also
describes evidence that this structure leaves room for 
improvement. Next (Sections 3-5), we turn to a description
of the concepts derived via content clustering under three
document representations: keyword, title only, and full-text.
Section 6 describes a two-part evaluation of the derived 
conceptual structures. Finally, we conclude in Section 7 by 
outlining upcoming work on the project.
2. STRUCTURING ACCESS TO THE BLS
WEBSITE
The Bureau of Labor Statistics is a federal government
agency charged with compiling and publishing statistics 
pertaining to labor and production in the US and abroad. Given
this broad mandate, the BLS publishes a wide array of 
information, intended for diverse audiences. The agency"s
website acts as a clearinghouse for this process. With over
15,000 text/html documents (and many more documents if
spreadsheets and typeset reports are included), providing
access to the collection provides a steep challenge to 
information architects.
2.1 The Relation Browser
The starting point of this work is the notion that access
to information in the BLS website could be improved by
the addition of a dynamic interface such as the relation
browser described by Marchionini and Brunk [13]. The 
relation browser allows users to traverse complex data sets by
iteratively slicing the data along several topics. In Figure
1 we see a prototype instantiation of the relation browser,
applied to the FedStats website3
.
The relation browser supports information seeking by 
allowing users to form queries in a stepwise fashion, slicing and
re-slicing the data as their interests dictate. Its motivation
is in keeping with Shneiderman"s suggestion that queries
and their results should be tightly coupled [2]. Thus in 
Fig3
http://www.fedstats.gov
Figure 1: Relation Browser Prototype
ure 1, users might limit their search set to those documents
about energy. Within this subset of the collection, they
might further eliminate documents published more than a
year ago. Finally, they might request to see only documents
published in PDF format.
As Marchionini and Brunk discuss, capturing the 
publication date and format of documents is trivial. But successful
implementations of the relation browser also rely on topical
classification. This presents two stumbling blocks for system
designers:
• Information architects must define the appropriate set
of topics for their collection
• Site maintainers must classify each document into its
appropriate categories
These tasks parallel common problems in the metadata
community: defining appropriate elements and marking up
documents to support metadata-aware information access.
Given a collection of over 15,000 documents, these 
hurdles are especially daunting, and automatic methods of 
approaching them are highly desirable.
2.2 A Pre-Existing Structure
Prior to our involvement with the project, designers at
BLS created a shallow classificatory structure for the most
important documents in their website. As seen in Figure 2,
the BLS home page organizes 65 top-level documents into
15 categories. These include topics such as Employment and
Unemployment, Productivity, and Inflation and Spending.
152
Figure 2: The BLS Home Page
We hoped initially that these pre-defined categories could
be used to train a 15-way document classifier, thus 
automating the process of populating the relation browser altogether.
However, this approach proved unsatisfactory. In personal
meetings, BLS officials voiced dissatisfaction with the 
existing topics. Their form, it was argued, owed as much to
the institutional structure of BLS as it did to the inherent
topology of the website"s information space. In other words,
the topics reflected official divisions rather than semantic
clusters. The BLS agents suggested that re-designing this
classification structure would be desirable.
The agents" misgivings were borne out in subsequent 
analysis. The BLS topics comprise a shallow classificatory 
structure; each of the 15 top-level categories is linked to a small
number of related pages. Thus there are 7 pages associated
with Inflation. Altogether, the link structure of this 
classificatory system contains 65 documents; that is, excluding
navigational links, there are 65 documents linked from the
BLS home page, where each hyperlink connects a document
to a topic (pages can be linked to multiple topics). Based on
this hyperlink structure, we defined M, a symmetric 65×65
matrix, where mij counts the number of topics in which 
documents i and j are both classified on the BLS home page. To
analyze the redundancy inherent in the pre-existing 
structure, we derived the principal components of M (cf. [11]).
Figure 3 shows the resultant scree plot4
.
Because all 65 documents belong to at least one BLS topic,
4
A scree plot shows the magnitude of the kth
eigenvalue
versus its rank. During principal component analysis scree
plots visualize the amount of variance captured by each 
component.
m00M0M
0
1010M10M
10
2020M20M
20
3030M30M
30
4040M40M
40
5050M50M
50
6060M60M
60
m00M0M
0
22M2M
2
44M4M
4
66M6M
6
88M8M
8
1010M10M
10
1212M12M
12
1414M14M
14
Eigenvalue RankMEigenvalue RankM
Eigenvalue Rank
Eigenvlue MagnitudeMEigenvlue MagnitudeM
EigenvlueMagnitude
Figure 3: Scree Plot of BLS Categories
the rank of M is guaranteed to be less than or equal to
15 (hence, eigenvalues 16 . . . 65 = 0). What is surprising
about Figure 3, however, is the precipitous decline in 
magnitude among the first four eigenvalues. The four largest
eigenvlaues account for 62.2% of the total variance in the
data. This fact suggests a high degree of redundancy among
the topics. Topical redundancy is not in itself problematic.
However, the documents in this very shallow classificatory
structure are almost all gateways to more specific 
information. Thus the listing of the Producer Price Index under
three categories could be confusing to the site"s users. In
light of this potential for confusion and the agency"s own 
request for redesign, we undertook the task of topic discovery
described in the following sections.
3. A HYBRID APPROACH TO TOPIC
DISCOVERY
To aid in the discovery of a new set of high-level topics for
the BLS website, we turned to unsupervised machine 
learning methods. In efforts to let the data speak for themselves,
we desired a means of concept discovery that would be based
not on the structure of the agency, but on the content of the
material. To begin this process, we crawled the BLS 
website, downloading all documents of MIME type text/html.
This led to a corpus of 15,165 documents. Based on this
corpus, we hoped to derive k ≈ 10 topical categories, such
that each document di is assigned to one or more classes.
153
Document clustering (cf. [16]) provided an obvious, but
only partial solution to the problem of automating this type
of high-level information architecture discovery. The 
problems with standard clustering are threefold.
1. Mutually exclusive clusters are inappropriate for 
identifying the topical content of documents, since 
documents may be about many subjects.
2. Due to the heterogeneity of the data housed in the
BLS collection (tables, lists, surveys, etc.), many 
documents" terms provide noisy topical information.
3. For application to the relation browser, we require a
small number (k ≈ 10) of topics. Without significant
data reduction, term-based clustering tends to deliver
clusters at too fine a level of granularity.
In light of these problems, we take a hybrid approach to
topic discovery. First, we limit the clustering process to
a sample of the entire collection, described in Section 4.
Working on a focused subset of the data helps to overcome
problems two and three, listed above. To address the 
problem of mutual exclusivity, we combine unsupervised with
supervised learning methods, as described in Section 5.
4. FOCUSING ON CONTENT-RICH
DOCUMENTS
To derive empirically evidenced topics we initially turned
to cluster analysis. Let A be the n×p data matrix with n 
observations in p variables. Thus aij shows the measurement
for the ith
observation on the jth
variable. As described
in [12], the goal of cluster analysis is to assign each of the
n observations to one of a small number k groups, each of
which is characterized by high intra-cluster correlation and
low inter-cluster correlation. Though the algorithms for 
accomplishing such an arrangement are legion, our analysis
focuses on k-means clustering5
, during which, each 
observation oi is assigned to the cluster Ck whose centroid is closest
to it, in terms of Euclidean distance. Readers interested in
the details of the algorithm are referred to [12] for a 
thorough treatment of the subject.
Clustering by k-means is well-studied in the statistical
literature, and has shown good results for text analysis (cf.
[8, 16]). However, k-means clustering requires that the 
researcher specify k, the number of clusters to define. When
applying k-means to our 15,000 document collection, 
indicators such as the gap statistic [17] and an analysis of
the mean-squared distance across values of k suggested that
k ≈ 80 was optimal. This paramterization led to 
semantically intelligible clusters. However, 80 clusters are far too
many for application to an interface such as the relation
5
We have focused on k-means as opposed to other clustering
algorithms for several reasons. Chief among these is the
computational efficiency enjoyed by the k-means approach.
Because we need only a flat clustering there is little to be
gained by the more expensive hierarchical algorithms. In
future work we will turn to model-based clustering [7] as a
more principled method of selecting the number of clusters
and of representing clusters.
browser. Moreover, the granularity of these clusters was 
unsuitably fine. For instance, the 80-cluster solution derived
a cluster whose most highly associated words (in terms of
log-odds ratio [1]) were drug, pharmacy, and chemist. These
words are certainly related, but they are related at a level
of specificity far below what we sought.
To remedy the high dimensionality of the data, we 
resolved to limit the algorithm to a subset of the collection.
In consultation with employees of the BLS, we continued
our analysis on documents that form a series titled From
the Editor"s Desk6
. These are brief articles, written by BLS
employees. BLS agents suggested that we focus on the 
Editor"s Desk because it is intended to span the intellectual
domain of the agency. The column is published daily, and
each entry describes an important current issue in the BLS
domain. The Editor"s Desk column has been written daily
(five times per week) since 1998. As such, we operated on a
set of N = 1279 documents.
Limiting attention to these 1279 documents not only 
reduced the dimensionality of the problem. It also allowed
the clustering process to learn on a relatively clean data set.
While the entire BLS collection contains a great deal of 
nonprose text (i.e. tables, lists, etc.), the Editor"s Desk 
documents are all written in clear, journalistic prose. Each 
document is highly topical, further aiding the discovery of 
termtopic relations. Finally, the Editor"s Desk column provided
an ideal learning environment because it is well-supplied
with topical metadata. Each of the 1279 documents 
contains a list of one or more keywords. Additionally, a subset
of the documents (1112) contained a subject heading. This
metadata informed our learning and evaluation, as described
in Section 6.1.
5. COMBINING SUPERVISED AND
UNSUPERVISED LEARNING FORTOPIC
DISCOVERY
To derive suitably general topics for the application of a
dynamic interface to the BLS collection, we combined 
document clustering with text classification techniques. 
Specifically, using k-means, we clustered each of the 1279 
documents into one of k clusters, with the number of clusters
chosen by analyzing the within-cluster mean squared 
distance at different values of k (see Section 6.1). 
Constructing mutually exclusive clusters violates our assumption that
documents may belong to multiple classes. However, these
clusters mark only the first step in a two-phase process of
topic identification. At the end of the process, 
documentcluster affinity is measured by a real-valued number.
Once the Editor"s Desk documents were assigned to 
clusters, we constructed a k-way classifier that estimates the
strength of evidence that a new document di is a member
of class Ck. We tested three statistical classification 
techniques: probabilistic Rocchio (prind), naive Bayes, and 
support vector machines (SVMs). All were implemented using
McCallum"s BOW text classification library [14]. Prind is a
probabilistic version of the Rocchio classification algorithm
[9]. Interested readers are referred to Joachims" article for
6
http://www.bls.gov/opub/ted
154
further details of the classification method. Like prind, naive
Bayes attempts to classify documents into the most 
probable class. It is described in detail in [15]. Finally, support
vector machines were thoroughly explicated by Vapnik [18],
and applied specifically to text in [10]. They define a 
decision boundary by finding the maximally separating 
hyperplane in a high-dimensional vector space in which document
classes become linearly separable.
Having clustered the documents and trained a suitable
classifier, the remaining 14,000 documents in the collection
are labeled by means of automatic classification. That is, for
each document di we derive a k-dimensional vector, 
quantifying the association between di and each class C1 . . . Ck.
Deriving topic scores via naive Bayes for the entire 
15,000document collection required less than two hours of CPU
time. The output of this process is a score for every 
document in the collection on each of the automatically 
discovered topics. These scores may then be used to populate a
relation browser interface, or they may be added to a 
traditional information retrieval system. To use these weights in
the relation browser we currently assign to each document
the two topics on which it scored highest. In future work we
will adopt a more rigorous method of deriving 
documenttopic weight thresholds. Also, evaluation of the utility of
the learned topics for users will be undertaken.
6. EVALUATION OF CONCEPT
DISCOVERY
Prior to implementing a relation browser interface and
undertaking the attendant user studies, it is of course 
important to evaluate the quality of the inferred concepts, and
the ability of the automatic classifier to assign documents
to the appropriate subjects. To evaluate the success of the
two-stage approach described in Section 5, we undertook
two experiments. During the first experiment we compared
three methods of document representation for the 
clustering task. The goal here was to compare the quality of 
document clusters derived by analysis of full-text documents,
documents represented only by their titles, and documents
represented by human-created keyword metadata. During
the second experiment, we analyzed the ability of the 
statistical classifiers to discern the subject matter of documents
from portions of the database in addition to the Editor"s
Desk.
6.1 Comparing Document Representations
Documents from The Editor"s Desk column came 
supplied with human-generated keyword metadata. 
Additionally, The titles of the Editor"s Desk documents tend to be
germane to the topic of their respective articles. With such
an array of distilled evidence of each document"s subject
matter, we undertook a comparison of document 
representations for topic discovery by clustering. We hypothesized
that keyword-based clustering would provide a useful model.
But we hoped to see whether comparable performance could
be attained by methods that did not require extensive 
human indexing, such as the title-only or full-text 
representations. To test this hypothesis, we defined three modes of
document representation-full-text, title-only, and keyword
only-we generated three sets of topics, Tfull, Ttitle, and
Tkw, respectively.
Topics based on full-text documents were derived by 
application of k-means clustering to the 1279 Editor"s Desk 
documents, where each document was represented by a 
1908dimensional vector. These 1908 dimensions captured the
TF.IDF weights [3] of each term ti in document dj, for all
terms that occurred at least three times in the data. To 
arrive at the appropriate number of clusters for these data, we
inspected the within-cluster mean-squared distance for each
value of k = 1 . . . 20. As k approached 10 the reduction in
error with the addition of more clusters declined notably,
suggesting that k ≈ 10 would yield good divisions. To 
select a single integer value, we calculated which value of k led
to the least variation in cluster size. This metric stemmed
from a desire to suppress the common result where one large
cluster emerges from the k-means algorithm, accompanied
by several accordingly small clusters. Without reason to
believe that any single topic should have dramatically high
prior odds of document membership, this heuristic led to
kfull = 10.
Clusters based on document titles were constructed 
similarly. However, in this case, each document was represented
in the vector space spanned by the 397 terms that occur
at least twice in document titles. Using the same method
of minimizing the variance in cluster membership ktitle-the
number of clusters in the title-based representation-was also
set to 10.
The dimensionality of the keyword-based clustering was
very similar to that of the title-based approach. There were
299 keywords in the data, all of which were retained. The
median number of keywords per document was 7, where a
keyword is understood to be either a single word, or a 
multiword term such as consumer price index. It is worth noting
that the keywords were not drawn from any controlled 
vocabulary; they were assigned to documents by publishers at
the BLS. Using the keywords, the documents were clustered
into 10 classes.
To evaluate the clusters derived by each method of 
document representation, we used the subject headings that were
included with 1112 of the Editor"s Desk documents. Each
of these 1112 documents was assigned one or more subject
headings, which were withheld from all of the cluster 
applications. Like the keywords, subject headings were assigned
to documents by BLS publishers. Unlike the keywords, 
however, subject headings were drawn from a controlled 
vocabulary. Our analysis began with the assumption that 
documents with the same subject headings should cluster 
together. To facilitate this analysis, we took a conservative
approach; we considered multi-subject classifications to be
unique. Thus if document di was assigned to a single 
subject prices, while document dj was assigned to two subjects,
international comparisons, prices, documents di and dj are
not considered to come from the same class.
Table 1 shows all Editor"s Desk subject headings that were
assigned to at least 10 documents. As noted in the table,
155
Table 1: Top Editor"s Desk Subject Headings
Subject Count
prices 92
unemployment 55
occupational safety & health 53
international comparisons, prices 48
manufacturing, prices 45
employment 44
productivity 40
consumer expenditures 36
earnings & wages 27
employment & unemployment 27
compensation costs 25
earnings & wages, metro. areas 18
benefits, compensation costs 18
earnings & wages, occupations 17
employment, occupations 14
benefits 14
earnings & wage, regions 13
work stoppages 12
earnings & wages, industries 11
Total 609
Table 2: Contingecy Table for Three Document
Representations
Representation Right Wrong Accuracy
Full-text 392 217 0.64
Title 441 168 0.72
Keyword 601 8 0.98
there were 19 such subject headings, which altogether 
covered 609 (54%) of the documents with subjects assigned.
These document-subject pairings formed the basis of our
analysis. Limiting analysis to subjects with N > 10 kept
the resultant χ2
tests suitably robust.
The clustering derived by each document representation
was tested by its ability to collocate documents with the
same subjects. Thus for each of the 19 subject headings
in Table 1, Si, we calculated the proportion of documents
assigned to Si that each clustering co-classified. Further,
we assumed that whichever cluster captured the majority of
documents for a given class constituted the right answer
for that class. For instance, There were 92 documents whose
subject heading was prices. Taking the BLS editors" 
classifications as ground truth, all 92 of these documents should
have ended up in the same cluster. Under the full-text 
representation 52 of these documents were clustered into category
5, while 35 were in category 3, and 5 documents were in 
category 6. Taking the majority cluster as the putative right
home for these documents, we consider the accuracy of this
clustering on this subject to be 52/92 = 0.56. Repeating
this process for each topic across all three representations
led to the contingency table shown in Table 2.
The obvious superiority of the keyword-based clustering
evidenced by Table 2 was borne out by a χ2
test on the
accuracy proportions. Comparing the proportion right and
Table 3: Keyword-Based Clusters
benefits costs international jobs
plans compensation import employment
benefits costs prices jobs
employees benefits petroleum youth
occupations prices productivity safety
workers prices productivity safety
earnings index output health
operators inflation nonfarm occupational
spending unemployment
expenditures unemployment
consumer mass
spending jobless
wrong achieved by keyword and title-based clustering led to
p 0.001. Due to this result, in the remainder of this paper,
we focus our attention on the clusters derived by analysis of
the Editor"s Desk keywords. The ten keyword-based clusters
are shown in Table 3, represented by the three terms most
highly associated with each cluster, in terms of the log-odds
ratio. Additionally, each cluster has been given a label by
the researchers.
Evaluating the results of clustering is notoriously difficult.
In order to lend our analysis suitable rigor and utility, we
made several simplifying assumptions. Most problematic is
the fact that we have assumed that each document belongs
in only a single category. This assumption is certainly false.
However, by taking an extremely rigid view of what 
constitutes a subject-that is, by taking a fully qualified and
often multipart subject heading as our unit of analysis-we
mitigate this problem. Analogically, this is akin to 
considering the location of books on a library shelf. Although a
given book may cover many subjects, a classification system
should be able to collocate books that are extremely similar,
say books about occupational safety and health. The most
serious liability with this evaluation, then, is the fact that
we have compressed multiple subject headings, say prices :
international into single subjects. This flattening obscures
the multivalence of documents. We turn to a more realistic
assessment of document-class relations in Section 6.2.
6.2 Accuracy of the Document Classifiers
Although the keyword-based clusters appear to classify
the Editor"s Desk documents very well, their discovery only
solved half of the problem required for the successful 
implementation of a dynamic user interface such as the 
relation browser. The matter of roughly fourteen thousand
unclassified documents remained to be addressed. To solve
this problem, we trained the statistical classifiers described
above in Section 5. For each document in the collection
di, these classifiers give pi, a k-vector of probabilities or 
distances (depending on the classification method used), where
pik quantifies the strength of association between the ith
document and the kth
class. All classifiers were trained on
the full text of each document, regardless of the 
representation used to discover the initial clusters. The different
training sets were thus constructed simply by changing the
156
Table 4: Cross Validation Results for 3 Classifiers
Method Av. Percent Accuracy SE
Prind 59.07 1.07
Naive Bayes 75.57 0.4
SVM 75.08 0.68
class variable for each instance (document) to reflect its 
assigned cluster under a given model.
To test the ability of each classifier to locate documents
correctly, we first performed a 10-fold cross validation on
the Editor"s Desk documents. During cross-validation the
data are split randomly into n subsets (in this case n = 10).
The process proceeds by iteratively holding out each of the
n subsets as a test collection for a model trained on the
remaining n − 1 subsets. Cross validation is described in
[15]. Using this methodology, we compared the performance
of the three classification models described above. Table 4
gives the results from cross validation.
Although naive Bayes is not significantly more accurate
for these data than the SVM classifier, we limit the 
remainder of our attention to analysis of its performance. Our
selection of naive Bayes is due to the fact that it appears to
work comparably to the SVM approach for these data, while
being much simpler, both in theory and implementation.
Because we have only 1279 documents and 10 classes, the
number of training documents per class is relatively small.
In addition to models fitted to the Editor"s Desk data, then,
we constructed a fourth model, supplementing the training
sets of each class by querying the Google search engine7
and
applying naive Bayes to the augmented test set. For each
class, we created a query by submitting the three terms
with the highest log-odds ratio with that class. Further,
each query was limited to the domain www.bls.gov. For
each class we retrieved up to 400 documents from Google
(the actual number varied depending on the size of the 
result set returned by Google). This led to a training set
of 4113 documents in the augmented model, as we call
it below8
. Cross validation suggested that the augmented
model decreased classification accuracy (accuracy= 58.16%,
with standard error= 0.32). As we discuss below, however,
augmenting the training set appeared to help generalization
during our second experiment.
The results of our cross validation experiment are 
encouraging. However, the success of our classifiers on the Editor"s
Desk documents that informed the cross validation study
may not be good predictors of the models" performance on
the remainder to the BLS website. To test the generality
of the naive Bayes classifier, we solicited input from 11 
human judges who were familiar with the BLS website. The
sample was chosen by convenience, and consisted of faculty
and graduate students who work on the GovStat project.
However, none of the reviewers had prior knowledge of the
outcome of the classification before their participation. For
the experiment, a random sample of 100 documents was
drawn from the entire BLS collection. On average each 
re7
http://www.google.com
8
A more formal treatment of the combination of labeled and
unlabeled data is available in [4].
Table 5: Human-Model Agreement on 100 Sample
Docs.
Human Judge 1st
Choice
Model Model 1st
Choice Model 2nd
Choice
N. Bayes (aug.) 14 24
N. Bayes 24 1
Human Judge 2nd
Choice
Model Model 1st
Choice Model 2nd
Choice
N. Bayes (aug.) 14 21
N. Bayes 21 4
viewer classified 83 documents, placing each document into
as many of the categories shown in Table 3 as he or she saw
fit.
Results from this experiment suggest that room for 
improvement remains with respect to generalizing to the whole
collection from the class models fitted to the Editor"s Desk
documents. In Table 5, we see, for each classifier, the 
number of documents for which it"s first or second most probable
class was voted best or second best by the 11 human judges.
In the context of this experiment, we consider a first- or
second-place classification by the machine to be accurate
because the relation browser interface operates on a 
multiway classification, where each document is classified into
multiple categories. Thus a document with the correct
class as its second choice would still be easily available to
a user. Likewise, a correct classification on either the most
popular or second most popular category among the human
judges is considered correct in cases where a given document
was classified into multiple classes. There were 72 
multiclass documents in our sample, as seen in Figure 4. The
remaining 28 documents were assigned to 1 or 0 classes.
Under this rationale, The augmented naive Bayes 
classifier correctly grouped 73 documents, while the smaller model
(not augmented by a Google search) correctly classified 50.
The resultant χ2
test gave p = 0.001, suggesting that 
increasing the training set improved the ability of the naive
Bayes model to generalize from the Editor"s Desk documents
to the collection as a whole. However, the improvement 
afforded by the augmented model comes at some cost. In 
particular, the augmented model is significantly inferior to the
model trained solely on Editor"s Desk documents if we 
concern ourselves only with documents selected by the majority
of human reviewers-i.e. only first-choice classes. Limiting
the right answers to the left column of Table 5 gives p = 0.02
in favor of the non-augmented model. For the purposes of
applying the relation browser to complex digital library 
content (where documents will be classified along multiple 
categories), the augmented model is preferable. But this is not
necessarily the case in general.
It must also be said that 73% accuracy under a fairly
liberal test condition leaves room for improvement in our
assignment of topics to categories. We may begin to 
understand the shortcomings of the described techniques by
consulting Figure 5, which shows the distribution of 
categories across documents given by humans and by the 
augmented naive Bayes model. The majority of reviewers put
157
Number of Human-Assigned ClassesMNumber of Human-Assigned ClassesM
Number of Human-Assigned Classes
FrequencyMFrequencyM
Frequency
m00M0M
0
11M1M
1
22M2M
2
33M3M
3
44M4M
4
55M5M
5
66M6M
6
77M7M
7
m00M0M 055M5M 51010M10M 101515M15M 152020M20M 202525M25M 253030M30M 303535M35M 35
Figure 4: Number of Classes Assigned to 
Documents by Judges
documents into only three categories, jobs, benefits, and 
occupations. On the other hand, the naive Bayes classifier 
distributed classes more evenly across the topics. This behavior
suggests areas for future improvement. Most importantly,
we observed a strong correlation among the three most 
frequent classes among the human judges (for instance, there
was 68% correlation between benefits and occupations). This
suggests that improving the clustering to produce topics
that were more nearly orthogonal might improve 
performance.
7. CONCLUSIONS AND FUTURE WORK
Many developers and maintainers of digital libraries share
the basic problem pursued here. Given increasingly large,
complex bodies of data, how may we improve access to 
collections without incurring extraordinary cost, and while also
keeping systems receptive to changes in content over time?
Data mining and machine learning methods hold a great deal
of promise with respect to this problem. Empirical 
methods of knowledge discovery can aid in the organization and
retrieval of information. As we have argued in this paper,
these methods may also be brought to bear on the design
and implementation of advanced user interfaces.
This study explored a hybrid technique for aiding 
information architects as they implement dynamic interfaces such
as the relation browser. Our approach combines 
unsupervised learning techniques, applied to a focused subset of the
BLS website. The goal of this initial stage is to discover the
most basic and far-reaching topics in the collection. Based
mjobsjobsMjobsM
jobs
benefitsunemploymentpricespricesMpricesM
prices
safetyinternationalspendingspendingMspendingM
spending
occupationscostscostsMcostsM
costs
productivityHuman ClassificationsMHuman ClassificationsM
Human Classifications
m0.000.00M0.00M
0.00
0.050.100.150.15M0.15M
0.15
0.200.25mjobsjobsMjobsM
jobs
benefitsunemploymentpricespricesMpricesM
prices
safetyinternationalspendingspendingMspendingM
spending
occupationscostscostsMcostsM
costs
productivityMachine ClassificationsMMachine ClassificationsM
Machine Classifications
m0.000.00M0.00M
0.00
0.050.100.10M0.10M
0.10
0.15
Figure 5: Distribution of Classes Across Documents
on a statistical model of these topics, the second phase of
our approach uses supervised learning (in particular, a naive
Bayes classifier, trained on individual words), to assign 
topical relations to the remaining documents in the collection.
In the study reported here, this approach has 
demonstrated promise. In its favor, our approach is highly scalable.
It also appears to give fairly good results. Comparing three
modes of document representation-full-text, title only, and
keyword-we found 98% accuracy as measured by 
collocation of documents with identical subject headings. While it
is not surprising that editor-generated keywords should give
strong evidence for such learning, their superiority over 
fulltext and titles was dramatic, suggesting that even a small
amount of metadata can be very useful for data mining.
However, we also found evidence that learning topics from
a subset of the collection may lead to overfitted models.
After clustering 1279 Editor"s Desk documents into 10 
categories, we fitted a 10-way naive Bayes classifier to categorize
the remaining 14,000 documents in the collection. While we
saw fairly good results (classification accuracy of 75% with
respect to a small sample of human judges), this experiment
forced us to reconsider the quality of the topics learned by
clustering. The high correlation among human judgments
in our sample suggests that the topics discovered by 
analysis of the Editor"s Desk were not independent. While we do
not desire mutually exclusive categories in our setting, we
do desire independence among the topics we model.
Overall, then, the techniques described here provide an
encouraging start to our work on acquiring subject 
metadata for dynamic interfaces automatically. It also suggests
that a more sophisticated modeling approach might yield
158
better results in the future. In upcoming work we will 
experiment with streamlining the two-phase technique described
here. Instead of clustering documents to find topics and
then fitting a model to the learned clusters, our goal is to
expand the unsupervised portion of our analysis beyond a
narrow subset of the collection, such as The Editor"s Desk.
In current work we have defined algorithms to identify 
documents likely to help the topic discovery task. Supplied with
a more comprehensive training set, we hope to experiment
with model-based clustering, which combines the clustering
and classification processes into a single modeling procedure.
Topic discovery and document classification have long been
recognized as fundamental problems in information retrieval
and other forms of text mining. What is increasingly clear,
however, as digital libraries grow in scope and complexity,
is the applicability of these techniques to problems at the
front-end of systems such as information architecture and
interface design. Finally, then, in future work we will build
on the user studies undertaken by Marchionini and Brunk
in efforts to evaluate the utility of automatically populated
dynamic interfaces for the users of digital libraries.
8. REFERENCES
[1] A. Agresti. An Introduction to Categorical Data
Analysis. Wiley, New York, 1996.
[2] C. Ahlberg, C. Williamson, and B. Shneiderman.
Dynamic queries for information exploration: an
implementation and evaluation. In Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 619-626, 1992.
[3] R. Baeza-Yates and B. Ribeiro-Neto. Modern
Information Retrieval. ACM Press, 1999.
[4] A. Blum and T. Mitchell. Combining labeled and
unlabeled data with co-training. In Proceedings of the
eleventh annual conference on Computational learning
theory, pages 92-100. ACM Press, 1998.
[5] H. Chen and S. Dumais. Hierarchical classification of
web content. In Proceedings of the 23rd annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 256-263,
2000.
[6] M. Efron, G. Marchionini, and J. Zhang. Implications
of the recursive representation problem for automatic
concept identification in on-line governmental
information. In Proceedings of the ASIST Special
Interest Group on Classification Research (ASIST
SIG-CR), 2003.
[7] C. Fraley and A. E. Raftery. How many clusters?
which clustering method? answers via model-based
cluster analysis. The Computer Journal,
41(8):578-588, 1998.
[8] A. K. Jain, M. N. Murty, and P. J. Flynn. Data
clustering: a review. ACM Computing Surveys,
31(3):264-323, September 1999.
[9] T. Joachims. A probabilistic analysis of the Rocchio
algorithm with TFIDF for text categorization. In
D. H. Fisher, editor, Proceedings of ICML-97, 14th
International Conference on Machine Learning, pages
143-151, Nashville, US, 1997. Morgan Kaufmann
Publishers, San Francisco, US.
[10] T. Joachims. Text categorization with support vector
machines: learning with many relevant features. In
C. N´edellec and C. Rouveirol, editors, Proceedings of
ECML-98, 10th European Conference on Machine
Learning, pages 137-142, Chemnitz, DE, 1998.
Springer Verlag, Heidelberg, DE.
[11] I. T. Jolliffe. Principal Component Analysis. Springer,
2nd edition, 2002.
[12] L. Kaufman and P. J. Rosseeuw. Finding Groups in
Data: an Introduction to Cluster Analysis. Wiley,
1990.
[13] G. Marchionini and B. Brunk. Toward a general
relation browser: a GUI for information architects.
Journal of Digital Information, 4(1), 2003.
http://jodi.ecs.soton.ac.uk/Articles/v04/i01/Marchionini/.
[14] A. K. McCallum. Bow: A toolkit for statistical
language modeling, text retrieval, classification and
clustering. http://www.cs.cmu.edu/˜mccallum/bow,
1996.
[15] T. Mitchell. Machine Learning. McGraw Hill, 1997.
[16] E. Rasmussen. Clustering algorithms. In W. B. Frakes
and R. Baeza-Yates, editors, Information Retrieval:
Data Structures and Algorithms, pages 419-442.
Prentice Hall, 1992.
[17] R. Tibshirani, G. Walther, and T. Hastie. Estimating
the number of clusters in a dataset via the gap
statistic, 2000.
http://citeseer.nj.nec.com/tibshirani00estimating.html.
[18] V. N. Vapnik. The Nature of Statistical Learning
Theory. Springer, 2000.
159
Query Performance Prediction in Web Search
Environments
Yun Zhou and W. Bruce Croft
Department of Computer Science
University of Massachusetts, Amherst
{yzhou, croft}@cs.umass.edu
ABSTRACT
Current prediction techniques, which are generally designed for
content-based queries and are typically evaluated on relatively
homogenous test collections of small sizes, face serious
challenges in web search environments where collections are
significantly more heterogeneous and different types of retrieval
tasks exist. In this paper, we present three techniques to address
these challenges. We focus on performance prediction for two
types of queries in web search environments: content-based and
Named-Page finding. Our evaluation is mainly performed on the
GOV2 collection. In addition to evaluating our models for the two
types of queries separately, we consider a more challenging and
realistic situation that the two types of queries are mixed together
without prior information on query types. To assist prediction
under the mixed-query situation, a novel query classifier is
adopted. Results show that our prediction of web query
performance is substantially more accurate than the current 
stateof-the-art prediction techniques. Consequently, our paper
provides a practical approach to performance prediction in 
realworld web settings.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval -Query formulation
General Terms
Algorithms, Experimentation, Theory
1. INTRODUCTION
Query performance prediction has many applications in a variety
of information retrieval (IR) areas such as improving retrieval
consistency, query refinement, and distributed IR. The importance
of this problem has been recognized by IR researchers and a
number of new methods have been proposed for prediction
recently [1, 2, 17].
Most work on prediction has focused on the traditional ad-hoc
retrieval task where query performance is measured according to
topical relevance. These prediction models are evaluated on
TREC document collections which typically consist of no more
than one million relatively homogenous newswire articles. With
the popularity and influence of the Web, prediction techniques
that will work well for web-style queries are highly preferable.
However, web search environments pose significant challenges to
current prediction models that are mainly designed for traditional
TREC settings. Here we outline some of these challenges.
First, web collections, which are much larger than conventional
TREC collections, include a variety of documents that are
different in many aspects such as quality and style. Current
prediction techniques can be vulnerable to these characteristics of
web collections. For example, the reported prediction accuracy of
the ranking robustness technique and the clarity technique on the
GOV2 collection (a large web collection) is significantly worse
compared to the other TREC collections [1]. Similar prediction
accuracy on the GOV2 collection using another technique is
reported in [2], confirming the difficult of predicting query
performance on a large web collection.
Furthermore, web search goes beyond the scope of the ad-hoc
retrieval task based on topical relevance. For example, the
Named-Page (NP) finding task, which is a navigational task, is
also popular in web retrieval. Query performance prediction for
the NP task is still necessary since NP retrieval performance is far
from perfect. In fact, according to the report on the NP task of the
2005 Terabyte Track [3], about 40% of the test queries perform
poorly (no correct answer in the first 10 search results) even in the
best run from the top group. To our knowledge, little research has
explicitly addressed the problem of NP-query performance
prediction. Current prediction models devised for content-based
queries will be less effective for NP queries considering the
fundamental differences between the two.
Third, in real-world web search environments, user queries are
usually a mixture of different types and prior knowledge about the
type of each query is generally unavailable. The mixed-query
situation raises new problems for query performance prediction.
For instance, we may need to incorporate a query classifier into
prediction models. Despite these problems, the ability to handle
this situation is a crucial step towards turning query performance
prediction from an interesting research topic into a practical tool
for web retrieval.
In this paper, we present three techniques to address the above
challenges that current prediction models face in Web search
environments. Our work focuses on query performance prediction
for the content-based (ad-hoc) retrieval task and the name-page
finding task in the context of web retrieval. Our first technique,
called weighted information gain (WIG), makes use of both single
term and term proximity features to estimate the quality of top
retrieved documents for prediction. We find that WIG offers
consistent prediction accuracy across various test collections and
query types. Moreover, we demonstrate that good prediction
accuracy can be achieved for the mixed-query situation by using
WIG with the help of a query type classifier. Query feedback and
first rank change, which are our second and third prediction
techniques, perform well for content-based queries and NP
queries respectively.
Our main contributions include: (1) considerably improved
prediction accuracy for web content-based queries over several
state-of-the-art techniques. (2) new techniques for successfully
predicting NP-query performance. (3) a practical and fully
automatic solution to predicting mixed-query performance. In
addition, one minor contribution is that we find that the
robustness score [1], which was originally proposed for
performance prediction, is helpful for query classification.
2. RELATED WORK
As we mentioned in the introduction, a number of prediction
techniques have been proposed recently that focus on 
contentbased queries in the topical relevance (ad-hoc) task. We know of
no published work that addresses other types of queries such as
NP queries, let alone a mixture of query types. Next we review
some representative models.
The major difficulty of performance prediction comes from the
fact that many factors have an impact on retrieval performance.
Each factor affects performance to a different degree and the
overall effect is hard to predict accurately. Therefore, it is not
surprising to notice that simple features, such as the frequency of
query terms in the collection [4] and the average IDF of query
terms [5], do not predict well. In fact, most of the successful
techniques are based on measuring some characteristics of the
retrieved document set to estimate topic difficulty. For example,
the clarity score [6] measures the coherence of a list of documents
by the KL-divergence between the query model and the collection
model. The robustness score [1] quantifies another property of a
ranked list: the robustness of the ranking in the presence of
uncertainty. Carmel et al. [2] found that the distance measured by
the Jensen-Shannon divergence between the retrieved document
set and the collection is significantly correlated to average
precision. Vinay et al.[7] proposed four measures to capture the
geometry of the top retrieved documents for prediction. The most
effective measure is the sensitivity to document perturbation, an
idea somewhat similar to the robustness score. Unfortunately,
their way of measuring the sensitivity does not perform equally
well for short queries and prediction accuracy drops considerably
when a state-of-the-art retrieval technique (like Okapi or a
language modeling approach) is adopted for retrieval instead of
the tf-idf weighting used in their paper [16].
The difficulties of applying these models in web search
environments have already been mentioned. In this paper, we
mainly adopt the clarity score and the robustness score as our
baselines. We experimentally show that the baselines, even after
being carefully tuned, are inadequate for the web environment.
One of our prediction models, WIG, is related to the Markov
random field (MRF) model for information retrieval [8]. The
MRF model directly models term dependence and is found be to
highly effective across a variety of test collections (particularly
web collections) and retrieval tasks. This model is used to
estimate the joint probability distribution over documents and
queries, an important part of WIG. The superiority of WIG over
other prediction techniques based on unigram features, which will
be demonstrated later in our paper, coincides with that of MRF
for retrieval. In other word, it is interesting to note that term
dependence, when being modeled appropriately, can be helpful
for both improving and predicting retrieval performance.
3. PREDICTION MODELS
3.1 Weighted Information Gain (WIG)
This section introduces a weighted information gain approach that
incorporates both single term and proximity features for
predicting performance for both content-based and Named-Page
(NP) finding queries.
Given a set of queries Q={Qs} (s=1,2,..N) which includes all
possible user queries and a set of documents D={Dt} (t=1,2…M),
we assume that each query-document pair (Qs,Dt) is manually
judged and will be put in a relevance list if Qs is found to be
relevant to Dt. The joint probability P(Qs,Dt) over queries Q and
documents D denotes the probability that pair (Qs,Dt) will be in
the relevance list. Such assumptions are similar to those used in
[8]. Assuming that the user issues query Qi ∈Q and the retrieval
results in response to Qi is a ranked list L of documents, we
calculate the amount of information contained in P(Qs,Dt) with
respect to Qi and L by Eq.1 which is a variant of entropy called
the weighted entropy[13]. The weights in Eq.1 are solely
determined by Qi and L.
)1(),(log),(),(
,
, ∑−=
ts
tststsLQ DQPDQweightDQH i
In this paper, we choose the weights as follows:
LindocumentsKtopthecontainsLTwhere
otherwise
LTDandisifK
DQweight
K
Kt
ts
)(
)2(
,0
)(,/1
),(
⎩
⎨
⎧ ∈=
=
The cutoff rank K is a parameter in our model that will be
discussed later. Accordingly, Eq.1 can be simplified as follows:
)3(),(log
1
),(
)(
, ∑∈
−=
LTD
titsLQ
Kt
i
DQP
K
DQH
Unfortunately, weighted entropy ),(, tsLQ DQH i
computed by Eq.3,
which represents the amount of information about how likely the
top ranked documents in L would be relevant to query Qi on
average, cannot be compared across different queries, making it
inappropriate for directly predicting query performance. To
mitigate this problem, we come up with a background distribution
P(Qs,C) over Q and D by imagining that every document in D is
replaced by the same special document C which represents
average language usage. In this paper, C is created by
concatenating every document in D. Roughly speaking, C is the
collection (the document set) {Dt} without document boundaries.
Similarly, weighted entropy ),(, CQH sLQi
calculated by Eq.3
represents the amount of information about how likely an average
document (represented by the whole collection) would be relevant
to query Qi.
Now we introduce our performance predictor WIG which is the
weighted information gain [13] computed as the difference
between ),(, tsLQ DQH i
and ),(, CQH sLQi
.Specifically, given query
Qi, collection C and ranked list L of documents, WIG is
calculated as follows:
)4(
),(
),(
log
1
),(
),(
log),(
),(),(),,(
)(,
,,
∑∑ ∈
==
−=
LTD i
ti
ts s
ts
ts
tsLQsLQi
Kt
ii
CQP
DQP
KCQP
DQP
DQweight
DQHCQHLCQWIG
WIG computed by Eq.4 measures the change in information about
the quality of retrieval (in response to query Qi) from an
imaginary state that only an average document is retrieved to a
posterior state that the actual search results are observed. We
hypothesize that WIG is positively correlated with retrieval
effectiveness because high quality retrieval should be much more
effective than just returning the average document.
The heart of this technique is how to estimate the joint
distribution P(Qs,Dt). In the language modeling approach to IR, a
variety of models can be applied readily to estimate this
distribution. Although most of these models are based on the 
bagof-words assumption, recent work on modeling term dependence
under the language modeling framework have shown consistent
and significant improvements in retrieval effectiveness over 
bagof-words models. Inspired by the success of incorporating term
proximity features into language models, we decide to adopt a
good dependence model to estimate the probability P(Qs,Dt). The
model we chose for this paper is Metzler and Croft"s Markov
Random Field (MRF) model, which has already demonstrated
superiority over a number of collections and different retrieval
tasks [8,9].
According to the MRF model, log P(Qi, Dt) can be written as
)5()|(loglog),(log
)(
1 ∑∈
+−=
iQF
tti DPZDQP
ξ
ξ ξλ
where Z1 is a constant that ensures that P(Qi, Dt) sums up to 1.
F(Qi) consists of a set of features expanded from the original
query Qi . For example, assuming that query Qi is talented
student program, F(Qi) includes features like program and
talented student. We consider two kinds of features: single
term features T and proximity features P. Proximity features
include exact phrase (#1) and unordered window (#uwN) features
as described in [8]. Note that F(Qi) is the union of T(Qi) and
P(Qi). For more details on F(Qi) such as how to expand the
original query Qi to F(Qi), we refer the reader to [8] and [9].
P(ξ|Dt) denotes the probability that feature ξ will occur in Dt.
More details on P(ξ|Dt) will be provided later in this section. The
choice of λξ is somewhat different from that used in [8] since λξ
plays a dual role in our model. The first role, which is the same as
in [8], is to weight between single term and proximity features.
The other role, which is specific to our prediction task, is to
normalize the size of F(Qi).We found that the following weight
strategy for λξ satisfies the above two roles and generalizes well
on a variety of collections and query types.
)6(
)(,
|)(|
1
)(,
|)(|
⎪
⎪
⎩
⎪
⎪
⎨
⎧
∈
−
∈
=
i
i
T
i
i
T
QP
QP
QT
QT
ξ
λ
ξ
λ
λξ
where |T(Qi)| and |P(Qi)| denote the number of single term and
proximity features in F(Qi) respectively. The reason for choosing
the square root function in the denominator of λξ is to penalize a
feature set of large size appropriately, making WIG more
comparable across queries of various lengths. λT is a fixed
parameter and set to 0.8 according to [8] throughout this paper.
Similarly, log P(Qi,C) can be written as:
)7()|(loglog),(log
)(
2 ∑∈
+−=
iQF
i CPZCQP
ξ
ξ ξλ
When constant Z1 and Z2 are dropped, WIG computed in Eq.4 can
be rewritten as follows by plugging in Eq.5 and Eq.7 :
)8(
)|(
)|(
log
1
),,(
)( )(
∑ ∑∈ ∈
=
LTD QF
t
i
Kt i
CP
DP
K
LCQWIG
ξ
ξ
ξ
ξ
λ
One of the advantages of WIG over other techniques is that it can
handle well both content-based and NP queries. Based on the type
(or the predicted type) of Qi, the calculation of WIG in Eq. 8
differs in two aspects: (1) how to estimate P(ξ|Dt) and P(ξ|C), and
(2) how to choose K.
For content-based queries, P(ξ|C) is estimated by the relative
frequency of feature ξ in collection C as a whole. The estimation
of P(ξ|Dt) is the same as in [8]. Namely, we estimate P(ξ|Dt) by
the relative frequency of feature ξ in Dt linearly smoothed with
collection frequency P(ξ|C). K in Eq.8 is treated as a free
parameter. Note that K is the only free parameter in the
computation of WIG for content-based queries because all
parameters involved in P(ξ|Dt) are assumed to be fixed by taking
the suggested values in [8].
Regarding NP queries, we make use of document structure to
estimate P(ξ|Dt) and P(ξ|C) by the so-called mixture of language
models proposed in [10] and incorporated into the MRF model for
Named-Page finding retrieval in [9]. The basic idea is that a
document (collection) is divided into several fields such as the
title field, the main-body field and the heading field. P(ξ|Dt) and
P(ξ|C) are estimated by a linear combination of the language
models from each field. Due to space constraints, we refer the
reader to [9] for details. We adopt the exact same set of
parameters as used in [9] for estimation. With regard to K in Eq.8,
we set K to 1 because the Named-Page finding task heavily
focuses on the first ranked document. Consequently, there are no
free parameters in the computation of WIG for NP queries.
3.2 Query Feedback
In this section, we introduce another technique called query
feedback (QF) for prediction. Suppose that a user issues query Q
to a retrieval system and a ranked list L of documents is returned.
We view the retrieval system as a noisy channel. Specifically, we
assume that the output of the channel is L and the input is Q.
After going through the channel, Q becomes corrupted and is
transformed to ranked list L.
By thinking about the retrieval process this way, the problem of
predicting retrieval effectiveness turns to the task of evaluating
the quality of the channel. In other words, prediction becomes
finding a way to measure the degree of corruption that arises
when Q is transformed to L. As directly computing the degree of
the corruption is difficult, we tackle this problem by
approximation. Our main idea is that we measure to what extent
information on Q can be recovered from L on the assumption that
only L is observed. Specifically, we design a decoder that can
accurately translate L back into new query Q" and the similarity S
between the original query Q and the new query Q" is adopted as
a performance predictor. This is a sketch of how the QF technique
predicts query performance. Before filling in more details, we
briefly discuss why this method would work.
There is a relation between the similarity S defined above and
retrieval performance. On the one hand, if the retrieval has
strayed from the original sense of the query Q, the new query Q"
extracted from ranked list L in response to Q would be very
different from the original query Q. On the other hand, a query
distilled from a ranked list containing many relevant documents is
likely to be similar to the original query. Further examples in
support of the relation will be provided later.
Next we detail how to build the decoder and how to measure the
similarity S.
In essence, the goal of the decoder is to compress ranked list L
into a few informative terms that should represent the content of
the top ranked documents in L. Our approach to this goal is to
represent ranked list L by a language model (distribution over
terms). Then terms are ranked by their contribution to the
language model"s KL (Kullback-Leibler) divergence from the
background collection model. Top ranked terms will be chosen to
form the new query Q". This approach is similar to that used in
Section 4.1 of [11].
Specifically, we take three steps to compress ranked list L into
query Q" without referring to the original query.
1. We adopt the ranked list language model [14], to estimate a
language model based on ranked list L. The model can be written
as:
)9()|()|()|( ∑∈
=
LD
LDPDwPLwP
where w is any term, D is a document. P(D|L) is estimated by a
linearly decreasing function of the rank of document D.
2. Each term in P(w|L) is ranked by the following KL-divergence
contribution:
)10(
)|(
)|(
log)|(
CwP
LwP
LwP
where P(w|C) is the collection model estimated by the relative
frequency of term w in collection C as a whole.
3. The top N ranked terms by Eq.10 form a weighted query
Q"={(wi,ti)} i=1,N. where wi denotes the i-th ranked term and
weight ti is the KL-divergence contribution of wi in Eq. 10.
Term cruise ship vessel sea passenger
KL
contribution
0.050 0.040 0.012 0.010 0.009
Table 1: top 5 terms compressed from the ranked list in
response to query Cruise ship damage sea life
Two representative examples, one for a poorly performing query
Cruise ship damage sea life (TREC topic 719; average
precision: 0.08) and the other for a high performing query
prostate cancer treatments( TREC topic 710; average precision:
0.49), are shown in Table 1 and 2 respectively. These examples
indicate how the similarity between the original and the new
query correlates with retrieval performance. The parameter N in
step 3 is set to 20 empirically and choosing a larger value of N is
unnecessary since the weights after the top 20 are usually too
small to make any difference.
Term prostate cancer treatment men therapy
KL
contribution
0.177 0.140 0.028 0.025 0.020
Table 2: top 5 terms compressed from the ranked list in
response to query prostate cancer treatments
To measure the similarity between original query Q and new
query Q", we first use Q" to do retrieval on the same collection. A
variant of the query likelihood model [15] is adopted for retrieval.
Namely, documents are ranked by:
)11()|()|'(
'),(
∑∈
=
Qtw
t
i
ii
i
DwPDQP
where wi is a term in Q" and ti is the associated weight. D is a
document.
Let L" denote the new ranked list returned from the above
retrieval. The similarity is measured by the overlap of documents
in L and L". Specifically, the percentage of documents in the top
K documents of L that are also present in the top K documents in
L". the cutoff K is treated as a free parameter.
We summarize here how the QF technique predicts performance
given a query Q and the associated ranked list L. We first obtain a
weighted query Q" compressed from L by the above three steps.
Then we use Q" to perform retrieval and the new ranked list is L".
The overlap of documents in L and L" is used for prediction.
3.3 First Rank Change (FRC)
In this section, we propose a method called the first rank change
(FRC) for performance prediction for NP queries. This method is
derived from the ranking robustness technique [1] that is mainly
designed for content-based queries. When directly applied to NP
queries, the robustness technique will be less effective because it
takes the top ranked documents as a whole into account while NP
queries usually have only one single relevant document. Instead,
our technique focuses on the first rank document while the main
idea of the robustness method remains. Specifically, the 
pseudocode for computing FRC is shown in figure 1.
Input: (1) ranked list L={Di} where i=1,100. Di denotes the i-th
ranked document. (2) query Q
1 initialize: (1) set the number of trials J=100000 (2) counter c=0;
2 for i=1 to J
3 Perturb every document in L, let the outcome be a set F={Di"}
where Di" denotes the perturbed version of Di.
4 Do retrieval with query Q on set F
5 c=c+1 if and only if D1" is ranked first in step 4
6 end of for
7 return the ratio c/J
Figure 1: pseudo-code for computing FRC
FRC approximates the probability that the first ranked document
in the original list L will remain ranked first even after the
documents are perturbed. The higher the probability is, the more
confidence we have in the first ranked document. On the other
hand, in the extreme case of a random ranking, the probability
would be as low as 0.5. We expect that FRC has a positive
association with NP query performance. We adopt [1] to
implement the document perturbation step (step 4 in Fig.1) using
Poisson distributions. For more details, we refer the reader to [1].
4. EVALUATION
We now present the results of predicting query performance by
our models. Three state-of-the-art techniques are adopted as our
baselines. We evaluate our techniques across a variety of Web
retrieval settings. As mentioned before, we consider two types of
queries, that is, content-based (CB) queries and Named-Page(NP)
finding queries.
First, suppose that the query types are known. We investigate the
correlation between the predicted retrieval performance and the
actual performance for both types of queries separately. Results
show that our methods yield considerable improvements over the
baselines.
We then consider a more challenging scenario where no prior
information on query types is available. Two sub-cases are
considered. In the first one, there exists only one type of query but
the actual type is unknown. We assume a mixture of the two
query types in the second case. We demonstrate that our models
achieve good accuracy under this demanding scenario, making
prediction practical in a real-world Web search environment.
4.1 Experimental Setup
Our evaluation focuses on the GOV2 collection which contains
about 25 million documents crawled from web sites in the .gov
domain during 2004 [3]. We create two kinds of data set for CB
queries and NP queries respectively. For the CB type, we use the
ad-hoc topics of the Terabyte Tracks of 2004, 2005 and 2006 and
name them TB04-adhoc, TB05-adhoc and TB06-adhoc
respectively. In addition, we also use the ad-hoc topics of the
2004 Robust Track (RT04) to test the adaptability of our
techniques to a non-Web environment. For NP queries, we use the
Named-Page finding topics of the Terabyte Tracks of 2005 and
2006 and we name them TB05-NP and TB06-NP respectively. All
queries used in our experiments are titles of TREC topics as we
center on web retrieval. Table 3 summarizes the above data sets.
Name Collection Topic Number Query Type
TB04-adhoc GOV2 701-750 CB
TB05-adhoc GOV2 751-800 CB
TB06-adhoc GOV2 801-850 CB
RT04 Disk 4+5
(minus CR)

301-450;601700
CB
TB05-NP GOV2 NP601-NP872 NP
TB06-NP GOV2 NP901-NP1081 NP
Table 3: Summary of test collections and topics
Retrieval performance of individual content-based and NP queries
is measured by the average precision and reciprocal rank of the
first correct answer respectively. We make use of the Markov
Random field model for both ad-hoc and Named-Page finding
retrieval. We adopt the same setting of retrieval parameters used
in [8,9]. The Indri search engine [12] is used for all of our
experiments. Though not reported here, we also tried the query
likelihood model for ad-hoc retrieval and found that the results
change little because of the very high correlation between the
query performances obtained by the two retrieval models (0.96
measured by Pearson"s coefficient).
4.2 Known Query Types
Suppose that query types are known. We treat each type of query
separately and measure the correlation with average precision (or
the reciprocal rank in the case of NP queries). We adopt the
Pearson"s correlation test which reflects the degree of linear
relationship between the predicted and the actual retrieval
performance.
4.2.1 Content-based Queries
Methods Clarity Robust JSD WIG QF WIG
+QF
TB04+0
5 adhoc
0.333 0.317 0.362 0.574 0.480 0.637
TB06
adhoc
0.076 0.294 N/A 0.464 0.422 0.511
Table 4: Pearson"s correlation coefficients for correlation with
average precision on the Terabyte Tracks (ad-hoc) for clarity
score, robustness score, the JSD-based method(we directly
cites the score reported in [2]), WIG, query feedback(QF) and
a linear combination of WIG and QF. Bold cases mean the
results are statistically significant at the 0.01 level.
Table 4 shows the correlation with average precision on two data
sets: one is a combination of TB04-adhoc and TB05-adhoc(100
topics in total) and the other is TB06-adhoc (50 topics). The
reason that we put TB04-adhoc and TB05-adhoc together is to
make our results comparable to [2]. Our baselines are the clarity
score (clarity) [6],the robustness score (robust)[1] and the 
JSDbased method (JSD) [2]. For the clarity and robustness score, we
have tried different parameter settings and report the highest
correlation coefficients we have found. We directly cite the result
of the JSD-based method reported in [2]. The table also shows the
results for the Weighted Information Gain (WIG) method and the
Query Feedback (QF) method for predicting content-based
queries. As we described in the previous section, both WIG and
QF have one free parameter to set, that is, the cutoff rank K. We
train the parameter on one dataset and test on the other. When
combining WIG and QF, a simple linear combination is used and
the combination weight is learned from the training data set.
From these results, we can see that our methods are considerably
more accurate compared to the baselines. We also observe that
further improvements are obtained from the combination of WIG
and QF, suggesting that they measure different properties of the
retrieval process that relate to performance.
We discover that our methods generalize well on TB06-adhoc
while the correlation for the clarity score with retrieval
performance on this data set is considerably worse. Further
investigation shows that the mean average precision of 
TB06-adhoc is 0.342 and is about 10% better than that of the first data set.
While the other three methods typically consider the top 100 or
less documents given a ranked list, the clarity method usually
needs the top 500 or more documents to adequately measure the
coherence of a ranked list. Higher mean average precision makes
ranked lists retrieved by different queries more similar in terms of
coherence at the level of top 500 documents. We believe that this
is the main reason for the low accuracy of the clarity score on the
second data set.
Though this paper focuses on a Web search environment, it is
desirable that our techniques will work consistently well in other
situations. To this end, we examine the effectiveness of our
techniques on the Robust 2004 Track. For our methods, we
evenly divide all of the test queries into five groups and perform
five-fold cross validation. Each time we use one group for
training and the remaining four groups for testing. We make use
of all of the queries for our two baselines, that is, the clarity score
and the robustness score. The parameters for our baselines are the
same as those used in [1].The results shown in Table 5
demonstrate that the prediction accuracy of our methods is on a
par with that of the two strong baselines.
Clarity Robust WIG QF
0.464 0.539 0.468 0.464
Table 5: Comparison of Pearson"s correlation coefficients on
the 2004 Robust Track for clarity score, robustness score,
WIG and query feedback (QF). Bold cases mean the results
are statistically significant at the 0.01 level.
Furthermore, we examine the prediction sensitivity of our
methods to the cutoff rank K. With respect to WIG, it is quite
robust to K on the Terabyte Tracks (2004-2006) while it prefers a
small value of K like 5 on the 2004 Robust Track. In other words,
a small value of K is a nearly-optimal choice for both kinds of
tracks. Considering the fact that all other parameters involved in
WIG are fixed and consequently the same for the two cases, this
means WIG can achieve nearly-optimal prediction accuracy in
two considerably different situations with exactly the same
parameter setting. Regarding QF, it prefers a larger value of K
such as 100 on the Terabyte Tracks and a smaller value of K such
as 25 on the 2004 Robust Track.
4.2.2 NP Queries
We adopt WIG and first rank change (FRC) for predicting 
NPquery performance. We also try a linear combination of the two as
in the previous section. The combination weight is obtained from
the other data set. We use the correlation with the reciprocal ranks
measured by the Pearson"s correlation test to evaluate prediction
quality. The results are presented in Table 6. Again, our baselines
are the clarity score and the robustness score.
To make a fair comparison, we tune the clarity score in different
ways. We found that using the first ranked document to build the
query model yields the best prediction accuracy. We also
attempted to utilize document structure by using the mixture of
language models mentioned in section 3.1. Little improvement
was obtained. The correlation coefficients for the clarity score
reported in Table 6 are the best we have found. As we can see,
our methods considerably outperform the clarity score technique
on both of the runs. This confirms our intuition that the use of a
coherence-based measure like the clarity score is inappropriate for
NP queries.
Methods Clarity Robust. WIG FRC WIG+FRC
TB05-NP 0.150 -0.370 0.458 0.440 0.525
TB06-NP 0.112 -0.160 0.478 0.386 0.515
Table 6: Pearson"s correlation coefficients for correlation with
reciprocal ranks on the Terabyte Tracks (NP) for clarity
score, robustness score, WIG, the first rank change (FRC)
and a linear combination of WIG and FRC. Bold cases mean
the results are statistically significant at the 0.01 level.
Regarding the robustness score, we also tune the parameters and
report the best we have found. We observe an interesting and
surprising negative correlation with reciprocal ranks. We explain
this finding briefly. A high robustness score means that a number
of top ranked documents in the original ranked list are still highly
ranked after perturbing the documents. The existence of such
documents is a good sign of high performance for content-based
queries as these queries usually contain a number of relevant
documents [1]. However, with regard to NP queries, one
fundamental difference is that there is only one relevant document
for each query. The existence of such documents can confuse the
ranking function and lead to low retrieval performance. Although
the negative correlation with retrieval performance exists, the
strength of the correlation is weaker and less consistent compared
to our methods as shown in Table 6.
Based on the above analysis, we can see that current prediction
techniques like clarity score and robustness score that are mainly
designed for content-based queries face significant challenges and
are inadequate to deal with NP queries. Our two techniques
proposed for NP queries consistently demonstrate good prediction
accuracy, displaying initial success in solving the problem of
predicting performance for NP queries. Another point we want to
stress is that the WIG method works well for both types of
queries, a desirable property that most prediction techniques lack.
4.3 Unknown Query Types
In this section, we run two kinds of experiments without access to
query type labels. First, we assume that only one type of query
exists but the type is unknown. Second, we experiment on a
mixture of content-based and NP queries. The following two
subsections will report results for the two conditions respectively.
4.3.1 Only One Type exists
We assume that all queries are of the same type, that is, they are
either NP queries or content-based queries. We choose WIG to
deal with this case because it shows good prediction accuracy for
both types of queries in the previous section. We consider two
cases: (1) CB: all 150 title queries from the ad-hoc task of the
Terabyte Tracks 2004-2006 (2)NP: all 433 NP queries from the
named page finding task of the Terabyte Tracks 2005 and 2006.
We take a simple strategy by labeling all of the queries in each
case as the same type (either NP or CB) regardless of their actual
type. The computation of WIG will be based on the labeled query
type instead of the actual type. There are four possibilities with
respect to the relation between the actual type and the labeled
type. The correlation with retrieval performance under the four
possibilities is presented in Table 7. For example, the value 0.445
at the intersection between the second row and the third column
shows the Pearson"s correlation coefficient for correlation with
average precision when the content-based queries are incorrectly
labeled as the NP type.
Based on these results, we recommend treating all queries as the
NP type when only one query type exists and accurate query
classification is not feasible, considering the risk that a large loss
of accuracy will occur if NP queries are incorrectly labeled as
content-based queries. These results also demonstrate the strong
adaptability of WIG to different query types.
CB (labeled) NP (labeled)
CB (actual) 0.536 0.445
NP (actual) 0.174 0.467
Table 7: Comparison of Pearson"s correlation coefficients for
correlation with retrieval performance under four possibilities
on the Terabyte Tracks (NP). Bold cases mean the results are
statistically significant at the 0.01 level.
4.3.2 A mixture of contented-based and NP queries
A mixture of the two types of queries is a more realistic situation
that a Web search engine will meet. We evaluate prediction
accuracy by how accurately poorly-performing queries can be
identified by the prediction method assuming that actual query
types are unknown (but we can predict query types). This is a
challenging task because both the predicted and actual
performance for one type of query can be incomparable to that for
the other type.
Next we discuss how to implement our evaluation. We create a
query pool which consists of all of the 150 ad-hoc title queries
from Terabyte Track 2004-2006 and all of the 433 NP queries
from Terabyte Track 2005&2006. We divide the queries in the
pool into classes: good (better than 50% of the queries of the
same type in terms of retrieval performance) and bad
(otherwise). According to these standards, a NP query with the
reciprocal rank above 0.2 or a content-based query with the
average precision above 0.315 will be considered as good.
Then, each time we randomly select one query Q from the pool
with probability p that Q is contented-based. The remaining
queries are used as training data. We first decide the type of query
Q according to a query classifier. Namely, the query classifier
tells us whether query Q is NP or content-based. Based on the
predicted query type and the score computed for query Q by a
prediction technique, a binary decision is made about whether
query Q is good or bad by comparing to the score threshold of the
predicted query type obtained from the training data. Prediction
accuracy is measured by the accuracy of the binary decision. In
our implementation, we repeatedly take a test query from the
query pool and prediction accuracy is computed as the
percentage of correct decisions, that is, a good(bad) query is
predicted to be good (bad). It is obvious that random guessing will
lead to 50% accuracy.
Let us take the WIG method for example to illustrate the process.
Two WIG thresholds (one for NP queries and the other for
content-based queries) are trained by maximizing the prediction
accuracy on the training data. When a test query is labeled as the
NP (CB) type by the query type classifier, it will be predicted to
be good if and only if the WIG score for this query is above the
NP (CB) threshold. Similar procedures will be taken for other
prediction techniques.
Now we briefly introduce the automatic query type classifier used
in this paper. We find that the robustness score, though originally
proposed for performance prediction, is a good indicator of query
types. We find that on average content-based queries have a
much higher robustness score than NP queries. For example,
Figure 2 shows the distributions of robustness scores for NP and
content-based queries. According to this finding, the robustness
score classifier will attach a NP (CB) label to the query if the
robustness score for the query is below (above) a threshold
trained from the training data.
0
0.5
1
1.5
2
2.5
-1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1
NP Content-based
Figure 2: Distribution of robustness scores for NP and CB
queries. The NP queries are the 252 NP topics from the 2005
Terabyte Track. The content-based queries are the 150 ad-hoc
title from the Terabyte Tracks 2004-2006. The probability
distributions are estimated by the Kernel density estimation
method.
Strategies Robust WIG-1 WIG-2 WIG-3 Optimal
p=0.6 0.565 0.624 0.665 0.684 0.701
P=0.4 0.567 0.633 0.654 0.673 0.696
Table 8: Comparison of prediction accuracy for five strategies
in the mixed-query situation. Two ways to sample a query
from the pool: (1) the sampled query is content-based with the
probability p=0.6. (that is, the query is NP with probability
0.4 ) (2) set the probability p=0.4.
We consider five strategies in our experiments. In the first
strategy (denoted by robust), we use the robustness score for
query performance prediction with the help of a perfect query
classifier that always correctly map a query into one of the two
categories (that is, NP or CB). This strategy represents the level of
prediction accuracy that current prediction techniques can achieve
in an ideal condition that query types are known. In the next
following three strategies, the WIG method is adopted for
performance prediction. The difference among the three is that
three different query classifiers are used for each strategy: (1) the
classifier always classifies a query into the NP type. (2) the
Robustness Score
ProbabilityDensity
classifier is the robust score classifier mentioned above. (3) the
classifier is a perfect one. These three strategies are denoted by
WIG-1, WIG-2 and WIG-3 respectively. The reason we are
interested in WIG-1 is based on the results from section 4.3.1. In
the last strategy (denoted by Optimal) which serves as an upper
bound on how well we can do so far, we fully make use of our
prediction techniques for each query type assuming a perfect
query classifier is available. Specifically, we linearly combine
WIG and QF for content-based queries and WIG and FRC for NP
queries.
The results for the five strategies are shown in Table 8. For each
strategy, we try two ways to sample a query from the pool: (1) the
sampled query is CB with probability p=0.6. (the query is NP
with probability 0.4) (2) set the probability p=0.4. From Table 8
We can see that in terms of prediction accuracy WIG-2 (the WIG
method with the automatic query classifier) is not only better than
the first two cases, but also is close to WIG-3 where a perfect
classifier is assumed. Some further improvements over WIG-3 are
observed when combined with other prediction techniques. The
merit of WIG-2 is that it provides a practical solution to
automatically identifying poorly performing queries in a Web
search environment with mixed query types, which poses
considerable obstacles to traditional prediction techniques.
5. CONCLUSIONS AND FUTURE WORK
To our knowledge, our paper is the first to thoroughly explore
prediction of query performance in web search environments. We
demonstrated that our models resulted in higher prediction
accuracy than previously published techniques not specially
devised for web search scenarios. In this paper, we focus on two
types of queries in web search: content-based and Named-Page
(NP) finding queries, corresponding to the ad-hoc retrieval task
and the Named-Page finding task respectively. For both types of
web queries, our prediction models were shown to be
substantially more accurate than the current state-of-the-art
techniques. Furthermore, we considered a more realistic case that
no prior information on query types is available. We
demonstrated that the WIG method is particularly suitable for this
situation. Considering the adaptability of WIG to a range of
collections and query types, one of our future plans is to apply
this method to predict user preference of search results on realistic
data collected from a commercial search engine.
Other than accuracy, another major issue that prediction
techniques have to deal with in a Web environment is efficiency.
Fortunately, since the WIG score is computed just over the terms
and the phrases that appear in the query, this calculation can be
made very efficient with the support of index. On the other hand,
the computation of QF and FRC is relatively less efficient since
QF needs to retrieve the whole collection twice and FRC needs to
repeatedly rank the perturbed documents. How to improve the
efficiency of QF and FRC is our future work.
In addition, the prediction techniques proposed in this paper have
the potential of improving retrieval performance by combining
with other IR techniques. For example, our techniques can be
incorporated to popular query modification techniques such as
query expansion and query relaxation. Guided by performance
prediction, we can make a better decision on when to or how to
modify queries to enhance retrieval effectiveness. We would like
to carry out research in this direction in the future.
6. ACKNOWLEGMENTS
This work was supported in part by the Center for Intelligent
Information Retrieval, in part by the Defense Advanced Research
Projects Agency (DARPA) under contract number 
HR0011-06-C0023, and in part by an award from Google. Any opinions,
findings and conclusions or recommendations expressed in this
material are those of the author and do not necessarily reflect
those of the sponsor. In addition, we thank Donald Metzler for his
valuable comments on this work.
7. REFERENCES
[1] Y. Zhou ,W. B. Croft ,Ranking Robustness: A Novel
Framework to Predict Query Performance, in Proceedings of
CIKM 2006.
[2] D.Carmel, E.Yom-Tov, A.Darlow,D.Pelleg, What Makes a
Query Difficult?, in Proceedings of SIGIR 2006.
[3] C.L.A. Clarke, F. Scholer, I.Soboroff, The TREC 2005
Terabyte Track, In the Online Proceedings of 2005 TREC.
[4] B. He and I.Ounis. Inferring query performance using 
preretrieval predictors. In proceedings of the SPIRE 2004.
[5] S. Tomlinson. Robust, Web and Terabyte Retrieval with
Hummingbird SearchServer at TREC 2004. In the Online
Proceedings of 2004 TREC.
[6] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting
Query Performance, in Proceedings of SIGIR 2002.
[7] V.Vinay, I.J.Cox, N.Mill-Frayling,K.Wood, On Ranking the
Effectiveness of Searcher, in Proceedings of SIGIR 2006.
[8] D.Metzler, W.B.Croft, A Markov Random Filed Model for
Term Dependencies, in Proceedings of SIGIR 2005.
[9] D.Metzler, T.Strohman,Y.Zhou,W.B.Croft, Indri at TREC
2005: Terabyte Track, In the Online Proceedings of 2004
TREC.
[10] P. Ogilvie and J. Callan, Combining document
representations for known-item search, in Proceedings of
SIGIR 2003.
[11] A.Berger, J.Lafferty, Information retrieval as statistical
translation, in Proceedings of SIGIR 1999.
[12] Indri search engine : http://www.lemurproject.org/indri/
[13] I.J. Taneja: On Generalized Information Measures and Their
Applications, Advances in Electronics and Electron Physics,
Academic Press (USA), 76, 1989, 327-413.
[14] S.Cronen-Townsend, Y. Zhou and Croft, W. B. , "A
Framework for Selective Query Expansion," in Proceedings
of CIKM 2004.
[15] F.Song, W.B.Croft, A general language model for
information retrieval, in Proceedings of SIGIR 1999.
[16] Personal email contact with Vishwa Vinay and our own
experiments
[17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, Learning to
Estimate Query Difficulty Including Applications to Missing
Content Detection and Distributed Information retrieval, in
Proceedings of SIGIR 2005
Using Asymmetric Distributions
to Improve Text Classifier Probability Estimates
Paul N. Bennett
Computer Science Dept.
Carnegie Mellon University
Pittsburgh, PA 15213
pbennett+@cs.cmu.edu
ABSTRACT
Text classifiers that give probability estimates are more readily 
applicable in a variety of scenarios. For example, rather than 
choosing one set decision threshold, they can be used in a Bayesian
risk model to issue a run-time decision which minimizes a 
userspecified cost function dynamically chosen at prediction time. 
However, the quality of the probability estimates is crucial. We review a
variety of standard approaches to converting scores (and poor 
probability estimates) from text classifiers to high quality estimates and
introduce new models motivated by the intuition that the empirical
score distribution for the extremely irrelevant, hard to 
discriminate, and obviously relevant items are often significantly 
different. Finally, we analyze the experimental performance of these
models over the outputs of two text classifiers. The analysis 
demonstrates that one of these models is theoretically attractive 
(introducing few new parameters while increasing flexibility), 
computationally efficient, and empirically preferable.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval; I.2.6 [Artificial Intelligence]: Learning; I.5.2 
[Pattern Recognition]: Design Methodology
General Terms
Algorithms, Experimentation, Reliability.
1. INTRODUCTION
Text classifiers that give probability estimates are more flexible
in practice than those that give only a simple classification or even a
ranking. For example, rather than choosing one set decision 
threshold, they can be used in a Bayesian risk model [8] to issue a 
runtime decision which minimizes the expected cost of a user-specified
cost function dynamically chosen at prediction time. This can be
used to minimize a linear utility cost function for filtering tasks
where pre-specified costs of relevant/irrelevant are not available
during training but are specified at prediction time. Furthermore,
the costs can be changed without retraining the model. 
Additionally, probability estimates are often used as the basis of deciding
which document"s label to request next during active learning [17,
23]. Effective active learning can be key in many information 
retrieval tasks where obtaining labeled data can be costly - severely
reducing the amount of labeled data needed to reach the same 
performance as when new labels are requested randomly [17]. Finally,
they are also amenable to making other types of cost-sensitive 
decisions [26] and for combining decisions [3]. However, in all of
these tasks, the quality of the probability estimates is crucial.
Parametric models generally use assumptions that the data 
conform to the model to trade-off flexibility with the ability to estimate
the model parameters accurately with little training data. Since
many text classification tasks often have very little training data, we
focus on parametric methods. However, most of the existing 
parametric methods that have been applied to this task have an 
assumption we find undesirable. While some of these methods allow the
distributions of the documents relevant and irrelevant to the topic
to have different variances, they typically enforce the unnecessary
constraint that the documents are symmetrically distributed around
their respective modes. We introduce several asymmetric 
parametric models that allow us to relax this assumption without 
significantly increasing the number of parameters and demonstrate how
we can efficiently fit the models. Additionally, these models can be
interpreted as assuming the scores produced by the text classifier
have three basic types of empirical behavior - one corresponding
to each of the extremely irrelevant, hard to discriminate, and
obviously relevant items.
We first review related work on improving probability estimates
and score modeling in information retrieval. Then, we discuss in
further detail the need for asymmetric models. After this, we 
describe two specific asymmetric models and, using two standard text
classifiers, na¨ıve Bayes and SVMs, demonstrate how they can be
efficiently used to recalibrate poor probability estimates or produce
high quality probability estimates from raw scores. We then review
experiments using previously proposed methods and the 
asymmetric methods over several text classification corpora to demonstrate
the strengths and weaknesses of the various methods. Finally, we
summarize our contributions and discuss future directions.
2. RELATED WORK
Parametric models have been employed to obtain probability 
estimates in several areas of information retrieval. Lewis & Gale [17]
use logistic regression to recalibrate na¨ıve Bayes though the quality
of the probability estimates are not directly evaluated; it is simply
performed as an intermediate step in active learning. Manmatha
et. al [20] introduced models appropriate to produce probability
estimates from relevance scores returned from search engines and
demonstrated how the resulting probability estimates could be 
subsequently employed to combine the outputs of several search 
engines. They use a different parametric distribution for the relevant
and irrelevant classes, but do not pursue two-sided asymmetric 
distributions for a single class as described here. They also survey the
long history of modeling the relevance scores of search engines.
Our work is similar in flavor to these previous attempts to model
search engine scores, but we target text classifier outputs which we
have found demonstrate a different type of score distribution 
behavior because of the role of training data.
Focus on improving probability estimates has been growing lately.
Zadrozny & Elkan [26] provide a corrective measure for decision
trees (termed curtailment) and a non-parametric method for 
recalibrating na¨ıve Bayes. In more recent work [27], they investigate
using a semi-parametric method that uses a monotonic 
piecewiseconstant fit to the data and apply the method to na¨ıve Bayes and a
linear SVM. While they compared their methods to other 
parametric methods based on symmetry, they fail to provide significance
test results. Our work provides asymmetric parametric methods
which complement the non-parametric and semi-parametric 
methods they propose when data scarcity is an issue. In addition, their
methods reduce the resolution of the scores output by the classifier
(the number of distinct values output), but the methods here do not
have such a weakness since they are continuous functions.
There is a variety of other work that this paper extends. Platt
[22] uses a logistic regression framework that models noisy class
labels to produce probabilities from the raw output of an SVM.
His work showed that this post-processing method not only can
produce probability estimates of similar quality to SVMs directly
trained to produce probabilities (regularized likelihood kernel 
methods), but it also tends to produce sparser kernels (which generalize
better). Finally, Bennett [1] obtained moderate gains by applying
Platt"s method to the recalibration of na¨ıve Bayes but found there
were more problematic areas than when it was applied to SVMs.
Recalibrating poorly calibrated classifiers is not a new problem.
Lindley et. al [19] first proposed the idea of recalibrating classifiers,
and DeGroot & Fienberg [5, 6] gave the now accepted standard
formalization for the problem of assessing calibration initiated by
others [4, 24].
3. PROBLEM DEFINITION & APPROACH
Our work differs from earlier approaches primarily in three points:
(1) We provide asymmetric parametric models suitable for use when
little training data is available; (2) We explicitly analyze the quality
of probability estimates these and competing methods produce and
provide significance tests for these results; (3) We target text 
classifier outputs where a majority of the previous literature targeted the
output of search engines.
3.1 Problem Definition
The general problem we are concerned with is highlighted in
Figure 1. A text classifier produces a prediction about a document
and gives a score s(d) indicating the strength of its decision that
the document belongs to the positive class (relevant to the topic).
We assume throughout there are only two classes: the positive and
the negative (or irrelevant) class ("+" and "-" respectively).
There are two general types of parametric approaches. The first
of these tries to fit the posterior function directly, i.e. there is one
p(s|+) p(s|−)
Bayes" RuleP(+) P(−)
Classifier
P(+| s(d))
Predict class, c(d)={+,−}
confidence s(d) that c(d)=+
Document, d
and give unnormalized
Figure 1: We are concerned with how to perform the box 
highlighted in grey. The internals are for one type of approach.
function estimator that performs a direct mapping of the score s to
the probability P(+|s(d)). The second type of approach breaks the
problem down as shown in the grey box of Figure 1. An estimator
for each of the class-conditional densities (i.e. p(s|+) and p(s|−))
is produced, then Bayes" rule and the class priors are used to obtain
the estimate for P(+|s(d)).
3.2 Motivation for Asymmetric Distributions
Most of the previous parametric approaches to this problem 
either directly or indirectly (when fitting only the posterior) 
correspond to fitting Gaussians to the class-conditional densities; they
differ only in the criterion used to estimate the parameters. We can
visualize this as depicted in Figure 2. Since increasing s usually 
indicates increased likelihood of belonging to the positive class, then
the rightmost distribution usually corresponds to p(s|+).
A B
C
0
0.2
0.4
0.6
0.8
1
−10 −5 0 5 10
p(s|Class={+,−})
Unnormalized Confidence Score s
p(s | Class = +)
p(s | Class = −)
Figure 2: Typical View of Discrimination based on Gaussians
However, using standard Gaussians fails to capitalize on a basic
characteristic commonly seen. Namely, if we have a raw output
score that can be used for discrimination, then the empirical 
behavior between the modes (label B in Figure 2) is often very different
than that outside of the modes (labels A and C in Figure 2). 
Intuitively, the area between the modes corresponds to the hard 
examples, which are difficult for this classifier to distinguish, while the
areas outside the modes are the extreme examples that are usually
easily distinguished. This suggests that we may want to uncouple
the scale of the outside and inside segments of the distribution (as
depicted by the curve denoted as A-Gaussian in Figure 3). As a 
result, an asymmetric distribution may be a more appropriate choice
for application to the raw output score of a classifier.
Ideally (i.e. perfect classification) there will exist scores θ− and
θ+ such that all examples with score greater than θ+ are relevant
and all examples with scores less than θ− are irrelevant. 
Furthermore, no examples fall between θ− and θ+. The distance
| θ− − θ+ | corresponds to the margin in some classifiers, and
an attempt is often made to maximize this quantity. Because text
classifiers have training data to use to separate the classes, the 
final behavior of the score distributions is primarily a factor of the
amount of training data and the consequent separation in the classes
achieved. This is in contrast to search engine retrieval where the
distribution of scores is more a factor of language distribution across
documents, the similarity function, and the length and type of query.
Perfect classification corresponds to using two very asymmetric
distributions, but in this case, the probabilities are actually one and
zero and many methods will work for typical purposes. Practically,
some examples will fall between θ− and θ+, and it is often 
important to estimate the probabilities of these examples well (since they
correspond to the hard examples). Justifications can be given for
both why you may find more and less examples between θ− and θ+
than outside of them, but there are few empirical reasons to believe
that the distributions should be symmetric.
A natural first candidate for an asymmetric distribution is to 
generalize a common symmetric distribution, e.g. the Laplace or the
Gaussian. An asymmetric Laplace distribution can be achieved by
placing two exponentials around the mode in the following manner:
p(x | θ, β, γ) =



βγ
β+γ
exp [−β (θ − x)] x ≤ θ
(β, γ > 0)
βγ
β+γ
exp [−γ (x − θ)] x > θ
(1)
where θ, β, and γ are the model parameters. θ is the mode of the
distribution, β is the inverse scale of the exponential to the left of
the mode, and γ is the inverse scale of the exponential to the right.
We will use the notation Λ(X | θ, β, γ) to refer to this distribution.
0
0.002
0.004
0.006
0.008
0.01
-300 -200 -100 0 100 200
p(s|Class={+,-})
Unnormalized Confidence Score s
Gaussian
A-Gaussian
Figure 3: Gaussians vs. Asymmetric Gaussians. A 
Shortcoming of Symmetric Distributions - The vertical lines show the
modes as estimated nonparametrically.
We can create an asymmetric Gaussian in the same manner:
p(x | θ, σl, σr) =



2√
2π(σl+σr)
exp −(x−θ)2
2σ2
l
x ≤ θ
(σl, σr > 0)
2√
2π(σl+σr)
exp −(x−θ)2
2σ2
r
x > θ
(2)
where θ, σl, and σr are the model parameters. To refer to this
asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr). While
these distributions are composed of halves, the resulting function
is a single continuous distribution.
These distributions allow us to fit our data with much greater
flexibility at the cost of only fitting six parameters. We could 
instead try mixture models for each component or other extensions,
but most other extensions require at least as many parameters (and
can often be more computationally expensive). In addition, the
motivation above should provide significant cause to believe the
underlying distributions actually behave in this way. Furthermore,
this family of distributions can still fit a symmetric distribution,
and finally, in the empirical evaluation, evidence is presented that
demonstrates this asymmetric behavior (see Figure 4).
To our knowledge, neither family of distributions has been 
previously used in machine learning or information retrieval. Both are
termed generalizations of an Asymmetric Laplace in [14], but we
refer to them as described above to reflect the nature of how we
derived them for this task.
3.3 Estimating the Parameters of the 
Asymmetric Distributions
This section develops the method for finding maximum 
likelihood estimates (MLE) of the parameters for the above asymmetric
distributions. In order to find the MLEs, we have two choices: (1)
use numerical estimation to estimate all three parameters at once
(2) fix the value of θ, and estimate the other two (β and γ or σl
and σr) given our choice of θ, then consider alternate values of θ.
Because of the simplicity of analysis in the latter alternative, we
choose this method.
3.3.1 Asymmetric Laplace MLEs
For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼
Λ(X | θ, β, γ), the likelihood is N
i Λ(X | θ, β, γ). Now, we fix
θ and compute the maximum likelihood for that choice of θ. Then,
we can simply consider all choices of θ and choose the one with
the maximum likelihood over all choices of θ.
The complete derivation is omitted because of space but is 
available in [2]. We define the following values:
Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} |
Sl =
x∈D|x≤θ
x Sr =
x∈D|x>θ
x
Dl = Nlθ − Sl Dr = Sr − Nrθ.
Note that Dl and Dr are the sum of the absolute differences 
between the x belonging to the left and right halves of the distribution
(respectively) and θ. Finally the MLEs for β and γ for a fixed θ are:
βMLE =
N
Dl +
√
DrDl
γMLE =
N
Dr +
√
DrDl
. (3)
These estimates are not wholly unexpected since we would obtain
Nl
Dl
if we were to estimate β independently of γ. The elegance of
the formulae is that the estimates will tend to be symmetric only
insofar as the data dictate it (i.e. the closer Dl and Dr are to being
equal, the closer the resulting inverse scales).
By continuity arguments, when N = 0, we assign β = γ = 0
where 0 is a small constant that acts to disperse the distribution to
a uniform. Similarly, when N = 0 and Dl = 0, we assign β = inf
where inf is a very large constant that corresponds to an extremely
sharp distribution (i.e. almost all mass at θ for that half). Dr = 0
is handled similarly.
Assuming that θ falls in some range [φ, ψ] dependent upon only
the observed documents, then this alternative is also easily 
computable. Given Nl, Sl, Nr, Sr, we can compute the posterior and
the MLEs in constant time. In addition, if the scores are sorted,
then we can perform the whole process quite efficiently. Starting
with the minimum θ = φ we would like to try, we loop through the
scores once and set Nl, Sl, Nr, Sr appropriately. Then we increase
θ and just step past the scores that have shifted from the right side
of the distribution to the left. Assuming the number of candidate
θs are O(n), this process is O(n), and the overall process is 
dominated by sorting the scores, O(n log n) (or expected linear time).
3.3.2 Asymmetric Gaussian MLEs
For D = {x1, x2, . . . , xN } where the xi are i.i.d. and X ∼
Γ(X | θ, σl, σr), the likelihood is N
i Γ(X | θ, β, γ). The MLEs
can be worked out similar to the above.
We assume the same definitions as above (the complete 
derivation omitted for space is available in [2]), and in addition, let:
Sl2 =
x∈D|x≤θ
x2
Sr2 =
x∈D|x>θ
x2
Dl2 = Sl2 − Slθ + θ2
Nl Dr2 = Sr2 − Srθ + θ2
Nr.
The analytical solution for the MLEs for a fixed θ is:
σl,MLE =
Dl2 + D
2/3
l2 D
1/3
r2
N
(4)
σr,MLE =
Dr2 + D
2/3
r2 D
1/3
l2
N
. (5)
By continuity arguments, when N = 0, we assign σr = σl =
inf , and when N = 0 and Dl2 = 0 (resp. Dr2 = 0), we 
assign σl = 0 (resp. σr = 0). Again, the same computational
complexity analysis applies to estimating these parameters.
4. EXPERIMENTAL ANALYSIS
4.1 Methods
For each of the methods that use a class prior, we use a smoothed
add-one estimate, i.e. P(c) = |c|+1
N+2
where N is the number of 
documents. For methods that fit the class-conditional densities, p(s|+)
and p(s|−), the resulting densities are inverted using Bayes" rule as
described above. All of the methods below are fit using maximum
likelihood estimates.
For recalibrating a classifier (i.e. correcting poor probability 
estimates output by the classifier), it is usual to use the log-odds of
the classifier"s estimate as s(d). The log-odds are defined to be
log P (+|d)
P (−|d)
. The normal decision threshold (minimizing error) in
terms of log-odds is at zero (i.e. P(+|d) = P(−|d) = 0.5).
Since it scales the outputs to a space [−∞, ∞], the log-odds
make normal (and similar distributions) applicable [19]. Lewis &
Gale [17] give a more motivating viewpoint that fitting the log-odds
is a dampening effect for the inaccurate independence assumption
and a bias correction for inaccurate estimates of the priors. In 
general, fitting the log-odds can serve to boost or dampen the signal
from the original classifier as the data dictate.
Gaussians
A Gaussian is fit to each of the class-conditional densities, using
the usual maximum likelihood estimates. This method is denoted
in the tables below as Gauss.
Asymmetric Gaussians
An asymmetric Gaussian is fit to each of the class-conditional
densities using the maximum likelihood estimation procedure 
described above. Intervals between adjacent scores are divided by 10
in testing candidate θs, i.e. 8 points between actual scores 
occurring in the data set are tested. This method is denoted as A. Gauss.
Laplace Distributions
Even though Laplace distributions are not typically applied to
this task, we also tried this method to isolate why benefit is gained
from the asymmetric form. The usual MLEs were used for 
estimating the location and scale of a classical symmetric Laplace 
distribution as described in [14]. We denote this method as Laplace below.
Asymmetric Laplace Distributions
An asymmetric Laplace is fit to each of the class-conditional
densities using the maximum likelihood estimation procedure 
described above. As with the asymmetric Gaussian, intervals between
adjacent scores are divided by 10 in testing candidate θs. This
method is denoted as A. Laplace below.
Logistic Regression
This method is the first of two methods we evaluated that 
directly fit the posterior, P(+|s(d)). Both methods restrict the set
of families to a two-parameter sigmoid family; they differ 
primarily in their model of class labels. As opposed to the above 
methods, one can argue that an additional boon of these methods is they
completely preserve the ranking given by the classifier. When this
is desired, these methods may be more appropriate. The previous
methods will mostly preserve the rankings, but they can deviate if
the data dictate it. Thus, they may model the data behavior better at
the cost of departing from a monotonicity constraint in the output
of the classifier.
Lewis & Gale [17] use logistic regression to recalibrate na¨ıve
Bayes for subsequent use in active learning. The model they use is:
P(+|s(d)) =
exp(a + b s(d))
1 + exp(a + b s(d))
. (6)
Instead of using the probabilities directly output by the classifier,
they use the loglikelihood ratio of the probabilities, log P (d|+)
P (d|−)
, as
the score s(d). Instead of using this below, we will use the 
logodds ratio. This does not affect the model as it simply shifts all of
the scores by a constant determined by the priors. We refer to this
method as LogReg below.
Logistic Regression with Noisy Class Labels
Platt [22] proposes a framework that extends the logistic 
regression model above to incorporate noisy class labels and uses it to
produce probability estimates from the raw output of an SVM.
This model differs from the LogReg model only in how the 
parameters are estimated. The parameters are still fit using maximum
likelihood estimation, but a model of noisy class labels is used in
addition to allow for the possibility that the class was mislabeled.
The noise is modeled by assuming there is a finite probability of
mislabeling a positive example and of mislabeling a negative 
example; these two noise estimates are determined by the number
of positive examples and the number of negative examples (using
Bayes" rule to infer the probability of incorrect label).
Even though the performance of this model would not be 
expected to deviate much from LogReg, we evaluate it for 
completeness. We refer to this method below as LR+Noise.
4.2 Data
We examined several corpora, including the MSN Web Directory,
Reuters, and TREC-AP.
MSN Web Directory
The MSN Web Directory is a large collection of heterogeneous
web pages (from a May 1999 web snapshot) that have been 
hierarchically classified. We used the same train/test split of 50078/10024
documents as that reported in [9]. The MSN Web hierarchy is a
seven-level hierarchy; we used all 13 of the top-level categories.
The class proportions in the training set vary from 1.15% to 22.29%.
In the testing set, they range from 1.14% to 21.54%. The classes
are general subjects such as Health & Fitness and Travel & Vacation.
Human indexers assigned the documents to zero or more categories.
For the experiments below, we used only the top 1000 words with
highest mutual information for each class; approximately 195K
words appear in at least three training documents.
Reuters
The Reuters 21578 corpus [16] contains Reuters news articles
from 1987. For this data set, we used the ModApte standard train/
test split of 9603/3299 documents (8676 unused documents). The
classes are economic subjects (e.g., acq for acquisitions, earn
for earnings, etc.) that human taggers applied to the document;
a document may have multiple subjects. There are actually 135
classes in this domain (only 90 of which occur in the training and
testing set); however, we only examined the ten most frequent classes
since small numbers of testing examples make interpreting some
performance measures difficult due to high variance.1
Limiting to
the ten largest classes allows us to compare our results to 
previously published results [10, 13, 21, 22]. The class proportions in
the training set vary from 1.88% to 29.96%. In the testing set, they
range from 1.7% to 32.95%.
For the experiments below we used only the top 300 words with
highest mutual information for each class; approximately 15K words
appear in at least three training documents.
TREC-AP
The TREC-AP corpus is a collection of AP news stories from
1988 to 1990. We used the same train/test split of 142791/66992
documents that was used in [18]. As described in [17] (see also
[15]), the categories are defined by keywords in a keyword field.
The title and body fields are used in the experiments below. There
are twenty categories in total. The class proportions in the training
set vary from 0.06% to 2.03%. In the testing set, they range from
0.03% to 4.32%.
For the experiments described below, we use only the top 1000
words with the highest mutual information for each class; 
approximately 123K words appear in at least 3 training documents.
4.3 Classifiers
We selected two classifiers for evaluation. A linear SVM 
classifier which is a discriminative classifier that does not normally 
output probability values, and a na¨ıve Bayes classifier whose 
probability outputs are often poor [1, 7] but can be improved [1, 26, 27].
1
A separate comparison of only LogReg, LR+Noise, and
A. Laplace over all 90 categories of Reuters was also conducted.
After accounting for the variance, that evaluation also supported
the claims made here.
SVM
For linear SVMs, we use the Smox toolkit which is based on
Platt"s Sequential Minimal Optimization algorithm. The features
were represented as continuous values. We used the raw output
score of the SVM as s(d) since it has been shown to be appropriate
before [22]. The normal decision threshold (assuming we are 
seeking to minimize errors) for this classifier is at zero.
Na¨ıve Bayes
The na¨ıve Bayes classifier model is a multinomial model [21].
We smoothed word and class probabilities using a Bayesian 
estimate (with the word prior) and a Laplace m-estimate, respectively.
We use the log-odds estimated by the classifier as s(d). The normal
decision threshold is at zero.
4.4 Performance Measures
We use log-loss [12] and squared error [4, 6] to evaluate the 
quality of the probability estimates. For a document d with class c(d) ∈
{+, −} (i.e. the data have known labels and not probabilities), 
logloss is defined as δ(c(d), +) log P(+|d) + δ(c(d), −) log P(−|d)
where δ(a, b)
.
= 1 if a = b and 0 otherwise. The squared error is
δ(c(d), +)(1 − P(+|d))2
+ δ(c(d), −)(1 − P(−|d))2
. When the
class of a document is correctly predicted with a probability of one,
log-loss is zero and squared error is zero. When the class of a 
document is incorrectly predicted with a probability of one, log-loss
is −∞ and squared error is one. Thus, both measures assess how
close an estimate comes to correctly predicting the item"s class but
vary in how harshly incorrect predictions are penalized.
We report only the sum of these measures and omit the averages
for space. Their averages, average log-loss and mean squared 
error (MSE), can be computed from these totals by dividing by the
number of binary decisions in a corpus.
In addition, we also compare the error of the classifiers at their
default thresholds and with the probabilities. This evaluates how
the probability estimates have improved with respect to the 
decision threshold P(+|d) = 0.5. Thus, error only indicates how the
methods would perform if a false positive was penalized the same
as a false negative and not the general quality of the probability
estimates. It is presented simply to provide the reader with a more
complete understanding of the empirical tendencies of the methods.
We use a a standard paired micro sign test [25] to determine 
statistical significance in the difference of all measures. Only pairs
that the methods disagree on are used in the sign test. This test
compares pairs of scores from two systems with the null 
hypothesis that the number of items they disagree on are binomially 
distributed. We use a significance level of p = 0.01.
4.5 Experimental Methodology
As the categories under consideration in the experiments are not
mutually exclusive, the classification was done by training n binary
classifiers, where n is the number of classes.
In order to generate the scores that each method uses to fit its
probability estimates, we use five-fold cross-validation on the 
training data. We note that even though it is computationally efficient
to perform leave-one-out cross-validation for the na¨ıve Bayes 
classifier, this may not be desirable since the distribution of scores can
be skewed as a result. Of course, as with any application of n-fold
cross-validation, it is also possible to bias the results by holding n
too low and underestimating the performance of the final classifier.
4.6 Results & Discussion
The results for recalibrating na¨ıve Bayes are given in Table 1a.
Table 1b gives results for producing probabilistic outputs for SVMs.
Log-loss Error2
Errors
MSN Web
Gauss -60656.41 10503.30 10754
A.Gauss -57262.26 8727.47 9675
Laplace -45363.84 8617.59 10927
A.Laplace -36765.88 6407.84†
8350
LogReg -36470.99 6525.47 8540
LR+Noise -36468.18 6534.61 8563
na¨ıve Bayes -1098900.83 17117.50 17834
Reuters
Gauss -5523.14 1124.17 1654
A.Gauss -4929.12 652.67 888
Laplace -5677.68 1157.33 1416
A.Laplace -3106.95‡
554.37‡
726
LogReg -3375.63 603.20 786
LR+Noise -3374.15 604.80 785
na¨ıve Bayes -52184.52 1969.41 2121
TREC-AP
Gauss -57872.57 8431.89 9705
A.Gauss -66009.43 7826.99 8865
Laplace -61548.42 9571.29 11442
A.Laplace -48711.55 7251.87‡
8642
LogReg -48250.81 7540.60 8797
LR+Noise -48251.51 7544.84 8801
na¨ıve Bayes -1903487.10 41770.21 43661
Log-loss Error2
Errors
MSN Web
Gauss -54463.32 9090.57 10555
A. Gauss -44363.70 6907.79 8375
Laplace -42429.25 7669.75 10201
A. Laplace -31133.83 5003.32 6170
LogReg -30209.36 5158.74 6480
LR+Noise -30294.01 5209.80 6551
Linear SVM N/A N/A 6602
Reuters
Gauss -3955.33 589.25 735
A. Gauss -4580.46 428.21 532
Laplace -3569.36 640.19 770
A. Laplace -2599.28 412.75 505
LogReg -2575.85 407.48 509
LR+Noise -2567.68 408.82 516
Linear SVM N/A N/A 516
TREC-AP
Gauss -54620.94 6525.71 7321
A. Gauss -77729.49 6062.64 6639
Laplace -54543.19 7508.37 9033
A. Laplace -48414.39 5761.25‡
6572‡
LogReg -48285.56 5914.04 6791
LR+Noise -48214.96 5919.25 6794
Linear SVM N/A N/A 6718
Table 1: (a) Results for na¨ıve Bayes (left) and (b) SVM (right). The best entry for a corpus is in bold. Entries that are statistically
significantly better than all other entries are underlined. A † denotes the method is significantly better than all other methods except
for na¨ıve Bayes. A ‡ denotes the entry is significantly better than all other methods except for A. Gauss (and na¨ıve Bayes for the table
on the left). The reason for this distinction in significance tests is described in the text.
We start with general observations that result from examining
the performance of these methods over the various corpora. The
first is that A. Laplace, LR+Noise, and LogReg, quite clearly 
outperform the other methods. There is usually little difference 
between the performance of LR+Noise and LogReg (both as shown
here and on a decision by decision basis), but this is unsurprising
since LR+Noise just adds noisy class labels to the LogReg model.
With respect to the three different measures, LR+Noise and 
LogReg tend to perform slightly better (but never significantly) than
A. Laplace at some tasks with respect to log-loss and squared error.
However, A. Laplace always produces the least number of errors
for all of the tasks, though at times the degree of improvement is
not significant.
In order to give the reader a better sense of the behavior of these
methods, Figures 4-5 show the fits produced by the most 
competitive of these methods versus the actual data behavior (as estimated
nonparametrically by binning) for class Earn in Reuters. Figure 4
shows the class-conditional densities, and thus only A. Laplace is
shown since LogReg fits the posterior directly. Figure 5 shows the
estimations of the log-odds, (i.e. log P (Earn|s(d))
P (¬Earn|s(d))
). Viewing the
log-odds (rather than the posterior) usually enables errors in 
estimation to be detected by the eye more easily.
We can break things down as the sign test does and just look at
wins and losses on the items that the methods disagree on. Looked
at in this way only two methods (na¨ıve Bayes and A. Gauss) ever
have more pairwise wins than A. Laplace; those two sometimes
have more pairwise wins on log-loss and squared error even though
the total never wins (i.e. they are dragged down by heavy penalties).
In addition, this comparison of pairwise wins means that for
those cases where LogReg and LR+Noise have better scores than
A. Laplace, it would not be deemed significant by the sign test at
any level since they do not have more wins. For example, of the
130K binary decisions over the MSN Web dataset, A. Laplace had
approximately 101K pairwise wins versus LogReg and LR+Noise.
No method ever has more pairwise wins than A. Laplace for the
error comparison nor does any method every achieve a better total.
The basic observation made about na¨ıve Bayes in previous work
is that it tends to produce estimates very close to zero and one [1,
17]. This means if it tends to be right enough of the time, it will
produce results that do not appear significant in a sign test that 
ignores size of difference (as the one here). The totals of the squared
error and log-loss bear out the previous observation that when it"s
wrong it"s really wrong.
There are several interesting points about the performance of the
asymmetric distributions as well. First, A. Gauss performs poorly
because (similar to na¨ıve Bayes) there are some examples where
it is penalized a large amount. This behavior results from a 
general tendency to perform like the picture shown in Figure 3 (note
the crossover at the tails). While the asymmetric Gaussian tends
to place the mode much more accurately than a symmetric 
Gaussian, its asymmetric flexibility combined with its distance function
causes it to distribute too much mass to the outside tails while 
failing to fit around the mode accurately enough to compensate. Figure
3 is actually a result of fitting the two distributions to real data. As
a result, at the tails there can be a large discrepancy between the
likelihood of belonging to each class. Thus when there are no 
outliers A. Gauss can perform quite competitively, but when there is an
0
0.002
0.004
0.006
0.008
0.01
0.012
-600 -400 -200 0 200 400
p(s(d)|Class={+,-})
s(d) = naive Bayes log-odds
Train
Test
A.Laplace
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
-15 -10 -5 0 5 10 15
p(s(d)|Class={+,-})
s(d) = linear SVM raw score
Train
Test
A.Laplace
Figure 4: The empirical distribution of classifier scores for documents in the training and the test set for class Earn in Reuters.
Also shown is the fit of the asymmetric Laplace distribution to the training score distribution. The positive class (i.e. Earn) is the
distribution on the right in each graph, and the negative class (i.e. ¬Earn) is that on the left in each graph.
-6
-4
-2
0
2
4
6
8
-250 -200 -150 -100 -50 0 50 100 150
LogOdds=logP(+|s(d))-logP(-|s(d))
s(d) = naive Bayes log-odds
Train
Test
A.Laplace
LogReg
-5
0
5
10
15
-4 -2 0 2 4 6
LogOdds=logP(+|s(d))-logP(-|s(d))
s(d) = linear SVM raw score
Train
Test
A.Laplace
LogReg
Figure 5: The fit produced by various methods compared to the empirical log-odds of the training data for class Earn in Reuters.
outlier A. Gauss is penalized quite heavily. There are enough such
cases overall that it seems clearly inferior to the top three methods.
However, the asymmetric Laplace places much more emphasis
around the mode (Figure 4) because of the different distance 
function (think of the sharp peak of an exponential). As a result most
of the mass stays centered around the mode, while the asymmetric
parameters still allow more flexibility than the standard Laplace.
Since the standard Laplace also corresponds to a piecewise fit in the
log-odds space, this highlights that part of the power of the 
asymmetric methods is their sensitivity in placing the knots at the actual
modes - rather than the symmetric assumption that the means 
correspond to the modes. Additionally, the asymmetric methods have
greater flexibility in fitting the slopes of the line segments as well.
Even in cases where the test distribution differs from the training
distribution (Figure 4), A. Laplace still yields a solution that gives
a better fit than LogReg (Figure 5), the next best competitor.
Finally, we can make a few observations about the usefulness
of the various performance metrics. First, log-loss only awards a
finite amount of credit as the degree to which something is 
correct improves (i.e. there are diminishing returns as it approaches
zero), but it can infinitely penalize for a wrong estimate. Thus, it
is possible for one outlier to skew the totals, but misclassifying this
example may not matter for any but a handful of actual utility 
functions used in practice. Secondly, squared error has a weakness in
the other direction. That is, its penalty and reward are bounded in
[0, 1], but if the number of errors is small enough, it is possible for
a method to appear better when it is producing what we generally
consider unhelpful probability estimates. For example, consider a
method that only estimates probabilities as zero or one (which na¨ıve
Bayes tends to but doesn"t quite reach if you use smoothing). This
method could win according to squared error, but with just one 
error it would never perform better on log-loss than any method that
assigns some non-zero probability to each outcome. For these 
reasons, we recommend that neither of these are used in isolation as
they each give slightly different insights to the quality of the 
estimates produced. These observations are straightforward from the
definitions but are underscored by the evaluation.
5. FUTURE WORK
A promising extension to the work presented here is a hybrid
distribution of a Gaussian (on the outside slopes) and exponentials
(on the inner slopes). From the empirical evidence presented in
[22], the expectation is that such a distribution might allow more
emphasis of the probability mass around the modes (as with the
exponential) while still providing more accurate estimates toward
the tails.
Just as logistic regression allows the log-odds of the posterior
distribution to be fit directly with a line, we could directly fit the
log-odds of the posterior with a three-piece line (a spline) instead of
indirectly doing the same thing by fitting the asymmetric Laplace.
This approach may provide more power since it retains the 
asymmetry assumption but not the assumption that the class-conditional
densities are from an asymmetric Laplace.
Finally, extending these methods to the outputs of other 
discriminative classifiers is an open area. We are currently evaluating the
appropriateness of these methods for the output of a voted 
perceptron [11]. By analogy to the log-odds, the operative score that 
appears promising is log
weight perceptrons voting +
weight perceptrons voting −
.
6. SUMMARY AND CONCLUSIONS
We have reviewed a wide variety of parametric methods for 
producing probability estimates from the raw scores of a discriminative
classifier and for recalibrating an uncalibrated probabilistic 
classifier. In addition, we have introduced two new families that attempt
to capitalize on the asymmetric behavior that tends to arise from
learning a discrimination function. We have given an efficient way
to estimate the parameters of these distributions.
While these distributions attempt to strike a balance between the
generalization power of parametric distributions and the flexibility
that the added asymmetric parameters give, the asymmetric 
Gaussian appears to have too great of an emphasis away from the modes.
In striking contrast, the asymmetric Laplace distribution appears to
be preferable over several large text domains and a variety of 
performance measures to the primary competing parametric methods,
though comparable performance is sometimes achieved with one
of two varieties of logistic regression. Given the ease of 
estimating the parameters of this distribution, it is a good first choice for
producing quality probability estimates.
Acknowledgments
We are grateful to Francisco Pereira for the sign test code, Anton
Likhodedov for logistic regression code, and John Platt for the code
support for the linear SVM classifier toolkit Smox. Also, we 
sincerely thank Chris Meek and John Platt for the very useful advice
provided in the early stages of this work. Thanks also to Jaime
Carbonell and John Lafferty for their useful feedback on the final
versions of this paper.
7. REFERENCES
[1] P. N. Bennett. Assessing the calibration of naive bayes"
posterior estimates. Technical Report CMU-CS-00-155,
Carnegie Mellon, School of Computer Science, 2000.
[2] P. N. Bennett. Using asymmetric distributions to improve
classifier probabilities: A comparison of new and standard
parametric methods. Technical Report CMU-CS-02-126,
Carnegie Mellon, School of Computer Science, 2002.
[3] H. Bourlard and N. Morgan. A continuous speech
recognition system embedding mlp into hmm. In NIPS "89,
1989.
[4] G. Brier. Verification of forecasts expressed in terms of
probability. Monthly Weather Review, 78:1-3, 1950.
[5] M. H. DeGroot and S. E. Fienberg. The comparison and
evaluation of forecasters. Statistician, 32:12-22, 1983.
[6] M. H. DeGroot and S. E. Fienberg. Comparing probability
forecasters: Basic binary concepts and multivariate
extensions. In P. Goel and A. Zellner, editors, Bayesian
Inference and Decision Techniques. Elsevier Science
Publishers B.V., 1986.
[7] P. Domingos and M. Pazzani. Beyond independence:
Conditions for the optimality of the simple bayesian
classifier. In ICML "96, 1996.
[8] R. Duda, P. Hart, and D. Stork. Pattern Classification. John
Wiley & Sons, Inc., 2001.
[9] S. T. Dumais and H. Chen. Hierarchical classification of web
content. In SIGIR "00, 2000.
[10] S. T. Dumais, J. Platt, D. Heckerman, and M. Sahami.
Inductive learning algorithms and representations for text
categorization. In CIKM "98, 1998.
[11] Y. Freund and R. Schapire. Large margin classification using
the perceptron algorithm. Machine Learning, 37(3):277-296,
1999.
[12] I. Good. Rational decisions. Journal of the Royal Statistical
Society, Series B, 1952.
[13] T. Joachims. Text categorization with support vector
machines: Learning with many relevant features. In ECML
"98, 1998.
[14] S. Kotz, T. J. Kozubowski, and K. Podgorski. The Laplace
Distribution and Generalizations: A Revisit with
Applications to Communications, Economics, Engineering,
and Finance. Birkh¨auser, 2001.
[15] D. D. Lewis. A sequential algorithm for training text
classifiers: Corrigendum and additional data. SIGIR Forum,
29(2):13-19, Fall 1995.
[16] D. D. Lewis. Reuters-21578, distribution 1.0.
http://www.daviddlewis.com/resources/
testcollections/reuters21578, January 1997.
[17] D. D. Lewis and W. A. Gale. A sequential algorithm for
training text classifiers. In SIGIR "94, 1994.
[18] D. D. Lewis, R. E. Schapire, J. P. Callan, and R. Papka.
Training algorithms for linear text classifiers. In SIGIR "96,
1996.
[19] D. Lindley, A. Tversky, and R. Brown. On the reconciliation
of probability assessments. Journal of the Royal Statistical
Society, 1979.
[20] R. Manmatha, T. Rath, and F. Feng. Modeling score
distributions for combining the outputs of search engines. In
SIGIR "01, 2001.
[21] A. McCallum and K. Nigam. A comparison of event models
for naive bayes text classification. In AAAI "98, Workshop on
Learning for Text Categorization, 1998.
[22] J. C. Platt. Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods. In A. J.
Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors,
Advances in Large Margin Classifiers. MIT Press, 1999.
[23] M. Saar-Tsechansky and F. Provost. Active learning for class
probability estimation and ranking. In IJCAI "01, 2001.
[24] R. L. Winkler. Scoring rules and the evaluation of probability
assessors. Journal of the American Statistical Association,
1969.
[25] Y. Yang and X. Liu. A re-examination of text categorization
methods. In SIGIR "99, 1999.
[26] B. Zadrozny and C. Elkan. Obtaining calibrated probability
estimates from decision trees and naive bayesian classifiers.
In ICML "01, 2001.
[27] B. Zadrozny and C. Elkan. Reducing multiclass to binary by
coupling probability estimates. In KDD "02, 2002.
